{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:49.558042Z",
     "start_time": "2019-10-07T13:35:49.311038Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:49.707830Z",
     "start_time": "2019-10-07T13:35:49.700854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\python\\recommender\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:50.649563Z",
     "start_time": "2019-10-07T13:35:50.229518Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import get_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:52.565182Z",
     "start_time": "2019-10-07T13:35:51.014089Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import SeqStackoverflow\n",
    "from models import FMLearner, TorchFM, TorchHrmFM, TorchPrmeFM, TorchTransFM\n",
    "from models.fm_learner import simple_loss, trans_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:52.791089Z",
     "start_time": "2019-10-07T13:35:52.567091Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = T.cuda.current_device()\n",
    "BATCH = 2000\n",
    "SHUFFLE = True\n",
    "WORKERS = 0\n",
    "NEG_SAMPLE = 5\n",
    "item_path = Path(\"./inputs/stackoverflow/item.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:54.353090Z",
     "start_time": "2019-10-07T13:35:52.793088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original user size: 75572\n",
      "Filtered user size: 14795\n",
      "Original item dataframe shape: (299845, 9)\n",
      "Filtered item dataframe shape: (217223, 9)\n",
      "Remove number of questions less than 10 date\n",
      "Filter item dateframe shape: (217146, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datasets.torch_stackoverflow.SeqStackoverflow at 0x1b49ee7f320>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = SeqStackoverflow(data_path=item_path, min_user=4)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:56.704276Z",
     "start_time": "2019-10-07T13:35:56.702276Z"
    }
   },
   "outputs": [],
   "source": [
    "db.config_db(batch_size=BATCH,\n",
    "             shuffle=SHUFFLE,\n",
    "             num_workers=WORKERS,\n",
    "             device=DEVICE,\n",
    "             neg_sample=NEG_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:57.609326Z",
     "start_time": "2019-10-07T13:35:57.605346Z"
    }
   },
   "outputs": [],
   "source": [
    "# regst setting\n",
    "LINEAR_REG = 1\n",
    "EMB_REG = 1\n",
    "TRANS_REG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:58.083892Z",
     "start_time": "2019-10-07T13:35:58.078866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_loss at 0x000001B49EE2A1E0>, 1, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_loss_callback = partial(simple_loss, LINEAR_REG, EMB_REG)\n",
    "simple_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:35:58.514210Z",
     "start_time": "2019-10-07T13:35:58.509241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_loss at 0x000001B49EE83400>, 1, 1, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_loss_callback = partial(trans_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_loss_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "### Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:36:00.089080Z",
     "start_time": "2019-10-07T13:36:00.086083Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:36:03.032696Z",
     "start_time": "2019-10-07T13:36:03.028689Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DECAY_FREQ = 1000\n",
    "DECAY_GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:01:55.313589Z",
     "start_time": "2019-10-07T13:01:54.501575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchFM()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model = TorchFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:01:59.984752Z",
     "start_time": "2019-10-07T13:01:59.979751Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(fm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:02:03.536412Z",
     "start_time": "2019-10-07T13:02:01.074100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1a692830860>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_learner = FMLearner(fm_model, adam_opt, schedular, db)\n",
    "fm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:03:15.918758Z",
     "start_time": "2019-10-07T13:03:15.915786Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:09:48.401444Z",
     "start_time": "2019-10-07T13:03:17.028789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 156350.40846478133\n",
      "Epoch 0 step 1: training accuarcy: 0.3155\n",
      "Epoch 0 step 1: training loss: 151627.33504858427\n",
      "Epoch 0 step 2: training accuarcy: 0.3795\n",
      "Epoch 0 step 2: training loss: 147201.86598852006\n",
      "Epoch 0 step 3: training accuarcy: 0.41100000000000003\n",
      "Epoch 0 step 3: training loss: 142689.1626520957\n",
      "Epoch 0 step 4: training accuarcy: 0.481\n",
      "Epoch 0 step 4: training loss: 138534.1221418247\n",
      "Epoch 0 step 5: training accuarcy: 0.494\n",
      "Epoch 0 step 5: training loss: 134301.95268291325\n",
      "Epoch 0 step 6: training accuarcy: 0.5545\n",
      "Epoch 0 step 6: training loss: 130296.83493591996\n",
      "Epoch 0 step 7: training accuarcy: 0.5875\n",
      "Epoch 0 step 7: training loss: 126347.68557358337\n",
      "Epoch 0 step 8: training accuarcy: 0.626\n",
      "Epoch 0 step 8: training loss: 122515.48753288513\n",
      "Epoch 0 step 9: training accuarcy: 0.6515\n",
      "Epoch 0 step 9: training loss: 118732.31667440773\n",
      "Epoch 0 step 10: training accuarcy: 0.7045\n",
      "Epoch 0 step 10: training loss: 115100.44612814624\n",
      "Epoch 0 step 11: training accuarcy: 0.733\n",
      "Epoch 0 step 11: training loss: 111577.81386793092\n",
      "Epoch 0 step 12: training accuarcy: 0.761\n",
      "Epoch 0 step 12: training loss: 108108.79117223722\n",
      "Epoch 0 step 13: training accuarcy: 0.791\n",
      "Epoch 0 step 13: training loss: 104765.80129092236\n",
      "Epoch 0 step 14: training accuarcy: 0.8130000000000001\n",
      "Epoch 0 step 14: training loss: 101463.11823100585\n",
      "Epoch 0 step 15: training accuarcy: 0.8485\n",
      "Epoch 0 step 15: training loss: 98334.10186114597\n",
      "Epoch 0 step 16: training accuarcy: 0.85\n",
      "Epoch 0 step 16: training loss: 95215.94794317785\n",
      "Epoch 0 step 17: training accuarcy: 0.8785000000000001\n",
      "Epoch 0 step 17: training loss: 92226.78355370075\n",
      "Epoch 0 step 18: training accuarcy: 0.887\n",
      "Epoch 0 step 18: training loss: 89297.73204255535\n",
      "Epoch 0 step 19: training accuarcy: 0.903\n",
      "Epoch 0 step 19: training loss: 86422.10527363382\n",
      "Epoch 0 step 20: training accuarcy: 0.925\n",
      "Epoch 0 step 20: training loss: 83675.72897107358\n",
      "Epoch 0 step 21: training accuarcy: 0.931\n",
      "Epoch 0 step 21: training loss: 81016.79582933092\n",
      "Epoch 0 step 22: training accuarcy: 0.926\n",
      "Epoch 0 step 22: training loss: 78378.2422499381\n",
      "Epoch 0 step 23: training accuarcy: 0.9420000000000001\n",
      "Epoch 0 step 23: training loss: 75824.03400860852\n",
      "Epoch 0 step 24: training accuarcy: 0.9485\n",
      "Epoch 0 step 24: training loss: 73354.85788180963\n",
      "Epoch 0 step 25: training accuarcy: 0.9585\n",
      "Epoch 0 step 25: training loss: 70980.41296962881\n",
      "Epoch 0 step 26: training accuarcy: 0.9505\n",
      "Epoch 0 step 26: training loss: 68630.48287850045\n",
      "Epoch 0 step 27: training accuarcy: 0.9570000000000001\n",
      "Epoch 0 step 27: training loss: 66346.446431114\n",
      "Epoch 0 step 28: training accuarcy: 0.97\n",
      "Epoch 0 step 28: training loss: 64172.215345495635\n",
      "Epoch 0 step 29: training accuarcy: 0.9645\n",
      "Epoch 0 step 29: training loss: 62022.91651912747\n",
      "Epoch 0 step 30: training accuarcy: 0.9705\n",
      "Epoch 0 step 30: training loss: 59940.092180765656\n",
      "Epoch 0 step 31: training accuarcy: 0.9755\n",
      "Epoch 0 step 31: training loss: 57942.39516060224\n",
      "Epoch 0 step 32: training accuarcy: 0.9695\n",
      "Epoch 0 step 32: training loss: 55970.53734116769\n",
      "Epoch 0 step 33: training accuarcy: 0.977\n",
      "Epoch 0 step 33: training loss: 54067.866722811224\n",
      "Epoch 0 step 34: training accuarcy: 0.9825\n",
      "Epoch 0 step 34: training loss: 52221.20925564126\n",
      "Epoch 0 step 35: training accuarcy: 0.985\n",
      "Epoch 0 step 35: training loss: 50444.78076647365\n",
      "Epoch 0 step 36: training accuarcy: 0.9835\n",
      "Epoch 0 step 36: training loss: 48714.38969560089\n",
      "Epoch 0 step 37: training accuarcy: 0.982\n",
      "Epoch 0 step 37: training loss: 47018.07863096842\n",
      "Epoch 0 step 38: training accuarcy: 0.9895\n",
      "Epoch 0 step 38: training loss: 45387.1655902244\n",
      "Epoch 0 step 39: training accuarcy: 0.9915\n",
      "Epoch 0 step 39: training loss: 43825.88567856383\n",
      "Epoch 0 step 40: training accuarcy: 0.9855\n",
      "Epoch 0 step 40: training loss: 42278.390718422976\n",
      "Epoch 0 step 41: training accuarcy: 0.992\n",
      "Epoch 0 step 41: training loss: 40802.02274008841\n",
      "Epoch 0 step 42: training accuarcy: 0.9905\n",
      "Epoch 0 step 42: training loss: 39371.72623954484\n",
      "Epoch 0 step 43: training accuarcy: 0.9895\n",
      "Epoch 0 step 43: training loss: 37974.835522275316\n",
      "Epoch 0 step 44: training accuarcy: 0.994\n",
      "Epoch 0 step 44: training loss: 36644.90982493693\n",
      "Epoch 0 step 45: training accuarcy: 0.9865\n",
      "Epoch 0 step 45: training loss: 35324.91112609964\n",
      "Epoch 0 step 46: training accuarcy: 0.9935\n",
      "Epoch 0 step 46: training loss: 34063.50064102103\n",
      "Epoch 0 step 47: training accuarcy: 0.9935\n",
      "Epoch 0 step 47: training loss: 32846.73665495409\n",
      "Epoch 0 step 48: training accuarcy: 0.9935\n",
      "Epoch 0 step 48: training loss: 31668.44850448678\n",
      "Epoch 0 step 49: training accuarcy: 0.9925\n",
      "Epoch 0 step 49: training loss: 30521.985934231107\n",
      "Epoch 0 step 50: training accuarcy: 0.994\n",
      "Epoch 0 step 50: training loss: 29411.656143084223\n",
      "Epoch 0 step 51: training accuarcy: 0.996\n",
      "Epoch 0 step 51: training loss: 28341.79737083757\n",
      "Epoch 0 step 52: training accuarcy: 0.998\n",
      "Epoch 0 step 52: training loss: 27312.973749602752\n",
      "Epoch 0 step 53: training accuarcy: 0.9955\n",
      "Epoch 0 step 53: training loss: 26314.35731226694\n",
      "Epoch 0 step 54: training accuarcy: 0.996\n",
      "Epoch 0 step 54: training loss: 25349.260470341604\n",
      "Epoch 0 step 55: training accuarcy: 0.995\n",
      "Epoch 0 step 55: training loss: 24414.639134694116\n",
      "Epoch 0 step 56: training accuarcy: 0.996\n",
      "Epoch 0 step 56: training loss: 23512.055256943862\n",
      "Epoch 0 step 57: training accuarcy: 0.998\n",
      "Epoch 0 step 57: training loss: 22637.156835734157\n",
      "Epoch 0 step 58: training accuarcy: 0.9985\n",
      "Epoch 0 step 58: training loss: 21797.09134961224\n",
      "Epoch 0 step 59: training accuarcy: 0.9985\n",
      "Epoch 0 step 59: training loss: 20984.551018511076\n",
      "Epoch 0 step 60: training accuarcy: 0.9985\n",
      "Epoch 0 step 60: training loss: 20201.076985709922\n",
      "Epoch 0 step 61: training accuarcy: 0.998\n",
      "Epoch 0 step 61: training loss: 19446.81990345337\n",
      "Epoch 0 step 62: training accuarcy: 0.998\n",
      "Epoch 0 step 62: training loss: 18711.039767079732\n",
      "Epoch 0 step 63: training accuarcy: 0.9995\n",
      "Epoch 0 step 63: training loss: 18004.715574827693\n",
      "Epoch 0 step 64: training accuarcy: 0.999\n",
      "Epoch 0 step 64: training loss: 17325.355649481586\n",
      "Epoch 0 step 65: training accuarcy: 0.9995\n",
      "Epoch 0 step 65: training loss: 16665.841794542288\n",
      "Epoch 0 step 66: training accuarcy: 0.9995\n",
      "Epoch 0 step 66: training loss: 16033.51072386963\n",
      "Epoch 0 step 67: training accuarcy: 0.999\n",
      "Epoch 0 step 67: training loss: 15427.421167155313\n",
      "Epoch 0 step 68: training accuarcy: 0.9965\n",
      "Epoch 0 step 68: training loss: 14834.224618208458\n",
      "Epoch 0 step 69: training accuarcy: 0.999\n",
      "Epoch 0 step 69: training loss: 14265.803309001434\n",
      "Epoch 0 step 70: training accuarcy: 0.9985\n",
      "Epoch 0 step 70: training loss: 13716.221206861783\n",
      "Epoch 0 step 71: training accuarcy: 0.9995\n",
      "Epoch 0 step 71: training loss: 13189.374030148882\n",
      "Epoch 0 step 72: training accuarcy: 0.999\n",
      "Epoch 0 step 72: training loss: 12677.408933555977\n",
      "Epoch 0 step 73: training accuarcy: 0.9995\n",
      "Epoch 0 step 73: training loss: 12188.711113456136\n",
      "Epoch 0 step 74: training accuarcy: 0.999\n",
      "Epoch 0 step 74: training loss: 11714.530576447743\n",
      "Epoch 0 step 75: training accuarcy: 1.0\n",
      "Epoch 0 step 75: training loss: 11260.697090566175\n",
      "Epoch 0 step 76: training accuarcy: 0.9995\n",
      "Epoch 0 step 76: training loss: 10818.994345309613\n",
      "Epoch 0 step 77: training accuarcy: 1.0\n",
      "Epoch 0 step 77: training loss: 10397.43969218243\n",
      "Epoch 0 step 78: training accuarcy: 1.0\n",
      "Epoch 0 step 78: training loss: 9991.500250569708\n",
      "Epoch 0 step 79: training accuarcy: 0.999\n",
      "Epoch 0 step 79: training loss: 9599.125563550995\n",
      "Epoch 0 step 80: training accuarcy: 0.9995\n",
      "Epoch 0 step 80: training loss: 9221.495046468832\n",
      "Epoch 0 step 81: training accuarcy: 0.9995\n",
      "Epoch 0 step 81: training loss: 8857.965023194032\n",
      "Epoch 0 step 82: training accuarcy: 1.0\n",
      "Epoch 0 step 82: training loss: 8509.29521671763\n",
      "Epoch 0 step 83: training accuarcy: 1.0\n",
      "Epoch 0 step 83: training loss: 8173.837748320653\n",
      "Epoch 0 step 84: training accuarcy: 0.9995\n",
      "Epoch 0 step 84: training loss: 7849.706336817347\n",
      "Epoch 0 step 85: training accuarcy: 1.0\n",
      "Epoch 0 step 85: training loss: 7538.649741565308\n",
      "Epoch 0 step 86: training accuarcy: 1.0\n",
      "Epoch 0 step 86: training loss: 7239.306818449358\n",
      "Epoch 0 step 87: training accuarcy: 1.0\n",
      "Epoch 0 step 87: training loss: 6951.819541725425\n",
      "Epoch 0 step 88: training accuarcy: 1.0\n",
      "Epoch 0 step 88: training loss: 6674.247327547107\n",
      "Epoch 0 step 89: training accuarcy: 0.9995\n",
      "Epoch 0 step 89: training loss: 6409.84932747882\n",
      "Epoch 0 step 90: training accuarcy: 0.9995\n",
      "Epoch 0 step 90: training loss: 6153.6449512804975\n",
      "Epoch 0 step 91: training accuarcy: 1.0\n",
      "Epoch 0 step 91: training loss: 5905.673914980387\n",
      "Epoch 0 step 92: training accuarcy: 1.0\n",
      "Epoch 0 step 92: training loss: 5669.717810544858\n",
      "Epoch 0 step 93: training accuarcy: 1.0\n",
      "Epoch 0 step 93: training loss: 5441.641325852566\n",
      "Epoch 0 step 94: training accuarcy: 0.9999999999999999\n",
      "Epoch 0: train loss 49039.191661085395, train accuarcy 0.9198328852653503\n",
      "Epoch 0: valid loss 5249.524791414713, valid accuarcy 0.9995944499969482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▌                                                                                                                                          | 1/5 [01:18<05:14, 78.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 94: training loss: 5226.360627683903\n",
      "Epoch 1 step 95: training accuarcy: 0.999\n",
      "Epoch 1 step 95: training loss: 5015.117588995671\n",
      "Epoch 1 step 96: training accuarcy: 1.0\n",
      "Epoch 1 step 96: training loss: 4813.5409984518765\n",
      "Epoch 1 step 97: training accuarcy: 1.0\n",
      "Epoch 1 step 97: training loss: 4620.309040198561\n",
      "Epoch 1 step 98: training accuarcy: 1.0\n",
      "Epoch 1 step 98: training loss: 4433.890697158946\n",
      "Epoch 1 step 99: training accuarcy: 1.0\n",
      "Epoch 1 step 99: training loss: 4257.869016747632\n",
      "Epoch 1 step 100: training accuarcy: 0.9995\n",
      "Epoch 1 step 100: training loss: 4084.7167375647077\n",
      "Epoch 1 step 101: training accuarcy: 1.0\n",
      "Epoch 1 step 101: training loss: 3919.3381046011314\n",
      "Epoch 1 step 102: training accuarcy: 1.0\n",
      "Epoch 1 step 102: training loss: 3762.4667937167324\n",
      "Epoch 1 step 103: training accuarcy: 1.0\n",
      "Epoch 1 step 103: training loss: 3610.8007383504987\n",
      "Epoch 1 step 104: training accuarcy: 1.0\n",
      "Epoch 1 step 104: training loss: 3465.4663681259335\n",
      "Epoch 1 step 105: training accuarcy: 1.0\n",
      "Epoch 1 step 105: training loss: 3327.8572399622312\n",
      "Epoch 1 step 106: training accuarcy: 0.9995\n",
      "Epoch 1 step 106: training loss: 3192.9807483755517\n",
      "Epoch 1 step 107: training accuarcy: 1.0\n",
      "Epoch 1 step 107: training loss: 3065.3292033196344\n",
      "Epoch 1 step 108: training accuarcy: 1.0\n",
      "Epoch 1 step 108: training loss: 2941.172041678875\n",
      "Epoch 1 step 109: training accuarcy: 1.0\n",
      "Epoch 1 step 109: training loss: 2822.03145692227\n",
      "Epoch 1 step 110: training accuarcy: 1.0\n",
      "Epoch 1 step 110: training loss: 2709.065117236274\n",
      "Epoch 1 step 111: training accuarcy: 1.0\n",
      "Epoch 1 step 111: training loss: 2600.1126460633755\n",
      "Epoch 1 step 112: training accuarcy: 1.0\n",
      "Epoch 1 step 112: training loss: 2496.5573716959134\n",
      "Epoch 1 step 113: training accuarcy: 1.0\n",
      "Epoch 1 step 113: training loss: 2396.840060271497\n",
      "Epoch 1 step 114: training accuarcy: 1.0\n",
      "Epoch 1 step 114: training loss: 2300.0213189852298\n",
      "Epoch 1 step 115: training accuarcy: 1.0\n",
      "Epoch 1 step 115: training loss: 2208.4634419358663\n",
      "Epoch 1 step 116: training accuarcy: 1.0\n",
      "Epoch 1 step 116: training loss: 2121.110927984306\n",
      "Epoch 1 step 117: training accuarcy: 1.0\n",
      "Epoch 1 step 117: training loss: 2036.8647136112752\n",
      "Epoch 1 step 118: training accuarcy: 1.0\n",
      "Epoch 1 step 118: training loss: 1955.6015310702066\n",
      "Epoch 1 step 119: training accuarcy: 1.0\n",
      "Epoch 1 step 119: training loss: 1878.4614751625188\n",
      "Epoch 1 step 120: training accuarcy: 1.0\n",
      "Epoch 1 step 120: training loss: 1804.7186953705097\n",
      "Epoch 1 step 121: training accuarcy: 1.0\n",
      "Epoch 1 step 121: training loss: 1733.6909280879406\n",
      "Epoch 1 step 122: training accuarcy: 1.0\n",
      "Epoch 1 step 122: training loss: 1665.18276686301\n",
      "Epoch 1 step 123: training accuarcy: 1.0\n",
      "Epoch 1 step 123: training loss: 1599.442015771901\n",
      "Epoch 1 step 124: training accuarcy: 1.0\n",
      "Epoch 1 step 124: training loss: 1537.6733220932636\n",
      "Epoch 1 step 125: training accuarcy: 1.0\n",
      "Epoch 1 step 125: training loss: 1477.1463261153833\n",
      "Epoch 1 step 126: training accuarcy: 1.0\n",
      "Epoch 1 step 126: training loss: 1419.3694903374212\n",
      "Epoch 1 step 127: training accuarcy: 1.0\n",
      "Epoch 1 step 127: training loss: 1364.7188131278783\n",
      "Epoch 1 step 128: training accuarcy: 1.0\n",
      "Epoch 1 step 128: training loss: 1311.7727433589\n",
      "Epoch 1 step 129: training accuarcy: 1.0\n",
      "Epoch 1 step 129: training loss: 1262.0746415523977\n",
      "Epoch 1 step 130: training accuarcy: 1.0\n",
      "Epoch 1 step 130: training loss: 1214.3395086611151\n",
      "Epoch 1 step 131: training accuarcy: 1.0\n",
      "Epoch 1 step 131: training loss: 1167.7965685384977\n",
      "Epoch 1 step 132: training accuarcy: 1.0\n",
      "Epoch 1 step 132: training loss: 1122.8935002024525\n",
      "Epoch 1 step 133: training accuarcy: 1.0\n",
      "Epoch 1 step 133: training loss: 1080.6015082034842\n",
      "Epoch 1 step 134: training accuarcy: 1.0\n",
      "Epoch 1 step 134: training loss: 1040.8161405535118\n",
      "Epoch 1 step 135: training accuarcy: 1.0\n",
      "Epoch 1 step 135: training loss: 1001.0940128273619\n",
      "Epoch 1 step 136: training accuarcy: 1.0\n",
      "Epoch 1 step 136: training loss: 964.1409767401702\n",
      "Epoch 1 step 137: training accuarcy: 1.0\n",
      "Epoch 1 step 137: training loss: 929.3998148647212\n",
      "Epoch 1 step 138: training accuarcy: 1.0\n",
      "Epoch 1 step 138: training loss: 893.6717143415303\n",
      "Epoch 1 step 139: training accuarcy: 1.0\n",
      "Epoch 1 step 139: training loss: 862.8461027408555\n",
      "Epoch 1 step 140: training accuarcy: 1.0\n",
      "Epoch 1 step 140: training loss: 830.4143386567744\n",
      "Epoch 1 step 141: training accuarcy: 1.0\n",
      "Epoch 1 step 141: training loss: 801.8868588143436\n",
      "Epoch 1 step 142: training accuarcy: 1.0\n",
      "Epoch 1 step 142: training loss: 772.4714072547329\n",
      "Epoch 1 step 143: training accuarcy: 1.0\n",
      "Epoch 1 step 143: training loss: 744.3563241042639\n",
      "Epoch 1 step 144: training accuarcy: 1.0\n",
      "Epoch 1 step 144: training loss: 718.3165167955377\n",
      "Epoch 1 step 145: training accuarcy: 1.0\n",
      "Epoch 1 step 145: training loss: 692.9821459520465\n",
      "Epoch 1 step 146: training accuarcy: 1.0\n",
      "Epoch 1 step 146: training loss: 668.196834805745\n",
      "Epoch 1 step 147: training accuarcy: 1.0\n",
      "Epoch 1 step 147: training loss: 645.7793633058786\n",
      "Epoch 1 step 148: training accuarcy: 1.0\n",
      "Epoch 1 step 148: training loss: 623.10262220747\n",
      "Epoch 1 step 149: training accuarcy: 1.0\n",
      "Epoch 1 step 149: training loss: 601.8772564904389\n",
      "Epoch 1 step 150: training accuarcy: 1.0\n",
      "Epoch 1 step 150: training loss: 582.1896443282487\n",
      "Epoch 1 step 151: training accuarcy: 1.0\n",
      "Epoch 1 step 151: training loss: 562.2381368370585\n",
      "Epoch 1 step 152: training accuarcy: 1.0\n",
      "Epoch 1 step 152: training loss: 543.3647387742095\n",
      "Epoch 1 step 153: training accuarcy: 1.0\n",
      "Epoch 1 step 153: training loss: 525.7897730413209\n",
      "Epoch 1 step 154: training accuarcy: 1.0\n",
      "Epoch 1 step 154: training loss: 508.95920000172043\n",
      "Epoch 1 step 155: training accuarcy: 1.0\n",
      "Epoch 1 step 155: training loss: 492.60583013473394\n",
      "Epoch 1 step 156: training accuarcy: 1.0\n",
      "Epoch 1 step 156: training loss: 476.9139256652606\n",
      "Epoch 1 step 157: training accuarcy: 1.0\n",
      "Epoch 1 step 157: training loss: 462.34758637999636\n",
      "Epoch 1 step 158: training accuarcy: 1.0\n",
      "Epoch 1 step 158: training loss: 446.92194633858253\n",
      "Epoch 1 step 159: training accuarcy: 1.0\n",
      "Epoch 1 step 159: training loss: 433.0676827064157\n",
      "Epoch 1 step 160: training accuarcy: 1.0\n",
      "Epoch 1 step 160: training loss: 420.1903248868114\n",
      "Epoch 1 step 161: training accuarcy: 1.0\n",
      "Epoch 1 step 161: training loss: 408.38049841820884\n",
      "Epoch 1 step 162: training accuarcy: 1.0\n",
      "Epoch 1 step 162: training loss: 396.25848890976135\n",
      "Epoch 1 step 163: training accuarcy: 1.0\n",
      "Epoch 1 step 163: training loss: 383.2769146923249\n",
      "Epoch 1 step 164: training accuarcy: 1.0\n",
      "Epoch 1 step 164: training loss: 372.02682422005523\n",
      "Epoch 1 step 165: training accuarcy: 1.0\n",
      "Epoch 1 step 165: training loss: 361.09226641541477\n",
      "Epoch 1 step 166: training accuarcy: 1.0\n",
      "Epoch 1 step 166: training loss: 351.1010910638024\n",
      "Epoch 1 step 167: training accuarcy: 1.0\n",
      "Epoch 1 step 167: training loss: 341.47177927818893\n",
      "Epoch 1 step 168: training accuarcy: 1.0\n",
      "Epoch 1 step 168: training loss: 331.8644955112426\n",
      "Epoch 1 step 169: training accuarcy: 1.0\n",
      "Epoch 1 step 169: training loss: 321.9118512421554\n",
      "Epoch 1 step 170: training accuarcy: 1.0\n",
      "Epoch 1 step 170: training loss: 313.469891773407\n",
      "Epoch 1 step 171: training accuarcy: 1.0\n",
      "Epoch 1 step 171: training loss: 304.8621475394434\n",
      "Epoch 1 step 172: training accuarcy: 1.0\n",
      "Epoch 1 step 172: training loss: 296.4103688914722\n",
      "Epoch 1 step 173: training accuarcy: 1.0\n",
      "Epoch 1 step 173: training loss: 288.6466117638187\n",
      "Epoch 1 step 174: training accuarcy: 1.0\n",
      "Epoch 1 step 174: training loss: 281.10859350287706\n",
      "Epoch 1 step 175: training accuarcy: 1.0\n",
      "Epoch 1 step 175: training loss: 274.164419016997\n",
      "Epoch 1 step 176: training accuarcy: 1.0\n",
      "Epoch 1 step 176: training loss: 266.6052553683324\n",
      "Epoch 1 step 177: training accuarcy: 1.0\n",
      "Epoch 1 step 177: training loss: 261.3748279522119\n",
      "Epoch 1 step 178: training accuarcy: 1.0\n",
      "Epoch 1 step 178: training loss: 254.38760570428238\n",
      "Epoch 1 step 179: training accuarcy: 1.0\n",
      "Epoch 1 step 179: training loss: 248.48131618200696\n",
      "Epoch 1 step 180: training accuarcy: 1.0\n",
      "Epoch 1 step 180: training loss: 241.92383572381513\n",
      "Epoch 1 step 181: training accuarcy: 1.0\n",
      "Epoch 1 step 181: training loss: 235.97946726892238\n",
      "Epoch 1 step 182: training accuarcy: 1.0\n",
      "Epoch 1 step 182: training loss: 230.19351491965298\n",
      "Epoch 1 step 183: training accuarcy: 1.0\n",
      "Epoch 1 step 183: training loss: 225.41159063407673\n",
      "Epoch 1 step 184: training accuarcy: 1.0\n",
      "Epoch 1 step 184: training loss: 220.3138067389031\n",
      "Epoch 1 step 185: training accuarcy: 1.0\n",
      "Epoch 1 step 185: training loss: 214.8984193160678\n",
      "Epoch 1 step 186: training accuarcy: 1.0\n",
      "Epoch 1 step 186: training loss: 209.79927857930718\n",
      "Epoch 1 step 187: training accuarcy: 1.0\n",
      "Epoch 1 step 187: training loss: 205.93801890495453\n",
      "Epoch 1 step 188: training accuarcy: 0.9999999999999999\n",
      "Epoch 1: train loss 1394.7985237578318, train accuarcy 0.999920666217804\n",
      "Epoch 1: valid loss 226.65485521098233, valid accuarcy 0.9999324083328247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████████████████████████████████████████▏                                                                                                       | 2/5 [02:36<03:55, 78.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 188: training loss: 200.95388679217115\n",
      "Epoch 2 step 189: training accuarcy: 1.0\n",
      "Epoch 2 step 189: training loss: 196.94700491329698\n",
      "Epoch 2 step 190: training accuarcy: 1.0\n",
      "Epoch 2 step 190: training loss: 193.4109453208173\n",
      "Epoch 2 step 191: training accuarcy: 1.0\n",
      "Epoch 2 step 191: training loss: 189.39837069906267\n",
      "Epoch 2 step 192: training accuarcy: 1.0\n",
      "Epoch 2 step 192: training loss: 184.95668767336556\n",
      "Epoch 2 step 193: training accuarcy: 1.0\n",
      "Epoch 2 step 193: training loss: 181.16358197287187\n",
      "Epoch 2 step 194: training accuarcy: 1.0\n",
      "Epoch 2 step 194: training loss: 177.78059323082138\n",
      "Epoch 2 step 195: training accuarcy: 1.0\n",
      "Epoch 2 step 195: training loss: 173.9218735530278\n",
      "Epoch 2 step 196: training accuarcy: 1.0\n",
      "Epoch 2 step 196: training loss: 170.84621511528593\n",
      "Epoch 2 step 197: training accuarcy: 1.0\n",
      "Epoch 2 step 197: training loss: 167.19094379364876\n",
      "Epoch 2 step 198: training accuarcy: 1.0\n",
      "Epoch 2 step 198: training loss: 165.1466749181467\n",
      "Epoch 2 step 199: training accuarcy: 1.0\n",
      "Epoch 2 step 199: training loss: 161.39075448595818\n",
      "Epoch 2 step 200: training accuarcy: 1.0\n",
      "Epoch 2 step 200: training loss: 158.074283402351\n",
      "Epoch 2 step 201: training accuarcy: 1.0\n",
      "Epoch 2 step 201: training loss: 155.3060358232434\n",
      "Epoch 2 step 202: training accuarcy: 1.0\n",
      "Epoch 2 step 202: training loss: 152.8712264345888\n",
      "Epoch 2 step 203: training accuarcy: 1.0\n",
      "Epoch 2 step 203: training loss: 150.0006612298632\n",
      "Epoch 2 step 204: training accuarcy: 1.0\n",
      "Epoch 2 step 204: training loss: 146.88437304258994\n",
      "Epoch 2 step 205: training accuarcy: 1.0\n",
      "Epoch 2 step 205: training loss: 145.4697653855739\n",
      "Epoch 2 step 206: training accuarcy: 1.0\n",
      "Epoch 2 step 206: training loss: 142.65514149355852\n",
      "Epoch 2 step 207: training accuarcy: 1.0\n",
      "Epoch 2 step 207: training loss: 139.4250551080819\n",
      "Epoch 2 step 208: training accuarcy: 1.0\n",
      "Epoch 2 step 208: training loss: 137.44670263391876\n",
      "Epoch 2 step 209: training accuarcy: 1.0\n",
      "Epoch 2 step 209: training loss: 136.0636410918565\n",
      "Epoch 2 step 210: training accuarcy: 1.0\n",
      "Epoch 2 step 210: training loss: 134.25527415849146\n",
      "Epoch 2 step 211: training accuarcy: 1.0\n",
      "Epoch 2 step 211: training loss: 130.94721376022156\n",
      "Epoch 2 step 212: training accuarcy: 1.0\n",
      "Epoch 2 step 212: training loss: 129.9202487906902\n",
      "Epoch 2 step 213: training accuarcy: 1.0\n",
      "Epoch 2 step 213: training loss: 126.93506247334302\n",
      "Epoch 2 step 214: training accuarcy: 1.0\n",
      "Epoch 2 step 214: training loss: 125.07995393335133\n",
      "Epoch 2 step 215: training accuarcy: 1.0\n",
      "Epoch 2 step 215: training loss: 123.16867913099142\n",
      "Epoch 2 step 216: training accuarcy: 1.0\n",
      "Epoch 2 step 216: training loss: 123.0867753288255\n",
      "Epoch 2 step 217: training accuarcy: 1.0\n",
      "Epoch 2 step 217: training loss: 120.13223391851064\n",
      "Epoch 2 step 218: training accuarcy: 1.0\n",
      "Epoch 2 step 218: training loss: 118.39603356760755\n",
      "Epoch 2 step 219: training accuarcy: 1.0\n",
      "Epoch 2 step 219: training loss: 116.33502601608508\n",
      "Epoch 2 step 220: training accuarcy: 1.0\n",
      "Epoch 2 step 220: training loss: 114.75409059461825\n",
      "Epoch 2 step 221: training accuarcy: 1.0\n",
      "Epoch 2 step 221: training loss: 113.02338165134051\n",
      "Epoch 2 step 222: training accuarcy: 1.0\n",
      "Epoch 2 step 222: training loss: 111.82888958911985\n",
      "Epoch 2 step 223: training accuarcy: 1.0\n",
      "Epoch 2 step 223: training loss: 110.07065358515112\n",
      "Epoch 2 step 224: training accuarcy: 1.0\n",
      "Epoch 2 step 224: training loss: 108.68401919511089\n",
      "Epoch 2 step 225: training accuarcy: 1.0\n",
      "Epoch 2 step 225: training loss: 106.85628241493247\n",
      "Epoch 2 step 226: training accuarcy: 1.0\n",
      "Epoch 2 step 226: training loss: 105.73532160994915\n",
      "Epoch 2 step 227: training accuarcy: 1.0\n",
      "Epoch 2 step 227: training loss: 104.31245516801202\n",
      "Epoch 2 step 228: training accuarcy: 1.0\n",
      "Epoch 2 step 228: training loss: 103.27449188551162\n",
      "Epoch 2 step 229: training accuarcy: 1.0\n",
      "Epoch 2 step 229: training loss: 102.5676979585397\n",
      "Epoch 2 step 230: training accuarcy: 1.0\n",
      "Epoch 2 step 230: training loss: 100.79262947241494\n",
      "Epoch 2 step 231: training accuarcy: 1.0\n",
      "Epoch 2 step 231: training loss: 99.60140151493032\n",
      "Epoch 2 step 232: training accuarcy: 1.0\n",
      "Epoch 2 step 232: training loss: 97.67129264234438\n",
      "Epoch 2 step 233: training accuarcy: 1.0\n",
      "Epoch 2 step 233: training loss: 96.70529335114675\n",
      "Epoch 2 step 234: training accuarcy: 1.0\n",
      "Epoch 2 step 234: training loss: 96.3243657311533\n",
      "Epoch 2 step 235: training accuarcy: 1.0\n",
      "Epoch 2 step 235: training loss: 94.36989432529839\n",
      "Epoch 2 step 236: training accuarcy: 1.0\n",
      "Epoch 2 step 236: training loss: 93.4308137748055\n",
      "Epoch 2 step 237: training accuarcy: 1.0\n",
      "Epoch 2 step 237: training loss: 91.79894820306282\n",
      "Epoch 2 step 238: training accuarcy: 1.0\n",
      "Epoch 2 step 238: training loss: 90.80820029052833\n",
      "Epoch 2 step 239: training accuarcy: 1.0\n",
      "Epoch 2 step 239: training loss: 89.99034073815791\n",
      "Epoch 2 step 240: training accuarcy: 1.0\n",
      "Epoch 2 step 240: training loss: 89.0611238008686\n",
      "Epoch 2 step 241: training accuarcy: 1.0\n",
      "Epoch 2 step 241: training loss: 87.46196315137723\n",
      "Epoch 2 step 242: training accuarcy: 1.0\n",
      "Epoch 2 step 242: training loss: 86.79534575704778\n",
      "Epoch 2 step 243: training accuarcy: 1.0\n",
      "Epoch 2 step 243: training loss: 86.17714846999974\n",
      "Epoch 2 step 244: training accuarcy: 1.0\n",
      "Epoch 2 step 244: training loss: 84.66619608155212\n",
      "Epoch 2 step 245: training accuarcy: 1.0\n",
      "Epoch 2 step 245: training loss: 84.00559248966167\n",
      "Epoch 2 step 246: training accuarcy: 1.0\n",
      "Epoch 2 step 246: training loss: 83.42135162015995\n",
      "Epoch 2 step 247: training accuarcy: 1.0\n",
      "Epoch 2 step 247: training loss: 81.93274843730576\n",
      "Epoch 2 step 248: training accuarcy: 1.0\n",
      "Epoch 2 step 248: training loss: 81.57500448515488\n",
      "Epoch 2 step 249: training accuarcy: 1.0\n",
      "Epoch 2 step 249: training loss: 80.72457051885372\n",
      "Epoch 2 step 250: training accuarcy: 1.0\n",
      "Epoch 2 step 250: training loss: 79.33835460481109\n",
      "Epoch 2 step 251: training accuarcy: 1.0\n",
      "Epoch 2 step 251: training loss: 78.94866500111938\n",
      "Epoch 2 step 252: training accuarcy: 1.0\n",
      "Epoch 2 step 252: training loss: 78.10823596794118\n",
      "Epoch 2 step 253: training accuarcy: 1.0\n",
      "Epoch 2 step 253: training loss: 77.00436942355272\n",
      "Epoch 2 step 254: training accuarcy: 1.0\n",
      "Epoch 2 step 254: training loss: 77.38503634712164\n",
      "Epoch 2 step 255: training accuarcy: 1.0\n",
      "Epoch 2 step 255: training loss: 75.92342079616856\n",
      "Epoch 2 step 256: training accuarcy: 1.0\n",
      "Epoch 2 step 256: training loss: 75.1191645678456\n",
      "Epoch 2 step 257: training accuarcy: 1.0\n",
      "Epoch 2 step 257: training loss: 74.4734170412007\n",
      "Epoch 2 step 258: training accuarcy: 1.0\n",
      "Epoch 2 step 258: training loss: 73.45306971591643\n",
      "Epoch 2 step 259: training accuarcy: 1.0\n",
      "Epoch 2 step 259: training loss: 72.63416996307592\n",
      "Epoch 2 step 260: training accuarcy: 1.0\n",
      "Epoch 2 step 260: training loss: 72.28691635068166\n",
      "Epoch 2 step 261: training accuarcy: 1.0\n",
      "Epoch 2 step 261: training loss: 71.65850369303388\n",
      "Epoch 2 step 262: training accuarcy: 1.0\n",
      "Epoch 2 step 262: training loss: 71.29442200270238\n",
      "Epoch 2 step 263: training accuarcy: 1.0\n",
      "Epoch 2 step 263: training loss: 70.56284357273945\n",
      "Epoch 2 step 264: training accuarcy: 1.0\n",
      "Epoch 2 step 264: training loss: 69.64669491772055\n",
      "Epoch 2 step 265: training accuarcy: 1.0\n",
      "Epoch 2 step 265: training loss: 69.27055162744169\n",
      "Epoch 2 step 266: training accuarcy: 1.0\n",
      "Epoch 2 step 266: training loss: 68.47724582148315\n",
      "Epoch 2 step 267: training accuarcy: 1.0\n",
      "Epoch 2 step 267: training loss: 67.46725863394978\n",
      "Epoch 2 step 268: training accuarcy: 1.0\n",
      "Epoch 2 step 268: training loss: 68.292547378945\n",
      "Epoch 2 step 269: training accuarcy: 1.0\n",
      "Epoch 2 step 269: training loss: 67.14577747660748\n",
      "Epoch 2 step 270: training accuarcy: 1.0\n",
      "Epoch 2 step 270: training loss: 65.53173495482517\n",
      "Epoch 2 step 271: training accuarcy: 1.0\n",
      "Epoch 2 step 271: training loss: 65.98449172758194\n",
      "Epoch 2 step 272: training accuarcy: 1.0\n",
      "Epoch 2 step 272: training loss: 64.87352858899652\n",
      "Epoch 2 step 273: training accuarcy: 1.0\n",
      "Epoch 2 step 273: training loss: 64.28871876808493\n",
      "Epoch 2 step 274: training accuarcy: 1.0\n",
      "Epoch 2 step 274: training loss: 63.92744864982796\n",
      "Epoch 2 step 275: training accuarcy: 1.0\n",
      "Epoch 2 step 275: training loss: 63.16982998152733\n",
      "Epoch 2 step 276: training accuarcy: 1.0\n",
      "Epoch 2 step 276: training loss: 63.357175755800654\n",
      "Epoch 2 step 277: training accuarcy: 1.0\n",
      "Epoch 2 step 277: training loss: 62.36821109812747\n",
      "Epoch 2 step 278: training accuarcy: 1.0\n",
      "Epoch 2 step 278: training loss: 61.94619403325501\n",
      "Epoch 2 step 279: training accuarcy: 1.0\n",
      "Epoch 2 step 279: training loss: 61.29653340359344\n",
      "Epoch 2 step 280: training accuarcy: 1.0\n",
      "Epoch 2 step 280: training loss: 61.4010423961333\n",
      "Epoch 2 step 281: training accuarcy: 1.0\n",
      "Epoch 2 step 281: training loss: 60.27613524972685\n",
      "Epoch 2 step 282: training accuarcy: 0.9999999999999999\n",
      "Epoch 2: train loss 106.0733418852326, train accuarcy 0.9999324083328247\n",
      "Epoch 2: valid loss 80.26759407804984, valid accuarcy 0.9999324083328247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                     | 3/5 [03:55<02:37, 78.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 282: training loss: 60.378811992301486\n",
      "Epoch 3 step 283: training accuarcy: 1.0\n",
      "Epoch 3 step 283: training loss: 60.56544144442725\n",
      "Epoch 3 step 284: training accuarcy: 1.0\n",
      "Epoch 3 step 284: training loss: 59.090601695054815\n",
      "Epoch 3 step 285: training accuarcy: 1.0\n",
      "Epoch 3 step 285: training loss: 58.7778670407271\n",
      "Epoch 3 step 286: training accuarcy: 1.0\n",
      "Epoch 3 step 286: training loss: 58.43989460277152\n",
      "Epoch 3 step 287: training accuarcy: 1.0\n",
      "Epoch 3 step 287: training loss: 57.55476142167913\n",
      "Epoch 3 step 288: training accuarcy: 1.0\n",
      "Epoch 3 step 288: training loss: 57.63787578484341\n",
      "Epoch 3 step 289: training accuarcy: 1.0\n",
      "Epoch 3 step 289: training loss: 56.80576665038936\n",
      "Epoch 3 step 290: training accuarcy: 1.0\n",
      "Epoch 3 step 290: training loss: 56.43283477469722\n",
      "Epoch 3 step 291: training accuarcy: 1.0\n",
      "Epoch 3 step 291: training loss: 55.90714193654783\n",
      "Epoch 3 step 292: training accuarcy: 1.0\n",
      "Epoch 3 step 292: training loss: 55.35509385390035\n",
      "Epoch 3 step 293: training accuarcy: 1.0\n",
      "Epoch 3 step 293: training loss: 54.51425352712147\n",
      "Epoch 3 step 294: training accuarcy: 1.0\n",
      "Epoch 3 step 294: training loss: 54.7756406341659\n",
      "Epoch 3 step 295: training accuarcy: 1.0\n",
      "Epoch 3 step 295: training loss: 53.95742576965561\n",
      "Epoch 3 step 296: training accuarcy: 1.0\n",
      "Epoch 3 step 296: training loss: 54.0812015956671\n",
      "Epoch 3 step 297: training accuarcy: 1.0\n",
      "Epoch 3 step 297: training loss: 53.76185007141397\n",
      "Epoch 3 step 298: training accuarcy: 1.0\n",
      "Epoch 3 step 298: training loss: 54.05209863094233\n",
      "Epoch 3 step 299: training accuarcy: 1.0\n",
      "Epoch 3 step 299: training loss: 52.404406390225894\n",
      "Epoch 3 step 300: training accuarcy: 1.0\n",
      "Epoch 3 step 300: training loss: 52.4446382090537\n",
      "Epoch 3 step 301: training accuarcy: 1.0\n",
      "Epoch 3 step 301: training loss: 51.5029089182219\n",
      "Epoch 3 step 302: training accuarcy: 1.0\n",
      "Epoch 3 step 302: training loss: 51.731285155138714\n",
      "Epoch 3 step 303: training accuarcy: 1.0\n",
      "Epoch 3 step 303: training loss: 51.07483469995946\n",
      "Epoch 3 step 304: training accuarcy: 1.0\n",
      "Epoch 3 step 304: training loss: 50.25421569215189\n",
      "Epoch 3 step 305: training accuarcy: 1.0\n",
      "Epoch 3 step 305: training loss: 49.70699543063885\n",
      "Epoch 3 step 306: training accuarcy: 1.0\n",
      "Epoch 3 step 306: training loss: 49.63585663871797\n",
      "Epoch 3 step 307: training accuarcy: 1.0\n",
      "Epoch 3 step 307: training loss: 50.21877792205255\n",
      "Epoch 3 step 308: training accuarcy: 1.0\n",
      "Epoch 3 step 308: training loss: 49.55896933978595\n",
      "Epoch 3 step 309: training accuarcy: 1.0\n",
      "Epoch 3 step 309: training loss: 49.16742405687572\n",
      "Epoch 3 step 310: training accuarcy: 1.0\n",
      "Epoch 3 step 310: training loss: 48.038874191232566\n",
      "Epoch 3 step 311: training accuarcy: 1.0\n",
      "Epoch 3 step 311: training loss: 48.673565723421\n",
      "Epoch 3 step 312: training accuarcy: 1.0\n",
      "Epoch 3 step 312: training loss: 48.334522476329816\n",
      "Epoch 3 step 313: training accuarcy: 1.0\n",
      "Epoch 3 step 313: training loss: 47.61229061585381\n",
      "Epoch 3 step 314: training accuarcy: 1.0\n",
      "Epoch 3 step 314: training loss: 47.028436154992434\n",
      "Epoch 3 step 315: training accuarcy: 1.0\n",
      "Epoch 3 step 315: training loss: 47.014491659832544\n",
      "Epoch 3 step 316: training accuarcy: 1.0\n",
      "Epoch 3 step 316: training loss: 47.10349556646399\n",
      "Epoch 3 step 317: training accuarcy: 1.0\n",
      "Epoch 3 step 317: training loss: 46.50690251892145\n",
      "Epoch 3 step 318: training accuarcy: 1.0\n",
      "Epoch 3 step 318: training loss: 45.88161263039067\n",
      "Epoch 3 step 319: training accuarcy: 1.0\n",
      "Epoch 3 step 319: training loss: 46.080288804045516\n",
      "Epoch 3 step 320: training accuarcy: 1.0\n",
      "Epoch 3 step 320: training loss: 45.502172876115274\n",
      "Epoch 3 step 321: training accuarcy: 1.0\n",
      "Epoch 3 step 321: training loss: 45.74151498894756\n",
      "Epoch 3 step 322: training accuarcy: 1.0\n",
      "Epoch 3 step 322: training loss: 45.2799346049985\n",
      "Epoch 3 step 323: training accuarcy: 1.0\n",
      "Epoch 3 step 323: training loss: 45.01124971550178\n",
      "Epoch 3 step 324: training accuarcy: 1.0\n",
      "Epoch 3 step 324: training loss: 44.72280079052758\n",
      "Epoch 3 step 325: training accuarcy: 1.0\n",
      "Epoch 3 step 325: training loss: 44.50783272053124\n",
      "Epoch 3 step 326: training accuarcy: 1.0\n",
      "Epoch 3 step 326: training loss: 44.15135506297081\n",
      "Epoch 3 step 327: training accuarcy: 1.0\n",
      "Epoch 3 step 327: training loss: 43.68432223455338\n",
      "Epoch 3 step 328: training accuarcy: 1.0\n",
      "Epoch 3 step 328: training loss: 43.863055563453166\n",
      "Epoch 3 step 329: training accuarcy: 1.0\n",
      "Epoch 3 step 329: training loss: 43.736919673450934\n",
      "Epoch 3 step 330: training accuarcy: 1.0\n",
      "Epoch 3 step 330: training loss: 43.03597118039161\n",
      "Epoch 3 step 331: training accuarcy: 1.0\n",
      "Epoch 3 step 331: training loss: 42.74645408073836\n",
      "Epoch 3 step 332: training accuarcy: 1.0\n",
      "Epoch 3 step 332: training loss: 42.67462152052201\n",
      "Epoch 3 step 333: training accuarcy: 1.0\n",
      "Epoch 3 step 333: training loss: 42.440271482513445\n",
      "Epoch 3 step 334: training accuarcy: 1.0\n",
      "Epoch 3 step 334: training loss: 42.76706457448583\n",
      "Epoch 3 step 335: training accuarcy: 1.0\n",
      "Epoch 3 step 335: training loss: 41.37463401921809\n",
      "Epoch 3 step 336: training accuarcy: 1.0\n",
      "Epoch 3 step 336: training loss: 41.67260956220451\n",
      "Epoch 3 step 337: training accuarcy: 1.0\n",
      "Epoch 3 step 337: training loss: 41.50900838256286\n",
      "Epoch 3 step 338: training accuarcy: 1.0\n",
      "Epoch 3 step 338: training loss: 41.399540923961254\n",
      "Epoch 3 step 339: training accuarcy: 1.0\n",
      "Epoch 3 step 339: training loss: 40.90060683165143\n",
      "Epoch 3 step 340: training accuarcy: 1.0\n",
      "Epoch 3 step 340: training loss: 41.190516773582736\n",
      "Epoch 3 step 341: training accuarcy: 1.0\n",
      "Epoch 3 step 341: training loss: 40.470964744191576\n",
      "Epoch 3 step 342: training accuarcy: 1.0\n",
      "Epoch 3 step 342: training loss: 40.4639683197956\n",
      "Epoch 3 step 343: training accuarcy: 1.0\n",
      "Epoch 3 step 343: training loss: 40.21828200544293\n",
      "Epoch 3 step 344: training accuarcy: 1.0\n",
      "Epoch 3 step 344: training loss: 39.57120817705796\n",
      "Epoch 3 step 345: training accuarcy: 1.0\n",
      "Epoch 3 step 345: training loss: 39.633233787539915\n",
      "Epoch 3 step 346: training accuarcy: 1.0\n",
      "Epoch 3 step 346: training loss: 39.72513914855714\n",
      "Epoch 3 step 347: training accuarcy: 1.0\n",
      "Epoch 3 step 347: training loss: 39.78377326575406\n",
      "Epoch 3 step 348: training accuarcy: 1.0\n",
      "Epoch 3 step 348: training loss: 38.9363800215567\n",
      "Epoch 3 step 349: training accuarcy: 1.0\n",
      "Epoch 3 step 349: training loss: 38.898461777779815\n",
      "Epoch 3 step 350: training accuarcy: 1.0\n",
      "Epoch 3 step 350: training loss: 38.843593093849336\n",
      "Epoch 3 step 351: training accuarcy: 1.0\n",
      "Epoch 3 step 351: training loss: 39.04160262427562\n",
      "Epoch 3 step 352: training accuarcy: 1.0\n",
      "Epoch 3 step 352: training loss: 38.47740368681219\n",
      "Epoch 3 step 353: training accuarcy: 1.0\n",
      "Epoch 3 step 353: training loss: 38.284591013620656\n",
      "Epoch 3 step 354: training accuarcy: 1.0\n",
      "Epoch 3 step 354: training loss: 38.092824255099366\n",
      "Epoch 3 step 355: training accuarcy: 1.0\n",
      "Epoch 3 step 355: training loss: 37.497193595483246\n",
      "Epoch 3 step 356: training accuarcy: 1.0\n",
      "Epoch 3 step 356: training loss: 37.46591205764172\n",
      "Epoch 3 step 357: training accuarcy: 1.0\n",
      "Epoch 3 step 357: training loss: 37.24773295738886\n",
      "Epoch 3 step 358: training accuarcy: 1.0\n",
      "Epoch 3 step 358: training loss: 37.24408304553245\n",
      "Epoch 3 step 359: training accuarcy: 1.0\n",
      "Epoch 3 step 359: training loss: 37.033343526726235\n",
      "Epoch 3 step 360: training accuarcy: 1.0\n",
      "Epoch 3 step 360: training loss: 36.612442294838054\n",
      "Epoch 3 step 361: training accuarcy: 1.0\n",
      "Epoch 3 step 361: training loss: 37.45711175980749\n",
      "Epoch 3 step 362: training accuarcy: 1.0\n",
      "Epoch 3 step 362: training loss: 36.37212236298776\n",
      "Epoch 3 step 363: training accuarcy: 1.0\n",
      "Epoch 3 step 363: training loss: 36.26281753252415\n",
      "Epoch 3 step 364: training accuarcy: 1.0\n",
      "Epoch 3 step 364: training loss: 36.0134658003404\n",
      "Epoch 3 step 365: training accuarcy: 1.0\n",
      "Epoch 3 step 365: training loss: 35.54422730751335\n",
      "Epoch 3 step 366: training accuarcy: 1.0\n",
      "Epoch 3 step 366: training loss: 35.86471171546214\n",
      "Epoch 3 step 367: training accuarcy: 1.0\n",
      "Epoch 3 step 367: training loss: 35.196076735547116\n",
      "Epoch 3 step 368: training accuarcy: 1.0\n",
      "Epoch 3 step 368: training loss: 35.28798002090132\n",
      "Epoch 3 step 369: training accuarcy: 1.0\n",
      "Epoch 3 step 369: training loss: 35.14845243393837\n",
      "Epoch 3 step 370: training accuarcy: 1.0\n",
      "Epoch 3 step 370: training loss: 35.05945585313505\n",
      "Epoch 3 step 371: training accuarcy: 1.0\n",
      "Epoch 3 step 371: training loss: 34.375445689327464\n",
      "Epoch 3 step 372: training accuarcy: 1.0\n",
      "Epoch 3 step 372: training loss: 34.47266848385738\n",
      "Epoch 3 step 373: training accuarcy: 1.0\n",
      "Epoch 3 step 373: training loss: 34.50369251010032\n",
      "Epoch 3 step 374: training accuarcy: 1.0\n",
      "Epoch 3 step 374: training loss: 34.82647795642106\n",
      "Epoch 3 step 375: training accuarcy: 1.0\n",
      "Epoch 3 step 375: training loss: 33.9949830124946\n",
      "Epoch 3 step 376: training accuarcy: 0.9999999999999999\n",
      "Epoch 3: train loss 44.72231525911486, train accuarcy 0.9999324083328247\n",
      "Epoch 3: valid loss 52.295169752002394, valid accuarcy 0.9999324083328247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                  | 4/5 [05:13<01:18, 78.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 376: training loss: 34.01134157192234\n",
      "Epoch 4 step 377: training accuarcy: 1.0\n",
      "Epoch 4 step 377: training loss: 33.324933272483094\n",
      "Epoch 4 step 378: training accuarcy: 1.0\n",
      "Epoch 4 step 378: training loss: 33.58446444920643\n",
      "Epoch 4 step 379: training accuarcy: 1.0\n",
      "Epoch 4 step 379: training loss: 33.58626178922205\n",
      "Epoch 4 step 380: training accuarcy: 1.0\n",
      "Epoch 4 step 380: training loss: 33.79700455288014\n",
      "Epoch 4 step 381: training accuarcy: 1.0\n",
      "Epoch 4 step 381: training loss: 33.12533024867399\n",
      "Epoch 4 step 382: training accuarcy: 1.0\n",
      "Epoch 4 step 382: training loss: 32.67779216082605\n",
      "Epoch 4 step 383: training accuarcy: 1.0\n",
      "Epoch 4 step 383: training loss: 33.0803382792642\n",
      "Epoch 4 step 384: training accuarcy: 1.0\n",
      "Epoch 4 step 384: training loss: 32.7119103369336\n",
      "Epoch 4 step 385: training accuarcy: 1.0\n",
      "Epoch 4 step 385: training loss: 32.23378004960491\n",
      "Epoch 4 step 386: training accuarcy: 1.0\n",
      "Epoch 4 step 386: training loss: 32.1579866557391\n",
      "Epoch 4 step 387: training accuarcy: 1.0\n",
      "Epoch 4 step 387: training loss: 32.437523653652136\n",
      "Epoch 4 step 388: training accuarcy: 1.0\n",
      "Epoch 4 step 388: training loss: 31.968646351081052\n",
      "Epoch 4 step 389: training accuarcy: 1.0\n",
      "Epoch 4 step 389: training loss: 32.1344453995577\n",
      "Epoch 4 step 390: training accuarcy: 1.0\n",
      "Epoch 4 step 390: training loss: 31.59702891687268\n",
      "Epoch 4 step 391: training accuarcy: 1.0\n",
      "Epoch 4 step 391: training loss: 32.036598510214056\n",
      "Epoch 4 step 392: training accuarcy: 1.0\n",
      "Epoch 4 step 392: training loss: 30.947496325086554\n",
      "Epoch 4 step 393: training accuarcy: 1.0\n",
      "Epoch 4 step 393: training loss: 31.39168613075117\n",
      "Epoch 4 step 394: training accuarcy: 1.0\n",
      "Epoch 4 step 394: training loss: 32.17026168443868\n",
      "Epoch 4 step 395: training accuarcy: 1.0\n",
      "Epoch 4 step 395: training loss: 31.72365720022111\n",
      "Epoch 4 step 396: training accuarcy: 1.0\n",
      "Epoch 4 step 396: training loss: 31.597707263529283\n",
      "Epoch 4 step 397: training accuarcy: 1.0\n",
      "Epoch 4 step 397: training loss: 30.5672581246701\n",
      "Epoch 4 step 398: training accuarcy: 1.0\n",
      "Epoch 4 step 398: training loss: 30.97641565199505\n",
      "Epoch 4 step 399: training accuarcy: 1.0\n",
      "Epoch 4 step 399: training loss: 30.971293879926495\n",
      "Epoch 4 step 400: training accuarcy: 1.0\n",
      "Epoch 4 step 400: training loss: 30.801317676300492\n",
      "Epoch 4 step 401: training accuarcy: 1.0\n",
      "Epoch 4 step 401: training loss: 30.47340458432149\n",
      "Epoch 4 step 402: training accuarcy: 1.0\n",
      "Epoch 4 step 402: training loss: 31.258367985630525\n",
      "Epoch 4 step 403: training accuarcy: 1.0\n",
      "Epoch 4 step 403: training loss: 30.570241155391066\n",
      "Epoch 4 step 404: training accuarcy: 1.0\n",
      "Epoch 4 step 404: training loss: 30.145936216232652\n",
      "Epoch 4 step 405: training accuarcy: 1.0\n",
      "Epoch 4 step 405: training loss: 30.184647281839577\n",
      "Epoch 4 step 406: training accuarcy: 1.0\n",
      "Epoch 4 step 406: training loss: 29.937858876987463\n",
      "Epoch 4 step 407: training accuarcy: 1.0\n",
      "Epoch 4 step 407: training loss: 29.504142071314156\n",
      "Epoch 4 step 408: training accuarcy: 1.0\n",
      "Epoch 4 step 408: training loss: 30.299155825004334\n",
      "Epoch 4 step 409: training accuarcy: 1.0\n",
      "Epoch 4 step 409: training loss: 29.630252394159545\n",
      "Epoch 4 step 410: training accuarcy: 1.0\n",
      "Epoch 4 step 410: training loss: 29.532799098339016\n",
      "Epoch 4 step 411: training accuarcy: 1.0\n",
      "Epoch 4 step 411: training loss: 29.342650299891915\n",
      "Epoch 4 step 412: training accuarcy: 1.0\n",
      "Epoch 4 step 412: training loss: 29.277916034831094\n",
      "Epoch 4 step 413: training accuarcy: 1.0\n",
      "Epoch 4 step 413: training loss: 29.24996961424109\n",
      "Epoch 4 step 414: training accuarcy: 1.0\n",
      "Epoch 4 step 414: training loss: 28.55742268005159\n",
      "Epoch 4 step 415: training accuarcy: 1.0\n",
      "Epoch 4 step 415: training loss: 28.93309927933773\n",
      "Epoch 4 step 416: training accuarcy: 1.0\n",
      "Epoch 4 step 416: training loss: 28.987055178504516\n",
      "Epoch 4 step 417: training accuarcy: 1.0\n",
      "Epoch 4 step 417: training loss: 29.330931575430878\n",
      "Epoch 4 step 418: training accuarcy: 1.0\n",
      "Epoch 4 step 418: training loss: 28.139360452074744\n",
      "Epoch 4 step 419: training accuarcy: 1.0\n",
      "Epoch 4 step 419: training loss: 28.440085839976675\n",
      "Epoch 4 step 420: training accuarcy: 1.0\n",
      "Epoch 4 step 420: training loss: 28.50638865356492\n",
      "Epoch 4 step 421: training accuarcy: 1.0\n",
      "Epoch 4 step 421: training loss: 28.27106896298572\n",
      "Epoch 4 step 422: training accuarcy: 1.0\n",
      "Epoch 4 step 422: training loss: 27.937954999917526\n",
      "Epoch 4 step 423: training accuarcy: 1.0\n",
      "Epoch 4 step 423: training loss: 27.9091264574432\n",
      "Epoch 4 step 424: training accuarcy: 1.0\n",
      "Epoch 4 step 424: training loss: 27.909524371107825\n",
      "Epoch 4 step 425: training accuarcy: 1.0\n",
      "Epoch 4 step 425: training loss: 27.662802588446784\n",
      "Epoch 4 step 426: training accuarcy: 1.0\n",
      "Epoch 4 step 426: training loss: 27.693180099931478\n",
      "Epoch 4 step 427: training accuarcy: 1.0\n",
      "Epoch 4 step 427: training loss: 27.87732602651383\n",
      "Epoch 4 step 428: training accuarcy: 1.0\n",
      "Epoch 4 step 428: training loss: 27.215130251782934\n",
      "Epoch 4 step 429: training accuarcy: 1.0\n",
      "Epoch 4 step 429: training loss: 27.296725191629154\n",
      "Epoch 4 step 430: training accuarcy: 1.0\n",
      "Epoch 4 step 430: training loss: 26.76942646777829\n",
      "Epoch 4 step 431: training accuarcy: 1.0\n",
      "Epoch 4 step 431: training loss: 27.103163594241607\n",
      "Epoch 4 step 432: training accuarcy: 1.0\n",
      "Epoch 4 step 432: training loss: 26.951622238032556\n",
      "Epoch 4 step 433: training accuarcy: 1.0\n",
      "Epoch 4 step 433: training loss: 27.20033894249935\n",
      "Epoch 4 step 434: training accuarcy: 1.0\n",
      "Epoch 4 step 434: training loss: 26.44288728255814\n",
      "Epoch 4 step 435: training accuarcy: 1.0\n",
      "Epoch 4 step 435: training loss: 26.8281569787997\n",
      "Epoch 4 step 436: training accuarcy: 1.0\n",
      "Epoch 4 step 436: training loss: 26.676664770584694\n",
      "Epoch 4 step 437: training accuarcy: 1.0\n",
      "Epoch 4 step 437: training loss: 26.76043035104585\n",
      "Epoch 4 step 438: training accuarcy: 1.0\n",
      "Epoch 4 step 438: training loss: 26.637803454508628\n",
      "Epoch 4 step 439: training accuarcy: 1.0\n",
      "Epoch 4 step 439: training loss: 26.56413852683332\n",
      "Epoch 4 step 440: training accuarcy: 1.0\n",
      "Epoch 4 step 440: training loss: 26.292922457744655\n",
      "Epoch 4 step 441: training accuarcy: 1.0\n",
      "Epoch 4 step 441: training loss: 25.993984564292493\n",
      "Epoch 4 step 442: training accuarcy: 1.0\n",
      "Epoch 4 step 442: training loss: 25.944547596438063\n",
      "Epoch 4 step 443: training accuarcy: 1.0\n",
      "Epoch 4 step 443: training loss: 26.0391072289791\n",
      "Epoch 4 step 444: training accuarcy: 1.0\n",
      "Epoch 4 step 444: training loss: 26.03594217324251\n",
      "Epoch 4 step 445: training accuarcy: 1.0\n",
      "Epoch 4 step 445: training loss: 25.56752583828255\n",
      "Epoch 4 step 446: training accuarcy: 1.0\n",
      "Epoch 4 step 446: training loss: 25.67008642950226\n",
      "Epoch 4 step 447: training accuarcy: 1.0\n",
      "Epoch 4 step 447: training loss: 25.28103583309106\n",
      "Epoch 4 step 448: training accuarcy: 1.0\n",
      "Epoch 4 step 448: training loss: 25.652299262462837\n",
      "Epoch 4 step 449: training accuarcy: 1.0\n",
      "Epoch 4 step 449: training loss: 25.701767636917175\n",
      "Epoch 4 step 450: training accuarcy: 1.0\n",
      "Epoch 4 step 450: training loss: 25.792599808568795\n",
      "Epoch 4 step 451: training accuarcy: 1.0\n",
      "Epoch 4 step 451: training loss: 25.298906154705936\n",
      "Epoch 4 step 452: training accuarcy: 1.0\n",
      "Epoch 4 step 452: training loss: 24.960252873610816\n",
      "Epoch 4 step 453: training accuarcy: 1.0\n",
      "Epoch 4 step 453: training loss: 25.30747480582967\n",
      "Epoch 4 step 454: training accuarcy: 1.0\n",
      "Epoch 4 step 454: training loss: 25.07356787031662\n",
      "Epoch 4 step 455: training accuarcy: 1.0\n",
      "Epoch 4 step 455: training loss: 25.091073987602293\n",
      "Epoch 4 step 456: training accuarcy: 1.0\n",
      "Epoch 4 step 456: training loss: 24.633556694348385\n",
      "Epoch 4 step 457: training accuarcy: 1.0\n",
      "Epoch 4 step 457: training loss: 24.913452581421787\n",
      "Epoch 4 step 458: training accuarcy: 1.0\n",
      "Epoch 4 step 458: training loss: 24.154155777064915\n",
      "Epoch 4 step 459: training accuarcy: 1.0\n",
      "Epoch 4 step 459: training loss: 24.57760828402663\n",
      "Epoch 4 step 460: training accuarcy: 1.0\n",
      "Epoch 4 step 460: training loss: 24.434720718299687\n",
      "Epoch 4 step 461: training accuarcy: 1.0\n",
      "Epoch 4 step 461: training loss: 24.67917017435418\n",
      "Epoch 4 step 462: training accuarcy: 1.0\n",
      "Epoch 4 step 462: training loss: 24.424594799308917\n",
      "Epoch 4 step 463: training accuarcy: 1.0\n",
      "Epoch 4 step 463: training loss: 24.20567561686971\n",
      "Epoch 4 step 464: training accuarcy: 1.0\n",
      "Epoch 4 step 464: training loss: 24.57114722055073\n",
      "Epoch 4 step 465: training accuarcy: 1.0\n",
      "Epoch 4 step 465: training loss: 23.850897565516952\n",
      "Epoch 4 step 466: training accuarcy: 1.0\n",
      "Epoch 4 step 466: training loss: 23.99613320607122\n",
      "Epoch 4 step 467: training accuarcy: 1.0\n",
      "Epoch 4 step 467: training loss: 23.916782218854024\n",
      "Epoch 4 step 468: training accuarcy: 1.0\n",
      "Epoch 4 step 468: training loss: 24.104096841804754\n",
      "Epoch 4 step 469: training accuarcy: 1.0\n",
      "Epoch 4 step 469: training loss: 23.289921573908963\n",
      "Epoch 4 step 470: training accuarcy: 0.9999999999999999\n",
      "Epoch 4: train loss 28.330602601944737, train accuarcy 0.9999324083328247\n",
      "Epoch 4: valid loss 40.3798855867505, valid accuarcy 0.9999324083328247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [06:31<00:00, 78.26s/it]\n"
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=5,\n",
    "               log_dir=get_log_dir(ds_type='seq_stackoverflow',\n",
    "                                   model_type='fm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:20:24.076824Z",
     "start_time": "2019-10-07T13:20:24.051852Z"
    }
   },
   "outputs": [],
   "source": [
    "del fm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HRM FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:10:37.111033Z",
     "start_time": "2019-10-07T13:10:36.277154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchHrmFM()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_model = TorchHrmFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "hrm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:10:37.118032Z",
     "start_time": "2019-10-07T13:10:37.113034Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(hrm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:10:40.585253Z",
     "start_time": "2019-10-07T13:10:40.460282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1a692553cf8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_learner = FMLearner(hrm_model, adam_opt, schedular, db)\n",
    "hrm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:10:55.366909Z",
     "start_time": "2019-10-07T13:10:55.363892Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:17:59.915066Z",
     "start_time": "2019-10-07T13:11:10.982287Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 159449.74616477397\n",
      "Epoch 0 step 1: training accuarcy: 0.4855\n",
      "Epoch 0 step 1: training loss: 154508.20718114648\n",
      "Epoch 0 step 2: training accuarcy: 0.51\n",
      "Epoch 0 step 2: training loss: 149945.26021111847\n",
      "Epoch 0 step 3: training accuarcy: 0.493\n",
      "Epoch 0 step 3: training loss: 145605.51904633694\n",
      "Epoch 0 step 4: training accuarcy: 0.506\n",
      "Epoch 0 step 4: training loss: 141027.2265681734\n",
      "Epoch 0 step 5: training accuarcy: 0.5105000000000001\n",
      "Epoch 0 step 5: training loss: 136983.23592669764\n",
      "Epoch 0 step 6: training accuarcy: 0.5045000000000001\n",
      "Epoch 0 step 6: training loss: 133017.319562297\n",
      "Epoch 0 step 7: training accuarcy: 0.495\n",
      "Epoch 0 step 7: training loss: 129023.99524061031\n",
      "Epoch 0 step 8: training accuarcy: 0.5075000000000001\n",
      "Epoch 0 step 8: training loss: 124990.26149430107\n",
      "Epoch 0 step 9: training accuarcy: 0.5005000000000001\n",
      "Epoch 0 step 9: training loss: 121112.51472496541\n",
      "Epoch 0 step 10: training accuarcy: 0.513\n",
      "Epoch 0 step 10: training loss: 117488.82251386195\n",
      "Epoch 0 step 11: training accuarcy: 0.5015000000000001\n",
      "Epoch 0 step 11: training loss: 113718.6908380564\n",
      "Epoch 0 step 12: training accuarcy: 0.5035000000000001\n",
      "Epoch 0 step 12: training loss: 110253.75789472731\n",
      "Epoch 0 step 13: training accuarcy: 0.5135\n",
      "Epoch 0 step 13: training loss: 106827.59920098523\n",
      "Epoch 0 step 14: training accuarcy: 0.4995\n",
      "Epoch 0 step 14: training loss: 103500.85276795496\n",
      "Epoch 0 step 15: training accuarcy: 0.4945\n",
      "Epoch 0 step 15: training loss: 100201.21345524315\n",
      "Epoch 0 step 16: training accuarcy: 0.504\n",
      "Epoch 0 step 16: training loss: 97026.49147790558\n",
      "Epoch 0 step 17: training accuarcy: 0.5155\n",
      "Epoch 0 step 17: training loss: 94072.37705019268\n",
      "Epoch 0 step 18: training accuarcy: 0.4995\n",
      "Epoch 0 step 18: training loss: 91021.94724484698\n",
      "Epoch 0 step 19: training accuarcy: 0.4925\n",
      "Epoch 0 step 19: training loss: 88148.91581467002\n",
      "Epoch 0 step 20: training accuarcy: 0.4925\n",
      "Epoch 0 step 20: training loss: 85339.52214057812\n",
      "Epoch 0 step 21: training accuarcy: 0.4885\n",
      "Epoch 0 step 21: training loss: 82374.00079457209\n",
      "Epoch 0 step 22: training accuarcy: 0.5065000000000001\n",
      "Epoch 0 step 22: training loss: 79671.75366159006\n",
      "Epoch 0 step 23: training accuarcy: 0.516\n",
      "Epoch 0 step 23: training loss: 76958.5335055639\n",
      "Epoch 0 step 24: training accuarcy: 0.5125\n",
      "Epoch 0 step 24: training loss: 74672.62652005046\n",
      "Epoch 0 step 25: training accuarcy: 0.4945\n",
      "Epoch 0 step 25: training loss: 72174.23336177511\n",
      "Epoch 0 step 26: training accuarcy: 0.5025000000000001\n",
      "Epoch 0 step 26: training loss: 69624.13527343428\n",
      "Epoch 0 step 27: training accuarcy: 0.5195\n",
      "Epoch 0 step 27: training loss: 67377.89080103915\n",
      "Epoch 0 step 28: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 28: training loss: 64997.713227235305\n",
      "Epoch 0 step 29: training accuarcy: 0.526\n",
      "Epoch 0 step 29: training loss: 62870.4734476188\n",
      "Epoch 0 step 30: training accuarcy: 0.4955\n",
      "Epoch 0 step 30: training loss: 60866.37976106441\n",
      "Epoch 0 step 31: training accuarcy: 0.5045000000000001\n",
      "Epoch 0 step 31: training loss: 58541.86496089337\n",
      "Epoch 0 step 32: training accuarcy: 0.535\n",
      "Epoch 0 step 32: training loss: 56771.24926340116\n",
      "Epoch 0 step 33: training accuarcy: 0.4945\n",
      "Epoch 0 step 33: training loss: 54663.75313717183\n",
      "Epoch 0 step 34: training accuarcy: 0.524\n",
      "Epoch 0 step 34: training loss: 52841.24483336954\n",
      "Epoch 0 step 35: training accuarcy: 0.5115000000000001\n",
      "Epoch 0 step 35: training loss: 51151.58136226641\n",
      "Epoch 0 step 36: training accuarcy: 0.507\n",
      "Epoch 0 step 36: training loss: 49270.15374720145\n",
      "Epoch 0 step 37: training accuarcy: 0.52\n",
      "Epoch 0 step 37: training loss: 47622.754556962405\n",
      "Epoch 0 step 38: training accuarcy: 0.5105000000000001\n",
      "Epoch 0 step 38: training loss: 45877.441992775544\n",
      "Epoch 0 step 39: training accuarcy: 0.523\n",
      "Epoch 0 step 39: training loss: 44374.34896203197\n",
      "Epoch 0 step 40: training accuarcy: 0.502\n",
      "Epoch 0 step 40: training loss: 42831.06244904658\n",
      "Epoch 0 step 41: training accuarcy: 0.5145\n",
      "Epoch 0 step 41: training loss: 41361.167082927925\n",
      "Epoch 0 step 42: training accuarcy: 0.51\n",
      "Epoch 0 step 42: training loss: 39929.13557274059\n",
      "Epoch 0 step 43: training accuarcy: 0.521\n",
      "Epoch 0 step 43: training loss: 38553.381581000016\n",
      "Epoch 0 step 44: training accuarcy: 0.501\n",
      "Epoch 0 step 44: training loss: 37144.451270319405\n",
      "Epoch 0 step 45: training accuarcy: 0.5105000000000001\n",
      "Epoch 0 step 45: training loss: 35902.43077289641\n",
      "Epoch 0 step 46: training accuarcy: 0.506\n",
      "Epoch 0 step 46: training loss: 34465.19196067319\n",
      "Epoch 0 step 47: training accuarcy: 0.5305\n",
      "Epoch 0 step 47: training loss: 33372.1460725803\n",
      "Epoch 0 step 48: training accuarcy: 0.502\n",
      "Epoch 0 step 48: training loss: 32119.205599335484\n",
      "Epoch 0 step 49: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 49: training loss: 30921.42372372032\n",
      "Epoch 0 step 50: training accuarcy: 0.5315\n",
      "Epoch 0 step 50: training loss: 29976.695540058514\n",
      "Epoch 0 step 51: training accuarcy: 0.5085000000000001\n",
      "Epoch 0 step 51: training loss: 28739.3992031175\n",
      "Epoch 0 step 52: training accuarcy: 0.5205\n",
      "Epoch 0 step 52: training loss: 27800.68338968855\n",
      "Epoch 0 step 53: training accuarcy: 0.518\n",
      "Epoch 0 step 53: training loss: 26775.068014814642\n",
      "Epoch 0 step 54: training accuarcy: 0.527\n",
      "Epoch 0 step 54: training loss: 25837.84390637064\n",
      "Epoch 0 step 55: training accuarcy: 0.5095000000000001\n",
      "Epoch 0 step 55: training loss: 24932.68250400675\n",
      "Epoch 0 step 56: training accuarcy: 0.5085000000000001\n",
      "Epoch 0 step 56: training loss: 24089.73217672085\n",
      "Epoch 0 step 57: training accuarcy: 0.511\n",
      "Epoch 0 step 57: training loss: 23200.086566252925\n",
      "Epoch 0 step 58: training accuarcy: 0.5105000000000001\n",
      "Epoch 0 step 58: training loss: 22339.087477650224\n",
      "Epoch 0 step 59: training accuarcy: 0.5215\n",
      "Epoch 0 step 59: training loss: 21500.3397668833\n",
      "Epoch 0 step 60: training accuarcy: 0.5265\n",
      "Epoch 0 step 60: training loss: 20811.686142953913\n",
      "Epoch 0 step 61: training accuarcy: 0.5035000000000001\n",
      "Epoch 0 step 61: training loss: 20047.523262741375\n",
      "Epoch 0 step 62: training accuarcy: 0.511\n",
      "Epoch 0 step 62: training loss: 19300.188268138525\n",
      "Epoch 0 step 63: training accuarcy: 0.52\n",
      "Epoch 0 step 63: training loss: 18528.21539678629\n",
      "Epoch 0 step 64: training accuarcy: 0.5305\n",
      "Epoch 0 step 64: training loss: 17929.237804679637\n",
      "Epoch 0 step 65: training accuarcy: 0.5235\n",
      "Epoch 0 step 65: training loss: 17261.206867780507\n",
      "Epoch 0 step 66: training accuarcy: 0.5265\n",
      "Epoch 0 step 66: training loss: 16676.76972581677\n",
      "Epoch 0 step 67: training accuarcy: 0.527\n",
      "Epoch 0 step 67: training loss: 16026.452676209938\n",
      "Epoch 0 step 68: training accuarcy: 0.5305\n",
      "Epoch 0 step 68: training loss: 15471.063747534223\n",
      "Epoch 0 step 69: training accuarcy: 0.519\n",
      "Epoch 0 step 69: training loss: 14896.04990638089\n",
      "Epoch 0 step 70: training accuarcy: 0.5475\n",
      "Epoch 0 step 70: training loss: 14385.414780351855\n",
      "Epoch 0 step 71: training accuarcy: 0.513\n",
      "Epoch 0 step 71: training loss: 13868.713197112716\n",
      "Epoch 0 step 72: training accuarcy: 0.522\n",
      "Epoch 0 step 72: training loss: 13360.68188337751\n",
      "Epoch 0 step 73: training accuarcy: 0.5285\n",
      "Epoch 0 step 73: training loss: 12829.444486007544\n",
      "Epoch 0 step 74: training accuarcy: 0.5425\n",
      "Epoch 0 step 74: training loss: 12437.34088757289\n",
      "Epoch 0 step 75: training accuarcy: 0.517\n",
      "Epoch 0 step 75: training loss: 12032.085457064184\n",
      "Epoch 0 step 76: training accuarcy: 0.5145\n",
      "Epoch 0 step 76: training loss: 11529.270354662813\n",
      "Epoch 0 step 77: training accuarcy: 0.5405\n",
      "Epoch 0 step 77: training loss: 11144.725171535923\n",
      "Epoch 0 step 78: training accuarcy: 0.53\n",
      "Epoch 0 step 78: training loss: 10781.422198082822\n",
      "Epoch 0 step 79: training accuarcy: 0.509\n",
      "Epoch 0 step 79: training loss: 10343.21581168905\n",
      "Epoch 0 step 80: training accuarcy: 0.533\n",
      "Epoch 0 step 80: training loss: 9988.911778125155\n",
      "Epoch 0 step 81: training accuarcy: 0.5415\n",
      "Epoch 0 step 81: training loss: 9635.564337430129\n",
      "Epoch 0 step 82: training accuarcy: 0.5475\n",
      "Epoch 0 step 82: training loss: 9358.723323062175\n",
      "Epoch 0 step 83: training accuarcy: 0.5155\n",
      "Epoch 0 step 83: training loss: 8968.323319389307\n",
      "Epoch 0 step 84: training accuarcy: 0.543\n",
      "Epoch 0 step 84: training loss: 8668.972465987074\n",
      "Epoch 0 step 85: training accuarcy: 0.53\n",
      "Epoch 0 step 85: training loss: 8350.373201778097\n",
      "Epoch 0 step 86: training accuarcy: 0.536\n",
      "Epoch 0 step 86: training loss: 8054.00054126971\n",
      "Epoch 0 step 87: training accuarcy: 0.537\n",
      "Epoch 0 step 87: training loss: 7823.309274769774\n",
      "Epoch 0 step 88: training accuarcy: 0.528\n",
      "Epoch 0 step 88: training loss: 7530.601426756062\n",
      "Epoch 0 step 89: training accuarcy: 0.5435\n",
      "Epoch 0 step 89: training loss: 7245.703660668005\n",
      "Epoch 0 step 90: training accuarcy: 0.547\n",
      "Epoch 0 step 90: training loss: 7017.791449164802\n",
      "Epoch 0 step 91: training accuarcy: 0.5405\n",
      "Epoch 0 step 91: training loss: 6790.638569466802\n",
      "Epoch 0 step 92: training accuarcy: 0.5375\n",
      "Epoch 0 step 92: training loss: 6548.735125444208\n",
      "Epoch 0 step 93: training accuarcy: 0.535\n",
      "Epoch 0 step 93: training loss: 6012.940613186648\n",
      "Epoch 0 step 94: training accuarcy: 0.5337186897880539\n",
      "Epoch 0: train loss 50100.12895141747, train accuarcy 0.5157609581947327\n",
      "Epoch 0: valid loss 11480.697535419467, valid accuarcy 0.5310577750205994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▌                                                                                                                                          | 1/5 [01:20<05:23, 80.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 94: training loss: 6120.881981504964\n",
      "Epoch 1 step 95: training accuarcy: 0.5435\n",
      "Epoch 1 step 95: training loss: 5905.930824136662\n",
      "Epoch 1 step 96: training accuarcy: 0.554\n",
      "Epoch 1 step 96: training loss: 5728.454368634429\n",
      "Epoch 1 step 97: training accuarcy: 0.5385\n",
      "Epoch 1 step 97: training loss: 5533.730901441544\n",
      "Epoch 1 step 98: training accuarcy: 0.5615\n",
      "Epoch 1 step 98: training loss: 5347.536542319389\n",
      "Epoch 1 step 99: training accuarcy: 0.5485\n",
      "Epoch 1 step 99: training loss: 5175.711708899931\n",
      "Epoch 1 step 100: training accuarcy: 0.556\n",
      "Epoch 1 step 100: training loss: 5011.179423012626\n",
      "Epoch 1 step 101: training accuarcy: 0.556\n",
      "Epoch 1 step 101: training loss: 4846.5108505765365\n",
      "Epoch 1 step 102: training accuarcy: 0.5690000000000001\n",
      "Epoch 1 step 102: training loss: 4732.235576822035\n",
      "Epoch 1 step 103: training accuarcy: 0.5615\n",
      "Epoch 1 step 103: training loss: 4586.728915962958\n",
      "Epoch 1 step 104: training accuarcy: 0.5465\n",
      "Epoch 1 step 104: training loss: 4434.268618364473\n",
      "Epoch 1 step 105: training accuarcy: 0.558\n",
      "Epoch 1 step 105: training loss: 4301.341234465196\n",
      "Epoch 1 step 106: training accuarcy: 0.5625\n",
      "Epoch 1 step 106: training loss: 4152.718671313029\n",
      "Epoch 1 step 107: training accuarcy: 0.5630000000000001\n",
      "Epoch 1 step 107: training loss: 4073.943259391349\n",
      "Epoch 1 step 108: training accuarcy: 0.5545\n",
      "Epoch 1 step 108: training loss: 3942.7953415518455\n",
      "Epoch 1 step 109: training accuarcy: 0.5565\n",
      "Epoch 1 step 109: training loss: 3845.1010685985616\n",
      "Epoch 1 step 110: training accuarcy: 0.5585\n",
      "Epoch 1 step 110: training loss: 3724.517613875199\n",
      "Epoch 1 step 111: training accuarcy: 0.5485\n",
      "Epoch 1 step 111: training loss: 3619.8642701770664\n",
      "Epoch 1 step 112: training accuarcy: 0.5575\n",
      "Epoch 1 step 112: training loss: 3519.8685064060014\n",
      "Epoch 1 step 113: training accuarcy: 0.559\n",
      "Epoch 1 step 113: training loss: 3429.8408643417315\n",
      "Epoch 1 step 114: training accuarcy: 0.5605\n",
      "Epoch 1 step 114: training loss: 3317.171246025193\n",
      "Epoch 1 step 115: training accuarcy: 0.5835\n",
      "Epoch 1 step 115: training loss: 3269.187232420293\n",
      "Epoch 1 step 116: training accuarcy: 0.5505\n",
      "Epoch 1 step 116: training loss: 3143.09769164175\n",
      "Epoch 1 step 117: training accuarcy: 0.5805\n",
      "Epoch 1 step 117: training loss: 3101.3105782452985\n",
      "Epoch 1 step 118: training accuarcy: 0.5735\n",
      "Epoch 1 step 118: training loss: 2998.5452066563525\n",
      "Epoch 1 step 119: training accuarcy: 0.5690000000000001\n",
      "Epoch 1 step 119: training loss: 2966.113051850607\n",
      "Epoch 1 step 120: training accuarcy: 0.5585\n",
      "Epoch 1 step 120: training loss: 2878.507129637528\n",
      "Epoch 1 step 121: training accuarcy: 0.558\n",
      "Epoch 1 step 121: training loss: 2808.2067537586713\n",
      "Epoch 1 step 122: training accuarcy: 0.5750000000000001\n",
      "Epoch 1 step 122: training loss: 2769.7318229136777\n",
      "Epoch 1 step 123: training accuarcy: 0.5555\n",
      "Epoch 1 step 123: training loss: 2694.2042718687326\n",
      "Epoch 1 step 124: training accuarcy: 0.5665\n",
      "Epoch 1 step 124: training loss: 2637.892571223434\n",
      "Epoch 1 step 125: training accuarcy: 0.5565\n",
      "Epoch 1 step 125: training loss: 2571.67125530036\n",
      "Epoch 1 step 126: training accuarcy: 0.5725\n",
      "Epoch 1 step 126: training loss: 2555.713106919163\n",
      "Epoch 1 step 127: training accuarcy: 0.534\n",
      "Epoch 1 step 127: training loss: 2478.4545893081695\n",
      "Epoch 1 step 128: training accuarcy: 0.5690000000000001\n",
      "Epoch 1 step 128: training loss: 2433.0444658185766\n",
      "Epoch 1 step 129: training accuarcy: 0.56\n",
      "Epoch 1 step 129: training loss: 2394.3364772197538\n",
      "Epoch 1 step 130: training accuarcy: 0.5660000000000001\n",
      "Epoch 1 step 130: training loss: 2335.694820736925\n",
      "Epoch 1 step 131: training accuarcy: 0.5755\n",
      "Epoch 1 step 131: training loss: 2289.275472414147\n",
      "Epoch 1 step 132: training accuarcy: 0.5675\n",
      "Epoch 1 step 132: training loss: 2255.664541223643\n",
      "Epoch 1 step 133: training accuarcy: 0.5765\n",
      "Epoch 1 step 133: training loss: 2204.116410090358\n",
      "Epoch 1 step 134: training accuarcy: 0.5715\n",
      "Epoch 1 step 134: training loss: 2173.642578027667\n",
      "Epoch 1 step 135: training accuarcy: 0.583\n",
      "Epoch 1 step 135: training loss: 2147.5813398412265\n",
      "Epoch 1 step 136: training accuarcy: 0.577\n",
      "Epoch 1 step 136: training loss: 2112.2613650892326\n",
      "Epoch 1 step 137: training accuarcy: 0.562\n",
      "Epoch 1 step 137: training loss: 2072.3823054615304\n",
      "Epoch 1 step 138: training accuarcy: 0.5670000000000001\n",
      "Epoch 1 step 138: training loss: 2036.529125932381\n",
      "Epoch 1 step 139: training accuarcy: 0.5760000000000001\n",
      "Epoch 1 step 139: training loss: 2007.349482881708\n",
      "Epoch 1 step 140: training accuarcy: 0.5805\n",
      "Epoch 1 step 140: training loss: 1983.6443453769052\n",
      "Epoch 1 step 141: training accuarcy: 0.5675\n",
      "Epoch 1 step 141: training loss: 1953.5311276758325\n",
      "Epoch 1 step 142: training accuarcy: 0.5725\n",
      "Epoch 1 step 142: training loss: 1943.1366542545188\n",
      "Epoch 1 step 143: training accuarcy: 0.5745\n",
      "Epoch 1 step 143: training loss: 1902.7961803982648\n",
      "Epoch 1 step 144: training accuarcy: 0.5805\n",
      "Epoch 1 step 144: training loss: 1887.4668706788855\n",
      "Epoch 1 step 145: training accuarcy: 0.5735\n",
      "Epoch 1 step 145: training loss: 1875.7968139461368\n",
      "Epoch 1 step 146: training accuarcy: 0.562\n",
      "Epoch 1 step 146: training loss: 1841.3817235692436\n",
      "Epoch 1 step 147: training accuarcy: 0.5725\n",
      "Epoch 1 step 147: training loss: 1819.5031340421845\n",
      "Epoch 1 step 148: training accuarcy: 0.5750000000000001\n",
      "Epoch 1 step 148: training loss: 1780.5277598647515\n",
      "Epoch 1 step 149: training accuarcy: 0.5875\n",
      "Epoch 1 step 149: training loss: 1775.01855681517\n",
      "Epoch 1 step 150: training accuarcy: 0.5775\n",
      "Epoch 1 step 150: training loss: 1771.7214688587896\n",
      "Epoch 1 step 151: training accuarcy: 0.5645\n",
      "Epoch 1 step 151: training loss: 1760.011102967885\n",
      "Epoch 1 step 152: training accuarcy: 0.5645\n",
      "Epoch 1 step 152: training loss: 1745.011571081338\n",
      "Epoch 1 step 153: training accuarcy: 0.5605\n",
      "Epoch 1 step 153: training loss: 1705.2998463791814\n",
      "Epoch 1 step 154: training accuarcy: 0.5750000000000001\n",
      "Epoch 1 step 154: training loss: 1697.216262365033\n",
      "Epoch 1 step 155: training accuarcy: 0.587\n",
      "Epoch 1 step 155: training loss: 1705.4101508025153\n",
      "Epoch 1 step 156: training accuarcy: 0.5695\n",
      "Epoch 1 step 156: training loss: 1682.42790342211\n",
      "Epoch 1 step 157: training accuarcy: 0.5650000000000001\n",
      "Epoch 1 step 157: training loss: 1657.712990256065\n",
      "Epoch 1 step 158: training accuarcy: 0.583\n",
      "Epoch 1 step 158: training loss: 1653.248125372721\n",
      "Epoch 1 step 159: training accuarcy: 0.5795\n",
      "Epoch 1 step 159: training loss: 1629.7455929016023\n",
      "Epoch 1 step 160: training accuarcy: 0.5655\n",
      "Epoch 1 step 160: training loss: 1627.8546624534904\n",
      "Epoch 1 step 161: training accuarcy: 0.5935\n",
      "Epoch 1 step 161: training loss: 1599.5439122284463\n",
      "Epoch 1 step 162: training accuarcy: 0.5795\n",
      "Epoch 1 step 162: training loss: 1604.421419385952\n",
      "Epoch 1 step 163: training accuarcy: 0.5665\n",
      "Epoch 1 step 163: training loss: 1592.8115605320381\n",
      "Epoch 1 step 164: training accuarcy: 0.5760000000000001\n",
      "Epoch 1 step 164: training loss: 1565.6325064137307\n",
      "Epoch 1 step 165: training accuarcy: 0.602\n",
      "Epoch 1 step 165: training loss: 1572.9976657786758\n",
      "Epoch 1 step 166: training accuarcy: 0.5865\n",
      "Epoch 1 step 166: training loss: 1551.0092766230573\n",
      "Epoch 1 step 167: training accuarcy: 0.599\n",
      "Epoch 1 step 167: training loss: 1568.6182065202597\n",
      "Epoch 1 step 168: training accuarcy: 0.5685\n",
      "Epoch 1 step 168: training loss: 1544.1962222048865\n",
      "Epoch 1 step 169: training accuarcy: 0.5745\n",
      "Epoch 1 step 169: training loss: 1551.659458628019\n",
      "Epoch 1 step 170: training accuarcy: 0.58\n",
      "Epoch 1 step 170: training loss: 1556.6690884606562\n",
      "Epoch 1 step 171: training accuarcy: 0.5700000000000001\n",
      "Epoch 1 step 171: training loss: 1530.3828642040198\n",
      "Epoch 1 step 172: training accuarcy: 0.589\n",
      "Epoch 1 step 172: training loss: 1522.0815754176006\n",
      "Epoch 1 step 173: training accuarcy: 0.583\n",
      "Epoch 1 step 173: training loss: 1512.7464100135344\n",
      "Epoch 1 step 174: training accuarcy: 0.592\n",
      "Epoch 1 step 174: training loss: 1518.6183018424301\n",
      "Epoch 1 step 175: training accuarcy: 0.578\n",
      "Epoch 1 step 175: training loss: 1500.674141674332\n",
      "Epoch 1 step 176: training accuarcy: 0.587\n",
      "Epoch 1 step 176: training loss: 1496.7424130142497\n",
      "Epoch 1 step 177: training accuarcy: 0.591\n",
      "Epoch 1 step 177: training loss: 1504.232651407304\n",
      "Epoch 1 step 178: training accuarcy: 0.5750000000000001\n",
      "Epoch 1 step 178: training loss: 1510.7119652439442\n",
      "Epoch 1 step 179: training accuarcy: 0.558\n",
      "Epoch 1 step 179: training loss: 1488.4874618619149\n",
      "Epoch 1 step 180: training accuarcy: 0.5775\n",
      "Epoch 1 step 180: training loss: 1448.8884480387665\n",
      "Epoch 1 step 181: training accuarcy: 0.607\n",
      "Epoch 1 step 181: training loss: 1483.688963958393\n",
      "Epoch 1 step 182: training accuarcy: 0.5645\n",
      "Epoch 1 step 182: training loss: 1443.9560893560383\n",
      "Epoch 1 step 183: training accuarcy: 0.6015\n",
      "Epoch 1 step 183: training loss: 1459.2712798368277\n",
      "Epoch 1 step 184: training accuarcy: 0.5795\n",
      "Epoch 1 step 184: training loss: 1452.4437053684373\n",
      "Epoch 1 step 185: training accuarcy: 0.585\n",
      "Epoch 1 step 185: training loss: 1453.4270456038912\n",
      "Epoch 1 step 186: training accuarcy: 0.592\n",
      "Epoch 1 step 186: training loss: 1446.5982605526813\n",
      "Epoch 1 step 187: training accuarcy: 0.5825\n",
      "Epoch 1 step 187: training loss: 1141.1731776476918\n",
      "Epoch 1 step 188: training accuarcy: 0.5908798972382787\n",
      "Epoch 1: train loss 2518.1382164851316, train accuarcy 0.5696715116500854\n",
      "Epoch 1: valid loss 6357.3272517917385, valid accuarcy 0.5705171823501587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████████████████████████████████████████▏                                                                                                       | 2/5 [02:43<04:04, 81.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 188: training loss: 1462.1588681213207\n",
      "Epoch 2 step 189: training accuarcy: 0.5760000000000001\n",
      "Epoch 2 step 189: training loss: 1459.5134983492237\n",
      "Epoch 2 step 190: training accuarcy: 0.5635\n",
      "Epoch 2 step 190: training loss: 1419.5122899628082\n",
      "Epoch 2 step 191: training accuarcy: 0.6095\n",
      "Epoch 2 step 191: training loss: 1427.6784447823543\n",
      "Epoch 2 step 192: training accuarcy: 0.599\n",
      "Epoch 2 step 192: training loss: 1430.9025668788129\n",
      "Epoch 2 step 193: training accuarcy: 0.5845\n",
      "Epoch 2 step 193: training loss: 1437.9288567525618\n",
      "Epoch 2 step 194: training accuarcy: 0.5760000000000001\n",
      "Epoch 2 step 194: training loss: 1420.411428771369\n",
      "Epoch 2 step 195: training accuarcy: 0.592\n",
      "Epoch 2 step 195: training loss: 1418.2684853701348\n",
      "Epoch 2 step 196: training accuarcy: 0.5935\n",
      "Epoch 2 step 196: training loss: 1412.1620188326933\n",
      "Epoch 2 step 197: training accuarcy: 0.595\n",
      "Epoch 2 step 197: training loss: 1408.43839176695\n",
      "Epoch 2 step 198: training accuarcy: 0.5905\n",
      "Epoch 2 step 198: training loss: 1409.0386254866705\n",
      "Epoch 2 step 199: training accuarcy: 0.609\n",
      "Epoch 2 step 199: training loss: 1411.2013692332487\n",
      "Epoch 2 step 200: training accuarcy: 0.5865\n",
      "Epoch 2 step 200: training loss: 1428.1186799727539\n",
      "Epoch 2 step 201: training accuarcy: 0.5795\n",
      "Epoch 2 step 201: training loss: 1430.3099138328812\n",
      "Epoch 2 step 202: training accuarcy: 0.5705\n",
      "Epoch 2 step 202: training loss: 1420.423782870545\n",
      "Epoch 2 step 203: training accuarcy: 0.586\n",
      "Epoch 2 step 203: training loss: 1414.912528740481\n",
      "Epoch 2 step 204: training accuarcy: 0.5895\n",
      "Epoch 2 step 204: training loss: 1424.2107263624498\n",
      "Epoch 2 step 205: training accuarcy: 0.5705\n",
      "Epoch 2 step 205: training loss: 1385.4870163610829\n",
      "Epoch 2 step 206: training accuarcy: 0.607\n",
      "Epoch 2 step 206: training loss: 1410.4696155665563\n",
      "Epoch 2 step 207: training accuarcy: 0.5710000000000001\n",
      "Epoch 2 step 207: training loss: 1399.8991756364849\n",
      "Epoch 2 step 208: training accuarcy: 0.5785\n",
      "Epoch 2 step 208: training loss: 1401.0526751307152\n",
      "Epoch 2 step 209: training accuarcy: 0.592\n",
      "Epoch 2 step 209: training loss: 1395.5311821499286\n",
      "Epoch 2 step 210: training accuarcy: 0.603\n",
      "Epoch 2 step 210: training loss: 1389.5300914960792\n",
      "Epoch 2 step 211: training accuarcy: 0.582\n",
      "Epoch 2 step 211: training loss: 1388.5631945005916\n",
      "Epoch 2 step 212: training accuarcy: 0.584\n",
      "Epoch 2 step 212: training loss: 1378.8010482622954\n",
      "Epoch 2 step 213: training accuarcy: 0.608\n",
      "Epoch 2 step 213: training loss: 1406.4700461830223\n",
      "Epoch 2 step 214: training accuarcy: 0.5710000000000001\n",
      "Epoch 2 step 214: training loss: 1393.8531620393148\n",
      "Epoch 2 step 215: training accuarcy: 0.5805\n",
      "Epoch 2 step 215: training loss: 1377.0956476861306\n",
      "Epoch 2 step 216: training accuarcy: 0.5755\n",
      "Epoch 2 step 216: training loss: 1391.2076146785441\n",
      "Epoch 2 step 217: training accuarcy: 0.6005\n",
      "Epoch 2 step 217: training loss: 1380.2354854763903\n",
      "Epoch 2 step 218: training accuarcy: 0.6055\n",
      "Epoch 2 step 218: training loss: 1370.7363998134479\n",
      "Epoch 2 step 219: training accuarcy: 0.594\n",
      "Epoch 2 step 219: training loss: 1372.427489447212\n",
      "Epoch 2 step 220: training accuarcy: 0.6035\n",
      "Epoch 2 step 220: training loss: 1378.715075768395\n",
      "Epoch 2 step 221: training accuarcy: 0.606\n",
      "Epoch 2 step 221: training loss: 1381.5622675566492\n",
      "Epoch 2 step 222: training accuarcy: 0.5915\n",
      "Epoch 2 step 222: training loss: 1382.529768753932\n",
      "Epoch 2 step 223: training accuarcy: 0.607\n",
      "Epoch 2 step 223: training loss: 1382.4801832211353\n",
      "Epoch 2 step 224: training accuarcy: 0.5845\n",
      "Epoch 2 step 224: training loss: 1382.1010789589513\n",
      "Epoch 2 step 225: training accuarcy: 0.6005\n",
      "Epoch 2 step 225: training loss: 1371.7299126123664\n",
      "Epoch 2 step 226: training accuarcy: 0.5945\n",
      "Epoch 2 step 226: training loss: 1362.4472729636507\n",
      "Epoch 2 step 227: training accuarcy: 0.595\n",
      "Epoch 2 step 227: training loss: 1373.0988416051027\n",
      "Epoch 2 step 228: training accuarcy: 0.606\n",
      "Epoch 2 step 228: training loss: 1365.8375077403718\n",
      "Epoch 2 step 229: training accuarcy: 0.599\n",
      "Epoch 2 step 229: training loss: 1382.3022188812897\n",
      "Epoch 2 step 230: training accuarcy: 0.589\n",
      "Epoch 2 step 230: training loss: 1371.2374784586827\n",
      "Epoch 2 step 231: training accuarcy: 0.601\n",
      "Epoch 2 step 231: training loss: 1394.209719406688\n",
      "Epoch 2 step 232: training accuarcy: 0.5675\n",
      "Epoch 2 step 232: training loss: 1369.9706009017416\n",
      "Epoch 2 step 233: training accuarcy: 0.5955\n",
      "Epoch 2 step 233: training loss: 1366.7729720172943\n",
      "Epoch 2 step 234: training accuarcy: 0.59\n",
      "Epoch 2 step 234: training loss: 1363.3804416699465\n",
      "Epoch 2 step 235: training accuarcy: 0.5905\n",
      "Epoch 2 step 235: training loss: 1367.5150111347434\n",
      "Epoch 2 step 236: training accuarcy: 0.5795\n",
      "Epoch 2 step 236: training loss: 1364.5770433867722\n",
      "Epoch 2 step 237: training accuarcy: 0.5975\n",
      "Epoch 2 step 237: training loss: 1367.947895151849\n",
      "Epoch 2 step 238: training accuarcy: 0.5885\n",
      "Epoch 2 step 238: training loss: 1366.1272789414636\n",
      "Epoch 2 step 239: training accuarcy: 0.597\n",
      "Epoch 2 step 239: training loss: 1369.8150254458262\n",
      "Epoch 2 step 240: training accuarcy: 0.5925\n",
      "Epoch 2 step 240: training loss: 1362.8026093117765\n",
      "Epoch 2 step 241: training accuarcy: 0.5995\n",
      "Epoch 2 step 241: training loss: 1360.4395788225202\n",
      "Epoch 2 step 242: training accuarcy: 0.596\n",
      "Epoch 2 step 242: training loss: 1367.5159082319537\n",
      "Epoch 2 step 243: training accuarcy: 0.5935\n",
      "Epoch 2 step 243: training loss: 1373.4502312084553\n",
      "Epoch 2 step 244: training accuarcy: 0.577\n",
      "Epoch 2 step 244: training loss: 1370.8130071634046\n",
      "Epoch 2 step 245: training accuarcy: 0.5955\n",
      "Epoch 2 step 245: training loss: 1348.5633393407331\n",
      "Epoch 2 step 246: training accuarcy: 0.607\n",
      "Epoch 2 step 246: training loss: 1365.419199067497\n",
      "Epoch 2 step 247: training accuarcy: 0.5815\n",
      "Epoch 2 step 247: training loss: 1369.9272756961873\n",
      "Epoch 2 step 248: training accuarcy: 0.602\n",
      "Epoch 2 step 248: training loss: 1360.1648962049674\n",
      "Epoch 2 step 249: training accuarcy: 0.592\n",
      "Epoch 2 step 249: training loss: 1369.3908606528196\n",
      "Epoch 2 step 250: training accuarcy: 0.5825\n",
      "Epoch 2 step 250: training loss: 1358.6891994595735\n",
      "Epoch 2 step 251: training accuarcy: 0.601\n",
      "Epoch 2 step 251: training loss: 1367.2869104553265\n",
      "Epoch 2 step 252: training accuarcy: 0.595\n",
      "Epoch 2 step 252: training loss: 1354.0748393325916\n",
      "Epoch 2 step 253: training accuarcy: 0.59\n",
      "Epoch 2 step 253: training loss: 1356.8050816447178\n",
      "Epoch 2 step 254: training accuarcy: 0.595\n",
      "Epoch 2 step 254: training loss: 1371.1291274804469\n",
      "Epoch 2 step 255: training accuarcy: 0.5755\n",
      "Epoch 2 step 255: training loss: 1370.4929010752332\n",
      "Epoch 2 step 256: training accuarcy: 0.5765\n",
      "Epoch 2 step 256: training loss: 1364.0044146720334\n",
      "Epoch 2 step 257: training accuarcy: 0.5855\n",
      "Epoch 2 step 257: training loss: 1354.3217492975493\n",
      "Epoch 2 step 258: training accuarcy: 0.59\n",
      "Epoch 2 step 258: training loss: 1356.5933431302592\n",
      "Epoch 2 step 259: training accuarcy: 0.583\n",
      "Epoch 2 step 259: training loss: 1363.6169421850516\n",
      "Epoch 2 step 260: training accuarcy: 0.5935\n",
      "Epoch 2 step 260: training loss: 1355.8898011890728\n",
      "Epoch 2 step 261: training accuarcy: 0.578\n",
      "Epoch 2 step 261: training loss: 1354.9953625586782\n",
      "Epoch 2 step 262: training accuarcy: 0.5855\n",
      "Epoch 2 step 262: training loss: 1365.6788048204312\n",
      "Epoch 2 step 263: training accuarcy: 0.5750000000000001\n",
      "Epoch 2 step 263: training loss: 1355.689062697982\n",
      "Epoch 2 step 264: training accuarcy: 0.579\n",
      "Epoch 2 step 264: training loss: 1345.9718282848776\n",
      "Epoch 2 step 265: training accuarcy: 0.592\n",
      "Epoch 2 step 265: training loss: 1366.1600673267264\n",
      "Epoch 2 step 266: training accuarcy: 0.5755\n",
      "Epoch 2 step 266: training loss: 1357.7893760602476\n",
      "Epoch 2 step 267: training accuarcy: 0.5865\n",
      "Epoch 2 step 267: training loss: 1369.4043780935453\n",
      "Epoch 2 step 268: training accuarcy: 0.5745\n",
      "Epoch 2 step 268: training loss: 1348.4224362983314\n",
      "Epoch 2 step 269: training accuarcy: 0.588\n",
      "Epoch 2 step 269: training loss: 1360.2496362218626\n",
      "Epoch 2 step 270: training accuarcy: 0.58\n",
      "Epoch 2 step 270: training loss: 1369.9477938977766\n",
      "Epoch 2 step 271: training accuarcy: 0.5690000000000001\n",
      "Epoch 2 step 271: training loss: 1348.668456871639\n",
      "Epoch 2 step 272: training accuarcy: 0.6005\n",
      "Epoch 2 step 272: training loss: 1342.3305204689725\n",
      "Epoch 2 step 273: training accuarcy: 0.594\n",
      "Epoch 2 step 273: training loss: 1362.1513634151338\n",
      "Epoch 2 step 274: training accuarcy: 0.588\n",
      "Epoch 2 step 274: training loss: 1345.816375637759\n",
      "Epoch 2 step 275: training accuarcy: 0.596\n",
      "Epoch 2 step 275: training loss: 1349.468867769946\n",
      "Epoch 2 step 276: training accuarcy: 0.601\n",
      "Epoch 2 step 276: training loss: 1349.094574315947\n",
      "Epoch 2 step 277: training accuarcy: 0.5985\n",
      "Epoch 2 step 277: training loss: 1350.154270427487\n",
      "Epoch 2 step 278: training accuarcy: 0.6\n",
      "Epoch 2 step 278: training loss: 1352.1447535897732\n",
      "Epoch 2 step 279: training accuarcy: 0.5875\n",
      "Epoch 2 step 279: training loss: 1358.773068634042\n",
      "Epoch 2 step 280: training accuarcy: 0.586\n",
      "Epoch 2 step 280: training loss: 1348.3162626246535\n",
      "Epoch 2 step 281: training accuarcy: 0.59\n",
      "Epoch 2 step 281: training loss: 1046.7154705579758\n",
      "Epoch 2 step 282: training accuarcy: 0.5960179833012202\n",
      "Epoch 2: train loss 1375.6408708647648, train accuarcy 0.5866575837135315\n",
      "Epoch 2: valid loss 6232.132208886287, valid accuarcy 0.5743697881698608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                     | 3/5 [04:06<02:43, 81.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 282: training loss: 1353.9985873569228\n",
      "Epoch 3 step 283: training accuarcy: 0.5825\n",
      "Epoch 3 step 283: training loss: 1344.3050780022159\n",
      "Epoch 3 step 284: training accuarcy: 0.596\n",
      "Epoch 3 step 284: training loss: 1353.9195510319328\n",
      "Epoch 3 step 285: training accuarcy: 0.597\n",
      "Epoch 3 step 285: training loss: 1347.4614304166591\n",
      "Epoch 3 step 286: training accuarcy: 0.6025\n",
      "Epoch 3 step 286: training loss: 1338.7372483156742\n",
      "Epoch 3 step 287: training accuarcy: 0.6025\n",
      "Epoch 3 step 287: training loss: 1340.5008611809385\n",
      "Epoch 3 step 288: training accuarcy: 0.581\n",
      "Epoch 3 step 288: training loss: 1327.5749387798894\n",
      "Epoch 3 step 289: training accuarcy: 0.61\n",
      "Epoch 3 step 289: training loss: 1356.6349051462687\n",
      "Epoch 3 step 290: training accuarcy: 0.5875\n",
      "Epoch 3 step 290: training loss: 1336.8306093867122\n",
      "Epoch 3 step 291: training accuarcy: 0.5955\n",
      "Epoch 3 step 291: training loss: 1336.0148650790172\n",
      "Epoch 3 step 292: training accuarcy: 0.609\n",
      "Epoch 3 step 292: training loss: 1345.3436580956734\n",
      "Epoch 3 step 293: training accuarcy: 0.603\n",
      "Epoch 3 step 293: training loss: 1348.7492513700406\n",
      "Epoch 3 step 294: training accuarcy: 0.58\n",
      "Epoch 3 step 294: training loss: 1348.2659409133487\n",
      "Epoch 3 step 295: training accuarcy: 0.5855\n",
      "Epoch 3 step 295: training loss: 1374.4632336161985\n",
      "Epoch 3 step 296: training accuarcy: 0.5670000000000001\n",
      "Epoch 3 step 296: training loss: 1363.6126971527517\n",
      "Epoch 3 step 297: training accuarcy: 0.5735\n",
      "Epoch 3 step 297: training loss: 1361.308366268094\n",
      "Epoch 3 step 298: training accuarcy: 0.594\n",
      "Epoch 3 step 298: training loss: 1349.013752523923\n",
      "Epoch 3 step 299: training accuarcy: 0.591\n",
      "Epoch 3 step 299: training loss: 1356.7167524329104\n",
      "Epoch 3 step 300: training accuarcy: 0.579\n",
      "Epoch 3 step 300: training loss: 1345.5124605192468\n",
      "Epoch 3 step 301: training accuarcy: 0.6015\n",
      "Epoch 3 step 301: training loss: 1338.1363362946033\n",
      "Epoch 3 step 302: training accuarcy: 0.5995\n",
      "Epoch 3 step 302: training loss: 1344.6325150359442\n",
      "Epoch 3 step 303: training accuarcy: 0.5885\n",
      "Epoch 3 step 303: training loss: 1354.5983718219125\n",
      "Epoch 3 step 304: training accuarcy: 0.589\n",
      "Epoch 3 step 304: training loss: 1365.5643915506969\n",
      "Epoch 3 step 305: training accuarcy: 0.5760000000000001\n",
      "Epoch 3 step 305: training loss: 1352.7118359147753\n",
      "Epoch 3 step 306: training accuarcy: 0.5730000000000001\n",
      "Epoch 3 step 306: training loss: 1356.231718842774\n",
      "Epoch 3 step 307: training accuarcy: 0.59\n",
      "Epoch 3 step 307: training loss: 1353.5611994118067\n",
      "Epoch 3 step 308: training accuarcy: 0.589\n",
      "Epoch 3 step 308: training loss: 1326.701059423571\n",
      "Epoch 3 step 309: training accuarcy: 0.607\n",
      "Epoch 3 step 309: training loss: 1333.720531990052\n",
      "Epoch 3 step 310: training accuarcy: 0.614\n",
      "Epoch 3 step 310: training loss: 1349.3906732500468\n",
      "Epoch 3 step 311: training accuarcy: 0.583\n",
      "Epoch 3 step 311: training loss: 1356.7997085788204\n",
      "Epoch 3 step 312: training accuarcy: 0.5885\n",
      "Epoch 3 step 312: training loss: 1340.3484833851207\n",
      "Epoch 3 step 313: training accuarcy: 0.6025\n",
      "Epoch 3 step 313: training loss: 1346.4105988852214\n",
      "Epoch 3 step 314: training accuarcy: 0.6005\n",
      "Epoch 3 step 314: training loss: 1360.3400214010805\n",
      "Epoch 3 step 315: training accuarcy: 0.5650000000000001\n",
      "Epoch 3 step 315: training loss: 1339.8649631886603\n",
      "Epoch 3 step 316: training accuarcy: 0.5885\n",
      "Epoch 3 step 316: training loss: 1345.37621626774\n",
      "Epoch 3 step 317: training accuarcy: 0.5935\n",
      "Epoch 3 step 317: training loss: 1341.8481093713488\n",
      "Epoch 3 step 318: training accuarcy: 0.6055\n",
      "Epoch 3 step 318: training loss: 1359.6345025728956\n",
      "Epoch 3 step 319: training accuarcy: 0.5670000000000001\n",
      "Epoch 3 step 319: training loss: 1341.3366222469901\n",
      "Epoch 3 step 320: training accuarcy: 0.602\n",
      "Epoch 3 step 320: training loss: 1374.3450610090472\n",
      "Epoch 3 step 321: training accuarcy: 0.5635\n",
      "Epoch 3 step 321: training loss: 1327.087266243488\n",
      "Epoch 3 step 322: training accuarcy: 0.61\n",
      "Epoch 3 step 322: training loss: 1369.1731701051303\n",
      "Epoch 3 step 323: training accuarcy: 0.5730000000000001\n",
      "Epoch 3 step 323: training loss: 1332.2900639791894\n",
      "Epoch 3 step 324: training accuarcy: 0.6035\n",
      "Epoch 3 step 324: training loss: 1326.4539576397628\n",
      "Epoch 3 step 325: training accuarcy: 0.598\n",
      "Epoch 3 step 325: training loss: 1343.9563756365108\n",
      "Epoch 3 step 326: training accuarcy: 0.585\n",
      "Epoch 3 step 326: training loss: 1337.5182187232956\n",
      "Epoch 3 step 327: training accuarcy: 0.6165\n",
      "Epoch 3 step 327: training loss: 1334.1131621530312\n",
      "Epoch 3 step 328: training accuarcy: 0.5945\n",
      "Epoch 3 step 328: training loss: 1339.2558780075396\n",
      "Epoch 3 step 329: training accuarcy: 0.586\n",
      "Epoch 3 step 329: training loss: 1345.603328784787\n",
      "Epoch 3 step 330: training accuarcy: 0.604\n",
      "Epoch 3 step 330: training loss: 1329.9854274807726\n",
      "Epoch 3 step 331: training accuarcy: 0.6155\n",
      "Epoch 3 step 331: training loss: 1337.563945491135\n",
      "Epoch 3 step 332: training accuarcy: 0.589\n",
      "Epoch 3 step 332: training loss: 1341.2719510398763\n",
      "Epoch 3 step 333: training accuarcy: 0.5885\n",
      "Epoch 3 step 333: training loss: 1347.3030580341367\n",
      "Epoch 3 step 334: training accuarcy: 0.5945\n",
      "Epoch 3 step 334: training loss: 1346.4461293190727\n",
      "Epoch 3 step 335: training accuarcy: 0.588\n",
      "Epoch 3 step 335: training loss: 1353.0605174037205\n",
      "Epoch 3 step 336: training accuarcy: 0.595\n",
      "Epoch 3 step 336: training loss: 1352.936545705243\n",
      "Epoch 3 step 337: training accuarcy: 0.5955\n",
      "Epoch 3 step 337: training loss: 1318.367462113326\n",
      "Epoch 3 step 338: training accuarcy: 0.6085\n",
      "Epoch 3 step 338: training loss: 1330.8472233412788\n",
      "Epoch 3 step 339: training accuarcy: 0.5925\n",
      "Epoch 3 step 339: training loss: 1325.2247224556174\n",
      "Epoch 3 step 340: training accuarcy: 0.5945\n",
      "Epoch 3 step 340: training loss: 1326.4771813809984\n",
      "Epoch 3 step 341: training accuarcy: 0.607\n",
      "Epoch 3 step 341: training loss: 1348.0640674150445\n",
      "Epoch 3 step 342: training accuarcy: 0.581\n",
      "Epoch 3 step 342: training loss: 1344.6229127590827\n",
      "Epoch 3 step 343: training accuarcy: 0.5905\n",
      "Epoch 3 step 343: training loss: 1354.9083658615255\n",
      "Epoch 3 step 344: training accuarcy: 0.5865\n",
      "Epoch 3 step 344: training loss: 1348.037649206145\n",
      "Epoch 3 step 345: training accuarcy: 0.5835\n",
      "Epoch 3 step 345: training loss: 1336.4662487466535\n",
      "Epoch 3 step 346: training accuarcy: 0.5985\n",
      "Epoch 3 step 346: training loss: 1361.3691422022325\n",
      "Epoch 3 step 347: training accuarcy: 0.583\n",
      "Epoch 3 step 347: training loss: 1338.514353909022\n",
      "Epoch 3 step 348: training accuarcy: 0.5985\n",
      "Epoch 3 step 348: training loss: 1344.5624158169767\n",
      "Epoch 3 step 349: training accuarcy: 0.5935\n",
      "Epoch 3 step 349: training loss: 1323.8323171878662\n",
      "Epoch 3 step 350: training accuarcy: 0.5985\n",
      "Epoch 3 step 350: training loss: 1351.4048176036447\n",
      "Epoch 3 step 351: training accuarcy: 0.5975\n",
      "Epoch 3 step 351: training loss: 1342.800816955282\n",
      "Epoch 3 step 352: training accuarcy: 0.588\n",
      "Epoch 3 step 352: training loss: 1338.4905491775753\n",
      "Epoch 3 step 353: training accuarcy: 0.593\n",
      "Epoch 3 step 353: training loss: 1350.15972109006\n",
      "Epoch 3 step 354: training accuarcy: 0.5845\n",
      "Epoch 3 step 354: training loss: 1348.8131369269338\n",
      "Epoch 3 step 355: training accuarcy: 0.5725\n",
      "Epoch 3 step 355: training loss: 1350.8129436544673\n",
      "Epoch 3 step 356: training accuarcy: 0.577\n",
      "Epoch 3 step 356: training loss: 1342.4314882722629\n",
      "Epoch 3 step 357: training accuarcy: 0.5875\n",
      "Epoch 3 step 357: training loss: 1349.6895155926761\n",
      "Epoch 3 step 358: training accuarcy: 0.578\n",
      "Epoch 3 step 358: training loss: 1361.5235371021133\n",
      "Epoch 3 step 359: training accuarcy: 0.5715\n",
      "Epoch 3 step 359: training loss: 1351.1017808779154\n",
      "Epoch 3 step 360: training accuarcy: 0.59\n",
      "Epoch 3 step 360: training loss: 1342.9635441285636\n",
      "Epoch 3 step 361: training accuarcy: 0.5855\n",
      "Epoch 3 step 361: training loss: 1337.2481480667955\n",
      "Epoch 3 step 362: training accuarcy: 0.592\n",
      "Epoch 3 step 362: training loss: 1344.1266120790128\n",
      "Epoch 3 step 363: training accuarcy: 0.6\n",
      "Epoch 3 step 363: training loss: 1352.8257698950104\n",
      "Epoch 3 step 364: training accuarcy: 0.5815\n",
      "Epoch 3 step 364: training loss: 1333.124303261034\n",
      "Epoch 3 step 365: training accuarcy: 0.598\n",
      "Epoch 3 step 365: training loss: 1334.813139733745\n",
      "Epoch 3 step 366: training accuarcy: 0.5815\n",
      "Epoch 3 step 366: training loss: 1335.7547008604147\n",
      "Epoch 3 step 367: training accuarcy: 0.593\n",
      "Epoch 3 step 367: training loss: 1348.5675334946704\n",
      "Epoch 3 step 368: training accuarcy: 0.582\n",
      "Epoch 3 step 368: training loss: 1341.4575289411018\n",
      "Epoch 3 step 369: training accuarcy: 0.594\n",
      "Epoch 3 step 369: training loss: 1361.5319354245778\n",
      "Epoch 3 step 370: training accuarcy: 0.5805\n",
      "Epoch 3 step 370: training loss: 1322.2978957697137\n",
      "Epoch 3 step 371: training accuarcy: 0.6025\n",
      "Epoch 3 step 371: training loss: 1351.2152066176752\n",
      "Epoch 3 step 372: training accuarcy: 0.5975\n",
      "Epoch 3 step 372: training loss: 1354.389429213065\n",
      "Epoch 3 step 373: training accuarcy: 0.5740000000000001\n",
      "Epoch 3 step 373: training loss: 1337.9083170042059\n",
      "Epoch 3 step 374: training accuarcy: 0.5825\n",
      "Epoch 3 step 374: training loss: 1333.490549025951\n",
      "Epoch 3 step 375: training accuarcy: 0.5865\n",
      "Epoch 3 step 375: training loss: 1053.8957464833597\n",
      "Epoch 3 step 376: training accuarcy: 0.5915221579961464\n",
      "Epoch 3: train loss 1342.007115249211, train accuarcy 0.589872419834137\n",
      "Epoch 3: valid loss 6221.546124163222, valid accuarcy 0.5713282823562622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                  | 4/5 [05:27<01:21, 81.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 376: training loss: 1348.1471435258568\n",
      "Epoch 4 step 377: training accuarcy: 0.5825\n",
      "Epoch 4 step 377: training loss: 1342.0333891995263\n",
      "Epoch 4 step 378: training accuarcy: 0.5875\n",
      "Epoch 4 step 378: training loss: 1340.9567959929414\n",
      "Epoch 4 step 379: training accuarcy: 0.59\n",
      "Epoch 4 step 379: training loss: 1340.508620966616\n",
      "Epoch 4 step 380: training accuarcy: 0.602\n",
      "Epoch 4 step 380: training loss: 1333.850060282119\n",
      "Epoch 4 step 381: training accuarcy: 0.59\n",
      "Epoch 4 step 381: training loss: 1342.1477539333177\n",
      "Epoch 4 step 382: training accuarcy: 0.5945\n",
      "Epoch 4 step 382: training loss: 1351.6574195347023\n",
      "Epoch 4 step 383: training accuarcy: 0.5905\n",
      "Epoch 4 step 383: training loss: 1354.0717183966487\n",
      "Epoch 4 step 384: training accuarcy: 0.582\n",
      "Epoch 4 step 384: training loss: 1328.1494206470863\n",
      "Epoch 4 step 385: training accuarcy: 0.602\n",
      "Epoch 4 step 385: training loss: 1340.558231826908\n",
      "Epoch 4 step 386: training accuarcy: 0.5915\n",
      "Epoch 4 step 386: training loss: 1321.7579721355062\n",
      "Epoch 4 step 387: training accuarcy: 0.6\n",
      "Epoch 4 step 387: training loss: 1334.0372395459221\n",
      "Epoch 4 step 388: training accuarcy: 0.5915\n",
      "Epoch 4 step 388: training loss: 1342.3622915700173\n",
      "Epoch 4 step 389: training accuarcy: 0.601\n",
      "Epoch 4 step 389: training loss: 1337.5062426143534\n",
      "Epoch 4 step 390: training accuarcy: 0.5875\n",
      "Epoch 4 step 390: training loss: 1353.9195805809559\n",
      "Epoch 4 step 391: training accuarcy: 0.5845\n",
      "Epoch 4 step 391: training loss: 1333.2884711151303\n",
      "Epoch 4 step 392: training accuarcy: 0.5855\n",
      "Epoch 4 step 392: training loss: 1329.399908714051\n",
      "Epoch 4 step 393: training accuarcy: 0.6005\n",
      "Epoch 4 step 393: training loss: 1348.5769575788997\n",
      "Epoch 4 step 394: training accuarcy: 0.594\n",
      "Epoch 4 step 394: training loss: 1332.5992732853097\n",
      "Epoch 4 step 395: training accuarcy: 0.6045\n",
      "Epoch 4 step 395: training loss: 1337.484874600418\n",
      "Epoch 4 step 396: training accuarcy: 0.595\n",
      "Epoch 4 step 396: training loss: 1327.663608745091\n",
      "Epoch 4 step 397: training accuarcy: 0.5965\n",
      "Epoch 4 step 397: training loss: 1333.7440058547606\n",
      "Epoch 4 step 398: training accuarcy: 0.6035\n",
      "Epoch 4 step 398: training loss: 1337.3523115564012\n",
      "Epoch 4 step 399: training accuarcy: 0.59\n",
      "Epoch 4 step 399: training loss: 1317.969484810965\n",
      "Epoch 4 step 400: training accuarcy: 0.599\n",
      "Epoch 4 step 400: training loss: 1327.3769773590839\n",
      "Epoch 4 step 401: training accuarcy: 0.5945\n",
      "Epoch 4 step 401: training loss: 1342.713494754346\n",
      "Epoch 4 step 402: training accuarcy: 0.5995\n",
      "Epoch 4 step 402: training loss: 1362.765238368819\n",
      "Epoch 4 step 403: training accuarcy: 0.58\n",
      "Epoch 4 step 403: training loss: 1341.574788730449\n",
      "Epoch 4 step 404: training accuarcy: 0.589\n",
      "Epoch 4 step 404: training loss: 1341.9154364765063\n",
      "Epoch 4 step 405: training accuarcy: 0.5895\n",
      "Epoch 4 step 405: training loss: 1348.5235657835176\n",
      "Epoch 4 step 406: training accuarcy: 0.5825\n",
      "Epoch 4 step 406: training loss: 1349.9114280162764\n",
      "Epoch 4 step 407: training accuarcy: 0.58\n",
      "Epoch 4 step 407: training loss: 1330.5710152971096\n",
      "Epoch 4 step 408: training accuarcy: 0.59\n",
      "Epoch 4 step 408: training loss: 1340.8998199355588\n",
      "Epoch 4 step 409: training accuarcy: 0.592\n",
      "Epoch 4 step 409: training loss: 1317.4528906917058\n",
      "Epoch 4 step 410: training accuarcy: 0.612\n",
      "Epoch 4 step 410: training loss: 1351.5059027015068\n",
      "Epoch 4 step 411: training accuarcy: 0.5865\n",
      "Epoch 4 step 411: training loss: 1332.5356545035752\n",
      "Epoch 4 step 412: training accuarcy: 0.5915\n",
      "Epoch 4 step 412: training loss: 1351.5007019221227\n",
      "Epoch 4 step 413: training accuarcy: 0.5895\n",
      "Epoch 4 step 413: training loss: 1338.057974214965\n",
      "Epoch 4 step 414: training accuarcy: 0.5945\n",
      "Epoch 4 step 414: training loss: 1343.269687910867\n",
      "Epoch 4 step 415: training accuarcy: 0.5905\n",
      "Epoch 4 step 415: training loss: 1316.1105447274035\n",
      "Epoch 4 step 416: training accuarcy: 0.6065\n",
      "Epoch 4 step 416: training loss: 1341.7577250586696\n",
      "Epoch 4 step 417: training accuarcy: 0.594\n",
      "Epoch 4 step 417: training loss: 1328.4554358254206\n",
      "Epoch 4 step 418: training accuarcy: 0.6035\n",
      "Epoch 4 step 418: training loss: 1354.8440581297511\n",
      "Epoch 4 step 419: training accuarcy: 0.5765\n",
      "Epoch 4 step 419: training loss: 1333.013308091899\n",
      "Epoch 4 step 420: training accuarcy: 0.598\n",
      "Epoch 4 step 420: training loss: 1346.4811117178833\n",
      "Epoch 4 step 421: training accuarcy: 0.585\n",
      "Epoch 4 step 421: training loss: 1340.1587400520468\n",
      "Epoch 4 step 422: training accuarcy: 0.5955\n",
      "Epoch 4 step 422: training loss: 1343.5943164786258\n",
      "Epoch 4 step 423: training accuarcy: 0.578\n",
      "Epoch 4 step 423: training loss: 1338.4604524011224\n",
      "Epoch 4 step 424: training accuarcy: 0.5885\n",
      "Epoch 4 step 424: training loss: 1338.3191009496575\n",
      "Epoch 4 step 425: training accuarcy: 0.5895\n",
      "Epoch 4 step 425: training loss: 1336.6761924178677\n",
      "Epoch 4 step 426: training accuarcy: 0.5875\n",
      "Epoch 4 step 426: training loss: 1332.5504478503312\n",
      "Epoch 4 step 427: training accuarcy: 0.587\n",
      "Epoch 4 step 427: training loss: 1330.876091127687\n",
      "Epoch 4 step 428: training accuarcy: 0.613\n",
      "Epoch 4 step 428: training loss: 1334.7733815072468\n",
      "Epoch 4 step 429: training accuarcy: 0.587\n",
      "Epoch 4 step 429: training loss: 1344.2455613643915\n",
      "Epoch 4 step 430: training accuarcy: 0.585\n",
      "Epoch 4 step 430: training loss: 1329.2555116242793\n",
      "Epoch 4 step 431: training accuarcy: 0.5945\n",
      "Epoch 4 step 431: training loss: 1330.1428753238692\n",
      "Epoch 4 step 432: training accuarcy: 0.6065\n",
      "Epoch 4 step 432: training loss: 1345.6933677750435\n",
      "Epoch 4 step 433: training accuarcy: 0.586\n",
      "Epoch 4 step 433: training loss: 1336.9783908812835\n",
      "Epoch 4 step 434: training accuarcy: 0.5965\n",
      "Epoch 4 step 434: training loss: 1332.314468389759\n",
      "Epoch 4 step 435: training accuarcy: 0.6\n",
      "Epoch 4 step 435: training loss: 1347.9689716052785\n",
      "Epoch 4 step 436: training accuarcy: 0.5805\n",
      "Epoch 4 step 436: training loss: 1339.539194760135\n",
      "Epoch 4 step 437: training accuarcy: 0.586\n",
      "Epoch 4 step 437: training loss: 1325.021522458308\n",
      "Epoch 4 step 438: training accuarcy: 0.61\n",
      "Epoch 4 step 438: training loss: 1351.0998011358722\n",
      "Epoch 4 step 439: training accuarcy: 0.5855\n",
      "Epoch 4 step 439: training loss: 1331.2438807438086\n",
      "Epoch 4 step 440: training accuarcy: 0.597\n",
      "Epoch 4 step 440: training loss: 1354.1827075541494\n",
      "Epoch 4 step 441: training accuarcy: 0.5690000000000001\n",
      "Epoch 4 step 441: training loss: 1338.7849998544993\n",
      "Epoch 4 step 442: training accuarcy: 0.5935\n",
      "Epoch 4 step 442: training loss: 1321.2229639674938\n",
      "Epoch 4 step 443: training accuarcy: 0.602\n",
      "Epoch 4 step 443: training loss: 1353.5551136425256\n",
      "Epoch 4 step 444: training accuarcy: 0.579\n",
      "Epoch 4 step 444: training loss: 1347.0483340519634\n",
      "Epoch 4 step 445: training accuarcy: 0.587\n",
      "Epoch 4 step 445: training loss: 1338.0089454815097\n",
      "Epoch 4 step 446: training accuarcy: 0.591\n",
      "Epoch 4 step 446: training loss: 1353.7391203529123\n",
      "Epoch 4 step 447: training accuarcy: 0.5855\n",
      "Epoch 4 step 447: training loss: 1320.6691368679694\n",
      "Epoch 4 step 448: training accuarcy: 0.6085\n",
      "Epoch 4 step 448: training loss: 1349.5237983813913\n",
      "Epoch 4 step 449: training accuarcy: 0.5825\n",
      "Epoch 4 step 449: training loss: 1339.6695270202647\n",
      "Epoch 4 step 450: training accuarcy: 0.5755\n",
      "Epoch 4 step 450: training loss: 1348.5565693955562\n",
      "Epoch 4 step 451: training accuarcy: 0.59\n",
      "Epoch 4 step 451: training loss: 1323.61519747992\n",
      "Epoch 4 step 452: training accuarcy: 0.5945\n",
      "Epoch 4 step 452: training loss: 1331.1471800504378\n",
      "Epoch 4 step 453: training accuarcy: 0.5995\n",
      "Epoch 4 step 453: training loss: 1333.354668714382\n",
      "Epoch 4 step 454: training accuarcy: 0.61\n",
      "Epoch 4 step 454: training loss: 1335.2555981990006\n",
      "Epoch 4 step 455: training accuarcy: 0.595\n",
      "Epoch 4 step 455: training loss: 1345.388608102125\n",
      "Epoch 4 step 456: training accuarcy: 0.5755\n",
      "Epoch 4 step 456: training loss: 1352.8268576550508\n",
      "Epoch 4 step 457: training accuarcy: 0.5665\n",
      "Epoch 4 step 457: training loss: 1341.9587525921074\n",
      "Epoch 4 step 458: training accuarcy: 0.5845\n",
      "Epoch 4 step 458: training loss: 1348.160918343637\n",
      "Epoch 4 step 459: training accuarcy: 0.5715\n",
      "Epoch 4 step 459: training loss: 1336.064231864555\n",
      "Epoch 4 step 460: training accuarcy: 0.5945\n",
      "Epoch 4 step 460: training loss: 1332.9658747154472\n",
      "Epoch 4 step 461: training accuarcy: 0.593\n",
      "Epoch 4 step 461: training loss: 1349.8824437878545\n",
      "Epoch 4 step 462: training accuarcy: 0.579\n",
      "Epoch 4 step 462: training loss: 1344.809322028942\n",
      "Epoch 4 step 463: training accuarcy: 0.5875\n",
      "Epoch 4 step 463: training loss: 1325.2876876770986\n",
      "Epoch 4 step 464: training accuarcy: 0.606\n",
      "Epoch 4 step 464: training loss: 1317.387916242911\n",
      "Epoch 4 step 465: training accuarcy: 0.605\n",
      "Epoch 4 step 465: training loss: 1338.6599183964297\n",
      "Epoch 4 step 466: training accuarcy: 0.5875\n",
      "Epoch 4 step 466: training loss: 1333.4297954061415\n",
      "Epoch 4 step 467: training accuarcy: 0.5985\n",
      "Epoch 4 step 467: training loss: 1333.234762098567\n",
      "Epoch 4 step 468: training accuarcy: 0.5905\n",
      "Epoch 4 step 468: training loss: 1330.4270016984444\n",
      "Epoch 4 step 469: training accuarcy: 0.584\n",
      "Epoch 4 step 469: training loss: 1021.9659126482086\n",
      "Epoch 4 step 470: training accuarcy: 0.6165703275529865\n",
      "Epoch 4: train loss 1335.1135709611597, train accuarcy 0.5869288444519043\n",
      "Epoch 4: valid loss 6200.890561880182, valid accuarcy 0.5745860934257507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [06:48<00:00, 81.52s/it]\n"
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=5,\n",
    "                log_dir=get_log_dir('seq_stackoverflow', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:20:41.889789Z",
     "start_time": "2019-10-07T13:20:41.885791Z"
    }
   },
   "outputs": [],
   "source": [
    "del hrm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PRME FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:20:53.448858Z",
     "start_time": "2019-10-07T13:20:52.636858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchPrmeFM()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_model = TorchPrmeFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "prme_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:20:54.104634Z",
     "start_time": "2019-10-07T13:20:54.101661Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(prme_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:20:54.827370Z",
     "start_time": "2019-10-07T13:20:54.701400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1a6900f7c18>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_learner = FMLearner(prme_model, adam_opt, schedular, db)\n",
    "prme_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:20:55.335971Z",
     "start_time": "2019-10-07T13:20:55.332756Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:28:33.509728Z",
     "start_time": "2019-10-07T13:21:30.798190Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 163625.31334404234\n",
      "Epoch 0 step 1: training accuarcy: 0.493\n",
      "Epoch 0 step 1: training loss: 159053.84386484543\n",
      "Epoch 0 step 2: training accuarcy: 0.496\n",
      "Epoch 0 step 2: training loss: 154421.37391361973\n",
      "Epoch 0 step 3: training accuarcy: 0.51\n",
      "Epoch 0 step 3: training loss: 150078.63620001235\n",
      "Epoch 0 step 4: training accuarcy: 0.5135\n",
      "Epoch 0 step 4: training loss: 144752.72417866022\n",
      "Epoch 0 step 5: training accuarcy: 0.5345\n",
      "Epoch 0 step 5: training loss: 141069.2796469489\n",
      "Epoch 0 step 6: training accuarcy: 0.513\n",
      "Epoch 0 step 6: training loss: 137534.93500742718\n",
      "Epoch 0 step 7: training accuarcy: 0.498\n",
      "Epoch 0 step 7: training loss: 132837.28214766507\n",
      "Epoch 0 step 8: training accuarcy: 0.5165\n",
      "Epoch 0 step 8: training loss: 129455.17493600384\n",
      "Epoch 0 step 9: training accuarcy: 0.5005000000000001\n",
      "Epoch 0 step 9: training loss: 125055.68127879125\n",
      "Epoch 0 step 10: training accuarcy: 0.51\n",
      "Epoch 0 step 10: training loss: 120937.32056976783\n",
      "Epoch 0 step 11: training accuarcy: 0.518\n",
      "Epoch 0 step 11: training loss: 118445.1433851997\n",
      "Epoch 0 step 12: training accuarcy: 0.4875\n",
      "Epoch 0 step 12: training loss: 114259.97557121745\n",
      "Epoch 0 step 13: training accuarcy: 0.5205\n",
      "Epoch 0 step 13: training loss: 111122.53800258145\n",
      "Epoch 0 step 14: training accuarcy: 0.501\n",
      "Epoch 0 step 14: training loss: 107547.67038060787\n",
      "Epoch 0 step 15: training accuarcy: 0.502\n",
      "Epoch 0 step 15: training loss: 104144.04138732383\n",
      "Epoch 0 step 16: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 16: training loss: 101412.78504967631\n",
      "Epoch 0 step 17: training accuarcy: 0.481\n",
      "Epoch 0 step 17: training loss: 98243.41855105179\n",
      "Epoch 0 step 18: training accuarcy: 0.502\n",
      "Epoch 0 step 18: training loss: 95235.46471330994\n",
      "Epoch 0 step 19: training accuarcy: 0.502\n",
      "Epoch 0 step 19: training loss: 91927.24817467778\n",
      "Epoch 0 step 20: training accuarcy: 0.516\n",
      "Epoch 0 step 20: training loss: 89729.4863428643\n",
      "Epoch 0 step 21: training accuarcy: 0.4985\n",
      "Epoch 0 step 21: training loss: 86502.92404674808\n",
      "Epoch 0 step 22: training accuarcy: 0.5115000000000001\n",
      "Epoch 0 step 22: training loss: 84069.58518708724\n",
      "Epoch 0 step 23: training accuarcy: 0.511\n",
      "Epoch 0 step 23: training loss: 81470.5349320419\n",
      "Epoch 0 step 24: training accuarcy: 0.494\n",
      "Epoch 0 step 24: training loss: 79029.01541363174\n",
      "Epoch 0 step 25: training accuarcy: 0.5005000000000001\n",
      "Epoch 0 step 25: training loss: 76677.09068464229\n",
      "Epoch 0 step 26: training accuarcy: 0.5085000000000001\n",
      "Epoch 0 step 26: training loss: 74008.80810951046\n",
      "Epoch 0 step 27: training accuarcy: 0.5225\n",
      "Epoch 0 step 27: training loss: 71850.40426447462\n",
      "Epoch 0 step 28: training accuarcy: 0.509\n",
      "Epoch 0 step 28: training loss: 69537.79606583748\n",
      "Epoch 0 step 29: training accuarcy: 0.4925\n",
      "Epoch 0 step 29: training loss: 67252.50993679454\n",
      "Epoch 0 step 30: training accuarcy: 0.495\n",
      "Epoch 0 step 30: training loss: 65220.9892247753\n",
      "Epoch 0 step 31: training accuarcy: 0.5085000000000001\n",
      "Epoch 0 step 31: training loss: 63224.87371740278\n",
      "Epoch 0 step 32: training accuarcy: 0.491\n",
      "Epoch 0 step 32: training loss: 61068.578500633164\n",
      "Epoch 0 step 33: training accuarcy: 0.489\n",
      "Epoch 0 step 33: training loss: 59234.51774264101\n",
      "Epoch 0 step 34: training accuarcy: 0.501\n",
      "Epoch 0 step 34: training loss: 57529.22293438967\n",
      "Epoch 0 step 35: training accuarcy: 0.493\n",
      "Epoch 0 step 35: training loss: 55383.029641262576\n",
      "Epoch 0 step 36: training accuarcy: 0.5085000000000001\n",
      "Epoch 0 step 36: training loss: 53773.40211590127\n",
      "Epoch 0 step 37: training accuarcy: 0.498\n",
      "Epoch 0 step 37: training loss: 51880.56997368085\n",
      "Epoch 0 step 38: training accuarcy: 0.5175\n",
      "Epoch 0 step 38: training loss: 50417.67514635637\n",
      "Epoch 0 step 39: training accuarcy: 0.5085000000000001\n",
      "Epoch 0 step 39: training loss: 49061.589637599376\n",
      "Epoch 0 step 40: training accuarcy: 0.488\n",
      "Epoch 0 step 40: training loss: 47382.08474269518\n",
      "Epoch 0 step 41: training accuarcy: 0.501\n",
      "Epoch 0 step 41: training loss: 45736.90206194058\n",
      "Epoch 0 step 42: training accuarcy: 0.493\n",
      "Epoch 0 step 42: training loss: 44102.61740581121\n",
      "Epoch 0 step 43: training accuarcy: 0.512\n",
      "Epoch 0 step 43: training loss: 42909.51282586875\n",
      "Epoch 0 step 44: training accuarcy: 0.4855\n",
      "Epoch 0 step 44: training loss: 41415.419812460226\n",
      "Epoch 0 step 45: training accuarcy: 0.519\n",
      "Epoch 0 step 45: training loss: 40204.67285927635\n",
      "Epoch 0 step 46: training accuarcy: 0.507\n",
      "Epoch 0 step 46: training loss: 38989.91166603851\n",
      "Epoch 0 step 47: training accuarcy: 0.4915\n",
      "Epoch 0 step 47: training loss: 37517.622809342094\n",
      "Epoch 0 step 48: training accuarcy: 0.5035000000000001\n",
      "Epoch 0 step 48: training loss: 36377.64010128251\n",
      "Epoch 0 step 49: training accuarcy: 0.5215\n",
      "Epoch 0 step 49: training loss: 35300.038373364136\n",
      "Epoch 0 step 50: training accuarcy: 0.5\n",
      "Epoch 0 step 50: training loss: 34276.11906163048\n",
      "Epoch 0 step 51: training accuarcy: 0.4915\n",
      "Epoch 0 step 51: training loss: 32884.96504020664\n",
      "Epoch 0 step 52: training accuarcy: 0.528\n",
      "Epoch 0 step 52: training loss: 31800.193164009692\n",
      "Epoch 0 step 53: training accuarcy: 0.5215\n",
      "Epoch 0 step 53: training loss: 30748.43192837848\n",
      "Epoch 0 step 54: training accuarcy: 0.5295\n",
      "Epoch 0 step 54: training loss: 29957.47611691031\n",
      "Epoch 0 step 55: training accuarcy: 0.4985\n",
      "Epoch 0 step 55: training loss: 28835.804242637958\n",
      "Epoch 0 step 56: training accuarcy: 0.528\n",
      "Epoch 0 step 56: training loss: 28038.816077443236\n",
      "Epoch 0 step 57: training accuarcy: 0.5185\n",
      "Epoch 0 step 57: training loss: 27279.35190143065\n",
      "Epoch 0 step 58: training accuarcy: 0.4895\n",
      "Epoch 0 step 58: training loss: 26217.939239129653\n",
      "Epoch 0 step 59: training accuarcy: 0.5265\n",
      "Epoch 0 step 59: training loss: 25348.49268718501\n",
      "Epoch 0 step 60: training accuarcy: 0.514\n",
      "Epoch 0 step 60: training loss: 24754.84399726019\n",
      "Epoch 0 step 61: training accuarcy: 0.492\n",
      "Epoch 0 step 61: training loss: 23717.553136488485\n",
      "Epoch 0 step 62: training accuarcy: 0.5135\n",
      "Epoch 0 step 62: training loss: 23066.165490303094\n",
      "Epoch 0 step 63: training accuarcy: 0.528\n",
      "Epoch 0 step 63: training loss: 22454.745867443882\n",
      "Epoch 0 step 64: training accuarcy: 0.494\n",
      "Epoch 0 step 64: training loss: 21595.031552354725\n",
      "Epoch 0 step 65: training accuarcy: 0.518\n",
      "Epoch 0 step 65: training loss: 21084.522897960902\n",
      "Epoch 0 step 66: training accuarcy: 0.488\n",
      "Epoch 0 step 66: training loss: 20264.60982865964\n",
      "Epoch 0 step 67: training accuarcy: 0.524\n",
      "Epoch 0 step 67: training loss: 19708.307197456983\n",
      "Epoch 0 step 68: training accuarcy: 0.5005000000000001\n",
      "Epoch 0 step 68: training loss: 19054.897651275867\n",
      "Epoch 0 step 69: training accuarcy: 0.505\n",
      "Epoch 0 step 69: training loss: 18329.548116942216\n",
      "Epoch 0 step 70: training accuarcy: 0.5165\n",
      "Epoch 0 step 70: training loss: 17976.5390933531\n",
      "Epoch 0 step 71: training accuarcy: 0.4925\n",
      "Epoch 0 step 71: training loss: 17360.66366402102\n",
      "Epoch 0 step 72: training accuarcy: 0.4915\n",
      "Epoch 0 step 72: training loss: 16733.568653113416\n",
      "Epoch 0 step 73: training accuarcy: 0.503\n",
      "Epoch 0 step 73: training loss: 16237.005123668936\n",
      "Epoch 0 step 74: training accuarcy: 0.495\n",
      "Epoch 0 step 74: training loss: 15715.652506094826\n",
      "Epoch 0 step 75: training accuarcy: 0.499\n",
      "Epoch 0 step 75: training loss: 15201.298504779945\n",
      "Epoch 0 step 76: training accuarcy: 0.522\n",
      "Epoch 0 step 76: training loss: 14688.325270850986\n",
      "Epoch 0 step 77: training accuarcy: 0.52\n",
      "Epoch 0 step 77: training loss: 14183.00537480407\n",
      "Epoch 0 step 78: training accuarcy: 0.532\n",
      "Epoch 0 step 78: training loss: 13965.250845895138\n",
      "Epoch 0 step 79: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 79: training loss: 13423.220989313333\n",
      "Epoch 0 step 80: training accuarcy: 0.508\n",
      "Epoch 0 step 80: training loss: 13060.318462303565\n",
      "Epoch 0 step 81: training accuarcy: 0.4885\n",
      "Epoch 0 step 81: training loss: 12544.108003484205\n",
      "Epoch 0 step 82: training accuarcy: 0.517\n",
      "Epoch 0 step 82: training loss: 12174.617585956652\n",
      "Epoch 0 step 83: training accuarcy: 0.5255\n",
      "Epoch 0 step 83: training loss: 11845.231540017587\n",
      "Epoch 0 step 84: training accuarcy: 0.5085000000000001\n",
      "Epoch 0 step 84: training loss: 11545.192118926403\n",
      "Epoch 0 step 85: training accuarcy: 0.493\n",
      "Epoch 0 step 85: training loss: 11181.556175782225\n",
      "Epoch 0 step 86: training accuarcy: 0.5085000000000001\n",
      "Epoch 0 step 86: training loss: 10831.215331189536\n",
      "Epoch 0 step 87: training accuarcy: 0.518\n",
      "Epoch 0 step 87: training loss: 10549.771750803055\n",
      "Epoch 0 step 88: training accuarcy: 0.5075000000000001\n",
      "Epoch 0 step 88: training loss: 10173.740597258213\n",
      "Epoch 0 step 89: training accuarcy: 0.527\n",
      "Epoch 0 step 89: training loss: 9874.21282763942\n",
      "Epoch 0 step 90: training accuarcy: 0.5145\n",
      "Epoch 0 step 90: training loss: 9600.179958388757\n",
      "Epoch 0 step 91: training accuarcy: 0.5125\n",
      "Epoch 0 step 91: training loss: 9313.696415615494\n",
      "Epoch 0 step 92: training accuarcy: 0.52\n",
      "Epoch 0 step 92: training loss: 8999.313937724748\n",
      "Epoch 0 step 93: training accuarcy: 0.5045000000000001\n",
      "Epoch 0 step 93: training loss: 8375.608270865854\n",
      "Epoch 0 step 94: training accuarcy: 0.48554913294797686\n",
      "Epoch 0: train loss 53967.97856145471, train accuarcy 0.5070427656173706\n",
      "Epoch 0: valid loss 15393.16038438102, valid accuarcy 0.5123487114906311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▌                                                                                                                                          | 1/5 [01:22<05:31, 82.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 94: training loss: 8490.246713243258\n",
      "Epoch 1 step 95: training accuarcy: 0.529\n",
      "Epoch 1 step 95: training loss: 8182.121943406677\n",
      "Epoch 1 step 96: training accuarcy: 0.538\n",
      "Epoch 1 step 96: training loss: 7983.988104257445\n",
      "Epoch 1 step 97: training accuarcy: 0.531\n",
      "Epoch 1 step 97: training loss: 7751.978461277806\n",
      "Epoch 1 step 98: training accuarcy: 0.5295\n",
      "Epoch 1 step 98: training loss: 7552.896497317663\n",
      "Epoch 1 step 99: training accuarcy: 0.5305\n",
      "Epoch 1 step 99: training loss: 7368.974533635669\n",
      "Epoch 1 step 100: training accuarcy: 0.513\n",
      "Epoch 1 step 100: training loss: 7083.361314707041\n",
      "Epoch 1 step 101: training accuarcy: 0.5255\n",
      "Epoch 1 step 101: training loss: 6914.690266067468\n",
      "Epoch 1 step 102: training accuarcy: 0.523\n",
      "Epoch 1 step 102: training loss: 6784.328299345862\n",
      "Epoch 1 step 103: training accuarcy: 0.513\n",
      "Epoch 1 step 103: training loss: 6534.229203209648\n",
      "Epoch 1 step 104: training accuarcy: 0.5435\n",
      "Epoch 1 step 104: training loss: 6336.477428611949\n",
      "Epoch 1 step 105: training accuarcy: 0.5385\n",
      "Epoch 1 step 105: training loss: 6209.410017370496\n",
      "Epoch 1 step 106: training accuarcy: 0.54\n",
      "Epoch 1 step 106: training loss: 6044.7815132557835\n",
      "Epoch 1 step 107: training accuarcy: 0.5385\n",
      "Epoch 1 step 107: training loss: 5922.576672487709\n",
      "Epoch 1 step 108: training accuarcy: 0.526\n",
      "Epoch 1 step 108: training loss: 5671.418499757076\n",
      "Epoch 1 step 109: training accuarcy: 0.555\n",
      "Epoch 1 step 109: training loss: 5578.410374626006\n",
      "Epoch 1 step 110: training accuarcy: 0.536\n",
      "Epoch 1 step 110: training loss: 5423.879295101349\n",
      "Epoch 1 step 111: training accuarcy: 0.5395\n",
      "Epoch 1 step 111: training loss: 5268.862319179326\n",
      "Epoch 1 step 112: training accuarcy: 0.553\n",
      "Epoch 1 step 112: training loss: 5138.043297803995\n",
      "Epoch 1 step 113: training accuarcy: 0.5465\n",
      "Epoch 1 step 113: training loss: 5092.859423339028\n",
      "Epoch 1 step 114: training accuarcy: 0.532\n",
      "Epoch 1 step 114: training loss: 4918.494633287746\n",
      "Epoch 1 step 115: training accuarcy: 0.543\n",
      "Epoch 1 step 115: training loss: 4820.555917560473\n",
      "Epoch 1 step 116: training accuarcy: 0.515\n",
      "Epoch 1 step 116: training loss: 4710.726952527502\n",
      "Epoch 1 step 117: training accuarcy: 0.5365\n",
      "Epoch 1 step 117: training loss: 4618.773883069032\n",
      "Epoch 1 step 118: training accuarcy: 0.5235\n",
      "Epoch 1 step 118: training loss: 4505.848364601533\n",
      "Epoch 1 step 119: training accuarcy: 0.5435\n",
      "Epoch 1 step 119: training loss: 4393.932054824241\n",
      "Epoch 1 step 120: training accuarcy: 0.5435\n",
      "Epoch 1 step 120: training loss: 4234.509698092526\n",
      "Epoch 1 step 121: training accuarcy: 0.5555\n",
      "Epoch 1 step 121: training loss: 4194.518981893828\n",
      "Epoch 1 step 122: training accuarcy: 0.548\n",
      "Epoch 1 step 122: training loss: 4077.3789554969035\n",
      "Epoch 1 step 123: training accuarcy: 0.5465\n",
      "Epoch 1 step 123: training loss: 3955.650659882927\n",
      "Epoch 1 step 124: training accuarcy: 0.559\n",
      "Epoch 1 step 124: training loss: 3887.1782092683416\n",
      "Epoch 1 step 125: training accuarcy: 0.5605\n",
      "Epoch 1 step 125: training loss: 3833.6967986206146\n",
      "Epoch 1 step 126: training accuarcy: 0.5415\n",
      "Epoch 1 step 126: training loss: 3723.605593141714\n",
      "Epoch 1 step 127: training accuarcy: 0.5700000000000001\n",
      "Epoch 1 step 127: training loss: 3671.737091723653\n",
      "Epoch 1 step 128: training accuarcy: 0.5405\n",
      "Epoch 1 step 128: training loss: 3608.841000804893\n",
      "Epoch 1 step 129: training accuarcy: 0.5535\n",
      "Epoch 1 step 129: training loss: 3542.4211792945816\n",
      "Epoch 1 step 130: training accuarcy: 0.56\n",
      "Epoch 1 step 130: training loss: 3431.1521276934322\n",
      "Epoch 1 step 131: training accuarcy: 0.552\n",
      "Epoch 1 step 131: training loss: 3403.143013417017\n",
      "Epoch 1 step 132: training accuarcy: 0.5395\n",
      "Epoch 1 step 132: training loss: 3383.4398525389543\n",
      "Epoch 1 step 133: training accuarcy: 0.536\n",
      "Epoch 1 step 133: training loss: 3306.961844756008\n",
      "Epoch 1 step 134: training accuarcy: 0.539\n",
      "Epoch 1 step 134: training loss: 3219.798881813469\n",
      "Epoch 1 step 135: training accuarcy: 0.5435\n",
      "Epoch 1 step 135: training loss: 3196.7770368098318\n",
      "Epoch 1 step 136: training accuarcy: 0.5325\n",
      "Epoch 1 step 136: training loss: 3105.337575496186\n",
      "Epoch 1 step 137: training accuarcy: 0.5415\n",
      "Epoch 1 step 137: training loss: 3045.659118377638\n",
      "Epoch 1 step 138: training accuarcy: 0.5565\n",
      "Epoch 1 step 138: training loss: 3017.909656106089\n",
      "Epoch 1 step 139: training accuarcy: 0.54\n",
      "Epoch 1 step 139: training loss: 2907.4282703602694\n",
      "Epoch 1 step 140: training accuarcy: 0.577\n",
      "Epoch 1 step 140: training loss: 2880.6186612228494\n",
      "Epoch 1 step 141: training accuarcy: 0.558\n",
      "Epoch 1 step 141: training loss: 2851.7446298482882\n",
      "Epoch 1 step 142: training accuarcy: 0.549\n",
      "Epoch 1 step 142: training loss: 2790.6103838353492\n",
      "Epoch 1 step 143: training accuarcy: 0.5635\n",
      "Epoch 1 step 143: training loss: 2757.382109827463\n",
      "Epoch 1 step 144: training accuarcy: 0.55\n",
      "Epoch 1 step 144: training loss: 2739.895518899885\n",
      "Epoch 1 step 145: training accuarcy: 0.5465\n",
      "Epoch 1 step 145: training loss: 2687.3854044685745\n",
      "Epoch 1 step 146: training accuarcy: 0.5475\n",
      "Epoch 1 step 146: training loss: 2679.177959757673\n",
      "Epoch 1 step 147: training accuarcy: 0.546\n",
      "Epoch 1 step 147: training loss: 2604.739916390212\n",
      "Epoch 1 step 148: training accuarcy: 0.5705\n",
      "Epoch 1 step 148: training loss: 2582.6515064518117\n",
      "Epoch 1 step 149: training accuarcy: 0.552\n",
      "Epoch 1 step 149: training loss: 2548.857330888446\n",
      "Epoch 1 step 150: training accuarcy: 0.5535\n",
      "Epoch 1 step 150: training loss: 2506.629778493107\n",
      "Epoch 1 step 151: training accuarcy: 0.537\n",
      "Epoch 1 step 151: training loss: 2445.570874494088\n",
      "Epoch 1 step 152: training accuarcy: 0.5660000000000001\n",
      "Epoch 1 step 152: training loss: 2423.803728671946\n",
      "Epoch 1 step 153: training accuarcy: 0.56\n",
      "Epoch 1 step 153: training loss: 2392.1732802251227\n",
      "Epoch 1 step 154: training accuarcy: 0.5525\n",
      "Epoch 1 step 154: training loss: 2382.9319059741492\n",
      "Epoch 1 step 155: training accuarcy: 0.5505\n",
      "Epoch 1 step 155: training loss: 2355.2236680454157\n",
      "Epoch 1 step 156: training accuarcy: 0.555\n",
      "Epoch 1 step 156: training loss: 2311.3442398333655\n",
      "Epoch 1 step 157: training accuarcy: 0.545\n",
      "Epoch 1 step 157: training loss: 2279.396538414341\n",
      "Epoch 1 step 158: training accuarcy: 0.5615\n",
      "Epoch 1 step 158: training loss: 2259.2688102787915\n",
      "Epoch 1 step 159: training accuarcy: 0.531\n",
      "Epoch 1 step 159: training loss: 2253.021705473935\n",
      "Epoch 1 step 160: training accuarcy: 0.5375\n",
      "Epoch 1 step 160: training loss: 2196.064301088345\n",
      "Epoch 1 step 161: training accuarcy: 0.5745\n",
      "Epoch 1 step 161: training loss: 2197.425903531477\n",
      "Epoch 1 step 162: training accuarcy: 0.559\n",
      "Epoch 1 step 162: training loss: 2151.0799378259517\n",
      "Epoch 1 step 163: training accuarcy: 0.5615\n",
      "Epoch 1 step 163: training loss: 2136.17917238252\n",
      "Epoch 1 step 164: training accuarcy: 0.5670000000000001\n",
      "Epoch 1 step 164: training loss: 2097.966984405862\n",
      "Epoch 1 step 165: training accuarcy: 0.5655\n",
      "Epoch 1 step 165: training loss: 2078.88161003133\n",
      "Epoch 1 step 166: training accuarcy: 0.5725\n",
      "Epoch 1 step 166: training loss: 2104.190962054757\n",
      "Epoch 1 step 167: training accuarcy: 0.542\n",
      "Epoch 1 step 167: training loss: 2051.2982196186986\n",
      "Epoch 1 step 168: training accuarcy: 0.55\n",
      "Epoch 1 step 168: training loss: 2056.964131126633\n",
      "Epoch 1 step 169: training accuarcy: 0.545\n",
      "Epoch 1 step 169: training loss: 2003.7643656507453\n",
      "Epoch 1 step 170: training accuarcy: 0.554\n",
      "Epoch 1 step 170: training loss: 1991.4470055599402\n",
      "Epoch 1 step 171: training accuarcy: 0.555\n",
      "Epoch 1 step 171: training loss: 1990.2160396385173\n",
      "Epoch 1 step 172: training accuarcy: 0.5690000000000001\n",
      "Epoch 1 step 172: training loss: 1979.8403074603189\n",
      "Epoch 1 step 173: training accuarcy: 0.5505\n",
      "Epoch 1 step 173: training loss: 1920.9900994610023\n",
      "Epoch 1 step 174: training accuarcy: 0.5775\n",
      "Epoch 1 step 174: training loss: 1951.4327961634435\n",
      "Epoch 1 step 175: training accuarcy: 0.561\n",
      "Epoch 1 step 175: training loss: 1937.9945841698614\n",
      "Epoch 1 step 176: training accuarcy: 0.5445\n",
      "Epoch 1 step 176: training loss: 1902.9706515709927\n",
      "Epoch 1 step 177: training accuarcy: 0.578\n",
      "Epoch 1 step 177: training loss: 1896.2537390800655\n",
      "Epoch 1 step 178: training accuarcy: 0.5375\n",
      "Epoch 1 step 178: training loss: 1876.7924201760843\n",
      "Epoch 1 step 179: training accuarcy: 0.577\n",
      "Epoch 1 step 179: training loss: 1836.496130505975\n",
      "Epoch 1 step 180: training accuarcy: 0.5835\n",
      "Epoch 1 step 180: training loss: 1841.3176085301634\n",
      "Epoch 1 step 181: training accuarcy: 0.5700000000000001\n",
      "Epoch 1 step 181: training loss: 1847.3359765162306\n",
      "Epoch 1 step 182: training accuarcy: 0.557\n",
      "Epoch 1 step 182: training loss: 1834.4433706721375\n",
      "Epoch 1 step 183: training accuarcy: 0.5565\n",
      "Epoch 1 step 183: training loss: 1814.5586272357173\n",
      "Epoch 1 step 184: training accuarcy: 0.5640000000000001\n",
      "Epoch 1 step 184: training loss: 1810.5975181965907\n",
      "Epoch 1 step 185: training accuarcy: 0.562\n",
      "Epoch 1 step 185: training loss: 1799.4479650345675\n",
      "Epoch 1 step 186: training accuarcy: 0.5595\n",
      "Epoch 1 step 186: training loss: 1800.1364813201055\n",
      "Epoch 1 step 187: training accuarcy: 0.5545\n",
      "Epoch 1 step 187: training loss: 1495.0792963191266\n",
      "Epoch 1 step 188: training accuarcy: 0.552344251766217\n",
      "Epoch 1: train loss 3567.900060492369, train accuarcy 0.5447326898574829\n",
      "Epoch 1: valid loss 6825.685084626494, valid accuarcy 0.5512539148330688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████████████████████████████████████████▏                                                                                                       | 2/5 [02:49<04:11, 83.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 188: training loss: 1735.253479598523\n",
      "Epoch 2 step 189: training accuarcy: 0.5845\n",
      "Epoch 2 step 189: training loss: 1749.5899712431624\n",
      "Epoch 2 step 190: training accuarcy: 0.5695\n",
      "Epoch 2 step 190: training loss: 1714.4795610396247\n",
      "Epoch 2 step 191: training accuarcy: 0.5835\n",
      "Epoch 2 step 191: training loss: 1712.1047253783054\n",
      "Epoch 2 step 192: training accuarcy: 0.5905\n",
      "Epoch 2 step 192: training loss: 1712.6854473102721\n",
      "Epoch 2 step 193: training accuarcy: 0.5825\n",
      "Epoch 2 step 193: training loss: 1704.7941100443982\n",
      "Epoch 2 step 194: training accuarcy: 0.5755\n",
      "Epoch 2 step 194: training loss: 1692.9159857701284\n",
      "Epoch 2 step 195: training accuarcy: 0.5750000000000001\n",
      "Epoch 2 step 195: training loss: 1684.6675285120075\n",
      "Epoch 2 step 196: training accuarcy: 0.5745\n",
      "Epoch 2 step 196: training loss: 1684.3824425898445\n",
      "Epoch 2 step 197: training accuarcy: 0.5685\n",
      "Epoch 2 step 197: training loss: 1681.1259530440186\n",
      "Epoch 2 step 198: training accuarcy: 0.5755\n",
      "Epoch 2 step 198: training loss: 1672.425891922363\n",
      "Epoch 2 step 199: training accuarcy: 0.5645\n",
      "Epoch 2 step 199: training loss: 1665.5230475448998\n",
      "Epoch 2 step 200: training accuarcy: 0.5735\n",
      "Epoch 2 step 200: training loss: 1655.6729720199053\n",
      "Epoch 2 step 201: training accuarcy: 0.577\n",
      "Epoch 2 step 201: training loss: 1648.8975314350084\n",
      "Epoch 2 step 202: training accuarcy: 0.5795\n",
      "Epoch 2 step 202: training loss: 1636.8449948988525\n",
      "Epoch 2 step 203: training accuarcy: 0.5690000000000001\n",
      "Epoch 2 step 203: training loss: 1631.6389841462153\n",
      "Epoch 2 step 204: training accuarcy: 0.5815\n",
      "Epoch 2 step 204: training loss: 1593.901619651116\n",
      "Epoch 2 step 205: training accuarcy: 0.597\n",
      "Epoch 2 step 205: training loss: 1622.4357608301625\n",
      "Epoch 2 step 206: training accuarcy: 0.5655\n",
      "Epoch 2 step 206: training loss: 1611.9592873013128\n",
      "Epoch 2 step 207: training accuarcy: 0.579\n",
      "Epoch 2 step 207: training loss: 1605.3414227372782\n",
      "Epoch 2 step 208: training accuarcy: 0.586\n",
      "Epoch 2 step 208: training loss: 1616.379183321848\n",
      "Epoch 2 step 209: training accuarcy: 0.5760000000000001\n",
      "Epoch 2 step 209: training loss: 1587.457047842982\n",
      "Epoch 2 step 210: training accuarcy: 0.5730000000000001\n",
      "Epoch 2 step 210: training loss: 1553.877068414485\n",
      "Epoch 2 step 211: training accuarcy: 0.608\n",
      "Epoch 2 step 211: training loss: 1593.816891024616\n",
      "Epoch 2 step 212: training accuarcy: 0.5725\n",
      "Epoch 2 step 212: training loss: 1574.6871837800122\n",
      "Epoch 2 step 213: training accuarcy: 0.5755\n",
      "Epoch 2 step 213: training loss: 1563.1810474466677\n",
      "Epoch 2 step 214: training accuarcy: 0.581\n",
      "Epoch 2 step 214: training loss: 1570.183827413879\n",
      "Epoch 2 step 215: training accuarcy: 0.5690000000000001\n",
      "Epoch 2 step 215: training loss: 1568.8513116088848\n",
      "Epoch 2 step 216: training accuarcy: 0.583\n",
      "Epoch 2 step 216: training loss: 1553.9285382249411\n",
      "Epoch 2 step 217: training accuarcy: 0.5785\n",
      "Epoch 2 step 217: training loss: 1548.4042799430304\n",
      "Epoch 2 step 218: training accuarcy: 0.587\n",
      "Epoch 2 step 218: training loss: 1554.5226120717718\n",
      "Epoch 2 step 219: training accuarcy: 0.5710000000000001\n",
      "Epoch 2 step 219: training loss: 1557.6237247890215\n",
      "Epoch 2 step 220: training accuarcy: 0.5655\n",
      "Epoch 2 step 220: training loss: 1539.3182138373809\n",
      "Epoch 2 step 221: training accuarcy: 0.5815\n",
      "Epoch 2 step 221: training loss: 1557.7538865447882\n",
      "Epoch 2 step 222: training accuarcy: 0.5735\n",
      "Epoch 2 step 222: training loss: 1542.362881449286\n",
      "Epoch 2 step 223: training accuarcy: 0.578\n",
      "Epoch 2 step 223: training loss: 1520.6149161975545\n",
      "Epoch 2 step 224: training accuarcy: 0.593\n",
      "Epoch 2 step 224: training loss: 1528.4822288878988\n",
      "Epoch 2 step 225: training accuarcy: 0.5735\n",
      "Epoch 2 step 225: training loss: 1534.860975951704\n",
      "Epoch 2 step 226: training accuarcy: 0.5685\n",
      "Epoch 2 step 226: training loss: 1517.6695217868184\n",
      "Epoch 2 step 227: training accuarcy: 0.602\n",
      "Epoch 2 step 227: training loss: 1513.6199448676302\n",
      "Epoch 2 step 228: training accuarcy: 0.591\n",
      "Epoch 2 step 228: training loss: 1493.8678359699297\n",
      "Epoch 2 step 229: training accuarcy: 0.5895\n",
      "Epoch 2 step 229: training loss: 1493.542994393046\n",
      "Epoch 2 step 230: training accuarcy: 0.5855\n",
      "Epoch 2 step 230: training loss: 1490.093042384778\n",
      "Epoch 2 step 231: training accuarcy: 0.5945\n",
      "Epoch 2 step 231: training loss: 1483.7158165955457\n",
      "Epoch 2 step 232: training accuarcy: 0.599\n",
      "Epoch 2 step 232: training loss: 1509.4704535467656\n",
      "Epoch 2 step 233: training accuarcy: 0.5750000000000001\n",
      "Epoch 2 step 233: training loss: 1495.7171018317717\n",
      "Epoch 2 step 234: training accuarcy: 0.5750000000000001\n",
      "Epoch 2 step 234: training loss: 1512.1980876012021\n",
      "Epoch 2 step 235: training accuarcy: 0.5670000000000001\n",
      "Epoch 2 step 235: training loss: 1481.2245686943447\n",
      "Epoch 2 step 236: training accuarcy: 0.5915\n",
      "Epoch 2 step 236: training loss: 1484.266778741845\n",
      "Epoch 2 step 237: training accuarcy: 0.5875\n",
      "Epoch 2 step 237: training loss: 1472.633793115011\n",
      "Epoch 2 step 238: training accuarcy: 0.5845\n",
      "Epoch 2 step 238: training loss: 1477.806089288546\n",
      "Epoch 2 step 239: training accuarcy: 0.5735\n",
      "Epoch 2 step 239: training loss: 1468.003165499549\n",
      "Epoch 2 step 240: training accuarcy: 0.5760000000000001\n",
      "Epoch 2 step 240: training loss: 1479.8798232991933\n",
      "Epoch 2 step 241: training accuarcy: 0.5750000000000001\n",
      "Epoch 2 step 241: training loss: 1481.9348675755034\n",
      "Epoch 2 step 242: training accuarcy: 0.5750000000000001\n",
      "Epoch 2 step 242: training loss: 1472.1683215032706\n",
      "Epoch 2 step 243: training accuarcy: 0.5855\n",
      "Epoch 2 step 243: training loss: 1478.8986163238908\n",
      "Epoch 2 step 244: training accuarcy: 0.5700000000000001\n",
      "Epoch 2 step 244: training loss: 1461.0382890241249\n",
      "Epoch 2 step 245: training accuarcy: 0.5855\n",
      "Epoch 2 step 245: training loss: 1461.6539873138954\n",
      "Epoch 2 step 246: training accuarcy: 0.592\n",
      "Epoch 2 step 246: training loss: 1469.0316212687799\n",
      "Epoch 2 step 247: training accuarcy: 0.585\n",
      "Epoch 2 step 247: training loss: 1459.8613833319446\n",
      "Epoch 2 step 248: training accuarcy: 0.5865\n",
      "Epoch 2 step 248: training loss: 1454.3840815474239\n",
      "Epoch 2 step 249: training accuarcy: 0.5865\n",
      "Epoch 2 step 249: training loss: 1456.282774582075\n",
      "Epoch 2 step 250: training accuarcy: 0.5715\n",
      "Epoch 2 step 250: training loss: 1465.926593325191\n",
      "Epoch 2 step 251: training accuarcy: 0.5725\n",
      "Epoch 2 step 251: training loss: 1448.262066592582\n",
      "Epoch 2 step 252: training accuarcy: 0.5855\n",
      "Epoch 2 step 252: training loss: 1449.1507747203261\n",
      "Epoch 2 step 253: training accuarcy: 0.5945\n",
      "Epoch 2 step 253: training loss: 1434.4801044277958\n",
      "Epoch 2 step 254: training accuarcy: 0.6\n",
      "Epoch 2 step 254: training loss: 1428.4195275919765\n",
      "Epoch 2 step 255: training accuarcy: 0.589\n",
      "Epoch 2 step 255: training loss: 1457.8995994756458\n",
      "Epoch 2 step 256: training accuarcy: 0.5760000000000001\n",
      "Epoch 2 step 256: training loss: 1453.5804104714825\n",
      "Epoch 2 step 257: training accuarcy: 0.5705\n",
      "Epoch 2 step 257: training loss: 1432.4076391036542\n",
      "Epoch 2 step 258: training accuarcy: 0.582\n",
      "Epoch 2 step 258: training loss: 1440.24515301661\n",
      "Epoch 2 step 259: training accuarcy: 0.5785\n",
      "Epoch 2 step 259: training loss: 1428.517887704676\n",
      "Epoch 2 step 260: training accuarcy: 0.588\n",
      "Epoch 2 step 260: training loss: 1423.2193830221288\n",
      "Epoch 2 step 261: training accuarcy: 0.5905\n",
      "Epoch 2 step 261: training loss: 1424.0107132989667\n",
      "Epoch 2 step 262: training accuarcy: 0.5925\n",
      "Epoch 2 step 262: training loss: 1425.0089067893575\n",
      "Epoch 2 step 263: training accuarcy: 0.584\n",
      "Epoch 2 step 263: training loss: 1444.6952008535259\n",
      "Epoch 2 step 264: training accuarcy: 0.5745\n",
      "Epoch 2 step 264: training loss: 1409.2191002486184\n",
      "Epoch 2 step 265: training accuarcy: 0.593\n",
      "Epoch 2 step 265: training loss: 1413.062554337438\n",
      "Epoch 2 step 266: training accuarcy: 0.584\n",
      "Epoch 2 step 266: training loss: 1433.5812168402529\n",
      "Epoch 2 step 267: training accuarcy: 0.5805\n",
      "Epoch 2 step 267: training loss: 1429.158848434757\n",
      "Epoch 2 step 268: training accuarcy: 0.586\n",
      "Epoch 2 step 268: training loss: 1411.1803085069914\n",
      "Epoch 2 step 269: training accuarcy: 0.592\n",
      "Epoch 2 step 269: training loss: 1424.775211735122\n",
      "Epoch 2 step 270: training accuarcy: 0.584\n",
      "Epoch 2 step 270: training loss: 1436.5107850738793\n",
      "Epoch 2 step 271: training accuarcy: 0.5725\n",
      "Epoch 2 step 271: training loss: 1420.2444284517608\n",
      "Epoch 2 step 272: training accuarcy: 0.5655\n",
      "Epoch 2 step 272: training loss: 1424.9717347914177\n",
      "Epoch 2 step 273: training accuarcy: 0.5740000000000001\n",
      "Epoch 2 step 273: training loss: 1418.2383987374897\n",
      "Epoch 2 step 274: training accuarcy: 0.59\n",
      "Epoch 2 step 274: training loss: 1415.011714960978\n",
      "Epoch 2 step 275: training accuarcy: 0.577\n",
      "Epoch 2 step 275: training loss: 1424.3693306813323\n",
      "Epoch 2 step 276: training accuarcy: 0.577\n",
      "Epoch 2 step 276: training loss: 1405.9557807215574\n",
      "Epoch 2 step 277: training accuarcy: 0.5925\n",
      "Epoch 2 step 277: training loss: 1404.9133466547044\n",
      "Epoch 2 step 278: training accuarcy: 0.594\n",
      "Epoch 2 step 278: training loss: 1427.4993347621196\n",
      "Epoch 2 step 279: training accuarcy: 0.5675\n",
      "Epoch 2 step 279: training loss: 1403.6066012468725\n",
      "Epoch 2 step 280: training accuarcy: 0.5855\n",
      "Epoch 2 step 280: training loss: 1413.0963890718688\n",
      "Epoch 2 step 281: training accuarcy: 0.5855\n",
      "Epoch 2 step 281: training loss: 1124.3716893585113\n",
      "Epoch 2 step 282: training accuarcy: 0.5863840719332049\n",
      "Epoch 2: train loss 1515.398938626219, train accuarcy 0.5773524641990662\n",
      "Epoch 2: valid loss 6330.77911278321, valid accuarcy 0.5693140029907227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                     | 3/5 [04:12<02:47, 83.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 282: training loss: 1409.6966998501161\n",
      "Epoch 3 step 283: training accuarcy: 0.587\n",
      "Epoch 3 step 283: training loss: 1411.8633999444946\n",
      "Epoch 3 step 284: training accuarcy: 0.5865\n",
      "Epoch 3 step 284: training loss: 1381.4901869120238\n",
      "Epoch 3 step 285: training accuarcy: 0.612\n",
      "Epoch 3 step 285: training loss: 1388.13925670535\n",
      "Epoch 3 step 286: training accuarcy: 0.5985\n",
      "Epoch 3 step 286: training loss: 1382.698158967659\n",
      "Epoch 3 step 287: training accuarcy: 0.602\n",
      "Epoch 3 step 287: training loss: 1389.9223615245196\n",
      "Epoch 3 step 288: training accuarcy: 0.603\n",
      "Epoch 3 step 288: training loss: 1383.2993944610123\n",
      "Epoch 3 step 289: training accuarcy: 0.6075\n",
      "Epoch 3 step 289: training loss: 1398.890204059772\n",
      "Epoch 3 step 290: training accuarcy: 0.5875\n",
      "Epoch 3 step 290: training loss: 1390.7510149107393\n",
      "Epoch 3 step 291: training accuarcy: 0.595\n",
      "Epoch 3 step 291: training loss: 1387.5828021902194\n",
      "Epoch 3 step 292: training accuarcy: 0.59\n",
      "Epoch 3 step 292: training loss: 1398.1483761391726\n",
      "Epoch 3 step 293: training accuarcy: 0.598\n",
      "Epoch 3 step 293: training loss: 1386.7004181451514\n",
      "Epoch 3 step 294: training accuarcy: 0.584\n",
      "Epoch 3 step 294: training loss: 1386.2844099707486\n",
      "Epoch 3 step 295: training accuarcy: 0.598\n",
      "Epoch 3 step 295: training loss: 1389.8305527530622\n",
      "Epoch 3 step 296: training accuarcy: 0.5805\n",
      "Epoch 3 step 296: training loss: 1395.5951896236288\n",
      "Epoch 3 step 297: training accuarcy: 0.589\n",
      "Epoch 3 step 297: training loss: 1387.8862510990366\n",
      "Epoch 3 step 298: training accuarcy: 0.582\n",
      "Epoch 3 step 298: training loss: 1380.6754258614376\n",
      "Epoch 3 step 299: training accuarcy: 0.5985\n",
      "Epoch 3 step 299: training loss: 1381.1859576874049\n",
      "Epoch 3 step 300: training accuarcy: 0.6145\n",
      "Epoch 3 step 300: training loss: 1378.7174259695944\n",
      "Epoch 3 step 301: training accuarcy: 0.592\n",
      "Epoch 3 step 301: training loss: 1397.0015628469707\n",
      "Epoch 3 step 302: training accuarcy: 0.577\n",
      "Epoch 3 step 302: training loss: 1397.2013831147585\n",
      "Epoch 3 step 303: training accuarcy: 0.577\n",
      "Epoch 3 step 303: training loss: 1375.7631563397897\n",
      "Epoch 3 step 304: training accuarcy: 0.59\n",
      "Epoch 3 step 304: training loss: 1370.7586649789673\n",
      "Epoch 3 step 305: training accuarcy: 0.591\n",
      "Epoch 3 step 305: training loss: 1389.2812672347081\n",
      "Epoch 3 step 306: training accuarcy: 0.5885\n",
      "Epoch 3 step 306: training loss: 1379.4148214183078\n",
      "Epoch 3 step 307: training accuarcy: 0.5925\n",
      "Epoch 3 step 307: training loss: 1380.4332714205298\n",
      "Epoch 3 step 308: training accuarcy: 0.5865\n",
      "Epoch 3 step 308: training loss: 1387.2080201313836\n",
      "Epoch 3 step 309: training accuarcy: 0.58\n",
      "Epoch 3 step 309: training loss: 1392.8648894248918\n",
      "Epoch 3 step 310: training accuarcy: 0.584\n",
      "Epoch 3 step 310: training loss: 1375.813059127962\n",
      "Epoch 3 step 311: training accuarcy: 0.5965\n",
      "Epoch 3 step 311: training loss: 1382.6860577086663\n",
      "Epoch 3 step 312: training accuarcy: 0.5785\n",
      "Epoch 3 step 312: training loss: 1378.133987869201\n",
      "Epoch 3 step 313: training accuarcy: 0.5785\n",
      "Epoch 3 step 313: training loss: 1382.363062126731\n",
      "Epoch 3 step 314: training accuarcy: 0.5955\n",
      "Epoch 3 step 314: training loss: 1368.2485430928416\n",
      "Epoch 3 step 315: training accuarcy: 0.5975\n",
      "Epoch 3 step 315: training loss: 1385.096316721629\n",
      "Epoch 3 step 316: training accuarcy: 0.583\n",
      "Epoch 3 step 316: training loss: 1378.706505291396\n",
      "Epoch 3 step 317: training accuarcy: 0.5915\n",
      "Epoch 3 step 317: training loss: 1368.701292492088\n",
      "Epoch 3 step 318: training accuarcy: 0.5865\n",
      "Epoch 3 step 318: training loss: 1368.4682463481452\n",
      "Epoch 3 step 319: training accuarcy: 0.584\n",
      "Epoch 3 step 319: training loss: 1376.2380954839346\n",
      "Epoch 3 step 320: training accuarcy: 0.5955\n",
      "Epoch 3 step 320: training loss: 1372.7873165305903\n",
      "Epoch 3 step 321: training accuarcy: 0.5985\n",
      "Epoch 3 step 321: training loss: 1357.7249609614867\n",
      "Epoch 3 step 322: training accuarcy: 0.609\n",
      "Epoch 3 step 322: training loss: 1364.9461550715178\n",
      "Epoch 3 step 323: training accuarcy: 0.603\n",
      "Epoch 3 step 323: training loss: 1371.445851407941\n",
      "Epoch 3 step 324: training accuarcy: 0.589\n",
      "Epoch 3 step 324: training loss: 1391.1228714736715\n",
      "Epoch 3 step 325: training accuarcy: 0.5720000000000001\n",
      "Epoch 3 step 325: training loss: 1360.209127873582\n",
      "Epoch 3 step 326: training accuarcy: 0.584\n",
      "Epoch 3 step 326: training loss: 1377.5193645964575\n",
      "Epoch 3 step 327: training accuarcy: 0.5660000000000001\n",
      "Epoch 3 step 327: training loss: 1380.4807223374976\n",
      "Epoch 3 step 328: training accuarcy: 0.5915\n",
      "Epoch 3 step 328: training loss: 1385.4733213333081\n",
      "Epoch 3 step 329: training accuarcy: 0.5670000000000001\n",
      "Epoch 3 step 329: training loss: 1370.3834261054567\n",
      "Epoch 3 step 330: training accuarcy: 0.595\n",
      "Epoch 3 step 330: training loss: 1362.6012024568977\n",
      "Epoch 3 step 331: training accuarcy: 0.6075\n",
      "Epoch 3 step 331: training loss: 1373.8654293017084\n",
      "Epoch 3 step 332: training accuarcy: 0.597\n",
      "Epoch 3 step 332: training loss: 1381.5871581129404\n",
      "Epoch 3 step 333: training accuarcy: 0.578\n",
      "Epoch 3 step 333: training loss: 1370.9297013659966\n",
      "Epoch 3 step 334: training accuarcy: 0.5895\n",
      "Epoch 3 step 334: training loss: 1389.7389372608498\n",
      "Epoch 3 step 335: training accuarcy: 0.584\n",
      "Epoch 3 step 335: training loss: 1375.2435359145675\n",
      "Epoch 3 step 336: training accuarcy: 0.59\n",
      "Epoch 3 step 336: training loss: 1392.1610303172192\n",
      "Epoch 3 step 337: training accuarcy: 0.5595\n",
      "Epoch 3 step 337: training loss: 1366.3817040663894\n",
      "Epoch 3 step 338: training accuarcy: 0.5835\n",
      "Epoch 3 step 338: training loss: 1371.528654316158\n",
      "Epoch 3 step 339: training accuarcy: 0.5695\n",
      "Epoch 3 step 339: training loss: 1364.6894445773478\n",
      "Epoch 3 step 340: training accuarcy: 0.5855\n",
      "Epoch 3 step 340: training loss: 1356.6489534866328\n",
      "Epoch 3 step 341: training accuarcy: 0.5875\n",
      "Epoch 3 step 341: training loss: 1368.201029589378\n",
      "Epoch 3 step 342: training accuarcy: 0.58\n",
      "Epoch 3 step 342: training loss: 1354.5064329749002\n",
      "Epoch 3 step 343: training accuarcy: 0.6035\n",
      "Epoch 3 step 343: training loss: 1360.911577737878\n",
      "Epoch 3 step 344: training accuarcy: 0.5925\n",
      "Epoch 3 step 344: training loss: 1354.839428241059\n",
      "Epoch 3 step 345: training accuarcy: 0.5975\n",
      "Epoch 3 step 345: training loss: 1365.7555942518416\n",
      "Epoch 3 step 346: training accuarcy: 0.5835\n",
      "Epoch 3 step 346: training loss: 1364.688611707234\n",
      "Epoch 3 step 347: training accuarcy: 0.591\n",
      "Epoch 3 step 347: training loss: 1362.1676308741482\n",
      "Epoch 3 step 348: training accuarcy: 0.597\n",
      "Epoch 3 step 348: training loss: 1369.741475560213\n",
      "Epoch 3 step 349: training accuarcy: 0.5775\n",
      "Epoch 3 step 349: training loss: 1357.5929108447424\n",
      "Epoch 3 step 350: training accuarcy: 0.5935\n",
      "Epoch 3 step 350: training loss: 1349.653674659042\n",
      "Epoch 3 step 351: training accuarcy: 0.611\n",
      "Epoch 3 step 351: training loss: 1345.1416984861953\n",
      "Epoch 3 step 352: training accuarcy: 0.5935\n",
      "Epoch 3 step 352: training loss: 1362.1093962712494\n",
      "Epoch 3 step 353: training accuarcy: 0.5720000000000001\n",
      "Epoch 3 step 353: training loss: 1351.4925992409073\n",
      "Epoch 3 step 354: training accuarcy: 0.5865\n",
      "Epoch 3 step 354: training loss: 1375.9680642725273\n",
      "Epoch 3 step 355: training accuarcy: 0.581\n",
      "Epoch 3 step 355: training loss: 1362.5902091584567\n",
      "Epoch 3 step 356: training accuarcy: 0.593\n",
      "Epoch 3 step 356: training loss: 1363.9048942683821\n",
      "Epoch 3 step 357: training accuarcy: 0.578\n",
      "Epoch 3 step 357: training loss: 1379.799774077686\n",
      "Epoch 3 step 358: training accuarcy: 0.5815\n",
      "Epoch 3 step 358: training loss: 1359.9781467239156\n",
      "Epoch 3 step 359: training accuarcy: 0.5925\n",
      "Epoch 3 step 359: training loss: 1363.6271459026696\n",
      "Epoch 3 step 360: training accuarcy: 0.5975\n",
      "Epoch 3 step 360: training loss: 1374.9006449039446\n",
      "Epoch 3 step 361: training accuarcy: 0.5905\n",
      "Epoch 3 step 361: training loss: 1370.3565234102477\n",
      "Epoch 3 step 362: training accuarcy: 0.593\n",
      "Epoch 3 step 362: training loss: 1346.7859261668527\n",
      "Epoch 3 step 363: training accuarcy: 0.603\n",
      "Epoch 3 step 363: training loss: 1360.2360814215926\n",
      "Epoch 3 step 364: training accuarcy: 0.6\n",
      "Epoch 3 step 364: training loss: 1336.9724940531169\n",
      "Epoch 3 step 365: training accuarcy: 0.5985\n",
      "Epoch 3 step 365: training loss: 1359.0294073026066\n",
      "Epoch 3 step 366: training accuarcy: 0.582\n",
      "Epoch 3 step 366: training loss: 1363.3519699274166\n",
      "Epoch 3 step 367: training accuarcy: 0.5855\n",
      "Epoch 3 step 367: training loss: 1348.3984257090465\n",
      "Epoch 3 step 368: training accuarcy: 0.597\n",
      "Epoch 3 step 368: training loss: 1361.9149839661807\n",
      "Epoch 3 step 369: training accuarcy: 0.5855\n",
      "Epoch 3 step 369: training loss: 1355.4894390180816\n",
      "Epoch 3 step 370: training accuarcy: 0.593\n",
      "Epoch 3 step 370: training loss: 1369.8589537119542\n",
      "Epoch 3 step 371: training accuarcy: 0.6015\n",
      "Epoch 3 step 371: training loss: 1362.1696611026273\n",
      "Epoch 3 step 372: training accuarcy: 0.578\n",
      "Epoch 3 step 372: training loss: 1365.5464774993088\n",
      "Epoch 3 step 373: training accuarcy: 0.5895\n",
      "Epoch 3 step 373: training loss: 1340.686407841147\n",
      "Epoch 3 step 374: training accuarcy: 0.5925\n",
      "Epoch 3 step 374: training loss: 1361.256302286846\n",
      "Epoch 3 step 375: training accuarcy: 0.5895\n",
      "Epoch 3 step 375: training loss: 1070.444527596234\n",
      "Epoch 3 step 376: training accuarcy: 0.581888246628131\n",
      "Epoch 3: train loss 1370.2263519103822, train accuarcy 0.5860992670059204\n",
      "Epoch 3: valid loss 6234.905117096439, valid accuarcy 0.572206974029541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                  | 4/5 [05:36<01:23, 83.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 376: training loss: 1360.9900585763228\n",
      "Epoch 4 step 377: training accuarcy: 0.5785\n",
      "Epoch 4 step 377: training loss: 1359.267329781073\n",
      "Epoch 4 step 378: training accuarcy: 0.6015\n",
      "Epoch 4 step 378: training loss: 1361.7449540081498\n",
      "Epoch 4 step 379: training accuarcy: 0.595\n",
      "Epoch 4 step 379: training loss: 1345.396363767901\n",
      "Epoch 4 step 380: training accuarcy: 0.6075\n",
      "Epoch 4 step 380: training loss: 1346.4767924012322\n",
      "Epoch 4 step 381: training accuarcy: 0.594\n",
      "Epoch 4 step 381: training loss: 1332.4465988798675\n",
      "Epoch 4 step 382: training accuarcy: 0.599\n",
      "Epoch 4 step 382: training loss: 1341.1260411701455\n",
      "Epoch 4 step 383: training accuarcy: 0.604\n",
      "Epoch 4 step 383: training loss: 1356.9727228753327\n",
      "Epoch 4 step 384: training accuarcy: 0.593\n",
      "Epoch 4 step 384: training loss: 1338.5238653857832\n",
      "Epoch 4 step 385: training accuarcy: 0.6055\n",
      "Epoch 4 step 385: training loss: 1356.2163716840184\n",
      "Epoch 4 step 386: training accuarcy: 0.5885\n",
      "Epoch 4 step 386: training loss: 1354.2504771917684\n",
      "Epoch 4 step 387: training accuarcy: 0.596\n",
      "Epoch 4 step 387: training loss: 1336.8222288245945\n",
      "Epoch 4 step 388: training accuarcy: 0.594\n",
      "Epoch 4 step 388: training loss: 1351.305298845177\n",
      "Epoch 4 step 389: training accuarcy: 0.5945\n",
      "Epoch 4 step 389: training loss: 1350.0484941661114\n",
      "Epoch 4 step 390: training accuarcy: 0.5855\n",
      "Epoch 4 step 390: training loss: 1372.3839848907262\n",
      "Epoch 4 step 391: training accuarcy: 0.5710000000000001\n",
      "Epoch 4 step 391: training loss: 1341.0640062277805\n",
      "Epoch 4 step 392: training accuarcy: 0.5955\n",
      "Epoch 4 step 392: training loss: 1346.4027959925186\n",
      "Epoch 4 step 393: training accuarcy: 0.597\n",
      "Epoch 4 step 393: training loss: 1366.6598420697767\n",
      "Epoch 4 step 394: training accuarcy: 0.5835\n",
      "Epoch 4 step 394: training loss: 1346.676295799833\n",
      "Epoch 4 step 395: training accuarcy: 0.581\n",
      "Epoch 4 step 395: training loss: 1351.535765724783\n",
      "Epoch 4 step 396: training accuarcy: 0.599\n",
      "Epoch 4 step 396: training loss: 1342.2414745377807\n",
      "Epoch 4 step 397: training accuarcy: 0.596\n",
      "Epoch 4 step 397: training loss: 1362.264936928694\n",
      "Epoch 4 step 398: training accuarcy: 0.5895\n",
      "Epoch 4 step 398: training loss: 1335.5723164193837\n",
      "Epoch 4 step 399: training accuarcy: 0.614\n",
      "Epoch 4 step 399: training loss: 1353.7423101151094\n",
      "Epoch 4 step 400: training accuarcy: 0.5805\n",
      "Epoch 4 step 400: training loss: 1346.6868922150284\n",
      "Epoch 4 step 401: training accuarcy: 0.6005\n",
      "Epoch 4 step 401: training loss: 1335.8512789955364\n",
      "Epoch 4 step 402: training accuarcy: 0.598\n",
      "Epoch 4 step 402: training loss: 1335.1227584579822\n",
      "Epoch 4 step 403: training accuarcy: 0.6015\n",
      "Epoch 4 step 403: training loss: 1344.9179028001206\n",
      "Epoch 4 step 404: training accuarcy: 0.612\n",
      "Epoch 4 step 404: training loss: 1358.5391815754915\n",
      "Epoch 4 step 405: training accuarcy: 0.5815\n",
      "Epoch 4 step 405: training loss: 1346.0772564053993\n",
      "Epoch 4 step 406: training accuarcy: 0.6\n",
      "Epoch 4 step 406: training loss: 1344.8526975191483\n",
      "Epoch 4 step 407: training accuarcy: 0.6015\n",
      "Epoch 4 step 407: training loss: 1333.2409396269625\n",
      "Epoch 4 step 408: training accuarcy: 0.5865\n",
      "Epoch 4 step 408: training loss: 1362.5327850366618\n",
      "Epoch 4 step 409: training accuarcy: 0.5825\n",
      "Epoch 4 step 409: training loss: 1353.151873803719\n",
      "Epoch 4 step 410: training accuarcy: 0.5735\n",
      "Epoch 4 step 410: training loss: 1351.3465286906471\n",
      "Epoch 4 step 411: training accuarcy: 0.5945\n",
      "Epoch 4 step 411: training loss: 1345.887308386773\n",
      "Epoch 4 step 412: training accuarcy: 0.596\n",
      "Epoch 4 step 412: training loss: 1357.283689499068\n",
      "Epoch 4 step 413: training accuarcy: 0.5815\n",
      "Epoch 4 step 413: training loss: 1342.1100019490125\n",
      "Epoch 4 step 414: training accuarcy: 0.5935\n",
      "Epoch 4 step 414: training loss: 1346.7887138981177\n",
      "Epoch 4 step 415: training accuarcy: 0.588\n",
      "Epoch 4 step 415: training loss: 1336.8033131731179\n",
      "Epoch 4 step 416: training accuarcy: 0.595\n",
      "Epoch 4 step 416: training loss: 1356.5469342807921\n",
      "Epoch 4 step 417: training accuarcy: 0.593\n",
      "Epoch 4 step 417: training loss: 1353.8353742188463\n",
      "Epoch 4 step 418: training accuarcy: 0.5925\n",
      "Epoch 4 step 418: training loss: 1358.3712797928083\n",
      "Epoch 4 step 419: training accuarcy: 0.578\n",
      "Epoch 4 step 419: training loss: 1341.4326598483394\n",
      "Epoch 4 step 420: training accuarcy: 0.609\n",
      "Epoch 4 step 420: training loss: 1354.9351485055327\n",
      "Epoch 4 step 421: training accuarcy: 0.584\n",
      "Epoch 4 step 421: training loss: 1357.0856303794687\n",
      "Epoch 4 step 422: training accuarcy: 0.5865\n",
      "Epoch 4 step 422: training loss: 1339.348247046344\n",
      "Epoch 4 step 423: training accuarcy: 0.595\n",
      "Epoch 4 step 423: training loss: 1334.2674022542787\n",
      "Epoch 4 step 424: training accuarcy: 0.6065\n",
      "Epoch 4 step 424: training loss: 1341.4024266174008\n",
      "Epoch 4 step 425: training accuarcy: 0.61\n",
      "Epoch 4 step 425: training loss: 1340.2291953293318\n",
      "Epoch 4 step 426: training accuarcy: 0.594\n",
      "Epoch 4 step 426: training loss: 1355.956328581366\n",
      "Epoch 4 step 427: training accuarcy: 0.583\n",
      "Epoch 4 step 427: training loss: 1342.5990026363215\n",
      "Epoch 4 step 428: training accuarcy: 0.5765\n",
      "Epoch 4 step 428: training loss: 1350.0500628252778\n",
      "Epoch 4 step 429: training accuarcy: 0.5815\n",
      "Epoch 4 step 429: training loss: 1352.3323394896436\n",
      "Epoch 4 step 430: training accuarcy: 0.5730000000000001\n",
      "Epoch 4 step 430: training loss: 1338.5263294237056\n",
      "Epoch 4 step 431: training accuarcy: 0.5925\n",
      "Epoch 4 step 431: training loss: 1347.152503091519\n",
      "Epoch 4 step 432: training accuarcy: 0.5805\n",
      "Epoch 4 step 432: training loss: 1344.6655661429124\n",
      "Epoch 4 step 433: training accuarcy: 0.586\n",
      "Epoch 4 step 433: training loss: 1352.3019016817982\n",
      "Epoch 4 step 434: training accuarcy: 0.584\n",
      "Epoch 4 step 434: training loss: 1350.172099296759\n",
      "Epoch 4 step 435: training accuarcy: 0.5825\n",
      "Epoch 4 step 435: training loss: 1331.5150012716692\n",
      "Epoch 4 step 436: training accuarcy: 0.6165\n",
      "Epoch 4 step 436: training loss: 1358.1241724471276\n",
      "Epoch 4 step 437: training accuarcy: 0.593\n",
      "Epoch 4 step 437: training loss: 1348.6639240539077\n",
      "Epoch 4 step 438: training accuarcy: 0.584\n",
      "Epoch 4 step 438: training loss: 1350.9075781502293\n",
      "Epoch 4 step 439: training accuarcy: 0.5875\n",
      "Epoch 4 step 439: training loss: 1350.0416537488327\n",
      "Epoch 4 step 440: training accuarcy: 0.5925\n",
      "Epoch 4 step 440: training loss: 1343.6192212107287\n",
      "Epoch 4 step 441: training accuarcy: 0.598\n",
      "Epoch 4 step 441: training loss: 1356.1280818080754\n",
      "Epoch 4 step 442: training accuarcy: 0.582\n",
      "Epoch 4 step 442: training loss: 1340.2840657994923\n",
      "Epoch 4 step 443: training accuarcy: 0.599\n",
      "Epoch 4 step 443: training loss: 1349.2741768562569\n",
      "Epoch 4 step 444: training accuarcy: 0.5915\n",
      "Epoch 4 step 444: training loss: 1337.5485513600477\n",
      "Epoch 4 step 445: training accuarcy: 0.5925\n",
      "Epoch 4 step 445: training loss: 1351.23005049964\n",
      "Epoch 4 step 446: training accuarcy: 0.5805\n",
      "Epoch 4 step 446: training loss: 1342.1023493821644\n",
      "Epoch 4 step 447: training accuarcy: 0.585\n",
      "Epoch 4 step 447: training loss: 1338.8374852716627\n",
      "Epoch 4 step 448: training accuarcy: 0.5925\n",
      "Epoch 4 step 448: training loss: 1341.2456343810047\n",
      "Epoch 4 step 449: training accuarcy: 0.59\n",
      "Epoch 4 step 449: training loss: 1337.0461787329216\n",
      "Epoch 4 step 450: training accuarcy: 0.5985\n",
      "Epoch 4 step 450: training loss: 1353.4048890051713\n",
      "Epoch 4 step 451: training accuarcy: 0.5835\n",
      "Epoch 4 step 451: training loss: 1346.9242235622883\n",
      "Epoch 4 step 452: training accuarcy: 0.5805\n",
      "Epoch 4 step 452: training loss: 1353.8586680615113\n",
      "Epoch 4 step 453: training accuarcy: 0.5855\n",
      "Epoch 4 step 453: training loss: 1330.302216346343\n",
      "Epoch 4 step 454: training accuarcy: 0.61\n",
      "Epoch 4 step 454: training loss: 1337.2552091402345\n",
      "Epoch 4 step 455: training accuarcy: 0.609\n",
      "Epoch 4 step 455: training loss: 1351.97355808145\n",
      "Epoch 4 step 456: training accuarcy: 0.5805\n",
      "Epoch 4 step 456: training loss: 1351.7526183951786\n",
      "Epoch 4 step 457: training accuarcy: 0.578\n",
      "Epoch 4 step 457: training loss: 1348.0672375672711\n",
      "Epoch 4 step 458: training accuarcy: 0.58\n",
      "Epoch 4 step 458: training loss: 1338.9683415433722\n",
      "Epoch 4 step 459: training accuarcy: 0.583\n",
      "Epoch 4 step 459: training loss: 1352.7218022307036\n",
      "Epoch 4 step 460: training accuarcy: 0.5905\n",
      "Epoch 4 step 460: training loss: 1340.7930165195023\n",
      "Epoch 4 step 461: training accuarcy: 0.597\n",
      "Epoch 4 step 461: training loss: 1322.9085676313136\n",
      "Epoch 4 step 462: training accuarcy: 0.61\n",
      "Epoch 4 step 462: training loss: 1326.0289316496833\n",
      "Epoch 4 step 463: training accuarcy: 0.605\n",
      "Epoch 4 step 463: training loss: 1345.337950729878\n",
      "Epoch 4 step 464: training accuarcy: 0.598\n",
      "Epoch 4 step 464: training loss: 1353.5141415246135\n",
      "Epoch 4 step 465: training accuarcy: 0.5975\n",
      "Epoch 4 step 465: training loss: 1339.0029080850395\n",
      "Epoch 4 step 466: training accuarcy: 0.5945\n",
      "Epoch 4 step 466: training loss: 1331.4975757873462\n",
      "Epoch 4 step 467: training accuarcy: 0.6005\n",
      "Epoch 4 step 467: training loss: 1338.7449823354457\n",
      "Epoch 4 step 468: training accuarcy: 0.607\n",
      "Epoch 4 step 468: training loss: 1348.0722661187795\n",
      "Epoch 4 step 469: training accuarcy: 0.596\n",
      "Epoch 4 step 469: training loss: 1048.807635862333\n",
      "Epoch 4 step 470: training accuarcy: 0.5940912010276171\n",
      "Epoch 4: train loss 1343.6068057967059, train accuarcy 0.5895145535469055\n",
      "Epoch 4: valid loss 6237.861293889521, valid accuarcy 0.5682596564292908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [07:02<00:00, 84.56s/it]\n"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=5,log_dir=get_log_dir('seq_stackoverflow', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:33:03.604016Z",
     "start_time": "2019-10-07T13:33:03.581012Z"
    }
   },
   "outputs": [],
   "source": [
    "del prme_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Trans FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:36:10.049489Z",
     "start_time": "2019-10-07T13:36:08.341698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTransFM()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model = TorchTransFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "trans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:36:10.888432Z",
     "start_time": "2019-10-07T13:36:10.885430Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(trans_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:36:14.087694Z",
     "start_time": "2019-10-07T13:36:11.557025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1b4aa9c7358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_learner = FMLearner(trans_model, adam_opt, schedular, db)\n",
    "trans_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:36:14.980297Z",
     "start_time": "2019-10-07T13:36:14.975293Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='base',\n",
    "                      valid_col='seq',\n",
    "                      test_col='seq',\n",
    "                      loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:43:55.158627Z",
     "start_time": "2019-10-07T13:36:15.803171Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 164958.5357854265\n",
      "Epoch 0 step 1: training accuarcy: 0.518\n",
      "Epoch 0 step 1: training loss: 160368.91942496574\n",
      "Epoch 0 step 2: training accuarcy: 0.5035000000000001\n",
      "Epoch 0 step 2: training loss: 155344.1250532723\n",
      "Epoch 0 step 3: training accuarcy: 0.5175\n",
      "Epoch 0 step 3: training loss: 151314.12961604865\n",
      "Epoch 0 step 4: training accuarcy: 0.513\n",
      "Epoch 0 step 4: training loss: 146592.98273384754\n",
      "Epoch 0 step 5: training accuarcy: 0.496\n",
      "Epoch 0 step 5: training loss: 142728.94892648255\n",
      "Epoch 0 step 6: training accuarcy: 0.4975\n",
      "Epoch 0 step 6: training loss: 138132.09020402856\n",
      "Epoch 0 step 7: training accuarcy: 0.518\n",
      "Epoch 0 step 7: training loss: 133968.60335181077\n",
      "Epoch 0 step 8: training accuarcy: 0.5075000000000001\n",
      "Epoch 0 step 8: training loss: 130519.33231767353\n",
      "Epoch 0 step 9: training accuarcy: 0.5075000000000001\n",
      "Epoch 0 step 9: training loss: 127024.94322712412\n",
      "Epoch 0 step 10: training accuarcy: 0.4865\n",
      "Epoch 0 step 10: training loss: 122531.02123567433\n",
      "Epoch 0 step 11: training accuarcy: 0.5035000000000001\n",
      "Epoch 0 step 11: training loss: 119714.22092771692\n",
      "Epoch 0 step 12: training accuarcy: 0.49\n",
      "Epoch 0 step 12: training loss: 115586.16828268337\n",
      "Epoch 0 step 13: training accuarcy: 0.5075000000000001\n",
      "Epoch 0 step 13: training loss: 112070.69135913465\n",
      "Epoch 0 step 14: training accuarcy: 0.505\n",
      "Epoch 0 step 14: training loss: 108592.58807269573\n",
      "Epoch 0 step 15: training accuarcy: 0.511\n",
      "Epoch 0 step 15: training loss: 105729.48173431632\n",
      "Epoch 0 step 16: training accuarcy: 0.484\n",
      "Epoch 0 step 16: training loss: 102161.96732684018\n",
      "Epoch 0 step 17: training accuarcy: 0.503\n",
      "Epoch 0 step 17: training loss: 98970.78438721538\n",
      "Epoch 0 step 18: training accuarcy: 0.508\n",
      "Epoch 0 step 18: training loss: 95891.86657599242\n",
      "Epoch 0 step 19: training accuarcy: 0.4965\n",
      "Epoch 0 step 19: training loss: 93223.01527224379\n",
      "Epoch 0 step 20: training accuarcy: 0.4975\n",
      "Epoch 0 step 20: training loss: 90135.8476556559\n",
      "Epoch 0 step 21: training accuarcy: 0.5075000000000001\n",
      "Epoch 0 step 21: training loss: 87523.47026438008\n",
      "Epoch 0 step 22: training accuarcy: 0.491\n",
      "Epoch 0 step 22: training loss: 84602.50534450237\n",
      "Epoch 0 step 23: training accuarcy: 0.5125\n",
      "Epoch 0 step 23: training loss: 82055.30442070181\n",
      "Epoch 0 step 24: training accuarcy: 0.5015000000000001\n",
      "Epoch 0 step 24: training loss: 79284.02178861556\n",
      "Epoch 0 step 25: training accuarcy: 0.503\n",
      "Epoch 0 step 25: training loss: 76715.2649442361\n",
      "Epoch 0 step 26: training accuarcy: 0.51\n",
      "Epoch 0 step 26: training loss: 74779.69019426503\n",
      "Epoch 0 step 27: training accuarcy: 0.5115000000000001\n",
      "Epoch 0 step 27: training loss: 72164.02437337309\n",
      "Epoch 0 step 28: training accuarcy: 0.5165\n",
      "Epoch 0 step 28: training loss: 69898.79212172372\n",
      "Epoch 0 step 29: training accuarcy: 0.507\n",
      "Epoch 0 step 29: training loss: 67674.38749126128\n",
      "Epoch 0 step 30: training accuarcy: 0.496\n",
      "Epoch 0 step 30: training loss: 65683.47747373416\n",
      "Epoch 0 step 31: training accuarcy: 0.5035000000000001\n",
      "Epoch 0 step 31: training loss: 63449.021316423954\n",
      "Epoch 0 step 32: training accuarcy: 0.507\n",
      "Epoch 0 step 32: training loss: 61475.80113545833\n",
      "Epoch 0 step 33: training accuarcy: 0.512\n",
      "Epoch 0 step 33: training loss: 59658.964554016755\n",
      "Epoch 0 step 34: training accuarcy: 0.494\n",
      "Epoch 0 step 34: training loss: 57883.707317749126\n",
      "Epoch 0 step 35: training accuarcy: 0.494\n",
      "Epoch 0 step 35: training loss: 56128.6578926071\n",
      "Epoch 0 step 36: training accuarcy: 0.495\n",
      "Epoch 0 step 36: training loss: 54069.15902215961\n",
      "Epoch 0 step 37: training accuarcy: 0.517\n",
      "Epoch 0 step 37: training loss: 52314.13091722123\n",
      "Epoch 0 step 38: training accuarcy: 0.5115000000000001\n",
      "Epoch 0 step 38: training loss: 50907.043381298354\n",
      "Epoch 0 step 39: training accuarcy: 0.47400000000000003\n",
      "Epoch 0 step 39: training loss: 49022.01452237715\n",
      "Epoch 0 step 40: training accuarcy: 0.4935\n",
      "Epoch 0 step 40: training loss: 47503.96868714102\n",
      "Epoch 0 step 41: training accuarcy: 0.5235\n",
      "Epoch 0 step 41: training loss: 45968.50461931145\n",
      "Epoch 0 step 42: training accuarcy: 0.511\n",
      "Epoch 0 step 42: training loss: 44538.700714173705\n",
      "Epoch 0 step 43: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 43: training loss: 42961.26036837973\n",
      "Epoch 0 step 44: training accuarcy: 0.495\n",
      "Epoch 0 step 44: training loss: 41796.68777732493\n",
      "Epoch 0 step 45: training accuarcy: 0.504\n",
      "Epoch 0 step 45: training loss: 40294.1046506567\n",
      "Epoch 0 step 46: training accuarcy: 0.5145\n",
      "Epoch 0 step 46: training loss: 39072.78979004733\n",
      "Epoch 0 step 47: training accuarcy: 0.4995\n",
      "Epoch 0 step 47: training loss: 37619.20924450545\n",
      "Epoch 0 step 48: training accuarcy: 0.517\n",
      "Epoch 0 step 48: training loss: 36715.436256127636\n",
      "Epoch 0 step 49: training accuarcy: 0.5115000000000001\n",
      "Epoch 0 step 49: training loss: 35600.709668352196\n",
      "Epoch 0 step 50: training accuarcy: 0.4935\n",
      "Epoch 0 step 50: training loss: 34257.291521630774\n",
      "Epoch 0 step 51: training accuarcy: 0.517\n",
      "Epoch 0 step 51: training loss: 33121.99823289989\n",
      "Epoch 0 step 52: training accuarcy: 0.5175\n",
      "Epoch 0 step 52: training loss: 32111.196347598903\n",
      "Epoch 0 step 53: training accuarcy: 0.507\n",
      "Epoch 0 step 53: training loss: 31143.49101642184\n",
      "Epoch 0 step 54: training accuarcy: 0.511\n",
      "Epoch 0 step 54: training loss: 30076.705860603804\n",
      "Epoch 0 step 55: training accuarcy: 0.5135\n",
      "Epoch 0 step 55: training loss: 29216.552677009106\n",
      "Epoch 0 step 56: training accuarcy: 0.515\n",
      "Epoch 0 step 56: training loss: 28139.409324166005\n",
      "Epoch 0 step 57: training accuarcy: 0.5125\n",
      "Epoch 0 step 57: training loss: 27355.682917284277\n",
      "Epoch 0 step 58: training accuarcy: 0.5195\n",
      "Epoch 0 step 58: training loss: 26457.532699287374\n",
      "Epoch 0 step 59: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 59: training loss: 25632.96224979572\n",
      "Epoch 0 step 60: training accuarcy: 0.509\n",
      "Epoch 0 step 60: training loss: 24897.51846469374\n",
      "Epoch 0 step 61: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 61: training loss: 24033.892049478294\n",
      "Epoch 0 step 62: training accuarcy: 0.5015000000000001\n",
      "Epoch 0 step 62: training loss: 23355.89559717237\n",
      "Epoch 0 step 63: training accuarcy: 0.5105000000000001\n",
      "Epoch 0 step 63: training loss: 22612.26640497009\n",
      "Epoch 0 step 64: training accuarcy: 0.499\n",
      "Epoch 0 step 64: training loss: 21808.692513209877\n",
      "Epoch 0 step 65: training accuarcy: 0.507\n",
      "Epoch 0 step 65: training loss: 21229.541107825356\n",
      "Epoch 0 step 66: training accuarcy: 0.47200000000000003\n",
      "Epoch 0 step 66: training loss: 20414.951508921185\n",
      "Epoch 0 step 67: training accuarcy: 0.511\n",
      "Epoch 0 step 67: training loss: 19943.45082187748\n",
      "Epoch 0 step 68: training accuarcy: 0.504\n",
      "Epoch 0 step 68: training loss: 19230.330777989402\n",
      "Epoch 0 step 69: training accuarcy: 0.5095000000000001\n",
      "Epoch 0 step 69: training loss: 18643.979696680963\n",
      "Epoch 0 step 70: training accuarcy: 0.498\n",
      "Epoch 0 step 70: training loss: 17953.81511766417\n",
      "Epoch 0 step 71: training accuarcy: 0.5265\n",
      "Epoch 0 step 71: training loss: 17565.460887411326\n",
      "Epoch 0 step 72: training accuarcy: 0.5105000000000001\n",
      "Epoch 0 step 72: training loss: 17036.273277902113\n",
      "Epoch 0 step 73: training accuarcy: 0.493\n",
      "Epoch 0 step 73: training loss: 16487.326340672265\n",
      "Epoch 0 step 74: training accuarcy: 0.503\n",
      "Epoch 0 step 74: training loss: 16065.2974323677\n",
      "Epoch 0 step 75: training accuarcy: 0.497\n",
      "Epoch 0 step 75: training loss: 15385.22676518067\n",
      "Epoch 0 step 76: training accuarcy: 0.524\n",
      "Epoch 0 step 76: training loss: 15040.274415633032\n",
      "Epoch 0 step 77: training accuarcy: 0.508\n",
      "Epoch 0 step 77: training loss: 14581.986045612433\n",
      "Epoch 0 step 78: training accuarcy: 0.4945\n",
      "Epoch 0 step 78: training loss: 14048.16396550641\n",
      "Epoch 0 step 79: training accuarcy: 0.5065000000000001\n",
      "Epoch 0 step 79: training loss: 13550.506412313527\n",
      "Epoch 0 step 80: training accuarcy: 0.5415\n",
      "Epoch 0 step 80: training loss: 13261.418340643337\n",
      "Epoch 0 step 81: training accuarcy: 0.504\n",
      "Epoch 0 step 81: training loss: 12838.719805573866\n",
      "Epoch 0 step 82: training accuarcy: 0.517\n",
      "Epoch 0 step 82: training loss: 12384.125019934252\n",
      "Epoch 0 step 83: training accuarcy: 0.5255\n",
      "Epoch 0 step 83: training loss: 12176.576753234745\n",
      "Epoch 0 step 84: training accuarcy: 0.506\n",
      "Epoch 0 step 84: training loss: 11766.641730311208\n",
      "Epoch 0 step 85: training accuarcy: 0.508\n",
      "Epoch 0 step 85: training loss: 11441.197317885399\n",
      "Epoch 0 step 86: training accuarcy: 0.5145\n",
      "Epoch 0 step 86: training loss: 11071.63733062639\n",
      "Epoch 0 step 87: training accuarcy: 0.497\n",
      "Epoch 0 step 87: training loss: 10787.848828934655\n",
      "Epoch 0 step 88: training accuarcy: 0.506\n",
      "Epoch 0 step 88: training loss: 10326.849302721013\n",
      "Epoch 0 step 89: training accuarcy: 0.5365\n",
      "Epoch 0 step 89: training loss: 10190.145706859306\n",
      "Epoch 0 step 90: training accuarcy: 0.5195\n",
      "Epoch 0 step 90: training loss: 9827.751920586736\n",
      "Epoch 0 step 91: training accuarcy: 0.5145\n",
      "Epoch 0 step 91: training loss: 9553.895995412964\n",
      "Epoch 0 step 92: training accuarcy: 0.5105000000000001\n",
      "Epoch 0 step 92: training loss: 9366.186673349328\n",
      "Epoch 0 step 93: training accuarcy: 0.508\n",
      "Epoch 0 step 93: training loss: 8637.228264037394\n",
      "Epoch 0 step 94: training accuarcy: 0.5221579961464354\n",
      "Epoch 0: train loss 54446.26562939217, train accuarcy 0.5061160326004028\n",
      "Epoch 0: valid loss 15760.497764402653, valid accuarcy 0.5176749229431152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▌                                                                                                                                          | 1/5 [01:31<06:04, 91.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 94: training loss: 8468.06361467602\n",
      "Epoch 1 step 95: training accuarcy: 0.619\n",
      "Epoch 1 step 95: training loss: 8237.7399516149\n",
      "Epoch 1 step 96: training accuarcy: 0.622\n",
      "Epoch 1 step 96: training loss: 8025.693425424888\n",
      "Epoch 1 step 97: training accuarcy: 0.615\n",
      "Epoch 1 step 97: training loss: 7775.535208786701\n",
      "Epoch 1 step 98: training accuarcy: 0.629\n",
      "Epoch 1 step 98: training loss: 7581.423166052558\n",
      "Epoch 1 step 99: training accuarcy: 0.618\n",
      "Epoch 1 step 99: training loss: 7346.008004475699\n",
      "Epoch 1 step 100: training accuarcy: 0.631\n",
      "Epoch 1 step 100: training loss: 7157.674886435413\n",
      "Epoch 1 step 101: training accuarcy: 0.614\n",
      "Epoch 1 step 101: training loss: 6975.372647907814\n",
      "Epoch 1 step 102: training accuarcy: 0.64\n",
      "Epoch 1 step 102: training loss: 6807.763966372768\n",
      "Epoch 1 step 103: training accuarcy: 0.61\n",
      "Epoch 1 step 103: training loss: 6616.774350835358\n",
      "Epoch 1 step 104: training accuarcy: 0.629\n",
      "Epoch 1 step 104: training loss: 6428.667614136943\n",
      "Epoch 1 step 105: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 105: training loss: 6264.774856418028\n",
      "Epoch 1 step 106: training accuarcy: 0.631\n",
      "Epoch 1 step 106: training loss: 6154.607888233768\n",
      "Epoch 1 step 107: training accuarcy: 0.636\n",
      "Epoch 1 step 107: training loss: 5951.293534068926\n",
      "Epoch 1 step 108: training accuarcy: 0.627\n",
      "Epoch 1 step 108: training loss: 5846.687191543333\n",
      "Epoch 1 step 109: training accuarcy: 0.6305000000000001\n",
      "Epoch 1 step 109: training loss: 5699.978126447311\n",
      "Epoch 1 step 110: training accuarcy: 0.638\n",
      "Epoch 1 step 110: training loss: 5598.673334045312\n",
      "Epoch 1 step 111: training accuarcy: 0.6165\n",
      "Epoch 1 step 111: training loss: 5371.9342773557\n",
      "Epoch 1 step 112: training accuarcy: 0.653\n",
      "Epoch 1 step 112: training loss: 5241.07520872812\n",
      "Epoch 1 step 113: training accuarcy: 0.651\n",
      "Epoch 1 step 113: training loss: 5157.87316408102\n",
      "Epoch 1 step 114: training accuarcy: 0.642\n",
      "Epoch 1 step 114: training loss: 5073.6535531662175\n",
      "Epoch 1 step 115: training accuarcy: 0.633\n",
      "Epoch 1 step 115: training loss: 4894.613343234342\n",
      "Epoch 1 step 116: training accuarcy: 0.636\n",
      "Epoch 1 step 116: training loss: 4807.524440027117\n",
      "Epoch 1 step 117: training accuarcy: 0.6365000000000001\n",
      "Epoch 1 step 117: training loss: 4698.901907657167\n",
      "Epoch 1 step 118: training accuarcy: 0.6405\n",
      "Epoch 1 step 118: training loss: 4606.790440486782\n",
      "Epoch 1 step 119: training accuarcy: 0.6485\n",
      "Epoch 1 step 119: training loss: 4528.217252168393\n",
      "Epoch 1 step 120: training accuarcy: 0.6365000000000001\n",
      "Epoch 1 step 120: training loss: 4460.76691101088\n",
      "Epoch 1 step 121: training accuarcy: 0.6285000000000001\n",
      "Epoch 1 step 121: training loss: 4339.627598114174\n",
      "Epoch 1 step 122: training accuarcy: 0.631\n",
      "Epoch 1 step 122: training loss: 4222.273263781099\n",
      "Epoch 1 step 123: training accuarcy: 0.6405\n",
      "Epoch 1 step 123: training loss: 4168.610498492066\n",
      "Epoch 1 step 124: training accuarcy: 0.6315000000000001\n",
      "Epoch 1 step 124: training loss: 4006.4369885867145\n",
      "Epoch 1 step 125: training accuarcy: 0.656\n",
      "Epoch 1 step 125: training loss: 3996.40421902555\n",
      "Epoch 1 step 126: training accuarcy: 0.632\n",
      "Epoch 1 step 126: training loss: 3866.658621023257\n",
      "Epoch 1 step 127: training accuarcy: 0.6595\n",
      "Epoch 1 step 127: training loss: 3805.102747990676\n",
      "Epoch 1 step 128: training accuarcy: 0.6555\n",
      "Epoch 1 step 128: training loss: 3759.437290044791\n",
      "Epoch 1 step 129: training accuarcy: 0.6495\n",
      "Epoch 1 step 129: training loss: 3680.5211295159015\n",
      "Epoch 1 step 130: training accuarcy: 0.6595\n",
      "Epoch 1 step 130: training loss: 3638.1340251068787\n",
      "Epoch 1 step 131: training accuarcy: 0.6395000000000001\n",
      "Epoch 1 step 131: training loss: 3583.6707848814876\n",
      "Epoch 1 step 132: training accuarcy: 0.629\n",
      "Epoch 1 step 132: training loss: 3501.8846284577285\n",
      "Epoch 1 step 133: training accuarcy: 0.6395000000000001\n",
      "Epoch 1 step 133: training loss: 3426.679631142716\n",
      "Epoch 1 step 134: training accuarcy: 0.6475\n",
      "Epoch 1 step 134: training loss: 3347.7784046929237\n",
      "Epoch 1 step 135: training accuarcy: 0.656\n",
      "Epoch 1 step 135: training loss: 3356.387359839192\n",
      "Epoch 1 step 136: training accuarcy: 0.639\n",
      "Epoch 1 step 136: training loss: 3269.36519718295\n",
      "Epoch 1 step 137: training accuarcy: 0.638\n",
      "Epoch 1 step 137: training loss: 3232.398035592925\n",
      "Epoch 1 step 138: training accuarcy: 0.6435\n",
      "Epoch 1 step 138: training loss: 3242.6504541994714\n",
      "Epoch 1 step 139: training accuarcy: 0.636\n",
      "Epoch 1 step 139: training loss: 3167.1493330700387\n",
      "Epoch 1 step 140: training accuarcy: 0.6195\n",
      "Epoch 1 step 140: training loss: 3092.245407933304\n",
      "Epoch 1 step 141: training accuarcy: 0.639\n",
      "Epoch 1 step 141: training loss: 3039.090875577356\n",
      "Epoch 1 step 142: training accuarcy: 0.643\n",
      "Epoch 1 step 142: training loss: 3025.4579096893867\n",
      "Epoch 1 step 143: training accuarcy: 0.634\n",
      "Epoch 1 step 143: training loss: 3006.9138821356655\n",
      "Epoch 1 step 144: training accuarcy: 0.616\n",
      "Epoch 1 step 144: training loss: 2934.420927996132\n",
      "Epoch 1 step 145: training accuarcy: 0.626\n",
      "Epoch 1 step 145: training loss: 2879.475381642601\n",
      "Epoch 1 step 146: training accuarcy: 0.645\n",
      "Epoch 1 step 146: training loss: 2888.2954299063513\n",
      "Epoch 1 step 147: training accuarcy: 0.614\n",
      "Epoch 1 step 147: training loss: 2822.4120697971557\n",
      "Epoch 1 step 148: training accuarcy: 0.628\n",
      "Epoch 1 step 148: training loss: 2821.0429653736937\n",
      "Epoch 1 step 149: training accuarcy: 0.6245\n",
      "Epoch 1 step 149: training loss: 2731.0607882233853\n",
      "Epoch 1 step 150: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 150: training loss: 2748.390236247671\n",
      "Epoch 1 step 151: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 151: training loss: 2698.7927646013977\n",
      "Epoch 1 step 152: training accuarcy: 0.6335000000000001\n",
      "Epoch 1 step 152: training loss: 2665.2759392128455\n",
      "Epoch 1 step 153: training accuarcy: 0.6335000000000001\n",
      "Epoch 1 step 153: training loss: 2608.832978049388\n",
      "Epoch 1 step 154: training accuarcy: 0.636\n",
      "Epoch 1 step 154: training loss: 2669.8873842944877\n",
      "Epoch 1 step 155: training accuarcy: 0.612\n",
      "Epoch 1 step 155: training loss: 2553.6036292487343\n",
      "Epoch 1 step 156: training accuarcy: 0.636\n",
      "Epoch 1 step 156: training loss: 2560.634710761745\n",
      "Epoch 1 step 157: training accuarcy: 0.63\n",
      "Epoch 1 step 157: training loss: 2492.068535002035\n",
      "Epoch 1 step 158: training accuarcy: 0.641\n",
      "Epoch 1 step 158: training loss: 2513.0919270885024\n",
      "Epoch 1 step 159: training accuarcy: 0.613\n",
      "Epoch 1 step 159: training loss: 2468.4906104070205\n",
      "Epoch 1 step 160: training accuarcy: 0.632\n",
      "Epoch 1 step 160: training loss: 2505.2217448278034\n",
      "Epoch 1 step 161: training accuarcy: 0.611\n",
      "Epoch 1 step 161: training loss: 2454.1518496278077\n",
      "Epoch 1 step 162: training accuarcy: 0.6065\n",
      "Epoch 1 step 162: training loss: 2425.2756854347126\n",
      "Epoch 1 step 163: training accuarcy: 0.6115\n",
      "Epoch 1 step 163: training loss: 2349.04181798639\n",
      "Epoch 1 step 164: training accuarcy: 0.622\n",
      "Epoch 1 step 164: training loss: 2390.027633423556\n",
      "Epoch 1 step 165: training accuarcy: 0.6195\n",
      "Epoch 1 step 165: training loss: 2386.7680320242853\n",
      "Epoch 1 step 166: training accuarcy: 0.61\n",
      "Epoch 1 step 166: training loss: 2297.963747846139\n",
      "Epoch 1 step 167: training accuarcy: 0.637\n",
      "Epoch 1 step 167: training loss: 2318.6036165374026\n",
      "Epoch 1 step 168: training accuarcy: 0.6065\n",
      "Epoch 1 step 168: training loss: 2269.9313471139158\n",
      "Epoch 1 step 169: training accuarcy: 0.6295000000000001\n",
      "Epoch 1 step 169: training loss: 2254.359853909976\n",
      "Epoch 1 step 170: training accuarcy: 0.618\n",
      "Epoch 1 step 170: training loss: 2269.7563807284173\n",
      "Epoch 1 step 171: training accuarcy: 0.61\n",
      "Epoch 1 step 171: training loss: 2199.103159765359\n",
      "Epoch 1 step 172: training accuarcy: 0.6465\n",
      "Epoch 1 step 172: training loss: 2251.327863486968\n",
      "Epoch 1 step 173: training accuarcy: 0.6105\n",
      "Epoch 1 step 173: training loss: 2226.2313360367375\n",
      "Epoch 1 step 174: training accuarcy: 0.6225\n",
      "Epoch 1 step 174: training loss: 2188.654329824329\n",
      "Epoch 1 step 175: training accuarcy: 0.619\n",
      "Epoch 1 step 175: training loss: 2211.520480679118\n",
      "Epoch 1 step 176: training accuarcy: 0.622\n",
      "Epoch 1 step 176: training loss: 2162.420349916816\n",
      "Epoch 1 step 177: training accuarcy: 0.6245\n",
      "Epoch 1 step 177: training loss: 2142.671747293637\n",
      "Epoch 1 step 178: training accuarcy: 0.6315000000000001\n",
      "Epoch 1 step 178: training loss: 2121.5321395160636\n",
      "Epoch 1 step 179: training accuarcy: 0.625\n",
      "Epoch 1 step 179: training loss: 2121.7628658376652\n",
      "Epoch 1 step 180: training accuarcy: 0.626\n",
      "Epoch 1 step 180: training loss: 2124.0594342502573\n",
      "Epoch 1 step 181: training accuarcy: 0.6015\n",
      "Epoch 1 step 181: training loss: 2134.9398912944716\n",
      "Epoch 1 step 182: training accuarcy: 0.6125\n",
      "Epoch 1 step 182: training loss: 2108.9320933955532\n",
      "Epoch 1 step 183: training accuarcy: 0.6085\n",
      "Epoch 1 step 183: training loss: 2135.8568153340875\n",
      "Epoch 1 step 184: training accuarcy: 0.5955\n",
      "Epoch 1 step 184: training loss: 2054.106131193623\n",
      "Epoch 1 step 185: training accuarcy: 0.6235\n",
      "Epoch 1 step 185: training loss: 2044.1510254736663\n",
      "Epoch 1 step 186: training accuarcy: 0.616\n",
      "Epoch 1 step 186: training loss: 2002.8622185634113\n",
      "Epoch 1 step 187: training accuarcy: 0.629\n",
      "Epoch 1 step 187: training loss: 1728.0410885461392\n",
      "Epoch 1 step 188: training accuarcy: 0.6069364161849711\n",
      "Epoch 1: train loss 3749.8724628637124, train accuarcy 0.6277985572814941\n",
      "Epoch 1: valid loss 8402.09065164618, valid accuarcy 0.5092396140098572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████████████████████████████████████████▏                                                                                                       | 2/5 [03:02<04:33, 91.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 188: training loss: 1781.0890677299585\n",
      "Epoch 2 step 189: training accuarcy: 0.725\n",
      "Epoch 2 step 189: training loss: 1814.0297021842025\n",
      "Epoch 2 step 190: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 190: training loss: 1756.4151257031826\n",
      "Epoch 2 step 191: training accuarcy: 0.715\n",
      "Epoch 2 step 191: training loss: 1748.5377328845552\n",
      "Epoch 2 step 192: training accuarcy: 0.711\n",
      "Epoch 2 step 192: training loss: 1717.0514120615628\n",
      "Epoch 2 step 193: training accuarcy: 0.72\n",
      "Epoch 2 step 193: training loss: 1753.252902917661\n",
      "Epoch 2 step 194: training accuarcy: 0.7105\n",
      "Epoch 2 step 194: training loss: 1704.5088133618492\n",
      "Epoch 2 step 195: training accuarcy: 0.716\n",
      "Epoch 2 step 195: training loss: 1726.4131658915412\n",
      "Epoch 2 step 196: training accuarcy: 0.717\n",
      "Epoch 2 step 196: training loss: 1721.4399752680092\n",
      "Epoch 2 step 197: training accuarcy: 0.724\n",
      "Epoch 2 step 197: training loss: 1700.8385204355768\n",
      "Epoch 2 step 198: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 198: training loss: 1709.708020951915\n",
      "Epoch 2 step 199: training accuarcy: 0.717\n",
      "Epoch 2 step 199: training loss: 1687.3069240024934\n",
      "Epoch 2 step 200: training accuarcy: 0.711\n",
      "Epoch 2 step 200: training loss: 1681.9059289996299\n",
      "Epoch 2 step 201: training accuarcy: 0.708\n",
      "Epoch 2 step 201: training loss: 1694.3013778902405\n",
      "Epoch 2 step 202: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 202: training loss: 1689.0037745075415\n",
      "Epoch 2 step 203: training accuarcy: 0.7005\n",
      "Epoch 2 step 203: training loss: 1686.7804890855855\n",
      "Epoch 2 step 204: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 204: training loss: 1645.8874436767442\n",
      "Epoch 2 step 205: training accuarcy: 0.7095\n",
      "Epoch 2 step 205: training loss: 1657.2292613806421\n",
      "Epoch 2 step 206: training accuarcy: 0.711\n",
      "Epoch 2 step 206: training loss: 1684.0768056439667\n",
      "Epoch 2 step 207: training accuarcy: 0.6935\n",
      "Epoch 2 step 207: training loss: 1687.5781621202282\n",
      "Epoch 2 step 208: training accuarcy: 0.6845\n",
      "Epoch 2 step 208: training loss: 1629.7603742191109\n",
      "Epoch 2 step 209: training accuarcy: 0.708\n",
      "Epoch 2 step 209: training loss: 1642.9945447943549\n",
      "Epoch 2 step 210: training accuarcy: 0.6955\n",
      "Epoch 2 step 210: training loss: 1630.3676544777136\n",
      "Epoch 2 step 211: training accuarcy: 0.7045\n",
      "Epoch 2 step 211: training loss: 1601.6134676706306\n",
      "Epoch 2 step 212: training accuarcy: 0.7105\n",
      "Epoch 2 step 212: training loss: 1666.3793124049052\n",
      "Epoch 2 step 213: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 213: training loss: 1627.9965359611301\n",
      "Epoch 2 step 214: training accuarcy: 0.6965\n",
      "Epoch 2 step 214: training loss: 1623.3879466340343\n",
      "Epoch 2 step 215: training accuarcy: 0.6925\n",
      "Epoch 2 step 215: training loss: 1658.83201731049\n",
      "Epoch 2 step 216: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 216: training loss: 1623.8499941234381\n",
      "Epoch 2 step 217: training accuarcy: 0.6900000000000001\n",
      "Epoch 2 step 217: training loss: 1629.9626536215706\n",
      "Epoch 2 step 218: training accuarcy: 0.685\n",
      "Epoch 2 step 218: training loss: 1605.0811868857827\n",
      "Epoch 2 step 219: training accuarcy: 0.6945\n",
      "Epoch 2 step 219: training loss: 1616.8091608073291\n",
      "Epoch 2 step 220: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 220: training loss: 1610.6603658657882\n",
      "Epoch 2 step 221: training accuarcy: 0.686\n",
      "Epoch 2 step 221: training loss: 1646.4147683391336\n",
      "Epoch 2 step 222: training accuarcy: 0.67\n",
      "Epoch 2 step 222: training loss: 1590.1879000472957\n",
      "Epoch 2 step 223: training accuarcy: 0.6925\n",
      "Epoch 2 step 223: training loss: 1595.9798244579324\n",
      "Epoch 2 step 224: training accuarcy: 0.682\n",
      "Epoch 2 step 224: training loss: 1590.3924594507025\n",
      "Epoch 2 step 225: training accuarcy: 0.6950000000000001\n",
      "Epoch 2 step 225: training loss: 1560.6894297993883\n",
      "Epoch 2 step 226: training accuarcy: 0.6995\n",
      "Epoch 2 step 226: training loss: 1607.4024266230745\n",
      "Epoch 2 step 227: training accuarcy: 0.672\n",
      "Epoch 2 step 227: training loss: 1635.9863524991924\n",
      "Epoch 2 step 228: training accuarcy: 0.674\n",
      "Epoch 2 step 228: training loss: 1583.2095298659415\n",
      "Epoch 2 step 229: training accuarcy: 0.683\n",
      "Epoch 2 step 229: training loss: 1619.3203023176407\n",
      "Epoch 2 step 230: training accuarcy: 0.6705\n",
      "Epoch 2 step 230: training loss: 1530.1639282084202\n",
      "Epoch 2 step 231: training accuarcy: 0.7015\n",
      "Epoch 2 step 231: training loss: 1583.1511603555596\n",
      "Epoch 2 step 232: training accuarcy: 0.6815\n",
      "Epoch 2 step 232: training loss: 1581.6467899117827\n",
      "Epoch 2 step 233: training accuarcy: 0.6865\n",
      "Epoch 2 step 233: training loss: 1561.9553880161016\n",
      "Epoch 2 step 234: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 234: training loss: 1616.055786909191\n",
      "Epoch 2 step 235: training accuarcy: 0.6735\n",
      "Epoch 2 step 235: training loss: 1607.4615401654341\n",
      "Epoch 2 step 236: training accuarcy: 0.6765\n",
      "Epoch 2 step 236: training loss: 1573.876823075628\n",
      "Epoch 2 step 237: training accuarcy: 0.6665\n",
      "Epoch 2 step 237: training loss: 1566.5921120038056\n",
      "Epoch 2 step 238: training accuarcy: 0.6855\n",
      "Epoch 2 step 238: training loss: 1622.110301682093\n",
      "Epoch 2 step 239: training accuarcy: 0.642\n",
      "Epoch 2 step 239: training loss: 1575.1649524783086\n",
      "Epoch 2 step 240: training accuarcy: 0.671\n",
      "Epoch 2 step 240: training loss: 1632.9194181069129\n",
      "Epoch 2 step 241: training accuarcy: 0.65\n",
      "Epoch 2 step 241: training loss: 1558.0046765263119\n",
      "Epoch 2 step 242: training accuarcy: 0.678\n",
      "Epoch 2 step 242: training loss: 1599.0210287280263\n",
      "Epoch 2 step 243: training accuarcy: 0.667\n",
      "Epoch 2 step 243: training loss: 1555.9898225756083\n",
      "Epoch 2 step 244: training accuarcy: 0.6775\n",
      "Epoch 2 step 244: training loss: 1624.6059917878736\n",
      "Epoch 2 step 245: training accuarcy: 0.6575\n",
      "Epoch 2 step 245: training loss: 1551.3301998630918\n",
      "Epoch 2 step 246: training accuarcy: 0.682\n",
      "Epoch 2 step 246: training loss: 1603.3259062465595\n",
      "Epoch 2 step 247: training accuarcy: 0.6525\n",
      "Epoch 2 step 247: training loss: 1574.518958637766\n",
      "Epoch 2 step 248: training accuarcy: 0.6675\n",
      "Epoch 2 step 248: training loss: 1586.706549201428\n",
      "Epoch 2 step 249: training accuarcy: 0.658\n",
      "Epoch 2 step 249: training loss: 1565.4770807980995\n",
      "Epoch 2 step 250: training accuarcy: 0.666\n",
      "Epoch 2 step 250: training loss: 1548.4262604772562\n",
      "Epoch 2 step 251: training accuarcy: 0.6725\n",
      "Epoch 2 step 251: training loss: 1570.6080862595313\n",
      "Epoch 2 step 252: training accuarcy: 0.6685\n",
      "Epoch 2 step 252: training loss: 1561.359823697626\n",
      "Epoch 2 step 253: training accuarcy: 0.67\n",
      "Epoch 2 step 253: training loss: 1581.8381519879908\n",
      "Epoch 2 step 254: training accuarcy: 0.6595\n",
      "Epoch 2 step 254: training loss: 1574.9514871867495\n",
      "Epoch 2 step 255: training accuarcy: 0.6645\n",
      "Epoch 2 step 255: training loss: 1520.0464167545829\n",
      "Epoch 2 step 256: training accuarcy: 0.6715\n",
      "Epoch 2 step 256: training loss: 1600.977824673124\n",
      "Epoch 2 step 257: training accuarcy: 0.6515\n",
      "Epoch 2 step 257: training loss: 1526.7189524386858\n",
      "Epoch 2 step 258: training accuarcy: 0.68\n",
      "Epoch 2 step 258: training loss: 1571.4865251758129\n",
      "Epoch 2 step 259: training accuarcy: 0.666\n",
      "Epoch 2 step 259: training loss: 1559.4004628029584\n",
      "Epoch 2 step 260: training accuarcy: 0.6685\n",
      "Epoch 2 step 260: training loss: 1554.6714147025689\n",
      "Epoch 2 step 261: training accuarcy: 0.6615\n",
      "Epoch 2 step 261: training loss: 1543.399840136677\n",
      "Epoch 2 step 262: training accuarcy: 0.6445\n",
      "Epoch 2 step 262: training loss: 1540.9545496454107\n",
      "Epoch 2 step 263: training accuarcy: 0.6675\n",
      "Epoch 2 step 263: training loss: 1533.9365990660617\n",
      "Epoch 2 step 264: training accuarcy: 0.6795\n",
      "Epoch 2 step 264: training loss: 1562.5167528082209\n",
      "Epoch 2 step 265: training accuarcy: 0.657\n",
      "Epoch 2 step 265: training loss: 1562.011765209154\n",
      "Epoch 2 step 266: training accuarcy: 0.6655\n",
      "Epoch 2 step 266: training loss: 1556.957180089257\n",
      "Epoch 2 step 267: training accuarcy: 0.6685\n",
      "Epoch 2 step 267: training loss: 1536.2894406929436\n",
      "Epoch 2 step 268: training accuarcy: 0.6485\n",
      "Epoch 2 step 268: training loss: 1576.5096627607315\n",
      "Epoch 2 step 269: training accuarcy: 0.6485\n",
      "Epoch 2 step 269: training loss: 1580.205857199674\n",
      "Epoch 2 step 270: training accuarcy: 0.641\n",
      "Epoch 2 step 270: training loss: 1533.2220720127145\n",
      "Epoch 2 step 271: training accuarcy: 0.6545\n",
      "Epoch 2 step 271: training loss: 1520.8135389140762\n",
      "Epoch 2 step 272: training accuarcy: 0.6605\n",
      "Epoch 2 step 272: training loss: 1598.2603007219636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 273: training accuarcy: 0.646\n",
      "Epoch 2 step 273: training loss: 1561.863964587468\n",
      "Epoch 2 step 274: training accuarcy: 0.65\n",
      "Epoch 2 step 274: training loss: 1541.5556917116032\n",
      "Epoch 2 step 275: training accuarcy: 0.6435\n",
      "Epoch 2 step 275: training loss: 1577.821052203236\n",
      "Epoch 2 step 276: training accuarcy: 0.635\n",
      "Epoch 2 step 276: training loss: 1558.0946636622025\n",
      "Epoch 2 step 277: training accuarcy: 0.6535\n",
      "Epoch 2 step 277: training loss: 1596.557529931368\n",
      "Epoch 2 step 278: training accuarcy: 0.638\n",
      "Epoch 2 step 278: training loss: 1540.178858296881\n",
      "Epoch 2 step 279: training accuarcy: 0.664\n",
      "Epoch 2 step 279: training loss: 1579.7071292522478\n",
      "Epoch 2 step 280: training accuarcy: 0.646\n",
      "Epoch 2 step 280: training loss: 1606.7505428627733\n",
      "Epoch 2 step 281: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 281: training loss: 1281.4635091059472\n",
      "Epoch 2 step 282: training accuarcy: 0.6384071933204881\n",
      "Epoch 2: train loss 1608.236884438077, train accuarcy 0.675064742565155\n",
      "Epoch 2: valid loss 8510.100525913602, valid accuarcy 0.48202767968177795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                     | 3/5 [04:33<03:02, 91.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 282: training loss: 1321.837596538467\n",
      "Epoch 3 step 283: training accuarcy: 0.7255\n",
      "Epoch 3 step 283: training loss: 1325.6472751078177\n",
      "Epoch 3 step 284: training accuarcy: 0.74\n",
      "Epoch 3 step 284: training loss: 1327.626364497246\n",
      "Epoch 3 step 285: training accuarcy: 0.733\n",
      "Epoch 3 step 285: training loss: 1365.6448296152348\n",
      "Epoch 3 step 286: training accuarcy: 0.7295\n",
      "Epoch 3 step 286: training loss: 1354.4467639057405\n",
      "Epoch 3 step 287: training accuarcy: 0.726\n",
      "Epoch 3 step 287: training loss: 1329.9979958535655\n",
      "Epoch 3 step 288: training accuarcy: 0.7375\n",
      "Epoch 3 step 288: training loss: 1302.4285655950382\n",
      "Epoch 3 step 289: training accuarcy: 0.7335\n",
      "Epoch 3 step 289: training loss: 1352.3322442710369\n",
      "Epoch 3 step 290: training accuarcy: 0.725\n",
      "Epoch 3 step 290: training loss: 1366.7640848759622\n",
      "Epoch 3 step 291: training accuarcy: 0.7205\n",
      "Epoch 3 step 291: training loss: 1316.3711810879238\n",
      "Epoch 3 step 292: training accuarcy: 0.7365\n",
      "Epoch 3 step 292: training loss: 1341.3081919144179\n",
      "Epoch 3 step 293: training accuarcy: 0.7255\n",
      "Epoch 3 step 293: training loss: 1307.2686467718436\n",
      "Epoch 3 step 294: training accuarcy: 0.744\n",
      "Epoch 3 step 294: training loss: 1328.9766074886365\n",
      "Epoch 3 step 295: training accuarcy: 0.7255\n",
      "Epoch 3 step 295: training loss: 1334.4564737308579\n",
      "Epoch 3 step 296: training accuarcy: 0.728\n",
      "Epoch 3 step 296: training loss: 1304.4591456268536\n",
      "Epoch 3 step 297: training accuarcy: 0.744\n",
      "Epoch 3 step 297: training loss: 1337.2647773025574\n",
      "Epoch 3 step 298: training accuarcy: 0.7375\n",
      "Epoch 3 step 298: training loss: 1331.3521213370582\n",
      "Epoch 3 step 299: training accuarcy: 0.724\n",
      "Epoch 3 step 299: training loss: 1268.3211736293879\n",
      "Epoch 3 step 300: training accuarcy: 0.7335\n",
      "Epoch 3 step 300: training loss: 1339.9273225848187\n",
      "Epoch 3 step 301: training accuarcy: 0.735\n",
      "Epoch 3 step 301: training loss: 1327.7059111727197\n",
      "Epoch 3 step 302: training accuarcy: 0.733\n",
      "Epoch 3 step 302: training loss: 1328.4394380380063\n",
      "Epoch 3 step 303: training accuarcy: 0.7275\n",
      "Epoch 3 step 303: training loss: 1365.4777736172346\n",
      "Epoch 3 step 304: training accuarcy: 0.7085\n",
      "Epoch 3 step 304: training loss: 1329.5177871877113\n",
      "Epoch 3 step 305: training accuarcy: 0.7215\n",
      "Epoch 3 step 305: training loss: 1322.8299814140912\n",
      "Epoch 3 step 306: training accuarcy: 0.717\n",
      "Epoch 3 step 306: training loss: 1403.4599370935207\n",
      "Epoch 3 step 307: training accuarcy: 0.6975\n",
      "Epoch 3 step 307: training loss: 1339.1098165460357\n",
      "Epoch 3 step 308: training accuarcy: 0.717\n",
      "Epoch 3 step 308: training loss: 1357.7541909234922\n",
      "Epoch 3 step 309: training accuarcy: 0.7225\n",
      "Epoch 3 step 309: training loss: 1365.2640119846596\n",
      "Epoch 3 step 310: training accuarcy: 0.7205\n",
      "Epoch 3 step 310: training loss: 1346.9409501981852\n",
      "Epoch 3 step 311: training accuarcy: 0.714\n",
      "Epoch 3 step 311: training loss: 1361.8655676374397\n",
      "Epoch 3 step 312: training accuarcy: 0.7145\n",
      "Epoch 3 step 312: training loss: 1311.3085160476428\n",
      "Epoch 3 step 313: training accuarcy: 0.721\n",
      "Epoch 3 step 313: training loss: 1356.315222453835\n",
      "Epoch 3 step 314: training accuarcy: 0.7095\n",
      "Epoch 3 step 314: training loss: 1335.9185891724833\n",
      "Epoch 3 step 315: training accuarcy: 0.716\n",
      "Epoch 3 step 315: training loss: 1382.432361155742\n",
      "Epoch 3 step 316: training accuarcy: 0.719\n",
      "Epoch 3 step 316: training loss: 1376.1294486443855\n",
      "Epoch 3 step 317: training accuarcy: 0.706\n",
      "Epoch 3 step 317: training loss: 1372.7710173800754\n",
      "Epoch 3 step 318: training accuarcy: 0.706\n",
      "Epoch 3 step 318: training loss: 1346.9879896931575\n",
      "Epoch 3 step 319: training accuarcy: 0.717\n",
      "Epoch 3 step 319: training loss: 1367.7570824989662\n",
      "Epoch 3 step 320: training accuarcy: 0.7065\n",
      "Epoch 3 step 320: training loss: 1359.8714812214857\n",
      "Epoch 3 step 321: training accuarcy: 0.7005\n",
      "Epoch 3 step 321: training loss: 1308.8083530060135\n",
      "Epoch 3 step 322: training accuarcy: 0.7165\n",
      "Epoch 3 step 322: training loss: 1359.3254493347652\n",
      "Epoch 3 step 323: training accuarcy: 0.7005\n",
      "Epoch 3 step 323: training loss: 1324.666952406414\n",
      "Epoch 3 step 324: training accuarcy: 0.721\n",
      "Epoch 3 step 324: training loss: 1384.722778449728\n",
      "Epoch 3 step 325: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 325: training loss: 1444.3371453277362\n",
      "Epoch 3 step 326: training accuarcy: 0.6725\n",
      "Epoch 3 step 326: training loss: 1301.9517779568741\n",
      "Epoch 3 step 327: training accuarcy: 0.7355\n",
      "Epoch 3 step 327: training loss: 1333.7031385963367\n",
      "Epoch 3 step 328: training accuarcy: 0.7095\n",
      "Epoch 3 step 328: training loss: 1368.8085823598096\n",
      "Epoch 3 step 329: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 329: training loss: 1393.8606396863304\n",
      "Epoch 3 step 330: training accuarcy: 0.6940000000000001\n",
      "Epoch 3 step 330: training loss: 1371.070536606411\n",
      "Epoch 3 step 331: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 331: training loss: 1415.9834335142223\n",
      "Epoch 3 step 332: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 332: training loss: 1385.9285304481784\n",
      "Epoch 3 step 333: training accuarcy: 0.7015\n",
      "Epoch 3 step 333: training loss: 1330.5686713743703\n",
      "Epoch 3 step 334: training accuarcy: 0.7055\n",
      "Epoch 3 step 334: training loss: 1342.1169162939877\n",
      "Epoch 3 step 335: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 335: training loss: 1349.8179965719246\n",
      "Epoch 3 step 336: training accuarcy: 0.7105\n",
      "Epoch 3 step 336: training loss: 1368.5504265237576\n",
      "Epoch 3 step 337: training accuarcy: 0.7005\n",
      "Epoch 3 step 337: training loss: 1414.074354613686\n",
      "Epoch 3 step 338: training accuarcy: 0.6815\n",
      "Epoch 3 step 338: training loss: 1419.7908201009232\n",
      "Epoch 3 step 339: training accuarcy: 0.6825\n",
      "Epoch 3 step 339: training loss: 1372.2216645375786\n",
      "Epoch 3 step 340: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 340: training loss: 1384.9942323618145\n",
      "Epoch 3 step 341: training accuarcy: 0.6915\n",
      "Epoch 3 step 341: training loss: 1382.9033528819482\n",
      "Epoch 3 step 342: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 342: training loss: 1485.4994174007181\n",
      "Epoch 3 step 343: training accuarcy: 0.6705\n",
      "Epoch 3 step 343: training loss: 1369.227567740316\n",
      "Epoch 3 step 344: training accuarcy: 0.7025\n",
      "Epoch 3 step 344: training loss: 1374.0723879068607\n",
      "Epoch 3 step 345: training accuarcy: 0.6895\n",
      "Epoch 3 step 345: training loss: 1444.7456667859642\n",
      "Epoch 3 step 346: training accuarcy: 0.667\n",
      "Epoch 3 step 346: training loss: 1399.587338898734\n",
      "Epoch 3 step 347: training accuarcy: 0.6885\n",
      "Epoch 3 step 347: training loss: 1397.448753624001\n",
      "Epoch 3 step 348: training accuarcy: 0.686\n",
      "Epoch 3 step 348: training loss: 1383.066183378329\n",
      "Epoch 3 step 349: training accuarcy: 0.6865\n",
      "Epoch 3 step 349: training loss: 1387.3497738318497\n",
      "Epoch 3 step 350: training accuarcy: 0.683\n",
      "Epoch 3 step 350: training loss: 1384.8944890407993\n",
      "Epoch 3 step 351: training accuarcy: 0.674\n",
      "Epoch 3 step 351: training loss: 1387.97387823786\n",
      "Epoch 3 step 352: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 352: training loss: 1443.0457471832633\n",
      "Epoch 3 step 353: training accuarcy: 0.664\n",
      "Epoch 3 step 353: training loss: 1413.714984966562\n",
      "Epoch 3 step 354: training accuarcy: 0.6785\n",
      "Epoch 3 step 354: training loss: 1417.0788490910127\n",
      "Epoch 3 step 355: training accuarcy: 0.6865\n",
      "Epoch 3 step 355: training loss: 1412.221758168567\n",
      "Epoch 3 step 356: training accuarcy: 0.685\n",
      "Epoch 3 step 356: training loss: 1418.3346477914165\n",
      "Epoch 3 step 357: training accuarcy: 0.6715\n",
      "Epoch 3 step 357: training loss: 1402.5626929492116\n",
      "Epoch 3 step 358: training accuarcy: 0.682\n",
      "Epoch 3 step 358: training loss: 1422.975034125669\n",
      "Epoch 3 step 359: training accuarcy: 0.6745\n",
      "Epoch 3 step 359: training loss: 1423.744588089859\n",
      "Epoch 3 step 360: training accuarcy: 0.6795\n",
      "Epoch 3 step 360: training loss: 1420.8837601908442\n",
      "Epoch 3 step 361: training accuarcy: 0.678\n",
      "Epoch 3 step 361: training loss: 1452.5941949584812\n",
      "Epoch 3 step 362: training accuarcy: 0.6735\n",
      "Epoch 3 step 362: training loss: 1477.2599515251936\n",
      "Epoch 3 step 363: training accuarcy: 0.662\n",
      "Epoch 3 step 363: training loss: 1445.378926018273\n",
      "Epoch 3 step 364: training accuarcy: 0.6595\n",
      "Epoch 3 step 364: training loss: 1394.4523476493835\n",
      "Epoch 3 step 365: training accuarcy: 0.6915\n",
      "Epoch 3 step 365: training loss: 1429.3223346792581\n",
      "Epoch 3 step 366: training accuarcy: 0.6735\n",
      "Epoch 3 step 366: training loss: 1428.8455288950418\n",
      "Epoch 3 step 367: training accuarcy: 0.6755\n",
      "Epoch 3 step 367: training loss: 1397.0506210000024\n",
      "Epoch 3 step 368: training accuarcy: 0.6915\n",
      "Epoch 3 step 368: training loss: 1435.8244665018701\n",
      "Epoch 3 step 369: training accuarcy: 0.6775\n",
      "Epoch 3 step 369: training loss: 1444.0892413940403\n",
      "Epoch 3 step 370: training accuarcy: 0.667\n",
      "Epoch 3 step 370: training loss: 1434.6012537319311\n",
      "Epoch 3 step 371: training accuarcy: 0.6745\n",
      "Epoch 3 step 371: training loss: 1412.884821039745\n",
      "Epoch 3 step 372: training accuarcy: 0.6675\n",
      "Epoch 3 step 372: training loss: 1430.8246259040718\n",
      "Epoch 3 step 373: training accuarcy: 0.6785\n",
      "Epoch 3 step 373: training loss: 1410.7091286243613\n",
      "Epoch 3 step 374: training accuarcy: 0.669\n",
      "Epoch 3 step 374: training loss: 1427.7443814609771\n",
      "Epoch 3 step 375: training accuarcy: 0.6745\n",
      "Epoch 3 step 375: training loss: 1176.7318647592126\n",
      "Epoch 3 step 376: training accuarcy: 0.6685934489402697\n",
      "Epoch 3: train loss 1372.9705893224657, train accuarcy 0.6972758769989014\n",
      "Epoch 3: valid loss 8790.263220420667, valid accuarcy 0.4708211421966553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                  | 4/5 [06:06<01:31, 91.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 376: training loss: 1193.7661577366157\n",
      "Epoch 4 step 377: training accuarcy: 0.7565000000000001\n",
      "Epoch 4 step 377: training loss: 1213.703070783885\n",
      "Epoch 4 step 378: training accuarcy: 0.763\n",
      "Epoch 4 step 378: training loss: 1246.8831791309726\n",
      "Epoch 4 step 379: training accuarcy: 0.7415\n",
      "Epoch 4 step 379: training loss: 1194.7704351348823\n",
      "Epoch 4 step 380: training accuarcy: 0.753\n",
      "Epoch 4 step 380: training loss: 1239.4857555369022\n",
      "Epoch 4 step 381: training accuarcy: 0.7515000000000001\n",
      "Epoch 4 step 381: training loss: 1221.842278153663\n",
      "Epoch 4 step 382: training accuarcy: 0.7495\n",
      "Epoch 4 step 382: training loss: 1194.8138116524128\n",
      "Epoch 4 step 383: training accuarcy: 0.759\n",
      "Epoch 4 step 383: training loss: 1240.3861053817477\n",
      "Epoch 4 step 384: training accuarcy: 0.7465\n",
      "Epoch 4 step 384: training loss: 1234.7102872370465\n",
      "Epoch 4 step 385: training accuarcy: 0.7515000000000001\n",
      "Epoch 4 step 385: training loss: 1246.9405335407016\n",
      "Epoch 4 step 386: training accuarcy: 0.755\n",
      "Epoch 4 step 386: training loss: 1206.2680920664825\n",
      "Epoch 4 step 387: training accuarcy: 0.7615000000000001\n",
      "Epoch 4 step 387: training loss: 1236.6476618320776\n",
      "Epoch 4 step 388: training accuarcy: 0.7355\n",
      "Epoch 4 step 388: training loss: 1214.7548304709887\n",
      "Epoch 4 step 389: training accuarcy: 0.743\n",
      "Epoch 4 step 389: training loss: 1254.1277006198038\n",
      "Epoch 4 step 390: training accuarcy: 0.738\n",
      "Epoch 4 step 390: training loss: 1230.513964716206\n",
      "Epoch 4 step 391: training accuarcy: 0.7405\n",
      "Epoch 4 step 391: training loss: 1230.8383365758305\n",
      "Epoch 4 step 392: training accuarcy: 0.745\n",
      "Epoch 4 step 392: training loss: 1274.0241720895146\n",
      "Epoch 4 step 393: training accuarcy: 0.738\n",
      "Epoch 4 step 393: training loss: 1258.6303283613881\n",
      "Epoch 4 step 394: training accuarcy: 0.741\n",
      "Epoch 4 step 394: training loss: 1254.0340194983137\n",
      "Epoch 4 step 395: training accuarcy: 0.741\n",
      "Epoch 4 step 395: training loss: 1252.787614684506\n",
      "Epoch 4 step 396: training accuarcy: 0.7445\n",
      "Epoch 4 step 396: training loss: 1241.3300367573631\n",
      "Epoch 4 step 397: training accuarcy: 0.742\n",
      "Epoch 4 step 397: training loss: 1232.2797892176798\n",
      "Epoch 4 step 398: training accuarcy: 0.746\n",
      "Epoch 4 step 398: training loss: 1252.2954913472672\n",
      "Epoch 4 step 399: training accuarcy: 0.7335\n",
      "Epoch 4 step 399: training loss: 1269.4924316806346\n",
      "Epoch 4 step 400: training accuarcy: 0.7265\n",
      "Epoch 4 step 400: training loss: 1273.407740038942\n",
      "Epoch 4 step 401: training accuarcy: 0.731\n",
      "Epoch 4 step 401: training loss: 1268.0231644714033\n",
      "Epoch 4 step 402: training accuarcy: 0.723\n",
      "Epoch 4 step 402: training loss: 1339.806615610294\n",
      "Epoch 4 step 403: training accuarcy: 0.713\n",
      "Epoch 4 step 403: training loss: 1278.7600192803998\n",
      "Epoch 4 step 404: training accuarcy: 0.7225\n",
      "Epoch 4 step 404: training loss: 1249.956976135684\n",
      "Epoch 4 step 405: training accuarcy: 0.7455\n",
      "Epoch 4 step 405: training loss: 1256.0187998116203\n",
      "Epoch 4 step 406: training accuarcy: 0.7355\n",
      "Epoch 4 step 406: training loss: 1263.6366211945312\n",
      "Epoch 4 step 407: training accuarcy: 0.7295\n",
      "Epoch 4 step 407: training loss: 1270.330835825843\n",
      "Epoch 4 step 408: training accuarcy: 0.727\n",
      "Epoch 4 step 408: training loss: 1270.998244035325\n",
      "Epoch 4 step 409: training accuarcy: 0.731\n",
      "Epoch 4 step 409: training loss: 1294.655253645622\n",
      "Epoch 4 step 410: training accuarcy: 0.7145\n",
      "Epoch 4 step 410: training loss: 1251.5591636093834\n",
      "Epoch 4 step 411: training accuarcy: 0.7325\n",
      "Epoch 4 step 411: training loss: 1228.9737396746063\n",
      "Epoch 4 step 412: training accuarcy: 0.7525000000000001\n",
      "Epoch 4 step 412: training loss: 1310.1831730426165\n",
      "Epoch 4 step 413: training accuarcy: 0.716\n",
      "Epoch 4 step 413: training loss: 1250.8488452233287\n",
      "Epoch 4 step 414: training accuarcy: 0.731\n",
      "Epoch 4 step 414: training loss: 1282.1934118629329\n",
      "Epoch 4 step 415: training accuarcy: 0.7325\n",
      "Epoch 4 step 415: training loss: 1284.8835544347166\n",
      "Epoch 4 step 416: training accuarcy: 0.715\n",
      "Epoch 4 step 416: training loss: 1283.3918942024961\n",
      "Epoch 4 step 417: training accuarcy: 0.717\n",
      "Epoch 4 step 417: training loss: 1280.9032900823138\n",
      "Epoch 4 step 418: training accuarcy: 0.7165\n",
      "Epoch 4 step 418: training loss: 1265.8324547450006\n",
      "Epoch 4 step 419: training accuarcy: 0.728\n",
      "Epoch 4 step 419: training loss: 1294.5591921073358\n",
      "Epoch 4 step 420: training accuarcy: 0.7115\n",
      "Epoch 4 step 420: training loss: 1320.2242856066807\n",
      "Epoch 4 step 421: training accuarcy: 0.7085\n",
      "Epoch 4 step 421: training loss: 1285.583141597847\n",
      "Epoch 4 step 422: training accuarcy: 0.7225\n",
      "Epoch 4 step 422: training loss: 1283.0743839680194\n",
      "Epoch 4 step 423: training accuarcy: 0.7335\n",
      "Epoch 4 step 423: training loss: 1276.0521409635007\n",
      "Epoch 4 step 424: training accuarcy: 0.723\n",
      "Epoch 4 step 424: training loss: 1267.0291542671482\n",
      "Epoch 4 step 425: training accuarcy: 0.723\n",
      "Epoch 4 step 425: training loss: 1293.828381384058\n",
      "Epoch 4 step 426: training accuarcy: 0.7205\n",
      "Epoch 4 step 426: training loss: 1293.963484484681\n",
      "Epoch 4 step 427: training accuarcy: 0.7195\n",
      "Epoch 4 step 427: training loss: 1277.3381123374236\n",
      "Epoch 4 step 428: training accuarcy: 0.722\n",
      "Epoch 4 step 428: training loss: 1268.7166570938273\n",
      "Epoch 4 step 429: training accuarcy: 0.728\n",
      "Epoch 4 step 429: training loss: 1301.8848413632\n",
      "Epoch 4 step 430: training accuarcy: 0.72\n",
      "Epoch 4 step 430: training loss: 1289.6018307669324\n",
      "Epoch 4 step 431: training accuarcy: 0.717\n",
      "Epoch 4 step 431: training loss: 1300.2075109848388\n",
      "Epoch 4 step 432: training accuarcy: 0.7185\n",
      "Epoch 4 step 432: training loss: 1300.5719376942943\n",
      "Epoch 4 step 433: training accuarcy: 0.7145\n",
      "Epoch 4 step 433: training loss: 1267.577826007217\n",
      "Epoch 4 step 434: training accuarcy: 0.72\n",
      "Epoch 4 step 434: training loss: 1291.9441580777216\n",
      "Epoch 4 step 435: training accuarcy: 0.7185\n",
      "Epoch 4 step 435: training loss: 1364.5617291008755\n",
      "Epoch 4 step 436: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 436: training loss: 1299.4618612490722\n",
      "Epoch 4 step 437: training accuarcy: 0.7135\n",
      "Epoch 4 step 437: training loss: 1295.1347862667808\n",
      "Epoch 4 step 438: training accuarcy: 0.7215\n",
      "Epoch 4 step 438: training loss: 1317.2987949313913\n",
      "Epoch 4 step 439: training accuarcy: 0.705\n",
      "Epoch 4 step 439: training loss: 1281.3745912760562\n",
      "Epoch 4 step 440: training accuarcy: 0.713\n",
      "Epoch 4 step 440: training loss: 1352.490790069305\n",
      "Epoch 4 step 441: training accuarcy: 0.6995\n",
      "Epoch 4 step 441: training loss: 1337.1725819962962\n",
      "Epoch 4 step 442: training accuarcy: 0.7205\n",
      "Epoch 4 step 442: training loss: 1283.001848885663\n",
      "Epoch 4 step 443: training accuarcy: 0.724\n",
      "Epoch 4 step 443: training loss: 1304.0441623284448\n",
      "Epoch 4 step 444: training accuarcy: 0.7155\n",
      "Epoch 4 step 444: training loss: 1296.209174092267\n",
      "Epoch 4 step 445: training accuarcy: 0.7075\n",
      "Epoch 4 step 445: training loss: 1335.2512169532492\n",
      "Epoch 4 step 446: training accuarcy: 0.6965\n",
      "Epoch 4 step 446: training loss: 1330.285762535652\n",
      "Epoch 4 step 447: training accuarcy: 0.7025\n",
      "Epoch 4 step 447: training loss: 1328.9116275432364\n",
      "Epoch 4 step 448: training accuarcy: 0.7075\n",
      "Epoch 4 step 448: training loss: 1334.100737799286\n",
      "Epoch 4 step 449: training accuarcy: 0.7065\n",
      "Epoch 4 step 449: training loss: 1349.3238627681833\n",
      "Epoch 4 step 450: training accuarcy: 0.6885\n",
      "Epoch 4 step 450: training loss: 1330.3989373020663\n",
      "Epoch 4 step 451: training accuarcy: 0.705\n",
      "Epoch 4 step 451: training loss: 1367.5869151219113\n",
      "Epoch 4 step 452: training accuarcy: 0.683\n",
      "Epoch 4 step 452: training loss: 1361.914033631989\n",
      "Epoch 4 step 453: training accuarcy: 0.6945\n",
      "Epoch 4 step 453: training loss: 1308.891119104715\n",
      "Epoch 4 step 454: training accuarcy: 0.7045\n",
      "Epoch 4 step 454: training loss: 1305.2470969561887\n",
      "Epoch 4 step 455: training accuarcy: 0.7055\n",
      "Epoch 4 step 455: training loss: 1341.314885262457\n",
      "Epoch 4 step 456: training accuarcy: 0.7075\n",
      "Epoch 4 step 456: training loss: 1335.7925627107084\n",
      "Epoch 4 step 457: training accuarcy: 0.7185\n",
      "Epoch 4 step 457: training loss: 1360.5045254065087\n",
      "Epoch 4 step 458: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 458: training loss: 1340.326380567433\n",
      "Epoch 4 step 459: training accuarcy: 0.6905\n",
      "Epoch 4 step 459: training loss: 1386.8359941368672\n",
      "Epoch 4 step 460: training accuarcy: 0.6765\n",
      "Epoch 4 step 460: training loss: 1348.5383998287975\n",
      "Epoch 4 step 461: training accuarcy: 0.6955\n",
      "Epoch 4 step 461: training loss: 1330.8875686470883\n",
      "Epoch 4 step 462: training accuarcy: 0.704\n",
      "Epoch 4 step 462: training loss: 1314.5513767208479\n",
      "Epoch 4 step 463: training accuarcy: 0.71\n",
      "Epoch 4 step 463: training loss: 1375.8146519015047\n",
      "Epoch 4 step 464: training accuarcy: 0.677\n",
      "Epoch 4 step 464: training loss: 1338.0567453106914\n",
      "Epoch 4 step 465: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 465: training loss: 1364.8147460318457\n",
      "Epoch 4 step 466: training accuarcy: 0.686\n",
      "Epoch 4 step 466: training loss: 1370.9553362559113\n",
      "Epoch 4 step 467: training accuarcy: 0.683\n",
      "Epoch 4 step 467: training loss: 1330.5782898071946\n",
      "Epoch 4 step 468: training accuarcy: 0.7085\n",
      "Epoch 4 step 468: training loss: 1370.658540820457\n",
      "Epoch 4 step 469: training accuarcy: 0.6865\n",
      "Epoch 4 step 469: training loss: 1118.4205018059452\n",
      "Epoch 4 step 470: training accuarcy: 0.692999357739242\n",
      "Epoch 4: train loss 1286.8357346272928, train accuarcy 0.7182552218437195\n",
      "Epoch 4: valid loss 9131.22943040011, valid accuarcy 0.4572625458240509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [07:39<00:00, 92.05s/it]\n"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=5, log_dir=get_log_dir('seq_stackoverflow', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trans_model\n",
    "T.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
