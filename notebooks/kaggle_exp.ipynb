{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:15:45.917912Z",
     "start_time": "2019-10-08T05:15:45.696283Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:15:46.252498Z",
     "start_time": "2019-10-08T05:15:46.246466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\python\\recommender\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:15:47.116935Z",
     "start_time": "2019-10-08T05:15:46.694880Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import get_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:15:48.933884Z",
     "start_time": "2019-10-08T05:15:47.421766Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import SeqKaggle\n",
    "from models import FMLearner, TorchFM, TorchHrmFM, TorchPrmeFM, TorchTransFM\n",
    "from models.fm_learner import simple_loss, trans_loss, simple_weight_loss, trans_weight_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:15:49.155795Z",
     "start_time": "2019-10-08T05:15:48.934767Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = T.cuda.current_device()\n",
    "BATCH = 2000\n",
    "SHUFFLE = True\n",
    "WORKERS = 0\n",
    "NEG_SAMPLE = 5\n",
    "item_path = Path(\"./inputs/kaggle/item.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:15:55.142767Z",
     "start_time": "2019-10-08T05:15:49.157766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataframe shape (476244, 8)\n",
      "After drop nan shape: (429988, 8)\n",
      "Original comptition size: 292\n",
      "Original competitor size: 140065\n",
      "Filtered competiter size: 27449\n",
      "Filtered dataframe shape: (284806, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datasets.torch_kaggle.SeqKaggle at 0x1dbc0971f60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = SeqKaggle(data_path=item_path, user_min=4)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:15:55.147771Z",
     "start_time": "2019-10-08T05:15:55.144791Z"
    }
   },
   "outputs": [],
   "source": [
    "db.config_db(batch_size=BATCH,\n",
    "             shuffle=SHUFFLE,\n",
    "             num_workers=WORKERS,\n",
    "             device=DEVICE,\n",
    "             neg_sample=NEG_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:15:57.976504Z",
     "start_time": "2019-10-08T05:15:57.974504Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:15:59.025696Z",
     "start_time": "2019-10-08T05:15:59.021696Z"
    }
   },
   "outputs": [],
   "source": [
    "# regst setting\n",
    "LINEAR_REG = 1\n",
    "EMB_REG = 1\n",
    "TRANS_REG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:16:04.362832Z",
     "start_time": "2019-10-08T05:16:04.357836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_loss at 0x000001DBC091FC80>, 1, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_loss_callback = partial(simple_loss, LINEAR_REG, EMB_REG)\n",
    "simple_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:16:04.812514Z",
     "start_time": "2019-10-08T05:16:04.806486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_loss at 0x000001DBC097BEA0>, 1, 1, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_loss_callback = partial(trans_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:16:05.202894Z",
     "start_time": "2019-10-08T05:16:05.197896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_weight_loss at 0x000001DBC097BF28>, 1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weight_loss_callback = partial(simple_weight_loss, LINEAR_REG, EMB_REG)\n",
    "simple_weight_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:16:05.681506Z",
     "start_time": "2019-10-08T05:16:05.677477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_weight_loss at 0x000001DBC0982048>, 1, 1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_weight_loss_callback = partial(trans_weight_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_weight_loss_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "### Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:16:07.768689Z",
     "start_time": "2019-10-08T05:16:07.765717Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:16:12.033394Z",
     "start_time": "2019-10-08T05:16:12.030419Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DECAY_FREQ = 1000\n",
    "DECAY_GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T03:51:29.693078Z",
     "start_time": "2019-10-08T03:51:29.619078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchFM()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model = TorchFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T03:51:30.153075Z",
     "start_time": "2019-10-08T03:51:30.149078Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(fm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt, step_size=DECAY_FREQ, gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T03:51:30.755386Z",
     "start_time": "2019-10-08T03:51:30.725383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x186c49f7e80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_learner = FMLearner(fm_model, adam_opt, schedular, db)\n",
    "fm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:05:11.214953Z",
     "start_time": "2019-10-07T12:05:11.210952Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T03:28:25.963128Z",
     "start_time": "2019-10-08T03:28:25.959098Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='seq',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T03:51:50.226545Z",
     "start_time": "2019-10-08T03:51:50.223515Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='seq',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T04:14:57.850778Z",
     "start_time": "2019-10-08T03:52:43.246503Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 11795.005449829247\n",
      "Epoch 0 step 1: training accuarcy: 0.9019\n",
      "Epoch 0 step 1: training loss: 11262.761859629521\n",
      "Epoch 0 step 2: training accuarcy: 0.9268000000000001\n",
      "Epoch 0 step 2: training loss: 10840.89296971423\n",
      "Epoch 0 step 3: training accuarcy: 0.9382\n",
      "Epoch 0 step 3: training loss: 10407.067260047588\n",
      "Epoch 0 step 4: training accuarcy: 0.9475\n",
      "Epoch 0 step 4: training loss: 10139.016290808704\n",
      "Epoch 0 step 5: training accuarcy: 0.9457000000000001\n",
      "Epoch 0 step 5: training loss: 9820.742045991497\n",
      "Epoch 0 step 6: training accuarcy: 0.9499000000000001\n",
      "Epoch 0 step 6: training loss: 9497.25482930984\n",
      "Epoch 0 step 7: training accuarcy: 0.9580000000000001\n",
      "Epoch 0 step 7: training loss: 9192.469390586777\n",
      "Epoch 0 step 8: training accuarcy: 0.9617\n",
      "Epoch 0 step 8: training loss: 8897.902274778286\n",
      "Epoch 0 step 9: training accuarcy: 0.9686\n",
      "Epoch 0 step 9: training loss: 8644.504184018338\n",
      "Epoch 0 step 10: training accuarcy: 0.9700000000000001\n",
      "Epoch 0 step 10: training loss: 8368.427056298824\n",
      "Epoch 0 step 11: training accuarcy: 0.9769000000000001\n",
      "Epoch 0 step 11: training loss: 8141.329619821205\n",
      "Epoch 0 step 12: training accuarcy: 0.9753000000000001\n",
      "Epoch 0 step 12: training loss: 7851.025129997222\n",
      "Epoch 0 step 13: training accuarcy: 0.9821000000000001\n",
      "Epoch 0 step 13: training loss: 7651.774110209729\n",
      "Epoch 0 step 14: training accuarcy: 0.9798\n",
      "Epoch 0 step 14: training loss: 7421.155407040258\n",
      "Epoch 0 step 15: training accuarcy: 0.9831000000000001\n",
      "Epoch 0 step 15: training loss: 7239.069192131651\n",
      "Epoch 0 step 16: training accuarcy: 0.9815\n",
      "Epoch 0 step 16: training loss: 7022.281099664571\n",
      "Epoch 0 step 17: training accuarcy: 0.9818\n",
      "Epoch 0 step 17: training loss: 6794.348069099147\n",
      "Epoch 0 step 18: training accuarcy: 0.9856\n",
      "Epoch 0 step 18: training loss: 6614.838506407737\n",
      "Epoch 0 step 19: training accuarcy: 0.9853000000000001\n",
      "Epoch 0 step 19: training loss: 6408.72317208769\n",
      "Epoch 0 step 20: training accuarcy: 0.9878\n",
      "Epoch 0 step 20: training loss: 6235.388422914837\n",
      "Epoch 0 step 21: training accuarcy: 0.9867\n",
      "Epoch 0 step 21: training loss: 6072.340762904838\n",
      "Epoch 0 step 22: training accuarcy: 0.9860000000000001\n",
      "Epoch 0 step 22: training loss: 5877.9233336697425\n",
      "Epoch 0 step 23: training accuarcy: 0.9877\n",
      "Epoch 0 step 23: training loss: 5698.7077818470025\n",
      "Epoch 0 step 24: training accuarcy: 0.9896\n",
      "Epoch 0 step 24: training loss: 5548.678469304231\n",
      "Epoch 0 step 25: training accuarcy: 0.9899\n",
      "Epoch 0 step 25: training loss: 5400.5203795360885\n",
      "Epoch 0 step 26: training accuarcy: 0.9882000000000001\n",
      "Epoch 0 step 26: training loss: 5251.580093944838\n",
      "Epoch 0 step 27: training accuarcy: 0.9895\n",
      "Epoch 0 step 27: training loss: 5085.065332959275\n",
      "Epoch 0 step 28: training accuarcy: 0.9901000000000001\n",
      "Epoch 0 step 28: training loss: 4960.8138622830775\n",
      "Epoch 0 step 29: training accuarcy: 0.9885\n",
      "Epoch 0 step 29: training loss: 4796.310993776013\n",
      "Epoch 0 step 30: training accuarcy: 0.9905\n",
      "Epoch 0 step 30: training loss: 4657.3163272657275\n",
      "Epoch 0 step 31: training accuarcy: 0.9915\n",
      "Epoch 0 step 31: training loss: 4533.066525838547\n",
      "Epoch 0 step 32: training accuarcy: 0.9897\n",
      "Epoch 0 step 32: training loss: 4396.314780449532\n",
      "Epoch 0 step 33: training accuarcy: 0.9897\n",
      "Epoch 0 step 33: training loss: 4268.471266237452\n",
      "Epoch 0 step 34: training accuarcy: 0.991\n",
      "Epoch 0 step 34: training loss: 4125.349910143448\n",
      "Epoch 0 step 35: training accuarcy: 0.9928\n",
      "Epoch 0 step 35: training loss: 4026.2349871659158\n",
      "Epoch 0 step 36: training accuarcy: 0.9911000000000001\n",
      "Epoch 0 step 36: training loss: 3890.925951351217\n",
      "Epoch 0 step 37: training accuarcy: 0.9933000000000001\n",
      "Epoch 0 step 37: training loss: 3771.634940340444\n",
      "Epoch 0 step 38: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 38: training loss: 3668.728010223562\n",
      "Epoch 0 step 39: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 39: training loss: 3553.334765684442\n",
      "Epoch 0 step 40: training accuarcy: 0.9954000000000001\n",
      "Epoch 0 step 40: training loss: 3449.9971628724825\n",
      "Epoch 0 step 41: training accuarcy: 0.9944000000000001\n",
      "Epoch 0 step 41: training loss: 3349.113374156262\n",
      "Epoch 0 step 42: training accuarcy: 0.9949\n",
      "Epoch 0 step 42: training loss: 3270.2372518248185\n",
      "Epoch 0 step 43: training accuarcy: 0.9942000000000001\n",
      "Epoch 0 step 43: training loss: 3155.4825458321434\n",
      "Epoch 0 step 44: training accuarcy: 0.9954000000000001\n",
      "Epoch 0 step 44: training loss: 3067.5551476072924\n",
      "Epoch 0 step 45: training accuarcy: 0.9951000000000001\n",
      "Epoch 0 step 45: training loss: 2975.769620075986\n",
      "Epoch 0 step 46: training accuarcy: 0.996\n",
      "Epoch 0 step 46: training loss: 2880.5850192057364\n",
      "Epoch 0 step 47: training accuarcy: 0.9956\n",
      "Epoch 0 step 47: training loss: 2798.4954117449115\n",
      "Epoch 0 step 48: training accuarcy: 0.9964000000000001\n",
      "Epoch 0 step 48: training loss: 2697.4870078060585\n",
      "Epoch 0 step 49: training accuarcy: 0.998\n",
      "Epoch 0 step 49: training loss: 2620.217844311907\n",
      "Epoch 0 step 50: training accuarcy: 0.9977\n",
      "Epoch 0 step 50: training loss: 2547.5918540537505\n",
      "Epoch 0 step 51: training accuarcy: 0.9967\n",
      "Epoch 0 step 51: training loss: 2471.1654903266017\n",
      "Epoch 0 step 52: training accuarcy: 0.9974000000000001\n",
      "Epoch 0 step 52: training loss: 2407.7600657737066\n",
      "Epoch 0 step 53: training accuarcy: 0.9972000000000001\n",
      "Epoch 0 step 53: training loss: 2336.3002926026015\n",
      "Epoch 0 step 54: training accuarcy: 0.9963000000000001\n",
      "Epoch 0 step 54: training loss: 2259.599904946868\n",
      "Epoch 0 step 55: training accuarcy: 0.9975\n",
      "Epoch 0 step 55: training loss: 2177.1439724459424\n",
      "Epoch 0 step 56: training accuarcy: 0.9984000000000001\n",
      "Epoch 0 step 56: training loss: 2134.7693040324425\n",
      "Epoch 0 step 57: training accuarcy: 0.9969\n",
      "Epoch 0 step 57: training loss: 2057.7212756231543\n",
      "Epoch 0 step 58: training accuarcy: 0.9979\n",
      "Epoch 0 step 58: training loss: 1993.5187227661497\n",
      "Epoch 0 step 59: training accuarcy: 0.9979\n",
      "Epoch 0 step 59: training loss: 1939.9575884140672\n",
      "Epoch 0 step 60: training accuarcy: 0.9984000000000001\n",
      "Epoch 0 step 60: training loss: 1877.417309432073\n",
      "Epoch 0 step 61: training accuarcy: 0.9977\n",
      "Epoch 0 step 61: training loss: 1825.074793179438\n",
      "Epoch 0 step 62: training accuarcy: 0.998\n",
      "Epoch 0 step 62: training loss: 1781.2474464533589\n",
      "Epoch 0 step 63: training accuarcy: 0.9974000000000001\n",
      "Epoch 0 step 63: training loss: 1717.8810143246321\n",
      "Epoch 0 step 64: training accuarcy: 0.9977\n",
      "Epoch 0 step 64: training loss: 1658.4801906317955\n",
      "Epoch 0 step 65: training accuarcy: 0.9987\n",
      "Epoch 0 step 65: training loss: 1605.6197213652667\n",
      "Epoch 0 step 66: training accuarcy: 0.9988\n",
      "Epoch 0 step 66: training loss: 1562.3844710769383\n",
      "Epoch 0 step 67: training accuarcy: 0.9989\n",
      "Epoch 0 step 67: training loss: 1520.5850430657258\n",
      "Epoch 0 step 68: training accuarcy: 0.9985\n",
      "Epoch 0 step 68: training loss: 1483.296367955038\n",
      "Epoch 0 step 69: training accuarcy: 0.9976\n",
      "Epoch 0 step 69: training loss: 1431.4455274502177\n",
      "Epoch 0 step 70: training accuarcy: 0.9983000000000001\n",
      "Epoch 0 step 70: training loss: 1400.8979019256576\n",
      "Epoch 0 step 71: training accuarcy: 0.9977\n",
      "Epoch 0 step 71: training loss: 1340.2435041542533\n",
      "Epoch 0 step 72: training accuarcy: 0.9989\n",
      "Epoch 0 step 72: training loss: 1308.645074763821\n",
      "Epoch 0 step 73: training accuarcy: 0.9992000000000001\n",
      "Epoch 0 step 73: training loss: 1268.7372553894368\n",
      "Epoch 0 step 74: training accuarcy: 0.9988\n",
      "Epoch 0 step 74: training loss: 1234.7594486802889\n",
      "Epoch 0 step 75: training accuarcy: 0.9988\n",
      "Epoch 0 step 75: training loss: 1200.047579546307\n",
      "Epoch 0 step 76: training accuarcy: 0.9987\n",
      "Epoch 0 step 76: training loss: 1166.272719437265\n",
      "Epoch 0 step 77: training accuarcy: 0.999\n",
      "Epoch 0 step 77: training loss: 1122.3298632920737\n",
      "Epoch 0 step 78: training accuarcy: 0.9985\n",
      "Epoch 0 step 78: training loss: 1097.002962135172\n",
      "Epoch 0 step 79: training accuarcy: 0.999\n",
      "Epoch 0 step 79: training loss: 1064.5091412646425\n",
      "Epoch 0 step 80: training accuarcy: 0.9991000000000001\n",
      "Epoch 0 step 80: training loss: 1031.1226260901396\n",
      "Epoch 0 step 81: training accuarcy: 0.9986\n",
      "Epoch 0 step 81: training loss: 1006.2847301359795\n",
      "Epoch 0 step 82: training accuarcy: 0.9986\n",
      "Epoch 0 step 82: training loss: 971.4934603328379\n",
      "Epoch 0 step 83: training accuarcy: 0.9988\n",
      "Epoch 0 step 83: training loss: 950.6109691645407\n",
      "Epoch 0 step 84: training accuarcy: 0.9985\n",
      "Epoch 0 step 84: training loss: 919.6694945641259\n",
      "Epoch 0 step 85: training accuarcy: 0.9988\n",
      "Epoch 0 step 85: training loss: 890.0863008046371\n",
      "Epoch 0 step 86: training accuarcy: 0.9991000000000001\n",
      "Epoch 0 step 86: training loss: 872.1138075834208\n",
      "Epoch 0 step 87: training accuarcy: 0.9987\n",
      "Epoch 0 step 87: training loss: 840.2907468199068\n",
      "Epoch 0 step 88: training accuarcy: 0.9993000000000001\n",
      "Epoch 0 step 88: training loss: 821.1092108850708\n",
      "Epoch 0 step 89: training accuarcy: 0.999\n",
      "Epoch 0 step 89: training loss: 801.5036223036891\n",
      "Epoch 0 step 90: training accuarcy: 0.9992000000000001\n",
      "Epoch 0 step 90: training loss: 776.8312486676757\n",
      "Epoch 0 step 91: training accuarcy: 0.9989\n",
      "Epoch 0 step 91: training loss: 747.1119944539286\n",
      "Epoch 0 step 92: training accuarcy: 0.9997\n",
      "Epoch 0 step 92: training loss: 729.8521503493708\n",
      "Epoch 0 step 93: training accuarcy: 0.999\n",
      "Epoch 0 step 93: training loss: 703.0566365516524\n",
      "Epoch 0 step 94: training accuarcy: 0.9993000000000001\n",
      "Epoch 0 step 94: training loss: 692.2530718086352\n",
      "Epoch 0 step 95: training accuarcy: 0.9986\n",
      "Epoch 0 step 95: training loss: 672.5403164125524\n",
      "Epoch 0 step 96: training accuarcy: 0.9987\n",
      "Epoch 0 step 96: training loss: 659.8779122434349\n",
      "Epoch 0 step 97: training accuarcy: 0.9989\n",
      "Epoch 0 step 97: training loss: 636.4685153123821\n",
      "Epoch 0 step 98: training accuarcy: 0.9992000000000001\n",
      "Epoch 0 step 98: training loss: 617.4986717257118\n",
      "Epoch 0 step 99: training accuarcy: 0.9996\n",
      "Epoch 0 step 99: training loss: 598.0429017882966\n",
      "Epoch 0 step 100: training accuarcy: 0.9991000000000001\n",
      "Epoch 0 step 100: training loss: 581.0581948770254\n",
      "Epoch 0 step 101: training accuarcy: 0.9996\n",
      "Epoch 0 step 101: training loss: 571.0117674615293\n",
      "Epoch 0 step 102: training accuarcy: 0.9991000000000001\n",
      "Epoch 0 step 102: training loss: 554.5095307271467\n",
      "Epoch 0 step 103: training accuarcy: 0.9991000000000001\n",
      "Epoch 0 step 103: training loss: 543.5783349258361\n",
      "Epoch 0 step 104: training accuarcy: 0.9993000000000001\n",
      "Epoch 0 step 104: training loss: 526.1238594794612\n",
      "Epoch 0 step 105: training accuarcy: 0.999\n",
      "Epoch 0 step 105: training loss: 511.56178351680114\n",
      "Epoch 0 step 106: training accuarcy: 0.9994000000000001\n",
      "Epoch 0 step 106: training loss: 496.81857129280985\n",
      "Epoch 0 step 107: training accuarcy: 0.9993000000000001\n",
      "Epoch 0 step 107: training loss: 492.87369632473377\n",
      "Epoch 0 step 108: training accuarcy: 0.9983000000000001\n",
      "Epoch 0 step 108: training loss: 472.1921128325452\n",
      "Epoch 0 step 109: training accuarcy: 0.9998\n",
      "Epoch 0 step 109: training loss: 457.6282326799243\n",
      "Epoch 0 step 110: training accuarcy: 0.9997\n",
      "Epoch 0 step 110: training loss: 456.1653914004582\n",
      "Epoch 0 step 111: training accuarcy: 0.9991000000000001\n",
      "Epoch 0 step 111: training loss: 448.7667279047575\n",
      "Epoch 0 step 112: training accuarcy: 0.9986\n",
      "Epoch 0 step 112: training loss: 422.07714657870497\n",
      "Epoch 0 step 113: training accuarcy: 0.9997\n",
      "Epoch 0 step 113: training loss: 416.09845314950746\n",
      "Epoch 0 step 114: training accuarcy: 0.9995\n",
      "Epoch 0 step 114: training loss: 409.918544297276\n",
      "Epoch 0 step 115: training accuarcy: 0.9991614255765199\n",
      "Epoch 0: train loss 3255.520362980043, train accuarcy 0.9910768270492554\n",
      "Epoch 0: valid loss 399.0151983044463, valid accuarcy 0.9995409250259399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [11:10<11:10, 670.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 393.6084775316136\n",
      "Epoch 1 step 116: training accuarcy: 0.9997\n",
      "Epoch 1 step 116: training loss: 383.87116867256964\n",
      "Epoch 1 step 117: training accuarcy: 0.9998\n",
      "Epoch 1 step 117: training loss: 370.6599519735693\n",
      "Epoch 1 step 118: training accuarcy: 0.9998\n",
      "Epoch 1 step 118: training loss: 369.7776940718562\n",
      "Epoch 1 step 119: training accuarcy: 0.9996\n",
      "Epoch 1 step 119: training loss: 355.76265512271584\n",
      "Epoch 1 step 120: training accuarcy: 1.0\n",
      "Epoch 1 step 120: training loss: 349.84685843749446\n",
      "Epoch 1 step 121: training accuarcy: 0.9996\n",
      "Epoch 1 step 121: training loss: 341.7259244850197\n",
      "Epoch 1 step 122: training accuarcy: 0.9997\n",
      "Epoch 1 step 122: training loss: 342.2537523285955\n",
      "Epoch 1 step 123: training accuarcy: 0.9995\n",
      "Epoch 1 step 123: training loss: 324.5306323480828\n",
      "Epoch 1 step 124: training accuarcy: 0.9997\n",
      "Epoch 1 step 124: training loss: 322.34660816559045\n",
      "Epoch 1 step 125: training accuarcy: 0.9996\n",
      "Epoch 1 step 125: training loss: 312.63274330221276\n",
      "Epoch 1 step 126: training accuarcy: 0.9996\n",
      "Epoch 1 step 126: training loss: 311.1985905071844\n",
      "Epoch 1 step 127: training accuarcy: 0.9992000000000001\n",
      "Epoch 1 step 127: training loss: 299.68242292303233\n",
      "Epoch 1 step 128: training accuarcy: 0.9994000000000001\n",
      "Epoch 1 step 128: training loss: 293.7672009074687\n",
      "Epoch 1 step 129: training accuarcy: 0.9995\n",
      "Epoch 1 step 129: training loss: 282.1519894426817\n",
      "Epoch 1 step 130: training accuarcy: 0.9997\n",
      "Epoch 1 step 130: training loss: 280.6650814328967\n",
      "Epoch 1 step 131: training accuarcy: 0.9995\n",
      "Epoch 1 step 131: training loss: 271.91401447463784\n",
      "Epoch 1 step 132: training accuarcy: 0.9996\n",
      "Epoch 1 step 132: training loss: 265.0584336397533\n",
      "Epoch 1 step 133: training accuarcy: 0.9993000000000001\n",
      "Epoch 1 step 133: training loss: 263.06455483227694\n",
      "Epoch 1 step 134: training accuarcy: 0.9997\n",
      "Epoch 1 step 134: training loss: 258.9383638706414\n",
      "Epoch 1 step 135: training accuarcy: 0.9996\n",
      "Epoch 1 step 135: training loss: 251.6006999344947\n",
      "Epoch 1 step 136: training accuarcy: 0.9998\n",
      "Epoch 1 step 136: training loss: 244.02694578893494\n",
      "Epoch 1 step 137: training accuarcy: 0.9998\n",
      "Epoch 1 step 137: training loss: 249.69125986364807\n",
      "Epoch 1 step 138: training accuarcy: 0.9988\n",
      "Epoch 1 step 138: training loss: 236.19886154562116\n",
      "Epoch 1 step 139: training accuarcy: 0.9995\n",
      "Epoch 1 step 139: training loss: 231.17625273791708\n",
      "Epoch 1 step 140: training accuarcy: 0.9998\n",
      "Epoch 1 step 140: training loss: 222.92077067119442\n",
      "Epoch 1 step 141: training accuarcy: 1.0\n",
      "Epoch 1 step 141: training loss: 221.74244253830102\n",
      "Epoch 1 step 142: training accuarcy: 0.9999\n",
      "Epoch 1 step 142: training loss: 222.271095307781\n",
      "Epoch 1 step 143: training accuarcy: 0.9996\n",
      "Epoch 1 step 143: training loss: 213.81657769514086\n",
      "Epoch 1 step 144: training accuarcy: 0.9999\n",
      "Epoch 1 step 144: training loss: 212.58145811168788\n",
      "Epoch 1 step 145: training accuarcy: 0.9998\n",
      "Epoch 1 step 145: training loss: 207.62341619629362\n",
      "Epoch 1 step 146: training accuarcy: 0.9995\n",
      "Epoch 1 step 146: training loss: 200.1548949493193\n",
      "Epoch 1 step 147: training accuarcy: 0.9997\n",
      "Epoch 1 step 147: training loss: 197.83682082068015\n",
      "Epoch 1 step 148: training accuarcy: 0.9996\n",
      "Epoch 1 step 148: training loss: 194.83474469401224\n",
      "Epoch 1 step 149: training accuarcy: 0.9996\n",
      "Epoch 1 step 149: training loss: 190.51991182631554\n",
      "Epoch 1 step 150: training accuarcy: 1.0\n",
      "Epoch 1 step 150: training loss: 185.33882302993192\n",
      "Epoch 1 step 151: training accuarcy: 0.9998\n",
      "Epoch 1 step 151: training loss: 188.5208688873574\n",
      "Epoch 1 step 152: training accuarcy: 0.9996\n",
      "Epoch 1 step 152: training loss: 181.02611817902005\n",
      "Epoch 1 step 153: training accuarcy: 0.9996\n",
      "Epoch 1 step 153: training loss: 176.4159965717142\n",
      "Epoch 1 step 154: training accuarcy: 0.9998\n",
      "Epoch 1 step 154: training loss: 174.84283452867976\n",
      "Epoch 1 step 155: training accuarcy: 0.9998\n",
      "Epoch 1 step 155: training loss: 175.7926952765096\n",
      "Epoch 1 step 156: training accuarcy: 0.9997\n",
      "Epoch 1 step 156: training loss: 168.90301592191523\n",
      "Epoch 1 step 157: training accuarcy: 0.9997\n",
      "Epoch 1 step 157: training loss: 166.1899041912743\n",
      "Epoch 1 step 158: training accuarcy: 1.0\n",
      "Epoch 1 step 158: training loss: 159.71732695471584\n",
      "Epoch 1 step 159: training accuarcy: 0.9999\n",
      "Epoch 1 step 159: training loss: 158.09625962412787\n",
      "Epoch 1 step 160: training accuarcy: 1.0\n",
      "Epoch 1 step 160: training loss: 159.3367586660718\n",
      "Epoch 1 step 161: training accuarcy: 0.9999\n",
      "Epoch 1 step 161: training loss: 159.41202361704927\n",
      "Epoch 1 step 162: training accuarcy: 0.9997\n",
      "Epoch 1 step 162: training loss: 150.36830876968864\n",
      "Epoch 1 step 163: training accuarcy: 1.0\n",
      "Epoch 1 step 163: training loss: 156.88271424730155\n",
      "Epoch 1 step 164: training accuarcy: 0.9991000000000001\n",
      "Epoch 1 step 164: training loss: 151.16630030371329\n",
      "Epoch 1 step 165: training accuarcy: 0.9998\n",
      "Epoch 1 step 165: training loss: 149.52910825693942\n",
      "Epoch 1 step 166: training accuarcy: 0.9995\n",
      "Epoch 1 step 166: training loss: 150.04131765386134\n",
      "Epoch 1 step 167: training accuarcy: 0.9996\n",
      "Epoch 1 step 167: training loss: 142.62791147098773\n",
      "Epoch 1 step 168: training accuarcy: 0.9999\n",
      "Epoch 1 step 168: training loss: 139.70062043801607\n",
      "Epoch 1 step 169: training accuarcy: 0.9999\n",
      "Epoch 1 step 169: training loss: 138.2224898398431\n",
      "Epoch 1 step 170: training accuarcy: 0.9998\n",
      "Epoch 1 step 170: training loss: 136.57505202932578\n",
      "Epoch 1 step 171: training accuarcy: 0.9997\n",
      "Epoch 1 step 171: training loss: 135.52187313982967\n",
      "Epoch 1 step 172: training accuarcy: 0.9996\n",
      "Epoch 1 step 172: training loss: 130.31158905799538\n",
      "Epoch 1 step 173: training accuarcy: 1.0\n",
      "Epoch 1 step 173: training loss: 130.6715620996181\n",
      "Epoch 1 step 174: training accuarcy: 1.0\n",
      "Epoch 1 step 174: training loss: 139.51231905960208\n",
      "Epoch 1 step 175: training accuarcy: 0.9992000000000001\n",
      "Epoch 1 step 175: training loss: 127.57904390065711\n",
      "Epoch 1 step 176: training accuarcy: 0.9998\n",
      "Epoch 1 step 176: training loss: 128.5421053860735\n",
      "Epoch 1 step 177: training accuarcy: 0.9996\n",
      "Epoch 1 step 177: training loss: 125.51944615269974\n",
      "Epoch 1 step 178: training accuarcy: 1.0\n",
      "Epoch 1 step 178: training loss: 125.21573340338315\n",
      "Epoch 1 step 179: training accuarcy: 0.9995\n",
      "Epoch 1 step 179: training loss: 121.28020204940236\n",
      "Epoch 1 step 180: training accuarcy: 0.9998\n",
      "Epoch 1 step 180: training loss: 120.35457716417068\n",
      "Epoch 1 step 181: training accuarcy: 0.9999\n",
      "Epoch 1 step 181: training loss: 118.3633474540907\n",
      "Epoch 1 step 182: training accuarcy: 0.9998\n",
      "Epoch 1 step 182: training loss: 119.33975070164922\n",
      "Epoch 1 step 183: training accuarcy: 0.9997\n",
      "Epoch 1 step 183: training loss: 118.94861211826591\n",
      "Epoch 1 step 184: training accuarcy: 0.9998\n",
      "Epoch 1 step 184: training loss: 117.91025468288116\n",
      "Epoch 1 step 185: training accuarcy: 0.9993000000000001\n",
      "Epoch 1 step 185: training loss: 113.5422603581766\n",
      "Epoch 1 step 186: training accuarcy: 0.9997\n",
      "Epoch 1 step 186: training loss: 113.64735390105795\n",
      "Epoch 1 step 187: training accuarcy: 0.9997\n",
      "Epoch 1 step 187: training loss: 111.42777167321132\n",
      "Epoch 1 step 188: training accuarcy: 1.0\n",
      "Epoch 1 step 188: training loss: 110.82002409857596\n",
      "Epoch 1 step 189: training accuarcy: 0.9999\n",
      "Epoch 1 step 189: training loss: 108.35838400844723\n",
      "Epoch 1 step 190: training accuarcy: 0.9998\n",
      "Epoch 1 step 190: training loss: 105.56049355300331\n",
      "Epoch 1 step 191: training accuarcy: 1.0\n",
      "Epoch 1 step 191: training loss: 104.10713321969563\n",
      "Epoch 1 step 192: training accuarcy: 1.0\n",
      "Epoch 1 step 192: training loss: 107.66375079072404\n",
      "Epoch 1 step 193: training accuarcy: 0.9996\n",
      "Epoch 1 step 193: training loss: 103.33259087849204\n",
      "Epoch 1 step 194: training accuarcy: 0.9998\n",
      "Epoch 1 step 194: training loss: 102.07001661611027\n",
      "Epoch 1 step 195: training accuarcy: 0.9999\n",
      "Epoch 1 step 195: training loss: 106.43905718287476\n",
      "Epoch 1 step 196: training accuarcy: 0.9998\n",
      "Epoch 1 step 196: training loss: 101.8730971332094\n",
      "Epoch 1 step 197: training accuarcy: 0.9997\n",
      "Epoch 1 step 197: training loss: 100.3922327614577\n",
      "Epoch 1 step 198: training accuarcy: 1.0\n",
      "Epoch 1 step 198: training loss: 96.22729195745217\n",
      "Epoch 1 step 199: training accuarcy: 1.0\n",
      "Epoch 1 step 199: training loss: 99.38721705772328\n",
      "Epoch 1 step 200: training accuarcy: 0.9995\n",
      "Epoch 1 step 200: training loss: 98.35470226515011\n",
      "Epoch 1 step 201: training accuarcy: 1.0\n",
      "Epoch 1 step 201: training loss: 98.60201152343132\n",
      "Epoch 1 step 202: training accuarcy: 0.9999\n",
      "Epoch 1 step 202: training loss: 100.30019552459159\n",
      "Epoch 1 step 203: training accuarcy: 0.9997\n",
      "Epoch 1 step 203: training loss: 92.64952643271319\n",
      "Epoch 1 step 204: training accuarcy: 1.0\n",
      "Epoch 1 step 204: training loss: 94.38578104729044\n",
      "Epoch 1 step 205: training accuarcy: 0.9997\n",
      "Epoch 1 step 205: training loss: 90.76049869558355\n",
      "Epoch 1 step 206: training accuarcy: 1.0\n",
      "Epoch 1 step 206: training loss: 96.19433770683972\n",
      "Epoch 1 step 207: training accuarcy: 0.9999\n",
      "Epoch 1 step 207: training loss: 92.15552068922554\n",
      "Epoch 1 step 208: training accuarcy: 0.9995\n",
      "Epoch 1 step 208: training loss: 88.1233940441555\n",
      "Epoch 1 step 209: training accuarcy: 1.0\n",
      "Epoch 1 step 209: training loss: 89.18784927762582\n",
      "Epoch 1 step 210: training accuarcy: 1.0\n",
      "Epoch 1 step 210: training loss: 89.24926465246385\n",
      "Epoch 1 step 211: training accuarcy: 0.9998\n",
      "Epoch 1 step 211: training loss: 89.10221804443975\n",
      "Epoch 1 step 212: training accuarcy: 0.9999\n",
      "Epoch 1 step 212: training loss: 89.12973052502338\n",
      "Epoch 1 step 213: training accuarcy: 0.9995\n",
      "Epoch 1 step 213: training loss: 88.63147666593349\n",
      "Epoch 1 step 214: training accuarcy: 0.9999\n",
      "Epoch 1 step 214: training loss: 84.42720617981452\n",
      "Epoch 1 step 215: training accuarcy: 1.0\n",
      "Epoch 1 step 215: training loss: 82.5217343013169\n",
      "Epoch 1 step 216: training accuarcy: 1.0\n",
      "Epoch 1 step 216: training loss: 84.19253181171388\n",
      "Epoch 1 step 217: training accuarcy: 1.0\n",
      "Epoch 1 step 217: training loss: 83.75503190670227\n",
      "Epoch 1 step 218: training accuarcy: 0.9999\n",
      "Epoch 1 step 218: training loss: 82.83342341151365\n",
      "Epoch 1 step 219: training accuarcy: 0.9999\n",
      "Epoch 1 step 219: training loss: 81.77945047197967\n",
      "Epoch 1 step 220: training accuarcy: 0.9999\n",
      "Epoch 1 step 220: training loss: 81.61800494938392\n",
      "Epoch 1 step 221: training accuarcy: 0.9999\n",
      "Epoch 1 step 221: training loss: 79.36645739432358\n",
      "Epoch 1 step 222: training accuarcy: 1.0\n",
      "Epoch 1 step 222: training loss: 80.66537411937092\n",
      "Epoch 1 step 223: training accuarcy: 1.0\n",
      "Epoch 1 step 223: training loss: 79.31623999402504\n",
      "Epoch 1 step 224: training accuarcy: 0.9998\n",
      "Epoch 1 step 224: training loss: 80.61455695083106\n",
      "Epoch 1 step 225: training accuarcy: 0.9999\n",
      "Epoch 1 step 225: training loss: 80.11497733227577\n",
      "Epoch 1 step 226: training accuarcy: 1.0\n",
      "Epoch 1 step 226: training loss: 78.67570426394417\n",
      "Epoch 1 step 227: training accuarcy: 0.9998\n",
      "Epoch 1 step 227: training loss: 77.24559448047617\n",
      "Epoch 1 step 228: training accuarcy: 1.0\n",
      "Epoch 1 step 228: training loss: 75.73249196940071\n",
      "Epoch 1 step 229: training accuarcy: 1.0\n",
      "Epoch 1 step 229: training loss: 79.8656121982973\n",
      "Epoch 1 step 230: training accuarcy: 0.9994758909853249\n",
      "Epoch 1: train loss 165.15330182608386, train accuarcy 0.9996984601020813\n",
      "Epoch 1: valid loss 77.68433867695171, valid accuarcy 0.9998906254768372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [22:14<00:00, 668.42s/it]"
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=2,\n",
    "               log_dir=get_log_dir(ds_type='simple_kaggle', model_type='fm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T03:51:13.052157Z",
     "start_time": "2019-10-08T03:51:13.046186Z"
    }
   },
   "outputs": [],
   "source": [
    "del fm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HRM FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:41:54.093302Z",
     "start_time": "2019-10-08T05:41:54.023329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchHrmFM()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_model = TorchHrmFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "hrm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:41:54.546520Z",
     "start_time": "2019-10-08T05:41:54.542538Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(hrm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:41:55.849797Z",
     "start_time": "2019-10-08T05:41:55.822823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1dbc6d807b8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_learner = FMLearner(hrm_model, adam_opt, schedular, db)\n",
    "hrm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:18:29.389502Z",
     "start_time": "2019-10-07T12:18:29.386499Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:16:53.394556Z",
     "start_time": "2019-10-08T05:16:53.390559Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='seq',\n",
    "                    valid_col='seq',\n",
    "                    test_col='seq',\n",
    "                    loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:42:16.194124Z",
     "start_time": "2019-10-08T05:42:16.190100Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='seq',\n",
    "                    valid_col='seq',\n",
    "                    test_col='seq',\n",
    "                    loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T05:47:55.214119Z",
     "start_time": "2019-10-08T05:42:18.278108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 491836.69427906047\n",
      "Epoch 0 step 1: training accuarcy: 0.1431\n",
      "Epoch 0 step 1: training loss: 492699.535647072\n",
      "Epoch 0 step 2: training accuarcy: 0.1641\n",
      "Epoch 0 step 2: training loss: 489907.8437827124\n",
      "Epoch 0 step 3: training accuarcy: 0.1915\n",
      "Epoch 0 step 3: training loss: 489669.58815967327\n",
      "Epoch 0 step 4: training accuarcy: 0.19940000000000002\n",
      "Epoch 0 step 4: training loss: 488313.8969329584\n",
      "Epoch 0 step 5: training accuarcy: 0.21230000000000002\n",
      "Epoch 0 step 5: training loss: 484669.38869526685\n",
      "Epoch 0 step 6: training accuarcy: 0.22310000000000002\n",
      "Epoch 0 step 6: training loss: 488643.7154459951\n",
      "Epoch 0 step 7: training accuarcy: 0.226\n",
      "Epoch 0 step 7: training loss: 488228.2421975789\n",
      "Epoch 0 step 8: training accuarcy: 0.2253\n",
      "Epoch 0 step 8: training loss: 487120.4144884759\n",
      "Epoch 0 step 9: training accuarcy: 0.22060000000000002\n",
      "Epoch 0 step 9: training loss: 486551.17407477234\n",
      "Epoch 0 step 10: training accuarcy: 0.2286\n",
      "Epoch 0 step 10: training loss: 485950.74038198707\n",
      "Epoch 0 step 11: training accuarcy: 0.22820000000000001\n",
      "Epoch 0 step 11: training loss: 484378.15106164594\n",
      "Epoch 0 step 12: training accuarcy: 0.22920000000000001\n",
      "Epoch 0 step 12: training loss: 486452.71313219226\n",
      "Epoch 0 step 13: training accuarcy: 0.23020000000000002\n",
      "Epoch 0 step 13: training loss: 486562.5766474652\n",
      "Epoch 0 step 14: training accuarcy: 0.22690000000000002\n",
      "Epoch 0 step 14: training loss: 482133.8793135949\n",
      "Epoch 0 step 15: training accuarcy: 0.2238\n",
      "Epoch 0 step 15: training loss: 483539.8978829222\n",
      "Epoch 0 step 16: training accuarcy: 0.2356\n",
      "Epoch 0 step 16: training loss: 482437.3073948337\n",
      "Epoch 0 step 17: training accuarcy: 0.22990000000000002\n",
      "Epoch 0 step 17: training loss: 486660.49390226445\n",
      "Epoch 0 step 18: training accuarcy: 0.2268\n",
      "Epoch 0 step 18: training loss: 479871.44574898004\n",
      "Epoch 0 step 19: training accuarcy: 0.23700000000000002\n",
      "Epoch 0 step 19: training loss: 487494.72809528623\n",
      "Epoch 0 step 20: training accuarcy: 0.2235\n",
      "Epoch 0 step 20: training loss: 483578.0624910847\n",
      "Epoch 0 step 21: training accuarcy: 0.229\n",
      "Epoch 0 step 21: training loss: 481427.3292375137\n",
      "Epoch 0 step 22: training accuarcy: 0.2291\n",
      "Epoch 0 step 22: training loss: 481365.8150492647\n",
      "Epoch 0 step 23: training accuarcy: 0.2373\n",
      "Epoch 0 step 23: training loss: 481047.4271880214\n",
      "Epoch 0 step 24: training accuarcy: 0.23570000000000002\n",
      "Epoch 0 step 24: training loss: 479094.9941221586\n",
      "Epoch 0 step 25: training accuarcy: 0.2412\n",
      "Epoch 0 step 25: training loss: 487145.1605146233\n",
      "Epoch 0 step 26: training accuarcy: 0.23120000000000002\n",
      "Epoch 0 step 26: training loss: 478774.44731445494\n",
      "Epoch 0 step 27: training accuarcy: 0.24500000000000002\n",
      "Epoch 0 step 27: training loss: 481325.5801874014\n",
      "Epoch 0 step 28: training accuarcy: 0.2381\n",
      "Epoch 0 step 28: training loss: 484341.9871641446\n",
      "Epoch 0 step 29: training accuarcy: 0.23320000000000002\n",
      "Epoch 0 step 29: training loss: 478206.43331120553\n",
      "Epoch 0 step 30: training accuarcy: 0.2422\n",
      "Epoch 0 step 30: training loss: 473905.7058079449\n",
      "Epoch 0 step 31: training accuarcy: 0.24860000000000002\n",
      "Epoch 0 step 31: training loss: 480809.8413634452\n",
      "Epoch 0 step 32: training accuarcy: 0.23620000000000002\n",
      "Epoch 0 step 32: training loss: 476025.8040730963\n",
      "Epoch 0 step 33: training accuarcy: 0.2437\n",
      "Epoch 0 step 33: training loss: 487703.4934427957\n",
      "Epoch 0 step 34: training accuarcy: 0.2341\n",
      "Epoch 0 step 34: training loss: 480403.41493963584\n",
      "Epoch 0 step 35: training accuarcy: 0.2386\n",
      "Epoch 0 step 35: training loss: 476870.28642918775\n",
      "Epoch 0 step 36: training accuarcy: 0.2427\n",
      "Epoch 0 step 36: training loss: 483694.65999619925\n",
      "Epoch 0 step 37: training accuarcy: 0.2321\n",
      "Epoch 0 step 37: training loss: 483530.7443199394\n",
      "Epoch 0 step 38: training accuarcy: 0.2371\n",
      "Epoch 0 step 38: training loss: 482133.5234317899\n",
      "Epoch 0 step 39: training accuarcy: 0.24200000000000002\n",
      "Epoch 0 step 39: training loss: 480501.1529246092\n",
      "Epoch 0 step 40: training accuarcy: 0.2323\n",
      "Epoch 0 step 40: training loss: 479026.7786555682\n",
      "Epoch 0 step 41: training accuarcy: 0.24180000000000001\n",
      "Epoch 0 step 41: training loss: 476805.4791258825\n",
      "Epoch 0 step 42: training accuarcy: 0.24700000000000003\n",
      "Epoch 0 step 42: training loss: 479976.28606500017\n",
      "Epoch 0 step 43: training accuarcy: 0.24150000000000002\n",
      "Epoch 0 step 43: training loss: 478226.6166172476\n",
      "Epoch 0 step 44: training accuarcy: 0.2406\n",
      "Epoch 0 step 44: training loss: 480791.96983407537\n",
      "Epoch 0 step 45: training accuarcy: 0.2364\n",
      "Epoch 0 step 45: training loss: 480616.8899041775\n",
      "Epoch 0 step 46: training accuarcy: 0.23900000000000002\n",
      "Epoch 0 step 46: training loss: 476640.5861128449\n",
      "Epoch 0 step 47: training accuarcy: 0.2396\n",
      "Epoch 0 step 47: training loss: 477764.6081729941\n",
      "Epoch 0 step 48: training accuarcy: 0.2376\n",
      "Epoch 0 step 48: training loss: 478608.70267683937\n",
      "Epoch 0 step 49: training accuarcy: 0.2411\n",
      "Epoch 0 step 49: training loss: 481656.82735138194\n",
      "Epoch 0 step 50: training accuarcy: 0.22840000000000002\n",
      "Epoch 0 step 50: training loss: 474429.0880745786\n",
      "Epoch 0 step 51: training accuarcy: 0.24580000000000002\n",
      "Epoch 0 step 51: training loss: 477105.21404947806\n",
      "Epoch 0 step 52: training accuarcy: 0.2451\n",
      "Epoch 0 step 52: training loss: 478463.5863898041\n",
      "Epoch 0 step 53: training accuarcy: 0.24200000000000002\n",
      "Epoch 0 step 53: training loss: 475042.00775275455\n",
      "Epoch 0 step 54: training accuarcy: 0.24650000000000002\n",
      "Epoch 0 step 54: training loss: 469899.73354930885\n",
      "Epoch 0 step 55: training accuarcy: 0.24700000000000003\n",
      "Epoch 0 step 55: training loss: 481753.6754399732\n",
      "Epoch 0 step 56: training accuarcy: 0.23220000000000002\n",
      "Epoch 0 step 56: training loss: 477148.59606624173\n",
      "Epoch 0 step 57: training accuarcy: 0.24130000000000001\n",
      "Epoch 0 step 57: training loss: 475082.34988627635\n",
      "Epoch 0 step 58: training accuarcy: 0.2444\n",
      "Epoch 0 step 58: training loss: 474451.1440191959\n",
      "Epoch 0 step 59: training accuarcy: 0.24500000000000002\n",
      "Epoch 0 step 59: training loss: 478381.4866456314\n",
      "Epoch 0 step 60: training accuarcy: 0.23440000000000003\n",
      "Epoch 0 step 60: training loss: 471428.4763314197\n",
      "Epoch 0 step 61: training accuarcy: 0.24710000000000001\n",
      "Epoch 0 step 61: training loss: 471482.52646925417\n",
      "Epoch 0 step 62: training accuarcy: 0.2454\n",
      "Epoch 0 step 62: training loss: 471594.69158716727\n",
      "Epoch 0 step 63: training accuarcy: 0.25120000000000003\n",
      "Epoch 0 step 63: training loss: 483048.9577438626\n",
      "Epoch 0 step 64: training accuarcy: 0.23270000000000002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-634a404f3034>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m hrm_learner.fit(epoch=2,\n\u001b[1;32m----> 2\u001b[1;33m                 log_dir=get_log_dir('simple_kaggle', 'hrm'))\n\u001b[0m",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, epoch, log_dir)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mschedular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muser_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\datasets\\torch_kaggle.py\u001b[0m in \u001b[0;36m_seq_collate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mper\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mper_series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         ]\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mneg_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# Encode perivous and postive feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                        copy=copy, sort=sort)\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    424\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m    425\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                 copy=self.copy)\n\u001b[0m\u001b[0;32m    427\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   2056\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_uniform_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2057\u001b[0m             b = join_units[0].block.concat_same_type(\n\u001b[1;32m-> 2058\u001b[1;33m                 [ju.block for ju in join_units], placement=placement)\n\u001b[0m\u001b[0;32m   2059\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m             b = make_block(\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mconcat_same_type\u001b[1;34m(self, to_concat, placement)\u001b[0m\n\u001b[0;32m   2995\u001b[0m         \"\"\"\n\u001b[0;32m   2996\u001b[0m         values = self._concatenator([blk.values for blk in to_concat],\n\u001b[1;32m-> 2997\u001b[1;33m                                     axis=self.ndim - 1)\n\u001b[0m\u001b[0;32m   2998\u001b[0m         \u001b[1;31m# not using self.make_block_same_class as values can be object dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2999\u001b[0m         return make_block(\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py\u001b[0m in \u001b[0;36m_concat_categorical\u001b[1;34m(to_concat, axis)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dtype_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0munion_categoricals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoricals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;31m# extract the categoricals & coerce to object if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py\u001b[0m in \u001b[0;36munion_categoricals\u001b[1;34m(to_union, sort_categories, ignore_order)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[0mordered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dtype_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_union\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m         \u001b[1;31m# identical categories - fastpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[0mordered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dtype_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_union\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m         \u001b[1;31m# identical categories - fastpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py\u001b[0m in \u001b[0;36mis_dtype_equal\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   2355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2357\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mhash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;31m# We *do* want to include the real self.ordered here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hash_categories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mordered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\u001b[0m in \u001b[0;36m_hash_categories\u001b[1;34m(categories, ordered)\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mordered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             cat_array = np.vstack([\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mcat_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             ])\n\u001b[0;32m    434\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \"\"\"\n\u001b[0;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=2,\n",
    "                log_dir=get_log_dir('simple_kaggle', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T06:10:46.355376Z",
     "start_time": "2019-10-08T05:48:42.729961Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 476156.54998684255\n",
      "Epoch 0 step 1: training accuarcy: 0.24070000000000003\n",
      "Epoch 0 step 1: training loss: 475096.2517683112\n",
      "Epoch 0 step 2: training accuarcy: 0.2451\n",
      "Epoch 0 step 2: training loss: 476683.3893662889\n",
      "Epoch 0 step 3: training accuarcy: 0.2388\n",
      "Epoch 0 step 3: training loss: 476621.68926722434\n",
      "Epoch 0 step 4: training accuarcy: 0.2424\n",
      "Epoch 0 step 4: training loss: 474116.48489528266\n",
      "Epoch 0 step 5: training accuarcy: 0.24250000000000002\n",
      "Epoch 0 step 5: training loss: 472665.09536336595\n",
      "Epoch 0 step 6: training accuarcy: 0.2479\n",
      "Epoch 0 step 6: training loss: 478215.3165373088\n",
      "Epoch 0 step 7: training accuarcy: 0.23240000000000002\n",
      "Epoch 0 step 7: training loss: 476077.3696475439\n",
      "Epoch 0 step 8: training accuarcy: 0.2441\n",
      "Epoch 0 step 8: training loss: 473822.3694914146\n",
      "Epoch 0 step 9: training accuarcy: 0.2459\n",
      "Epoch 0 step 9: training loss: 476906.9368236094\n",
      "Epoch 0 step 10: training accuarcy: 0.2399\n",
      "Epoch 0 step 10: training loss: 478092.02576132875\n",
      "Epoch 0 step 11: training accuarcy: 0.2393\n",
      "Epoch 0 step 11: training loss: 480840.6103360135\n",
      "Epoch 0 step 12: training accuarcy: 0.23020000000000002\n",
      "Epoch 0 step 12: training loss: 479856.28004990827\n",
      "Epoch 0 step 13: training accuarcy: 0.23770000000000002\n",
      "Epoch 0 step 13: training loss: 474077.7295179913\n",
      "Epoch 0 step 14: training accuarcy: 0.24070000000000003\n",
      "Epoch 0 step 14: training loss: 475631.12660750607\n",
      "Epoch 0 step 15: training accuarcy: 0.24480000000000002\n",
      "Epoch 0 step 15: training loss: 482962.14495526504\n",
      "Epoch 0 step 16: training accuarcy: 0.23140000000000002\n",
      "Epoch 0 step 16: training loss: 477996.7758504399\n",
      "Epoch 0 step 17: training accuarcy: 0.2451\n",
      "Epoch 0 step 17: training loss: 478127.0979807773\n",
      "Epoch 0 step 18: training accuarcy: 0.2368\n",
      "Epoch 0 step 18: training loss: 474680.26854990656\n",
      "Epoch 0 step 19: training accuarcy: 0.23950000000000002\n",
      "Epoch 0 step 19: training loss: 477137.9523863635\n",
      "Epoch 0 step 20: training accuarcy: 0.24020000000000002\n",
      "Epoch 0 step 20: training loss: 467541.5490392982\n",
      "Epoch 0 step 21: training accuarcy: 0.2546\n",
      "Epoch 0 step 21: training loss: 476384.60168991383\n",
      "Epoch 0 step 22: training accuarcy: 0.2469\n",
      "Epoch 0 step 22: training loss: 473402.9994823938\n",
      "Epoch 0 step 23: training accuarcy: 0.24580000000000002\n",
      "Epoch 0 step 23: training loss: 476204.35227327433\n",
      "Epoch 0 step 24: training accuarcy: 0.2452\n",
      "Epoch 0 step 24: training loss: 472280.15878944757\n",
      "Epoch 0 step 25: training accuarcy: 0.24600000000000002\n",
      "Epoch 0 step 25: training loss: 476284.5740327107\n",
      "Epoch 0 step 26: training accuarcy: 0.24500000000000002\n",
      "Epoch 0 step 26: training loss: 481054.6599340319\n",
      "Epoch 0 step 27: training accuarcy: 0.2336\n",
      "Epoch 0 step 27: training loss: 472468.4862383513\n",
      "Epoch 0 step 28: training accuarcy: 0.2461\n",
      "Epoch 0 step 28: training loss: 471579.78278384585\n",
      "Epoch 0 step 29: training accuarcy: 0.2543\n",
      "Epoch 0 step 29: training loss: 476757.7116046868\n",
      "Epoch 0 step 30: training accuarcy: 0.24050000000000002\n",
      "Epoch 0 step 30: training loss: 469887.4473842333\n",
      "Epoch 0 step 31: training accuarcy: 0.25170000000000003\n",
      "Epoch 0 step 31: training loss: 466324.086538369\n",
      "Epoch 0 step 32: training accuarcy: 0.25880000000000003\n",
      "Epoch 0 step 32: training loss: 472482.1699876222\n",
      "Epoch 0 step 33: training accuarcy: 0.251\n",
      "Epoch 0 step 33: training loss: 471311.4479079442\n",
      "Epoch 0 step 34: training accuarcy: 0.24980000000000002\n",
      "Epoch 0 step 34: training loss: 469891.0886909383\n",
      "Epoch 0 step 35: training accuarcy: 0.2549\n",
      "Epoch 0 step 35: training loss: 472845.4105298207\n",
      "Epoch 0 step 36: training accuarcy: 0.25120000000000003\n",
      "Epoch 0 step 36: training loss: 470141.3224354419\n",
      "Epoch 0 step 37: training accuarcy: 0.2521\n",
      "Epoch 0 step 37: training loss: 474476.559373837\n",
      "Epoch 0 step 38: training accuarcy: 0.2466\n",
      "Epoch 0 step 38: training loss: 466368.58544212754\n",
      "Epoch 0 step 39: training accuarcy: 0.2616\n",
      "Epoch 0 step 39: training loss: 466521.6215821858\n",
      "Epoch 0 step 40: training accuarcy: 0.2586\n",
      "Epoch 0 step 40: training loss: 467311.9618306088\n",
      "Epoch 0 step 41: training accuarcy: 0.2579\n",
      "Epoch 0 step 41: training loss: 471957.1462618996\n",
      "Epoch 0 step 42: training accuarcy: 0.25\n",
      "Epoch 0 step 42: training loss: 467768.46893266385\n",
      "Epoch 0 step 43: training accuarcy: 0.2546\n",
      "Epoch 0 step 43: training loss: 470599.4853387117\n",
      "Epoch 0 step 44: training accuarcy: 0.2539\n",
      "Epoch 0 step 44: training loss: 470922.1988353374\n",
      "Epoch 0 step 45: training accuarcy: 0.2515\n",
      "Epoch 0 step 45: training loss: 472237.78498620045\n",
      "Epoch 0 step 46: training accuarcy: 0.2485\n",
      "Epoch 0 step 46: training loss: 467545.9812474881\n",
      "Epoch 0 step 47: training accuarcy: 0.2548\n",
      "Epoch 0 step 47: training loss: 469929.2262915686\n",
      "Epoch 0 step 48: training accuarcy: 0.2601\n",
      "Epoch 0 step 48: training loss: 458992.42743237497\n",
      "Epoch 0 step 49: training accuarcy: 0.27190000000000003\n",
      "Epoch 0 step 49: training loss: 468108.31327023264\n",
      "Epoch 0 step 50: training accuarcy: 0.2644\n",
      "Epoch 0 step 50: training loss: 461115.2180721187\n",
      "Epoch 0 step 51: training accuarcy: 0.2705\n",
      "Epoch 0 step 51: training loss: 469305.5601732821\n",
      "Epoch 0 step 52: training accuarcy: 0.2606\n",
      "Epoch 0 step 52: training loss: 466170.2086815161\n",
      "Epoch 0 step 53: training accuarcy: 0.2641\n",
      "Epoch 0 step 53: training loss: 466129.0271966949\n",
      "Epoch 0 step 54: training accuarcy: 0.2637\n",
      "Epoch 0 step 54: training loss: 470046.3958890378\n",
      "Epoch 0 step 55: training accuarcy: 0.2566\n",
      "Epoch 0 step 55: training loss: 466245.2324325123\n",
      "Epoch 0 step 56: training accuarcy: 0.2632\n",
      "Epoch 0 step 56: training loss: 462469.9886141762\n",
      "Epoch 0 step 57: training accuarcy: 0.2716\n",
      "Epoch 0 step 57: training loss: 460402.85095851193\n",
      "Epoch 0 step 58: training accuarcy: 0.2736\n",
      "Epoch 0 step 58: training loss: 459843.7322445647\n",
      "Epoch 0 step 59: training accuarcy: 0.2812\n",
      "Epoch 0 step 59: training loss: 458262.77248954296\n",
      "Epoch 0 step 60: training accuarcy: 0.2831\n",
      "Epoch 0 step 60: training loss: 461814.08336078946\n",
      "Epoch 0 step 61: training accuarcy: 0.2795\n",
      "Epoch 0 step 61: training loss: 456673.8589620926\n",
      "Epoch 0 step 62: training accuarcy: 0.28300000000000003\n",
      "Epoch 0 step 62: training loss: 455994.59985369234\n",
      "Epoch 0 step 63: training accuarcy: 0.2846\n",
      "Epoch 0 step 63: training loss: 462542.89013730024\n",
      "Epoch 0 step 64: training accuarcy: 0.2761\n",
      "Epoch 0 step 64: training loss: 457596.177186187\n",
      "Epoch 0 step 65: training accuarcy: 0.2828\n",
      "Epoch 0 step 65: training loss: 462282.1033255203\n",
      "Epoch 0 step 66: training accuarcy: 0.2766\n",
      "Epoch 0 step 66: training loss: 462740.5857136037\n",
      "Epoch 0 step 67: training accuarcy: 0.2756\n",
      "Epoch 0 step 67: training loss: 460464.2743017503\n",
      "Epoch 0 step 68: training accuarcy: 0.27990000000000004\n",
      "Epoch 0 step 68: training loss: 463048.27517394983\n",
      "Epoch 0 step 69: training accuarcy: 0.27340000000000003\n",
      "Epoch 0 step 69: training loss: 458412.91002279724\n",
      "Epoch 0 step 70: training accuarcy: 0.2838\n",
      "Epoch 0 step 70: training loss: 464690.3157982543\n",
      "Epoch 0 step 71: training accuarcy: 0.2756\n",
      "Epoch 0 step 71: training loss: 454016.50826631824\n",
      "Epoch 0 step 72: training accuarcy: 0.2894\n",
      "Epoch 0 step 72: training loss: 460932.3350116376\n",
      "Epoch 0 step 73: training accuarcy: 0.2762\n",
      "Epoch 0 step 73: training loss: 460302.8895606445\n",
      "Epoch 0 step 74: training accuarcy: 0.2791\n",
      "Epoch 0 step 74: training loss: 461822.4027035087\n",
      "Epoch 0 step 75: training accuarcy: 0.2775\n",
      "Epoch 0 step 75: training loss: 462502.8141055667\n",
      "Epoch 0 step 76: training accuarcy: 0.27640000000000003\n",
      "Epoch 0 step 76: training loss: 457862.4420704019\n",
      "Epoch 0 step 77: training accuarcy: 0.2853\n",
      "Epoch 0 step 77: training loss: 455306.56469255657\n",
      "Epoch 0 step 78: training accuarcy: 0.2899\n",
      "Epoch 0 step 78: training loss: 459255.8768472972\n",
      "Epoch 0 step 79: training accuarcy: 0.28140000000000004\n",
      "Epoch 0 step 79: training loss: 457124.7080937\n",
      "Epoch 0 step 80: training accuarcy: 0.2848\n",
      "Epoch 0 step 80: training loss: 453799.82834493334\n",
      "Epoch 0 step 81: training accuarcy: 0.2918\n",
      "Epoch 0 step 81: training loss: 456923.0218994323\n",
      "Epoch 0 step 82: training accuarcy: 0.2831\n",
      "Epoch 0 step 82: training loss: 456315.11355908087\n",
      "Epoch 0 step 83: training accuarcy: 0.28900000000000003\n",
      "Epoch 0 step 83: training loss: 461959.955886147\n",
      "Epoch 0 step 84: training accuarcy: 0.2778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 84: training loss: 456295.89030583657\n",
      "Epoch 0 step 85: training accuarcy: 0.28550000000000003\n",
      "Epoch 0 step 85: training loss: 462609.0593516595\n",
      "Epoch 0 step 86: training accuarcy: 0.27640000000000003\n",
      "Epoch 0 step 86: training loss: 457376.88746239943\n",
      "Epoch 0 step 87: training accuarcy: 0.2858\n",
      "Epoch 0 step 87: training loss: 458630.3304057371\n",
      "Epoch 0 step 88: training accuarcy: 0.28600000000000003\n",
      "Epoch 0 step 88: training loss: 457845.20144336956\n",
      "Epoch 0 step 89: training accuarcy: 0.28400000000000003\n",
      "Epoch 0 step 89: training loss: 463652.0738980753\n",
      "Epoch 0 step 90: training accuarcy: 0.27440000000000003\n",
      "Epoch 0 step 90: training loss: 462923.12193969113\n",
      "Epoch 0 step 91: training accuarcy: 0.2742\n",
      "Epoch 0 step 91: training loss: 462083.0978537885\n",
      "Epoch 0 step 92: training accuarcy: 0.27540000000000003\n",
      "Epoch 0 step 92: training loss: 453528.07437427534\n",
      "Epoch 0 step 93: training accuarcy: 0.2914\n",
      "Epoch 0 step 93: training loss: 457608.1776239768\n",
      "Epoch 0 step 94: training accuarcy: 0.2873\n",
      "Epoch 0 step 94: training loss: 460109.3348680989\n",
      "Epoch 0 step 95: training accuarcy: 0.2823\n",
      "Epoch 0 step 95: training loss: 460148.70728310646\n",
      "Epoch 0 step 96: training accuarcy: 0.2781\n",
      "Epoch 0 step 96: training loss: 459209.7253340731\n",
      "Epoch 0 step 97: training accuarcy: 0.2826\n",
      "Epoch 0 step 97: training loss: 463263.93909117184\n",
      "Epoch 0 step 98: training accuarcy: 0.2765\n",
      "Epoch 0 step 98: training loss: 458174.0436980474\n",
      "Epoch 0 step 99: training accuarcy: 0.2837\n",
      "Epoch 0 step 99: training loss: 454307.77410473337\n",
      "Epoch 0 step 100: training accuarcy: 0.2906\n",
      "Epoch 0 step 100: training loss: 461882.21376217174\n",
      "Epoch 0 step 101: training accuarcy: 0.2802\n",
      "Epoch 0 step 101: training loss: 460996.0821523702\n",
      "Epoch 0 step 102: training accuarcy: 0.2788\n",
      "Epoch 0 step 102: training loss: 453053.99207321054\n",
      "Epoch 0 step 103: training accuarcy: 0.2893\n",
      "Epoch 0 step 103: training loss: 464454.13274899434\n",
      "Epoch 0 step 104: training accuarcy: 0.2737\n",
      "Epoch 0 step 104: training loss: 458144.49688318954\n",
      "Epoch 0 step 105: training accuarcy: 0.2836\n",
      "Epoch 0 step 105: training loss: 459094.5310248085\n",
      "Epoch 0 step 106: training accuarcy: 0.2791\n",
      "Epoch 0 step 106: training loss: 462846.37759718444\n",
      "Epoch 0 step 107: training accuarcy: 0.2775\n",
      "Epoch 0 step 107: training loss: 461170.3455604248\n",
      "Epoch 0 step 108: training accuarcy: 0.27840000000000004\n",
      "Epoch 0 step 108: training loss: 452132.4334944471\n",
      "Epoch 0 step 109: training accuarcy: 0.2931\n",
      "Epoch 0 step 109: training loss: 460912.8158451953\n",
      "Epoch 0 step 110: training accuarcy: 0.2792\n",
      "Epoch 0 step 110: training loss: 454638.6457494486\n",
      "Epoch 0 step 111: training accuarcy: 0.2898\n",
      "Epoch 0 step 111: training loss: 457018.3561774516\n",
      "Epoch 0 step 112: training accuarcy: 0.2857\n",
      "Epoch 0 step 112: training loss: 462987.8071696817\n",
      "Epoch 0 step 113: training accuarcy: 0.2753\n",
      "Epoch 0 step 113: training loss: 457371.5314077563\n",
      "Epoch 0 step 114: training accuarcy: 0.2833\n",
      "Epoch 0 step 114: training loss: 436772.13653733523\n",
      "Epoch 0 step 115: training accuarcy: 0.2880503144654088\n",
      "Epoch 0: train loss 465573.5820361833, train accuarcy 0.2644672095775604\n",
      "Epoch 0: valid loss 460479.14867992705, valid accuarcy 0.2730143666267395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [11:03<11:03, 663.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 450378.8645166672\n",
      "Epoch 1 step 116: training accuarcy: 0.29700000000000004\n",
      "Epoch 1 step 116: training loss: 458080.3530401238\n",
      "Epoch 1 step 117: training accuarcy: 0.2836\n",
      "Epoch 1 step 117: training loss: 456139.74620575464\n",
      "Epoch 1 step 118: training accuarcy: 0.2854\n",
      "Epoch 1 step 118: training loss: 459225.96323987877\n",
      "Epoch 1 step 119: training accuarcy: 0.2836\n",
      "Epoch 1 step 119: training loss: 463845.515827573\n",
      "Epoch 1 step 120: training accuarcy: 0.2772\n",
      "Epoch 1 step 120: training loss: 461358.83543013706\n",
      "Epoch 1 step 121: training accuarcy: 0.2771\n",
      "Epoch 1 step 121: training loss: 458276.1446818058\n",
      "Epoch 1 step 122: training accuarcy: 0.28040000000000004\n",
      "Epoch 1 step 122: training loss: 462817.93154526915\n",
      "Epoch 1 step 123: training accuarcy: 0.27940000000000004\n",
      "Epoch 1 step 123: training loss: 455134.1138683906\n",
      "Epoch 1 step 124: training accuarcy: 0.2898\n",
      "Epoch 1 step 124: training loss: 466948.87889836164\n",
      "Epoch 1 step 125: training accuarcy: 0.2721\n",
      "Epoch 1 step 125: training loss: 455569.482729843\n",
      "Epoch 1 step 126: training accuarcy: 0.2862\n",
      "Epoch 1 step 126: training loss: 453084.3174610146\n",
      "Epoch 1 step 127: training accuarcy: 0.2937\n",
      "Epoch 1 step 127: training loss: 455068.93701651547\n",
      "Epoch 1 step 128: training accuarcy: 0.28850000000000003\n",
      "Epoch 1 step 128: training loss: 453750.5993428215\n",
      "Epoch 1 step 129: training accuarcy: 0.2921\n",
      "Epoch 1 step 129: training loss: 458757.72932708246\n",
      "Epoch 1 step 130: training accuarcy: 0.28390000000000004\n",
      "Epoch 1 step 130: training loss: 458263.41023204493\n",
      "Epoch 1 step 131: training accuarcy: 0.2853\n",
      "Epoch 1 step 131: training loss: 458018.0962687715\n",
      "Epoch 1 step 132: training accuarcy: 0.2857\n",
      "Epoch 1 step 132: training loss: 454813.4021973079\n",
      "Epoch 1 step 133: training accuarcy: 0.2912\n",
      "Epoch 1 step 133: training loss: 450478.57489051664\n",
      "Epoch 1 step 134: training accuarcy: 0.29550000000000004\n",
      "Epoch 1 step 134: training loss: 457523.55236522655\n",
      "Epoch 1 step 135: training accuarcy: 0.2869\n",
      "Epoch 1 step 135: training loss: 455294.4234259508\n",
      "Epoch 1 step 136: training accuarcy: 0.2892\n",
      "Epoch 1 step 136: training loss: 462516.41778380465\n",
      "Epoch 1 step 137: training accuarcy: 0.279\n",
      "Epoch 1 step 137: training loss: 462972.018529399\n",
      "Epoch 1 step 138: training accuarcy: 0.2806\n",
      "Epoch 1 step 138: training loss: 456974.90793865715\n",
      "Epoch 1 step 139: training accuarcy: 0.2852\n",
      "Epoch 1 step 139: training loss: 456015.22795345954\n",
      "Epoch 1 step 140: training accuarcy: 0.2866\n",
      "Epoch 1 step 140: training loss: 452928.62960144284\n",
      "Epoch 1 step 141: training accuarcy: 0.2906\n",
      "Epoch 1 step 141: training loss: 463645.3648198412\n",
      "Epoch 1 step 142: training accuarcy: 0.2745\n",
      "Epoch 1 step 142: training loss: 460936.63766897033\n",
      "Epoch 1 step 143: training accuarcy: 0.27940000000000004\n",
      "Epoch 1 step 143: training loss: 451020.37739875924\n",
      "Epoch 1 step 144: training accuarcy: 0.2938\n",
      "Epoch 1 step 144: training loss: 455242.9822592409\n",
      "Epoch 1 step 145: training accuarcy: 0.29100000000000004\n",
      "Epoch 1 step 145: training loss: 456070.74545046646\n",
      "Epoch 1 step 146: training accuarcy: 0.2857\n",
      "Epoch 1 step 146: training loss: 453789.4506970115\n",
      "Epoch 1 step 147: training accuarcy: 0.2909\n",
      "Epoch 1 step 147: training loss: 453671.1634734654\n",
      "Epoch 1 step 148: training accuarcy: 0.2876\n",
      "Epoch 1 step 148: training loss: 450624.39406839036\n",
      "Epoch 1 step 149: training accuarcy: 0.2964\n",
      "Epoch 1 step 149: training loss: 455853.6600019892\n",
      "Epoch 1 step 150: training accuarcy: 0.2894\n",
      "Epoch 1 step 150: training loss: 461646.3157678324\n",
      "Epoch 1 step 151: training accuarcy: 0.27790000000000004\n",
      "Epoch 1 step 151: training loss: 459005.473734151\n",
      "Epoch 1 step 152: training accuarcy: 0.2828\n",
      "Epoch 1 step 152: training loss: 459536.7709143455\n",
      "Epoch 1 step 153: training accuarcy: 0.2837\n",
      "Epoch 1 step 153: training loss: 457072.9399826328\n",
      "Epoch 1 step 154: training accuarcy: 0.2869\n",
      "Epoch 1 step 154: training loss: 460007.0202103434\n",
      "Epoch 1 step 155: training accuarcy: 0.2816\n",
      "Epoch 1 step 155: training loss: 460213.3041212713\n",
      "Epoch 1 step 156: training accuarcy: 0.27990000000000004\n",
      "Epoch 1 step 156: training loss: 461104.4675163685\n",
      "Epoch 1 step 157: training accuarcy: 0.281\n",
      "Epoch 1 step 157: training loss: 462800.87746713\n",
      "Epoch 1 step 158: training accuarcy: 0.2753\n",
      "Epoch 1 step 158: training loss: 458868.3485786754\n",
      "Epoch 1 step 159: training accuarcy: 0.2841\n",
      "Epoch 1 step 159: training loss: 462608.34050700633\n",
      "Epoch 1 step 160: training accuarcy: 0.2791\n",
      "Epoch 1 step 160: training loss: 457426.2185279021\n",
      "Epoch 1 step 161: training accuarcy: 0.2858\n",
      "Epoch 1 step 161: training loss: 455093.8666396156\n",
      "Epoch 1 step 162: training accuarcy: 0.2906\n",
      "Epoch 1 step 162: training loss: 457772.62640756625\n",
      "Epoch 1 step 163: training accuarcy: 0.28750000000000003\n",
      "Epoch 1 step 163: training loss: 456127.9034276935\n",
      "Epoch 1 step 164: training accuarcy: 0.2893\n",
      "Epoch 1 step 164: training loss: 460079.3844410273\n",
      "Epoch 1 step 165: training accuarcy: 0.28300000000000003\n",
      "Epoch 1 step 165: training loss: 459530.1920885343\n",
      "Epoch 1 step 166: training accuarcy: 0.2876\n",
      "Epoch 1 step 166: training loss: 458883.93448718125\n",
      "Epoch 1 step 167: training accuarcy: 0.2838\n",
      "Epoch 1 step 167: training loss: 449495.8246199837\n",
      "Epoch 1 step 168: training accuarcy: 0.2988\n",
      "Epoch 1 step 168: training loss: 450743.12773720326\n",
      "Epoch 1 step 169: training accuarcy: 0.2972\n",
      "Epoch 1 step 169: training loss: 454016.9298165146\n",
      "Epoch 1 step 170: training accuarcy: 0.29050000000000004\n",
      "Epoch 1 step 170: training loss: 449990.08687653707\n",
      "Epoch 1 step 171: training accuarcy: 0.29510000000000003\n",
      "Epoch 1 step 171: training loss: 455045.7630221904\n",
      "Epoch 1 step 172: training accuarcy: 0.2867\n",
      "Epoch 1 step 172: training loss: 450347.11424109736\n",
      "Epoch 1 step 173: training accuarcy: 0.2978\n",
      "Epoch 1 step 173: training loss: 454106.86246537114\n",
      "Epoch 1 step 174: training accuarcy: 0.2931\n",
      "Epoch 1 step 174: training loss: 455313.49895719555\n",
      "Epoch 1 step 175: training accuarcy: 0.29100000000000004\n",
      "Epoch 1 step 175: training loss: 452629.9750763674\n",
      "Epoch 1 step 176: training accuarcy: 0.2904\n",
      "Epoch 1 step 176: training loss: 456260.2229732395\n",
      "Epoch 1 step 177: training accuarcy: 0.2878\n",
      "Epoch 1 step 177: training loss: 458841.1216186787\n",
      "Epoch 1 step 178: training accuarcy: 0.28450000000000003\n",
      "Epoch 1 step 178: training loss: 461501.3539696871\n",
      "Epoch 1 step 179: training accuarcy: 0.281\n",
      "Epoch 1 step 179: training loss: 455789.6846780646\n",
      "Epoch 1 step 180: training accuarcy: 0.2911\n",
      "Epoch 1 step 180: training loss: 454907.763805301\n",
      "Epoch 1 step 181: training accuarcy: 0.2893\n",
      "Epoch 1 step 181: training loss: 456384.0630428895\n",
      "Epoch 1 step 182: training accuarcy: 0.29000000000000004\n",
      "Epoch 1 step 182: training loss: 447091.6914430132\n",
      "Epoch 1 step 183: training accuarcy: 0.3033\n",
      "Epoch 1 step 183: training loss: 454448.31405587913\n",
      "Epoch 1 step 184: training accuarcy: 0.2898\n",
      "Epoch 1 step 184: training loss: 456198.1700148111\n",
      "Epoch 1 step 185: training accuarcy: 0.2891\n",
      "Epoch 1 step 185: training loss: 453846.98618862237\n",
      "Epoch 1 step 186: training accuarcy: 0.2908\n",
      "Epoch 1 step 186: training loss: 455609.46464050625\n",
      "Epoch 1 step 187: training accuarcy: 0.2911\n",
      "Epoch 1 step 187: training loss: 452660.6591696407\n",
      "Epoch 1 step 188: training accuarcy: 0.2928\n",
      "Epoch 1 step 188: training loss: 451374.452754744\n",
      "Epoch 1 step 189: training accuarcy: 0.29650000000000004\n",
      "Epoch 1 step 189: training loss: 447325.86379135714\n",
      "Epoch 1 step 190: training accuarcy: 0.3018\n",
      "Epoch 1 step 190: training loss: 454206.8052164933\n",
      "Epoch 1 step 191: training accuarcy: 0.2913\n",
      "Epoch 1 step 191: training loss: 452691.96776348545\n",
      "Epoch 1 step 192: training accuarcy: 0.2943\n",
      "Epoch 1 step 192: training loss: 452349.21762124554\n",
      "Epoch 1 step 193: training accuarcy: 0.2929\n",
      "Epoch 1 step 193: training loss: 452536.9756953106\n",
      "Epoch 1 step 194: training accuarcy: 0.2927\n",
      "Epoch 1 step 194: training loss: 455117.7352303283\n",
      "Epoch 1 step 195: training accuarcy: 0.2892\n",
      "Epoch 1 step 195: training loss: 455083.8890904381\n",
      "Epoch 1 step 196: training accuarcy: 0.2899\n",
      "Epoch 1 step 196: training loss: 454741.46254248294\n",
      "Epoch 1 step 197: training accuarcy: 0.2929\n",
      "Epoch 1 step 197: training loss: 455858.6626780862\n",
      "Epoch 1 step 198: training accuarcy: 0.29000000000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 198: training loss: 453849.4865175936\n",
      "Epoch 1 step 199: training accuarcy: 0.29100000000000004\n",
      "Epoch 1 step 199: training loss: 454847.26430594985\n",
      "Epoch 1 step 200: training accuarcy: 0.2914\n",
      "Epoch 1 step 200: training loss: 452340.755908188\n",
      "Epoch 1 step 201: training accuarcy: 0.2909\n",
      "Epoch 1 step 201: training loss: 449426.9151146434\n",
      "Epoch 1 step 202: training accuarcy: 0.2987\n",
      "Epoch 1 step 202: training loss: 454528.04437924037\n",
      "Epoch 1 step 203: training accuarcy: 0.2896\n",
      "Epoch 1 step 203: training loss: 455821.61266889266\n",
      "Epoch 1 step 204: training accuarcy: 0.28900000000000003\n",
      "Epoch 1 step 204: training loss: 451911.4399040594\n",
      "Epoch 1 step 205: training accuarcy: 0.29300000000000004\n",
      "Epoch 1 step 205: training loss: 448254.6464454324\n",
      "Epoch 1 step 206: training accuarcy: 0.29910000000000003\n",
      "Epoch 1 step 206: training loss: 450619.50617974723\n",
      "Epoch 1 step 207: training accuarcy: 0.2969\n",
      "Epoch 1 step 207: training loss: 455622.871717773\n",
      "Epoch 1 step 208: training accuarcy: 0.2881\n",
      "Epoch 1 step 208: training loss: 454988.53291980916\n",
      "Epoch 1 step 209: training accuarcy: 0.2901\n",
      "Epoch 1 step 209: training loss: 460251.39472465526\n",
      "Epoch 1 step 210: training accuarcy: 0.2833\n",
      "Epoch 1 step 210: training loss: 451334.9881755911\n",
      "Epoch 1 step 211: training accuarcy: 0.2957\n",
      "Epoch 1 step 211: training loss: 452412.50969102944\n",
      "Epoch 1 step 212: training accuarcy: 0.2903\n",
      "Epoch 1 step 212: training loss: 454562.51751653693\n",
      "Epoch 1 step 213: training accuarcy: 0.2901\n",
      "Epoch 1 step 213: training loss: 456865.09791908343\n",
      "Epoch 1 step 214: training accuarcy: 0.2889\n",
      "Epoch 1 step 214: training loss: 445898.24935317173\n",
      "Epoch 1 step 215: training accuarcy: 0.3034\n",
      "Epoch 1 step 215: training loss: 458514.8929808454\n",
      "Epoch 1 step 216: training accuarcy: 0.2837\n",
      "Epoch 1 step 216: training loss: 453739.1043239766\n",
      "Epoch 1 step 217: training accuarcy: 0.29300000000000004\n",
      "Epoch 1 step 217: training loss: 452984.1228462538\n",
      "Epoch 1 step 218: training accuarcy: 0.2922\n",
      "Epoch 1 step 218: training loss: 452399.49573726533\n",
      "Epoch 1 step 219: training accuarcy: 0.2954\n",
      "Epoch 1 step 219: training loss: 451516.3304283415\n",
      "Epoch 1 step 220: training accuarcy: 0.298\n",
      "Epoch 1 step 220: training loss: 452431.89580219705\n",
      "Epoch 1 step 221: training accuarcy: 0.29400000000000004\n",
      "Epoch 1 step 221: training loss: 458637.4840467521\n",
      "Epoch 1 step 222: training accuarcy: 0.2862\n",
      "Epoch 1 step 222: training loss: 454860.4617188674\n",
      "Epoch 1 step 223: training accuarcy: 0.2891\n",
      "Epoch 1 step 223: training loss: 460526.72433774034\n",
      "Epoch 1 step 224: training accuarcy: 0.2833\n",
      "Epoch 1 step 224: training loss: 455080.4450778739\n",
      "Epoch 1 step 225: training accuarcy: 0.2918\n",
      "Epoch 1 step 225: training loss: 455270.76147435664\n",
      "Epoch 1 step 226: training accuarcy: 0.2922\n",
      "Epoch 1 step 226: training loss: 454607.76534348715\n",
      "Epoch 1 step 227: training accuarcy: 0.2907\n",
      "Epoch 1 step 227: training loss: 449539.51999088173\n",
      "Epoch 1 step 228: training accuarcy: 0.2989\n",
      "Epoch 1 step 228: training loss: 454417.9719190964\n",
      "Epoch 1 step 229: training accuarcy: 0.29400000000000004\n",
      "Epoch 1 step 229: training loss: 438940.399950425\n",
      "Epoch 1 step 230: training accuarcy: 0.28259958071278823\n",
      "Epoch 1: train loss 455446.85004514555, train accuarcy 0.2950201630592346\n",
      "Epoch 1: valid loss 456976.8984410852, valid accuarcy 0.2795865535736084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [22:03<00:00, 662.51s/it]"
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=2,\n",
    "                log_dir=get_log_dir('weight_kaggle', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T06:11:32.587238Z",
     "start_time": "2019-10-08T06:11:32.581264Z"
    }
   },
   "outputs": [],
   "source": [
    "del hrm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PRME FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T06:38:25.950397Z",
     "start_time": "2019-10-08T06:38:25.879426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchPrmeFM()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_model = TorchPrmeFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "prme_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T06:38:26.401275Z",
     "start_time": "2019-10-08T06:38:26.398278Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(prme_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T06:38:27.906571Z",
     "start_time": "2019-10-08T06:38:27.878571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1dbc0a6e828>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_learner = FMLearner(prme_model, adam_opt, schedular, db)\n",
    "prme_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:31:00.334783Z",
     "start_time": "2019-10-07T12:31:00.330751Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T06:12:04.897454Z",
     "start_time": "2019-10-08T06:12:04.894457Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='seq',\n",
    "                     valid_col='seq',\n",
    "                     test_col='seq',\n",
    "                     loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T06:38:31.585413Z",
     "start_time": "2019-10-08T06:38:31.582411Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='seq',\n",
    "                     valid_col='seq',\n",
    "                     test_col='seq',\n",
    "                     loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T06:35:48.669447Z",
     "start_time": "2019-10-08T06:12:51.351380Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 185907.28668004088\n",
      "Epoch 0 step 1: training accuarcy: 0.12150000000000001\n",
      "Epoch 0 step 1: training loss: 185854.96380865783\n",
      "Epoch 0 step 2: training accuarcy: 0.1522\n",
      "Epoch 0 step 2: training loss: 184933.9662284925\n",
      "Epoch 0 step 3: training accuarcy: 0.1831\n",
      "Epoch 0 step 3: training loss: 183437.2582335082\n",
      "Epoch 0 step 4: training accuarcy: 0.2101\n",
      "Epoch 0 step 4: training loss: 183257.959741619\n",
      "Epoch 0 step 5: training accuarcy: 0.2184\n",
      "Epoch 0 step 5: training loss: 182150.9192518685\n",
      "Epoch 0 step 6: training accuarcy: 0.23020000000000002\n",
      "Epoch 0 step 6: training loss: 184614.11063343473\n",
      "Epoch 0 step 7: training accuarcy: 0.2215\n",
      "Epoch 0 step 7: training loss: 183668.9743565071\n",
      "Epoch 0 step 8: training accuarcy: 0.22410000000000002\n",
      "Epoch 0 step 8: training loss: 184832.76475309592\n",
      "Epoch 0 step 9: training accuarcy: 0.22010000000000002\n",
      "Epoch 0 step 9: training loss: 181807.46099861656\n",
      "Epoch 0 step 10: training accuarcy: 0.23290000000000002\n",
      "Epoch 0 step 10: training loss: 180918.4783905069\n",
      "Epoch 0 step 11: training accuarcy: 0.233\n",
      "Epoch 0 step 11: training loss: 181720.59401471537\n",
      "Epoch 0 step 12: training accuarcy: 0.22870000000000001\n",
      "Epoch 0 step 12: training loss: 181819.38574652493\n",
      "Epoch 0 step 13: training accuarcy: 0.2267\n",
      "Epoch 0 step 13: training loss: 180159.36061200083\n",
      "Epoch 0 step 14: training accuarcy: 0.23240000000000002\n",
      "Epoch 0 step 14: training loss: 182300.34943543415\n",
      "Epoch 0 step 15: training accuarcy: 0.2205\n",
      "Epoch 0 step 15: training loss: 181008.38831469513\n",
      "Epoch 0 step 16: training accuarcy: 0.2263\n",
      "Epoch 0 step 16: training loss: 177814.7728689931\n",
      "Epoch 0 step 17: training accuarcy: 0.23720000000000002\n",
      "Epoch 0 step 17: training loss: 179384.6780612785\n",
      "Epoch 0 step 18: training accuarcy: 0.22970000000000002\n",
      "Epoch 0 step 18: training loss: 180522.15941947937\n",
      "Epoch 0 step 19: training accuarcy: 0.22360000000000002\n",
      "Epoch 0 step 19: training loss: 179466.109302676\n",
      "Epoch 0 step 20: training accuarcy: 0.232\n",
      "Epoch 0 step 20: training loss: 178613.01611304993\n",
      "Epoch 0 step 21: training accuarcy: 0.2331\n",
      "Epoch 0 step 21: training loss: 179563.2098408711\n",
      "Epoch 0 step 22: training accuarcy: 0.2315\n",
      "Epoch 0 step 22: training loss: 180280.5632236082\n",
      "Epoch 0 step 23: training accuarcy: 0.22990000000000002\n",
      "Epoch 0 step 23: training loss: 178284.786633743\n",
      "Epoch 0 step 24: training accuarcy: 0.2354\n",
      "Epoch 0 step 24: training loss: 177064.7503580796\n",
      "Epoch 0 step 25: training accuarcy: 0.2409\n",
      "Epoch 0 step 25: training loss: 178915.7209537946\n",
      "Epoch 0 step 26: training accuarcy: 0.2338\n",
      "Epoch 0 step 26: training loss: 177085.32091789984\n",
      "Epoch 0 step 27: training accuarcy: 0.2403\n",
      "Epoch 0 step 27: training loss: 178256.46076950023\n",
      "Epoch 0 step 28: training accuarcy: 0.23770000000000002\n",
      "Epoch 0 step 28: training loss: 177642.3672588989\n",
      "Epoch 0 step 29: training accuarcy: 0.2398\n",
      "Epoch 0 step 29: training loss: 177720.32373125775\n",
      "Epoch 0 step 30: training accuarcy: 0.23870000000000002\n",
      "Epoch 0 step 30: training loss: 178019.63164040804\n",
      "Epoch 0 step 31: training accuarcy: 0.23720000000000002\n",
      "Epoch 0 step 31: training loss: 177487.21098037844\n",
      "Epoch 0 step 32: training accuarcy: 0.23800000000000002\n",
      "Epoch 0 step 32: training loss: 177998.86645412215\n",
      "Epoch 0 step 33: training accuarcy: 0.2349\n",
      "Epoch 0 step 33: training loss: 176449.41799662163\n",
      "Epoch 0 step 34: training accuarcy: 0.2442\n",
      "Epoch 0 step 34: training loss: 174629.4936837907\n",
      "Epoch 0 step 35: training accuarcy: 0.2504\n",
      "Epoch 0 step 35: training loss: 176806.70453255365\n",
      "Epoch 0 step 36: training accuarcy: 0.2388\n",
      "Epoch 0 step 36: training loss: 176805.8484529916\n",
      "Epoch 0 step 37: training accuarcy: 0.2384\n",
      "Epoch 0 step 37: training loss: 175571.25479913046\n",
      "Epoch 0 step 38: training accuarcy: 0.2442\n",
      "Epoch 0 step 38: training loss: 175146.79007184008\n",
      "Epoch 0 step 39: training accuarcy: 0.24480000000000002\n",
      "Epoch 0 step 39: training loss: 174834.55415570101\n",
      "Epoch 0 step 40: training accuarcy: 0.2446\n",
      "Epoch 0 step 40: training loss: 175815.76192329786\n",
      "Epoch 0 step 41: training accuarcy: 0.24050000000000002\n",
      "Epoch 0 step 41: training loss: 176584.13605914556\n",
      "Epoch 0 step 42: training accuarcy: 0.2388\n",
      "Epoch 0 step 42: training loss: 175802.96551670542\n",
      "Epoch 0 step 43: training accuarcy: 0.2386\n",
      "Epoch 0 step 43: training loss: 174711.7562171638\n",
      "Epoch 0 step 44: training accuarcy: 0.2422\n",
      "Epoch 0 step 44: training loss: 175369.95828659867\n",
      "Epoch 0 step 45: training accuarcy: 0.24080000000000001\n",
      "Epoch 0 step 45: training loss: 175769.2190513385\n",
      "Epoch 0 step 46: training accuarcy: 0.2398\n",
      "Epoch 0 step 46: training loss: 173940.1480027482\n",
      "Epoch 0 step 47: training accuarcy: 0.24730000000000002\n",
      "Epoch 0 step 47: training loss: 175147.2562193723\n",
      "Epoch 0 step 48: training accuarcy: 0.24000000000000002\n",
      "Epoch 0 step 48: training loss: 174337.93645074696\n",
      "Epoch 0 step 49: training accuarcy: 0.2461\n",
      "Epoch 0 step 49: training loss: 174361.35106587238\n",
      "Epoch 0 step 50: training accuarcy: 0.24400000000000002\n",
      "Epoch 0 step 50: training loss: 175409.3216813453\n",
      "Epoch 0 step 51: training accuarcy: 0.2363\n",
      "Epoch 0 step 51: training loss: 174632.18279504968\n",
      "Epoch 0 step 52: training accuarcy: 0.24080000000000001\n",
      "Epoch 0 step 52: training loss: 174014.510369268\n",
      "Epoch 0 step 53: training accuarcy: 0.2447\n",
      "Epoch 0 step 53: training loss: 173689.18961144972\n",
      "Epoch 0 step 54: training accuarcy: 0.2459\n",
      "Epoch 0 step 54: training loss: 173750.89470920994\n",
      "Epoch 0 step 55: training accuarcy: 0.24200000000000002\n",
      "Epoch 0 step 55: training loss: 174004.74628949692\n",
      "Epoch 0 step 56: training accuarcy: 0.2452\n",
      "Epoch 0 step 56: training loss: 174614.05525714974\n",
      "Epoch 0 step 57: training accuarcy: 0.2401\n",
      "Epoch 0 step 57: training loss: 175026.97037351353\n",
      "Epoch 0 step 58: training accuarcy: 0.2396\n",
      "Epoch 0 step 58: training loss: 176257.7535641349\n",
      "Epoch 0 step 59: training accuarcy: 0.2354\n",
      "Epoch 0 step 59: training loss: 173935.9963717385\n",
      "Epoch 0 step 60: training accuarcy: 0.2441\n",
      "Epoch 0 step 60: training loss: 174853.5838936753\n",
      "Epoch 0 step 61: training accuarcy: 0.2403\n",
      "Epoch 0 step 61: training loss: 174503.87511669556\n",
      "Epoch 0 step 62: training accuarcy: 0.24050000000000002\n",
      "Epoch 0 step 62: training loss: 175658.43848053142\n",
      "Epoch 0 step 63: training accuarcy: 0.23550000000000001\n",
      "Epoch 0 step 63: training loss: 174901.69809681427\n",
      "Epoch 0 step 64: training accuarcy: 0.23750000000000002\n",
      "Epoch 0 step 64: training loss: 174353.35364634797\n",
      "Epoch 0 step 65: training accuarcy: 0.24080000000000001\n",
      "Epoch 0 step 65: training loss: 175129.04286602032\n",
      "Epoch 0 step 66: training accuarcy: 0.2378\n",
      "Epoch 0 step 66: training loss: 173055.26800999587\n",
      "Epoch 0 step 67: training accuarcy: 0.24630000000000002\n",
      "Epoch 0 step 67: training loss: 174277.53720136097\n",
      "Epoch 0 step 68: training accuarcy: 0.2401\n",
      "Epoch 0 step 68: training loss: 171748.0837145401\n",
      "Epoch 0 step 69: training accuarcy: 0.2516\n",
      "Epoch 0 step 69: training loss: 171855.12709582542\n",
      "Epoch 0 step 70: training accuarcy: 0.2497\n",
      "Epoch 0 step 70: training loss: 173570.49187525996\n",
      "Epoch 0 step 71: training accuarcy: 0.2421\n",
      "Epoch 0 step 71: training loss: 175412.69874861947\n",
      "Epoch 0 step 72: training accuarcy: 0.2356\n",
      "Epoch 0 step 72: training loss: 172112.03816649597\n",
      "Epoch 0 step 73: training accuarcy: 0.24580000000000002\n",
      "Epoch 0 step 73: training loss: 174796.56754051556\n",
      "Epoch 0 step 74: training accuarcy: 0.23650000000000002\n",
      "Epoch 0 step 74: training loss: 173107.1379248992\n",
      "Epoch 0 step 75: training accuarcy: 0.2432\n",
      "Epoch 0 step 75: training loss: 172863.31857239318\n",
      "Epoch 0 step 76: training accuarcy: 0.2436\n",
      "Epoch 0 step 76: training loss: 175181.98539090288\n",
      "Epoch 0 step 77: training accuarcy: 0.23570000000000002\n",
      "Epoch 0 step 77: training loss: 173288.14717243865\n",
      "Epoch 0 step 78: training accuarcy: 0.24180000000000001\n",
      "Epoch 0 step 78: training loss: 172878.59222088198\n",
      "Epoch 0 step 79: training accuarcy: 0.24550000000000002\n",
      "Epoch 0 step 79: training loss: 172570.84168695996\n",
      "Epoch 0 step 80: training accuarcy: 0.24580000000000002\n",
      "Epoch 0 step 80: training loss: 171067.229470953\n",
      "Epoch 0 step 81: training accuarcy: 0.24960000000000002\n",
      "Epoch 0 step 81: training loss: 173570.88379809502\n",
      "Epoch 0 step 82: training accuarcy: 0.2409\n",
      "Epoch 0 step 82: training loss: 173097.46984980942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 83: training accuarcy: 0.24200000000000002\n",
      "Epoch 0 step 83: training loss: 174757.0904206139\n",
      "Epoch 0 step 84: training accuarcy: 0.2341\n",
      "Epoch 0 step 84: training loss: 173827.0898328011\n",
      "Epoch 0 step 85: training accuarcy: 0.2389\n",
      "Epoch 0 step 85: training loss: 171825.06579376027\n",
      "Epoch 0 step 86: training accuarcy: 0.2494\n",
      "Epoch 0 step 86: training loss: 173857.12502389672\n",
      "Epoch 0 step 87: training accuarcy: 0.24020000000000002\n",
      "Epoch 0 step 87: training loss: 173202.98805840008\n",
      "Epoch 0 step 88: training accuarcy: 0.2432\n",
      "Epoch 0 step 88: training loss: 172927.33162710498\n",
      "Epoch 0 step 89: training accuarcy: 0.2442\n",
      "Epoch 0 step 89: training loss: 171977.56767326145\n",
      "Epoch 0 step 90: training accuarcy: 0.2482\n",
      "Epoch 0 step 90: training loss: 173432.23474081184\n",
      "Epoch 0 step 91: training accuarcy: 0.24150000000000002\n",
      "Epoch 0 step 91: training loss: 172398.09092915198\n",
      "Epoch 0 step 92: training accuarcy: 0.2461\n",
      "Epoch 0 step 92: training loss: 172844.4220802758\n",
      "Epoch 0 step 93: training accuarcy: 0.2436\n",
      "Epoch 0 step 93: training loss: 173921.08613509723\n",
      "Epoch 0 step 94: training accuarcy: 0.23800000000000002\n",
      "Epoch 0 step 94: training loss: 171692.99065968383\n",
      "Epoch 0 step 95: training accuarcy: 0.2477\n",
      "Epoch 0 step 95: training loss: 173803.6541614906\n",
      "Epoch 0 step 96: training accuarcy: 0.23920000000000002\n",
      "Epoch 0 step 96: training loss: 172849.09622851823\n",
      "Epoch 0 step 97: training accuarcy: 0.2444\n",
      "Epoch 0 step 97: training loss: 172209.6705160397\n",
      "Epoch 0 step 98: training accuarcy: 0.2451\n",
      "Epoch 0 step 98: training loss: 170750.56360217655\n",
      "Epoch 0 step 99: training accuarcy: 0.2534\n",
      "Epoch 0 step 99: training loss: 174099.16331430917\n",
      "Epoch 0 step 100: training accuarcy: 0.2391\n",
      "Epoch 0 step 100: training loss: 172943.23292769297\n",
      "Epoch 0 step 101: training accuarcy: 0.2427\n",
      "Epoch 0 step 101: training loss: 173109.62669817204\n",
      "Epoch 0 step 102: training accuarcy: 0.2396\n",
      "Epoch 0 step 102: training loss: 171716.41276721985\n",
      "Epoch 0 step 103: training accuarcy: 0.2457\n",
      "Epoch 0 step 103: training loss: 173494.91644861465\n",
      "Epoch 0 step 104: training accuarcy: 0.2411\n",
      "Epoch 0 step 104: training loss: 172925.48047438034\n",
      "Epoch 0 step 105: training accuarcy: 0.24250000000000002\n",
      "Epoch 0 step 105: training loss: 172443.47351783523\n",
      "Epoch 0 step 106: training accuarcy: 0.24530000000000002\n",
      "Epoch 0 step 106: training loss: 172759.46437688422\n",
      "Epoch 0 step 107: training accuarcy: 0.2444\n",
      "Epoch 0 step 107: training loss: 170064.6678673895\n",
      "Epoch 0 step 108: training accuarcy: 0.253\n",
      "Epoch 0 step 108: training loss: 173466.25704373053\n",
      "Epoch 0 step 109: training accuarcy: 0.23970000000000002\n",
      "Epoch 0 step 109: training loss: 173602.70682423675\n",
      "Epoch 0 step 110: training accuarcy: 0.2379\n",
      "Epoch 0 step 110: training loss: 173042.4498841503\n",
      "Epoch 0 step 111: training accuarcy: 0.24200000000000002\n",
      "Epoch 0 step 111: training loss: 174463.13048807273\n",
      "Epoch 0 step 112: training accuarcy: 0.2358\n",
      "Epoch 0 step 112: training loss: 172496.92249598016\n",
      "Epoch 0 step 113: training accuarcy: 0.2449\n",
      "Epoch 0 step 113: training loss: 172912.72047429538\n",
      "Epoch 0 step 114: training accuarcy: 0.2419\n",
      "Epoch 0 step 114: training loss: 166966.92060216988\n",
      "Epoch 0 step 115: training accuarcy: 0.23375262054507337\n",
      "Epoch 0: train loss 175744.08145620846, train accuarcy 0.22976061701774597\n",
      "Epoch 0: valid loss 167355.36079226734, valid accuarcy 0.2532176971435547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [11:27<11:27, 687.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 173367.75662909678\n",
      "Epoch 1 step 116: training accuarcy: 0.2401\n",
      "Epoch 1 step 116: training loss: 172121.59036878622\n",
      "Epoch 1 step 117: training accuarcy: 0.2482\n",
      "Epoch 1 step 117: training loss: 173394.39221695228\n",
      "Epoch 1 step 118: training accuarcy: 0.2393\n",
      "Epoch 1 step 118: training loss: 172953.98294053925\n",
      "Epoch 1 step 119: training accuarcy: 0.2427\n",
      "Epoch 1 step 119: training loss: 174123.6051510823\n",
      "Epoch 1 step 120: training accuarcy: 0.2374\n",
      "Epoch 1 step 120: training loss: 172814.79981924684\n",
      "Epoch 1 step 121: training accuarcy: 0.24350000000000002\n",
      "Epoch 1 step 121: training loss: 172134.87371858204\n",
      "Epoch 1 step 122: training accuarcy: 0.24480000000000002\n",
      "Epoch 1 step 122: training loss: 172971.76418508976\n",
      "Epoch 1 step 123: training accuarcy: 0.24080000000000001\n",
      "Epoch 1 step 123: training loss: 175290.1779008029\n",
      "Epoch 1 step 124: training accuarcy: 0.2305\n",
      "Epoch 1 step 124: training loss: 172473.34178168638\n",
      "Epoch 1 step 125: training accuarcy: 0.2447\n",
      "Epoch 1 step 125: training loss: 173838.94431616785\n",
      "Epoch 1 step 126: training accuarcy: 0.2394\n",
      "Epoch 1 step 126: training loss: 172956.0809088369\n",
      "Epoch 1 step 127: training accuarcy: 0.2432\n",
      "Epoch 1 step 127: training loss: 173070.43812920066\n",
      "Epoch 1 step 128: training accuarcy: 0.24230000000000002\n",
      "Epoch 1 step 128: training loss: 173101.8113226741\n",
      "Epoch 1 step 129: training accuarcy: 0.23920000000000002\n",
      "Epoch 1 step 129: training loss: 172824.26885301256\n",
      "Epoch 1 step 130: training accuarcy: 0.2432\n",
      "Epoch 1 step 130: training loss: 171222.01572911046\n",
      "Epoch 1 step 131: training accuarcy: 0.25120000000000003\n",
      "Epoch 1 step 131: training loss: 173325.89656514954\n",
      "Epoch 1 step 132: training accuarcy: 0.2389\n",
      "Epoch 1 step 132: training loss: 172080.01535582126\n",
      "Epoch 1 step 133: training accuarcy: 0.24500000000000002\n",
      "Epoch 1 step 133: training loss: 174366.561600666\n",
      "Epoch 1 step 134: training accuarcy: 0.2363\n",
      "Epoch 1 step 134: training loss: 172511.53953411902\n",
      "Epoch 1 step 135: training accuarcy: 0.2431\n",
      "Epoch 1 step 135: training loss: 171745.4684827694\n",
      "Epoch 1 step 136: training accuarcy: 0.2456\n",
      "Epoch 1 step 136: training loss: 169797.95529124438\n",
      "Epoch 1 step 137: training accuarcy: 0.25320000000000004\n",
      "Epoch 1 step 137: training loss: 171145.94107474317\n",
      "Epoch 1 step 138: training accuarcy: 0.25070000000000003\n",
      "Epoch 1 step 138: training loss: 172194.6741320155\n",
      "Epoch 1 step 139: training accuarcy: 0.2451\n",
      "Epoch 1 step 139: training loss: 169450.7808332839\n",
      "Epoch 1 step 140: training accuarcy: 0.2566\n",
      "Epoch 1 step 140: training loss: 170045.89449391048\n",
      "Epoch 1 step 141: training accuarcy: 0.2504\n",
      "Epoch 1 step 141: training loss: 168531.04338997148\n",
      "Epoch 1 step 142: training accuarcy: 0.25880000000000003\n",
      "Epoch 1 step 142: training loss: 170354.806501039\n",
      "Epoch 1 step 143: training accuarcy: 0.2545\n",
      "Epoch 1 step 143: training loss: 168212.54489627946\n",
      "Epoch 1 step 144: training accuarcy: 0.2626\n",
      "Epoch 1 step 144: training loss: 170033.31024107637\n",
      "Epoch 1 step 145: training accuarcy: 0.2553\n",
      "Epoch 1 step 145: training loss: 168800.8814047915\n",
      "Epoch 1 step 146: training accuarcy: 0.2605\n",
      "Epoch 1 step 146: training loss: 171040.94426966718\n",
      "Epoch 1 step 147: training accuarcy: 0.24910000000000002\n",
      "Epoch 1 step 147: training loss: 170522.98242517497\n",
      "Epoch 1 step 148: training accuarcy: 0.2536\n",
      "Epoch 1 step 148: training loss: 167677.59455738668\n",
      "Epoch 1 step 149: training accuarcy: 0.2655\n",
      "Epoch 1 step 149: training loss: 167340.73530720302\n",
      "Epoch 1 step 150: training accuarcy: 0.2665\n",
      "Epoch 1 step 150: training loss: 168236.49273939768\n",
      "Epoch 1 step 151: training accuarcy: 0.2634\n",
      "Epoch 1 step 151: training loss: 167472.4263937484\n",
      "Epoch 1 step 152: training accuarcy: 0.2644\n",
      "Epoch 1 step 152: training loss: 169936.42636778756\n",
      "Epoch 1 step 153: training accuarcy: 0.2558\n",
      "Epoch 1 step 153: training loss: 167516.47009784688\n",
      "Epoch 1 step 154: training accuarcy: 0.2661\n",
      "Epoch 1 step 154: training loss: 167736.75025215064\n",
      "Epoch 1 step 155: training accuarcy: 0.2636\n",
      "Epoch 1 step 155: training loss: 169547.04454145598\n",
      "Epoch 1 step 156: training accuarcy: 0.2565\n",
      "Epoch 1 step 156: training loss: 165992.85680183556\n",
      "Epoch 1 step 157: training accuarcy: 0.2726\n",
      "Epoch 1 step 157: training loss: 166004.64275366018\n",
      "Epoch 1 step 158: training accuarcy: 0.271\n",
      "Epoch 1 step 158: training loss: 166429.69641683105\n",
      "Epoch 1 step 159: training accuarcy: 0.2701\n",
      "Epoch 1 step 159: training loss: 166950.9689335327\n",
      "Epoch 1 step 160: training accuarcy: 0.2672\n",
      "Epoch 1 step 160: training loss: 169252.62860909177\n",
      "Epoch 1 step 161: training accuarcy: 0.25620000000000004\n",
      "Epoch 1 step 161: training loss: 163814.0877973605\n",
      "Epoch 1 step 162: training accuarcy: 0.2757\n",
      "Epoch 1 step 162: training loss: 165001.99690897777\n",
      "Epoch 1 step 163: training accuarcy: 0.2757\n",
      "Epoch 1 step 163: training loss: 165884.132644157\n",
      "Epoch 1 step 164: training accuarcy: 0.27390000000000003\n",
      "Epoch 1 step 164: training loss: 164479.23307383637\n",
      "Epoch 1 step 165: training accuarcy: 0.2787\n",
      "Epoch 1 step 165: training loss: 165680.43453560874\n",
      "Epoch 1 step 166: training accuarcy: 0.2752\n",
      "Epoch 1 step 166: training loss: 165241.680521091\n",
      "Epoch 1 step 167: training accuarcy: 0.2768\n",
      "Epoch 1 step 167: training loss: 166241.60332005465\n",
      "Epoch 1 step 168: training accuarcy: 0.2723\n",
      "Epoch 1 step 168: training loss: 166449.36033223473\n",
      "Epoch 1 step 169: training accuarcy: 0.27\n",
      "Epoch 1 step 169: training loss: 165936.08293027294\n",
      "Epoch 1 step 170: training accuarcy: 0.2731\n",
      "Epoch 1 step 170: training loss: 166601.5522095831\n",
      "Epoch 1 step 171: training accuarcy: 0.268\n",
      "Epoch 1 step 171: training loss: 163876.22769342767\n",
      "Epoch 1 step 172: training accuarcy: 0.2816\n",
      "Epoch 1 step 172: training loss: 164309.19422752058\n",
      "Epoch 1 step 173: training accuarcy: 0.2806\n",
      "Epoch 1 step 173: training loss: 165447.81802820397\n",
      "Epoch 1 step 174: training accuarcy: 0.274\n",
      "Epoch 1 step 174: training loss: 167351.56354876838\n",
      "Epoch 1 step 175: training accuarcy: 0.267\n",
      "Epoch 1 step 175: training loss: 163241.73579288772\n",
      "Epoch 1 step 176: training accuarcy: 0.2842\n",
      "Epoch 1 step 176: training loss: 164324.01463800285\n",
      "Epoch 1 step 177: training accuarcy: 0.2805\n",
      "Epoch 1 step 177: training loss: 164552.8044013518\n",
      "Epoch 1 step 178: training accuarcy: 0.2798\n",
      "Epoch 1 step 178: training loss: 164250.15031600406\n",
      "Epoch 1 step 179: training accuarcy: 0.28\n",
      "Epoch 1 step 179: training loss: 165027.57930030566\n",
      "Epoch 1 step 180: training accuarcy: 0.27790000000000004\n",
      "Epoch 1 step 180: training loss: 165493.28810319435\n",
      "Epoch 1 step 181: training accuarcy: 0.2745\n",
      "Epoch 1 step 181: training loss: 162009.2732526338\n",
      "Epoch 1 step 182: training accuarcy: 0.2908\n",
      "Epoch 1 step 182: training loss: 163728.75558169483\n",
      "Epoch 1 step 183: training accuarcy: 0.2848\n",
      "Epoch 1 step 183: training loss: 164076.52549066886\n",
      "Epoch 1 step 184: training accuarcy: 0.2821\n",
      "Epoch 1 step 184: training loss: 161371.35663960263\n",
      "Epoch 1 step 185: training accuarcy: 0.29300000000000004\n",
      "Epoch 1 step 185: training loss: 163770.8980800905\n",
      "Epoch 1 step 186: training accuarcy: 0.281\n",
      "Epoch 1 step 186: training loss: 162314.6674456542\n",
      "Epoch 1 step 187: training accuarcy: 0.28850000000000003\n",
      "Epoch 1 step 187: training loss: 163429.27623161772\n",
      "Epoch 1 step 188: training accuarcy: 0.2854\n",
      "Epoch 1 step 188: training loss: 163920.77365827558\n",
      "Epoch 1 step 189: training accuarcy: 0.2798\n",
      "Epoch 1 step 189: training loss: 162125.65707809106\n",
      "Epoch 1 step 190: training accuarcy: 0.2897\n",
      "Epoch 1 step 190: training loss: 163811.53322348473\n",
      "Epoch 1 step 191: training accuarcy: 0.2838\n",
      "Epoch 1 step 191: training loss: 161897.65829799185\n",
      "Epoch 1 step 192: training accuarcy: 0.2907\n",
      "Epoch 1 step 192: training loss: 164118.5643245721\n",
      "Epoch 1 step 193: training accuarcy: 0.2823\n",
      "Epoch 1 step 193: training loss: 163814.13621051793\n",
      "Epoch 1 step 194: training accuarcy: 0.2838\n",
      "Epoch 1 step 194: training loss: 161588.59887185643\n",
      "Epoch 1 step 195: training accuarcy: 0.29410000000000003\n",
      "Epoch 1 step 195: training loss: 162468.7749903696\n",
      "Epoch 1 step 196: training accuarcy: 0.28850000000000003\n",
      "Epoch 1 step 196: training loss: 164064.3232197402\n",
      "Epoch 1 step 197: training accuarcy: 0.2807\n",
      "Epoch 1 step 197: training loss: 163229.06189767417\n",
      "Epoch 1 step 198: training accuarcy: 0.28550000000000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 198: training loss: 162610.53283019626\n",
      "Epoch 1 step 199: training accuarcy: 0.2891\n",
      "Epoch 1 step 199: training loss: 162199.46468374698\n",
      "Epoch 1 step 200: training accuarcy: 0.2898\n",
      "Epoch 1 step 200: training loss: 162427.2045080176\n",
      "Epoch 1 step 201: training accuarcy: 0.2886\n",
      "Epoch 1 step 201: training loss: 164564.78317812993\n",
      "Epoch 1 step 202: training accuarcy: 0.28140000000000004\n",
      "Epoch 1 step 202: training loss: 161991.48053854122\n",
      "Epoch 1 step 203: training accuarcy: 0.29250000000000004\n",
      "Epoch 1 step 203: training loss: 164103.1041296549\n",
      "Epoch 1 step 204: training accuarcy: 0.28150000000000003\n",
      "Epoch 1 step 204: training loss: 163178.19183562676\n",
      "Epoch 1 step 205: training accuarcy: 0.2874\n",
      "Epoch 1 step 205: training loss: 163649.64187065902\n",
      "Epoch 1 step 206: training accuarcy: 0.28450000000000003\n",
      "Epoch 1 step 206: training loss: 159755.57006093411\n",
      "Epoch 1 step 207: training accuarcy: 0.30110000000000003\n",
      "Epoch 1 step 207: training loss: 161762.42751478814\n",
      "Epoch 1 step 208: training accuarcy: 0.2921\n",
      "Epoch 1 step 208: training loss: 163126.92289000834\n",
      "Epoch 1 step 209: training accuarcy: 0.28600000000000003\n",
      "Epoch 1 step 209: training loss: 161024.37858313447\n",
      "Epoch 1 step 210: training accuarcy: 0.29610000000000003\n",
      "Epoch 1 step 210: training loss: 161011.89979041117\n",
      "Epoch 1 step 211: training accuarcy: 0.2948\n",
      "Epoch 1 step 211: training loss: 164436.65121937462\n",
      "Epoch 1 step 212: training accuarcy: 0.2788\n",
      "Epoch 1 step 212: training loss: 162017.2802380196\n",
      "Epoch 1 step 213: training accuarcy: 0.2914\n",
      "Epoch 1 step 213: training loss: 161983.24659101453\n",
      "Epoch 1 step 214: training accuarcy: 0.29150000000000004\n",
      "Epoch 1 step 214: training loss: 163610.39521156618\n",
      "Epoch 1 step 215: training accuarcy: 0.28290000000000004\n",
      "Epoch 1 step 215: training loss: 161987.70542096993\n",
      "Epoch 1 step 216: training accuarcy: 0.29150000000000004\n",
      "Epoch 1 step 216: training loss: 162475.7886728708\n",
      "Epoch 1 step 217: training accuarcy: 0.2882\n",
      "Epoch 1 step 217: training loss: 160995.4393812477\n",
      "Epoch 1 step 218: training accuarcy: 0.29550000000000004\n",
      "Epoch 1 step 218: training loss: 161847.24727640627\n",
      "Epoch 1 step 219: training accuarcy: 0.2903\n",
      "Epoch 1 step 219: training loss: 162166.6885812565\n",
      "Epoch 1 step 220: training accuarcy: 0.2898\n",
      "Epoch 1 step 220: training loss: 161096.1742709898\n",
      "Epoch 1 step 221: training accuarcy: 0.2938\n",
      "Epoch 1 step 221: training loss: 164224.46209475424\n",
      "Epoch 1 step 222: training accuarcy: 0.2822\n",
      "Epoch 1 step 222: training loss: 160979.05970263053\n",
      "Epoch 1 step 223: training accuarcy: 0.2963\n",
      "Epoch 1 step 223: training loss: 162643.39288404252\n",
      "Epoch 1 step 224: training accuarcy: 0.2896\n",
      "Epoch 1 step 224: training loss: 162539.73176859756\n",
      "Epoch 1 step 225: training accuarcy: 0.2887\n",
      "Epoch 1 step 225: training loss: 163763.9797239723\n",
      "Epoch 1 step 226: training accuarcy: 0.2842\n",
      "Epoch 1 step 226: training loss: 164085.66696023254\n",
      "Epoch 1 step 227: training accuarcy: 0.28290000000000004\n",
      "Epoch 1 step 227: training loss: 162556.0510268313\n",
      "Epoch 1 step 228: training accuarcy: 0.2916\n",
      "Epoch 1 step 228: training loss: 163254.08582747498\n",
      "Epoch 1 step 229: training accuarcy: 0.2851\n",
      "Epoch 1 step 229: training loss: 155880.66954499573\n",
      "Epoch 1 step 230: training accuarcy: 0.2858490566037736\n",
      "Epoch 1: train loss 166276.60445098265, train accuarcy 0.27093538641929626\n",
      "Epoch 1: valid loss 162558.52807948366, valid accuarcy 0.27479222416877747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [22:57<00:00, 688.37s/it]"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=2, log_dir=get_log_dir('simple_kaggle', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:00:57.569614Z",
     "start_time": "2019-10-08T06:38:33.678579Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 487422.82394555013\n",
      "Epoch 0 step 1: training accuarcy: 0.12040000000000001\n",
      "Epoch 0 step 1: training loss: 489450.34019803803\n",
      "Epoch 0 step 2: training accuarcy: 0.1499\n",
      "Epoch 0 step 2: training loss: 488016.4392187835\n",
      "Epoch 0 step 3: training accuarcy: 0.1763\n",
      "Epoch 0 step 3: training loss: 491882.392495647\n",
      "Epoch 0 step 4: training accuarcy: 0.1903\n",
      "Epoch 0 step 4: training loss: 490320.818300761\n",
      "Epoch 0 step 5: training accuarcy: 0.2059\n",
      "Epoch 0 step 5: training loss: 490525.54563383816\n",
      "Epoch 0 step 6: training accuarcy: 0.215\n",
      "Epoch 0 step 6: training loss: 485183.02029845637\n",
      "Epoch 0 step 7: training accuarcy: 0.22240000000000001\n",
      "Epoch 0 step 7: training loss: 487375.3059561003\n",
      "Epoch 0 step 8: training accuarcy: 0.2267\n",
      "Epoch 0 step 8: training loss: 489671.4944303962\n",
      "Epoch 0 step 9: training accuarcy: 0.22710000000000002\n",
      "Epoch 0 step 9: training loss: 492945.4432845665\n",
      "Epoch 0 step 10: training accuarcy: 0.21630000000000002\n",
      "Epoch 0 step 10: training loss: 485618.86971229274\n",
      "Epoch 0 step 11: training accuarcy: 0.22360000000000002\n",
      "Epoch 0 step 11: training loss: 481924.2236165382\n",
      "Epoch 0 step 12: training accuarcy: 0.22660000000000002\n",
      "Epoch 0 step 12: training loss: 491339.75576577766\n",
      "Epoch 0 step 13: training accuarcy: 0.2233\n",
      "Epoch 0 step 13: training loss: 488367.53813181724\n",
      "Epoch 0 step 14: training accuarcy: 0.2258\n",
      "Epoch 0 step 14: training loss: 483523.847770809\n",
      "Epoch 0 step 15: training accuarcy: 0.23020000000000002\n",
      "Epoch 0 step 15: training loss: 486763.69367996114\n",
      "Epoch 0 step 16: training accuarcy: 0.23120000000000002\n",
      "Epoch 0 step 16: training loss: 486232.6285644128\n",
      "Epoch 0 step 17: training accuarcy: 0.2214\n",
      "Epoch 0 step 17: training loss: 483148.86890955543\n",
      "Epoch 0 step 18: training accuarcy: 0.2286\n",
      "Epoch 0 step 18: training loss: 482858.13638685306\n",
      "Epoch 0 step 19: training accuarcy: 0.2315\n",
      "Epoch 0 step 19: training loss: 485363.78684356326\n",
      "Epoch 0 step 20: training accuarcy: 0.2277\n",
      "Epoch 0 step 20: training loss: 484103.3900745615\n",
      "Epoch 0 step 21: training accuarcy: 0.2281\n",
      "Epoch 0 step 21: training loss: 485416.6377357448\n",
      "Epoch 0 step 22: training accuarcy: 0.23370000000000002\n",
      "Epoch 0 step 22: training loss: 485515.76741738536\n",
      "Epoch 0 step 23: training accuarcy: 0.22080000000000002\n",
      "Epoch 0 step 23: training loss: 480802.65000682743\n",
      "Epoch 0 step 24: training accuarcy: 0.232\n",
      "Epoch 0 step 24: training loss: 485630.6975052574\n",
      "Epoch 0 step 25: training accuarcy: 0.22640000000000002\n",
      "Epoch 0 step 25: training loss: 477681.2091909277\n",
      "Epoch 0 step 26: training accuarcy: 0.2368\n",
      "Epoch 0 step 26: training loss: 484092.8102827786\n",
      "Epoch 0 step 27: training accuarcy: 0.2343\n",
      "Epoch 0 step 27: training loss: 480900.4100219151\n",
      "Epoch 0 step 28: training accuarcy: 0.24130000000000001\n",
      "Epoch 0 step 28: training loss: 477079.6037208454\n",
      "Epoch 0 step 29: training accuarcy: 0.2459\n",
      "Epoch 0 step 29: training loss: 483581.37923893036\n",
      "Epoch 0 step 30: training accuarcy: 0.2386\n",
      "Epoch 0 step 30: training loss: 478388.32270909066\n",
      "Epoch 0 step 31: training accuarcy: 0.24150000000000002\n",
      "Epoch 0 step 31: training loss: 482445.9243850662\n",
      "Epoch 0 step 32: training accuarcy: 0.2384\n",
      "Epoch 0 step 32: training loss: 482646.86687485554\n",
      "Epoch 0 step 33: training accuarcy: 0.2335\n",
      "Epoch 0 step 33: training loss: 480221.91045303806\n",
      "Epoch 0 step 34: training accuarcy: 0.24070000000000003\n",
      "Epoch 0 step 34: training loss: 479591.6007355801\n",
      "Epoch 0 step 35: training accuarcy: 0.2406\n",
      "Epoch 0 step 35: training loss: 482458.1001969027\n",
      "Epoch 0 step 36: training accuarcy: 0.2326\n",
      "Epoch 0 step 36: training loss: 479426.9147239134\n",
      "Epoch 0 step 37: training accuarcy: 0.2446\n",
      "Epoch 0 step 37: training loss: 478655.40239639184\n",
      "Epoch 0 step 38: training accuarcy: 0.2427\n",
      "Epoch 0 step 38: training loss: 479430.2729291643\n",
      "Epoch 0 step 39: training accuarcy: 0.24200000000000002\n",
      "Epoch 0 step 39: training loss: 482326.3525337798\n",
      "Epoch 0 step 40: training accuarcy: 0.2379\n",
      "Epoch 0 step 40: training loss: 480015.36872619315\n",
      "Epoch 0 step 41: training accuarcy: 0.23700000000000002\n",
      "Epoch 0 step 41: training loss: 473420.14831632935\n",
      "Epoch 0 step 42: training accuarcy: 0.2505\n",
      "Epoch 0 step 42: training loss: 479230.4407186036\n",
      "Epoch 0 step 43: training accuarcy: 0.2401\n",
      "Epoch 0 step 43: training loss: 479827.1208117478\n",
      "Epoch 0 step 44: training accuarcy: 0.23850000000000002\n",
      "Epoch 0 step 44: training loss: 476361.25789532956\n",
      "Epoch 0 step 45: training accuarcy: 0.24730000000000002\n",
      "Epoch 0 step 45: training loss: 480359.14400409843\n",
      "Epoch 0 step 46: training accuarcy: 0.2427\n",
      "Epoch 0 step 46: training loss: 471918.3167380343\n",
      "Epoch 0 step 47: training accuarcy: 0.2526\n",
      "Epoch 0 step 47: training loss: 482224.57267192076\n",
      "Epoch 0 step 48: training accuarcy: 0.2412\n",
      "Epoch 0 step 48: training loss: 479131.19117903523\n",
      "Epoch 0 step 49: training accuarcy: 0.2384\n",
      "Epoch 0 step 49: training loss: 473964.2546226123\n",
      "Epoch 0 step 50: training accuarcy: 0.2495\n",
      "Epoch 0 step 50: training loss: 469606.9357640375\n",
      "Epoch 0 step 51: training accuarcy: 0.2551\n",
      "Epoch 0 step 51: training loss: 472943.7442066543\n",
      "Epoch 0 step 52: training accuarcy: 0.2432\n",
      "Epoch 0 step 52: training loss: 477397.75140559673\n",
      "Epoch 0 step 53: training accuarcy: 0.2406\n",
      "Epoch 0 step 53: training loss: 476155.28015405923\n",
      "Epoch 0 step 54: training accuarcy: 0.24500000000000002\n",
      "Epoch 0 step 54: training loss: 476795.690876611\n",
      "Epoch 0 step 55: training accuarcy: 0.24400000000000002\n",
      "Epoch 0 step 55: training loss: 479220.0636449787\n",
      "Epoch 0 step 56: training accuarcy: 0.24150000000000002\n",
      "Epoch 0 step 56: training loss: 478427.42211325606\n",
      "Epoch 0 step 57: training accuarcy: 0.2411\n",
      "Epoch 0 step 57: training loss: 475246.95976036094\n",
      "Epoch 0 step 58: training accuarcy: 0.24600000000000002\n",
      "Epoch 0 step 58: training loss: 476272.92378506553\n",
      "Epoch 0 step 59: training accuarcy: 0.2411\n",
      "Epoch 0 step 59: training loss: 471000.0990837947\n",
      "Epoch 0 step 60: training accuarcy: 0.25120000000000003\n",
      "Epoch 0 step 60: training loss: 475861.62556064746\n",
      "Epoch 0 step 61: training accuarcy: 0.2421\n",
      "Epoch 0 step 61: training loss: 476710.2193631792\n",
      "Epoch 0 step 62: training accuarcy: 0.2386\n",
      "Epoch 0 step 62: training loss: 481705.7309640892\n",
      "Epoch 0 step 63: training accuarcy: 0.2354\n",
      "Epoch 0 step 63: training loss: 475725.7823832582\n",
      "Epoch 0 step 64: training accuarcy: 0.24710000000000001\n",
      "Epoch 0 step 64: training loss: 477129.90766529914\n",
      "Epoch 0 step 65: training accuarcy: 0.24070000000000003\n",
      "Epoch 0 step 65: training loss: 476582.11010700365\n",
      "Epoch 0 step 66: training accuarcy: 0.2419\n",
      "Epoch 0 step 66: training loss: 479672.2305486268\n",
      "Epoch 0 step 67: training accuarcy: 0.23970000000000002\n",
      "Epoch 0 step 67: training loss: 472166.3435621857\n",
      "Epoch 0 step 68: training accuarcy: 0.24500000000000002\n",
      "Epoch 0 step 68: training loss: 473811.8840207485\n",
      "Epoch 0 step 69: training accuarcy: 0.2457\n",
      "Epoch 0 step 69: training loss: 475381.61056836153\n",
      "Epoch 0 step 70: training accuarcy: 0.2439\n",
      "Epoch 0 step 70: training loss: 473077.8126223592\n",
      "Epoch 0 step 71: training accuarcy: 0.24250000000000002\n",
      "Epoch 0 step 71: training loss: 473796.2692385389\n",
      "Epoch 0 step 72: training accuarcy: 0.2462\n",
      "Epoch 0 step 72: training loss: 469886.5514797755\n",
      "Epoch 0 step 73: training accuarcy: 0.2506\n",
      "Epoch 0 step 73: training loss: 473068.9271792987\n",
      "Epoch 0 step 74: training accuarcy: 0.24860000000000002\n",
      "Epoch 0 step 74: training loss: 479723.63758752774\n",
      "Epoch 0 step 75: training accuarcy: 0.2411\n",
      "Epoch 0 step 75: training loss: 477665.4704563511\n",
      "Epoch 0 step 76: training accuarcy: 0.24250000000000002\n",
      "Epoch 0 step 76: training loss: 472526.2980048974\n",
      "Epoch 0 step 77: training accuarcy: 0.24530000000000002\n",
      "Epoch 0 step 77: training loss: 473959.4863511967\n",
      "Epoch 0 step 78: training accuarcy: 0.2447\n",
      "Epoch 0 step 78: training loss: 478297.8155352172\n",
      "Epoch 0 step 79: training accuarcy: 0.23770000000000002\n",
      "Epoch 0 step 79: training loss: 470254.6816403735\n",
      "Epoch 0 step 80: training accuarcy: 0.2489\n",
      "Epoch 0 step 80: training loss: 472857.66895228403\n",
      "Epoch 0 step 81: training accuarcy: 0.2485\n",
      "Epoch 0 step 81: training loss: 473781.7710066886\n",
      "Epoch 0 step 82: training accuarcy: 0.24180000000000001\n",
      "Epoch 0 step 82: training loss: 477626.01098983455\n",
      "Epoch 0 step 83: training accuarcy: 0.2389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 83: training loss: 473360.3601560681\n",
      "Epoch 0 step 84: training accuarcy: 0.2485\n",
      "Epoch 0 step 84: training loss: 473256.6977034972\n",
      "Epoch 0 step 85: training accuarcy: 0.24710000000000001\n",
      "Epoch 0 step 85: training loss: 475983.0429531854\n",
      "Epoch 0 step 86: training accuarcy: 0.24180000000000001\n",
      "Epoch 0 step 86: training loss: 474157.9603834493\n",
      "Epoch 0 step 87: training accuarcy: 0.2464\n",
      "Epoch 0 step 87: training loss: 482785.3637185157\n",
      "Epoch 0 step 88: training accuarcy: 0.234\n",
      "Epoch 0 step 88: training loss: 470584.06494812225\n",
      "Epoch 0 step 89: training accuarcy: 0.2482\n",
      "Epoch 0 step 89: training loss: 474801.79766546725\n",
      "Epoch 0 step 90: training accuarcy: 0.2479\n",
      "Epoch 0 step 90: training loss: 476233.41311513004\n",
      "Epoch 0 step 91: training accuarcy: 0.2422\n",
      "Epoch 0 step 91: training loss: 471155.5216672345\n",
      "Epoch 0 step 92: training accuarcy: 0.2492\n",
      "Epoch 0 step 92: training loss: 473965.4642374708\n",
      "Epoch 0 step 93: training accuarcy: 0.2432\n",
      "Epoch 0 step 93: training loss: 475442.20213515\n",
      "Epoch 0 step 94: training accuarcy: 0.2442\n",
      "Epoch 0 step 94: training loss: 477530.0785089288\n",
      "Epoch 0 step 95: training accuarcy: 0.2447\n",
      "Epoch 0 step 95: training loss: 476648.6376304773\n",
      "Epoch 0 step 96: training accuarcy: 0.2386\n",
      "Epoch 0 step 96: training loss: 476449.0248376904\n",
      "Epoch 0 step 97: training accuarcy: 0.2436\n",
      "Epoch 0 step 97: training loss: 472156.72685854643\n",
      "Epoch 0 step 98: training accuarcy: 0.24860000000000002\n",
      "Epoch 0 step 98: training loss: 479257.51417563716\n",
      "Epoch 0 step 99: training accuarcy: 0.2379\n",
      "Epoch 0 step 99: training loss: 475652.4979275395\n",
      "Epoch 0 step 100: training accuarcy: 0.2434\n",
      "Epoch 0 step 100: training loss: 468823.89673885365\n",
      "Epoch 0 step 101: training accuarcy: 0.2506\n",
      "Epoch 0 step 101: training loss: 473284.7696568626\n",
      "Epoch 0 step 102: training accuarcy: 0.24150000000000002\n",
      "Epoch 0 step 102: training loss: 474773.3655130503\n",
      "Epoch 0 step 103: training accuarcy: 0.24580000000000002\n",
      "Epoch 0 step 103: training loss: 474911.00931784115\n",
      "Epoch 0 step 104: training accuarcy: 0.24480000000000002\n",
      "Epoch 0 step 104: training loss: 475833.6463724885\n",
      "Epoch 0 step 105: training accuarcy: 0.2447\n",
      "Epoch 0 step 105: training loss: 479762.9579922233\n",
      "Epoch 0 step 106: training accuarcy: 0.2386\n",
      "Epoch 0 step 106: training loss: 472543.2189903743\n",
      "Epoch 0 step 107: training accuarcy: 0.2454\n",
      "Epoch 0 step 107: training loss: 477484.7928463566\n",
      "Epoch 0 step 108: training accuarcy: 0.23900000000000002\n",
      "Epoch 0 step 108: training loss: 471119.5550954073\n",
      "Epoch 0 step 109: training accuarcy: 0.2487\n",
      "Epoch 0 step 109: training loss: 475915.3977123195\n",
      "Epoch 0 step 110: training accuarcy: 0.24050000000000002\n",
      "Epoch 0 step 110: training loss: 478960.179714758\n",
      "Epoch 0 step 111: training accuarcy: 0.23820000000000002\n",
      "Epoch 0 step 111: training loss: 472059.04837932036\n",
      "Epoch 0 step 112: training accuarcy: 0.2436\n",
      "Epoch 0 step 112: training loss: 473435.2846994167\n",
      "Epoch 0 step 113: training accuarcy: 0.2449\n",
      "Epoch 0 step 113: training loss: 475645.5833186046\n",
      "Epoch 0 step 114: training accuarcy: 0.2456\n",
      "Epoch 0 step 114: training loss: 454659.9260527874\n",
      "Epoch 0 step 115: training accuarcy: 0.24129979035639412\n",
      "Epoch 0: train loss 478415.24112665677, train accuarcy 0.229522243142128\n",
      "Epoch 0: valid loss 467618.67501359433, valid accuarcy 0.25559306144714355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [11:09<11:09, 669.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 474542.40284475335\n",
      "Epoch 1 step 116: training accuarcy: 0.24000000000000002\n",
      "Epoch 1 step 116: training loss: 473252.3797201521\n",
      "Epoch 1 step 117: training accuarcy: 0.24650000000000002\n",
      "Epoch 1 step 117: training loss: 477294.37638240063\n",
      "Epoch 1 step 118: training accuarcy: 0.2396\n",
      "Epoch 1 step 118: training loss: 476808.94339811546\n",
      "Epoch 1 step 119: training accuarcy: 0.2421\n",
      "Epoch 1 step 119: training loss: 471741.0805636938\n",
      "Epoch 1 step 120: training accuarcy: 0.2466\n",
      "Epoch 1 step 120: training loss: 473419.45447076997\n",
      "Epoch 1 step 121: training accuarcy: 0.2457\n",
      "Epoch 1 step 121: training loss: 476231.16271982976\n",
      "Epoch 1 step 122: training accuarcy: 0.2431\n",
      "Epoch 1 step 122: training loss: 478678.3387199232\n",
      "Epoch 1 step 123: training accuarcy: 0.24000000000000002\n",
      "Epoch 1 step 123: training loss: 476005.4940440219\n",
      "Epoch 1 step 124: training accuarcy: 0.2411\n",
      "Epoch 1 step 124: training loss: 475530.66795238934\n",
      "Epoch 1 step 125: training accuarcy: 0.2441\n",
      "Epoch 1 step 125: training loss: 475503.6535952018\n",
      "Epoch 1 step 126: training accuarcy: 0.24000000000000002\n",
      "Epoch 1 step 126: training loss: 472338.9939544402\n",
      "Epoch 1 step 127: training accuarcy: 0.24980000000000002\n",
      "Epoch 1 step 127: training loss: 475140.81905924383\n",
      "Epoch 1 step 128: training accuarcy: 0.24150000000000002\n",
      "Epoch 1 step 128: training loss: 470774.02901696396\n",
      "Epoch 1 step 129: training accuarcy: 0.2494\n",
      "Epoch 1 step 129: training loss: 473343.23760191124\n",
      "Epoch 1 step 130: training accuarcy: 0.2456\n",
      "Epoch 1 step 130: training loss: 476725.3726933659\n",
      "Epoch 1 step 131: training accuarcy: 0.2394\n",
      "Epoch 1 step 131: training loss: 477164.6359732853\n",
      "Epoch 1 step 132: training accuarcy: 0.24070000000000003\n",
      "Epoch 1 step 132: training loss: 477955.79256252723\n",
      "Epoch 1 step 133: training accuarcy: 0.23870000000000002\n",
      "Epoch 1 step 133: training loss: 476665.71019125875\n",
      "Epoch 1 step 134: training accuarcy: 0.2421\n",
      "Epoch 1 step 134: training loss: 468373.0534638013\n",
      "Epoch 1 step 135: training accuarcy: 0.2523\n",
      "Epoch 1 step 135: training loss: 474915.9234216354\n",
      "Epoch 1 step 136: training accuarcy: 0.2451\n",
      "Epoch 1 step 136: training loss: 469840.9751266362\n",
      "Epoch 1 step 137: training accuarcy: 0.2482\n",
      "Epoch 1 step 137: training loss: 473163.4904349916\n",
      "Epoch 1 step 138: training accuarcy: 0.24600000000000002\n",
      "Epoch 1 step 138: training loss: 478990.7926641886\n",
      "Epoch 1 step 139: training accuarcy: 0.23800000000000002\n",
      "Epoch 1 step 139: training loss: 475019.5153592695\n",
      "Epoch 1 step 140: training accuarcy: 0.24300000000000002\n",
      "Epoch 1 step 140: training loss: 473260.8687215163\n",
      "Epoch 1 step 141: training accuarcy: 0.2492\n",
      "Epoch 1 step 141: training loss: 473418.25049860845\n",
      "Epoch 1 step 142: training accuarcy: 0.2426\n",
      "Epoch 1 step 142: training loss: 472692.56305354234\n",
      "Epoch 1 step 143: training accuarcy: 0.2464\n",
      "Epoch 1 step 143: training loss: 473941.17035435804\n",
      "Epoch 1 step 144: training accuarcy: 0.24300000000000002\n",
      "Epoch 1 step 144: training loss: 473984.7233651401\n",
      "Epoch 1 step 145: training accuarcy: 0.2477\n",
      "Epoch 1 step 145: training loss: 475267.7159949169\n",
      "Epoch 1 step 146: training accuarcy: 0.25070000000000003\n",
      "Epoch 1 step 146: training loss: 475232.6604404257\n",
      "Epoch 1 step 147: training accuarcy: 0.2429\n",
      "Epoch 1 step 147: training loss: 471159.4428069352\n",
      "Epoch 1 step 148: training accuarcy: 0.24380000000000002\n",
      "Epoch 1 step 148: training loss: 476249.1195350899\n",
      "Epoch 1 step 149: training accuarcy: 0.23900000000000002\n",
      "Epoch 1 step 149: training loss: 472628.4533704171\n",
      "Epoch 1 step 150: training accuarcy: 0.2492\n",
      "Epoch 1 step 150: training loss: 474003.53723805334\n",
      "Epoch 1 step 151: training accuarcy: 0.2437\n",
      "Epoch 1 step 151: training loss: 471745.3795238059\n",
      "Epoch 1 step 152: training accuarcy: 0.2482\n",
      "Epoch 1 step 152: training loss: 473483.44731394935\n",
      "Epoch 1 step 153: training accuarcy: 0.24430000000000002\n",
      "Epoch 1 step 153: training loss: 475618.1869899123\n",
      "Epoch 1 step 154: training accuarcy: 0.2403\n",
      "Epoch 1 step 154: training loss: 474983.14196289086\n",
      "Epoch 1 step 155: training accuarcy: 0.24330000000000002\n",
      "Epoch 1 step 155: training loss: 472370.7586912844\n",
      "Epoch 1 step 156: training accuarcy: 0.24550000000000002\n",
      "Epoch 1 step 156: training loss: 479230.7831570247\n",
      "Epoch 1 step 157: training accuarcy: 0.2389\n",
      "Epoch 1 step 157: training loss: 469359.1345728038\n",
      "Epoch 1 step 158: training accuarcy: 0.25270000000000004\n",
      "Epoch 1 step 158: training loss: 479456.5183995244\n",
      "Epoch 1 step 159: training accuarcy: 0.2371\n",
      "Epoch 1 step 159: training loss: 470509.0278988765\n",
      "Epoch 1 step 160: training accuarcy: 0.2492\n",
      "Epoch 1 step 160: training loss: 471944.4175836351\n",
      "Epoch 1 step 161: training accuarcy: 0.2467\n",
      "Epoch 1 step 161: training loss: 471404.9272799444\n",
      "Epoch 1 step 162: training accuarcy: 0.2482\n",
      "Epoch 1 step 162: training loss: 472489.0761901332\n",
      "Epoch 1 step 163: training accuarcy: 0.24960000000000002\n",
      "Epoch 1 step 163: training loss: 474581.1249302003\n",
      "Epoch 1 step 164: training accuarcy: 0.2436\n",
      "Epoch 1 step 164: training loss: 470672.02658437996\n",
      "Epoch 1 step 165: training accuarcy: 0.2442\n",
      "Epoch 1 step 165: training loss: 472422.8846936844\n",
      "Epoch 1 step 166: training accuarcy: 0.2506\n",
      "Epoch 1 step 166: training loss: 470783.8259156555\n",
      "Epoch 1 step 167: training accuarcy: 0.24080000000000001\n",
      "Epoch 1 step 167: training loss: 479872.5249052734\n",
      "Epoch 1 step 168: training accuarcy: 0.2341\n",
      "Epoch 1 step 168: training loss: 476439.2598621541\n",
      "Epoch 1 step 169: training accuarcy: 0.23950000000000002\n",
      "Epoch 1 step 169: training loss: 476623.71055467287\n",
      "Epoch 1 step 170: training accuarcy: 0.24000000000000002\n",
      "Epoch 1 step 170: training loss: 475135.8510060807\n",
      "Epoch 1 step 171: training accuarcy: 0.2449\n",
      "Epoch 1 step 171: training loss: 472656.41421734955\n",
      "Epoch 1 step 172: training accuarcy: 0.24650000000000002\n",
      "Epoch 1 step 172: training loss: 477598.95674719167\n",
      "Epoch 1 step 173: training accuarcy: 0.24100000000000002\n",
      "Epoch 1 step 173: training loss: 478115.4725002511\n",
      "Epoch 1 step 174: training accuarcy: 0.23850000000000002\n",
      "Epoch 1 step 174: training loss: 478931.03678638674\n",
      "Epoch 1 step 175: training accuarcy: 0.2374\n",
      "Epoch 1 step 175: training loss: 469426.9773035712\n",
      "Epoch 1 step 176: training accuarcy: 0.25220000000000004\n",
      "Epoch 1 step 176: training loss: 467790.04311140865\n",
      "Epoch 1 step 177: training accuarcy: 0.252\n",
      "Epoch 1 step 177: training loss: 479005.4485922391\n",
      "Epoch 1 step 178: training accuarcy: 0.2374\n",
      "Epoch 1 step 178: training loss: 476209.39892088086\n",
      "Epoch 1 step 179: training accuarcy: 0.2381\n",
      "Epoch 1 step 179: training loss: 481089.69702404534\n",
      "Epoch 1 step 180: training accuarcy: 0.2356\n",
      "Epoch 1 step 180: training loss: 470567.56890811486\n",
      "Epoch 1 step 181: training accuarcy: 0.24550000000000002\n",
      "Epoch 1 step 181: training loss: 479986.23241696064\n",
      "Epoch 1 step 182: training accuarcy: 0.23390000000000002\n",
      "Epoch 1 step 182: training loss: 476665.91496751737\n",
      "Epoch 1 step 183: training accuarcy: 0.2429\n",
      "Epoch 1 step 183: training loss: 472854.56282461155\n",
      "Epoch 1 step 184: training accuarcy: 0.2464\n",
      "Epoch 1 step 184: training loss: 472386.7101182811\n",
      "Epoch 1 step 185: training accuarcy: 0.2431\n",
      "Epoch 1 step 185: training loss: 469821.81098769547\n",
      "Epoch 1 step 186: training accuarcy: 0.2533\n",
      "Epoch 1 step 186: training loss: 478870.2933628682\n",
      "Epoch 1 step 187: training accuarcy: 0.2403\n",
      "Epoch 1 step 187: training loss: 477610.5694842426\n",
      "Epoch 1 step 188: training accuarcy: 0.2439\n",
      "Epoch 1 step 188: training loss: 475135.204463925\n",
      "Epoch 1 step 189: training accuarcy: 0.24480000000000002\n",
      "Epoch 1 step 189: training loss: 471458.1747687697\n",
      "Epoch 1 step 190: training accuarcy: 0.24830000000000002\n",
      "Epoch 1 step 190: training loss: 472700.53462532343\n",
      "Epoch 1 step 191: training accuarcy: 0.24780000000000002\n",
      "Epoch 1 step 191: training loss: 467332.1072909343\n",
      "Epoch 1 step 192: training accuarcy: 0.25370000000000004\n",
      "Epoch 1 step 192: training loss: 475753.3108908743\n",
      "Epoch 1 step 193: training accuarcy: 0.2429\n",
      "Epoch 1 step 193: training loss: 471193.2698555315\n",
      "Epoch 1 step 194: training accuarcy: 0.2479\n",
      "Epoch 1 step 194: training loss: 478180.56736699614\n",
      "Epoch 1 step 195: training accuarcy: 0.24080000000000001\n",
      "Epoch 1 step 195: training loss: 475707.81753621646\n",
      "Epoch 1 step 196: training accuarcy: 0.2426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 196: training loss: 474832.1428100941\n",
      "Epoch 1 step 197: training accuarcy: 0.2441\n",
      "Epoch 1 step 197: training loss: 470904.4547174661\n",
      "Epoch 1 step 198: training accuarcy: 0.248\n",
      "Epoch 1 step 198: training loss: 476812.9139500183\n",
      "Epoch 1 step 199: training accuarcy: 0.2411\n",
      "Epoch 1 step 199: training loss: 476426.25979537575\n",
      "Epoch 1 step 200: training accuarcy: 0.23970000000000002\n",
      "Epoch 1 step 200: training loss: 473637.82748101675\n",
      "Epoch 1 step 201: training accuarcy: 0.24480000000000002\n",
      "Epoch 1 step 201: training loss: 471086.9820538843\n",
      "Epoch 1 step 202: training accuarcy: 0.24760000000000001\n",
      "Epoch 1 step 202: training loss: 480343.3390708109\n",
      "Epoch 1 step 203: training accuarcy: 0.23750000000000002\n",
      "Epoch 1 step 203: training loss: 475530.0101194482\n",
      "Epoch 1 step 204: training accuarcy: 0.2487\n",
      "Epoch 1 step 204: training loss: 471033.1300218787\n",
      "Epoch 1 step 205: training accuarcy: 0.2482\n",
      "Epoch 1 step 205: training loss: 471966.6060932759\n",
      "Epoch 1 step 206: training accuarcy: 0.2503\n",
      "Epoch 1 step 206: training loss: 473708.77108403016\n",
      "Epoch 1 step 207: training accuarcy: 0.2484\n",
      "Epoch 1 step 207: training loss: 476017.5607633512\n",
      "Epoch 1 step 208: training accuarcy: 0.2446\n",
      "Epoch 1 step 208: training loss: 473263.18796833255\n",
      "Epoch 1 step 209: training accuarcy: 0.2477\n",
      "Epoch 1 step 209: training loss: 473857.56084945187\n",
      "Epoch 1 step 210: training accuarcy: 0.24830000000000002\n",
      "Epoch 1 step 210: training loss: 473332.56873573374\n",
      "Epoch 1 step 211: training accuarcy: 0.24810000000000001\n",
      "Epoch 1 step 211: training loss: 473661.8990252027\n",
      "Epoch 1 step 212: training accuarcy: 0.2469\n",
      "Epoch 1 step 212: training loss: 477652.6967955479\n",
      "Epoch 1 step 213: training accuarcy: 0.2446\n",
      "Epoch 1 step 213: training loss: 469890.80443860695\n",
      "Epoch 1 step 214: training accuarcy: 0.2565\n",
      "Epoch 1 step 214: training loss: 470877.6648042571\n",
      "Epoch 1 step 215: training accuarcy: 0.2524\n",
      "Epoch 1 step 215: training loss: 469575.4531248522\n",
      "Epoch 1 step 216: training accuarcy: 0.2553\n",
      "Epoch 1 step 216: training loss: 471623.9210170697\n",
      "Epoch 1 step 217: training accuarcy: 0.2525\n",
      "Epoch 1 step 217: training loss: 469678.4644267016\n",
      "Epoch 1 step 218: training accuarcy: 0.25470000000000004\n",
      "Epoch 1 step 218: training loss: 471507.27180064464\n",
      "Epoch 1 step 219: training accuarcy: 0.2561\n",
      "Epoch 1 step 219: training loss: 462750.2507738345\n",
      "Epoch 1 step 220: training accuarcy: 0.2617\n",
      "Epoch 1 step 220: training loss: 472252.1140297548\n",
      "Epoch 1 step 221: training accuarcy: 0.2526\n",
      "Epoch 1 step 221: training loss: 473452.458659612\n",
      "Epoch 1 step 222: training accuarcy: 0.24830000000000002\n",
      "Epoch 1 step 222: training loss: 468475.76051148423\n",
      "Epoch 1 step 223: training accuarcy: 0.2586\n",
      "Epoch 1 step 223: training loss: 473112.07601903466\n",
      "Epoch 1 step 224: training accuarcy: 0.2513\n",
      "Epoch 1 step 224: training loss: 472083.33287373174\n",
      "Epoch 1 step 225: training accuarcy: 0.2576\n",
      "Epoch 1 step 225: training loss: 465897.3558225278\n",
      "Epoch 1 step 226: training accuarcy: 0.264\n",
      "Epoch 1 step 226: training loss: 471458.30032675806\n",
      "Epoch 1 step 227: training accuarcy: 0.25720000000000004\n",
      "Epoch 1 step 227: training loss: 464170.0023289248\n",
      "Epoch 1 step 228: training accuarcy: 0.262\n",
      "Epoch 1 step 228: training loss: 466902.4755681322\n",
      "Epoch 1 step 229: training accuarcy: 0.2574\n",
      "Epoch 1 step 229: training loss: 452668.684815294\n",
      "Epoch 1 step 230: training accuarcy: 0.25178197064989516\n",
      "Epoch 1: train loss 473546.7583224172, train accuarcy 0.2388252317905426\n",
      "Epoch 1: valid loss 465730.3779119585, valid accuarcy 0.260809987783432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [22:23<00:00, 670.79s/it]"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=2, log_dir=get_log_dir('weight_kaggle', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:05:33.764298Z",
     "start_time": "2019-10-08T07:05:33.759300Z"
    }
   },
   "outputs": [],
   "source": [
    "del prme_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Trans FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:30:55.560644Z",
     "start_time": "2019-10-08T07:30:55.404621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTransFM()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model = TorchTransFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "trans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:30:55.747622Z",
     "start_time": "2019-10-08T07:30:55.742623Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(trans_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:30:56.835121Z",
     "start_time": "2019-10-08T07:30:56.799146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1dbc0a3fb38>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_learner = FMLearner(trans_model, adam_opt, schedular, db)\n",
    "trans_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:44:11.457366Z",
     "start_time": "2019-10-07T12:44:11.453365Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='base',\n",
    "                      valid_col='seq',\n",
    "                      test_col='seq',\n",
    "                      loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:06:42.130011Z",
     "start_time": "2019-10-08T07:06:42.127013Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='seq',\n",
    "                      valid_col='seq',\n",
    "                      test_col='seq',\n",
    "                      loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:31:31.300062Z",
     "start_time": "2019-10-08T07:31:31.296096Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='seq',\n",
    "                      valid_col='seq',\n",
    "                      test_col='seq',\n",
    "                      loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:29:01.738355Z",
     "start_time": "2019-10-08T07:06:42.742447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 13806.398639158986\n",
      "Epoch 0 step 1: training accuarcy: 0.8815000000000001\n",
      "Epoch 0 step 1: training loss: 13303.707887064213\n",
      "Epoch 0 step 2: training accuarcy: 0.9153\n",
      "Epoch 0 step 2: training loss: 12815.493552099597\n",
      "Epoch 0 step 3: training accuarcy: 0.9420000000000001\n",
      "Epoch 0 step 3: training loss: 12345.804191474945\n",
      "Epoch 0 step 4: training accuarcy: 0.9599000000000001\n",
      "Epoch 0 step 4: training loss: 11859.877499910504\n",
      "Epoch 0 step 5: training accuarcy: 0.9687\n",
      "Epoch 0 step 5: training loss: 11405.112049870202\n",
      "Epoch 0 step 6: training accuarcy: 0.9733\n",
      "Epoch 0 step 6: training loss: 10962.206704302316\n",
      "Epoch 0 step 7: training accuarcy: 0.9766\n",
      "Epoch 0 step 7: training loss: 10561.753157123152\n",
      "Epoch 0 step 8: training accuarcy: 0.9771000000000001\n",
      "Epoch 0 step 8: training loss: 10169.054863215862\n",
      "Epoch 0 step 9: training accuarcy: 0.9764\n",
      "Epoch 0 step 9: training loss: 9814.151412577852\n",
      "Epoch 0 step 10: training accuarcy: 0.9774\n",
      "Epoch 0 step 10: training loss: 9507.975828872615\n",
      "Epoch 0 step 11: training accuarcy: 0.9762000000000001\n",
      "Epoch 0 step 11: training loss: 9138.04612018276\n",
      "Epoch 0 step 12: training accuarcy: 0.9779\n",
      "Epoch 0 step 12: training loss: 8830.612081770494\n",
      "Epoch 0 step 13: training accuarcy: 0.9762000000000001\n",
      "Epoch 0 step 13: training loss: 8484.686798235556\n",
      "Epoch 0 step 14: training accuarcy: 0.9780000000000001\n",
      "Epoch 0 step 14: training loss: 8176.3735247246705\n",
      "Epoch 0 step 15: training accuarcy: 0.9794\n",
      "Epoch 0 step 15: training loss: 7926.002164685155\n",
      "Epoch 0 step 16: training accuarcy: 0.9818\n",
      "Epoch 0 step 16: training loss: 7676.431616843785\n",
      "Epoch 0 step 17: training accuarcy: 0.9838\n",
      "Epoch 0 step 17: training loss: 7396.996731953763\n",
      "Epoch 0 step 18: training accuarcy: 0.9843000000000001\n",
      "Epoch 0 step 18: training loss: 7146.150876319835\n",
      "Epoch 0 step 19: training accuarcy: 0.9851000000000001\n",
      "Epoch 0 step 19: training loss: 6892.470053080517\n",
      "Epoch 0 step 20: training accuarcy: 0.9875\n",
      "Epoch 0 step 20: training loss: 6665.802956453698\n",
      "Epoch 0 step 21: training accuarcy: 0.9862000000000001\n",
      "Epoch 0 step 21: training loss: 6395.48932894403\n",
      "Epoch 0 step 22: training accuarcy: 0.9902000000000001\n",
      "Epoch 0 step 22: training loss: 6185.794227115256\n",
      "Epoch 0 step 23: training accuarcy: 0.9902000000000001\n",
      "Epoch 0 step 23: training loss: 6018.6531518587835\n",
      "Epoch 0 step 24: training accuarcy: 0.9869\n",
      "Epoch 0 step 24: training loss: 5794.386975477771\n",
      "Epoch 0 step 25: training accuarcy: 0.9890000000000001\n",
      "Epoch 0 step 25: training loss: 5597.470278208517\n",
      "Epoch 0 step 26: training accuarcy: 0.9897\n",
      "Epoch 0 step 26: training loss: 5417.771312227127\n",
      "Epoch 0 step 27: training accuarcy: 0.9889\n",
      "Epoch 0 step 27: training loss: 5189.7249679880615\n",
      "Epoch 0 step 28: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 28: training loss: 5008.214812250379\n",
      "Epoch 0 step 29: training accuarcy: 0.9917\n",
      "Epoch 0 step 29: training loss: 4866.252187951961\n",
      "Epoch 0 step 30: training accuarcy: 0.9901000000000001\n",
      "Epoch 0 step 30: training loss: 4693.5211203251965\n",
      "Epoch 0 step 31: training accuarcy: 0.9892000000000001\n",
      "Epoch 0 step 31: training loss: 4533.496005891882\n",
      "Epoch 0 step 32: training accuarcy: 0.9907\n",
      "Epoch 0 step 32: training loss: 4385.691984193383\n",
      "Epoch 0 step 33: training accuarcy: 0.9903000000000001\n",
      "Epoch 0 step 33: training loss: 4224.398751799086\n",
      "Epoch 0 step 34: training accuarcy: 0.9916\n",
      "Epoch 0 step 34: training loss: 4079.0725889360224\n",
      "Epoch 0 step 35: training accuarcy: 0.9908\n",
      "Epoch 0 step 35: training loss: 3961.5770315721884\n",
      "Epoch 0 step 36: training accuarcy: 0.9894000000000001\n",
      "Epoch 0 step 36: training loss: 3793.584606808228\n",
      "Epoch 0 step 37: training accuarcy: 0.9917\n",
      "Epoch 0 step 37: training loss: 3671.7411253412865\n",
      "Epoch 0 step 38: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 38: training loss: 3540.5045772747403\n",
      "Epoch 0 step 39: training accuarcy: 0.9909\n",
      "Epoch 0 step 39: training loss: 3416.0960719303453\n",
      "Epoch 0 step 40: training accuarcy: 0.9909\n",
      "Epoch 0 step 40: training loss: 3342.6149105416703\n",
      "Epoch 0 step 41: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 41: training loss: 3190.2965613156957\n",
      "Epoch 0 step 42: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 42: training loss: 3074.411901553434\n",
      "Epoch 0 step 43: training accuarcy: 0.9923000000000001\n",
      "Epoch 0 step 43: training loss: 2993.012844935872\n",
      "Epoch 0 step 44: training accuarcy: 0.9897\n",
      "Epoch 0 step 44: training loss: 2856.7519551073224\n",
      "Epoch 0 step 45: training accuarcy: 0.9919\n",
      "Epoch 0 step 45: training loss: 2780.1628306863868\n",
      "Epoch 0 step 46: training accuarcy: 0.9916\n",
      "Epoch 0 step 46: training loss: 2652.8503075469757\n",
      "Epoch 0 step 47: training accuarcy: 0.9919\n",
      "Epoch 0 step 47: training loss: 2585.0604525159138\n",
      "Epoch 0 step 48: training accuarcy: 0.9907\n",
      "Epoch 0 step 48: training loss: 2504.667818415768\n",
      "Epoch 0 step 49: training accuarcy: 0.9917\n",
      "Epoch 0 step 49: training loss: 2395.0476244077154\n",
      "Epoch 0 step 50: training accuarcy: 0.9922000000000001\n",
      "Epoch 0 step 50: training loss: 2336.2424121639046\n",
      "Epoch 0 step 51: training accuarcy: 0.9917\n",
      "Epoch 0 step 51: training loss: 2248.141989713813\n",
      "Epoch 0 step 52: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 52: training loss: 2173.411788769239\n",
      "Epoch 0 step 53: training accuarcy: 0.9891000000000001\n",
      "Epoch 0 step 53: training loss: 2094.135097214062\n",
      "Epoch 0 step 54: training accuarcy: 0.9916\n",
      "Epoch 0 step 54: training loss: 2020.4211324365585\n",
      "Epoch 0 step 55: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 55: training loss: 1956.5290704286797\n",
      "Epoch 0 step 56: training accuarcy: 0.9898\n",
      "Epoch 0 step 56: training loss: 1889.9028733975526\n",
      "Epoch 0 step 57: training accuarcy: 0.9908\n",
      "Epoch 0 step 57: training loss: 1808.0071661219877\n",
      "Epoch 0 step 58: training accuarcy: 0.9923000000000001\n",
      "Epoch 0 step 58: training loss: 1744.5874003690499\n",
      "Epoch 0 step 59: training accuarcy: 0.9928\n",
      "Epoch 0 step 59: training loss: 1716.3501181495913\n",
      "Epoch 0 step 60: training accuarcy: 0.9882000000000001\n",
      "Epoch 0 step 60: training loss: 1626.33596575897\n",
      "Epoch 0 step 61: training accuarcy: 0.9916\n",
      "Epoch 0 step 61: training loss: 1559.55037256238\n",
      "Epoch 0 step 62: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 62: training loss: 1515.7816523317927\n",
      "Epoch 0 step 63: training accuarcy: 0.9922000000000001\n",
      "Epoch 0 step 63: training loss: 1463.091455596012\n",
      "Epoch 0 step 64: training accuarcy: 0.9924000000000001\n",
      "Epoch 0 step 64: training loss: 1411.5134113996546\n",
      "Epoch 0 step 65: training accuarcy: 0.992\n",
      "Epoch 0 step 65: training loss: 1367.0198303507364\n",
      "Epoch 0 step 66: training accuarcy: 0.991\n",
      "Epoch 0 step 66: training loss: 1306.8607211167848\n",
      "Epoch 0 step 67: training accuarcy: 0.992\n",
      "Epoch 0 step 67: training loss: 1270.0174209748332\n",
      "Epoch 0 step 68: training accuarcy: 0.9916\n",
      "Epoch 0 step 68: training loss: 1235.0087243038654\n",
      "Epoch 0 step 69: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 69: training loss: 1186.6194195974967\n",
      "Epoch 0 step 70: training accuarcy: 0.9925\n",
      "Epoch 0 step 70: training loss: 1149.0778241979579\n",
      "Epoch 0 step 71: training accuarcy: 0.9925\n",
      "Epoch 0 step 71: training loss: 1102.2914131185557\n",
      "Epoch 0 step 72: training accuarcy: 0.9928\n",
      "Epoch 0 step 72: training loss: 1089.1195838026838\n",
      "Epoch 0 step 73: training accuarcy: 0.9915\n",
      "Epoch 0 step 73: training loss: 1043.7445853646063\n",
      "Epoch 0 step 74: training accuarcy: 0.9916\n",
      "Epoch 0 step 74: training loss: 1008.0798227472603\n",
      "Epoch 0 step 75: training accuarcy: 0.9912000000000001\n",
      "Epoch 0 step 75: training loss: 968.5690025989593\n",
      "Epoch 0 step 76: training accuarcy: 0.9919\n",
      "Epoch 0 step 76: training loss: 914.3494189705887\n",
      "Epoch 0 step 77: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 77: training loss: 916.6885465615885\n",
      "Epoch 0 step 78: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 78: training loss: 868.5612766509898\n",
      "Epoch 0 step 79: training accuarcy: 0.9917\n",
      "Epoch 0 step 79: training loss: 870.7105216106231\n",
      "Epoch 0 step 80: training accuarcy: 0.9923000000000001\n",
      "Epoch 0 step 80: training loss: 806.0737955261101\n",
      "Epoch 0 step 81: training accuarcy: 0.9937\n",
      "Epoch 0 step 81: training loss: 780.3145122727816\n",
      "Epoch 0 step 82: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 82: training loss: 771.5281812516389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 83: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 83: training loss: 755.7244235455796\n",
      "Epoch 0 step 84: training accuarcy: 0.9916\n",
      "Epoch 0 step 84: training loss: 714.1708805339747\n",
      "Epoch 0 step 85: training accuarcy: 0.9923000000000001\n",
      "Epoch 0 step 85: training loss: 684.6375754587165\n",
      "Epoch 0 step 86: training accuarcy: 0.9933000000000001\n",
      "Epoch 0 step 86: training loss: 698.6962445972293\n",
      "Epoch 0 step 87: training accuarcy: 0.9907\n",
      "Epoch 0 step 87: training loss: 674.8777554267851\n",
      "Epoch 0 step 88: training accuarcy: 0.9906\n",
      "Epoch 0 step 88: training loss: 645.7831004724978\n",
      "Epoch 0 step 89: training accuarcy: 0.9925\n",
      "Epoch 0 step 89: training loss: 624.1942273867717\n",
      "Epoch 0 step 90: training accuarcy: 0.9926\n",
      "Epoch 0 step 90: training loss: 614.444426267431\n",
      "Epoch 0 step 91: training accuarcy: 0.9903000000000001\n",
      "Epoch 0 step 91: training loss: 587.1531958147577\n",
      "Epoch 0 step 92: training accuarcy: 0.993\n",
      "Epoch 0 step 92: training loss: 561.3538307026868\n",
      "Epoch 0 step 93: training accuarcy: 0.9922000000000001\n",
      "Epoch 0 step 93: training loss: 550.570816824073\n",
      "Epoch 0 step 94: training accuarcy: 0.9923000000000001\n",
      "Epoch 0 step 94: training loss: 541.1813678729106\n",
      "Epoch 0 step 95: training accuarcy: 0.9924000000000001\n",
      "Epoch 0 step 95: training loss: 511.1568334617015\n",
      "Epoch 0 step 96: training accuarcy: 0.9935\n",
      "Epoch 0 step 96: training loss: 499.50460685340283\n",
      "Epoch 0 step 97: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 97: training loss: 510.10201953512205\n",
      "Epoch 0 step 98: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 98: training loss: 475.1314888406165\n",
      "Epoch 0 step 99: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 99: training loss: 477.4303975104008\n",
      "Epoch 0 step 100: training accuarcy: 0.9925\n",
      "Epoch 0 step 100: training loss: 458.7825189746069\n",
      "Epoch 0 step 101: training accuarcy: 0.993\n",
      "Epoch 0 step 101: training loss: 447.08488884354\n",
      "Epoch 0 step 102: training accuarcy: 0.9925\n",
      "Epoch 0 step 102: training loss: 430.8632622543495\n",
      "Epoch 0 step 103: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 103: training loss: 418.4103639196634\n",
      "Epoch 0 step 104: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 104: training loss: 401.7306260725482\n",
      "Epoch 0 step 105: training accuarcy: 0.9946\n",
      "Epoch 0 step 105: training loss: 409.79620934976253\n",
      "Epoch 0 step 106: training accuarcy: 0.9927\n",
      "Epoch 0 step 106: training loss: 419.9709587230517\n",
      "Epoch 0 step 107: training accuarcy: 0.9915\n",
      "Epoch 0 step 107: training loss: 394.0771578727549\n",
      "Epoch 0 step 108: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 108: training loss: 389.4055720834474\n",
      "Epoch 0 step 109: training accuarcy: 0.9925\n",
      "Epoch 0 step 109: training loss: 368.5703441440381\n",
      "Epoch 0 step 110: training accuarcy: 0.9939\n",
      "Epoch 0 step 110: training loss: 377.79567547141613\n",
      "Epoch 0 step 111: training accuarcy: 0.9916\n",
      "Epoch 0 step 111: training loss: 366.81574076939114\n",
      "Epoch 0 step 112: training accuarcy: 0.9923000000000001\n",
      "Epoch 0 step 112: training loss: 322.96967394853743\n",
      "Epoch 0 step 113: training accuarcy: 0.9952000000000001\n",
      "Epoch 0 step 113: training loss: 332.57469850909683\n",
      "Epoch 0 step 114: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 114: training loss: 334.0896305852647\n",
      "Epoch 0 step 115: training accuarcy: 0.9928721174004193\n",
      "Epoch 0: train loss 3358.705339505209, train accuarcy 0.9864691495895386\n",
      "Epoch 0: valid loss 302.39434895598913, valid accuarcy 0.993952214717865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [11:09<11:09, 669.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 318.6883368290194\n",
      "Epoch 1 step 116: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 116: training loss: 330.6143448810297\n",
      "Epoch 1 step 117: training accuarcy: 0.9927\n",
      "Epoch 1 step 117: training loss: 328.35024013344776\n",
      "Epoch 1 step 118: training accuarcy: 0.9918\n",
      "Epoch 1 step 118: training loss: 312.9796494906412\n",
      "Epoch 1 step 119: training accuarcy: 0.9925\n",
      "Epoch 1 step 119: training loss: 299.61080719406596\n",
      "Epoch 1 step 120: training accuarcy: 0.9936\n",
      "Epoch 1 step 120: training loss: 275.24004544813397\n",
      "Epoch 1 step 121: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 121: training loss: 296.7289028365274\n",
      "Epoch 1 step 122: training accuarcy: 0.9935\n",
      "Epoch 1 step 122: training loss: 292.669406522361\n",
      "Epoch 1 step 123: training accuarcy: 0.9926\n",
      "Epoch 1 step 123: training loss: 273.0930750524542\n",
      "Epoch 1 step 124: training accuarcy: 0.9937\n",
      "Epoch 1 step 124: training loss: 280.5020568114061\n",
      "Epoch 1 step 125: training accuarcy: 0.9928\n",
      "Epoch 1 step 125: training loss: 267.349023791596\n",
      "Epoch 1 step 126: training accuarcy: 0.9935\n",
      "Epoch 1 step 126: training loss: 272.56478817634604\n",
      "Epoch 1 step 127: training accuarcy: 0.994\n",
      "Epoch 1 step 127: training loss: 285.465897211569\n",
      "Epoch 1 step 128: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 128: training loss: 261.7355636039848\n",
      "Epoch 1 step 129: training accuarcy: 0.9929\n",
      "Epoch 1 step 129: training loss: 252.67561659760045\n",
      "Epoch 1 step 130: training accuarcy: 0.9946\n",
      "Epoch 1 step 130: training loss: 268.3419710863875\n",
      "Epoch 1 step 131: training accuarcy: 0.9921000000000001\n",
      "Epoch 1 step 131: training loss: 256.06754364075533\n",
      "Epoch 1 step 132: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 132: training loss: 252.3341025143098\n",
      "Epoch 1 step 133: training accuarcy: 0.9925\n",
      "Epoch 1 step 133: training loss: 244.83778380783895\n",
      "Epoch 1 step 134: training accuarcy: 0.9938\n",
      "Epoch 1 step 134: training loss: 239.93223488988042\n",
      "Epoch 1 step 135: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 135: training loss: 247.38417624431148\n",
      "Epoch 1 step 136: training accuarcy: 0.9929\n",
      "Epoch 1 step 136: training loss: 227.73953489500272\n",
      "Epoch 1 step 137: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 137: training loss: 234.97355364432525\n",
      "Epoch 1 step 138: training accuarcy: 0.9938\n",
      "Epoch 1 step 138: training loss: 231.84619954321647\n",
      "Epoch 1 step 139: training accuarcy: 0.9946\n",
      "Epoch 1 step 139: training loss: 237.8571437508346\n",
      "Epoch 1 step 140: training accuarcy: 0.9936\n",
      "Epoch 1 step 140: training loss: 235.30134135080357\n",
      "Epoch 1 step 141: training accuarcy: 0.9931000000000001\n",
      "Epoch 1 step 141: training loss: 199.40010251131798\n",
      "Epoch 1 step 142: training accuarcy: 0.9949\n",
      "Epoch 1 step 142: training loss: 211.77409173624255\n",
      "Epoch 1 step 143: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 143: training loss: 211.1033890756554\n",
      "Epoch 1 step 144: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 144: training loss: 226.00797124496566\n",
      "Epoch 1 step 145: training accuarcy: 0.9945\n",
      "Epoch 1 step 145: training loss: 224.74650365967625\n",
      "Epoch 1 step 146: training accuarcy: 0.9931000000000001\n",
      "Epoch 1 step 146: training loss: 213.12205120467644\n",
      "Epoch 1 step 147: training accuarcy: 0.9945\n",
      "Epoch 1 step 147: training loss: 218.06764703373307\n",
      "Epoch 1 step 148: training accuarcy: 0.994\n",
      "Epoch 1 step 148: training loss: 211.22794634135167\n",
      "Epoch 1 step 149: training accuarcy: 0.9937\n",
      "Epoch 1 step 149: training loss: 199.67609051769676\n",
      "Epoch 1 step 150: training accuarcy: 0.9937\n",
      "Epoch 1 step 150: training loss: 195.12141650758366\n",
      "Epoch 1 step 151: training accuarcy: 0.9958\n",
      "Epoch 1 step 151: training loss: 199.36113777179003\n",
      "Epoch 1 step 152: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 152: training loss: 197.32734696544765\n",
      "Epoch 1 step 153: training accuarcy: 0.995\n",
      "Epoch 1 step 153: training loss: 210.85124248036644\n",
      "Epoch 1 step 154: training accuarcy: 0.9939\n",
      "Epoch 1 step 154: training loss: 214.38176324326488\n",
      "Epoch 1 step 155: training accuarcy: 0.9928\n",
      "Epoch 1 step 155: training loss: 202.1682353071551\n",
      "Epoch 1 step 156: training accuarcy: 0.994\n",
      "Epoch 1 step 156: training loss: 208.26446479453568\n",
      "Epoch 1 step 157: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 157: training loss: 207.97745185662754\n",
      "Epoch 1 step 158: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 158: training loss: 195.53618683493897\n",
      "Epoch 1 step 159: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 159: training loss: 197.13007478252084\n",
      "Epoch 1 step 160: training accuarcy: 0.9947\n",
      "Epoch 1 step 160: training loss: 201.04450154818477\n",
      "Epoch 1 step 161: training accuarcy: 0.9945\n",
      "Epoch 1 step 161: training loss: 178.94763222518384\n",
      "Epoch 1 step 162: training accuarcy: 0.9959\n",
      "Epoch 1 step 162: training loss: 219.41107393190376\n",
      "Epoch 1 step 163: training accuarcy: 0.9931000000000001\n",
      "Epoch 1 step 163: training loss: 189.30361488933738\n",
      "Epoch 1 step 164: training accuarcy: 0.995\n",
      "Epoch 1 step 164: training loss: 191.85615986041506\n",
      "Epoch 1 step 165: training accuarcy: 0.9946\n",
      "Epoch 1 step 165: training loss: 176.6640097143676\n",
      "Epoch 1 step 166: training accuarcy: 0.9948\n",
      "Epoch 1 step 166: training loss: 193.87867368545136\n",
      "Epoch 1 step 167: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 167: training loss: 199.42575994506277\n",
      "Epoch 1 step 168: training accuarcy: 0.9946\n",
      "Epoch 1 step 168: training loss: 195.84266818510432\n",
      "Epoch 1 step 169: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 169: training loss: 201.28912913362052\n",
      "Epoch 1 step 170: training accuarcy: 0.994\n",
      "Epoch 1 step 170: training loss: 172.87001795171813\n",
      "Epoch 1 step 171: training accuarcy: 0.996\n",
      "Epoch 1 step 171: training loss: 185.62874537386796\n",
      "Epoch 1 step 172: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 172: training loss: 191.24627624525783\n",
      "Epoch 1 step 173: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 173: training loss: 195.4783730983559\n",
      "Epoch 1 step 174: training accuarcy: 0.9949\n",
      "Epoch 1 step 174: training loss: 173.2024421767961\n",
      "Epoch 1 step 175: training accuarcy: 0.9955\n",
      "Epoch 1 step 175: training loss: 199.339598670601\n",
      "Epoch 1 step 176: training accuarcy: 0.9945\n",
      "Epoch 1 step 176: training loss: 178.67985823459654\n",
      "Epoch 1 step 177: training accuarcy: 0.9955\n",
      "Epoch 1 step 177: training loss: 188.05647642064125\n",
      "Epoch 1 step 178: training accuarcy: 0.9936\n",
      "Epoch 1 step 178: training loss: 185.76564703238813\n",
      "Epoch 1 step 179: training accuarcy: 0.9948\n",
      "Epoch 1 step 179: training loss: 202.0993448526038\n",
      "Epoch 1 step 180: training accuarcy: 0.9934000000000001\n",
      "Epoch 1 step 180: training loss: 196.22733769724954\n",
      "Epoch 1 step 181: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 181: training loss: 167.63639738476388\n",
      "Epoch 1 step 182: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 182: training loss: 187.07287615555757\n",
      "Epoch 1 step 183: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 183: training loss: 181.75107043959548\n",
      "Epoch 1 step 184: training accuarcy: 0.9945\n",
      "Epoch 1 step 184: training loss: 164.32631801843965\n",
      "Epoch 1 step 185: training accuarcy: 0.9956\n",
      "Epoch 1 step 185: training loss: 174.09482315961782\n",
      "Epoch 1 step 186: training accuarcy: 0.9948\n",
      "Epoch 1 step 186: training loss: 168.4795887243383\n",
      "Epoch 1 step 187: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 187: training loss: 187.7226220143037\n",
      "Epoch 1 step 188: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 188: training loss: 182.24535368475466\n",
      "Epoch 1 step 189: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 189: training loss: 166.71699693890164\n",
      "Epoch 1 step 190: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 190: training loss: 160.49013800395625\n",
      "Epoch 1 step 191: training accuarcy: 0.9948\n",
      "Epoch 1 step 191: training loss: 170.6464377834824\n",
      "Epoch 1 step 192: training accuarcy: 0.9949\n",
      "Epoch 1 step 192: training loss: 162.7691042246577\n",
      "Epoch 1 step 193: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 193: training loss: 180.73658895731108\n",
      "Epoch 1 step 194: training accuarcy: 0.9937\n",
      "Epoch 1 step 194: training loss: 160.88984839443657\n",
      "Epoch 1 step 195: training accuarcy: 0.995\n",
      "Epoch 1 step 195: training loss: 167.05524221750954\n",
      "Epoch 1 step 196: training accuarcy: 0.9939\n",
      "Epoch 1 step 196: training loss: 163.96240500735146\n",
      "Epoch 1 step 197: training accuarcy: 0.9955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 197: training loss: 178.9023833051987\n",
      "Epoch 1 step 198: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 198: training loss: 168.30235408147863\n",
      "Epoch 1 step 199: training accuarcy: 0.9948\n",
      "Epoch 1 step 199: training loss: 162.7889748578452\n",
      "Epoch 1 step 200: training accuarcy: 0.9946\n",
      "Epoch 1 step 200: training loss: 164.77832768623597\n",
      "Epoch 1 step 201: training accuarcy: 0.9947\n",
      "Epoch 1 step 201: training loss: 182.24448627529296\n",
      "Epoch 1 step 202: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 202: training loss: 168.18048031785122\n",
      "Epoch 1 step 203: training accuarcy: 0.9947\n",
      "Epoch 1 step 203: training loss: 176.313799077576\n",
      "Epoch 1 step 204: training accuarcy: 0.9935\n",
      "Epoch 1 step 204: training loss: 160.91246318041846\n",
      "Epoch 1 step 205: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 205: training loss: 168.94934256112273\n",
      "Epoch 1 step 206: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 206: training loss: 165.9653310506707\n",
      "Epoch 1 step 207: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 207: training loss: 154.47695070673032\n",
      "Epoch 1 step 208: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 208: training loss: 172.66314250894914\n",
      "Epoch 1 step 209: training accuarcy: 0.9939\n",
      "Epoch 1 step 209: training loss: 155.41026973318031\n",
      "Epoch 1 step 210: training accuarcy: 0.9957\n",
      "Epoch 1 step 210: training loss: 159.57892651092268\n",
      "Epoch 1 step 211: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 211: training loss: 162.1267271035083\n",
      "Epoch 1 step 212: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 212: training loss: 169.25213653225316\n",
      "Epoch 1 step 213: training accuarcy: 0.9939\n",
      "Epoch 1 step 213: training loss: 185.42582870260287\n",
      "Epoch 1 step 214: training accuarcy: 0.993\n",
      "Epoch 1 step 214: training loss: 156.91430239734623\n",
      "Epoch 1 step 215: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 215: training loss: 166.70937733336183\n",
      "Epoch 1 step 216: training accuarcy: 0.9949\n",
      "Epoch 1 step 216: training loss: 167.82112863828226\n",
      "Epoch 1 step 217: training accuarcy: 0.9946\n",
      "Epoch 1 step 217: training loss: 169.68795430636348\n",
      "Epoch 1 step 218: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 218: training loss: 140.63931025497027\n",
      "Epoch 1 step 219: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 219: training loss: 164.61955905159832\n",
      "Epoch 1 step 220: training accuarcy: 0.9937\n",
      "Epoch 1 step 220: training loss: 155.18951741288936\n",
      "Epoch 1 step 221: training accuarcy: 0.995\n",
      "Epoch 1 step 221: training loss: 158.90943967597377\n",
      "Epoch 1 step 222: training accuarcy: 0.9947\n",
      "Epoch 1 step 222: training loss: 167.9497164553864\n",
      "Epoch 1 step 223: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 223: training loss: 180.15907593119903\n",
      "Epoch 1 step 224: training accuarcy: 0.9934000000000001\n",
      "Epoch 1 step 224: training loss: 156.9602167355872\n",
      "Epoch 1 step 225: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 225: training loss: 151.01181403885403\n",
      "Epoch 1 step 226: training accuarcy: 0.9946\n",
      "Epoch 1 step 226: training loss: 163.72020169221128\n",
      "Epoch 1 step 227: training accuarcy: 0.9946\n",
      "Epoch 1 step 227: training loss: 147.77632322061265\n",
      "Epoch 1 step 228: training accuarcy: 0.9957\n",
      "Epoch 1 step 228: training loss: 164.7479196584946\n",
      "Epoch 1 step 229: training accuarcy: 0.9929\n",
      "Epoch 1 step 229: training loss: 152.71871807139658\n",
      "Epoch 1 step 230: training accuarcy: 0.9947589098532494\n",
      "Epoch 1: train loss 201.32859221622127, train accuarcy 0.9935839772224426\n",
      "Epoch 1: valid loss 131.2331078485354, valid accuarcy 0.9955116510391235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [22:18<00:00, 669.58s/it]"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=2,\n",
    "                  log_dir=get_log_dir('simple_kaggle', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T07:54:21.939201Z",
     "start_time": "2019-10-08T07:31:55.253873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 17381.002641464496\n",
      "Epoch 0 step 1: training accuarcy: 0.8965000000000001\n",
      "Epoch 0 step 1: training loss: 16496.53774284842\n",
      "Epoch 0 step 2: training accuarcy: 0.92\n",
      "Epoch 0 step 2: training loss: 15744.861349360843\n",
      "Epoch 0 step 3: training accuarcy: 0.9458000000000001\n",
      "Epoch 0 step 3: training loss: 14956.835339405248\n",
      "Epoch 0 step 4: training accuarcy: 0.9612\n",
      "Epoch 0 step 4: training loss: 14224.434288367096\n",
      "Epoch 0 step 5: training accuarcy: 0.9690000000000001\n",
      "Epoch 0 step 5: training loss: 13756.064527740442\n",
      "Epoch 0 step 6: training accuarcy: 0.9709000000000001\n",
      "Epoch 0 step 6: training loss: 12986.189324055866\n",
      "Epoch 0 step 7: training accuarcy: 0.9756\n",
      "Epoch 0 step 7: training loss: 12541.985667730762\n",
      "Epoch 0 step 8: training accuarcy: 0.9728\n",
      "Epoch 0 step 8: training loss: 12022.593255255164\n",
      "Epoch 0 step 9: training accuarcy: 0.9726\n",
      "Epoch 0 step 9: training loss: 11626.595902241628\n",
      "Epoch 0 step 10: training accuarcy: 0.9755\n",
      "Epoch 0 step 10: training loss: 11020.495967649902\n",
      "Epoch 0 step 11: training accuarcy: 0.9753000000000001\n",
      "Epoch 0 step 11: training loss: 10480.529120131037\n",
      "Epoch 0 step 12: training accuarcy: 0.9770000000000001\n",
      "Epoch 0 step 12: training loss: 10113.683960886636\n",
      "Epoch 0 step 13: training accuarcy: 0.9788\n",
      "Epoch 0 step 13: training loss: 9798.418408041223\n",
      "Epoch 0 step 14: training accuarcy: 0.9780000000000001\n",
      "Epoch 0 step 14: training loss: 9486.036227504128\n",
      "Epoch 0 step 15: training accuarcy: 0.9762000000000001\n",
      "Epoch 0 step 15: training loss: 9171.204879732255\n",
      "Epoch 0 step 16: training accuarcy: 0.9781000000000001\n",
      "Epoch 0 step 16: training loss: 8612.60778285415\n",
      "Epoch 0 step 17: training accuarcy: 0.9843000000000001\n",
      "Epoch 0 step 17: training loss: 8366.660462749238\n",
      "Epoch 0 step 18: training accuarcy: 0.9866\n",
      "Epoch 0 step 18: training loss: 7994.849612117572\n",
      "Epoch 0 step 19: training accuarcy: 0.9874\n",
      "Epoch 0 step 19: training loss: 7768.276270238277\n",
      "Epoch 0 step 20: training accuarcy: 0.9875\n",
      "Epoch 0 step 20: training loss: 7563.585585727014\n",
      "Epoch 0 step 21: training accuarcy: 0.9864\n",
      "Epoch 0 step 21: training loss: 7281.063291852267\n",
      "Epoch 0 step 22: training accuarcy: 0.9881000000000001\n",
      "Epoch 0 step 22: training loss: 7096.257452979517\n",
      "Epoch 0 step 23: training accuarcy: 0.9883000000000001\n",
      "Epoch 0 step 23: training loss: 6784.886633025118\n",
      "Epoch 0 step 24: training accuarcy: 0.9894000000000001\n",
      "Epoch 0 step 24: training loss: 6617.462754208256\n",
      "Epoch 0 step 25: training accuarcy: 0.9886\n",
      "Epoch 0 step 25: training loss: 6492.173820903635\n",
      "Epoch 0 step 26: training accuarcy: 0.9872000000000001\n",
      "Epoch 0 step 26: training loss: 6217.064883340337\n",
      "Epoch 0 step 27: training accuarcy: 0.9874\n",
      "Epoch 0 step 27: training loss: 5984.803890745046\n",
      "Epoch 0 step 28: training accuarcy: 0.9884000000000001\n",
      "Epoch 0 step 28: training loss: 5736.998482529158\n",
      "Epoch 0 step 29: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 29: training loss: 5652.10339818513\n",
      "Epoch 0 step 30: training accuarcy: 0.9884000000000001\n",
      "Epoch 0 step 30: training loss: 5456.401821847264\n",
      "Epoch 0 step 31: training accuarcy: 0.9881000000000001\n",
      "Epoch 0 step 31: training loss: 5242.570377135056\n",
      "Epoch 0 step 32: training accuarcy: 0.9901000000000001\n",
      "Epoch 0 step 32: training loss: 5082.675331502027\n",
      "Epoch 0 step 33: training accuarcy: 0.9906\n",
      "Epoch 0 step 33: training loss: 4895.46696976909\n",
      "Epoch 0 step 34: training accuarcy: 0.9898\n",
      "Epoch 0 step 34: training loss: 4668.322890858976\n",
      "Epoch 0 step 35: training accuarcy: 0.9926\n",
      "Epoch 0 step 35: training loss: 4626.180325347771\n",
      "Epoch 0 step 36: training accuarcy: 0.9892000000000001\n",
      "Epoch 0 step 36: training loss: 4421.884707437732\n",
      "Epoch 0 step 37: training accuarcy: 0.991\n",
      "Epoch 0 step 37: training loss: 4269.04319158458\n",
      "Epoch 0 step 38: training accuarcy: 0.9919\n",
      "Epoch 0 step 38: training loss: 4206.876760076647\n",
      "Epoch 0 step 39: training accuarcy: 0.9909\n",
      "Epoch 0 step 39: training loss: 4083.899897041266\n",
      "Epoch 0 step 40: training accuarcy: 0.9909\n",
      "Epoch 0 step 40: training loss: 3973.260441685346\n",
      "Epoch 0 step 41: training accuarcy: 0.9891000000000001\n",
      "Epoch 0 step 41: training loss: 3767.2859939452537\n",
      "Epoch 0 step 42: training accuarcy: 0.9922000000000001\n",
      "Epoch 0 step 42: training loss: 3657.32409483583\n",
      "Epoch 0 step 43: training accuarcy: 0.9926\n",
      "Epoch 0 step 43: training loss: 3582.673720298975\n",
      "Epoch 0 step 44: training accuarcy: 0.9911000000000001\n",
      "Epoch 0 step 44: training loss: 3496.275115276012\n",
      "Epoch 0 step 45: training accuarcy: 0.9905\n",
      "Epoch 0 step 45: training loss: 3349.7970312253155\n",
      "Epoch 0 step 46: training accuarcy: 0.9903000000000001\n",
      "Epoch 0 step 46: training loss: 3232.6187179028307\n",
      "Epoch 0 step 47: training accuarcy: 0.992\n",
      "Epoch 0 step 47: training loss: 3134.420505987319\n",
      "Epoch 0 step 48: training accuarcy: 0.9904000000000001\n",
      "Epoch 0 step 48: training loss: 3081.4720491089547\n",
      "Epoch 0 step 49: training accuarcy: 0.9902000000000001\n",
      "Epoch 0 step 49: training loss: 3005.2303448605853\n",
      "Epoch 0 step 50: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 50: training loss: 2904.7950863379683\n",
      "Epoch 0 step 51: training accuarcy: 0.9901000000000001\n",
      "Epoch 0 step 51: training loss: 2777.8949345099663\n",
      "Epoch 0 step 52: training accuarcy: 0.9911000000000001\n",
      "Epoch 0 step 52: training loss: 2759.8296514980448\n",
      "Epoch 0 step 53: training accuarcy: 0.9894000000000001\n",
      "Epoch 0 step 53: training loss: 2578.482664991648\n",
      "Epoch 0 step 54: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 54: training loss: 2566.350803068101\n",
      "Epoch 0 step 55: training accuarcy: 0.9911000000000001\n",
      "Epoch 0 step 55: training loss: 2543.5355736827487\n",
      "Epoch 0 step 56: training accuarcy: 0.9880000000000001\n",
      "Epoch 0 step 56: training loss: 2353.9009129417877\n",
      "Epoch 0 step 57: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 57: training loss: 2433.3049865345283\n",
      "Epoch 0 step 58: training accuarcy: 0.9887\n",
      "Epoch 0 step 58: training loss: 2256.4782696723614\n",
      "Epoch 0 step 59: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 59: training loss: 2187.8258241620047\n",
      "Epoch 0 step 60: training accuarcy: 0.9925\n",
      "Epoch 0 step 60: training loss: 2138.489401086647\n",
      "Epoch 0 step 61: training accuarcy: 0.992\n",
      "Epoch 0 step 61: training loss: 2044.158892825751\n",
      "Epoch 0 step 62: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 62: training loss: 2002.369694834342\n",
      "Epoch 0 step 63: training accuarcy: 0.9922000000000001\n",
      "Epoch 0 step 63: training loss: 1955.16458820358\n",
      "Epoch 0 step 64: training accuarcy: 0.992\n",
      "Epoch 0 step 64: training loss: 1835.6681929855115\n",
      "Epoch 0 step 65: training accuarcy: 0.9936\n",
      "Epoch 0 step 65: training loss: 1836.7212459811878\n",
      "Epoch 0 step 66: training accuarcy: 0.9926\n",
      "Epoch 0 step 66: training loss: 1755.5941477586312\n",
      "Epoch 0 step 67: training accuarcy: 0.9928\n",
      "Epoch 0 step 67: training loss: 1673.262888135577\n",
      "Epoch 0 step 68: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 68: training loss: 1710.1009751605643\n",
      "Epoch 0 step 69: training accuarcy: 0.9915\n",
      "Epoch 0 step 69: training loss: 1628.0643743572532\n",
      "Epoch 0 step 70: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 70: training loss: 1624.467568090508\n",
      "Epoch 0 step 71: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 71: training loss: 1577.7786784651257\n",
      "Epoch 0 step 72: training accuarcy: 0.9904000000000001\n",
      "Epoch 0 step 72: training loss: 1520.6679132995712\n",
      "Epoch 0 step 73: training accuarcy: 0.9918\n",
      "Epoch 0 step 73: training loss: 1499.1243360746082\n",
      "Epoch 0 step 74: training accuarcy: 0.9916\n",
      "Epoch 0 step 74: training loss: 1487.3394527003895\n",
      "Epoch 0 step 75: training accuarcy: 0.9909\n",
      "Epoch 0 step 75: training loss: 1385.0575489213152\n",
      "Epoch 0 step 76: training accuarcy: 0.9919\n",
      "Epoch 0 step 76: training loss: 1349.0589632215895\n",
      "Epoch 0 step 77: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 77: training loss: 1323.111754327772\n",
      "Epoch 0 step 78: training accuarcy: 0.992\n",
      "Epoch 0 step 78: training loss: 1328.1721339632886\n",
      "Epoch 0 step 79: training accuarcy: 0.9909\n",
      "Epoch 0 step 79: training loss: 1276.2615463308043\n",
      "Epoch 0 step 80: training accuarcy: 0.992\n",
      "Epoch 0 step 80: training loss: 1256.0892662422916\n",
      "Epoch 0 step 81: training accuarcy: 0.9912000000000001\n",
      "Epoch 0 step 81: training loss: 1168.8389524385884\n",
      "Epoch 0 step 82: training accuarcy: 0.9937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 82: training loss: 1189.7284796752924\n",
      "Epoch 0 step 83: training accuarcy: 0.9908\n",
      "Epoch 0 step 83: training loss: 1086.96257575096\n",
      "Epoch 0 step 84: training accuarcy: 0.9937\n",
      "Epoch 0 step 84: training loss: 1116.573634323385\n",
      "Epoch 0 step 85: training accuarcy: 0.9922000000000001\n",
      "Epoch 0 step 85: training loss: 1114.9663954367995\n",
      "Epoch 0 step 86: training accuarcy: 0.9905\n",
      "Epoch 0 step 86: training loss: 1067.1138154366363\n",
      "Epoch 0 step 87: training accuarcy: 0.992\n",
      "Epoch 0 step 87: training loss: 1005.6448374576487\n",
      "Epoch 0 step 88: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 88: training loss: 988.8289004242874\n",
      "Epoch 0 step 89: training accuarcy: 0.9926\n",
      "Epoch 0 step 89: training loss: 1045.8426020888674\n",
      "Epoch 0 step 90: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 90: training loss: 1001.5470468685141\n",
      "Epoch 0 step 91: training accuarcy: 0.9915\n",
      "Epoch 0 step 91: training loss: 983.6988965070402\n",
      "Epoch 0 step 92: training accuarcy: 0.9919\n",
      "Epoch 0 step 92: training loss: 866.2985189180945\n",
      "Epoch 0 step 93: training accuarcy: 0.9943000000000001\n",
      "Epoch 0 step 93: training loss: 941.4790213027171\n",
      "Epoch 0 step 94: training accuarcy: 0.9926\n",
      "Epoch 0 step 94: training loss: 902.4298330283958\n",
      "Epoch 0 step 95: training accuarcy: 0.9922000000000001\n",
      "Epoch 0 step 95: training loss: 860.1086185496895\n",
      "Epoch 0 step 96: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 96: training loss: 876.4946764490032\n",
      "Epoch 0 step 97: training accuarcy: 0.9927\n",
      "Epoch 0 step 97: training loss: 837.6337664295112\n",
      "Epoch 0 step 98: training accuarcy: 0.9943000000000001\n",
      "Epoch 0 step 98: training loss: 838.4127911529808\n",
      "Epoch 0 step 99: training accuarcy: 0.9919\n",
      "Epoch 0 step 99: training loss: 774.7452318675246\n",
      "Epoch 0 step 100: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 100: training loss: 793.9093846817816\n",
      "Epoch 0 step 101: training accuarcy: 0.9922000000000001\n",
      "Epoch 0 step 101: training loss: 796.3398556821267\n",
      "Epoch 0 step 102: training accuarcy: 0.9912000000000001\n",
      "Epoch 0 step 102: training loss: 776.8223422120342\n",
      "Epoch 0 step 103: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 103: training loss: 723.987099618696\n",
      "Epoch 0 step 104: training accuarcy: 0.9939\n",
      "Epoch 0 step 104: training loss: 769.1615533614327\n",
      "Epoch 0 step 105: training accuarcy: 0.9915\n",
      "Epoch 0 step 105: training loss: 722.7133614708011\n",
      "Epoch 0 step 106: training accuarcy: 0.9923000000000001\n",
      "Epoch 0 step 106: training loss: 727.8071325980661\n",
      "Epoch 0 step 107: training accuarcy: 0.9922000000000001\n",
      "Epoch 0 step 107: training loss: 753.7671943483474\n",
      "Epoch 0 step 108: training accuarcy: 0.9925\n",
      "Epoch 0 step 108: training loss: 732.8625292948698\n",
      "Epoch 0 step 109: training accuarcy: 0.9908\n",
      "Epoch 0 step 109: training loss: 625.9349843707233\n",
      "Epoch 0 step 110: training accuarcy: 0.9943000000000001\n",
      "Epoch 0 step 110: training loss: 669.918490389244\n",
      "Epoch 0 step 111: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 111: training loss: 649.6947497024534\n",
      "Epoch 0 step 112: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 112: training loss: 665.8610553040145\n",
      "Epoch 0 step 113: training accuarcy: 0.9933000000000001\n",
      "Epoch 0 step 113: training loss: 634.7671844276717\n",
      "Epoch 0 step 114: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 114: training loss: 627.9333851277944\n",
      "Epoch 0 step 115: training accuarcy: 0.9918238993710692\n",
      "Epoch 0: train loss 4087.9816902492003, train accuarcy 0.9860076904296875\n",
      "Epoch 0: valid loss 558.9601700495679, valid accuarcy 0.9938648343086243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [11:15<11:15, 675.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 664.9170099245275\n",
      "Epoch 1 step 116: training accuarcy: 0.9927\n",
      "Epoch 1 step 116: training loss: 599.046155383268\n",
      "Epoch 1 step 117: training accuarcy: 0.9926\n",
      "Epoch 1 step 117: training loss: 615.8627759908185\n",
      "Epoch 1 step 118: training accuarcy: 0.9916\n",
      "Epoch 1 step 118: training loss: 631.6514158098196\n",
      "Epoch 1 step 119: training accuarcy: 0.9922000000000001\n",
      "Epoch 1 step 119: training loss: 657.8047752044704\n",
      "Epoch 1 step 120: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 120: training loss: 606.0884341488816\n",
      "Epoch 1 step 121: training accuarcy: 0.9927\n",
      "Epoch 1 step 121: training loss: 530.9986929363481\n",
      "Epoch 1 step 122: training accuarcy: 0.9939\n",
      "Epoch 1 step 122: training loss: 563.6020754520605\n",
      "Epoch 1 step 123: training accuarcy: 0.9926\n",
      "Epoch 1 step 123: training loss: 556.0102186648044\n",
      "Epoch 1 step 124: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 124: training loss: 534.6703722445317\n",
      "Epoch 1 step 125: training accuarcy: 0.9935\n",
      "Epoch 1 step 125: training loss: 576.5458353228742\n",
      "Epoch 1 step 126: training accuarcy: 0.9931000000000001\n",
      "Epoch 1 step 126: training loss: 587.4239884662046\n",
      "Epoch 1 step 127: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 127: training loss: 517.0409571449137\n",
      "Epoch 1 step 128: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 128: training loss: 524.7356067499118\n",
      "Epoch 1 step 129: training accuarcy: 0.9937\n",
      "Epoch 1 step 129: training loss: 585.0095206651847\n",
      "Epoch 1 step 130: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 130: training loss: 503.5209410886905\n",
      "Epoch 1 step 131: training accuarcy: 0.9939\n",
      "Epoch 1 step 131: training loss: 546.3866088769205\n",
      "Epoch 1 step 132: training accuarcy: 0.9931000000000001\n",
      "Epoch 1 step 132: training loss: 514.8851609657181\n",
      "Epoch 1 step 133: training accuarcy: 0.9937\n",
      "Epoch 1 step 133: training loss: 476.558085690915\n",
      "Epoch 1 step 134: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 134: training loss: 445.6722250299388\n",
      "Epoch 1 step 135: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 135: training loss: 488.48915063145375\n",
      "Epoch 1 step 136: training accuarcy: 0.9958\n",
      "Epoch 1 step 136: training loss: 521.703145835218\n",
      "Epoch 1 step 137: training accuarcy: 0.9928\n",
      "Epoch 1 step 137: training loss: 529.8154738240905\n",
      "Epoch 1 step 138: training accuarcy: 0.9923000000000001\n",
      "Epoch 1 step 138: training loss: 489.96691541997956\n",
      "Epoch 1 step 139: training accuarcy: 0.9939\n",
      "Epoch 1 step 139: training loss: 493.1014448332278\n",
      "Epoch 1 step 140: training accuarcy: 0.9949\n",
      "Epoch 1 step 140: training loss: 426.0826751237414\n",
      "Epoch 1 step 141: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 141: training loss: 477.04412960336344\n",
      "Epoch 1 step 142: training accuarcy: 0.9936\n",
      "Epoch 1 step 142: training loss: 459.48082210921314\n",
      "Epoch 1 step 143: training accuarcy: 0.9946\n",
      "Epoch 1 step 143: training loss: 502.5655632070225\n",
      "Epoch 1 step 144: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 144: training loss: 523.5247383815839\n",
      "Epoch 1 step 145: training accuarcy: 0.9931000000000001\n",
      "Epoch 1 step 145: training loss: 483.71700031868755\n",
      "Epoch 1 step 146: training accuarcy: 0.9948\n",
      "Epoch 1 step 146: training loss: 450.9580225990395\n",
      "Epoch 1 step 147: training accuarcy: 0.9945\n",
      "Epoch 1 step 147: training loss: 484.13157071454725\n",
      "Epoch 1 step 148: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 148: training loss: 490.9032726050798\n",
      "Epoch 1 step 149: training accuarcy: 0.9926\n",
      "Epoch 1 step 149: training loss: 431.07547871350135\n",
      "Epoch 1 step 150: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 150: training loss: 474.3874722567855\n",
      "Epoch 1 step 151: training accuarcy: 0.9939\n",
      "Epoch 1 step 151: training loss: 446.0944498297983\n",
      "Epoch 1 step 152: training accuarcy: 0.994\n",
      "Epoch 1 step 152: training loss: 437.9782010712255\n",
      "Epoch 1 step 153: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 153: training loss: 486.61807699728877\n",
      "Epoch 1 step 154: training accuarcy: 0.9934000000000001\n",
      "Epoch 1 step 154: training loss: 459.87447400274914\n",
      "Epoch 1 step 155: training accuarcy: 0.993\n",
      "Epoch 1 step 155: training loss: 431.5204991113385\n",
      "Epoch 1 step 156: training accuarcy: 0.9947\n",
      "Epoch 1 step 156: training loss: 419.19088165635003\n",
      "Epoch 1 step 157: training accuarcy: 0.995\n",
      "Epoch 1 step 157: training loss: 473.0034532518521\n",
      "Epoch 1 step 158: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 158: training loss: 454.4253992992602\n",
      "Epoch 1 step 159: training accuarcy: 0.9946\n",
      "Epoch 1 step 159: training loss: 474.10917684336516\n",
      "Epoch 1 step 160: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 160: training loss: 485.67165837199707\n",
      "Epoch 1 step 161: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 161: training loss: 474.84503183802394\n",
      "Epoch 1 step 162: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 162: training loss: 412.36244048772085\n",
      "Epoch 1 step 163: training accuarcy: 0.9935\n",
      "Epoch 1 step 163: training loss: 413.98058832226764\n",
      "Epoch 1 step 164: training accuarcy: 0.9948\n",
      "Epoch 1 step 164: training loss: 430.0440221128116\n",
      "Epoch 1 step 165: training accuarcy: 0.9937\n",
      "Epoch 1 step 165: training loss: 460.49992064380183\n",
      "Epoch 1 step 166: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 166: training loss: 394.06351296712364\n",
      "Epoch 1 step 167: training accuarcy: 0.9948\n",
      "Epoch 1 step 167: training loss: 391.8316673284931\n",
      "Epoch 1 step 168: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 168: training loss: 413.250767882874\n",
      "Epoch 1 step 169: training accuarcy: 0.9948\n",
      "Epoch 1 step 169: training loss: 442.9850985909792\n",
      "Epoch 1 step 170: training accuarcy: 0.9946\n",
      "Epoch 1 step 170: training loss: 454.0327906019111\n",
      "Epoch 1 step 171: training accuarcy: 0.9929\n",
      "Epoch 1 step 171: training loss: 344.4427830411237\n",
      "Epoch 1 step 172: training accuarcy: 0.9959\n",
      "Epoch 1 step 172: training loss: 426.29542376242784\n",
      "Epoch 1 step 173: training accuarcy: 0.994\n",
      "Epoch 1 step 173: training loss: 414.6018328823633\n",
      "Epoch 1 step 174: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 174: training loss: 410.2044934010322\n",
      "Epoch 1 step 175: training accuarcy: 0.9949\n",
      "Epoch 1 step 175: training loss: 451.3206289995785\n",
      "Epoch 1 step 176: training accuarcy: 0.9929\n",
      "Epoch 1 step 176: training loss: 390.15226314146605\n",
      "Epoch 1 step 177: training accuarcy: 0.9949\n",
      "Epoch 1 step 177: training loss: 445.79013260108286\n",
      "Epoch 1 step 178: training accuarcy: 0.9934000000000001\n",
      "Epoch 1 step 178: training loss: 384.78456949456734\n",
      "Epoch 1 step 179: training accuarcy: 0.9949\n",
      "Epoch 1 step 179: training loss: 374.2927371231083\n",
      "Epoch 1 step 180: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 180: training loss: 416.99977563412847\n",
      "Epoch 1 step 181: training accuarcy: 0.9934000000000001\n",
      "Epoch 1 step 181: training loss: 434.89719717594596\n",
      "Epoch 1 step 182: training accuarcy: 0.9936\n",
      "Epoch 1 step 182: training loss: 377.7115626547178\n",
      "Epoch 1 step 183: training accuarcy: 0.9948\n",
      "Epoch 1 step 183: training loss: 387.4479968794165\n",
      "Epoch 1 step 184: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 184: training loss: 425.8903598804338\n",
      "Epoch 1 step 185: training accuarcy: 0.9935\n",
      "Epoch 1 step 185: training loss: 400.9172941235746\n",
      "Epoch 1 step 186: training accuarcy: 0.9939\n",
      "Epoch 1 step 186: training loss: 377.484322036285\n",
      "Epoch 1 step 187: training accuarcy: 0.9947\n",
      "Epoch 1 step 187: training loss: 439.9480941434409\n",
      "Epoch 1 step 188: training accuarcy: 0.9938\n",
      "Epoch 1 step 188: training loss: 382.59766466016595\n",
      "Epoch 1 step 189: training accuarcy: 0.9946\n",
      "Epoch 1 step 189: training loss: 423.50272006682417\n",
      "Epoch 1 step 190: training accuarcy: 0.9931000000000001\n",
      "Epoch 1 step 190: training loss: 416.734370935245\n",
      "Epoch 1 step 191: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 191: training loss: 424.2093679652372\n",
      "Epoch 1 step 192: training accuarcy: 0.9936\n",
      "Epoch 1 step 192: training loss: 397.25616571069344\n",
      "Epoch 1 step 193: training accuarcy: 0.9935\n",
      "Epoch 1 step 193: training loss: 391.8930612541054\n",
      "Epoch 1 step 194: training accuarcy: 0.9948\n",
      "Epoch 1 step 194: training loss: 368.4195676638155\n",
      "Epoch 1 step 195: training accuarcy: 0.995\n",
      "Epoch 1 step 195: training loss: 389.6439446474147\n",
      "Epoch 1 step 196: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 196: training loss: 428.57450524278875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 197: training accuarcy: 0.9935\n",
      "Epoch 1 step 197: training loss: 345.6891099117653\n",
      "Epoch 1 step 198: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 198: training loss: 378.44271589610366\n",
      "Epoch 1 step 199: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 199: training loss: 378.5350091680062\n",
      "Epoch 1 step 200: training accuarcy: 0.9957\n",
      "Epoch 1 step 200: training loss: 389.882784343892\n",
      "Epoch 1 step 201: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 201: training loss: 337.50168711458946\n",
      "Epoch 1 step 202: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 202: training loss: 389.4433368269072\n",
      "Epoch 1 step 203: training accuarcy: 0.994\n",
      "Epoch 1 step 203: training loss: 356.5700536028768\n",
      "Epoch 1 step 204: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 204: training loss: 417.68784188598306\n",
      "Epoch 1 step 205: training accuarcy: 0.9935\n",
      "Epoch 1 step 205: training loss: 371.4368643084421\n",
      "Epoch 1 step 206: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 206: training loss: 401.8936109498742\n",
      "Epoch 1 step 207: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 207: training loss: 446.4586385038922\n",
      "Epoch 1 step 208: training accuarcy: 0.9929\n",
      "Epoch 1 step 208: training loss: 384.01817875742705\n",
      "Epoch 1 step 209: training accuarcy: 0.9955\n",
      "Epoch 1 step 209: training loss: 398.49187838378776\n",
      "Epoch 1 step 210: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 210: training loss: 362.9313423113944\n",
      "Epoch 1 step 211: training accuarcy: 0.9949\n",
      "Epoch 1 step 211: training loss: 365.0298014086553\n",
      "Epoch 1 step 212: training accuarcy: 0.9946\n",
      "Epoch 1 step 212: training loss: 341.5647644900364\n",
      "Epoch 1 step 213: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 213: training loss: 438.86445421221515\n",
      "Epoch 1 step 214: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 214: training loss: 362.5000953833732\n",
      "Epoch 1 step 215: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 215: training loss: 343.1488921068943\n",
      "Epoch 1 step 216: training accuarcy: 0.9955\n",
      "Epoch 1 step 216: training loss: 391.8868329572543\n",
      "Epoch 1 step 217: training accuarcy: 0.9945\n",
      "Epoch 1 step 217: training loss: 351.77094890351634\n",
      "Epoch 1 step 218: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 218: training loss: 371.1360420998275\n",
      "Epoch 1 step 219: training accuarcy: 0.996\n",
      "Epoch 1 step 219: training loss: 373.0369558020351\n",
      "Epoch 1 step 220: training accuarcy: 0.9938\n",
      "Epoch 1 step 220: training loss: 316.5380839653136\n",
      "Epoch 1 step 221: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 221: training loss: 417.91872071815976\n",
      "Epoch 1 step 222: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 222: training loss: 388.17544026921735\n",
      "Epoch 1 step 223: training accuarcy: 0.9938\n",
      "Epoch 1 step 223: training loss: 399.7126688259524\n",
      "Epoch 1 step 224: training accuarcy: 0.9939\n",
      "Epoch 1 step 224: training loss: 332.4752402341727\n",
      "Epoch 1 step 225: training accuarcy: 0.9957\n",
      "Epoch 1 step 225: training loss: 318.045283356287\n",
      "Epoch 1 step 226: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 226: training loss: 356.17892976585676\n",
      "Epoch 1 step 227: training accuarcy: 0.9946\n",
      "Epoch 1 step 227: training loss: 361.36888451679306\n",
      "Epoch 1 step 228: training accuarcy: 0.9946\n",
      "Epoch 1 step 228: training loss: 353.0512200223526\n",
      "Epoch 1 step 229: training accuarcy: 0.9938\n",
      "Epoch 1 step 229: training loss: 307.2142920766345\n",
      "Epoch 1 step 230: training accuarcy: 0.9957023060796645\n",
      "Epoch 1: train loss 439.96846331607867, train accuarcy 0.9934626817703247\n",
      "Epoch 1: valid loss 309.11901439024416, valid accuarcy 0.995110809803009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [22:26<00:00, 674.30s/it]"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=2, log_dir=get_log_dir('weight_kaggle', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T08:02:48.422964Z",
     "start_time": "2019-10-08T08:02:48.416961Z"
    }
   },
   "outputs": [],
   "source": [
    "del trans_model\n",
    "T.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
