{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:48.051519Z",
     "start_time": "2019-09-25T08:31:47.808458Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:48.490616Z",
     "start_time": "2019-09-25T08:31:48.483587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\python\\recommender\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:49.428967Z",
     "start_time": "2019-09-25T08:31:49.019878Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import get_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:51.355016Z",
     "start_time": "2019-09-25T08:31:49.859998Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import TorchKaggle\n",
    "from models import FMLearner, TorchFM, TorchHrmFM, TorchPrmeFM, TorchTransFM\n",
    "from models.fm_learner import simple_loss, trans_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:52.010375Z",
     "start_time": "2019-09-25T08:31:51.811288Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = T.cuda.current_device()\n",
    "BATCH = 2000\n",
    "SHUFFLE = True\n",
    "WORKERS = 0\n",
    "item_path = Path(\"./inputs/kaggle/item.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:54.188966Z",
     "start_time": "2019-09-25T08:31:52.530917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataframe shape (476244, 7)\n",
      "After drop nan shape: (429988, 7)\n",
      "Original comptition size: 292\n",
      "Original competitor size: 140065\n",
      "Filtered competiter size: 27449\n",
      "Filtered dataframe shape: (284806, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datasets.torch_kaggle.TorchKaggle at 0x1edd5428710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = TorchKaggle(data_path=item_path, user_min=4)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:54.799354Z",
     "start_time": "2019-09-25T08:31:54.795358Z"
    }
   },
   "outputs": [],
   "source": [
    "db.config_db(batch_size=BATCH,\n",
    "             shuffle=SHUFFLE,\n",
    "             num_workers=WORKERS,\n",
    "             device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:56.877296Z",
     "start_time": "2019-09-25T08:31:56.875327Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:57.766363Z",
     "start_time": "2019-09-25T08:31:57.763332Z"
    }
   },
   "outputs": [],
   "source": [
    "# regst setting\n",
    "LINEAR_REG = 1\n",
    "EMB_REG = 1\n",
    "TRANS_REG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:58.287358Z",
     "start_time": "2019-09-25T08:31:58.283357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_loss at 0x000001EDD53DBAE8>, 1, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_loss_callback = partial(simple_loss, LINEAR_REG, EMB_REG)\n",
    "simple_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:58.784907Z",
     "start_time": "2019-09-25T08:31:58.778910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_loss at 0x000001EDD5432BF8>, 1, 1, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_loss_callback = partial(trans_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_loss_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "### Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:31:59.879104Z",
     "start_time": "2019-09-25T08:31:59.876102Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:32:02.637968Z",
     "start_time": "2019-09-25T08:32:02.634997Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DECAY_FREQ = 1000\n",
    "DECAY_GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:32:03.186729Z",
     "start_time": "2019-09-25T08:32:03.116762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchFM()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model = TorchFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:32:03.645861Z",
     "start_time": "2019-09-25T08:32:03.642887Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(fm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt, step_size=DECAY_FREQ, gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:32:06.380580Z",
     "start_time": "2019-09-25T08:32:04.146973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1edcc967f60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_learner = FMLearner(fm_model, adam_opt, schedular, db)\n",
    "fm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:38:40.163586Z",
     "start_time": "2019-09-25T08:32:06.796252Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 48414.34139250202\n",
      "Epoch 0 step 1: training accuarcy: 0.06\n",
      "Epoch 0 step 1: training loss: 48823.16708975167\n",
      "Epoch 0 step 2: training accuarcy: 0.07200000000000001\n",
      "Epoch 0 step 2: training loss: 47717.218700082536\n",
      "Epoch 0 step 3: training accuarcy: 0.1015\n",
      "Epoch 0 step 3: training loss: 47393.68143863562\n",
      "Epoch 0 step 4: training accuarcy: 0.1155\n",
      "Epoch 0 step 4: training loss: 48496.713193549214\n",
      "Epoch 0 step 5: training accuarcy: 0.094\n",
      "Epoch 0 step 5: training loss: 47173.28324278945\n",
      "Epoch 0 step 6: training accuarcy: 0.124\n",
      "Epoch 0 step 6: training loss: 46900.029386558475\n",
      "Epoch 0 step 7: training accuarcy: 0.123\n",
      "Epoch 0 step 7: training loss: 46350.61429493471\n",
      "Epoch 0 step 8: training accuarcy: 0.1355\n",
      "Epoch 0 step 8: training loss: 46138.23666630378\n",
      "Epoch 0 step 9: training accuarcy: 0.137\n",
      "Epoch 0 step 9: training loss: 45006.27565242712\n",
      "Epoch 0 step 10: training accuarcy: 0.152\n",
      "Epoch 0 step 10: training loss: 44627.071136788574\n",
      "Epoch 0 step 11: training accuarcy: 0.158\n",
      "Epoch 0 step 11: training loss: 45132.869933131195\n",
      "Epoch 0 step 12: training accuarcy: 0.138\n",
      "Epoch 0 step 12: training loss: 44561.032711932785\n",
      "Epoch 0 step 13: training accuarcy: 0.157\n",
      "Epoch 0 step 13: training loss: 43511.753365008895\n",
      "Epoch 0 step 14: training accuarcy: 0.167\n",
      "Epoch 0 step 14: training loss: 43451.95985505073\n",
      "Epoch 0 step 15: training accuarcy: 0.162\n",
      "Epoch 0 step 15: training loss: 42763.31669035687\n",
      "Epoch 0 step 16: training accuarcy: 0.17500000000000002\n",
      "Epoch 0 step 16: training loss: 42301.964186099554\n",
      "Epoch 0 step 17: training accuarcy: 0.1775\n",
      "Epoch 0 step 17: training loss: 42514.88933363245\n",
      "Epoch 0 step 18: training accuarcy: 0.182\n",
      "Epoch 0 step 18: training loss: 41383.577957582835\n",
      "Epoch 0 step 19: training accuarcy: 0.201\n",
      "Epoch 0 step 19: training loss: 41128.18388235719\n",
      "Epoch 0 step 20: training accuarcy: 0.20550000000000002\n",
      "Epoch 0 step 20: training loss: 40203.36355501285\n",
      "Epoch 0 step 21: training accuarcy: 0.2165\n",
      "Epoch 0 step 21: training loss: 40162.5701731241\n",
      "Epoch 0 step 22: training accuarcy: 0.213\n",
      "Epoch 0 step 22: training loss: 39145.23013415609\n",
      "Epoch 0 step 23: training accuarcy: 0.23700000000000002\n",
      "Epoch 0 step 23: training loss: 39876.84575877614\n",
      "Epoch 0 step 24: training accuarcy: 0.2205\n",
      "Epoch 0 step 24: training loss: 38570.81126464584\n",
      "Epoch 0 step 25: training accuarcy: 0.2425\n",
      "Epoch 0 step 25: training loss: 38931.20552982739\n",
      "Epoch 0 step 26: training accuarcy: 0.2325\n",
      "Epoch 0 step 26: training loss: 37623.497559742755\n",
      "Epoch 0 step 27: training accuarcy: 0.2575\n",
      "Epoch 0 step 27: training loss: 38752.203543266376\n",
      "Epoch 0 step 28: training accuarcy: 0.23600000000000002\n",
      "Epoch 0 step 28: training loss: 38249.082353011414\n",
      "Epoch 0 step 29: training accuarcy: 0.23850000000000002\n",
      "Epoch 0 step 29: training loss: 37003.50587173137\n",
      "Epoch 0 step 30: training accuarcy: 0.2655\n",
      "Epoch 0 step 30: training loss: 37162.226558946415\n",
      "Epoch 0 step 31: training accuarcy: 0.263\n",
      "Epoch 0 step 31: training loss: 36742.4222043878\n",
      "Epoch 0 step 32: training accuarcy: 0.2665\n",
      "Epoch 0 step 32: training loss: 35926.07358679274\n",
      "Epoch 0 step 33: training accuarcy: 0.28150000000000003\n",
      "Epoch 0 step 33: training loss: 35711.37132375422\n",
      "Epoch 0 step 34: training accuarcy: 0.2775\n",
      "Epoch 0 step 34: training loss: 36062.1048452194\n",
      "Epoch 0 step 35: training accuarcy: 0.2725\n",
      "Epoch 0 step 35: training loss: 35318.193629985006\n",
      "Epoch 0 step 36: training accuarcy: 0.29\n",
      "Epoch 0 step 36: training loss: 34502.62252841163\n",
      "Epoch 0 step 37: training accuarcy: 0.2975\n",
      "Epoch 0 step 37: training loss: 33775.55531572858\n",
      "Epoch 0 step 38: training accuarcy: 0.315\n",
      "Epoch 0 step 38: training loss: 32944.50824533719\n",
      "Epoch 0 step 39: training accuarcy: 0.3325\n",
      "Epoch 0 step 39: training loss: 32649.229554289614\n",
      "Epoch 0 step 40: training accuarcy: 0.3355\n",
      "Epoch 0 step 40: training loss: 31394.220638689792\n",
      "Epoch 0 step 41: training accuarcy: 0.3605\n",
      "Epoch 0 step 41: training loss: 29250.576968611553\n",
      "Epoch 0 step 42: training accuarcy: 0.393\n",
      "Epoch 0 step 42: training loss: 28352.457740896505\n",
      "Epoch 0 step 43: training accuarcy: 0.41500000000000004\n",
      "Epoch 0 step 43: training loss: 25439.816916265983\n",
      "Epoch 0 step 44: training accuarcy: 0.48\n",
      "Epoch 0 step 44: training loss: 23362.112366367204\n",
      "Epoch 0 step 45: training accuarcy: 0.505\n",
      "Epoch 0 step 45: training loss: 18298.360741592136\n",
      "Epoch 0 step 46: training accuarcy: 0.614\n",
      "Epoch 0 step 46: training loss: 12345.59310526168\n",
      "Epoch 0 step 47: training accuarcy: 0.736\n",
      "Epoch 0 step 47: training loss: 3999.618407647809\n",
      "Epoch 0 step 48: training accuarcy: 0.9265\n",
      "Epoch 0 step 48: training loss: 2792.4441911661734\n",
      "Epoch 0 step 49: training accuarcy: 0.9925\n",
      "Epoch 0 step 49: training loss: 2711.281508846186\n",
      "Epoch 0 step 50: training accuarcy: 0.9945\n",
      "Epoch 0 step 50: training loss: 2632.913322520539\n",
      "Epoch 0 step 51: training accuarcy: 0.9955\n",
      "Epoch 0 step 51: training loss: 2559.513249496928\n",
      "Epoch 0 step 52: training accuarcy: 0.9965\n",
      "Epoch 0 step 52: training loss: 2498.8743882668205\n",
      "Epoch 0 step 53: training accuarcy: 0.994\n",
      "Epoch 0 step 53: training loss: 2423.904433790786\n",
      "Epoch 0 step 54: training accuarcy: 0.9975\n",
      "Epoch 0 step 54: training loss: 2358.7101121445044\n",
      "Epoch 0 step 55: training accuarcy: 0.998\n",
      "Epoch 0 step 55: training loss: 2298.6541111967413\n",
      "Epoch 0 step 56: training accuarcy: 0.9955\n",
      "Epoch 0 step 56: training loss: 2245.205395429727\n",
      "Epoch 0 step 57: training accuarcy: 0.993\n",
      "Epoch 0 step 57: training loss: 2179.5041911364738\n",
      "Epoch 0 step 58: training accuarcy: 0.996\n",
      "Epoch 0 step 58: training loss: 2125.6585956902986\n",
      "Epoch 0 step 59: training accuarcy: 0.9945\n",
      "Epoch 0 step 59: training loss: 2071.390244558453\n",
      "Epoch 0 step 60: training accuarcy: 0.994\n",
      "Epoch 0 step 60: training loss: 2017.280241532224\n",
      "Epoch 0 step 61: training accuarcy: 0.995\n",
      "Epoch 0 step 61: training loss: 1961.5090279425076\n",
      "Epoch 0 step 62: training accuarcy: 0.997\n",
      "Epoch 0 step 62: training loss: 1914.4227260149933\n",
      "Epoch 0 step 63: training accuarcy: 0.996\n",
      "Epoch 0 step 63: training loss: 1868.0216541458979\n",
      "Epoch 0 step 64: training accuarcy: 0.9955\n",
      "Epoch 0 step 64: training loss: 1815.8138179862262\n",
      "Epoch 0 step 65: training accuarcy: 0.9965\n",
      "Epoch 0 step 65: training loss: 1777.4019268752354\n",
      "Epoch 0 step 66: training accuarcy: 0.9935\n",
      "Epoch 0 step 66: training loss: 1728.7008673007729\n",
      "Epoch 0 step 67: training accuarcy: 0.9965\n",
      "Epoch 0 step 67: training loss: 1688.590780188355\n",
      "Epoch 0 step 68: training accuarcy: 0.994\n",
      "Epoch 0 step 68: training loss: 1649.847314891928\n",
      "Epoch 0 step 69: training accuarcy: 0.9955\n",
      "Epoch 0 step 69: training loss: 1608.6809932554602\n",
      "Epoch 0 step 70: training accuarcy: 0.995\n",
      "Epoch 0 step 70: training loss: 1565.7740554278612\n",
      "Epoch 0 step 71: training accuarcy: 0.9975\n",
      "Epoch 0 step 71: training loss: 1531.5481315491606\n",
      "Epoch 0 step 72: training accuarcy: 0.9955\n",
      "Epoch 0 step 72: training loss: 1494.2723915708682\n",
      "Epoch 0 step 73: training accuarcy: 0.9955\n",
      "Epoch 0 step 73: training loss: 1458.1501588330823\n",
      "Epoch 0 step 74: training accuarcy: 0.996\n",
      "Epoch 0 step 74: training loss: 1424.9726383423254\n",
      "Epoch 0 step 75: training accuarcy: 0.9975\n",
      "Epoch 0 step 75: training loss: 1394.0388488026915\n",
      "Epoch 0 step 76: training accuarcy: 0.9955\n",
      "Epoch 0 step 76: training loss: 1364.3392079029331\n",
      "Epoch 0 step 77: training accuarcy: 0.994\n",
      "Epoch 0 step 77: training loss: 1330.7069205360772\n",
      "Epoch 0 step 78: training accuarcy: 0.996\n",
      "Epoch 0 step 78: training loss: 1297.3432909463606\n",
      "Epoch 0 step 79: training accuarcy: 0.9975\n",
      "Epoch 0 step 79: training loss: 1271.9019053783963\n",
      "Epoch 0 step 80: training accuarcy: 0.996\n",
      "Epoch 0 step 80: training loss: 1244.4896398127623\n",
      "Epoch 0 step 81: training accuarcy: 0.996\n",
      "Epoch 0 step 81: training loss: 1220.2816074563245\n",
      "Epoch 0 step 82: training accuarcy: 0.994\n",
      "Epoch 0 step 82: training loss: 1194.6798935779389\n",
      "Epoch 0 step 83: training accuarcy: 0.994\n",
      "Epoch 0 step 83: training loss: 1167.6227877593456\n",
      "Epoch 0 step 84: training accuarcy: 0.995\n",
      "Epoch 0 step 84: training loss: 1142.1873616982284\n",
      "Epoch 0 step 85: training accuarcy: 0.9965\n",
      "Epoch 0 step 85: training loss: 1123.4898587698285\n",
      "Epoch 0 step 86: training accuarcy: 0.9935\n",
      "Epoch 0 step 86: training loss: 1094.8743831696931\n",
      "Epoch 0 step 87: training accuarcy: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 87: training loss: 1076.587434787167\n",
      "Epoch 0 step 88: training accuarcy: 0.995\n",
      "Epoch 0 step 88: training loss: 1053.1617675210052\n",
      "Epoch 0 step 89: training accuarcy: 0.9985\n",
      "Epoch 0 step 89: training loss: 1031.8444001514476\n",
      "Epoch 0 step 90: training accuarcy: 0.998\n",
      "Epoch 0 step 90: training loss: 1014.1985229698965\n",
      "Epoch 0 step 91: training accuarcy: 0.9955\n",
      "Epoch 0 step 91: training loss: 993.5876761668814\n",
      "Epoch 0 step 92: training accuarcy: 0.9955\n",
      "Epoch 0 step 92: training loss: 975.1689707038748\n",
      "Epoch 0 step 93: training accuarcy: 0.9965\n",
      "Epoch 0 step 93: training loss: 957.6688646031182\n",
      "Epoch 0 step 94: training accuarcy: 0.9975\n",
      "Epoch 0 step 94: training loss: 936.7250568067739\n",
      "Epoch 0 step 95: training accuarcy: 0.9985\n",
      "Epoch 0 step 95: training loss: 920.0271281022422\n",
      "Epoch 0 step 96: training accuarcy: 0.998\n",
      "Epoch 0 step 96: training loss: 905.0953875605263\n",
      "Epoch 0 step 97: training accuarcy: 0.998\n",
      "Epoch 0 step 97: training loss: 891.669021012198\n",
      "Epoch 0 step 98: training accuarcy: 0.9955\n",
      "Epoch 0 step 98: training loss: 875.3914588037313\n",
      "Epoch 0 step 99: training accuarcy: 0.997\n",
      "Epoch 0 step 99: training loss: 859.9267691517412\n",
      "Epoch 0 step 100: training accuarcy: 0.9975\n",
      "Epoch 0 step 100: training loss: 847.4811344166112\n",
      "Epoch 0 step 101: training accuarcy: 0.998\n",
      "Epoch 0 step 101: training loss: 831.6728551595636\n",
      "Epoch 0 step 102: training accuarcy: 0.997\n",
      "Epoch 0 step 102: training loss: 817.1896042328838\n",
      "Epoch 0 step 103: training accuarcy: 0.9995\n",
      "Epoch 0 step 103: training loss: 800.9353471416713\n",
      "Epoch 0 step 104: training accuarcy: 0.9985\n",
      "Epoch 0 step 104: training loss: 790.2434835068659\n",
      "Epoch 0 step 105: training accuarcy: 0.9985\n",
      "Epoch 0 step 105: training loss: 777.6379263104038\n",
      "Epoch 0 step 106: training accuarcy: 0.9985\n",
      "Epoch 0 step 106: training loss: 771.1370221044036\n",
      "Epoch 0 step 107: training accuarcy: 0.9965\n",
      "Epoch 0 step 107: training loss: 757.7373057221916\n",
      "Epoch 0 step 108: training accuarcy: 0.9985\n",
      "Epoch 0 step 108: training loss: 744.6127158764413\n",
      "Epoch 0 step 109: training accuarcy: 0.9985\n",
      "Epoch 0 step 109: training loss: 737.0776918136655\n",
      "Epoch 0 step 110: training accuarcy: 0.9975\n",
      "Epoch 0 step 110: training loss: 722.429516280941\n",
      "Epoch 0 step 111: training accuarcy: 0.998\n",
      "Epoch 0 step 111: training loss: 715.3787269369726\n",
      "Epoch 0 step 112: training accuarcy: 0.9975\n",
      "Epoch 0 step 112: training loss: 700.2075857368692\n",
      "Epoch 0 step 113: training accuarcy: 1.0\n",
      "Epoch 0 step 113: training loss: 695.6427744619381\n",
      "Epoch 0 step 114: training accuarcy: 0.9985\n",
      "Epoch 0 step 114: training loss: 686.8449065038982\n",
      "Epoch 0 step 115: training accuarcy: 0.9968553459119497\n",
      "Epoch 0: train loss 16597.233371838363, train accuarcy 0.6835309863090515\n",
      "Epoch 0: valid loss 669.764011179, valid accuarcy 0.9993078112602234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████▏                                                                                                                                     | 1/8 [00:51<06:02, 51.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 672.0394434303187\n",
      "Epoch 1 step 116: training accuarcy: 1.0\n",
      "Epoch 1 step 116: training loss: 667.3819621922501\n",
      "Epoch 1 step 117: training accuarcy: 0.9975\n",
      "Epoch 1 step 117: training loss: 660.4615419638493\n",
      "Epoch 1 step 118: training accuarcy: 0.998\n",
      "Epoch 1 step 118: training loss: 645.4609685067533\n",
      "Epoch 1 step 119: training accuarcy: 0.9995\n",
      "Epoch 1 step 119: training loss: 637.636123822194\n",
      "Epoch 1 step 120: training accuarcy: 1.0\n",
      "Epoch 1 step 120: training loss: 629.5567146957729\n",
      "Epoch 1 step 121: training accuarcy: 1.0\n",
      "Epoch 1 step 121: training loss: 625.1310815217778\n",
      "Epoch 1 step 122: training accuarcy: 0.999\n",
      "Epoch 1 step 122: training loss: 620.3083580548066\n",
      "Epoch 1 step 123: training accuarcy: 0.997\n",
      "Epoch 1 step 123: training loss: 608.4111965165807\n",
      "Epoch 1 step 124: training accuarcy: 0.9995\n",
      "Epoch 1 step 124: training loss: 603.5191606763558\n",
      "Epoch 1 step 125: training accuarcy: 0.9985\n",
      "Epoch 1 step 125: training loss: 599.2072506815748\n",
      "Epoch 1 step 126: training accuarcy: 0.9975\n",
      "Epoch 1 step 126: training loss: 590.6068058696396\n",
      "Epoch 1 step 127: training accuarcy: 0.9975\n",
      "Epoch 1 step 127: training loss: 581.9075433125674\n",
      "Epoch 1 step 128: training accuarcy: 0.999\n",
      "Epoch 1 step 128: training loss: 575.2975641950765\n",
      "Epoch 1 step 129: training accuarcy: 0.999\n",
      "Epoch 1 step 129: training loss: 570.8881928502556\n",
      "Epoch 1 step 130: training accuarcy: 0.998\n",
      "Epoch 1 step 130: training loss: 561.2965271960919\n",
      "Epoch 1 step 131: training accuarcy: 0.999\n",
      "Epoch 1 step 131: training loss: 556.3738799180836\n",
      "Epoch 1 step 132: training accuarcy: 0.9985\n",
      "Epoch 1 step 132: training loss: 549.2708116505488\n",
      "Epoch 1 step 133: training accuarcy: 0.9995\n",
      "Epoch 1 step 133: training loss: 543.6517029163399\n",
      "Epoch 1 step 134: training accuarcy: 1.0\n",
      "Epoch 1 step 134: training loss: 540.2393323583289\n",
      "Epoch 1 step 135: training accuarcy: 0.9995\n",
      "Epoch 1 step 135: training loss: 534.8977582136939\n",
      "Epoch 1 step 136: training accuarcy: 0.999\n",
      "Epoch 1 step 136: training loss: 531.8670579333944\n",
      "Epoch 1 step 137: training accuarcy: 0.997\n",
      "Epoch 1 step 137: training loss: 522.642043753093\n",
      "Epoch 1 step 138: training accuarcy: 0.999\n",
      "Epoch 1 step 138: training loss: 520.1682778064903\n",
      "Epoch 1 step 139: training accuarcy: 0.997\n",
      "Epoch 1 step 139: training loss: 514.8583343167522\n",
      "Epoch 1 step 140: training accuarcy: 0.9975\n",
      "Epoch 1 step 140: training loss: 507.7087547759174\n",
      "Epoch 1 step 141: training accuarcy: 0.9995\n",
      "Epoch 1 step 141: training loss: 502.9453490191308\n",
      "Epoch 1 step 142: training accuarcy: 0.999\n",
      "Epoch 1 step 142: training loss: 499.65267671805503\n",
      "Epoch 1 step 143: training accuarcy: 0.9985\n",
      "Epoch 1 step 143: training loss: 492.22761724321134\n",
      "Epoch 1 step 144: training accuarcy: 1.0\n",
      "Epoch 1 step 144: training loss: 494.0585693835637\n",
      "Epoch 1 step 145: training accuarcy: 0.998\n",
      "Epoch 1 step 145: training loss: 481.951190366923\n",
      "Epoch 1 step 146: training accuarcy: 0.9995\n",
      "Epoch 1 step 146: training loss: 483.12740099392244\n",
      "Epoch 1 step 147: training accuarcy: 0.9975\n",
      "Epoch 1 step 147: training loss: 476.02625133405354\n",
      "Epoch 1 step 148: training accuarcy: 0.999\n",
      "Epoch 1 step 148: training loss: 472.8973647019036\n",
      "Epoch 1 step 149: training accuarcy: 0.9975\n",
      "Epoch 1 step 149: training loss: 470.75327036701583\n",
      "Epoch 1 step 150: training accuarcy: 0.9995\n",
      "Epoch 1 step 150: training loss: 464.73771801817054\n",
      "Epoch 1 step 151: training accuarcy: 0.9995\n",
      "Epoch 1 step 151: training loss: 461.6395305353938\n",
      "Epoch 1 step 152: training accuarcy: 0.998\n",
      "Epoch 1 step 152: training loss: 457.0176750944116\n",
      "Epoch 1 step 153: training accuarcy: 0.999\n",
      "Epoch 1 step 153: training loss: 451.9939871482511\n",
      "Epoch 1 step 154: training accuarcy: 0.999\n",
      "Epoch 1 step 154: training loss: 448.8063557539701\n",
      "Epoch 1 step 155: training accuarcy: 0.999\n",
      "Epoch 1 step 155: training loss: 442.07241568582054\n",
      "Epoch 1 step 156: training accuarcy: 1.0\n",
      "Epoch 1 step 156: training loss: 444.4369019451831\n",
      "Epoch 1 step 157: training accuarcy: 0.9985\n",
      "Epoch 1 step 157: training loss: 437.75266013836267\n",
      "Epoch 1 step 158: training accuarcy: 0.999\n",
      "Epoch 1 step 158: training loss: 434.0876229350269\n",
      "Epoch 1 step 159: training accuarcy: 0.9995\n",
      "Epoch 1 step 159: training loss: 433.1664687077747\n",
      "Epoch 1 step 160: training accuarcy: 0.9985\n",
      "Epoch 1 step 160: training loss: 427.65494857403917\n",
      "Epoch 1 step 161: training accuarcy: 1.0\n",
      "Epoch 1 step 161: training loss: 422.1732278040864\n",
      "Epoch 1 step 162: training accuarcy: 0.9985\n",
      "Epoch 1 step 162: training loss: 420.7614087903375\n",
      "Epoch 1 step 163: training accuarcy: 0.999\n",
      "Epoch 1 step 163: training loss: 420.65729534012826\n",
      "Epoch 1 step 164: training accuarcy: 0.998\n",
      "Epoch 1 step 164: training loss: 416.52121322155256\n",
      "Epoch 1 step 165: training accuarcy: 0.9975\n",
      "Epoch 1 step 165: training loss: 408.50785249636846\n",
      "Epoch 1 step 166: training accuarcy: 1.0\n",
      "Epoch 1 step 166: training loss: 406.23022344978915\n",
      "Epoch 1 step 167: training accuarcy: 0.999\n",
      "Epoch 1 step 167: training loss: 407.19254320863996\n",
      "Epoch 1 step 168: training accuarcy: 0.9975\n",
      "Epoch 1 step 168: training loss: 403.43970161508923\n",
      "Epoch 1 step 169: training accuarcy: 0.9965\n",
      "Epoch 1 step 169: training loss: 398.75209532964243\n",
      "Epoch 1 step 170: training accuarcy: 0.998\n",
      "Epoch 1 step 170: training loss: 393.5948288961892\n",
      "Epoch 1 step 171: training accuarcy: 0.9985\n",
      "Epoch 1 step 171: training loss: 391.110859812304\n",
      "Epoch 1 step 172: training accuarcy: 0.999\n",
      "Epoch 1 step 172: training loss: 392.15944494928215\n",
      "Epoch 1 step 173: training accuarcy: 0.997\n",
      "Epoch 1 step 173: training loss: 384.9353118498372\n",
      "Epoch 1 step 174: training accuarcy: 0.9995\n",
      "Epoch 1 step 174: training loss: 383.77429506088805\n",
      "Epoch 1 step 175: training accuarcy: 0.998\n",
      "Epoch 1 step 175: training loss: 381.94474812326445\n",
      "Epoch 1 step 176: training accuarcy: 0.9975\n",
      "Epoch 1 step 176: training loss: 379.3440496202092\n",
      "Epoch 1 step 177: training accuarcy: 0.998\n",
      "Epoch 1 step 177: training loss: 374.9947773905637\n",
      "Epoch 1 step 178: training accuarcy: 0.9985\n",
      "Epoch 1 step 178: training loss: 371.27539600887854\n",
      "Epoch 1 step 179: training accuarcy: 0.999\n",
      "Epoch 1 step 179: training loss: 370.219276699693\n",
      "Epoch 1 step 180: training accuarcy: 0.999\n",
      "Epoch 1 step 180: training loss: 365.62672484994584\n",
      "Epoch 1 step 181: training accuarcy: 0.999\n",
      "Epoch 1 step 181: training loss: 366.17948194692747\n",
      "Epoch 1 step 182: training accuarcy: 0.998\n",
      "Epoch 1 step 182: training loss: 361.87251485706554\n",
      "Epoch 1 step 183: training accuarcy: 0.999\n",
      "Epoch 1 step 183: training loss: 361.10438244717545\n",
      "Epoch 1 step 184: training accuarcy: 0.9975\n",
      "Epoch 1 step 184: training loss: 354.8289063855788\n",
      "Epoch 1 step 185: training accuarcy: 1.0\n",
      "Epoch 1 step 185: training loss: 353.9576218599244\n",
      "Epoch 1 step 186: training accuarcy: 0.998\n",
      "Epoch 1 step 186: training loss: 349.10524505467976\n",
      "Epoch 1 step 187: training accuarcy: 0.9995\n",
      "Epoch 1 step 187: training loss: 348.6678321374891\n",
      "Epoch 1 step 188: training accuarcy: 0.9995\n",
      "Epoch 1 step 188: training loss: 345.51004318214666\n",
      "Epoch 1 step 189: training accuarcy: 0.999\n",
      "Epoch 1 step 189: training loss: 342.0304910587378\n",
      "Epoch 1 step 190: training accuarcy: 1.0\n",
      "Epoch 1 step 190: training loss: 340.56670805341105\n",
      "Epoch 1 step 191: training accuarcy: 0.999\n",
      "Epoch 1 step 191: training loss: 340.772447961063\n",
      "Epoch 1 step 192: training accuarcy: 0.998\n",
      "Epoch 1 step 192: training loss: 336.654314041624\n",
      "Epoch 1 step 193: training accuarcy: 0.9985\n",
      "Epoch 1 step 193: training loss: 334.92587829415714\n",
      "Epoch 1 step 194: training accuarcy: 0.9995\n",
      "Epoch 1 step 194: training loss: 330.427292740153\n",
      "Epoch 1 step 195: training accuarcy: 1.0\n",
      "Epoch 1 step 195: training loss: 332.3455675536503\n",
      "Epoch 1 step 196: training accuarcy: 0.9985\n",
      "Epoch 1 step 196: training loss: 327.02769169652123\n",
      "Epoch 1 step 197: training accuarcy: 0.9995\n",
      "Epoch 1 step 197: training loss: 325.1639133018353\n",
      "Epoch 1 step 198: training accuarcy: 0.9985\n",
      "Epoch 1 step 198: training loss: 324.0658052829523\n",
      "Epoch 1 step 199: training accuarcy: 0.9985\n",
      "Epoch 1 step 199: training loss: 321.3821075139372\n",
      "Epoch 1 step 200: training accuarcy: 0.9995\n",
      "Epoch 1 step 200: training loss: 318.8452775626948\n",
      "Epoch 1 step 201: training accuarcy: 0.999\n",
      "Epoch 1 step 201: training loss: 316.560658147855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 202: training accuarcy: 0.999\n",
      "Epoch 1 step 202: training loss: 314.2062954537682\n",
      "Epoch 1 step 203: training accuarcy: 1.0\n",
      "Epoch 1 step 203: training loss: 310.87292242158856\n",
      "Epoch 1 step 204: training accuarcy: 0.9995\n",
      "Epoch 1 step 204: training loss: 312.86835208372463\n",
      "Epoch 1 step 205: training accuarcy: 0.9985\n",
      "Epoch 1 step 205: training loss: 306.21922201614456\n",
      "Epoch 1 step 206: training accuarcy: 1.0\n",
      "Epoch 1 step 206: training loss: 309.20279191128344\n",
      "Epoch 1 step 207: training accuarcy: 0.9985\n",
      "Epoch 1 step 207: training loss: 302.35783586369865\n",
      "Epoch 1 step 208: training accuarcy: 1.0\n",
      "Epoch 1 step 208: training loss: 303.1132197838573\n",
      "Epoch 1 step 209: training accuarcy: 0.999\n",
      "Epoch 1 step 209: training loss: 298.54200122726013\n",
      "Epoch 1 step 210: training accuarcy: 1.0\n",
      "Epoch 1 step 210: training loss: 297.98395618981493\n",
      "Epoch 1 step 211: training accuarcy: 0.999\n",
      "Epoch 1 step 211: training loss: 294.7415266472844\n",
      "Epoch 1 step 212: training accuarcy: 0.999\n",
      "Epoch 1 step 212: training loss: 295.4843001708247\n",
      "Epoch 1 step 213: training accuarcy: 0.999\n",
      "Epoch 1 step 213: training loss: 293.8682200866774\n",
      "Epoch 1 step 214: training accuarcy: 0.998\n",
      "Epoch 1 step 214: training loss: 288.2765611417464\n",
      "Epoch 1 step 215: training accuarcy: 1.0\n",
      "Epoch 1 step 215: training loss: 288.0126096631233\n",
      "Epoch 1 step 216: training accuarcy: 0.9995\n",
      "Epoch 1 step 216: training loss: 286.77079098672544\n",
      "Epoch 1 step 217: training accuarcy: 1.0\n",
      "Epoch 1 step 217: training loss: 285.3479751739278\n",
      "Epoch 1 step 218: training accuarcy: 1.0\n",
      "Epoch 1 step 218: training loss: 284.307774126789\n",
      "Epoch 1 step 219: training accuarcy: 0.9995\n",
      "Epoch 1 step 219: training loss: 280.513090943979\n",
      "Epoch 1 step 220: training accuarcy: 0.999\n",
      "Epoch 1 step 220: training loss: 279.797043727789\n",
      "Epoch 1 step 221: training accuarcy: 1.0\n",
      "Epoch 1 step 221: training loss: 278.7884779025039\n",
      "Epoch 1 step 222: training accuarcy: 0.9985\n",
      "Epoch 1 step 222: training loss: 277.41230344718355\n",
      "Epoch 1 step 223: training accuarcy: 0.998\n",
      "Epoch 1 step 223: training loss: 271.80012518300316\n",
      "Epoch 1 step 224: training accuarcy: 1.0\n",
      "Epoch 1 step 224: training loss: 272.84870199778356\n",
      "Epoch 1 step 225: training accuarcy: 0.9995\n",
      "Epoch 1 step 225: training loss: 271.2418559049832\n",
      "Epoch 1 step 226: training accuarcy: 0.9985\n",
      "Epoch 1 step 226: training loss: 268.5228392347808\n",
      "Epoch 1 step 227: training accuarcy: 0.9995\n",
      "Epoch 1 step 227: training loss: 267.38532723165827\n",
      "Epoch 1 step 228: training accuarcy: 0.9995\n",
      "Epoch 1 step 228: training loss: 263.6070517822092\n",
      "Epoch 1 step 229: training accuarcy: 1.0\n",
      "Epoch 1 step 229: training loss: 263.3566864591942\n",
      "Epoch 1 step 230: training accuarcy: 1.0\n",
      "Epoch 1: train loss 413.3952493752368, train accuarcy 0.9985054731369019\n",
      "Epoch 1: valid loss 260.78980328382994, valid accuarcy 0.9995263814926147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████▎                                                                                                                  | 2/8 [01:42<05:08, 51.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 230: training loss: 259.88627454218266\n",
      "Epoch 2 step 231: training accuarcy: 1.0\n",
      "Epoch 2 step 231: training loss: 258.40485416576576\n",
      "Epoch 2 step 232: training accuarcy: 1.0\n",
      "Epoch 2 step 232: training loss: 260.3119922853133\n",
      "Epoch 2 step 233: training accuarcy: 0.9995\n",
      "Epoch 2 step 233: training loss: 257.2688257054081\n",
      "Epoch 2 step 234: training accuarcy: 0.999\n",
      "Epoch 2 step 234: training loss: 255.87762954585818\n",
      "Epoch 2 step 235: training accuarcy: 0.9995\n",
      "Epoch 2 step 235: training loss: 255.68862290531334\n",
      "Epoch 2 step 236: training accuarcy: 0.9985\n",
      "Epoch 2 step 236: training loss: 255.440431690503\n",
      "Epoch 2 step 237: training accuarcy: 0.9975\n",
      "Epoch 2 step 237: training loss: 250.1001852368558\n",
      "Epoch 2 step 238: training accuarcy: 0.999\n",
      "Epoch 2 step 238: training loss: 248.80297271899423\n",
      "Epoch 2 step 239: training accuarcy: 0.9995\n",
      "Epoch 2 step 239: training loss: 246.91249977564226\n",
      "Epoch 2 step 240: training accuarcy: 0.9995\n",
      "Epoch 2 step 240: training loss: 245.27455739767453\n",
      "Epoch 2 step 241: training accuarcy: 0.9995\n",
      "Epoch 2 step 241: training loss: 246.1182248075475\n",
      "Epoch 2 step 242: training accuarcy: 0.9985\n",
      "Epoch 2 step 242: training loss: 244.05680629709877\n",
      "Epoch 2 step 243: training accuarcy: 0.9985\n",
      "Epoch 2 step 243: training loss: 239.7199833505738\n",
      "Epoch 2 step 244: training accuarcy: 0.9995\n",
      "Epoch 2 step 244: training loss: 240.78623957366653\n",
      "Epoch 2 step 245: training accuarcy: 0.999\n",
      "Epoch 2 step 245: training loss: 237.77367007506814\n",
      "Epoch 2 step 246: training accuarcy: 0.9995\n",
      "Epoch 2 step 246: training loss: 239.15849702040353\n",
      "Epoch 2 step 247: training accuarcy: 0.9985\n",
      "Epoch 2 step 247: training loss: 235.1866355041665\n",
      "Epoch 2 step 248: training accuarcy: 0.9995\n",
      "Epoch 2 step 248: training loss: 235.02582298541355\n",
      "Epoch 2 step 249: training accuarcy: 0.9995\n",
      "Epoch 2 step 249: training loss: 233.1609877696121\n",
      "Epoch 2 step 250: training accuarcy: 0.999\n",
      "Epoch 2 step 250: training loss: 229.47432934161077\n",
      "Epoch 2 step 251: training accuarcy: 1.0\n",
      "Epoch 2 step 251: training loss: 230.97663754914242\n",
      "Epoch 2 step 252: training accuarcy: 0.9995\n",
      "Epoch 2 step 252: training loss: 230.47465349298062\n",
      "Epoch 2 step 253: training accuarcy: 0.999\n",
      "Epoch 2 step 253: training loss: 227.13726110624592\n",
      "Epoch 2 step 254: training accuarcy: 0.9995\n",
      "Epoch 2 step 254: training loss: 226.3622360726444\n",
      "Epoch 2 step 255: training accuarcy: 0.9995\n",
      "Epoch 2 step 255: training loss: 224.51749709718555\n",
      "Epoch 2 step 256: training accuarcy: 0.999\n",
      "Epoch 2 step 256: training loss: 222.96777722587427\n",
      "Epoch 2 step 257: training accuarcy: 0.999\n",
      "Epoch 2 step 257: training loss: 220.96206329132366\n",
      "Epoch 2 step 258: training accuarcy: 0.9995\n",
      "Epoch 2 step 258: training loss: 218.83280515429487\n",
      "Epoch 2 step 259: training accuarcy: 1.0\n",
      "Epoch 2 step 259: training loss: 219.51153137502104\n",
      "Epoch 2 step 260: training accuarcy: 1.0\n",
      "Epoch 2 step 260: training loss: 217.3276479484664\n",
      "Epoch 2 step 261: training accuarcy: 0.9995\n",
      "Epoch 2 step 261: training loss: 215.22874044824576\n",
      "Epoch 2 step 262: training accuarcy: 1.0\n",
      "Epoch 2 step 262: training loss: 217.00774794276856\n",
      "Epoch 2 step 263: training accuarcy: 0.9995\n",
      "Epoch 2 step 263: training loss: 212.77291254810336\n",
      "Epoch 2 step 264: training accuarcy: 1.0\n",
      "Epoch 2 step 264: training loss: 213.67461838227928\n",
      "Epoch 2 step 265: training accuarcy: 0.999\n",
      "Epoch 2 step 265: training loss: 212.0135849039356\n",
      "Epoch 2 step 266: training accuarcy: 0.9995\n",
      "Epoch 2 step 266: training loss: 211.4756778135551\n",
      "Epoch 2 step 267: training accuarcy: 0.999\n",
      "Epoch 2 step 267: training loss: 210.37154508192643\n",
      "Epoch 2 step 268: training accuarcy: 0.999\n",
      "Epoch 2 step 268: training loss: 210.30897886773326\n",
      "Epoch 2 step 269: training accuarcy: 0.9985\n",
      "Epoch 2 step 269: training loss: 207.33774211231923\n",
      "Epoch 2 step 270: training accuarcy: 0.9985\n",
      "Epoch 2 step 270: training loss: 209.27821923926862\n",
      "Epoch 2 step 271: training accuarcy: 0.999\n",
      "Epoch 2 step 271: training loss: 205.7997205399559\n",
      "Epoch 2 step 272: training accuarcy: 0.999\n",
      "Epoch 2 step 272: training loss: 204.13866983593869\n",
      "Epoch 2 step 273: training accuarcy: 0.9995\n",
      "Epoch 2 step 273: training loss: 204.19972097277986\n",
      "Epoch 2 step 274: training accuarcy: 0.999\n",
      "Epoch 2 step 274: training loss: 201.47706180023613\n",
      "Epoch 2 step 275: training accuarcy: 0.999\n",
      "Epoch 2 step 275: training loss: 200.36937762042007\n",
      "Epoch 2 step 276: training accuarcy: 1.0\n",
      "Epoch 2 step 276: training loss: 199.76868948373624\n",
      "Epoch 2 step 277: training accuarcy: 0.9995\n",
      "Epoch 2 step 277: training loss: 197.5578795104084\n",
      "Epoch 2 step 278: training accuarcy: 1.0\n",
      "Epoch 2 step 278: training loss: 197.73344654525476\n",
      "Epoch 2 step 279: training accuarcy: 0.9985\n",
      "Epoch 2 step 279: training loss: 194.62061074051715\n",
      "Epoch 2 step 280: training accuarcy: 0.9995\n",
      "Epoch 2 step 280: training loss: 194.81493791759974\n",
      "Epoch 2 step 281: training accuarcy: 0.9985\n",
      "Epoch 2 step 281: training loss: 195.05002381990658\n",
      "Epoch 2 step 282: training accuarcy: 0.9985\n",
      "Epoch 2 step 282: training loss: 191.53684544586304\n",
      "Epoch 2 step 283: training accuarcy: 1.0\n",
      "Epoch 2 step 283: training loss: 191.25895983537976\n",
      "Epoch 2 step 284: training accuarcy: 1.0\n",
      "Epoch 2 step 284: training loss: 191.14556517339432\n",
      "Epoch 2 step 285: training accuarcy: 0.999\n",
      "Epoch 2 step 285: training loss: 189.3637534025892\n",
      "Epoch 2 step 286: training accuarcy: 0.999\n",
      "Epoch 2 step 286: training loss: 190.55517751862763\n",
      "Epoch 2 step 287: training accuarcy: 0.999\n",
      "Epoch 2 step 287: training loss: 185.3199611982417\n",
      "Epoch 2 step 288: training accuarcy: 1.0\n",
      "Epoch 2 step 288: training loss: 184.39302365281534\n",
      "Epoch 2 step 289: training accuarcy: 1.0\n",
      "Epoch 2 step 289: training loss: 185.4545174122496\n",
      "Epoch 2 step 290: training accuarcy: 0.9995\n",
      "Epoch 2 step 290: training loss: 185.5065484350936\n",
      "Epoch 2 step 291: training accuarcy: 0.998\n",
      "Epoch 2 step 291: training loss: 184.1781072570576\n",
      "Epoch 2 step 292: training accuarcy: 0.9995\n",
      "Epoch 2 step 292: training loss: 183.01769791127472\n",
      "Epoch 2 step 293: training accuarcy: 0.999\n",
      "Epoch 2 step 293: training loss: 181.61086956580834\n",
      "Epoch 2 step 294: training accuarcy: 1.0\n",
      "Epoch 2 step 294: training loss: 178.5280287359895\n",
      "Epoch 2 step 295: training accuarcy: 0.9995\n",
      "Epoch 2 step 295: training loss: 180.5028995942726\n",
      "Epoch 2 step 296: training accuarcy: 0.999\n",
      "Epoch 2 step 296: training loss: 179.38275019855797\n",
      "Epoch 2 step 297: training accuarcy: 0.999\n",
      "Epoch 2 step 297: training loss: 177.929434355022\n",
      "Epoch 2 step 298: training accuarcy: 0.999\n",
      "Epoch 2 step 298: training loss: 176.87497000870616\n",
      "Epoch 2 step 299: training accuarcy: 0.999\n",
      "Epoch 2 step 299: training loss: 176.55878260726746\n",
      "Epoch 2 step 300: training accuarcy: 0.9995\n",
      "Epoch 2 step 300: training loss: 175.04752006096317\n",
      "Epoch 2 step 301: training accuarcy: 0.999\n",
      "Epoch 2 step 301: training loss: 174.1315467400951\n",
      "Epoch 2 step 302: training accuarcy: 0.9995\n",
      "Epoch 2 step 302: training loss: 171.41986091308647\n",
      "Epoch 2 step 303: training accuarcy: 0.9995\n",
      "Epoch 2 step 303: training loss: 173.1139106585492\n",
      "Epoch 2 step 304: training accuarcy: 0.9995\n",
      "Epoch 2 step 304: training loss: 169.9289596249968\n",
      "Epoch 2 step 305: training accuarcy: 1.0\n",
      "Epoch 2 step 305: training loss: 173.2449120403406\n",
      "Epoch 2 step 306: training accuarcy: 0.9985\n",
      "Epoch 2 step 306: training loss: 170.53812496335945\n",
      "Epoch 2 step 307: training accuarcy: 0.999\n",
      "Epoch 2 step 307: training loss: 167.66855920562855\n",
      "Epoch 2 step 308: training accuarcy: 0.9995\n",
      "Epoch 2 step 308: training loss: 169.69023785977376\n",
      "Epoch 2 step 309: training accuarcy: 0.9985\n",
      "Epoch 2 step 309: training loss: 166.8111422726769\n",
      "Epoch 2 step 310: training accuarcy: 0.9995\n",
      "Epoch 2 step 310: training loss: 163.3788029104265\n",
      "Epoch 2 step 311: training accuarcy: 1.0\n",
      "Epoch 2 step 311: training loss: 164.8517467442124\n",
      "Epoch 2 step 312: training accuarcy: 0.999\n",
      "Epoch 2 step 312: training loss: 162.59956086631163\n",
      "Epoch 2 step 313: training accuarcy: 0.9995\n",
      "Epoch 2 step 313: training loss: 161.10367672820243\n",
      "Epoch 2 step 314: training accuarcy: 1.0\n",
      "Epoch 2 step 314: training loss: 160.8312846813406\n",
      "Epoch 2 step 315: training accuarcy: 1.0\n",
      "Epoch 2 step 315: training loss: 161.26069888709006\n",
      "Epoch 2 step 316: training accuarcy: 0.9995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 316: training loss: 159.68924025762607\n",
      "Epoch 2 step 317: training accuarcy: 1.0\n",
      "Epoch 2 step 317: training loss: 158.0665082446616\n",
      "Epoch 2 step 318: training accuarcy: 1.0\n",
      "Epoch 2 step 318: training loss: 159.03083164284473\n",
      "Epoch 2 step 319: training accuarcy: 0.999\n",
      "Epoch 2 step 319: training loss: 159.94201393622606\n",
      "Epoch 2 step 320: training accuarcy: 0.999\n",
      "Epoch 2 step 320: training loss: 157.85410310241255\n",
      "Epoch 2 step 321: training accuarcy: 0.999\n",
      "Epoch 2 step 321: training loss: 157.4545943994408\n",
      "Epoch 2 step 322: training accuarcy: 0.9995\n",
      "Epoch 2 step 322: training loss: 155.99210712576738\n",
      "Epoch 2 step 323: training accuarcy: 0.9985\n",
      "Epoch 2 step 323: training loss: 155.83942984821522\n",
      "Epoch 2 step 324: training accuarcy: 0.999\n",
      "Epoch 2 step 324: training loss: 153.8080531855861\n",
      "Epoch 2 step 325: training accuarcy: 0.999\n",
      "Epoch 2 step 325: training loss: 152.49084835122738\n",
      "Epoch 2 step 326: training accuarcy: 0.9995\n",
      "Epoch 2 step 326: training loss: 151.46209831190396\n",
      "Epoch 2 step 327: training accuarcy: 1.0\n",
      "Epoch 2 step 327: training loss: 149.14569933325964\n",
      "Epoch 2 step 328: training accuarcy: 1.0\n",
      "Epoch 2 step 328: training loss: 149.47076404918263\n",
      "Epoch 2 step 329: training accuarcy: 1.0\n",
      "Epoch 2 step 329: training loss: 149.75404509640617\n",
      "Epoch 2 step 330: training accuarcy: 0.999\n",
      "Epoch 2 step 330: training loss: 149.14447119994878\n",
      "Epoch 2 step 331: training accuarcy: 0.9995\n",
      "Epoch 2 step 331: training loss: 148.57399516854267\n",
      "Epoch 2 step 332: training accuarcy: 0.9995\n",
      "Epoch 2 step 332: training loss: 150.7973380231487\n",
      "Epoch 2 step 333: training accuarcy: 0.9985\n",
      "Epoch 2 step 333: training loss: 151.96222786810924\n",
      "Epoch 2 step 334: training accuarcy: 0.997\n",
      "Epoch 2 step 334: training loss: 146.63336539771686\n",
      "Epoch 2 step 335: training accuarcy: 0.999\n",
      "Epoch 2 step 335: training loss: 146.39040413452835\n",
      "Epoch 2 step 336: training accuarcy: 0.9995\n",
      "Epoch 2 step 336: training loss: 144.03131118015764\n",
      "Epoch 2 step 337: training accuarcy: 1.0\n",
      "Epoch 2 step 337: training loss: 143.7259642435989\n",
      "Epoch 2 step 338: training accuarcy: 0.9995\n",
      "Epoch 2 step 338: training loss: 143.6032903763037\n",
      "Epoch 2 step 339: training accuarcy: 0.999\n",
      "Epoch 2 step 339: training loss: 142.01809073318478\n",
      "Epoch 2 step 340: training accuarcy: 1.0\n",
      "Epoch 2 step 340: training loss: 141.6088308548231\n",
      "Epoch 2 step 341: training accuarcy: 1.0\n",
      "Epoch 2 step 341: training loss: 145.02581043385214\n",
      "Epoch 2 step 342: training accuarcy: 0.998\n",
      "Epoch 2 step 342: training loss: 141.2129214938623\n",
      "Epoch 2 step 343: training accuarcy: 0.999\n",
      "Epoch 2 step 343: training loss: 142.62197128684997\n",
      "Epoch 2 step 344: training accuarcy: 0.999\n",
      "Epoch 2 step 344: training loss: 139.2240973714669\n",
      "Epoch 2 step 345: training accuarcy: 1.0\n",
      "Epoch 2: train loss 191.94619933261626, train accuarcy 0.9990841746330261\n",
      "Epoch 2: valid loss 139.25741907710784, valid accuarcy 0.9992349743843079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████▍                                                                                               | 3/8 [02:30<04:12, 50.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 345: training loss: 139.66003968549992\n",
      "Epoch 3 step 346: training accuarcy: 0.999\n",
      "Epoch 3 step 346: training loss: 137.89372944793254\n",
      "Epoch 3 step 347: training accuarcy: 0.9995\n",
      "Epoch 3 step 347: training loss: 137.59315131835572\n",
      "Epoch 3 step 348: training accuarcy: 0.9995\n",
      "Epoch 3 step 348: training loss: 137.35558036705154\n",
      "Epoch 3 step 349: training accuarcy: 0.9995\n",
      "Epoch 3 step 349: training loss: 135.80308818879638\n",
      "Epoch 3 step 350: training accuarcy: 0.9995\n",
      "Epoch 3 step 350: training loss: 134.6452889809503\n",
      "Epoch 3 step 351: training accuarcy: 1.0\n",
      "Epoch 3 step 351: training loss: 135.00634870510598\n",
      "Epoch 3 step 352: training accuarcy: 1.0\n",
      "Epoch 3 step 352: training loss: 132.0638073334307\n",
      "Epoch 3 step 353: training accuarcy: 1.0\n",
      "Epoch 3 step 353: training loss: 132.51957301097136\n",
      "Epoch 3 step 354: training accuarcy: 1.0\n",
      "Epoch 3 step 354: training loss: 132.2340435391871\n",
      "Epoch 3 step 355: training accuarcy: 0.9995\n",
      "Epoch 3 step 355: training loss: 132.15135182804727\n",
      "Epoch 3 step 356: training accuarcy: 1.0\n",
      "Epoch 3 step 356: training loss: 130.5228403622907\n",
      "Epoch 3 step 357: training accuarcy: 0.9995\n",
      "Epoch 3 step 357: training loss: 132.38481939786166\n",
      "Epoch 3 step 358: training accuarcy: 0.9985\n",
      "Epoch 3 step 358: training loss: 131.55042270151012\n",
      "Epoch 3 step 359: training accuarcy: 1.0\n",
      "Epoch 3 step 359: training loss: 128.66228556897997\n",
      "Epoch 3 step 360: training accuarcy: 1.0\n",
      "Epoch 3 step 360: training loss: 130.51062881080927\n",
      "Epoch 3 step 361: training accuarcy: 0.999\n",
      "Epoch 3 step 361: training loss: 127.90493281845026\n",
      "Epoch 3 step 362: training accuarcy: 0.9995\n",
      "Epoch 3 step 362: training loss: 126.79204115868279\n",
      "Epoch 3 step 363: training accuarcy: 1.0\n",
      "Epoch 3 step 363: training loss: 128.13828299502617\n",
      "Epoch 3 step 364: training accuarcy: 1.0\n",
      "Epoch 3 step 364: training loss: 127.57254697953951\n",
      "Epoch 3 step 365: training accuarcy: 0.999\n",
      "Epoch 3 step 365: training loss: 126.79995014034193\n",
      "Epoch 3 step 366: training accuarcy: 0.9995\n",
      "Epoch 3 step 366: training loss: 128.07941942026915\n",
      "Epoch 3 step 367: training accuarcy: 0.9985\n",
      "Epoch 3 step 367: training loss: 125.4586745699296\n",
      "Epoch 3 step 368: training accuarcy: 1.0\n",
      "Epoch 3 step 368: training loss: 124.77443506159463\n",
      "Epoch 3 step 369: training accuarcy: 0.999\n",
      "Epoch 3 step 369: training loss: 126.78463676965622\n",
      "Epoch 3 step 370: training accuarcy: 0.999\n",
      "Epoch 3 step 370: training loss: 124.52378166711875\n",
      "Epoch 3 step 371: training accuarcy: 0.9985\n",
      "Epoch 3 step 371: training loss: 123.21313957364475\n",
      "Epoch 3 step 372: training accuarcy: 0.9995\n",
      "Epoch 3 step 372: training loss: 121.95167072654769\n",
      "Epoch 3 step 373: training accuarcy: 1.0\n",
      "Epoch 3 step 373: training loss: 123.69222693409115\n",
      "Epoch 3 step 374: training accuarcy: 0.998\n",
      "Epoch 3 step 374: training loss: 119.59070905018203\n",
      "Epoch 3 step 375: training accuarcy: 1.0\n",
      "Epoch 3 step 375: training loss: 121.66314655492677\n",
      "Epoch 3 step 376: training accuarcy: 0.9995\n",
      "Epoch 3 step 376: training loss: 120.40447962519141\n",
      "Epoch 3 step 377: training accuarcy: 0.9985\n",
      "Epoch 3 step 377: training loss: 122.27064012203203\n",
      "Epoch 3 step 378: training accuarcy: 0.999\n",
      "Epoch 3 step 378: training loss: 120.27259544311495\n",
      "Epoch 3 step 379: training accuarcy: 0.999\n",
      "Epoch 3 step 379: training loss: 119.80940010943787\n",
      "Epoch 3 step 380: training accuarcy: 0.999\n",
      "Epoch 3 step 380: training loss: 117.75861651025564\n",
      "Epoch 3 step 381: training accuarcy: 0.999\n",
      "Epoch 3 step 381: training loss: 119.618998521017\n",
      "Epoch 3 step 382: training accuarcy: 0.999\n",
      "Epoch 3 step 382: training loss: 116.12674687451015\n",
      "Epoch 3 step 383: training accuarcy: 1.0\n",
      "Epoch 3 step 383: training loss: 115.53508123116929\n",
      "Epoch 3 step 384: training accuarcy: 0.9995\n",
      "Epoch 3 step 384: training loss: 117.09690063320208\n",
      "Epoch 3 step 385: training accuarcy: 0.999\n",
      "Epoch 3 step 385: training loss: 116.99177093590211\n",
      "Epoch 3 step 386: training accuarcy: 0.999\n",
      "Epoch 3 step 386: training loss: 114.88278964035244\n",
      "Epoch 3 step 387: training accuarcy: 0.9995\n",
      "Epoch 3 step 387: training loss: 113.67829004821678\n",
      "Epoch 3 step 388: training accuarcy: 1.0\n",
      "Epoch 3 step 388: training loss: 113.12798913584348\n",
      "Epoch 3 step 389: training accuarcy: 1.0\n",
      "Epoch 3 step 389: training loss: 113.22040997634748\n",
      "Epoch 3 step 390: training accuarcy: 0.9995\n",
      "Epoch 3 step 390: training loss: 112.84094332427436\n",
      "Epoch 3 step 391: training accuarcy: 0.9995\n",
      "Epoch 3 step 391: training loss: 113.8453177268809\n",
      "Epoch 3 step 392: training accuarcy: 0.9985\n",
      "Epoch 3 step 392: training loss: 110.90274328480035\n",
      "Epoch 3 step 393: training accuarcy: 1.0\n",
      "Epoch 3 step 393: training loss: 110.13030736664669\n",
      "Epoch 3 step 394: training accuarcy: 1.0\n",
      "Epoch 3 step 394: training loss: 109.85114199347625\n",
      "Epoch 3 step 395: training accuarcy: 1.0\n",
      "Epoch 3 step 395: training loss: 109.48776676283703\n",
      "Epoch 3 step 396: training accuarcy: 1.0\n",
      "Epoch 3 step 396: training loss: 110.52358182584076\n",
      "Epoch 3 step 397: training accuarcy: 1.0\n",
      "Epoch 3 step 397: training loss: 108.99245823899113\n",
      "Epoch 3 step 398: training accuarcy: 1.0\n",
      "Epoch 3 step 398: training loss: 109.56022069023727\n",
      "Epoch 3 step 399: training accuarcy: 0.9995\n",
      "Epoch 3 step 399: training loss: 109.11893366796156\n",
      "Epoch 3 step 400: training accuarcy: 0.9995\n",
      "Epoch 3 step 400: training loss: 107.57466795239719\n",
      "Epoch 3 step 401: training accuarcy: 1.0\n",
      "Epoch 3 step 401: training loss: 108.64235796032041\n",
      "Epoch 3 step 402: training accuarcy: 0.999\n",
      "Epoch 3 step 402: training loss: 107.26785953176301\n",
      "Epoch 3 step 403: training accuarcy: 0.9995\n",
      "Epoch 3 step 403: training loss: 105.00701518671166\n",
      "Epoch 3 step 404: training accuarcy: 1.0\n",
      "Epoch 3 step 404: training loss: 108.11802537325025\n",
      "Epoch 3 step 405: training accuarcy: 0.999\n",
      "Epoch 3 step 405: training loss: 107.29881544739938\n",
      "Epoch 3 step 406: training accuarcy: 0.999\n",
      "Epoch 3 step 406: training loss: 106.8766068713882\n",
      "Epoch 3 step 407: training accuarcy: 0.9985\n",
      "Epoch 3 step 407: training loss: 106.12372298479485\n",
      "Epoch 3 step 408: training accuarcy: 0.9995\n",
      "Epoch 3 step 408: training loss: 105.55450441026544\n",
      "Epoch 3 step 409: training accuarcy: 0.999\n",
      "Epoch 3 step 409: training loss: 104.10855599280136\n",
      "Epoch 3 step 410: training accuarcy: 0.9995\n",
      "Epoch 3 step 410: training loss: 104.216369368901\n",
      "Epoch 3 step 411: training accuarcy: 0.9995\n",
      "Epoch 3 step 411: training loss: 103.62921850738637\n",
      "Epoch 3 step 412: training accuarcy: 0.999\n",
      "Epoch 3 step 412: training loss: 101.68420689059549\n",
      "Epoch 3 step 413: training accuarcy: 1.0\n",
      "Epoch 3 step 413: training loss: 101.65072301237673\n",
      "Epoch 3 step 414: training accuarcy: 1.0\n",
      "Epoch 3 step 414: training loss: 102.3721415601039\n",
      "Epoch 3 step 415: training accuarcy: 0.9995\n",
      "Epoch 3 step 415: training loss: 101.34128178697456\n",
      "Epoch 3 step 416: training accuarcy: 0.9995\n",
      "Epoch 3 step 416: training loss: 101.24710013647362\n",
      "Epoch 3 step 417: training accuarcy: 0.9995\n",
      "Epoch 3 step 417: training loss: 99.64722395947373\n",
      "Epoch 3 step 418: training accuarcy: 1.0\n",
      "Epoch 3 step 418: training loss: 100.902069247719\n",
      "Epoch 3 step 419: training accuarcy: 0.9995\n",
      "Epoch 3 step 419: training loss: 99.77583748865595\n",
      "Epoch 3 step 420: training accuarcy: 1.0\n",
      "Epoch 3 step 420: training loss: 99.82176856579916\n",
      "Epoch 3 step 421: training accuarcy: 0.999\n",
      "Epoch 3 step 421: training loss: 100.54769637559617\n",
      "Epoch 3 step 422: training accuarcy: 0.9995\n",
      "Epoch 3 step 422: training loss: 97.37608402913833\n",
      "Epoch 3 step 423: training accuarcy: 1.0\n",
      "Epoch 3 step 423: training loss: 99.49761218520504\n",
      "Epoch 3 step 424: training accuarcy: 0.999\n",
      "Epoch 3 step 424: training loss: 98.81872282582223\n",
      "Epoch 3 step 425: training accuarcy: 0.9985\n",
      "Epoch 3 step 425: training loss: 97.38592655954942\n",
      "Epoch 3 step 426: training accuarcy: 0.999\n",
      "Epoch 3 step 426: training loss: 97.81564927501532\n",
      "Epoch 3 step 427: training accuarcy: 0.9985\n",
      "Epoch 3 step 427: training loss: 96.54410356719143\n",
      "Epoch 3 step 428: training accuarcy: 0.9995\n",
      "Epoch 3 step 428: training loss: 95.8517343656015\n",
      "Epoch 3 step 429: training accuarcy: 0.9995\n",
      "Epoch 3 step 429: training loss: 95.34372737663477\n",
      "Epoch 3 step 430: training accuarcy: 0.9995\n",
      "Epoch 3 step 430: training loss: 95.53565899377689\n",
      "Epoch 3 step 431: training accuarcy: 0.999\n",
      "Epoch 3 step 431: training loss: 93.77116160765812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 432: training accuarcy: 1.0\n",
      "Epoch 3 step 432: training loss: 94.97589889662352\n",
      "Epoch 3 step 433: training accuarcy: 0.999\n",
      "Epoch 3 step 433: training loss: 97.98545148042321\n",
      "Epoch 3 step 434: training accuarcy: 0.999\n",
      "Epoch 3 step 434: training loss: 96.41923686963673\n",
      "Epoch 3 step 435: training accuarcy: 0.9995\n",
      "Epoch 3 step 435: training loss: 93.5251051777447\n",
      "Epoch 3 step 436: training accuarcy: 1.0\n",
      "Epoch 3 step 436: training loss: 92.32148214636855\n",
      "Epoch 3 step 437: training accuarcy: 1.0\n",
      "Epoch 3 step 437: training loss: 92.15902374383131\n",
      "Epoch 3 step 438: training accuarcy: 0.9995\n",
      "Epoch 3 step 438: training loss: 93.35481454432481\n",
      "Epoch 3 step 439: training accuarcy: 1.0\n",
      "Epoch 3 step 439: training loss: 93.97894163569482\n",
      "Epoch 3 step 440: training accuarcy: 0.999\n",
      "Epoch 3 step 440: training loss: 91.72984329322448\n",
      "Epoch 3 step 441: training accuarcy: 0.999\n",
      "Epoch 3 step 441: training loss: 90.2198978332915\n",
      "Epoch 3 step 442: training accuarcy: 1.0\n",
      "Epoch 3 step 442: training loss: 90.36365543465011\n",
      "Epoch 3 step 443: training accuarcy: 1.0\n",
      "Epoch 3 step 443: training loss: 90.8679589629903\n",
      "Epoch 3 step 444: training accuarcy: 0.999\n",
      "Epoch 3 step 444: training loss: 91.14670732326688\n",
      "Epoch 3 step 445: training accuarcy: 0.999\n",
      "Epoch 3 step 445: training loss: 90.28504924178975\n",
      "Epoch 3 step 446: training accuarcy: 1.0\n",
      "Epoch 3 step 446: training loss: 91.09246445603182\n",
      "Epoch 3 step 447: training accuarcy: 0.9995\n",
      "Epoch 3 step 447: training loss: 88.85599932290327\n",
      "Epoch 3 step 448: training accuarcy: 0.9995\n",
      "Epoch 3 step 448: training loss: 88.63140291311927\n",
      "Epoch 3 step 449: training accuarcy: 1.0\n",
      "Epoch 3 step 449: training loss: 89.98756006192724\n",
      "Epoch 3 step 450: training accuarcy: 0.999\n",
      "Epoch 3 step 450: training loss: 88.75025369431317\n",
      "Epoch 3 step 451: training accuarcy: 0.999\n",
      "Epoch 3 step 451: training loss: 87.43102311797897\n",
      "Epoch 3 step 452: training accuarcy: 0.9995\n",
      "Epoch 3 step 452: training loss: 87.70032071437595\n",
      "Epoch 3 step 453: training accuarcy: 0.9995\n",
      "Epoch 3 step 453: training loss: 87.53465008884962\n",
      "Epoch 3 step 454: training accuarcy: 0.9995\n",
      "Epoch 3 step 454: training loss: 86.4644440762535\n",
      "Epoch 3 step 455: training accuarcy: 0.9995\n",
      "Epoch 3 step 455: training loss: 85.83731194944608\n",
      "Epoch 3 step 456: training accuarcy: 1.0\n",
      "Epoch 3 step 456: training loss: 86.0304455536291\n",
      "Epoch 3 step 457: training accuarcy: 0.9995\n",
      "Epoch 3 step 457: training loss: 85.04397959120915\n",
      "Epoch 3 step 458: training accuarcy: 1.0\n",
      "Epoch 3 step 458: training loss: 85.93143656579662\n",
      "Epoch 3 step 459: training accuarcy: 0.9995\n",
      "Epoch 3 step 459: training loss: 84.24926128871914\n",
      "Epoch 3 step 460: training accuarcy: 1.0\n",
      "Epoch 3: train loss 109.02405640635193, train accuarcy 0.9992923736572266\n",
      "Epoch 3: valid loss 85.85679385396804, valid accuarcy 0.999489963054657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 4/8 [03:18<03:19, 49.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 460: training loss: 86.4993586036428\n",
      "Epoch 4 step 461: training accuarcy: 0.9995\n",
      "Epoch 4 step 461: training loss: 86.92593381124078\n",
      "Epoch 4 step 462: training accuarcy: 0.9995\n",
      "Epoch 4 step 462: training loss: 83.73118545790784\n",
      "Epoch 4 step 463: training accuarcy: 1.0\n",
      "Epoch 4 step 463: training loss: 84.23778823962473\n",
      "Epoch 4 step 464: training accuarcy: 0.9995\n",
      "Epoch 4 step 464: training loss: 84.70309171352038\n",
      "Epoch 4 step 465: training accuarcy: 0.999\n",
      "Epoch 4 step 465: training loss: 86.21460871971861\n",
      "Epoch 4 step 466: training accuarcy: 0.9985\n",
      "Epoch 4 step 466: training loss: 81.73991174730767\n",
      "Epoch 4 step 467: training accuarcy: 1.0\n",
      "Epoch 4 step 467: training loss: 82.99058843736499\n",
      "Epoch 4 step 468: training accuarcy: 0.9995\n",
      "Epoch 4 step 468: training loss: 83.21893147803074\n",
      "Epoch 4 step 469: training accuarcy: 0.9995\n",
      "Epoch 4 step 469: training loss: 83.23204798830147\n",
      "Epoch 4 step 470: training accuarcy: 0.9985\n",
      "Epoch 4 step 470: training loss: 85.46115173910266\n",
      "Epoch 4 step 471: training accuarcy: 0.9985\n",
      "Epoch 4 step 471: training loss: 84.2011792175107\n",
      "Epoch 4 step 472: training accuarcy: 0.9985\n",
      "Epoch 4 step 472: training loss: 83.31300465715165\n",
      "Epoch 4 step 473: training accuarcy: 0.9985\n",
      "Epoch 4 step 473: training loss: 82.45123925943699\n",
      "Epoch 4 step 474: training accuarcy: 0.9995\n",
      "Epoch 4 step 474: training loss: 79.99496755517085\n",
      "Epoch 4 step 475: training accuarcy: 0.9995\n",
      "Epoch 4 step 475: training loss: 80.32742776037576\n",
      "Epoch 4 step 476: training accuarcy: 1.0\n",
      "Epoch 4 step 476: training loss: 81.64832719086712\n",
      "Epoch 4 step 477: training accuarcy: 0.999\n",
      "Epoch 4 step 477: training loss: 79.20681119890854\n",
      "Epoch 4 step 478: training accuarcy: 0.9995\n",
      "Epoch 4 step 478: training loss: 79.8745164852077\n",
      "Epoch 4 step 479: training accuarcy: 0.9995\n",
      "Epoch 4 step 479: training loss: 79.7120503423358\n",
      "Epoch 4 step 480: training accuarcy: 0.9995\n",
      "Epoch 4 step 480: training loss: 79.18102620076087\n",
      "Epoch 4 step 481: training accuarcy: 0.9995\n",
      "Epoch 4 step 481: training loss: 77.87496805342701\n",
      "Epoch 4 step 482: training accuarcy: 1.0\n",
      "Epoch 4 step 482: training loss: 80.88931255293801\n",
      "Epoch 4 step 483: training accuarcy: 0.999\n",
      "Epoch 4 step 483: training loss: 79.75762514961046\n",
      "Epoch 4 step 484: training accuarcy: 0.9995\n",
      "Epoch 4 step 484: training loss: 77.42989483917273\n",
      "Epoch 4 step 485: training accuarcy: 0.9995\n",
      "Epoch 4 step 485: training loss: 77.15504242621097\n",
      "Epoch 4 step 486: training accuarcy: 1.0\n",
      "Epoch 4 step 486: training loss: 77.17077134482699\n",
      "Epoch 4 step 487: training accuarcy: 0.9995\n",
      "Epoch 4 step 487: training loss: 79.8326363670569\n",
      "Epoch 4 step 488: training accuarcy: 0.999\n",
      "Epoch 4 step 488: training loss: 76.09507516750624\n",
      "Epoch 4 step 489: training accuarcy: 0.9995\n",
      "Epoch 4 step 489: training loss: 76.58038651169164\n",
      "Epoch 4 step 490: training accuarcy: 1.0\n",
      "Epoch 4 step 490: training loss: 76.92897309022257\n",
      "Epoch 4 step 491: training accuarcy: 1.0\n",
      "Epoch 4 step 491: training loss: 75.3786460721399\n",
      "Epoch 4 step 492: training accuarcy: 1.0\n",
      "Epoch 4 step 492: training loss: 77.37549296917946\n",
      "Epoch 4 step 493: training accuarcy: 0.9995\n",
      "Epoch 4 step 493: training loss: 77.21493105644878\n",
      "Epoch 4 step 494: training accuarcy: 0.9985\n",
      "Epoch 4 step 494: training loss: 75.3665590953375\n",
      "Epoch 4 step 495: training accuarcy: 1.0\n",
      "Epoch 4 step 495: training loss: 75.61936282465888\n",
      "Epoch 4 step 496: training accuarcy: 0.999\n",
      "Epoch 4 step 496: training loss: 74.79824989120704\n",
      "Epoch 4 step 497: training accuarcy: 0.9995\n",
      "Epoch 4 step 497: training loss: 75.88176337111229\n",
      "Epoch 4 step 498: training accuarcy: 0.9995\n",
      "Epoch 4 step 498: training loss: 74.98326143491357\n",
      "Epoch 4 step 499: training accuarcy: 0.9995\n",
      "Epoch 4 step 499: training loss: 75.2954393983029\n",
      "Epoch 4 step 500: training accuarcy: 0.9985\n",
      "Epoch 4 step 500: training loss: 75.27398448070116\n",
      "Epoch 4 step 501: training accuarcy: 1.0\n",
      "Epoch 4 step 501: training loss: 73.97926841383332\n",
      "Epoch 4 step 502: training accuarcy: 0.9995\n",
      "Epoch 4 step 502: training loss: 74.41467053626974\n",
      "Epoch 4 step 503: training accuarcy: 0.999\n",
      "Epoch 4 step 503: training loss: 73.17786742852876\n",
      "Epoch 4 step 504: training accuarcy: 1.0\n",
      "Epoch 4 step 504: training loss: 73.8344145091636\n",
      "Epoch 4 step 505: training accuarcy: 0.999\n",
      "Epoch 4 step 505: training loss: 73.35225821329628\n",
      "Epoch 4 step 506: training accuarcy: 1.0\n",
      "Epoch 4 step 506: training loss: 73.75280087473406\n",
      "Epoch 4 step 507: training accuarcy: 0.9995\n",
      "Epoch 4 step 507: training loss: 73.16330086808013\n",
      "Epoch 4 step 508: training accuarcy: 1.0\n",
      "Epoch 4 step 508: training loss: 71.70950519213606\n",
      "Epoch 4 step 509: training accuarcy: 1.0\n",
      "Epoch 4 step 509: training loss: 71.751669551293\n",
      "Epoch 4 step 510: training accuarcy: 1.0\n",
      "Epoch 4 step 510: training loss: 72.55717744574193\n",
      "Epoch 4 step 511: training accuarcy: 0.999\n",
      "Epoch 4 step 511: training loss: 71.16017042945819\n",
      "Epoch 4 step 512: training accuarcy: 1.0\n",
      "Epoch 4 step 512: training loss: 71.40891740711662\n",
      "Epoch 4 step 513: training accuarcy: 0.9995\n",
      "Epoch 4 step 513: training loss: 71.81485618394004\n",
      "Epoch 4 step 514: training accuarcy: 0.9995\n",
      "Epoch 4 step 514: training loss: 71.75018331175917\n",
      "Epoch 4 step 515: training accuarcy: 0.9995\n",
      "Epoch 4 step 515: training loss: 74.63005686506982\n",
      "Epoch 4 step 516: training accuarcy: 0.9975\n",
      "Epoch 4 step 516: training loss: 69.04221325168191\n",
      "Epoch 4 step 517: training accuarcy: 0.9995\n",
      "Epoch 4 step 517: training loss: 73.3569248395091\n",
      "Epoch 4 step 518: training accuarcy: 0.9985\n",
      "Epoch 4 step 518: training loss: 69.09680303391279\n",
      "Epoch 4 step 519: training accuarcy: 0.9995\n",
      "Epoch 4 step 519: training loss: 68.73506046574094\n",
      "Epoch 4 step 520: training accuarcy: 1.0\n",
      "Epoch 4 step 520: training loss: 67.79955038777916\n",
      "Epoch 4 step 521: training accuarcy: 1.0\n",
      "Epoch 4 step 521: training loss: 71.32573531869694\n",
      "Epoch 4 step 522: training accuarcy: 0.9985\n",
      "Epoch 4 step 522: training loss: 70.10576238134398\n",
      "Epoch 4 step 523: training accuarcy: 0.999\n",
      "Epoch 4 step 523: training loss: 68.36552669299755\n",
      "Epoch 4 step 524: training accuarcy: 0.9995\n",
      "Epoch 4 step 524: training loss: 68.78826088105552\n",
      "Epoch 4 step 525: training accuarcy: 0.999\n",
      "Epoch 4 step 525: training loss: 68.05561601940863\n",
      "Epoch 4 step 526: training accuarcy: 0.9995\n",
      "Epoch 4 step 526: training loss: 68.99523780915598\n",
      "Epoch 4 step 527: training accuarcy: 1.0\n",
      "Epoch 4 step 527: training loss: 69.2344850886748\n",
      "Epoch 4 step 528: training accuarcy: 0.999\n",
      "Epoch 4 step 528: training loss: 69.591484136971\n",
      "Epoch 4 step 529: training accuarcy: 0.9985\n",
      "Epoch 4 step 529: training loss: 68.66710652683986\n",
      "Epoch 4 step 530: training accuarcy: 0.999\n",
      "Epoch 4 step 530: training loss: 68.31945638819057\n",
      "Epoch 4 step 531: training accuarcy: 0.9995\n",
      "Epoch 4 step 531: training loss: 68.05374152313318\n",
      "Epoch 4 step 532: training accuarcy: 0.9985\n",
      "Epoch 4 step 532: training loss: 66.20660607970899\n",
      "Epoch 4 step 533: training accuarcy: 1.0\n",
      "Epoch 4 step 533: training loss: 67.18601372316515\n",
      "Epoch 4 step 534: training accuarcy: 0.9995\n",
      "Epoch 4 step 534: training loss: 65.39345185556613\n",
      "Epoch 4 step 535: training accuarcy: 0.9995\n",
      "Epoch 4 step 535: training loss: 69.02728645669168\n",
      "Epoch 4 step 536: training accuarcy: 0.999\n",
      "Epoch 4 step 536: training loss: 67.94716139845784\n",
      "Epoch 4 step 537: training accuarcy: 0.9995\n",
      "Epoch 4 step 537: training loss: 65.21821824592035\n",
      "Epoch 4 step 538: training accuarcy: 0.9995\n",
      "Epoch 4 step 538: training loss: 68.6154519424676\n",
      "Epoch 4 step 539: training accuarcy: 0.9985\n",
      "Epoch 4 step 539: training loss: 66.85321178780885\n",
      "Epoch 4 step 540: training accuarcy: 0.999\n",
      "Epoch 4 step 540: training loss: 64.91016791282318\n",
      "Epoch 4 step 541: training accuarcy: 0.9995\n",
      "Epoch 4 step 541: training loss: 66.06568137459227\n",
      "Epoch 4 step 542: training accuarcy: 0.999\n",
      "Epoch 4 step 542: training loss: 64.44600405964401\n",
      "Epoch 4 step 543: training accuarcy: 1.0\n",
      "Epoch 4 step 543: training loss: 66.92146690708005\n",
      "Epoch 4 step 544: training accuarcy: 0.9995\n",
      "Epoch 4 step 544: training loss: 64.91182075859162\n",
      "Epoch 4 step 545: training accuarcy: 1.0\n",
      "Epoch 4 step 545: training loss: 65.32585071232823\n",
      "Epoch 4 step 546: training accuarcy: 0.9995\n",
      "Epoch 4 step 546: training loss: 69.09786530785546\n",
      "Epoch 4 step 547: training accuarcy: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 547: training loss: 63.94596788309927\n",
      "Epoch 4 step 548: training accuarcy: 1.0\n",
      "Epoch 4 step 548: training loss: 63.422287713465536\n",
      "Epoch 4 step 549: training accuarcy: 0.9995\n",
      "Epoch 4 step 549: training loss: 64.9328810181892\n",
      "Epoch 4 step 550: training accuarcy: 0.9985\n",
      "Epoch 4 step 550: training loss: 62.741690318540414\n",
      "Epoch 4 step 551: training accuarcy: 1.0\n",
      "Epoch 4 step 551: training loss: 61.7761532242444\n",
      "Epoch 4 step 552: training accuarcy: 1.0\n",
      "Epoch 4 step 552: training loss: 64.02456080903944\n",
      "Epoch 4 step 553: training accuarcy: 1.0\n",
      "Epoch 4 step 553: training loss: 63.353901630434535\n",
      "Epoch 4 step 554: training accuarcy: 0.9995\n",
      "Epoch 4 step 554: training loss: 61.87360784603142\n",
      "Epoch 4 step 555: training accuarcy: 1.0\n",
      "Epoch 4 step 555: training loss: 63.609855062039706\n",
      "Epoch 4 step 556: training accuarcy: 0.999\n",
      "Epoch 4 step 556: training loss: 61.388646618970256\n",
      "Epoch 4 step 557: training accuarcy: 0.9995\n",
      "Epoch 4 step 557: training loss: 61.59908743416075\n",
      "Epoch 4 step 558: training accuarcy: 1.0\n",
      "Epoch 4 step 558: training loss: 62.82798595796293\n",
      "Epoch 4 step 559: training accuarcy: 0.9985\n",
      "Epoch 4 step 559: training loss: 62.15090379445638\n",
      "Epoch 4 step 560: training accuarcy: 0.9995\n",
      "Epoch 4 step 560: training loss: 62.33368602166869\n",
      "Epoch 4 step 561: training accuarcy: 0.999\n",
      "Epoch 4 step 561: training loss: 61.0344700551123\n",
      "Epoch 4 step 562: training accuarcy: 1.0\n",
      "Epoch 4 step 562: training loss: 60.121424479203604\n",
      "Epoch 4 step 563: training accuarcy: 0.9995\n",
      "Epoch 4 step 563: training loss: 62.19180049607444\n",
      "Epoch 4 step 564: training accuarcy: 0.999\n",
      "Epoch 4 step 564: training loss: 59.53308217630037\n",
      "Epoch 4 step 565: training accuarcy: 1.0\n",
      "Epoch 4 step 565: training loss: 61.801106055093456\n",
      "Epoch 4 step 566: training accuarcy: 0.9985\n",
      "Epoch 4 step 566: training loss: 61.385929352386164\n",
      "Epoch 4 step 567: training accuarcy: 0.999\n",
      "Epoch 4 step 567: training loss: 59.791991910835385\n",
      "Epoch 4 step 568: training accuarcy: 1.0\n",
      "Epoch 4 step 568: training loss: 58.804765715973815\n",
      "Epoch 4 step 569: training accuarcy: 0.9995\n",
      "Epoch 4 step 569: training loss: 61.704815121312734\n",
      "Epoch 4 step 570: training accuarcy: 0.9985\n",
      "Epoch 4 step 570: training loss: 58.9336314845401\n",
      "Epoch 4 step 571: training accuarcy: 1.0\n",
      "Epoch 4 step 571: training loss: 60.03792487616393\n",
      "Epoch 4 step 572: training accuarcy: 0.9995\n",
      "Epoch 4 step 572: training loss: 58.059438251984396\n",
      "Epoch 4 step 573: training accuarcy: 1.0\n",
      "Epoch 4 step 573: training loss: 59.8376247384916\n",
      "Epoch 4 step 574: training accuarcy: 1.0\n",
      "Epoch 4 step 574: training loss: 59.6739570838882\n",
      "Epoch 4 step 575: training accuarcy: 0.999475890985325\n",
      "Epoch 4: train loss 71.27632210024372, train accuarcy 0.9993640184402466\n",
      "Epoch 4: valid loss 59.15735215346575, valid accuarcy 0.9993806481361389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 5/8 [04:06<02:27, 49.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 575: training loss: 58.244890393102814\n",
      "Epoch 5 step 576: training accuarcy: 1.0\n",
      "Epoch 5 step 576: training loss: 60.78276706511589\n",
      "Epoch 5 step 577: training accuarcy: 0.999\n",
      "Epoch 5 step 577: training loss: 62.037989845889086\n",
      "Epoch 5 step 578: training accuarcy: 0.998\n",
      "Epoch 5 step 578: training loss: 58.981827027817424\n",
      "Epoch 5 step 579: training accuarcy: 0.9995\n",
      "Epoch 5 step 579: training loss: 57.820694589410806\n",
      "Epoch 5 step 580: training accuarcy: 0.9995\n",
      "Epoch 5 step 580: training loss: 57.144941333031085\n",
      "Epoch 5 step 581: training accuarcy: 1.0\n",
      "Epoch 5 step 581: training loss: 56.43214692376665\n",
      "Epoch 5 step 582: training accuarcy: 1.0\n",
      "Epoch 5 step 582: training loss: 57.19732261796346\n",
      "Epoch 5 step 583: training accuarcy: 0.9995\n",
      "Epoch 5 step 583: training loss: 58.49935022651574\n",
      "Epoch 5 step 584: training accuarcy: 0.9995\n",
      "Epoch 5 step 584: training loss: 56.543686113836756\n",
      "Epoch 5 step 585: training accuarcy: 1.0\n",
      "Epoch 5 step 585: training loss: 59.23957229039325\n",
      "Epoch 5 step 586: training accuarcy: 0.999\n",
      "Epoch 5 step 586: training loss: 58.639829315870244\n",
      "Epoch 5 step 587: training accuarcy: 0.9985\n",
      "Epoch 5 step 587: training loss: 56.0744380268573\n",
      "Epoch 5 step 588: training accuarcy: 1.0\n",
      "Epoch 5 step 588: training loss: 58.06163838593121\n",
      "Epoch 5 step 589: training accuarcy: 0.9985\n",
      "Epoch 5 step 589: training loss: 56.907676493802285\n",
      "Epoch 5 step 590: training accuarcy: 0.999\n",
      "Epoch 5 step 590: training loss: 56.0733476767546\n",
      "Epoch 5 step 591: training accuarcy: 1.0\n",
      "Epoch 5 step 591: training loss: 56.421575849899206\n",
      "Epoch 5 step 592: training accuarcy: 0.9995\n",
      "Epoch 5 step 592: training loss: 55.505858704577435\n",
      "Epoch 5 step 593: training accuarcy: 0.9995\n",
      "Epoch 5 step 593: training loss: 57.967580154673925\n",
      "Epoch 5 step 594: training accuarcy: 0.9985\n",
      "Epoch 5 step 594: training loss: 55.87044442490633\n",
      "Epoch 5 step 595: training accuarcy: 0.9995\n",
      "Epoch 5 step 595: training loss: 55.91821536049906\n",
      "Epoch 5 step 596: training accuarcy: 0.999\n",
      "Epoch 5 step 596: training loss: 54.88773301663103\n",
      "Epoch 5 step 597: training accuarcy: 0.9995\n",
      "Epoch 5 step 597: training loss: 55.13068211656103\n",
      "Epoch 5 step 598: training accuarcy: 0.9995\n",
      "Epoch 5 step 598: training loss: 54.25702734474102\n",
      "Epoch 5 step 599: training accuarcy: 0.9995\n",
      "Epoch 5 step 599: training loss: 57.79399046587291\n",
      "Epoch 5 step 600: training accuarcy: 0.9985\n",
      "Epoch 5 step 600: training loss: 56.55546193284082\n",
      "Epoch 5 step 601: training accuarcy: 0.999\n",
      "Epoch 5 step 601: training loss: 53.774837527952954\n",
      "Epoch 5 step 602: training accuarcy: 1.0\n",
      "Epoch 5 step 602: training loss: 54.41523649380779\n",
      "Epoch 5 step 603: training accuarcy: 0.9995\n",
      "Epoch 5 step 603: training loss: 55.30794085483006\n",
      "Epoch 5 step 604: training accuarcy: 0.9995\n",
      "Epoch 5 step 604: training loss: 53.633948493354794\n",
      "Epoch 5 step 605: training accuarcy: 0.9995\n",
      "Epoch 5 step 605: training loss: 53.417329103974595\n",
      "Epoch 5 step 606: training accuarcy: 0.9995\n",
      "Epoch 5 step 606: training loss: 54.084630558444836\n",
      "Epoch 5 step 607: training accuarcy: 0.9995\n",
      "Epoch 5 step 607: training loss: 54.007979146476565\n",
      "Epoch 5 step 608: training accuarcy: 1.0\n",
      "Epoch 5 step 608: training loss: 54.29607503922019\n",
      "Epoch 5 step 609: training accuarcy: 0.999\n",
      "Epoch 5 step 609: training loss: 52.82301754262842\n",
      "Epoch 5 step 610: training accuarcy: 0.9995\n",
      "Epoch 5 step 610: training loss: 51.92975314481705\n",
      "Epoch 5 step 611: training accuarcy: 1.0\n",
      "Epoch 5 step 611: training loss: 51.897945539548694\n",
      "Epoch 5 step 612: training accuarcy: 0.9995\n",
      "Epoch 5 step 612: training loss: 54.7625324288278\n",
      "Epoch 5 step 613: training accuarcy: 0.999\n",
      "Epoch 5 step 613: training loss: 51.21207365770507\n",
      "Epoch 5 step 614: training accuarcy: 1.0\n",
      "Epoch 5 step 614: training loss: 53.20971577778358\n",
      "Epoch 5 step 615: training accuarcy: 0.999\n",
      "Epoch 5 step 615: training loss: 53.921811634351606\n",
      "Epoch 5 step 616: training accuarcy: 0.9985\n",
      "Epoch 5 step 616: training loss: 51.26302562628418\n",
      "Epoch 5 step 617: training accuarcy: 1.0\n",
      "Epoch 5 step 617: training loss: 51.3819383250512\n",
      "Epoch 5 step 618: training accuarcy: 1.0\n",
      "Epoch 5 step 618: training loss: 53.18045778836407\n",
      "Epoch 5 step 619: training accuarcy: 0.999\n",
      "Epoch 5 step 619: training loss: 52.10140187761837\n",
      "Epoch 5 step 620: training accuarcy: 0.9995\n",
      "Epoch 5 step 620: training loss: 51.1369390859574\n",
      "Epoch 5 step 621: training accuarcy: 1.0\n",
      "Epoch 5 step 621: training loss: 50.66521742999825\n",
      "Epoch 5 step 622: training accuarcy: 1.0\n",
      "Epoch 5 step 622: training loss: 51.12286350274075\n",
      "Epoch 5 step 623: training accuarcy: 0.9995\n",
      "Epoch 5 step 623: training loss: 50.76707333413447\n",
      "Epoch 5 step 624: training accuarcy: 0.9995\n",
      "Epoch 5 step 624: training loss: 51.241745181025024\n",
      "Epoch 5 step 625: training accuarcy: 0.9995\n",
      "Epoch 5 step 625: training loss: 50.498387240582865\n",
      "Epoch 5 step 626: training accuarcy: 0.9995\n",
      "Epoch 5 step 626: training loss: 50.05836216405666\n",
      "Epoch 5 step 627: training accuarcy: 1.0\n",
      "Epoch 5 step 627: training loss: 50.294162464932185\n",
      "Epoch 5 step 628: training accuarcy: 0.9995\n",
      "Epoch 5 step 628: training loss: 51.80027469079782\n",
      "Epoch 5 step 629: training accuarcy: 0.999\n",
      "Epoch 5 step 629: training loss: 53.42792092933011\n",
      "Epoch 5 step 630: training accuarcy: 0.9985\n",
      "Epoch 5 step 630: training loss: 48.426126585093094\n",
      "Epoch 5 step 631: training accuarcy: 1.0\n",
      "Epoch 5 step 631: training loss: 50.057491321687806\n",
      "Epoch 5 step 632: training accuarcy: 0.9995\n",
      "Epoch 5 step 632: training loss: 50.480003450208784\n",
      "Epoch 5 step 633: training accuarcy: 0.9995\n",
      "Epoch 5 step 633: training loss: 49.18316635999055\n",
      "Epoch 5 step 634: training accuarcy: 0.9995\n",
      "Epoch 5 step 634: training loss: 50.5264988136248\n",
      "Epoch 5 step 635: training accuarcy: 0.999\n",
      "Epoch 5 step 635: training loss: 48.134184967698815\n",
      "Epoch 5 step 636: training accuarcy: 1.0\n",
      "Epoch 5 step 636: training loss: 50.578114994566704\n",
      "Epoch 5 step 637: training accuarcy: 0.999\n",
      "Epoch 5 step 637: training loss: 49.47725857849051\n",
      "Epoch 5 step 638: training accuarcy: 1.0\n",
      "Epoch 5 step 638: training loss: 49.96524050311645\n",
      "Epoch 5 step 639: training accuarcy: 0.999\n",
      "Epoch 5 step 639: training loss: 47.57497090005314\n",
      "Epoch 5 step 640: training accuarcy: 1.0\n",
      "Epoch 5 step 640: training loss: 48.14630368543397\n",
      "Epoch 5 step 641: training accuarcy: 1.0\n",
      "Epoch 5 step 641: training loss: 48.19059414042628\n",
      "Epoch 5 step 642: training accuarcy: 0.9995\n",
      "Epoch 5 step 642: training loss: 49.46404896394758\n",
      "Epoch 5 step 643: training accuarcy: 1.0\n",
      "Epoch 5 step 643: training loss: 49.9532809433771\n",
      "Epoch 5 step 644: training accuarcy: 0.998\n",
      "Epoch 5 step 644: training loss: 48.04212145997222\n",
      "Epoch 5 step 645: training accuarcy: 1.0\n",
      "Epoch 5 step 645: training loss: 47.66235321290637\n",
      "Epoch 5 step 646: training accuarcy: 1.0\n",
      "Epoch 5 step 646: training loss: 49.25408766170433\n",
      "Epoch 5 step 647: training accuarcy: 0.999\n",
      "Epoch 5 step 647: training loss: 48.36272449964848\n",
      "Epoch 5 step 648: training accuarcy: 1.0\n",
      "Epoch 5 step 648: training loss: 49.32790310782234\n",
      "Epoch 5 step 649: training accuarcy: 0.9985\n",
      "Epoch 5 step 649: training loss: 46.461558091587364\n",
      "Epoch 5 step 650: training accuarcy: 1.0\n",
      "Epoch 5 step 650: training loss: 48.36687261099795\n",
      "Epoch 5 step 651: training accuarcy: 0.9995\n",
      "Epoch 5 step 651: training loss: 50.20135585937387\n",
      "Epoch 5 step 652: training accuarcy: 0.9985\n",
      "Epoch 5 step 652: training loss: 46.20924817524727\n",
      "Epoch 5 step 653: training accuarcy: 1.0\n",
      "Epoch 5 step 653: training loss: 48.56279710799737\n",
      "Epoch 5 step 654: training accuarcy: 0.9985\n",
      "Epoch 5 step 654: training loss: 47.316814804342116\n",
      "Epoch 5 step 655: training accuarcy: 0.999\n",
      "Epoch 5 step 655: training loss: 48.992444852588314\n",
      "Epoch 5 step 656: training accuarcy: 0.999\n",
      "Epoch 5 step 656: training loss: 47.08202460881336\n",
      "Epoch 5 step 657: training accuarcy: 0.9995\n",
      "Epoch 5 step 657: training loss: 46.37387397501308\n",
      "Epoch 5 step 658: training accuarcy: 0.9995\n",
      "Epoch 5 step 658: training loss: 48.95024412820662\n",
      "Epoch 5 step 659: training accuarcy: 0.9985\n",
      "Epoch 5 step 659: training loss: 45.57004224523821\n",
      "Epoch 5 step 660: training accuarcy: 0.9995\n",
      "Epoch 5 step 660: training loss: 48.71545016980136\n",
      "Epoch 5 step 661: training accuarcy: 0.9985\n",
      "Epoch 5 step 661: training loss: 46.94038117383114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 662: training accuarcy: 0.999\n",
      "Epoch 5 step 662: training loss: 45.34718532230846\n",
      "Epoch 5 step 663: training accuarcy: 1.0\n",
      "Epoch 5 step 663: training loss: 45.33285734845379\n",
      "Epoch 5 step 664: training accuarcy: 0.9995\n",
      "Epoch 5 step 664: training loss: 46.78795620180711\n",
      "Epoch 5 step 665: training accuarcy: 0.9995\n",
      "Epoch 5 step 665: training loss: 49.003258488868454\n",
      "Epoch 5 step 666: training accuarcy: 0.9985\n",
      "Epoch 5 step 666: training loss: 45.99924310331096\n",
      "Epoch 5 step 667: training accuarcy: 0.9995\n",
      "Epoch 5 step 667: training loss: 45.99092072580966\n",
      "Epoch 5 step 668: training accuarcy: 0.999\n",
      "Epoch 5 step 668: training loss: 46.064443598958015\n",
      "Epoch 5 step 669: training accuarcy: 0.9985\n",
      "Epoch 5 step 669: training loss: 46.48884497286703\n",
      "Epoch 5 step 670: training accuarcy: 0.999\n",
      "Epoch 5 step 670: training loss: 44.36791987763531\n",
      "Epoch 5 step 671: training accuarcy: 0.9995\n",
      "Epoch 5 step 671: training loss: 48.17572558061062\n",
      "Epoch 5 step 672: training accuarcy: 0.9985\n",
      "Epoch 5 step 672: training loss: 44.73431578618894\n",
      "Epoch 5 step 673: training accuarcy: 0.999\n",
      "Epoch 5 step 673: training loss: 45.03117679238291\n",
      "Epoch 5 step 674: training accuarcy: 0.999\n",
      "Epoch 5 step 674: training loss: 45.44941941377371\n",
      "Epoch 5 step 675: training accuarcy: 0.999\n",
      "Epoch 5 step 675: training loss: 46.188082877055265\n",
      "Epoch 5 step 676: training accuarcy: 0.999\n",
      "Epoch 5 step 676: training loss: 45.09081747121895\n",
      "Epoch 5 step 677: training accuarcy: 0.999\n",
      "Epoch 5 step 677: training loss: 44.84922943413817\n",
      "Epoch 5 step 678: training accuarcy: 0.9995\n",
      "Epoch 5 step 678: training loss: 43.97388589807211\n",
      "Epoch 5 step 679: training accuarcy: 0.9995\n",
      "Epoch 5 step 679: training loss: 46.122446417020704\n",
      "Epoch 5 step 680: training accuarcy: 0.999\n",
      "Epoch 5 step 680: training loss: 45.39653161011015\n",
      "Epoch 5 step 681: training accuarcy: 0.999\n",
      "Epoch 5 step 681: training loss: 44.546272385998165\n",
      "Epoch 5 step 682: training accuarcy: 0.9995\n",
      "Epoch 5 step 682: training loss: 45.640068069567825\n",
      "Epoch 5 step 683: training accuarcy: 1.0\n",
      "Epoch 5 step 683: training loss: 45.171164073842434\n",
      "Epoch 5 step 684: training accuarcy: 0.999\n",
      "Epoch 5 step 684: training loss: 43.406260773586524\n",
      "Epoch 5 step 685: training accuarcy: 0.9995\n",
      "Epoch 5 step 685: training loss: 43.971495287652964\n",
      "Epoch 5 step 686: training accuarcy: 0.9995\n",
      "Epoch 5 step 686: training loss: 44.149130741082956\n",
      "Epoch 5 step 687: training accuarcy: 0.9995\n",
      "Epoch 5 step 687: training loss: 43.853748852427444\n",
      "Epoch 5 step 688: training accuarcy: 0.999\n",
      "Epoch 5 step 688: training loss: 43.22551444367915\n",
      "Epoch 5 step 689: training accuarcy: 0.999\n",
      "Epoch 5 step 689: training loss: 42.062667657170316\n",
      "Epoch 5 step 690: training accuarcy: 1.0\n",
      "Epoch 5: train loss 50.692134342958816, train accuarcy 0.999291181564331\n",
      "Epoch 5: valid loss 43.410101553476935, valid accuarcy 0.999489963054657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 6/8 [04:54<01:37, 48.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 690: training loss: 43.46647194461901\n",
      "Epoch 6 step 691: training accuarcy: 0.999\n",
      "Epoch 6 step 691: training loss: 43.18119557218349\n",
      "Epoch 6 step 692: training accuarcy: 0.9995\n",
      "Epoch 6 step 692: training loss: 43.47910340710074\n",
      "Epoch 6 step 693: training accuarcy: 0.999\n",
      "Epoch 6 step 693: training loss: 43.05772320476593\n",
      "Epoch 6 step 694: training accuarcy: 0.9995\n",
      "Epoch 6 step 694: training loss: 41.26756500370266\n",
      "Epoch 6 step 695: training accuarcy: 1.0\n",
      "Epoch 6 step 695: training loss: 43.52965908307408\n",
      "Epoch 6 step 696: training accuarcy: 0.999\n",
      "Epoch 6 step 696: training loss: 42.37229310233914\n",
      "Epoch 6 step 697: training accuarcy: 0.9995\n",
      "Epoch 6 step 697: training loss: 43.828857127300964\n",
      "Epoch 6 step 698: training accuarcy: 0.999\n",
      "Epoch 6 step 698: training loss: 42.60101072680792\n",
      "Epoch 6 step 699: training accuarcy: 1.0\n",
      "Epoch 6 step 699: training loss: 42.96954339777794\n",
      "Epoch 6 step 700: training accuarcy: 0.9995\n",
      "Epoch 6 step 700: training loss: 41.797075504211286\n",
      "Epoch 6 step 701: training accuarcy: 0.9995\n",
      "Epoch 6 step 701: training loss: 43.39344985998921\n",
      "Epoch 6 step 702: training accuarcy: 0.9985\n",
      "Epoch 6 step 702: training loss: 41.597764137184235\n",
      "Epoch 6 step 703: training accuarcy: 1.0\n",
      "Epoch 6 step 703: training loss: 42.46691592352869\n",
      "Epoch 6 step 704: training accuarcy: 0.999\n",
      "Epoch 6 step 704: training loss: 41.09802110964596\n",
      "Epoch 6 step 705: training accuarcy: 1.0\n",
      "Epoch 6 step 705: training loss: 42.75828425289185\n",
      "Epoch 6 step 706: training accuarcy: 0.999\n",
      "Epoch 6 step 706: training loss: 43.450007000470194\n",
      "Epoch 6 step 707: training accuarcy: 0.9995\n",
      "Epoch 6 step 707: training loss: 40.048354912604495\n",
      "Epoch 6 step 708: training accuarcy: 1.0\n",
      "Epoch 6 step 708: training loss: 43.11679528957878\n",
      "Epoch 6 step 709: training accuarcy: 0.9985\n",
      "Epoch 6 step 709: training loss: 39.73865522509308\n",
      "Epoch 6 step 710: training accuarcy: 1.0\n",
      "Epoch 6 step 710: training loss: 41.25255881238851\n",
      "Epoch 6 step 711: training accuarcy: 0.9995\n",
      "Epoch 6 step 711: training loss: 43.45910146233325\n",
      "Epoch 6 step 712: training accuarcy: 0.998\n",
      "Epoch 6 step 712: training loss: 42.237363970775796\n",
      "Epoch 6 step 713: training accuarcy: 0.9995\n",
      "Epoch 6 step 713: training loss: 41.545919729109244\n",
      "Epoch 6 step 714: training accuarcy: 0.9995\n",
      "Epoch 6 step 714: training loss: 41.8594297638097\n",
      "Epoch 6 step 715: training accuarcy: 0.9985\n",
      "Epoch 6 step 715: training loss: 41.171123881365254\n",
      "Epoch 6 step 716: training accuarcy: 0.999\n",
      "Epoch 6 step 716: training loss: 41.08418223089814\n",
      "Epoch 6 step 717: training accuarcy: 0.999\n",
      "Epoch 6 step 717: training loss: 41.66534755012955\n",
      "Epoch 6 step 718: training accuarcy: 0.999\n",
      "Epoch 6 step 718: training loss: 40.26398658289371\n",
      "Epoch 6 step 719: training accuarcy: 1.0\n",
      "Epoch 6 step 719: training loss: 40.42723994783685\n",
      "Epoch 6 step 720: training accuarcy: 1.0\n",
      "Epoch 6 step 720: training loss: 43.55506764595668\n",
      "Epoch 6 step 721: training accuarcy: 0.9985\n",
      "Epoch 6 step 721: training loss: 41.05145707005163\n",
      "Epoch 6 step 722: training accuarcy: 0.999\n",
      "Epoch 6 step 722: training loss: 40.22655913126927\n",
      "Epoch 6 step 723: training accuarcy: 0.9995\n",
      "Epoch 6 step 723: training loss: 40.11419491703328\n",
      "Epoch 6 step 724: training accuarcy: 0.9995\n",
      "Epoch 6 step 724: training loss: 39.64591263420002\n",
      "Epoch 6 step 725: training accuarcy: 0.9995\n",
      "Epoch 6 step 725: training loss: 40.42632086313725\n",
      "Epoch 6 step 726: training accuarcy: 0.999\n",
      "Epoch 6 step 726: training loss: 40.03227537633854\n",
      "Epoch 6 step 727: training accuarcy: 0.999\n",
      "Epoch 6 step 727: training loss: 39.58872984971247\n",
      "Epoch 6 step 728: training accuarcy: 0.9995\n",
      "Epoch 6 step 728: training loss: 39.643809732001344\n",
      "Epoch 6 step 729: training accuarcy: 1.0\n",
      "Epoch 6 step 729: training loss: 39.04480437956663\n",
      "Epoch 6 step 730: training accuarcy: 1.0\n",
      "Epoch 6 step 730: training loss: 39.68717791956051\n",
      "Epoch 6 step 731: training accuarcy: 1.0\n",
      "Epoch 6 step 731: training loss: 38.52320708869318\n",
      "Epoch 6 step 732: training accuarcy: 1.0\n",
      "Epoch 6 step 732: training loss: 39.82950767165207\n",
      "Epoch 6 step 733: training accuarcy: 0.999\n",
      "Epoch 6 step 733: training loss: 39.74741142774674\n",
      "Epoch 6 step 734: training accuarcy: 0.999\n",
      "Epoch 6 step 734: training loss: 42.18615289351497\n",
      "Epoch 6 step 735: training accuarcy: 0.9985\n",
      "Epoch 6 step 735: training loss: 38.30401411636148\n",
      "Epoch 6 step 736: training accuarcy: 1.0\n",
      "Epoch 6 step 736: training loss: 40.115533777197776\n",
      "Epoch 6 step 737: training accuarcy: 0.999\n",
      "Epoch 6 step 737: training loss: 39.19639870210905\n",
      "Epoch 6 step 738: training accuarcy: 0.9995\n",
      "Epoch 6 step 738: training loss: 38.79394510334068\n",
      "Epoch 6 step 739: training accuarcy: 0.9995\n",
      "Epoch 6 step 739: training loss: 37.751447288369036\n",
      "Epoch 6 step 740: training accuarcy: 1.0\n",
      "Epoch 6 step 740: training loss: 38.09438556565064\n",
      "Epoch 6 step 741: training accuarcy: 1.0\n",
      "Epoch 6 step 741: training loss: 38.451554621336584\n",
      "Epoch 6 step 742: training accuarcy: 0.9995\n",
      "Epoch 6 step 742: training loss: 39.8318407631574\n",
      "Epoch 6 step 743: training accuarcy: 0.9995\n",
      "Epoch 6 step 743: training loss: 37.43239586116977\n",
      "Epoch 6 step 744: training accuarcy: 1.0\n",
      "Epoch 6 step 744: training loss: 37.80593336771687\n",
      "Epoch 6 step 745: training accuarcy: 0.9995\n",
      "Epoch 6 step 745: training loss: 39.78778827987141\n",
      "Epoch 6 step 746: training accuarcy: 0.9985\n",
      "Epoch 6 step 746: training loss: 38.70864619122023\n",
      "Epoch 6 step 747: training accuarcy: 0.999\n",
      "Epoch 6 step 747: training loss: 37.430560166970594\n",
      "Epoch 6 step 748: training accuarcy: 0.9995\n",
      "Epoch 6 step 748: training loss: 38.81758301170622\n",
      "Epoch 6 step 749: training accuarcy: 0.999\n",
      "Epoch 6 step 749: training loss: 37.57010628889437\n",
      "Epoch 6 step 750: training accuarcy: 1.0\n",
      "Epoch 6 step 750: training loss: 39.898141959347186\n",
      "Epoch 6 step 751: training accuarcy: 0.999\n",
      "Epoch 6 step 751: training loss: 38.45024196401717\n",
      "Epoch 6 step 752: training accuarcy: 0.9995\n",
      "Epoch 6 step 752: training loss: 39.518269842339734\n",
      "Epoch 6 step 753: training accuarcy: 0.999\n",
      "Epoch 6 step 753: training loss: 39.34067506298344\n",
      "Epoch 6 step 754: training accuarcy: 0.9985\n",
      "Epoch 6 step 754: training loss: 36.84105115675139\n",
      "Epoch 6 step 755: training accuarcy: 0.9995\n",
      "Epoch 6 step 755: training loss: 38.03428147028369\n",
      "Epoch 6 step 756: training accuarcy: 0.9995\n",
      "Epoch 6 step 756: training loss: 37.95455726428351\n",
      "Epoch 6 step 757: training accuarcy: 0.9995\n",
      "Epoch 6 step 757: training loss: 37.973872215231076\n",
      "Epoch 6 step 758: training accuarcy: 0.999\n",
      "Epoch 6 step 758: training loss: 38.14925854975988\n",
      "Epoch 6 step 759: training accuarcy: 0.999\n",
      "Epoch 6 step 759: training loss: 36.22164113230175\n",
      "Epoch 6 step 760: training accuarcy: 0.9995\n",
      "Epoch 6 step 760: training loss: 36.910783425052344\n",
      "Epoch 6 step 761: training accuarcy: 0.9995\n",
      "Epoch 6 step 761: training loss: 37.68605361089068\n",
      "Epoch 6 step 762: training accuarcy: 0.999\n",
      "Epoch 6 step 762: training loss: 34.949765099065296\n",
      "Epoch 6 step 763: training accuarcy: 1.0\n",
      "Epoch 6 step 763: training loss: 36.23191333927361\n",
      "Epoch 6 step 764: training accuarcy: 1.0\n",
      "Epoch 6 step 764: training loss: 36.941777664700446\n",
      "Epoch 6 step 765: training accuarcy: 0.9995\n",
      "Epoch 6 step 765: training loss: 38.458061833529634\n",
      "Epoch 6 step 766: training accuarcy: 0.9995\n",
      "Epoch 6 step 766: training loss: 35.815813628849554\n",
      "Epoch 6 step 767: training accuarcy: 0.9995\n",
      "Epoch 6 step 767: training loss: 35.70412587311731\n",
      "Epoch 6 step 768: training accuarcy: 1.0\n",
      "Epoch 6 step 768: training loss: 38.30968778878734\n",
      "Epoch 6 step 769: training accuarcy: 0.9995\n",
      "Epoch 6 step 769: training loss: 34.699716991943234\n",
      "Epoch 6 step 770: training accuarcy: 1.0\n",
      "Epoch 6 step 770: training loss: 36.357912563689275\n",
      "Epoch 6 step 771: training accuarcy: 0.999\n",
      "Epoch 6 step 771: training loss: 34.905318975392966\n",
      "Epoch 6 step 772: training accuarcy: 1.0\n",
      "Epoch 6 step 772: training loss: 37.544997949166465\n",
      "Epoch 6 step 773: training accuarcy: 0.9995\n",
      "Epoch 6 step 773: training loss: 35.18468850660932\n",
      "Epoch 6 step 774: training accuarcy: 1.0\n",
      "Epoch 6 step 774: training loss: 37.19844564770635\n",
      "Epoch 6 step 775: training accuarcy: 0.999\n",
      "Epoch 6 step 775: training loss: 35.07678494313966\n",
      "Epoch 6 step 776: training accuarcy: 1.0\n",
      "Epoch 6 step 776: training loss: 37.11063791594702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 777: training accuarcy: 0.999\n",
      "Epoch 6 step 777: training loss: 35.170973126211344\n",
      "Epoch 6 step 778: training accuarcy: 0.9995\n",
      "Epoch 6 step 778: training loss: 36.43577128564611\n",
      "Epoch 6 step 779: training accuarcy: 0.999\n",
      "Epoch 6 step 779: training loss: 34.7258736339958\n",
      "Epoch 6 step 780: training accuarcy: 0.9995\n",
      "Epoch 6 step 780: training loss: 36.739514658577285\n",
      "Epoch 6 step 781: training accuarcy: 0.9995\n",
      "Epoch 6 step 781: training loss: 36.4532933782602\n",
      "Epoch 6 step 782: training accuarcy: 0.9995\n",
      "Epoch 6 step 782: training loss: 35.71718085836716\n",
      "Epoch 6 step 783: training accuarcy: 1.0\n",
      "Epoch 6 step 783: training loss: 35.66251654186964\n",
      "Epoch 6 step 784: training accuarcy: 0.9995\n",
      "Epoch 6 step 784: training loss: 33.32077106700435\n",
      "Epoch 6 step 785: training accuarcy: 1.0\n",
      "Epoch 6 step 785: training loss: 35.82718247917695\n",
      "Epoch 6 step 786: training accuarcy: 1.0\n",
      "Epoch 6 step 786: training loss: 34.89929089225908\n",
      "Epoch 6 step 787: training accuarcy: 1.0\n",
      "Epoch 6 step 787: training loss: 36.74126400683808\n",
      "Epoch 6 step 788: training accuarcy: 0.9985\n",
      "Epoch 6 step 788: training loss: 34.9587898526141\n",
      "Epoch 6 step 789: training accuarcy: 1.0\n",
      "Epoch 6 step 789: training loss: 34.901374123947605\n",
      "Epoch 6 step 790: training accuarcy: 0.999\n",
      "Epoch 6 step 790: training loss: 36.181550721547616\n",
      "Epoch 6 step 791: training accuarcy: 1.0\n",
      "Epoch 6 step 791: training loss: 33.9714767202632\n",
      "Epoch 6 step 792: training accuarcy: 1.0\n",
      "Epoch 6 step 792: training loss: 34.858607294228364\n",
      "Epoch 6 step 793: training accuarcy: 0.999\n",
      "Epoch 6 step 793: training loss: 34.81430892778445\n",
      "Epoch 6 step 794: training accuarcy: 0.9995\n",
      "Epoch 6 step 794: training loss: 37.551661930403355\n",
      "Epoch 6 step 795: training accuarcy: 0.9985\n",
      "Epoch 6 step 795: training loss: 34.168128138869015\n",
      "Epoch 6 step 796: training accuarcy: 0.999\n",
      "Epoch 6 step 796: training loss: 36.246988938219516\n",
      "Epoch 6 step 797: training accuarcy: 0.9995\n",
      "Epoch 6 step 797: training loss: 33.46718525475575\n",
      "Epoch 6 step 798: training accuarcy: 1.0\n",
      "Epoch 6 step 798: training loss: 33.1959776107777\n",
      "Epoch 6 step 799: training accuarcy: 0.9995\n",
      "Epoch 6 step 799: training loss: 35.71382490230712\n",
      "Epoch 6 step 800: training accuarcy: 0.9985\n",
      "Epoch 6 step 800: training loss: 32.56976415139817\n",
      "Epoch 6 step 801: training accuarcy: 1.0\n",
      "Epoch 6 step 801: training loss: 33.328508714823556\n",
      "Epoch 6 step 802: training accuarcy: 0.9995\n",
      "Epoch 6 step 802: training loss: 34.55380525106756\n",
      "Epoch 6 step 803: training accuarcy: 0.999\n",
      "Epoch 6 step 803: training loss: 34.79182415973665\n",
      "Epoch 6 step 804: training accuarcy: 0.9985\n",
      "Epoch 6 step 804: training loss: 32.913141195129434\n",
      "Epoch 6 step 805: training accuarcy: 0.999475890985325\n",
      "Epoch 6: train loss 38.49503258793176, train accuarcy 0.9993175268173218\n",
      "Epoch 6: valid loss 32.823158604946904, valid accuarcy 0.9997814297676086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 7/8 [05:44<00:49, 49.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 805: training loss: 32.36311428430497\n",
      "Epoch 7 step 806: training accuarcy: 1.0\n",
      "Epoch 7 step 806: training loss: 33.652171504709585\n",
      "Epoch 7 step 807: training accuarcy: 0.999\n",
      "Epoch 7 step 807: training loss: 32.196356396777794\n",
      "Epoch 7 step 808: training accuarcy: 1.0\n",
      "Epoch 7 step 808: training loss: 33.07616727402335\n",
      "Epoch 7 step 809: training accuarcy: 1.0\n",
      "Epoch 7 step 809: training loss: 32.44762822017671\n",
      "Epoch 7 step 810: training accuarcy: 0.9995\n",
      "Epoch 7 step 810: training loss: 32.81408076595477\n",
      "Epoch 7 step 811: training accuarcy: 1.0\n",
      "Epoch 7 step 811: training loss: 33.86650816035693\n",
      "Epoch 7 step 812: training accuarcy: 0.999\n",
      "Epoch 7 step 812: training loss: 33.38997050923709\n",
      "Epoch 7 step 813: training accuarcy: 0.9995\n",
      "Epoch 7 step 813: training loss: 34.52110445125161\n",
      "Epoch 7 step 814: training accuarcy: 0.9985\n",
      "Epoch 7 step 814: training loss: 32.34357605857335\n",
      "Epoch 7 step 815: training accuarcy: 0.9995\n",
      "Epoch 7 step 815: training loss: 32.76080979603159\n",
      "Epoch 7 step 816: training accuarcy: 1.0\n",
      "Epoch 7 step 816: training loss: 32.96945792564065\n",
      "Epoch 7 step 817: training accuarcy: 0.999\n",
      "Epoch 7 step 817: training loss: 35.27427577491782\n",
      "Epoch 7 step 818: training accuarcy: 0.999\n",
      "Epoch 7 step 818: training loss: 32.136690227775276\n",
      "Epoch 7 step 819: training accuarcy: 1.0\n",
      "Epoch 7 step 819: training loss: 32.361276851901934\n",
      "Epoch 7 step 820: training accuarcy: 0.9995\n",
      "Epoch 7 step 820: training loss: 31.153525936711688\n",
      "Epoch 7 step 821: training accuarcy: 1.0\n",
      "Epoch 7 step 821: training loss: 34.62402671322057\n",
      "Epoch 7 step 822: training accuarcy: 0.9995\n",
      "Epoch 7 step 822: training loss: 31.428137713520552\n",
      "Epoch 7 step 823: training accuarcy: 0.9995\n",
      "Epoch 7 step 823: training loss: 32.08347285068845\n",
      "Epoch 7 step 824: training accuarcy: 1.0\n",
      "Epoch 7 step 824: training loss: 32.46164456896556\n",
      "Epoch 7 step 825: training accuarcy: 1.0\n",
      "Epoch 7 step 825: training loss: 34.125581883717906\n",
      "Epoch 7 step 826: training accuarcy: 0.9985\n",
      "Epoch 7 step 826: training loss: 35.196085081160575\n",
      "Epoch 7 step 827: training accuarcy: 0.9985\n",
      "Epoch 7 step 827: training loss: 31.37768274519695\n",
      "Epoch 7 step 828: training accuarcy: 1.0\n",
      "Epoch 7 step 828: training loss: 30.912667010948358\n",
      "Epoch 7 step 829: training accuarcy: 1.0\n",
      "Epoch 7 step 829: training loss: 31.948267990309596\n",
      "Epoch 7 step 830: training accuarcy: 1.0\n",
      "Epoch 7 step 830: training loss: 30.754511675498975\n",
      "Epoch 7 step 831: training accuarcy: 1.0\n",
      "Epoch 7 step 831: training loss: 30.78871622990868\n",
      "Epoch 7 step 832: training accuarcy: 1.0\n",
      "Epoch 7 step 832: training loss: 33.397058486644\n",
      "Epoch 7 step 833: training accuarcy: 0.9995\n",
      "Epoch 7 step 833: training loss: 31.84004691741874\n",
      "Epoch 7 step 834: training accuarcy: 0.9995\n",
      "Epoch 7 step 834: training loss: 32.34426572382624\n",
      "Epoch 7 step 835: training accuarcy: 0.999\n",
      "Epoch 7 step 835: training loss: 32.51833290130751\n",
      "Epoch 7 step 836: training accuarcy: 0.9995\n",
      "Epoch 7 step 836: training loss: 31.704396097914024\n",
      "Epoch 7 step 837: training accuarcy: 1.0\n",
      "Epoch 7 step 837: training loss: 32.45751995189585\n",
      "Epoch 7 step 838: training accuarcy: 0.9995\n",
      "Epoch 7 step 838: training loss: 31.506229404641367\n",
      "Epoch 7 step 839: training accuarcy: 1.0\n",
      "Epoch 7 step 839: training loss: 32.0871336045085\n",
      "Epoch 7 step 840: training accuarcy: 0.9995\n",
      "Epoch 7 step 840: training loss: 32.21954955065723\n",
      "Epoch 7 step 841: training accuarcy: 1.0\n",
      "Epoch 7 step 841: training loss: 30.985835515531473\n",
      "Epoch 7 step 842: training accuarcy: 1.0\n",
      "Epoch 7 step 842: training loss: 32.43724132078984\n",
      "Epoch 7 step 843: training accuarcy: 0.999\n",
      "Epoch 7 step 843: training loss: 30.5635461899514\n",
      "Epoch 7 step 844: training accuarcy: 1.0\n",
      "Epoch 7 step 844: training loss: 31.163308410709707\n",
      "Epoch 7 step 845: training accuarcy: 1.0\n",
      "Epoch 7 step 845: training loss: 31.34930154899415\n",
      "Epoch 7 step 846: training accuarcy: 1.0\n",
      "Epoch 7 step 846: training loss: 32.828634298879294\n",
      "Epoch 7 step 847: training accuarcy: 0.998\n",
      "Epoch 7 step 847: training loss: 33.656440766408856\n",
      "Epoch 7 step 848: training accuarcy: 0.9985\n",
      "Epoch 7 step 848: training loss: 29.072808729115604\n",
      "Epoch 7 step 849: training accuarcy: 1.0\n",
      "Epoch 7 step 849: training loss: 31.968687764907664\n",
      "Epoch 7 step 850: training accuarcy: 0.9995\n",
      "Epoch 7 step 850: training loss: 32.28905768995457\n",
      "Epoch 7 step 851: training accuarcy: 0.9995\n",
      "Epoch 7 step 851: training loss: 32.48653078252116\n",
      "Epoch 7 step 852: training accuarcy: 0.999\n",
      "Epoch 7 step 852: training loss: 29.68864958972992\n",
      "Epoch 7 step 853: training accuarcy: 1.0\n",
      "Epoch 7 step 853: training loss: 30.687245392298692\n",
      "Epoch 7 step 854: training accuarcy: 1.0\n",
      "Epoch 7 step 854: training loss: 30.092750557932924\n",
      "Epoch 7 step 855: training accuarcy: 1.0\n",
      "Epoch 7 step 855: training loss: 30.22893490379585\n",
      "Epoch 7 step 856: training accuarcy: 0.999\n",
      "Epoch 7 step 856: training loss: 30.217507292025513\n",
      "Epoch 7 step 857: training accuarcy: 0.9995\n",
      "Epoch 7 step 857: training loss: 30.845989546276435\n",
      "Epoch 7 step 858: training accuarcy: 1.0\n",
      "Epoch 7 step 858: training loss: 29.50445971912719\n",
      "Epoch 7 step 859: training accuarcy: 1.0\n",
      "Epoch 7 step 859: training loss: 30.160040732911376\n",
      "Epoch 7 step 860: training accuarcy: 0.9995\n",
      "Epoch 7 step 860: training loss: 31.26878623798615\n",
      "Epoch 7 step 861: training accuarcy: 1.0\n",
      "Epoch 7 step 861: training loss: 30.748044081969777\n",
      "Epoch 7 step 862: training accuarcy: 0.999\n",
      "Epoch 7 step 862: training loss: 30.46687145296525\n",
      "Epoch 7 step 863: training accuarcy: 0.9995\n",
      "Epoch 7 step 863: training loss: 28.747417896830076\n",
      "Epoch 7 step 864: training accuarcy: 1.0\n",
      "Epoch 7 step 864: training loss: 29.971839128398265\n",
      "Epoch 7 step 865: training accuarcy: 0.9995\n",
      "Epoch 7 step 865: training loss: 28.351379015565353\n",
      "Epoch 7 step 866: training accuarcy: 1.0\n",
      "Epoch 7 step 866: training loss: 28.22212001097026\n",
      "Epoch 7 step 867: training accuarcy: 1.0\n",
      "Epoch 7 step 867: training loss: 30.187588724913347\n",
      "Epoch 7 step 868: training accuarcy: 0.9995\n",
      "Epoch 7 step 868: training loss: 30.26439360399043\n",
      "Epoch 7 step 869: training accuarcy: 0.9995\n",
      "Epoch 7 step 869: training loss: 31.051475764356287\n",
      "Epoch 7 step 870: training accuarcy: 0.9995\n",
      "Epoch 7 step 870: training loss: 30.136444451539223\n",
      "Epoch 7 step 871: training accuarcy: 0.9995\n",
      "Epoch 7 step 871: training loss: 29.35395424676527\n",
      "Epoch 7 step 872: training accuarcy: 0.9995\n",
      "Epoch 7 step 872: training loss: 30.64309799772564\n",
      "Epoch 7 step 873: training accuarcy: 0.9995\n",
      "Epoch 7 step 873: training loss: 29.27218715355837\n",
      "Epoch 7 step 874: training accuarcy: 0.9995\n",
      "Epoch 7 step 874: training loss: 32.35384334490523\n",
      "Epoch 7 step 875: training accuarcy: 0.9995\n",
      "Epoch 7 step 875: training loss: 27.561963586349627\n",
      "Epoch 7 step 876: training accuarcy: 1.0\n",
      "Epoch 7 step 876: training loss: 29.103391699971578\n",
      "Epoch 7 step 877: training accuarcy: 0.9995\n",
      "Epoch 7 step 877: training loss: 29.328646532888957\n",
      "Epoch 7 step 878: training accuarcy: 1.0\n",
      "Epoch 7 step 878: training loss: 28.75897575075886\n",
      "Epoch 7 step 879: training accuarcy: 0.9995\n",
      "Epoch 7 step 879: training loss: 28.573662952271007\n",
      "Epoch 7 step 880: training accuarcy: 1.0\n",
      "Epoch 7 step 880: training loss: 28.661937935324705\n",
      "Epoch 7 step 881: training accuarcy: 1.0\n",
      "Epoch 7 step 881: training loss: 27.89824606618772\n",
      "Epoch 7 step 882: training accuarcy: 1.0\n",
      "Epoch 7 step 882: training loss: 28.02592050168613\n",
      "Epoch 7 step 883: training accuarcy: 1.0\n",
      "Epoch 7 step 883: training loss: 27.638181386530636\n",
      "Epoch 7 step 884: training accuarcy: 1.0\n",
      "Epoch 7 step 884: training loss: 29.25753891314991\n",
      "Epoch 7 step 885: training accuarcy: 0.999\n",
      "Epoch 7 step 885: training loss: 28.040844501887143\n",
      "Epoch 7 step 886: training accuarcy: 1.0\n",
      "Epoch 7 step 886: training loss: 27.784216092541403\n",
      "Epoch 7 step 887: training accuarcy: 1.0\n",
      "Epoch 7 step 887: training loss: 28.24432468462546\n",
      "Epoch 7 step 888: training accuarcy: 0.9995\n",
      "Epoch 7 step 888: training loss: 27.41847604554197\n",
      "Epoch 7 step 889: training accuarcy: 1.0\n",
      "Epoch 7 step 889: training loss: 26.967958300986897\n",
      "Epoch 7 step 890: training accuarcy: 1.0\n",
      "Epoch 7 step 890: training loss: 28.79589899302985\n",
      "Epoch 7 step 891: training accuarcy: 0.9995\n",
      "Epoch 7 step 891: training loss: 28.85744913118789\n",
      "Epoch 7 step 892: training accuarcy: 0.9995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 892: training loss: 30.021005712313432\n",
      "Epoch 7 step 893: training accuarcy: 0.9995\n",
      "Epoch 7 step 893: training loss: 27.385529988427308\n",
      "Epoch 7 step 894: training accuarcy: 1.0\n",
      "Epoch 7 step 894: training loss: 26.444238024106994\n",
      "Epoch 7 step 895: training accuarcy: 1.0\n",
      "Epoch 7 step 895: training loss: 26.95295217740551\n",
      "Epoch 7 step 896: training accuarcy: 1.0\n",
      "Epoch 7 step 896: training loss: 27.959808265784304\n",
      "Epoch 7 step 897: training accuarcy: 0.9995\n",
      "Epoch 7 step 897: training loss: 29.21539195879513\n",
      "Epoch 7 step 898: training accuarcy: 0.999\n",
      "Epoch 7 step 898: training loss: 26.86335639864698\n",
      "Epoch 7 step 899: training accuarcy: 1.0\n",
      "Epoch 7 step 899: training loss: 27.607304828050303\n",
      "Epoch 7 step 900: training accuarcy: 1.0\n",
      "Epoch 7 step 900: training loss: 27.5211562433774\n",
      "Epoch 7 step 901: training accuarcy: 1.0\n",
      "Epoch 7 step 901: training loss: 27.91290352663156\n",
      "Epoch 7 step 902: training accuarcy: 0.9995\n",
      "Epoch 7 step 902: training loss: 27.913353322800937\n",
      "Epoch 7 step 903: training accuarcy: 1.0\n",
      "Epoch 7 step 903: training loss: 28.064424454941637\n",
      "Epoch 7 step 904: training accuarcy: 1.0\n",
      "Epoch 7 step 904: training loss: 27.87754968637433\n",
      "Epoch 7 step 905: training accuarcy: 0.999\n",
      "Epoch 7 step 905: training loss: 28.673068453258512\n",
      "Epoch 7 step 906: training accuarcy: 0.999\n",
      "Epoch 7 step 906: training loss: 27.271629656168656\n",
      "Epoch 7 step 907: training accuarcy: 1.0\n",
      "Epoch 7 step 907: training loss: 26.987098097744923\n",
      "Epoch 7 step 908: training accuarcy: 0.9995\n",
      "Epoch 7 step 908: training loss: 29.232234838105146\n",
      "Epoch 7 step 909: training accuarcy: 1.0\n",
      "Epoch 7 step 909: training loss: 27.553559314197823\n",
      "Epoch 7 step 910: training accuarcy: 0.9995\n",
      "Epoch 7 step 910: training loss: 28.192208627969826\n",
      "Epoch 7 step 911: training accuarcy: 0.9985\n",
      "Epoch 7 step 911: training loss: 27.641459116751548\n",
      "Epoch 7 step 912: training accuarcy: 1.0\n",
      "Epoch 7 step 912: training loss: 29.662980941462443\n",
      "Epoch 7 step 913: training accuarcy: 1.0\n",
      "Epoch 7 step 913: training loss: 26.20317778225669\n",
      "Epoch 7 step 914: training accuarcy: 1.0\n",
      "Epoch 7 step 914: training loss: 26.50746055187793\n",
      "Epoch 7 step 915: training accuarcy: 1.0\n",
      "Epoch 7 step 915: training loss: 27.422161923924087\n",
      "Epoch 7 step 916: training accuarcy: 0.9995\n",
      "Epoch 7 step 916: training loss: 26.320109299612824\n",
      "Epoch 7 step 917: training accuarcy: 1.0\n",
      "Epoch 7 step 917: training loss: 26.480138500284355\n",
      "Epoch 7 step 918: training accuarcy: 1.0\n",
      "Epoch 7 step 918: training loss: 25.72009148531888\n",
      "Epoch 7 step 919: training accuarcy: 1.0\n",
      "Epoch 7 step 919: training loss: 29.225609932249988\n",
      "Epoch 7 step 920: training accuarcy: 1.0\n",
      "Epoch 7: train loss 30.17493957642047, train accuarcy 0.9996886253356934\n",
      "Epoch 7: valid loss 26.4438617503881, valid accuarcy 0.9999271631240845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [06:33<00:00, 49.02s/it]\n"
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=8,\n",
    "               loss_callback=simple_loss_callback,\n",
    "               log_dir=get_log_dir(ds_type='kaggle', model_type='fm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:40:59.898662Z",
     "start_time": "2019-09-25T08:40:59.892636Z"
    }
   },
   "outputs": [],
   "source": [
    "del fm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HRM FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:41:28.012353Z",
     "start_time": "2019-09-25T08:41:27.932382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchHrmFM()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_model = TorchHrmFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "hrm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:41:32.262693Z",
     "start_time": "2019-09-25T08:41:32.259714Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(hrm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:41:36.646976Z",
     "start_time": "2019-09-25T08:41:36.619976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1edb713b5c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_learner = FMLearner(hrm_model, adam_opt, schedular, db)\n",
    "hrm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:48:35.440903Z",
     "start_time": "2019-09-25T08:41:53.964282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 49104.56763214461\n",
      "Epoch 0 step 1: training accuarcy: 0.112\n",
      "Epoch 0 step 1: training loss: 48457.58778517936\n",
      "Epoch 0 step 2: training accuarcy: 0.10200000000000001\n",
      "Epoch 0 step 2: training loss: 48962.610062743945\n",
      "Epoch 0 step 3: training accuarcy: 0.1245\n",
      "Epoch 0 step 3: training loss: 48548.96847246657\n",
      "Epoch 0 step 4: training accuarcy: 0.1275\n",
      "Epoch 0 step 4: training loss: 47459.58334606852\n",
      "Epoch 0 step 5: training accuarcy: 0.1465\n",
      "Epoch 0 step 5: training loss: 47724.11967629842\n",
      "Epoch 0 step 6: training accuarcy: 0.14150000000000001\n",
      "Epoch 0 step 6: training loss: 47846.84756590407\n",
      "Epoch 0 step 7: training accuarcy: 0.137\n",
      "Epoch 0 step 7: training loss: 47630.811309626995\n",
      "Epoch 0 step 8: training accuarcy: 0.14300000000000002\n",
      "Epoch 0 step 8: training loss: 46890.35215113851\n",
      "Epoch 0 step 9: training accuarcy: 0.14200000000000002\n",
      "Epoch 0 step 9: training loss: 46172.72804600619\n",
      "Epoch 0 step 10: training accuarcy: 0.153\n",
      "Epoch 0 step 10: training loss: 46505.08664178944\n",
      "Epoch 0 step 11: training accuarcy: 0.1455\n",
      "Epoch 0 step 11: training loss: 46016.66048464048\n",
      "Epoch 0 step 12: training accuarcy: 0.1545\n",
      "Epoch 0 step 12: training loss: 45857.90914173979\n",
      "Epoch 0 step 13: training accuarcy: 0.1505\n",
      "Epoch 0 step 13: training loss: 45634.2171169253\n",
      "Epoch 0 step 14: training accuarcy: 0.153\n",
      "Epoch 0 step 14: training loss: 45133.00405749433\n",
      "Epoch 0 step 15: training accuarcy: 0.157\n",
      "Epoch 0 step 15: training loss: 44502.86988216625\n",
      "Epoch 0 step 16: training accuarcy: 0.1605\n",
      "Epoch 0 step 16: training loss: 44972.835501896334\n",
      "Epoch 0 step 17: training accuarcy: 0.1495\n",
      "Epoch 0 step 17: training loss: 45139.49346441808\n",
      "Epoch 0 step 18: training accuarcy: 0.14200000000000002\n",
      "Epoch 0 step 18: training loss: 43832.966599111554\n",
      "Epoch 0 step 19: training accuarcy: 0.161\n",
      "Epoch 0 step 19: training loss: 43307.93512185054\n",
      "Epoch 0 step 20: training accuarcy: 0.17250000000000001\n",
      "Epoch 0 step 20: training loss: 44285.76204901982\n",
      "Epoch 0 step 21: training accuarcy: 0.1485\n",
      "Epoch 0 step 21: training loss: 44343.79904711875\n",
      "Epoch 0 step 22: training accuarcy: 0.1475\n",
      "Epoch 0 step 22: training loss: 43592.69046150789\n",
      "Epoch 0 step 23: training accuarcy: 0.1595\n",
      "Epoch 0 step 23: training loss: 43240.18352417319\n",
      "Epoch 0 step 24: training accuarcy: 0.1615\n",
      "Epoch 0 step 24: training loss: 42910.71185349\n",
      "Epoch 0 step 25: training accuarcy: 0.1665\n",
      "Epoch 0 step 25: training loss: 43261.217516827564\n",
      "Epoch 0 step 26: training accuarcy: 0.155\n",
      "Epoch 0 step 26: training loss: 42600.33116314705\n",
      "Epoch 0 step 27: training accuarcy: 0.167\n",
      "Epoch 0 step 27: training loss: 42180.74530597304\n",
      "Epoch 0 step 28: training accuarcy: 0.171\n",
      "Epoch 0 step 28: training loss: 41490.71061176969\n",
      "Epoch 0 step 29: training accuarcy: 0.1815\n",
      "Epoch 0 step 29: training loss: 42562.26218653032\n",
      "Epoch 0 step 30: training accuarcy: 0.161\n",
      "Epoch 0 step 30: training loss: 41926.01261735248\n",
      "Epoch 0 step 31: training accuarcy: 0.1665\n",
      "Epoch 0 step 31: training loss: 41724.59878281576\n",
      "Epoch 0 step 32: training accuarcy: 0.1695\n",
      "Epoch 0 step 32: training loss: 42361.36395937589\n",
      "Epoch 0 step 33: training accuarcy: 0.1525\n",
      "Epoch 0 step 33: training loss: 41497.34782220838\n",
      "Epoch 0 step 34: training accuarcy: 0.17\n",
      "Epoch 0 step 34: training loss: 40928.15790527529\n",
      "Epoch 0 step 35: training accuarcy: 0.1795\n",
      "Epoch 0 step 35: training loss: 40488.01022346414\n",
      "Epoch 0 step 36: training accuarcy: 0.1885\n",
      "Epoch 0 step 36: training loss: 40862.55766914524\n",
      "Epoch 0 step 37: training accuarcy: 0.17450000000000002\n",
      "Epoch 0 step 37: training loss: 40412.79809872795\n",
      "Epoch 0 step 38: training accuarcy: 0.1805\n",
      "Epoch 0 step 38: training loss: 40857.460071839385\n",
      "Epoch 0 step 39: training accuarcy: 0.1665\n",
      "Epoch 0 step 39: training loss: 40737.99332028318\n",
      "Epoch 0 step 40: training accuarcy: 0.167\n",
      "Epoch 0 step 40: training loss: 40182.19787472397\n",
      "Epoch 0 step 41: training accuarcy: 0.178\n",
      "Epoch 0 step 41: training loss: 40294.3847544352\n",
      "Epoch 0 step 42: training accuarcy: 0.177\n",
      "Epoch 0 step 42: training loss: 39555.31392281443\n",
      "Epoch 0 step 43: training accuarcy: 0.1895\n",
      "Epoch 0 step 43: training loss: 40653.032389780805\n",
      "Epoch 0 step 44: training accuarcy: 0.1635\n",
      "Epoch 0 step 44: training loss: 39325.324293871316\n",
      "Epoch 0 step 45: training accuarcy: 0.1885\n",
      "Epoch 0 step 45: training loss: 39481.276392557156\n",
      "Epoch 0 step 46: training accuarcy: 0.1805\n",
      "Epoch 0 step 46: training loss: 40158.5492565205\n",
      "Epoch 0 step 47: training accuarcy: 0.171\n",
      "Epoch 0 step 47: training loss: 40884.022551241826\n",
      "Epoch 0 step 48: training accuarcy: 0.1535\n",
      "Epoch 0 step 48: training loss: 40162.03102483193\n",
      "Epoch 0 step 49: training accuarcy: 0.1665\n",
      "Epoch 0 step 49: training loss: 39373.989108659436\n",
      "Epoch 0 step 50: training accuarcy: 0.1805\n",
      "Epoch 0 step 50: training loss: 38878.609545752704\n",
      "Epoch 0 step 51: training accuarcy: 0.1905\n",
      "Epoch 0 step 51: training loss: 39862.73987397244\n",
      "Epoch 0 step 52: training accuarcy: 0.1675\n",
      "Epoch 0 step 52: training loss: 40116.009071233384\n",
      "Epoch 0 step 53: training accuarcy: 0.161\n",
      "Epoch 0 step 53: training loss: 39384.24312757482\n",
      "Epoch 0 step 54: training accuarcy: 0.1715\n",
      "Epoch 0 step 54: training loss: 38982.42842391297\n",
      "Epoch 0 step 55: training accuarcy: 0.1815\n",
      "Epoch 0 step 55: training loss: 38935.82120583073\n",
      "Epoch 0 step 56: training accuarcy: 0.1805\n",
      "Epoch 0 step 56: training loss: 39235.71441983964\n",
      "Epoch 0 step 57: training accuarcy: 0.176\n",
      "Epoch 0 step 57: training loss: 37950.88389369636\n",
      "Epoch 0 step 58: training accuarcy: 0.1955\n",
      "Epoch 0 step 58: training loss: 38847.00464057492\n",
      "Epoch 0 step 59: training accuarcy: 0.17500000000000002\n",
      "Epoch 0 step 59: training loss: 38627.452925231395\n",
      "Epoch 0 step 60: training accuarcy: 0.1865\n",
      "Epoch 0 step 60: training loss: 39325.000549365046\n",
      "Epoch 0 step 61: training accuarcy: 0.169\n",
      "Epoch 0 step 61: training loss: 38984.94645894082\n",
      "Epoch 0 step 62: training accuarcy: 0.17450000000000002\n",
      "Epoch 0 step 62: training loss: 38909.871007838476\n",
      "Epoch 0 step 63: training accuarcy: 0.17550000000000002\n",
      "Epoch 0 step 63: training loss: 38826.688644847374\n",
      "Epoch 0 step 64: training accuarcy: 0.17250000000000001\n",
      "Epoch 0 step 64: training loss: 38369.814222936235\n",
      "Epoch 0 step 65: training accuarcy: 0.1805\n",
      "Epoch 0 step 65: training loss: 38101.558144044684\n",
      "Epoch 0 step 66: training accuarcy: 0.179\n",
      "Epoch 0 step 66: training loss: 38363.87719015115\n",
      "Epoch 0 step 67: training accuarcy: 0.17450000000000002\n",
      "Epoch 0 step 67: training loss: 37248.12081123449\n",
      "Epoch 0 step 68: training accuarcy: 0.1945\n",
      "Epoch 0 step 68: training loss: 36677.55490892678\n",
      "Epoch 0 step 69: training accuarcy: 0.2095\n",
      "Epoch 0 step 69: training loss: 36018.85185884415\n",
      "Epoch 0 step 70: training accuarcy: 0.223\n",
      "Epoch 0 step 70: training loss: 35116.74970037862\n",
      "Epoch 0 step 71: training accuarcy: 0.232\n",
      "Epoch 0 step 71: training loss: 33743.89479860356\n",
      "Epoch 0 step 72: training accuarcy: 0.257\n",
      "Epoch 0 step 72: training loss: 32596.77953274129\n",
      "Epoch 0 step 73: training accuarcy: 0.28350000000000003\n",
      "Epoch 0 step 73: training loss: 31814.52798035059\n",
      "Epoch 0 step 74: training accuarcy: 0.3025\n",
      "Epoch 0 step 74: training loss: 29457.79245721971\n",
      "Epoch 0 step 75: training accuarcy: 0.359\n",
      "Epoch 0 step 75: training loss: 27591.972532385927\n",
      "Epoch 0 step 76: training accuarcy: 0.403\n",
      "Epoch 0 step 76: training loss: 25562.804698099077\n",
      "Epoch 0 step 77: training accuarcy: 0.448\n",
      "Epoch 0 step 77: training loss: 24327.421729455393\n",
      "Epoch 0 step 78: training accuarcy: 0.468\n",
      "Epoch 0 step 78: training loss: 24306.09240455036\n",
      "Epoch 0 step 79: training accuarcy: 0.468\n",
      "Epoch 0 step 79: training loss: 23926.016681612444\n",
      "Epoch 0 step 80: training accuarcy: 0.4815\n",
      "Epoch 0 step 80: training loss: 23208.589660620546\n",
      "Epoch 0 step 81: training accuarcy: 0.498\n",
      "Epoch 0 step 81: training loss: 22146.046301790164\n",
      "Epoch 0 step 82: training accuarcy: 0.518\n",
      "Epoch 0 step 82: training loss: 21749.0698242783\n",
      "Epoch 0 step 83: training accuarcy: 0.532\n",
      "Epoch 0 step 83: training loss: 20868.682522573443\n",
      "Epoch 0 step 84: training accuarcy: 0.5455\n",
      "Epoch 0 step 84: training loss: 20010.91479363869\n",
      "Epoch 0 step 85: training accuarcy: 0.5670000000000001\n",
      "Epoch 0 step 85: training loss: 19520.08231930418\n",
      "Epoch 0 step 86: training accuarcy: 0.5725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 86: training loss: 18909.942909548365\n",
      "Epoch 0 step 87: training accuarcy: 0.5845\n",
      "Epoch 0 step 87: training loss: 17364.675307133417\n",
      "Epoch 0 step 88: training accuarcy: 0.6205\n",
      "Epoch 0 step 88: training loss: 16654.727954710874\n",
      "Epoch 0 step 89: training accuarcy: 0.634\n",
      "Epoch 0 step 89: training loss: 16635.03591320687\n",
      "Epoch 0 step 90: training accuarcy: 0.6365000000000001\n",
      "Epoch 0 step 90: training loss: 14984.404132969783\n",
      "Epoch 0 step 91: training accuarcy: 0.6715\n",
      "Epoch 0 step 91: training loss: 14970.360199124605\n",
      "Epoch 0 step 92: training accuarcy: 0.6755\n",
      "Epoch 0 step 92: training loss: 13225.959184956548\n",
      "Epoch 0 step 93: training accuarcy: 0.712\n",
      "Epoch 0 step 93: training loss: 13763.68740450565\n",
      "Epoch 0 step 94: training accuarcy: 0.6965\n",
      "Epoch 0 step 94: training loss: 13164.94939376108\n",
      "Epoch 0 step 95: training accuarcy: 0.7030000000000001\n",
      "Epoch 0 step 95: training loss: 12441.060539406677\n",
      "Epoch 0 step 96: training accuarcy: 0.726\n",
      "Epoch 0 step 96: training loss: 11342.084044986224\n",
      "Epoch 0 step 97: training accuarcy: 0.752\n",
      "Epoch 0 step 97: training loss: 10357.609892961382\n",
      "Epoch 0 step 98: training accuarcy: 0.7745\n",
      "Epoch 0 step 98: training loss: 10279.968130310006\n",
      "Epoch 0 step 99: training accuarcy: 0.779\n",
      "Epoch 0 step 99: training loss: 10981.50209242603\n",
      "Epoch 0 step 100: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 100: training loss: 9841.96350518995\n",
      "Epoch 0 step 101: training accuarcy: 0.788\n",
      "Epoch 0 step 101: training loss: 9907.354520711098\n",
      "Epoch 0 step 102: training accuarcy: 0.783\n",
      "Epoch 0 step 102: training loss: 9422.117492319168\n",
      "Epoch 0 step 103: training accuarcy: 0.7965\n",
      "Epoch 0 step 103: training loss: 9779.710558812403\n",
      "Epoch 0 step 104: training accuarcy: 0.787\n",
      "Epoch 0 step 104: training loss: 8933.437674446706\n",
      "Epoch 0 step 105: training accuarcy: 0.8045\n",
      "Epoch 0 step 105: training loss: 8852.33130152738\n",
      "Epoch 0 step 106: training accuarcy: 0.8085\n",
      "Epoch 0 step 106: training loss: 9569.503290229617\n",
      "Epoch 0 step 107: training accuarcy: 0.795\n",
      "Epoch 0 step 107: training loss: 8879.54017361983\n",
      "Epoch 0 step 108: training accuarcy: 0.807\n",
      "Epoch 0 step 108: training loss: 9149.839988686119\n",
      "Epoch 0 step 109: training accuarcy: 0.7995\n",
      "Epoch 0 step 109: training loss: 8586.991459226281\n",
      "Epoch 0 step 110: training accuarcy: 0.8135\n",
      "Epoch 0 step 110: training loss: 10024.348938621546\n",
      "Epoch 0 step 111: training accuarcy: 0.783\n",
      "Epoch 0 step 111: training loss: 8475.956624994216\n",
      "Epoch 0 step 112: training accuarcy: 0.8150000000000001\n",
      "Epoch 0 step 112: training loss: 7935.7353102242005\n",
      "Epoch 0 step 113: training accuarcy: 0.8240000000000001\n",
      "Epoch 0 step 113: training loss: 8230.183173079224\n",
      "Epoch 0 step 114: training accuarcy: 0.8195\n",
      "Epoch 0 step 114: training loss: 7522.244979784206\n",
      "Epoch 0 step 115: training accuarcy: 0.829140461215933\n",
      "Epoch 0: train loss 32085.37256324278, train accuarcy 0.3515337109565735\n",
      "Epoch 0: valid loss 7775.092378071496, valid accuarcy 0.828372597694397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████▏                                                                                                                                     | 1/8 [00:51<05:57, 51.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 7969.722969820458\n",
      "Epoch 1 step 116: training accuarcy: 0.8275\n",
      "Epoch 1 step 116: training loss: 8023.574539651519\n",
      "Epoch 1 step 117: training accuarcy: 0.8245\n",
      "Epoch 1 step 117: training loss: 7245.850289879148\n",
      "Epoch 1 step 118: training accuarcy: 0.841\n",
      "Epoch 1 step 118: training loss: 7009.452441708271\n",
      "Epoch 1 step 119: training accuarcy: 0.8475\n",
      "Epoch 1 step 119: training loss: 8074.418023554054\n",
      "Epoch 1 step 120: training accuarcy: 0.8240000000000001\n",
      "Epoch 1 step 120: training loss: 7842.338344856498\n",
      "Epoch 1 step 121: training accuarcy: 0.8295\n",
      "Epoch 1 step 121: training loss: 7402.593849453467\n",
      "Epoch 1 step 122: training accuarcy: 0.8405\n",
      "Epoch 1 step 122: training loss: 7173.281090398528\n",
      "Epoch 1 step 123: training accuarcy: 0.843\n",
      "Epoch 1 step 123: training loss: 6883.990589033709\n",
      "Epoch 1 step 124: training accuarcy: 0.8485\n",
      "Epoch 1 step 124: training loss: 7006.314368317555\n",
      "Epoch 1 step 125: training accuarcy: 0.8495\n",
      "Epoch 1 step 125: training loss: 6733.407938906819\n",
      "Epoch 1 step 126: training accuarcy: 0.8535\n",
      "Epoch 1 step 126: training loss: 6434.589028010395\n",
      "Epoch 1 step 127: training accuarcy: 0.858\n",
      "Epoch 1 step 127: training loss: 7298.804276567078\n",
      "Epoch 1 step 128: training accuarcy: 0.8385\n",
      "Epoch 1 step 128: training loss: 6485.853507803574\n",
      "Epoch 1 step 129: training accuarcy: 0.861\n",
      "Epoch 1 step 129: training loss: 6791.525390303767\n",
      "Epoch 1 step 130: training accuarcy: 0.8505\n",
      "Epoch 1 step 130: training loss: 5442.6277685648\n",
      "Epoch 1 step 131: training accuarcy: 0.883\n",
      "Epoch 1 step 131: training loss: 6830.140405228711\n",
      "Epoch 1 step 132: training accuarcy: 0.8505\n",
      "Epoch 1 step 132: training loss: 7444.41098488272\n",
      "Epoch 1 step 133: training accuarcy: 0.837\n",
      "Epoch 1 step 133: training loss: 6460.980772273039\n",
      "Epoch 1 step 134: training accuarcy: 0.8585\n",
      "Epoch 1 step 134: training loss: 6913.172450452686\n",
      "Epoch 1 step 135: training accuarcy: 0.8485\n",
      "Epoch 1 step 135: training loss: 5938.494837318117\n",
      "Epoch 1 step 136: training accuarcy: 0.8715\n",
      "Epoch 1 step 136: training loss: 6438.454907441758\n",
      "Epoch 1 step 137: training accuarcy: 0.8595\n",
      "Epoch 1 step 137: training loss: 6403.423097954469\n",
      "Epoch 1 step 138: training accuarcy: 0.86\n",
      "Epoch 1 step 138: training loss: 5332.575004674115\n",
      "Epoch 1 step 139: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 139: training loss: 6035.307059991723\n",
      "Epoch 1 step 140: training accuarcy: 0.869\n",
      "Epoch 1 step 140: training loss: 6045.516629681273\n",
      "Epoch 1 step 141: training accuarcy: 0.8705\n",
      "Epoch 1 step 141: training loss: 5954.821588090615\n",
      "Epoch 1 step 142: training accuarcy: 0.869\n",
      "Epoch 1 step 142: training loss: 5886.530055854768\n",
      "Epoch 1 step 143: training accuarcy: 0.87\n",
      "Epoch 1 step 143: training loss: 5670.752057161908\n",
      "Epoch 1 step 144: training accuarcy: 0.877\n",
      "Epoch 1 step 144: training loss: 5703.88409041427\n",
      "Epoch 1 step 145: training accuarcy: 0.8725\n",
      "Epoch 1 step 145: training loss: 6136.460843460747\n",
      "Epoch 1 step 146: training accuarcy: 0.863\n",
      "Epoch 1 step 146: training loss: 5937.429582180429\n",
      "Epoch 1 step 147: training accuarcy: 0.867\n",
      "Epoch 1 step 147: training loss: 6025.9063415420405\n",
      "Epoch 1 step 148: training accuarcy: 0.869\n",
      "Epoch 1 step 148: training loss: 5656.144702149403\n",
      "Epoch 1 step 149: training accuarcy: 0.874\n",
      "Epoch 1 step 149: training loss: 6296.293437866353\n",
      "Epoch 1 step 150: training accuarcy: 0.864\n",
      "Epoch 1 step 150: training loss: 5360.345776441821\n",
      "Epoch 1 step 151: training accuarcy: 0.883\n",
      "Epoch 1 step 151: training loss: 5896.381601196257\n",
      "Epoch 1 step 152: training accuarcy: 0.8705\n",
      "Epoch 1 step 152: training loss: 5395.734543459404\n",
      "Epoch 1 step 153: training accuarcy: 0.881\n",
      "Epoch 1 step 153: training loss: 5474.574957241471\n",
      "Epoch 1 step 154: training accuarcy: 0.8825000000000001\n",
      "Epoch 1 step 154: training loss: 5134.656052628789\n",
      "Epoch 1 step 155: training accuarcy: 0.8865000000000001\n",
      "Epoch 1 step 155: training loss: 5604.198700651345\n",
      "Epoch 1 step 156: training accuarcy: 0.8765000000000001\n",
      "Epoch 1 step 156: training loss: 5221.088111173139\n",
      "Epoch 1 step 157: training accuarcy: 0.8835000000000001\n",
      "Epoch 1 step 157: training loss: 5568.711206906544\n",
      "Epoch 1 step 158: training accuarcy: 0.874\n",
      "Epoch 1 step 158: training loss: 5207.47874656225\n",
      "Epoch 1 step 159: training accuarcy: 0.886\n",
      "Epoch 1 step 159: training loss: 5058.494869834052\n",
      "Epoch 1 step 160: training accuarcy: 0.8905000000000001\n",
      "Epoch 1 step 160: training loss: 5194.171610008301\n",
      "Epoch 1 step 161: training accuarcy: 0.886\n",
      "Epoch 1 step 161: training loss: 5117.732601004524\n",
      "Epoch 1 step 162: training accuarcy: 0.888\n",
      "Epoch 1 step 162: training loss: 5574.229068321497\n",
      "Epoch 1 step 163: training accuarcy: 0.879\n",
      "Epoch 1 step 163: training loss: 5388.1835899968055\n",
      "Epoch 1 step 164: training accuarcy: 0.8825000000000001\n",
      "Epoch 1 step 164: training loss: 5116.6152929224845\n",
      "Epoch 1 step 165: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 165: training loss: 5382.150334017281\n",
      "Epoch 1 step 166: training accuarcy: 0.8825000000000001\n",
      "Epoch 1 step 166: training loss: 5000.804269836883\n",
      "Epoch 1 step 167: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 167: training loss: 6057.614763290993\n",
      "Epoch 1 step 168: training accuarcy: 0.8655\n",
      "Epoch 1 step 168: training loss: 5202.808823384369\n",
      "Epoch 1 step 169: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 169: training loss: 5602.47610730666\n",
      "Epoch 1 step 170: training accuarcy: 0.877\n",
      "Epoch 1 step 170: training loss: 5617.257623450705\n",
      "Epoch 1 step 171: training accuarcy: 0.876\n",
      "Epoch 1 step 171: training loss: 5869.095115607826\n",
      "Epoch 1 step 172: training accuarcy: 0.873\n",
      "Epoch 1 step 172: training loss: 5311.119972286301\n",
      "Epoch 1 step 173: training accuarcy: 0.886\n",
      "Epoch 1 step 173: training loss: 4794.17175498899\n",
      "Epoch 1 step 174: training accuarcy: 0.8955000000000001\n",
      "Epoch 1 step 174: training loss: 5032.923580941625\n",
      "Epoch 1 step 175: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 175: training loss: 5243.558377755332\n",
      "Epoch 1 step 176: training accuarcy: 0.8835000000000001\n",
      "Epoch 1 step 176: training loss: 4853.537610405215\n",
      "Epoch 1 step 177: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 177: training loss: 4668.5654638426\n",
      "Epoch 1 step 178: training accuarcy: 0.898\n",
      "Epoch 1 step 178: training loss: 5083.229941617946\n",
      "Epoch 1 step 179: training accuarcy: 0.889\n",
      "Epoch 1 step 179: training loss: 4767.721675897958\n",
      "Epoch 1 step 180: training accuarcy: 0.896\n",
      "Epoch 1 step 180: training loss: 4838.330912269315\n",
      "Epoch 1 step 181: training accuarcy: 0.895\n",
      "Epoch 1 step 181: training loss: 5032.961885166954\n",
      "Epoch 1 step 182: training accuarcy: 0.8885000000000001\n",
      "Epoch 1 step 182: training loss: 4621.800271837697\n",
      "Epoch 1 step 183: training accuarcy: 0.8985\n",
      "Epoch 1 step 183: training loss: 4840.647485672379\n",
      "Epoch 1 step 184: training accuarcy: 0.894\n",
      "Epoch 1 step 184: training loss: 5041.90219815386\n",
      "Epoch 1 step 185: training accuarcy: 0.8905000000000001\n",
      "Epoch 1 step 185: training loss: 5556.215228697307\n",
      "Epoch 1 step 186: training accuarcy: 0.8795000000000001\n",
      "Epoch 1 step 186: training loss: 4832.17395842018\n",
      "Epoch 1 step 187: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 187: training loss: 5055.509505187783\n",
      "Epoch 1 step 188: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 188: training loss: 4692.585720240828\n",
      "Epoch 1 step 189: training accuarcy: 0.8985\n",
      "Epoch 1 step 189: training loss: 4644.088856749617\n",
      "Epoch 1 step 190: training accuarcy: 0.9015\n",
      "Epoch 1 step 190: training loss: 4536.8132059822065\n",
      "Epoch 1 step 191: training accuarcy: 0.902\n",
      "Epoch 1 step 191: training loss: 4734.095871293757\n",
      "Epoch 1 step 192: training accuarcy: 0.8965\n",
      "Epoch 1 step 192: training loss: 4717.948021068615\n",
      "Epoch 1 step 193: training accuarcy: 0.8965\n",
      "Epoch 1 step 193: training loss: 4958.43753043083\n",
      "Epoch 1 step 194: training accuarcy: 0.889\n",
      "Epoch 1 step 194: training loss: 4409.140614357814\n",
      "Epoch 1 step 195: training accuarcy: 0.9035\n",
      "Epoch 1 step 195: training loss: 4953.181330491224\n",
      "Epoch 1 step 196: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 196: training loss: 4644.316481326548\n",
      "Epoch 1 step 197: training accuarcy: 0.898\n",
      "Epoch 1 step 197: training loss: 4502.888007555463\n",
      "Epoch 1 step 198: training accuarcy: 0.9\n",
      "Epoch 1 step 198: training loss: 4516.041599540077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 199: training accuarcy: 0.902\n",
      "Epoch 1 step 199: training loss: 4437.2694900471615\n",
      "Epoch 1 step 200: training accuarcy: 0.903\n",
      "Epoch 1 step 200: training loss: 4317.442493134324\n",
      "Epoch 1 step 201: training accuarcy: 0.907\n",
      "Epoch 1 step 201: training loss: 4448.575159951717\n",
      "Epoch 1 step 202: training accuarcy: 0.9035\n",
      "Epoch 1 step 202: training loss: 4044.0421716500605\n",
      "Epoch 1 step 203: training accuarcy: 0.91\n",
      "Epoch 1 step 203: training loss: 4325.57591947085\n",
      "Epoch 1 step 204: training accuarcy: 0.9045\n",
      "Epoch 1 step 204: training loss: 4771.504021717737\n",
      "Epoch 1 step 205: training accuarcy: 0.8955000000000001\n",
      "Epoch 1 step 205: training loss: 4017.8563478122983\n",
      "Epoch 1 step 206: training accuarcy: 0.912\n",
      "Epoch 1 step 206: training loss: 4445.189407775164\n",
      "Epoch 1 step 207: training accuarcy: 0.9035\n",
      "Epoch 1 step 207: training loss: 4864.580427168675\n",
      "Epoch 1 step 208: training accuarcy: 0.895\n",
      "Epoch 1 step 208: training loss: 4948.820494376307\n",
      "Epoch 1 step 209: training accuarcy: 0.893\n",
      "Epoch 1 step 209: training loss: 4348.675133518382\n",
      "Epoch 1 step 210: training accuarcy: 0.9045\n",
      "Epoch 1 step 210: training loss: 3865.5018037257837\n",
      "Epoch 1 step 211: training accuarcy: 0.917\n",
      "Epoch 1 step 211: training loss: 4392.9446796010725\n",
      "Epoch 1 step 212: training accuarcy: 0.903\n",
      "Epoch 1 step 212: training loss: 4138.667419871149\n",
      "Epoch 1 step 213: training accuarcy: 0.9105\n",
      "Epoch 1 step 213: training loss: 3904.5184028966523\n",
      "Epoch 1 step 214: training accuarcy: 0.9165\n",
      "Epoch 1 step 214: training loss: 4339.829145778909\n",
      "Epoch 1 step 215: training accuarcy: 0.9065\n",
      "Epoch 1 step 215: training loss: 4076.9565733482236\n",
      "Epoch 1 step 216: training accuarcy: 0.9095\n",
      "Epoch 1 step 216: training loss: 4664.303003321647\n",
      "Epoch 1 step 217: training accuarcy: 0.8975\n",
      "Epoch 1 step 217: training loss: 4894.006540643859\n",
      "Epoch 1 step 218: training accuarcy: 0.892\n",
      "Epoch 1 step 218: training loss: 4934.201070432468\n",
      "Epoch 1 step 219: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 219: training loss: 3949.017640493126\n",
      "Epoch 1 step 220: training accuarcy: 0.9155\n",
      "Epoch 1 step 220: training loss: 4676.684893744859\n",
      "Epoch 1 step 221: training accuarcy: 0.8985\n",
      "Epoch 1 step 221: training loss: 4390.250727087697\n",
      "Epoch 1 step 222: training accuarcy: 0.9065\n",
      "Epoch 1 step 222: training loss: 4771.599997575428\n",
      "Epoch 1 step 223: training accuarcy: 0.8965\n",
      "Epoch 1 step 223: training loss: 4383.339909853162\n",
      "Epoch 1 step 224: training accuarcy: 0.905\n",
      "Epoch 1 step 224: training loss: 4614.9190738543975\n",
      "Epoch 1 step 225: training accuarcy: 0.8985\n",
      "Epoch 1 step 225: training loss: 4134.581065809408\n",
      "Epoch 1 step 226: training accuarcy: 0.9095\n",
      "Epoch 1 step 226: training loss: 4243.04952393021\n",
      "Epoch 1 step 227: training accuarcy: 0.9065\n",
      "Epoch 1 step 227: training loss: 4112.632641749686\n",
      "Epoch 1 step 228: training accuarcy: 0.9105\n",
      "Epoch 1 step 228: training loss: 4513.88207579034\n",
      "Epoch 1 step 229: training accuarcy: 0.903\n",
      "Epoch 1 step 229: training loss: 4451.830606344248\n",
      "Epoch 1 step 230: training accuarcy: 0.8977987421383649\n",
      "Epoch 1: train loss 5367.65219091605, train accuarcy 0.8827626705169678\n",
      "Epoch 1: valid loss 4367.107599841506, valid accuarcy 0.9026922583580017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████▎                                                                                                                  | 2/8 [01:41<05:05, 50.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 230: training loss: 4215.8720639266285\n",
      "Epoch 2 step 231: training accuarcy: 0.9085\n",
      "Epoch 2 step 231: training loss: 4723.774136155541\n",
      "Epoch 2 step 232: training accuarcy: 0.8975\n",
      "Epoch 2 step 232: training loss: 4471.680994361982\n",
      "Epoch 2 step 233: training accuarcy: 0.9015\n",
      "Epoch 2 step 233: training loss: 4781.803063797556\n",
      "Epoch 2 step 234: training accuarcy: 0.894\n",
      "Epoch 2 step 234: training loss: 4218.459336557109\n",
      "Epoch 2 step 235: training accuarcy: 0.9085\n",
      "Epoch 2 step 235: training loss: 4054.258202147154\n",
      "Epoch 2 step 236: training accuarcy: 0.911\n",
      "Epoch 2 step 236: training loss: 4504.010776551391\n",
      "Epoch 2 step 237: training accuarcy: 0.9005\n",
      "Epoch 2 step 237: training loss: 4383.3486332634275\n",
      "Epoch 2 step 238: training accuarcy: 0.9045\n",
      "Epoch 2 step 238: training loss: 4119.64024300951\n",
      "Epoch 2 step 239: training accuarcy: 0.91\n",
      "Epoch 2 step 239: training loss: 4060.9977116298455\n",
      "Epoch 2 step 240: training accuarcy: 0.9125\n",
      "Epoch 2 step 240: training loss: 4566.2424551550375\n",
      "Epoch 2 step 241: training accuarcy: 0.8985\n",
      "Epoch 2 step 241: training loss: 4000.155948889857\n",
      "Epoch 2 step 242: training accuarcy: 0.912\n",
      "Epoch 2 step 242: training loss: 4336.572673284414\n",
      "Epoch 2 step 243: training accuarcy: 0.9045\n",
      "Epoch 2 step 243: training loss: 4242.793895137049\n",
      "Epoch 2 step 244: training accuarcy: 0.9085\n",
      "Epoch 2 step 244: training loss: 4252.510853532583\n",
      "Epoch 2 step 245: training accuarcy: 0.9055\n",
      "Epoch 2 step 245: training loss: 4449.753238706408\n",
      "Epoch 2 step 246: training accuarcy: 0.903\n",
      "Epoch 2 step 246: training loss: 4425.625957456793\n",
      "Epoch 2 step 247: training accuarcy: 0.9035\n",
      "Epoch 2 step 247: training loss: 4904.498337441287\n",
      "Epoch 2 step 248: training accuarcy: 0.8945000000000001\n",
      "Epoch 2 step 248: training loss: 4388.538132472816\n",
      "Epoch 2 step 249: training accuarcy: 0.903\n",
      "Epoch 2 step 249: training loss: 4703.683916837566\n",
      "Epoch 2 step 250: training accuarcy: 0.9\n",
      "Epoch 2 step 250: training loss: 4412.239807804284\n",
      "Epoch 2 step 251: training accuarcy: 0.905\n",
      "Epoch 2 step 251: training loss: 3968.790649327375\n",
      "Epoch 2 step 252: training accuarcy: 0.9135\n",
      "Epoch 2 step 252: training loss: 4057.5602223857513\n",
      "Epoch 2 step 253: training accuarcy: 0.9105\n",
      "Epoch 2 step 253: training loss: 4224.2254632744325\n",
      "Epoch 2 step 254: training accuarcy: 0.9085\n",
      "Epoch 2 step 254: training loss: 4210.979162781999\n",
      "Epoch 2 step 255: training accuarcy: 0.909\n",
      "Epoch 2 step 255: training loss: 4219.81356596961\n",
      "Epoch 2 step 256: training accuarcy: 0.9095\n",
      "Epoch 2 step 256: training loss: 4203.682515172503\n",
      "Epoch 2 step 257: training accuarcy: 0.91\n",
      "Epoch 2 step 257: training loss: 3579.811563864111\n",
      "Epoch 2 step 258: training accuarcy: 0.9205\n",
      "Epoch 2 step 258: training loss: 4237.1826898309755\n",
      "Epoch 2 step 259: training accuarcy: 0.9085\n",
      "Epoch 2 step 259: training loss: 4496.799295092213\n",
      "Epoch 2 step 260: training accuarcy: 0.903\n",
      "Epoch 2 step 260: training loss: 3946.358639891133\n",
      "Epoch 2 step 261: training accuarcy: 0.912\n",
      "Epoch 2 step 261: training loss: 4425.263343743436\n",
      "Epoch 2 step 262: training accuarcy: 0.9035\n",
      "Epoch 2 step 262: training loss: 4072.8887916791905\n",
      "Epoch 2 step 263: training accuarcy: 0.911\n",
      "Epoch 2 step 263: training loss: 4919.907602381772\n",
      "Epoch 2 step 264: training accuarcy: 0.8945000000000001\n",
      "Epoch 2 step 264: training loss: 4518.386692761143\n",
      "Epoch 2 step 265: training accuarcy: 0.8995\n",
      "Epoch 2 step 265: training loss: 3794.820708930118\n",
      "Epoch 2 step 266: training accuarcy: 0.919\n",
      "Epoch 2 step 266: training loss: 4452.498248833814\n",
      "Epoch 2 step 267: training accuarcy: 0.903\n",
      "Epoch 2 step 267: training loss: 3990.8323590724026\n",
      "Epoch 2 step 268: training accuarcy: 0.9145\n",
      "Epoch 2 step 268: training loss: 4287.950329314055\n",
      "Epoch 2 step 269: training accuarcy: 0.908\n",
      "Epoch 2 step 269: training loss: 4170.840301720301\n",
      "Epoch 2 step 270: training accuarcy: 0.9095\n",
      "Epoch 2 step 270: training loss: 4112.36720480883\n",
      "Epoch 2 step 271: training accuarcy: 0.908\n",
      "Epoch 2 step 271: training loss: 4171.960202596556\n",
      "Epoch 2 step 272: training accuarcy: 0.9105\n",
      "Epoch 2 step 272: training loss: 4311.871952345307\n",
      "Epoch 2 step 273: training accuarcy: 0.906\n",
      "Epoch 2 step 273: training loss: 4525.501336309885\n",
      "Epoch 2 step 274: training accuarcy: 0.9\n",
      "Epoch 2 step 274: training loss: 4495.915186666701\n",
      "Epoch 2 step 275: training accuarcy: 0.9015\n",
      "Epoch 2 step 275: training loss: 4306.341517982897\n",
      "Epoch 2 step 276: training accuarcy: 0.9075\n",
      "Epoch 2 step 276: training loss: 3823.731584620064\n",
      "Epoch 2 step 277: training accuarcy: 0.915\n",
      "Epoch 2 step 277: training loss: 4470.320954260246\n",
      "Epoch 2 step 278: training accuarcy: 0.9035\n",
      "Epoch 2 step 278: training loss: 4224.6361744916485\n",
      "Epoch 2 step 279: training accuarcy: 0.906\n",
      "Epoch 2 step 279: training loss: 4695.7048480228605\n",
      "Epoch 2 step 280: training accuarcy: 0.8975\n",
      "Epoch 2 step 280: training loss: 4798.173260926771\n",
      "Epoch 2 step 281: training accuarcy: 0.896\n",
      "Epoch 2 step 281: training loss: 3891.1904739106067\n",
      "Epoch 2 step 282: training accuarcy: 0.915\n",
      "Epoch 2 step 282: training loss: 4668.192262499741\n",
      "Epoch 2 step 283: training accuarcy: 0.9005\n",
      "Epoch 2 step 283: training loss: 4913.115711575844\n",
      "Epoch 2 step 284: training accuarcy: 0.8915000000000001\n",
      "Epoch 2 step 284: training loss: 4335.791776174092\n",
      "Epoch 2 step 285: training accuarcy: 0.906\n",
      "Epoch 2 step 285: training loss: 3557.4357390385485\n",
      "Epoch 2 step 286: training accuarcy: 0.923\n",
      "Epoch 2 step 286: training loss: 4439.387315852875\n",
      "Epoch 2 step 287: training accuarcy: 0.9015\n",
      "Epoch 2 step 287: training loss: 3829.4305487371453\n",
      "Epoch 2 step 288: training accuarcy: 0.916\n",
      "Epoch 2 step 288: training loss: 4531.031704319208\n",
      "Epoch 2 step 289: training accuarcy: 0.902\n",
      "Epoch 2 step 289: training loss: 4116.4454602967235\n",
      "Epoch 2 step 290: training accuarcy: 0.907\n",
      "Epoch 2 step 290: training loss: 4523.421890854763\n",
      "Epoch 2 step 291: training accuarcy: 0.9025\n",
      "Epoch 2 step 291: training loss: 4457.4082637360225\n",
      "Epoch 2 step 292: training accuarcy: 0.9035\n",
      "Epoch 2 step 292: training loss: 4766.54885808216\n",
      "Epoch 2 step 293: training accuarcy: 0.896\n",
      "Epoch 2 step 293: training loss: 4426.012674177934\n",
      "Epoch 2 step 294: training accuarcy: 0.903\n",
      "Epoch 2 step 294: training loss: 4199.175518789249\n",
      "Epoch 2 step 295: training accuarcy: 0.9075\n",
      "Epoch 2 step 295: training loss: 4128.585465688407\n",
      "Epoch 2 step 296: training accuarcy: 0.9095\n",
      "Epoch 2 step 296: training loss: 4363.848060663225\n",
      "Epoch 2 step 297: training accuarcy: 0.903\n",
      "Epoch 2 step 297: training loss: 4042.3046137616816\n",
      "Epoch 2 step 298: training accuarcy: 0.9085\n",
      "Epoch 2 step 298: training loss: 4227.636537400181\n",
      "Epoch 2 step 299: training accuarcy: 0.906\n",
      "Epoch 2 step 299: training loss: 3993.5332353175604\n",
      "Epoch 2 step 300: training accuarcy: 0.91\n",
      "Epoch 2 step 300: training loss: 4174.55204511349\n",
      "Epoch 2 step 301: training accuarcy: 0.9105\n",
      "Epoch 2 step 301: training loss: 4439.132745813522\n",
      "Epoch 2 step 302: training accuarcy: 0.9015\n",
      "Epoch 2 step 302: training loss: 4259.990533905406\n",
      "Epoch 2 step 303: training accuarcy: 0.9065\n",
      "Epoch 2 step 303: training loss: 4418.321857569742\n",
      "Epoch 2 step 304: training accuarcy: 0.9055\n",
      "Epoch 2 step 304: training loss: 3849.078606411403\n",
      "Epoch 2 step 305: training accuarcy: 0.9165\n",
      "Epoch 2 step 305: training loss: 4110.125857465604\n",
      "Epoch 2 step 306: training accuarcy: 0.9115\n",
      "Epoch 2 step 306: training loss: 4401.9040437047215\n",
      "Epoch 2 step 307: training accuarcy: 0.9035\n",
      "Epoch 2 step 307: training loss: 3811.5120455663223\n",
      "Epoch 2 step 308: training accuarcy: 0.914\n",
      "Epoch 2 step 308: training loss: 3349.8397397072413\n",
      "Epoch 2 step 309: training accuarcy: 0.926\n",
      "Epoch 2 step 309: training loss: 4010.565943593778\n",
      "Epoch 2 step 310: training accuarcy: 0.913\n",
      "Epoch 2 step 310: training loss: 4078.3262459497973\n",
      "Epoch 2 step 311: training accuarcy: 0.911\n",
      "Epoch 2 step 311: training loss: 4388.821377031051\n",
      "Epoch 2 step 312: training accuarcy: 0.9055\n",
      "Epoch 2 step 312: training loss: 4013.8892225359114\n",
      "Epoch 2 step 313: training accuarcy: 0.912\n",
      "Epoch 2 step 313: training loss: 4380.717002418315\n",
      "Epoch 2 step 314: training accuarcy: 0.9025\n",
      "Epoch 2 step 314: training loss: 4661.9614417292905\n",
      "Epoch 2 step 315: training accuarcy: 0.8995\n",
      "Epoch 2 step 315: training loss: 3779.599546197081\n",
      "Epoch 2 step 316: training accuarcy: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 316: training loss: 4006.265215643417\n",
      "Epoch 2 step 317: training accuarcy: 0.912\n",
      "Epoch 2 step 317: training loss: 4447.944037410425\n",
      "Epoch 2 step 318: training accuarcy: 0.9035\n",
      "Epoch 2 step 318: training loss: 3925.5206894663143\n",
      "Epoch 2 step 319: training accuarcy: 0.9125\n",
      "Epoch 2 step 319: training loss: 3948.620543997659\n",
      "Epoch 2 step 320: training accuarcy: 0.9135\n",
      "Epoch 2 step 320: training loss: 4129.128683224954\n",
      "Epoch 2 step 321: training accuarcy: 0.911\n",
      "Epoch 2 step 321: training loss: 4108.464786006944\n",
      "Epoch 2 step 322: training accuarcy: 0.91\n",
      "Epoch 2 step 322: training loss: 3798.2596668148917\n",
      "Epoch 2 step 323: training accuarcy: 0.9155\n",
      "Epoch 2 step 323: training loss: 4056.508101981382\n",
      "Epoch 2 step 324: training accuarcy: 0.912\n",
      "Epoch 2 step 324: training loss: 3802.4398184669317\n",
      "Epoch 2 step 325: training accuarcy: 0.9165\n",
      "Epoch 2 step 325: training loss: 3733.0564202588157\n",
      "Epoch 2 step 326: training accuarcy: 0.9195\n",
      "Epoch 2 step 326: training loss: 3653.4894970112637\n",
      "Epoch 2 step 327: training accuarcy: 0.92\n",
      "Epoch 2 step 327: training loss: 3821.2419447539824\n",
      "Epoch 2 step 328: training accuarcy: 0.918\n",
      "Epoch 2 step 328: training loss: 3943.8215962160966\n",
      "Epoch 2 step 329: training accuarcy: 0.912\n",
      "Epoch 2 step 329: training loss: 3597.441779040854\n",
      "Epoch 2 step 330: training accuarcy: 0.9215\n",
      "Epoch 2 step 330: training loss: 3842.7474939930376\n",
      "Epoch 2 step 331: training accuarcy: 0.914\n",
      "Epoch 2 step 331: training loss: 3352.1751067939977\n",
      "Epoch 2 step 332: training accuarcy: 0.928\n",
      "Epoch 2 step 332: training loss: 3607.794783478853\n",
      "Epoch 2 step 333: training accuarcy: 0.9225\n",
      "Epoch 2 step 333: training loss: 3436.7771654582043\n",
      "Epoch 2 step 334: training accuarcy: 0.925\n",
      "Epoch 2 step 334: training loss: 3551.1898669883785\n",
      "Epoch 2 step 335: training accuarcy: 0.921\n",
      "Epoch 2 step 335: training loss: 4017.945528467093\n",
      "Epoch 2 step 336: training accuarcy: 0.913\n",
      "Epoch 2 step 336: training loss: 3914.2010812647786\n",
      "Epoch 2 step 337: training accuarcy: 0.9135\n",
      "Epoch 2 step 337: training loss: 3796.2057171186234\n",
      "Epoch 2 step 338: training accuarcy: 0.918\n",
      "Epoch 2 step 338: training loss: 3503.571724071683\n",
      "Epoch 2 step 339: training accuarcy: 0.9235\n",
      "Epoch 2 step 339: training loss: 3646.6921132856814\n",
      "Epoch 2 step 340: training accuarcy: 0.9195\n",
      "Epoch 2 step 340: training loss: 3850.232060314505\n",
      "Epoch 2 step 341: training accuarcy: 0.917\n",
      "Epoch 2 step 341: training loss: 3710.588149558775\n",
      "Epoch 2 step 342: training accuarcy: 0.9185\n",
      "Epoch 2 step 342: training loss: 3259.7231241815134\n",
      "Epoch 2 step 343: training accuarcy: 0.93\n",
      "Epoch 2 step 343: training loss: 3461.8864281453543\n",
      "Epoch 2 step 344: training accuarcy: 0.923\n",
      "Epoch 2 step 344: training loss: 4016.504093732824\n",
      "Epoch 2 step 345: training accuarcy: 0.9077568134171908\n",
      "Epoch 2: train loss 4151.398681129671, train accuarcy 0.909821629524231\n",
      "Epoch 2: valid loss 3615.734330088539, valid accuarcy 0.9193049073219299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████▍                                                                                               | 3/8 [02:31<04:12, 50.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 345: training loss: 3189.839048313607\n",
      "Epoch 3 step 346: training accuarcy: 0.9325\n",
      "Epoch 3 step 346: training loss: 3681.3170867177123\n",
      "Epoch 3 step 347: training accuarcy: 0.92\n",
      "Epoch 3 step 347: training loss: 3898.511623527658\n",
      "Epoch 3 step 348: training accuarcy: 0.9145\n",
      "Epoch 3 step 348: training loss: 3684.8510818196087\n",
      "Epoch 3 step 349: training accuarcy: 0.9225\n",
      "Epoch 3 step 349: training loss: 3552.573568508522\n",
      "Epoch 3 step 350: training accuarcy: 0.9205\n",
      "Epoch 3 step 350: training loss: 3225.9386116131295\n",
      "Epoch 3 step 351: training accuarcy: 0.929\n",
      "Epoch 3 step 351: training loss: 3736.507296955722\n",
      "Epoch 3 step 352: training accuarcy: 0.9185\n",
      "Epoch 3 step 352: training loss: 3438.446716119369\n",
      "Epoch 3 step 353: training accuarcy: 0.924\n",
      "Epoch 3 step 353: training loss: 4007.2340586984105\n",
      "Epoch 3 step 354: training accuarcy: 0.9125\n",
      "Epoch 3 step 354: training loss: 3470.6937029188625\n",
      "Epoch 3 step 355: training accuarcy: 0.925\n",
      "Epoch 3 step 355: training loss: 3895.363916737927\n",
      "Epoch 3 step 356: training accuarcy: 0.917\n",
      "Epoch 3 step 356: training loss: 3369.542382025693\n",
      "Epoch 3 step 357: training accuarcy: 0.926\n",
      "Epoch 3 step 357: training loss: 3955.4405720480995\n",
      "Epoch 3 step 358: training accuarcy: 0.9125\n",
      "Epoch 3 step 358: training loss: 3364.472243078654\n",
      "Epoch 3 step 359: training accuarcy: 0.9265\n",
      "Epoch 3 step 359: training loss: 3807.3045186628806\n",
      "Epoch 3 step 360: training accuarcy: 0.9145\n",
      "Epoch 3 step 360: training loss: 4048.844145348843\n",
      "Epoch 3 step 361: training accuarcy: 0.9115\n",
      "Epoch 3 step 361: training loss: 3808.1124706206465\n",
      "Epoch 3 step 362: training accuarcy: 0.918\n",
      "Epoch 3 step 362: training loss: 3383.932780028374\n",
      "Epoch 3 step 363: training accuarcy: 0.9285\n",
      "Epoch 3 step 363: training loss: 3397.4663590018\n",
      "Epoch 3 step 364: training accuarcy: 0.9245\n",
      "Epoch 3 step 364: training loss: 3716.1012682582245\n",
      "Epoch 3 step 365: training accuarcy: 0.92\n",
      "Epoch 3 step 365: training loss: 3984.5138966650575\n",
      "Epoch 3 step 366: training accuarcy: 0.9125\n",
      "Epoch 3 step 366: training loss: 3545.0883443368166\n",
      "Epoch 3 step 367: training accuarcy: 0.922\n",
      "Epoch 3 step 367: training loss: 3296.4245677916833\n",
      "Epoch 3 step 368: training accuarcy: 0.9275\n",
      "Epoch 3 step 368: training loss: 3506.743373190657\n",
      "Epoch 3 step 369: training accuarcy: 0.923\n",
      "Epoch 3 step 369: training loss: 3770.2517647529257\n",
      "Epoch 3 step 370: training accuarcy: 0.918\n",
      "Epoch 3 step 370: training loss: 3818.989132453283\n",
      "Epoch 3 step 371: training accuarcy: 0.9175\n",
      "Epoch 3 step 371: training loss: 3632.3620794393432\n",
      "Epoch 3 step 372: training accuarcy: 0.92\n",
      "Epoch 3 step 372: training loss: 3406.6813511866694\n",
      "Epoch 3 step 373: training accuarcy: 0.9255\n",
      "Epoch 3 step 373: training loss: 3564.7373392225104\n",
      "Epoch 3 step 374: training accuarcy: 0.9205\n",
      "Epoch 3 step 374: training loss: 3531.0803817965334\n",
      "Epoch 3 step 375: training accuarcy: 0.9205\n",
      "Epoch 3 step 375: training loss: 3523.7103779742997\n",
      "Epoch 3 step 376: training accuarcy: 0.9195\n",
      "Epoch 3 step 376: training loss: 3120.5324483839413\n",
      "Epoch 3 step 377: training accuarcy: 0.9325\n",
      "Epoch 3 step 377: training loss: 3034.4407260268113\n",
      "Epoch 3 step 378: training accuarcy: 0.9335\n",
      "Epoch 3 step 378: training loss: 3869.7496861866935\n",
      "Epoch 3 step 379: training accuarcy: 0.915\n",
      "Epoch 3 step 379: training loss: 3817.780091269771\n",
      "Epoch 3 step 380: training accuarcy: 0.917\n",
      "Epoch 3 step 380: training loss: 3471.106011776109\n",
      "Epoch 3 step 381: training accuarcy: 0.9255\n",
      "Epoch 3 step 381: training loss: 3088.8581870197063\n",
      "Epoch 3 step 382: training accuarcy: 0.934\n",
      "Epoch 3 step 382: training loss: 3476.9777072025345\n",
      "Epoch 3 step 383: training accuarcy: 0.925\n",
      "Epoch 3 step 383: training loss: 3744.042283349624\n",
      "Epoch 3 step 384: training accuarcy: 0.92\n",
      "Epoch 3 step 384: training loss: 3637.9713315894337\n",
      "Epoch 3 step 385: training accuarcy: 0.9205\n",
      "Epoch 3 step 385: training loss: 3290.2411639695433\n",
      "Epoch 3 step 386: training accuarcy: 0.929\n",
      "Epoch 3 step 386: training loss: 3589.7570893622824\n",
      "Epoch 3 step 387: training accuarcy: 0.921\n",
      "Epoch 3 step 387: training loss: 3797.8105214334423\n",
      "Epoch 3 step 388: training accuarcy: 0.9165\n",
      "Epoch 3 step 388: training loss: 3493.1587843819034\n",
      "Epoch 3 step 389: training accuarcy: 0.923\n",
      "Epoch 3 step 389: training loss: 3102.632559303359\n",
      "Epoch 3 step 390: training accuarcy: 0.9315\n",
      "Epoch 3 step 390: training loss: 3851.97209597376\n",
      "Epoch 3 step 391: training accuarcy: 0.917\n",
      "Epoch 3 step 391: training loss: 3845.2127497443753\n",
      "Epoch 3 step 392: training accuarcy: 0.917\n",
      "Epoch 3 step 392: training loss: 3340.6728866744606\n",
      "Epoch 3 step 393: training accuarcy: 0.928\n",
      "Epoch 3 step 393: training loss: 3791.4226109567403\n",
      "Epoch 3 step 394: training accuarcy: 0.919\n",
      "Epoch 3 step 394: training loss: 3380.966214728615\n",
      "Epoch 3 step 395: training accuarcy: 0.925\n",
      "Epoch 3 step 395: training loss: 3418.2288710996195\n",
      "Epoch 3 step 396: training accuarcy: 0.926\n",
      "Epoch 3 step 396: training loss: 3261.446769083499\n",
      "Epoch 3 step 397: training accuarcy: 0.9295\n",
      "Epoch 3 step 397: training loss: 3679.795221483999\n",
      "Epoch 3 step 398: training accuarcy: 0.922\n",
      "Epoch 3 step 398: training loss: 3503.4030196805\n",
      "Epoch 3 step 399: training accuarcy: 0.924\n",
      "Epoch 3 step 399: training loss: 3154.8595062807567\n",
      "Epoch 3 step 400: training accuarcy: 0.9295\n",
      "Epoch 3 step 400: training loss: 3063.193157857378\n",
      "Epoch 3 step 401: training accuarcy: 0.9345\n",
      "Epoch 3 step 401: training loss: 3597.1900680609656\n",
      "Epoch 3 step 402: training accuarcy: 0.923\n",
      "Epoch 3 step 402: training loss: 3318.1672390981485\n",
      "Epoch 3 step 403: training accuarcy: 0.9275\n",
      "Epoch 3 step 403: training loss: 3147.1262963573477\n",
      "Epoch 3 step 404: training accuarcy: 0.932\n",
      "Epoch 3 step 404: training loss: 3271.547868202968\n",
      "Epoch 3 step 405: training accuarcy: 0.9275\n",
      "Epoch 3 step 405: training loss: 3339.007849099582\n",
      "Epoch 3 step 406: training accuarcy: 0.928\n",
      "Epoch 3 step 406: training loss: 3424.9527364932383\n",
      "Epoch 3 step 407: training accuarcy: 0.926\n",
      "Epoch 3 step 407: training loss: 3504.2917537143585\n",
      "Epoch 3 step 408: training accuarcy: 0.9225\n",
      "Epoch 3 step 408: training loss: 3473.336614571881\n",
      "Epoch 3 step 409: training accuarcy: 0.9245\n",
      "Epoch 3 step 409: training loss: 3453.3749976272147\n",
      "Epoch 3 step 410: training accuarcy: 0.923\n",
      "Epoch 3 step 410: training loss: 2835.4617984873425\n",
      "Epoch 3 step 411: training accuarcy: 0.937\n",
      "Epoch 3 step 411: training loss: 3252.568303006366\n",
      "Epoch 3 step 412: training accuarcy: 0.9285\n",
      "Epoch 3 step 412: training loss: 3503.301904451384\n",
      "Epoch 3 step 413: training accuarcy: 0.9235\n",
      "Epoch 3 step 413: training loss: 3082.413061736575\n",
      "Epoch 3 step 414: training accuarcy: 0.9305\n",
      "Epoch 3 step 414: training loss: 2858.6316984912582\n",
      "Epoch 3 step 415: training accuarcy: 0.9375\n",
      "Epoch 3 step 415: training loss: 3266.645336807006\n",
      "Epoch 3 step 416: training accuarcy: 0.9295\n",
      "Epoch 3 step 416: training loss: 3224.9252863896236\n",
      "Epoch 3 step 417: training accuarcy: 0.9295\n",
      "Epoch 3 step 417: training loss: 3097.2733763122746\n",
      "Epoch 3 step 418: training accuarcy: 0.932\n",
      "Epoch 3 step 418: training loss: 2691.190877883467\n",
      "Epoch 3 step 419: training accuarcy: 0.9400000000000001\n",
      "Epoch 3 step 419: training loss: 3184.42359550325\n",
      "Epoch 3 step 420: training accuarcy: 0.9315\n",
      "Epoch 3 step 420: training loss: 2943.4892211671063\n",
      "Epoch 3 step 421: training accuarcy: 0.936\n",
      "Epoch 3 step 421: training loss: 3565.09740046334\n",
      "Epoch 3 step 422: training accuarcy: 0.9195\n",
      "Epoch 3 step 422: training loss: 2855.7210250386606\n",
      "Epoch 3 step 423: training accuarcy: 0.9345\n",
      "Epoch 3 step 423: training loss: 2645.25584094798\n",
      "Epoch 3 step 424: training accuarcy: 0.9425\n",
      "Epoch 3 step 424: training loss: 2742.5144646378576\n",
      "Epoch 3 step 425: training accuarcy: 0.9410000000000001\n",
      "Epoch 3 step 425: training loss: 2831.208460117818\n",
      "Epoch 3 step 426: training accuarcy: 0.9365\n",
      "Epoch 3 step 426: training loss: 3051.0415242449726\n",
      "Epoch 3 step 427: training accuarcy: 0.9355\n",
      "Epoch 3 step 427: training loss: 2761.284358613596\n",
      "Epoch 3 step 428: training accuarcy: 0.9385\n",
      "Epoch 3 step 428: training loss: 2289.2329397846765\n",
      "Epoch 3 step 429: training accuarcy: 0.9500000000000001\n",
      "Epoch 3 step 429: training loss: 2553.7046740020032\n",
      "Epoch 3 step 430: training accuarcy: 0.9440000000000001\n",
      "Epoch 3 step 430: training loss: 2426.9744239381416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 431: training accuarcy: 0.9475\n",
      "Epoch 3 step 431: training loss: 2535.3518418400095\n",
      "Epoch 3 step 432: training accuarcy: 0.9460000000000001\n",
      "Epoch 3 step 432: training loss: 2462.4772886825003\n",
      "Epoch 3 step 433: training accuarcy: 0.9475\n",
      "Epoch 3 step 433: training loss: 2544.6880908742064\n",
      "Epoch 3 step 434: training accuarcy: 0.9445\n",
      "Epoch 3 step 434: training loss: 2482.8236561022477\n",
      "Epoch 3 step 435: training accuarcy: 0.9450000000000001\n",
      "Epoch 3 step 435: training loss: 2812.63542581057\n",
      "Epoch 3 step 436: training accuarcy: 0.9395\n",
      "Epoch 3 step 436: training loss: 2735.476176224429\n",
      "Epoch 3 step 437: training accuarcy: 0.9415\n",
      "Epoch 3 step 437: training loss: 3063.031730467118\n",
      "Epoch 3 step 438: training accuarcy: 0.9345\n",
      "Epoch 3 step 438: training loss: 3081.223216266974\n",
      "Epoch 3 step 439: training accuarcy: 0.9325\n",
      "Epoch 3 step 439: training loss: 2548.057007208557\n",
      "Epoch 3 step 440: training accuarcy: 0.9445\n",
      "Epoch 3 step 440: training loss: 2713.914674284374\n",
      "Epoch 3 step 441: training accuarcy: 0.9405\n",
      "Epoch 3 step 441: training loss: 3061.7606879674845\n",
      "Epoch 3 step 442: training accuarcy: 0.9345\n",
      "Epoch 3 step 442: training loss: 3085.0595611454683\n",
      "Epoch 3 step 443: training accuarcy: 0.9305\n",
      "Epoch 3 step 443: training loss: 2970.884941728086\n",
      "Epoch 3 step 444: training accuarcy: 0.9355\n",
      "Epoch 3 step 444: training loss: 2897.994385894092\n",
      "Epoch 3 step 445: training accuarcy: 0.9335\n",
      "Epoch 3 step 445: training loss: 2763.6457057671223\n",
      "Epoch 3 step 446: training accuarcy: 0.9390000000000001\n",
      "Epoch 3 step 446: training loss: 2449.7815206204746\n",
      "Epoch 3 step 447: training accuarcy: 0.9440000000000001\n",
      "Epoch 3 step 447: training loss: 3329.656077091545\n",
      "Epoch 3 step 448: training accuarcy: 0.9295\n",
      "Epoch 3 step 448: training loss: 2530.789919217459\n",
      "Epoch 3 step 449: training accuarcy: 0.9450000000000001\n",
      "Epoch 3 step 449: training loss: 2591.2454156075787\n",
      "Epoch 3 step 450: training accuarcy: 0.9425\n",
      "Epoch 3 step 450: training loss: 2641.242563325569\n",
      "Epoch 3 step 451: training accuarcy: 0.9420000000000001\n",
      "Epoch 3 step 451: training loss: 2573.943509956748\n",
      "Epoch 3 step 452: training accuarcy: 0.9430000000000001\n",
      "Epoch 3 step 452: training loss: 2751.9136155231367\n",
      "Epoch 3 step 453: training accuarcy: 0.9400000000000001\n",
      "Epoch 3 step 453: training loss: 2421.0149809795785\n",
      "Epoch 3 step 454: training accuarcy: 0.9470000000000001\n",
      "Epoch 3 step 454: training loss: 2670.12199624407\n",
      "Epoch 3 step 455: training accuarcy: 0.9395\n",
      "Epoch 3 step 455: training loss: 2501.3128573092786\n",
      "Epoch 3 step 456: training accuarcy: 0.9480000000000001\n",
      "Epoch 3 step 456: training loss: 2574.2234948776795\n",
      "Epoch 3 step 457: training accuarcy: 0.9415\n",
      "Epoch 3 step 457: training loss: 2683.4539578572144\n",
      "Epoch 3 step 458: training accuarcy: 0.9440000000000001\n",
      "Epoch 3 step 458: training loss: 2446.0471070397753\n",
      "Epoch 3 step 459: training accuarcy: 0.9485\n",
      "Epoch 3 step 459: training loss: 2180.4850276497314\n",
      "Epoch 3 step 460: training accuarcy: 0.9507337526205452\n",
      "Epoch 3: train loss 3211.60799289953, train accuarcy 0.9297798871994019\n",
      "Epoch 3: valid loss 2393.415272551158, valid accuarcy 0.9467740058898926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 4/8 [03:21<03:21, 50.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 460: training loss: 2599.9246059905813\n",
      "Epoch 4 step 461: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 461: training loss: 2855.3756191131733\n",
      "Epoch 4 step 462: training accuarcy: 0.936\n",
      "Epoch 4 step 462: training loss: 2525.1371951679403\n",
      "Epoch 4 step 463: training accuarcy: 0.9445\n",
      "Epoch 4 step 463: training loss: 2732.339218165611\n",
      "Epoch 4 step 464: training accuarcy: 0.9425\n",
      "Epoch 4 step 464: training loss: 2418.457727311047\n",
      "Epoch 4 step 465: training accuarcy: 0.9475\n",
      "Epoch 4 step 465: training loss: 3012.5823254887723\n",
      "Epoch 4 step 466: training accuarcy: 0.934\n",
      "Epoch 4 step 466: training loss: 2288.9122966289483\n",
      "Epoch 4 step 467: training accuarcy: 0.9495\n",
      "Epoch 4 step 467: training loss: 2125.497413423672\n",
      "Epoch 4 step 468: training accuarcy: 0.9515\n",
      "Epoch 4 step 468: training loss: 2674.807992217363\n",
      "Epoch 4 step 469: training accuarcy: 0.9405\n",
      "Epoch 4 step 469: training loss: 2545.7389850913846\n",
      "Epoch 4 step 470: training accuarcy: 0.9445\n",
      "Epoch 4 step 470: training loss: 2618.7491495121008\n",
      "Epoch 4 step 471: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 471: training loss: 2331.4561166735853\n",
      "Epoch 4 step 472: training accuarcy: 0.9500000000000001\n",
      "Epoch 4 step 472: training loss: 2421.744323327983\n",
      "Epoch 4 step 473: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 473: training loss: 2302.3769746102676\n",
      "Epoch 4 step 474: training accuarcy: 0.9485\n",
      "Epoch 4 step 474: training loss: 2367.5124165456414\n",
      "Epoch 4 step 475: training accuarcy: 0.9505\n",
      "Epoch 4 step 475: training loss: 2671.641621819247\n",
      "Epoch 4 step 476: training accuarcy: 0.9415\n",
      "Epoch 4 step 476: training loss: 2956.2664312771717\n",
      "Epoch 4 step 477: training accuarcy: 0.9355\n",
      "Epoch 4 step 477: training loss: 2605.47867461668\n",
      "Epoch 4 step 478: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 478: training loss: 2485.1160994645934\n",
      "Epoch 4 step 479: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 479: training loss: 2532.600337632198\n",
      "Epoch 4 step 480: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 480: training loss: 2492.5831405738145\n",
      "Epoch 4 step 481: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 481: training loss: 2270.858546114414\n",
      "Epoch 4 step 482: training accuarcy: 0.9495\n",
      "Epoch 4 step 482: training loss: 2433.8700065042076\n",
      "Epoch 4 step 483: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 483: training loss: 2546.789764210898\n",
      "Epoch 4 step 484: training accuarcy: 0.9445\n",
      "Epoch 4 step 484: training loss: 2885.707543253732\n",
      "Epoch 4 step 485: training accuarcy: 0.9375\n",
      "Epoch 4 step 485: training loss: 2717.023721490324\n",
      "Epoch 4 step 486: training accuarcy: 0.9415\n",
      "Epoch 4 step 486: training loss: 2474.9173362725287\n",
      "Epoch 4 step 487: training accuarcy: 0.9455\n",
      "Epoch 4 step 487: training loss: 2535.322122828067\n",
      "Epoch 4 step 488: training accuarcy: 0.9455\n",
      "Epoch 4 step 488: training loss: 2247.33867079388\n",
      "Epoch 4 step 489: training accuarcy: 0.9465\n",
      "Epoch 4 step 489: training loss: 2579.8320765514404\n",
      "Epoch 4 step 490: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 490: training loss: 2295.6749294618917\n",
      "Epoch 4 step 491: training accuarcy: 0.9500000000000001\n",
      "Epoch 4 step 491: training loss: 2782.0935166791983\n",
      "Epoch 4 step 492: training accuarcy: 0.9385\n",
      "Epoch 4 step 492: training loss: 2280.7583987496996\n",
      "Epoch 4 step 493: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 493: training loss: 2784.3503466767074\n",
      "Epoch 4 step 494: training accuarcy: 0.9385\n",
      "Epoch 4 step 494: training loss: 2986.8186300490984\n",
      "Epoch 4 step 495: training accuarcy: 0.9375\n",
      "Epoch 4 step 495: training loss: 2204.4934453939422\n",
      "Epoch 4 step 496: training accuarcy: 0.9530000000000001\n",
      "Epoch 4 step 496: training loss: 2208.4478124946486\n",
      "Epoch 4 step 497: training accuarcy: 0.9535\n",
      "Epoch 4 step 497: training loss: 2411.0434058522146\n",
      "Epoch 4 step 498: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 498: training loss: 2369.870321904125\n",
      "Epoch 4 step 499: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 499: training loss: 2511.2066155088078\n",
      "Epoch 4 step 500: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 500: training loss: 2874.3129569657976\n",
      "Epoch 4 step 501: training accuarcy: 0.9375\n",
      "Epoch 4 step 501: training loss: 2252.702905988128\n",
      "Epoch 4 step 502: training accuarcy: 0.9510000000000001\n",
      "Epoch 4 step 502: training loss: 2472.9514562979252\n",
      "Epoch 4 step 503: training accuarcy: 0.9475\n",
      "Epoch 4 step 503: training loss: 2594.191580346769\n",
      "Epoch 4 step 504: training accuarcy: 0.9425\n",
      "Epoch 4 step 504: training loss: 2062.444639820933\n",
      "Epoch 4 step 505: training accuarcy: 0.9545\n",
      "Epoch 4 step 505: training loss: 2261.314921619352\n",
      "Epoch 4 step 506: training accuarcy: 0.9500000000000001\n",
      "Epoch 4 step 506: training loss: 2189.919135867086\n",
      "Epoch 4 step 507: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 507: training loss: 2365.3041779639775\n",
      "Epoch 4 step 508: training accuarcy: 0.9490000000000001\n",
      "Epoch 4 step 508: training loss: 2643.0788415186776\n",
      "Epoch 4 step 509: training accuarcy: 0.9425\n",
      "Epoch 4 step 509: training loss: 2486.227390253756\n",
      "Epoch 4 step 510: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 510: training loss: 2439.1544822889955\n",
      "Epoch 4 step 511: training accuarcy: 0.9455\n",
      "Epoch 4 step 511: training loss: 2366.49759320588\n",
      "Epoch 4 step 512: training accuarcy: 0.9490000000000001\n",
      "Epoch 4 step 512: training loss: 2792.851849864545\n",
      "Epoch 4 step 513: training accuarcy: 0.9405\n",
      "Epoch 4 step 513: training loss: 2271.5429840395163\n",
      "Epoch 4 step 514: training accuarcy: 0.9515\n",
      "Epoch 4 step 514: training loss: 2270.5830936368816\n",
      "Epoch 4 step 515: training accuarcy: 0.9515\n",
      "Epoch 4 step 515: training loss: 2371.076621811413\n",
      "Epoch 4 step 516: training accuarcy: 0.9475\n",
      "Epoch 4 step 516: training loss: 2476.630015415193\n",
      "Epoch 4 step 517: training accuarcy: 0.9475\n",
      "Epoch 4 step 517: training loss: 1966.4392039950596\n",
      "Epoch 4 step 518: training accuarcy: 0.9555\n",
      "Epoch 4 step 518: training loss: 2360.555996625321\n",
      "Epoch 4 step 519: training accuarcy: 0.9500000000000001\n",
      "Epoch 4 step 519: training loss: 2209.4532378547565\n",
      "Epoch 4 step 520: training accuarcy: 0.9510000000000001\n",
      "Epoch 4 step 520: training loss: 2211.939891560602\n",
      "Epoch 4 step 521: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 521: training loss: 2284.4653696572313\n",
      "Epoch 4 step 522: training accuarcy: 0.9505\n",
      "Epoch 4 step 522: training loss: 2280.4468075523\n",
      "Epoch 4 step 523: training accuarcy: 0.9505\n",
      "Epoch 4 step 523: training loss: 2497.905219254146\n",
      "Epoch 4 step 524: training accuarcy: 0.9445\n",
      "Epoch 4 step 524: training loss: 1922.0120640824657\n",
      "Epoch 4 step 525: training accuarcy: 0.9605\n",
      "Epoch 4 step 525: training loss: 2156.910332269055\n",
      "Epoch 4 step 526: training accuarcy: 0.9540000000000001\n",
      "Epoch 4 step 526: training loss: 1904.106111323708\n",
      "Epoch 4 step 527: training accuarcy: 0.9580000000000001\n",
      "Epoch 4 step 527: training loss: 2428.0057059536534\n",
      "Epoch 4 step 528: training accuarcy: 0.9485\n",
      "Epoch 4 step 528: training loss: 2100.634517310414\n",
      "Epoch 4 step 529: training accuarcy: 0.9530000000000001\n",
      "Epoch 4 step 529: training loss: 2394.151507471097\n",
      "Epoch 4 step 530: training accuarcy: 0.9505\n",
      "Epoch 4 step 530: training loss: 2305.69608975661\n",
      "Epoch 4 step 531: training accuarcy: 0.9505\n",
      "Epoch 4 step 531: training loss: 2347.359681308286\n",
      "Epoch 4 step 532: training accuarcy: 0.9465\n",
      "Epoch 4 step 532: training loss: 2342.0927759047713\n",
      "Epoch 4 step 533: training accuarcy: 0.9495\n",
      "Epoch 4 step 533: training loss: 2255.183791710641\n",
      "Epoch 4 step 534: training accuarcy: 0.9525\n",
      "Epoch 4 step 534: training loss: 2178.4254632897755\n",
      "Epoch 4 step 535: training accuarcy: 0.9515\n",
      "Epoch 4 step 535: training loss: 1850.5946311968082\n",
      "Epoch 4 step 536: training accuarcy: 0.9585\n",
      "Epoch 4 step 536: training loss: 2113.9589236970314\n",
      "Epoch 4 step 537: training accuarcy: 0.9525\n",
      "Epoch 4 step 537: training loss: 2370.0045823077567\n",
      "Epoch 4 step 538: training accuarcy: 0.9495\n",
      "Epoch 4 step 538: training loss: 2595.1354815924174\n",
      "Epoch 4 step 539: training accuarcy: 0.9435\n",
      "Epoch 4 step 539: training loss: 1944.6606907705968\n",
      "Epoch 4 step 540: training accuarcy: 0.9595\n",
      "Epoch 4 step 540: training loss: 2366.7011622881278\n",
      "Epoch 4 step 541: training accuarcy: 0.9490000000000001\n",
      "Epoch 4 step 541: training loss: 2372.3557440244153\n",
      "Epoch 4 step 542: training accuarcy: 0.9485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 542: training loss: 2194.6101500522905\n",
      "Epoch 4 step 543: training accuarcy: 0.9505\n",
      "Epoch 4 step 543: training loss: 2243.567321934124\n",
      "Epoch 4 step 544: training accuarcy: 0.9505\n",
      "Epoch 4 step 544: training loss: 1934.258387348529\n",
      "Epoch 4 step 545: training accuarcy: 0.9590000000000001\n",
      "Epoch 4 step 545: training loss: 2390.9349499712876\n",
      "Epoch 4 step 546: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 546: training loss: 2280.0711008479866\n",
      "Epoch 4 step 547: training accuarcy: 0.9500000000000001\n",
      "Epoch 4 step 547: training loss: 2324.721811380932\n",
      "Epoch 4 step 548: training accuarcy: 0.9455\n",
      "Epoch 4 step 548: training loss: 2388.3845697906386\n",
      "Epoch 4 step 549: training accuarcy: 0.9500000000000001\n",
      "Epoch 4 step 549: training loss: 2102.2666000317195\n",
      "Epoch 4 step 550: training accuarcy: 0.9560000000000001\n",
      "Epoch 4 step 550: training loss: 2184.273355275576\n",
      "Epoch 4 step 551: training accuarcy: 0.9540000000000001\n",
      "Epoch 4 step 551: training loss: 2204.591182987447\n",
      "Epoch 4 step 552: training accuarcy: 0.9505\n",
      "Epoch 4 step 552: training loss: 2503.18061743048\n",
      "Epoch 4 step 553: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 553: training loss: 1965.0241134589119\n",
      "Epoch 4 step 554: training accuarcy: 0.9575\n",
      "Epoch 4 step 554: training loss: 2353.9399013347847\n",
      "Epoch 4 step 555: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 555: training loss: 2003.9947427230813\n",
      "Epoch 4 step 556: training accuarcy: 0.9570000000000001\n",
      "Epoch 4 step 556: training loss: 2251.8955772386453\n",
      "Epoch 4 step 557: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 557: training loss: 2248.439414862254\n",
      "Epoch 4 step 558: training accuarcy: 0.9505\n",
      "Epoch 4 step 558: training loss: 1798.0301348905978\n",
      "Epoch 4 step 559: training accuarcy: 0.9595\n",
      "Epoch 4 step 559: training loss: 2083.283189400008\n",
      "Epoch 4 step 560: training accuarcy: 0.9525\n",
      "Epoch 4 step 560: training loss: 2230.6859939335823\n",
      "Epoch 4 step 561: training accuarcy: 0.9495\n",
      "Epoch 4 step 561: training loss: 2386.3757626936685\n",
      "Epoch 4 step 562: training accuarcy: 0.9495\n",
      "Epoch 4 step 562: training loss: 1844.4430880391324\n",
      "Epoch 4 step 563: training accuarcy: 0.96\n",
      "Epoch 4 step 563: training loss: 1949.0447500023715\n",
      "Epoch 4 step 564: training accuarcy: 0.9560000000000001\n",
      "Epoch 4 step 564: training loss: 2070.546223723527\n",
      "Epoch 4 step 565: training accuarcy: 0.9525\n",
      "Epoch 4 step 565: training loss: 1815.3909685378708\n",
      "Epoch 4 step 566: training accuarcy: 0.9590000000000001\n",
      "Epoch 4 step 566: training loss: 1882.3024777899566\n",
      "Epoch 4 step 567: training accuarcy: 0.9595\n",
      "Epoch 4 step 567: training loss: 2440.057362999357\n",
      "Epoch 4 step 568: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 568: training loss: 2473.45760977772\n",
      "Epoch 4 step 569: training accuarcy: 0.9465\n",
      "Epoch 4 step 569: training loss: 2278.583308659308\n",
      "Epoch 4 step 570: training accuarcy: 0.9500000000000001\n",
      "Epoch 4 step 570: training loss: 2021.1265541871892\n",
      "Epoch 4 step 571: training accuarcy: 0.9530000000000001\n",
      "Epoch 4 step 571: training loss: 2154.445584962849\n",
      "Epoch 4 step 572: training accuarcy: 0.9510000000000001\n",
      "Epoch 4 step 572: training loss: 1933.2787132774088\n",
      "Epoch 4 step 573: training accuarcy: 0.9580000000000001\n",
      "Epoch 4 step 573: training loss: 2651.582242424242\n",
      "Epoch 4 step 574: training accuarcy: 0.9415\n",
      "Epoch 4 step 574: training loss: 1777.6208761954547\n",
      "Epoch 4 step 575: training accuarcy: 0.9601677148846961\n",
      "Epoch 4: train loss 2342.653079506666, train accuarcy 0.9494801759719849\n",
      "Epoch 4: valid loss 2015.4854710187121, valid accuarcy 0.9552260637283325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 5/8 [04:10<02:30, 50.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 575: training loss: 2140.0531093962195\n",
      "Epoch 5 step 576: training accuarcy: 0.9520000000000001\n",
      "Epoch 5 step 576: training loss: 1807.3905960293455\n",
      "Epoch 5 step 577: training accuarcy: 0.962\n",
      "Epoch 5 step 577: training loss: 1724.5173608838888\n",
      "Epoch 5 step 578: training accuarcy: 0.961\n",
      "Epoch 5 step 578: training loss: 2225.9616849495646\n",
      "Epoch 5 step 579: training accuarcy: 0.9505\n",
      "Epoch 5 step 579: training loss: 1930.5615093248837\n",
      "Epoch 5 step 580: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 580: training loss: 2792.3604405554242\n",
      "Epoch 5 step 581: training accuarcy: 0.9385\n",
      "Epoch 5 step 581: training loss: 1728.175751653015\n",
      "Epoch 5 step 582: training accuarcy: 0.9625\n",
      "Epoch 5 step 582: training loss: 2249.761471985503\n",
      "Epoch 5 step 583: training accuarcy: 0.9485\n",
      "Epoch 5 step 583: training loss: 1702.8908212821193\n",
      "Epoch 5 step 584: training accuarcy: 0.9615\n",
      "Epoch 5 step 584: training loss: 1797.912552790028\n",
      "Epoch 5 step 585: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 585: training loss: 2333.2392395673787\n",
      "Epoch 5 step 586: training accuarcy: 0.9485\n",
      "Epoch 5 step 586: training loss: 2138.057448130463\n",
      "Epoch 5 step 587: training accuarcy: 0.9520000000000001\n",
      "Epoch 5 step 587: training loss: 2357.694866049128\n",
      "Epoch 5 step 588: training accuarcy: 0.9495\n",
      "Epoch 5 step 588: training loss: 1950.2057314111378\n",
      "Epoch 5 step 589: training accuarcy: 0.9595\n",
      "Epoch 5 step 589: training loss: 2326.695605276319\n",
      "Epoch 5 step 590: training accuarcy: 0.9485\n",
      "Epoch 5 step 590: training loss: 2314.7798721294853\n",
      "Epoch 5 step 591: training accuarcy: 0.9500000000000001\n",
      "Epoch 5 step 591: training loss: 1913.6917531171562\n",
      "Epoch 5 step 592: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 592: training loss: 1926.6193763319086\n",
      "Epoch 5 step 593: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 593: training loss: 1586.311605964493\n",
      "Epoch 5 step 594: training accuarcy: 0.965\n",
      "Epoch 5 step 594: training loss: 2020.7418136242527\n",
      "Epoch 5 step 595: training accuarcy: 0.9545\n",
      "Epoch 5 step 595: training loss: 2388.9278387700638\n",
      "Epoch 5 step 596: training accuarcy: 0.9460000000000001\n",
      "Epoch 5 step 596: training loss: 1681.1920372315708\n",
      "Epoch 5 step 597: training accuarcy: 0.9645\n",
      "Epoch 5 step 597: training loss: 1831.8152603407193\n",
      "Epoch 5 step 598: training accuarcy: 0.9605\n",
      "Epoch 5 step 598: training loss: 1943.1811351706301\n",
      "Epoch 5 step 599: training accuarcy: 0.9570000000000001\n",
      "Epoch 5 step 599: training loss: 2135.499875011223\n",
      "Epoch 5 step 600: training accuarcy: 0.9550000000000001\n",
      "Epoch 5 step 600: training loss: 2022.871831352335\n",
      "Epoch 5 step 601: training accuarcy: 0.9570000000000001\n",
      "Epoch 5 step 601: training loss: 1894.9635939695695\n",
      "Epoch 5 step 602: training accuarcy: 0.9585\n",
      "Epoch 5 step 602: training loss: 2071.4281918208444\n",
      "Epoch 5 step 603: training accuarcy: 0.9550000000000001\n",
      "Epoch 5 step 603: training loss: 2006.749417105675\n",
      "Epoch 5 step 604: training accuarcy: 0.9570000000000001\n",
      "Epoch 5 step 604: training loss: 2265.0281808069817\n",
      "Epoch 5 step 605: training accuarcy: 0.9510000000000001\n",
      "Epoch 5 step 605: training loss: 2294.1238361136675\n",
      "Epoch 5 step 606: training accuarcy: 0.9505\n",
      "Epoch 5 step 606: training loss: 2206.5957878102263\n",
      "Epoch 5 step 607: training accuarcy: 0.9520000000000001\n",
      "Epoch 5 step 607: training loss: 1900.9317369821563\n",
      "Epoch 5 step 608: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 608: training loss: 2257.124113320508\n",
      "Epoch 5 step 609: training accuarcy: 0.9480000000000001\n",
      "Epoch 5 step 609: training loss: 1939.5146588462135\n",
      "Epoch 5 step 610: training accuarcy: 0.9570000000000001\n",
      "Epoch 5 step 610: training loss: 1563.4605801523019\n",
      "Epoch 5 step 611: training accuarcy: 0.9665\n",
      "Epoch 5 step 611: training loss: 2162.4358870121964\n",
      "Epoch 5 step 612: training accuarcy: 0.9530000000000001\n",
      "Epoch 5 step 612: training loss: 1974.5016435477921\n",
      "Epoch 5 step 613: training accuarcy: 0.9560000000000001\n",
      "Epoch 5 step 613: training loss: 2283.562573989899\n",
      "Epoch 5 step 614: training accuarcy: 0.9500000000000001\n",
      "Epoch 5 step 614: training loss: 1985.8589260159106\n",
      "Epoch 5 step 615: training accuarcy: 0.9565\n",
      "Epoch 5 step 615: training loss: 1999.3962170340008\n",
      "Epoch 5 step 616: training accuarcy: 0.9570000000000001\n",
      "Epoch 5 step 616: training loss: 1967.6319283194018\n",
      "Epoch 5 step 617: training accuarcy: 0.9575\n",
      "Epoch 5 step 617: training loss: 1795.3188345440074\n",
      "Epoch 5 step 618: training accuarcy: 0.96\n",
      "Epoch 5 step 618: training loss: 1938.9119590251507\n",
      "Epoch 5 step 619: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 619: training loss: 2275.015681192935\n",
      "Epoch 5 step 620: training accuarcy: 0.9495\n",
      "Epoch 5 step 620: training loss: 1936.7381697775647\n",
      "Epoch 5 step 621: training accuarcy: 0.9585\n",
      "Epoch 5 step 621: training loss: 2266.4028323991415\n",
      "Epoch 5 step 622: training accuarcy: 0.9490000000000001\n",
      "Epoch 5 step 622: training loss: 1719.793717116464\n",
      "Epoch 5 step 623: training accuarcy: 0.9625\n",
      "Epoch 5 step 623: training loss: 1837.7289535050322\n",
      "Epoch 5 step 624: training accuarcy: 0.9570000000000001\n",
      "Epoch 5 step 624: training loss: 1744.7384143113877\n",
      "Epoch 5 step 625: training accuarcy: 0.962\n",
      "Epoch 5 step 625: training loss: 2227.9559891499457\n",
      "Epoch 5 step 626: training accuarcy: 0.9535\n",
      "Epoch 5 step 626: training loss: 2160.271208051883\n",
      "Epoch 5 step 627: training accuarcy: 0.9535\n",
      "Epoch 5 step 627: training loss: 2032.6601557031863\n",
      "Epoch 5 step 628: training accuarcy: 0.9545\n",
      "Epoch 5 step 628: training loss: 1795.1923753495319\n",
      "Epoch 5 step 629: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 629: training loss: 1972.6492563873953\n",
      "Epoch 5 step 630: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 630: training loss: 2317.5118074841694\n",
      "Epoch 5 step 631: training accuarcy: 0.9500000000000001\n",
      "Epoch 5 step 631: training loss: 1993.298144065721\n",
      "Epoch 5 step 632: training accuarcy: 0.9565\n",
      "Epoch 5 step 632: training loss: 1935.8221277253936\n",
      "Epoch 5 step 633: training accuarcy: 0.9585\n",
      "Epoch 5 step 633: training loss: 2092.0686181383235\n",
      "Epoch 5 step 634: training accuarcy: 0.9555\n",
      "Epoch 5 step 634: training loss: 2001.4189403996318\n",
      "Epoch 5 step 635: training accuarcy: 0.9565\n",
      "Epoch 5 step 635: training loss: 1864.230002803862\n",
      "Epoch 5 step 636: training accuarcy: 0.9585\n",
      "Epoch 5 step 636: training loss: 1969.2285864763057\n",
      "Epoch 5 step 637: training accuarcy: 0.9565\n",
      "Epoch 5 step 637: training loss: 1650.7164990964568\n",
      "Epoch 5 step 638: training accuarcy: 0.964\n",
      "Epoch 5 step 638: training loss: 1773.6126933266887\n",
      "Epoch 5 step 639: training accuarcy: 0.96\n",
      "Epoch 5 step 639: training loss: 1952.968515462097\n",
      "Epoch 5 step 640: training accuarcy: 0.9575\n",
      "Epoch 5 step 640: training loss: 2133.8351556544294\n",
      "Epoch 5 step 641: training accuarcy: 0.9535\n",
      "Epoch 5 step 641: training loss: 1739.5437295592117\n",
      "Epoch 5 step 642: training accuarcy: 0.9625\n",
      "Epoch 5 step 642: training loss: 1607.6958199704911\n",
      "Epoch 5 step 643: training accuarcy: 0.9655\n",
      "Epoch 5 step 643: training loss: 1853.2690848223697\n",
      "Epoch 5 step 644: training accuarcy: 0.961\n",
      "Epoch 5 step 644: training loss: 1956.5392204522489\n",
      "Epoch 5 step 645: training accuarcy: 0.9575\n",
      "Epoch 5 step 645: training loss: 1853.3356434923696\n",
      "Epoch 5 step 646: training accuarcy: 0.961\n",
      "Epoch 5 step 646: training loss: 1999.6240642132243\n",
      "Epoch 5 step 647: training accuarcy: 0.9575\n",
      "Epoch 5 step 647: training loss: 2165.796375865307\n",
      "Epoch 5 step 648: training accuarcy: 0.9540000000000001\n",
      "Epoch 5 step 648: training loss: 1597.3277989681778\n",
      "Epoch 5 step 649: training accuarcy: 0.9645\n",
      "Epoch 5 step 649: training loss: 1947.2424806657755\n",
      "Epoch 5 step 650: training accuarcy: 0.9555\n",
      "Epoch 5 step 650: training loss: 2038.785539475487\n",
      "Epoch 5 step 651: training accuarcy: 0.9550000000000001\n",
      "Epoch 5 step 651: training loss: 1862.4284744101203\n",
      "Epoch 5 step 652: training accuarcy: 0.9585\n",
      "Epoch 5 step 652: training loss: 1894.3546172145727\n",
      "Epoch 5 step 653: training accuarcy: 0.9585\n",
      "Epoch 5 step 653: training loss: 1830.972723647727\n",
      "Epoch 5 step 654: training accuarcy: 0.9595\n",
      "Epoch 5 step 654: training loss: 2087.591157310494\n",
      "Epoch 5 step 655: training accuarcy: 0.9555\n",
      "Epoch 5 step 655: training loss: 1813.6080093282603\n",
      "Epoch 5 step 656: training accuarcy: 0.961\n",
      "Epoch 5 step 656: training loss: 1684.0543259299945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 657: training accuarcy: 0.963\n",
      "Epoch 5 step 657: training loss: 1825.382616041654\n",
      "Epoch 5 step 658: training accuarcy: 0.9605\n",
      "Epoch 5 step 658: training loss: 1749.3303874080566\n",
      "Epoch 5 step 659: training accuarcy: 0.962\n",
      "Epoch 5 step 659: training loss: 1742.3556733962414\n",
      "Epoch 5 step 660: training accuarcy: 0.964\n",
      "Epoch 5 step 660: training loss: 2065.5218367572825\n",
      "Epoch 5 step 661: training accuarcy: 0.9565\n",
      "Epoch 5 step 661: training loss: 1596.1738497944928\n",
      "Epoch 5 step 662: training accuarcy: 0.9635\n",
      "Epoch 5 step 662: training loss: 1882.4626198135336\n",
      "Epoch 5 step 663: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 663: training loss: 1763.7802394969037\n",
      "Epoch 5 step 664: training accuarcy: 0.963\n",
      "Epoch 5 step 664: training loss: 1912.744241020403\n",
      "Epoch 5 step 665: training accuarcy: 0.96\n",
      "Epoch 5 step 665: training loss: 1646.6443993936603\n",
      "Epoch 5 step 666: training accuarcy: 0.9635\n",
      "Epoch 5 step 666: training loss: 1951.2742285104848\n",
      "Epoch 5 step 667: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 667: training loss: 2149.0763632135804\n",
      "Epoch 5 step 668: training accuarcy: 0.9520000000000001\n",
      "Epoch 5 step 668: training loss: 2108.6775173920023\n",
      "Epoch 5 step 669: training accuarcy: 0.9555\n",
      "Epoch 5 step 669: training loss: 1511.5407081117762\n",
      "Epoch 5 step 670: training accuarcy: 0.9665\n",
      "Epoch 5 step 670: training loss: 1796.5781817050192\n",
      "Epoch 5 step 671: training accuarcy: 0.9605\n",
      "Epoch 5 step 671: training loss: 1487.190367293476\n",
      "Epoch 5 step 672: training accuarcy: 0.9665\n",
      "Epoch 5 step 672: training loss: 1626.983540494566\n",
      "Epoch 5 step 673: training accuarcy: 0.966\n",
      "Epoch 5 step 673: training loss: 1687.0741312930968\n",
      "Epoch 5 step 674: training accuarcy: 0.9635\n",
      "Epoch 5 step 674: training loss: 1978.6392244633641\n",
      "Epoch 5 step 675: training accuarcy: 0.9555\n",
      "Epoch 5 step 675: training loss: 1806.9218561239686\n",
      "Epoch 5 step 676: training accuarcy: 0.9595\n",
      "Epoch 5 step 676: training loss: 1802.9896545876984\n",
      "Epoch 5 step 677: training accuarcy: 0.962\n",
      "Epoch 5 step 677: training loss: 1440.996809975414\n",
      "Epoch 5 step 678: training accuarcy: 0.9695\n",
      "Epoch 5 step 678: training loss: 1614.553336322983\n",
      "Epoch 5 step 679: training accuarcy: 0.963\n",
      "Epoch 5 step 679: training loss: 1707.1811302365809\n",
      "Epoch 5 step 680: training accuarcy: 0.9645\n",
      "Epoch 5 step 680: training loss: 1726.4641330426164\n",
      "Epoch 5 step 681: training accuarcy: 0.964\n",
      "Epoch 5 step 681: training loss: 1837.5268596284145\n",
      "Epoch 5 step 682: training accuarcy: 0.96\n",
      "Epoch 5 step 682: training loss: 1929.4283512334857\n",
      "Epoch 5 step 683: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 683: training loss: 1657.757202654593\n",
      "Epoch 5 step 684: training accuarcy: 0.9645\n",
      "Epoch 5 step 684: training loss: 1468.7797259075217\n",
      "Epoch 5 step 685: training accuarcy: 0.9685\n",
      "Epoch 5 step 685: training loss: 1818.7897323092345\n",
      "Epoch 5 step 686: training accuarcy: 0.9605\n",
      "Epoch 5 step 686: training loss: 1784.7785583254652\n",
      "Epoch 5 step 687: training accuarcy: 0.96\n",
      "Epoch 5 step 687: training loss: 1470.8010985652697\n",
      "Epoch 5 step 688: training accuarcy: 0.968\n",
      "Epoch 5 step 688: training loss: 1694.5538710903047\n",
      "Epoch 5 step 689: training accuarcy: 0.962\n",
      "Epoch 5 step 689: training loss: 1479.496431797394\n",
      "Epoch 5 step 690: training accuarcy: 0.9685534591194969\n",
      "Epoch 5: train loss 1919.888038617814, train accuarcy 0.9582445621490479\n",
      "Epoch 5: valid loss 1656.181513584447, valid accuarcy 0.9629858732223511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 6/8 [05:00<01:39, 50.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 690: training loss: 1584.7252742950047\n",
      "Epoch 6 step 691: training accuarcy: 0.9665\n",
      "Epoch 6 step 691: training loss: 1745.3406839665693\n",
      "Epoch 6 step 692: training accuarcy: 0.963\n",
      "Epoch 6 step 692: training loss: 1553.4594874806494\n",
      "Epoch 6 step 693: training accuarcy: 0.966\n",
      "Epoch 6 step 693: training loss: 1602.7518316330168\n",
      "Epoch 6 step 694: training accuarcy: 0.9645\n",
      "Epoch 6 step 694: training loss: 1560.7280944108093\n",
      "Epoch 6 step 695: training accuarcy: 0.966\n",
      "Epoch 6 step 695: training loss: 1864.1161819043027\n",
      "Epoch 6 step 696: training accuarcy: 0.9590000000000001\n",
      "Epoch 6 step 696: training loss: 2001.5224616632229\n",
      "Epoch 6 step 697: training accuarcy: 0.9560000000000001\n",
      "Epoch 6 step 697: training loss: 1585.942990431048\n",
      "Epoch 6 step 698: training accuarcy: 0.967\n",
      "Epoch 6 step 698: training loss: 1738.9154037080957\n",
      "Epoch 6 step 699: training accuarcy: 0.9615\n",
      "Epoch 6 step 699: training loss: 1625.7628083661407\n",
      "Epoch 6 step 700: training accuarcy: 0.965\n",
      "Epoch 6 step 700: training loss: 1924.453651482933\n",
      "Epoch 6 step 701: training accuarcy: 0.9565\n",
      "Epoch 6 step 701: training loss: 1715.6490597980883\n",
      "Epoch 6 step 702: training accuarcy: 0.965\n",
      "Epoch 6 step 702: training loss: 1763.4671794984213\n",
      "Epoch 6 step 703: training accuarcy: 0.9625\n",
      "Epoch 6 step 703: training loss: 1811.946124914689\n",
      "Epoch 6 step 704: training accuarcy: 0.9585\n",
      "Epoch 6 step 704: training loss: 1462.6521729389272\n",
      "Epoch 6 step 705: training accuarcy: 0.968\n",
      "Epoch 6 step 705: training loss: 1722.5711823538702\n",
      "Epoch 6 step 706: training accuarcy: 0.963\n",
      "Epoch 6 step 706: training loss: 1907.073441178862\n",
      "Epoch 6 step 707: training accuarcy: 0.9580000000000001\n",
      "Epoch 6 step 707: training loss: 1626.811729674254\n",
      "Epoch 6 step 708: training accuarcy: 0.9635\n",
      "Epoch 6 step 708: training loss: 1439.5415594588515\n",
      "Epoch 6 step 709: training accuarcy: 0.9695\n",
      "Epoch 6 step 709: training loss: 1765.6455732453915\n",
      "Epoch 6 step 710: training accuarcy: 0.9590000000000001\n",
      "Epoch 6 step 710: training loss: 1587.0345946050927\n",
      "Epoch 6 step 711: training accuarcy: 0.965\n",
      "Epoch 6 step 711: training loss: 1630.4697961215777\n",
      "Epoch 6 step 712: training accuarcy: 0.966\n",
      "Epoch 6 step 712: training loss: 1782.8116752389985\n",
      "Epoch 6 step 713: training accuarcy: 0.961\n",
      "Epoch 6 step 713: training loss: 1410.644430587899\n",
      "Epoch 6 step 714: training accuarcy: 0.9665\n",
      "Epoch 6 step 714: training loss: 1811.3314400193694\n",
      "Epoch 6 step 715: training accuarcy: 0.9605\n",
      "Epoch 6 step 715: training loss: 1881.9867413471586\n",
      "Epoch 6 step 716: training accuarcy: 0.9605\n",
      "Epoch 6 step 716: training loss: 1833.151020852479\n",
      "Epoch 6 step 717: training accuarcy: 0.96\n",
      "Epoch 6 step 717: training loss: 1313.3267838013198\n",
      "Epoch 6 step 718: training accuarcy: 0.972\n",
      "Epoch 6 step 718: training loss: 1687.4915068358537\n",
      "Epoch 6 step 719: training accuarcy: 0.9625\n",
      "Epoch 6 step 719: training loss: 1600.1194850817415\n",
      "Epoch 6 step 720: training accuarcy: 0.9655\n",
      "Epoch 6 step 720: training loss: 1676.2411960154077\n",
      "Epoch 6 step 721: training accuarcy: 0.9615\n",
      "Epoch 6 step 721: training loss: 1333.936353241521\n",
      "Epoch 6 step 722: training accuarcy: 0.97\n",
      "Epoch 6 step 722: training loss: 1631.0269873234915\n",
      "Epoch 6 step 723: training accuarcy: 0.965\n",
      "Epoch 6 step 723: training loss: 1656.1068862031475\n",
      "Epoch 6 step 724: training accuarcy: 0.9635\n",
      "Epoch 6 step 724: training loss: 1551.1664033280738\n",
      "Epoch 6 step 725: training accuarcy: 0.967\n",
      "Epoch 6 step 725: training loss: 1763.5008167216597\n",
      "Epoch 6 step 726: training accuarcy: 0.9615\n",
      "Epoch 6 step 726: training loss: 1670.8643441186182\n",
      "Epoch 6 step 727: training accuarcy: 0.9635\n",
      "Epoch 6 step 727: training loss: 1620.2474292741124\n",
      "Epoch 6 step 728: training accuarcy: 0.963\n",
      "Epoch 6 step 728: training loss: 1495.167310215691\n",
      "Epoch 6 step 729: training accuarcy: 0.9665\n",
      "Epoch 6 step 729: training loss: 1702.7252655808556\n",
      "Epoch 6 step 730: training accuarcy: 0.961\n",
      "Epoch 6 step 730: training loss: 1587.501144379108\n",
      "Epoch 6 step 731: training accuarcy: 0.9645\n",
      "Epoch 6 step 731: training loss: 1588.979743844311\n",
      "Epoch 6 step 732: training accuarcy: 0.965\n",
      "Epoch 6 step 732: training loss: 1715.1540026099672\n",
      "Epoch 6 step 733: training accuarcy: 0.963\n",
      "Epoch 6 step 733: training loss: 1757.886541972088\n",
      "Epoch 6 step 734: training accuarcy: 0.9615\n",
      "Epoch 6 step 734: training loss: 1539.1108203174324\n",
      "Epoch 6 step 735: training accuarcy: 0.9665\n",
      "Epoch 6 step 735: training loss: 1602.3887244309237\n",
      "Epoch 6 step 736: training accuarcy: 0.9665\n",
      "Epoch 6 step 736: training loss: 1783.8985192823154\n",
      "Epoch 6 step 737: training accuarcy: 0.961\n",
      "Epoch 6 step 737: training loss: 1414.4417758727175\n",
      "Epoch 6 step 738: training accuarcy: 0.97\n",
      "Epoch 6 step 738: training loss: 1932.3513171806637\n",
      "Epoch 6 step 739: training accuarcy: 0.9570000000000001\n",
      "Epoch 6 step 739: training loss: 2065.4682396968055\n",
      "Epoch 6 step 740: training accuarcy: 0.9550000000000001\n",
      "Epoch 6 step 740: training loss: 1854.0208013622835\n",
      "Epoch 6 step 741: training accuarcy: 0.9595\n",
      "Epoch 6 step 741: training loss: 1743.8279687373993\n",
      "Epoch 6 step 742: training accuarcy: 0.9635\n",
      "Epoch 6 step 742: training loss: 1669.042555104886\n",
      "Epoch 6 step 743: training accuarcy: 0.9635\n",
      "Epoch 6 step 743: training loss: 1816.3046939594499\n",
      "Epoch 6 step 744: training accuarcy: 0.961\n",
      "Epoch 6 step 744: training loss: 1510.7583976813467\n",
      "Epoch 6 step 745: training accuarcy: 0.966\n",
      "Epoch 6 step 745: training loss: 1830.1629030018773\n",
      "Epoch 6 step 746: training accuarcy: 0.96\n",
      "Epoch 6 step 746: training loss: 1631.522984503542\n",
      "Epoch 6 step 747: training accuarcy: 0.9645\n",
      "Epoch 6 step 747: training loss: 1840.1645886989436\n",
      "Epoch 6 step 748: training accuarcy: 0.9605\n",
      "Epoch 6 step 748: training loss: 1575.822771840857\n",
      "Epoch 6 step 749: training accuarcy: 0.967\n",
      "Epoch 6 step 749: training loss: 1747.965685273175\n",
      "Epoch 6 step 750: training accuarcy: 0.9625\n",
      "Epoch 6 step 750: training loss: 1768.1391028705018\n",
      "Epoch 6 step 751: training accuarcy: 0.9590000000000001\n",
      "Epoch 6 step 751: training loss: 1510.866185095796\n",
      "Epoch 6 step 752: training accuarcy: 0.968\n",
      "Epoch 6 step 752: training loss: 1469.6639957262832\n",
      "Epoch 6 step 753: training accuarcy: 0.9665\n",
      "Epoch 6 step 753: training loss: 1379.4831092304823\n",
      "Epoch 6 step 754: training accuarcy: 0.9685\n",
      "Epoch 6 step 754: training loss: 1837.9345187538843\n",
      "Epoch 6 step 755: training accuarcy: 0.96\n",
      "Epoch 6 step 755: training loss: 1696.957738113829\n",
      "Epoch 6 step 756: training accuarcy: 0.965\n",
      "Epoch 6 step 756: training loss: 1977.4564950396991\n",
      "Epoch 6 step 757: training accuarcy: 0.9565\n",
      "Epoch 6 step 757: training loss: 1537.918437919892\n",
      "Epoch 6 step 758: training accuarcy: 0.967\n",
      "Epoch 6 step 758: training loss: 1429.2615595388293\n",
      "Epoch 6 step 759: training accuarcy: 0.969\n",
      "Epoch 6 step 759: training loss: 1770.119636764556\n",
      "Epoch 6 step 760: training accuarcy: 0.961\n",
      "Epoch 6 step 760: training loss: 1957.2451303841644\n",
      "Epoch 6 step 761: training accuarcy: 0.9560000000000001\n",
      "Epoch 6 step 761: training loss: 1646.830398000065\n",
      "Epoch 6 step 762: training accuarcy: 0.9635\n",
      "Epoch 6 step 762: training loss: 1636.2287244978465\n",
      "Epoch 6 step 763: training accuarcy: 0.962\n",
      "Epoch 6 step 763: training loss: 1960.1627162246652\n",
      "Epoch 6 step 764: training accuarcy: 0.9580000000000001\n",
      "Epoch 6 step 764: training loss: 1812.8294852759789\n",
      "Epoch 6 step 765: training accuarcy: 0.9595\n",
      "Epoch 6 step 765: training loss: 1716.4989821214683\n",
      "Epoch 6 step 766: training accuarcy: 0.9635\n",
      "Epoch 6 step 766: training loss: 1418.028569185884\n",
      "Epoch 6 step 767: training accuarcy: 0.968\n",
      "Epoch 6 step 767: training loss: 1717.420873300701\n",
      "Epoch 6 step 768: training accuarcy: 0.9585\n",
      "Epoch 6 step 768: training loss: 1493.4001573883925\n",
      "Epoch 6 step 769: training accuarcy: 0.9655\n",
      "Epoch 6 step 769: training loss: 1517.7096007442954\n",
      "Epoch 6 step 770: training accuarcy: 0.965\n",
      "Epoch 6 step 770: training loss: 1683.4288310344307\n",
      "Epoch 6 step 771: training accuarcy: 0.961\n",
      "Epoch 6 step 771: training loss: 1973.896117441929\n",
      "Epoch 6 step 772: training accuarcy: 0.9560000000000001\n",
      "Epoch 6 step 772: training loss: 1949.1888445334655\n",
      "Epoch 6 step 773: training accuarcy: 0.9570000000000001\n",
      "Epoch 6 step 773: training loss: 1487.4757785438053\n",
      "Epoch 6 step 774: training accuarcy: 0.968\n",
      "Epoch 6 step 774: training loss: 1810.787659976496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 775: training accuarcy: 0.9615\n",
      "Epoch 6 step 775: training loss: 1603.5512894202554\n",
      "Epoch 6 step 776: training accuarcy: 0.964\n",
      "Epoch 6 step 776: training loss: 1549.1039703163337\n",
      "Epoch 6 step 777: training accuarcy: 0.9665\n",
      "Epoch 6 step 777: training loss: 1403.5415672458885\n",
      "Epoch 6 step 778: training accuarcy: 0.969\n",
      "Epoch 6 step 778: training loss: 1807.4142631067523\n",
      "Epoch 6 step 779: training accuarcy: 0.962\n",
      "Epoch 6 step 779: training loss: 1755.9015985737872\n",
      "Epoch 6 step 780: training accuarcy: 0.9615\n",
      "Epoch 6 step 780: training loss: 1670.3043249261586\n",
      "Epoch 6 step 781: training accuarcy: 0.964\n",
      "Epoch 6 step 781: training loss: 1623.015748922515\n",
      "Epoch 6 step 782: training accuarcy: 0.966\n",
      "Epoch 6 step 782: training loss: 1690.6778684346984\n",
      "Epoch 6 step 783: training accuarcy: 0.964\n",
      "Epoch 6 step 783: training loss: 1830.1994760210193\n",
      "Epoch 6 step 784: training accuarcy: 0.9595\n",
      "Epoch 6 step 784: training loss: 1564.8977937738841\n",
      "Epoch 6 step 785: training accuarcy: 0.968\n",
      "Epoch 6 step 785: training loss: 1674.1603486439103\n",
      "Epoch 6 step 786: training accuarcy: 0.9625\n",
      "Epoch 6 step 786: training loss: 1579.9693363941642\n",
      "Epoch 6 step 787: training accuarcy: 0.968\n",
      "Epoch 6 step 787: training loss: 1973.3330042710127\n",
      "Epoch 6 step 788: training accuarcy: 0.9540000000000001\n",
      "Epoch 6 step 788: training loss: 1917.0128784479662\n",
      "Epoch 6 step 789: training accuarcy: 0.9590000000000001\n",
      "Epoch 6 step 789: training loss: 1923.0900059076553\n",
      "Epoch 6 step 790: training accuarcy: 0.96\n",
      "Epoch 6 step 790: training loss: 1573.3134922426905\n",
      "Epoch 6 step 791: training accuarcy: 0.965\n",
      "Epoch 6 step 791: training loss: 1342.3566629017262\n",
      "Epoch 6 step 792: training accuarcy: 0.971\n",
      "Epoch 6 step 792: training loss: 1272.4109151529958\n",
      "Epoch 6 step 793: training accuarcy: 0.971\n",
      "Epoch 6 step 793: training loss: 1478.2670021188405\n",
      "Epoch 6 step 794: training accuarcy: 0.968\n",
      "Epoch 6 step 794: training loss: 1778.993244634585\n",
      "Epoch 6 step 795: training accuarcy: 0.9615\n",
      "Epoch 6 step 795: training loss: 1763.308636221448\n",
      "Epoch 6 step 796: training accuarcy: 0.9605\n",
      "Epoch 6 step 796: training loss: 1798.222044465525\n",
      "Epoch 6 step 797: training accuarcy: 0.9590000000000001\n",
      "Epoch 6 step 797: training loss: 1633.336208716443\n",
      "Epoch 6 step 798: training accuarcy: 0.963\n",
      "Epoch 6 step 798: training loss: 1925.8021866508943\n",
      "Epoch 6 step 799: training accuarcy: 0.9580000000000001\n",
      "Epoch 6 step 799: training loss: 1879.6934636445478\n",
      "Epoch 6 step 800: training accuarcy: 0.9595\n",
      "Epoch 6 step 800: training loss: 2117.3545369051226\n",
      "Epoch 6 step 801: training accuarcy: 0.9515\n",
      "Epoch 6 step 801: training loss: 1489.4637335852483\n",
      "Epoch 6 step 802: training accuarcy: 0.967\n",
      "Epoch 6 step 802: training loss: 1417.9782699054822\n",
      "Epoch 6 step 803: training accuarcy: 0.97\n",
      "Epoch 6 step 803: training loss: 1829.6964186998337\n",
      "Epoch 6 step 804: training accuarcy: 0.962\n",
      "Epoch 6 step 804: training loss: 1508.4763758705806\n",
      "Epoch 6 step 805: training accuarcy: 0.9643605870020965\n",
      "Epoch 6: train loss 1681.5911378477967, train accuarcy 0.9626257419586182\n",
      "Epoch 6: valid loss 1617.0951619534371, valid accuarcy 0.9642245769500732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 7/8 [05:50<00:50, 50.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 805: training loss: 2224.926208144229\n",
      "Epoch 7 step 806: training accuarcy: 0.9505\n",
      "Epoch 7 step 806: training loss: 1877.4336140917253\n",
      "Epoch 7 step 807: training accuarcy: 0.9565\n",
      "Epoch 7 step 807: training loss: 1954.6797397679984\n",
      "Epoch 7 step 808: training accuarcy: 0.9565\n",
      "Epoch 7 step 808: training loss: 1789.6648401751604\n",
      "Epoch 7 step 809: training accuarcy: 0.9615\n",
      "Epoch 7 step 809: training loss: 1871.9731062347973\n",
      "Epoch 7 step 810: training accuarcy: 0.9595\n",
      "Epoch 7 step 810: training loss: 1785.0710895667562\n",
      "Epoch 7 step 811: training accuarcy: 0.96\n",
      "Epoch 7 step 811: training loss: 1924.0741870615263\n",
      "Epoch 7 step 812: training accuarcy: 0.9585\n",
      "Epoch 7 step 812: training loss: 1753.7472427775908\n",
      "Epoch 7 step 813: training accuarcy: 0.961\n",
      "Epoch 7 step 813: training loss: 1715.5617916735328\n",
      "Epoch 7 step 814: training accuarcy: 0.9625\n",
      "Epoch 7 step 814: training loss: 1270.444419478824\n",
      "Epoch 7 step 815: training accuarcy: 0.972\n",
      "Epoch 7 step 815: training loss: 1769.1602587394714\n",
      "Epoch 7 step 816: training accuarcy: 0.96\n",
      "Epoch 7 step 816: training loss: 1976.507048704557\n",
      "Epoch 7 step 817: training accuarcy: 0.9575\n",
      "Epoch 7 step 817: training loss: 1793.775771868592\n",
      "Epoch 7 step 818: training accuarcy: 0.961\n",
      "Epoch 7 step 818: training loss: 1443.5339017322\n",
      "Epoch 7 step 819: training accuarcy: 0.967\n",
      "Epoch 7 step 819: training loss: 2014.8373077288488\n",
      "Epoch 7 step 820: training accuarcy: 0.9585\n",
      "Epoch 7 step 820: training loss: 1509.459388775328\n",
      "Epoch 7 step 821: training accuarcy: 0.9665\n",
      "Epoch 7 step 821: training loss: 1374.5773025796607\n",
      "Epoch 7 step 822: training accuarcy: 0.9675\n",
      "Epoch 7 step 822: training loss: 1628.6577757759073\n",
      "Epoch 7 step 823: training accuarcy: 0.9645\n",
      "Epoch 7 step 823: training loss: 1995.396009757446\n",
      "Epoch 7 step 824: training accuarcy: 0.9550000000000001\n",
      "Epoch 7 step 824: training loss: 1583.6216716273514\n",
      "Epoch 7 step 825: training accuarcy: 0.964\n",
      "Epoch 7 step 825: training loss: 1918.6944111036214\n",
      "Epoch 7 step 826: training accuarcy: 0.9585\n",
      "Epoch 7 step 826: training loss: 1717.628829364391\n",
      "Epoch 7 step 827: training accuarcy: 0.9615\n",
      "Epoch 7 step 827: training loss: 1856.3338610840558\n",
      "Epoch 7 step 828: training accuarcy: 0.9595\n",
      "Epoch 7 step 828: training loss: 1882.7729472147548\n",
      "Epoch 7 step 829: training accuarcy: 0.9585\n",
      "Epoch 7 step 829: training loss: 1644.7899424563843\n",
      "Epoch 7 step 830: training accuarcy: 0.9645\n",
      "Epoch 7 step 830: training loss: 1634.0170188457523\n",
      "Epoch 7 step 831: training accuarcy: 0.9645\n",
      "Epoch 7 step 831: training loss: 1656.8083089950978\n",
      "Epoch 7 step 832: training accuarcy: 0.962\n",
      "Epoch 7 step 832: training loss: 1624.9193966456444\n",
      "Epoch 7 step 833: training accuarcy: 0.9645\n",
      "Epoch 7 step 833: training loss: 1837.1415764107048\n",
      "Epoch 7 step 834: training accuarcy: 0.9595\n",
      "Epoch 7 step 834: training loss: 2244.68653888013\n",
      "Epoch 7 step 835: training accuarcy: 0.9510000000000001\n",
      "Epoch 7 step 835: training loss: 1702.0867272353014\n",
      "Epoch 7 step 836: training accuarcy: 0.961\n",
      "Epoch 7 step 836: training loss: 1584.4619682219447\n",
      "Epoch 7 step 837: training accuarcy: 0.9655\n",
      "Epoch 7 step 837: training loss: 1684.457243307045\n",
      "Epoch 7 step 838: training accuarcy: 0.963\n",
      "Epoch 7 step 838: training loss: 1779.8719733755308\n",
      "Epoch 7 step 839: training accuarcy: 0.961\n",
      "Epoch 7 step 839: training loss: 1442.8003148875\n",
      "Epoch 7 step 840: training accuarcy: 0.968\n",
      "Epoch 7 step 840: training loss: 1654.2309119529725\n",
      "Epoch 7 step 841: training accuarcy: 0.9625\n",
      "Epoch 7 step 841: training loss: 2031.886512467705\n",
      "Epoch 7 step 842: training accuarcy: 0.9550000000000001\n",
      "Epoch 7 step 842: training loss: 1872.5929987642876\n",
      "Epoch 7 step 843: training accuarcy: 0.96\n",
      "Epoch 7 step 843: training loss: 1807.6385749295835\n",
      "Epoch 7 step 844: training accuarcy: 0.9595\n",
      "Epoch 7 step 844: training loss: 2018.6190599788672\n",
      "Epoch 7 step 845: training accuarcy: 0.9560000000000001\n",
      "Epoch 7 step 845: training loss: 1550.9507999437972\n",
      "Epoch 7 step 846: training accuarcy: 0.9675\n",
      "Epoch 7 step 846: training loss: 1745.674268244766\n",
      "Epoch 7 step 847: training accuarcy: 0.961\n",
      "Epoch 7 step 847: training loss: 1368.8334538634258\n",
      "Epoch 7 step 848: training accuarcy: 0.9725\n",
      "Epoch 7 step 848: training loss: 1741.9649577086368\n",
      "Epoch 7 step 849: training accuarcy: 0.9635\n",
      "Epoch 7 step 849: training loss: 1620.5971765111267\n",
      "Epoch 7 step 850: training accuarcy: 0.966\n",
      "Epoch 7 step 850: training loss: 1612.1781881163254\n",
      "Epoch 7 step 851: training accuarcy: 0.9635\n",
      "Epoch 7 step 851: training loss: 1666.842425223553\n",
      "Epoch 7 step 852: training accuarcy: 0.9645\n",
      "Epoch 7 step 852: training loss: 2132.0429163174213\n",
      "Epoch 7 step 853: training accuarcy: 0.9535\n",
      "Epoch 7 step 853: training loss: 1709.1573423058928\n",
      "Epoch 7 step 854: training accuarcy: 0.961\n",
      "Epoch 7 step 854: training loss: 1806.2266446099281\n",
      "Epoch 7 step 855: training accuarcy: 0.9590000000000001\n",
      "Epoch 7 step 855: training loss: 1229.4758558118965\n",
      "Epoch 7 step 856: training accuarcy: 0.9745\n",
      "Epoch 7 step 856: training loss: 1574.192697671263\n",
      "Epoch 7 step 857: training accuarcy: 0.967\n",
      "Epoch 7 step 857: training loss: 1721.4233345348052\n",
      "Epoch 7 step 858: training accuarcy: 0.9635\n",
      "Epoch 7 step 858: training loss: 1771.11249962675\n",
      "Epoch 7 step 859: training accuarcy: 0.9605\n",
      "Epoch 7 step 859: training loss: 1596.569992185806\n",
      "Epoch 7 step 860: training accuarcy: 0.965\n",
      "Epoch 7 step 860: training loss: 1641.2779889324522\n",
      "Epoch 7 step 861: training accuarcy: 0.963\n",
      "Epoch 7 step 861: training loss: 1644.974792798002\n",
      "Epoch 7 step 862: training accuarcy: 0.9625\n",
      "Epoch 7 step 862: training loss: 1630.4930431364373\n",
      "Epoch 7 step 863: training accuarcy: 0.9645\n",
      "Epoch 7 step 863: training loss: 1940.2805944608738\n",
      "Epoch 7 step 864: training accuarcy: 0.9580000000000001\n",
      "Epoch 7 step 864: training loss: 1859.337857104223\n",
      "Epoch 7 step 865: training accuarcy: 0.9595\n",
      "Epoch 7 step 865: training loss: 2128.2534814586065\n",
      "Epoch 7 step 866: training accuarcy: 0.9550000000000001\n",
      "Epoch 7 step 866: training loss: 1697.1035450390907\n",
      "Epoch 7 step 867: training accuarcy: 0.964\n",
      "Epoch 7 step 867: training loss: 1768.0974291167327\n",
      "Epoch 7 step 868: training accuarcy: 0.9625\n",
      "Epoch 7 step 868: training loss: 2274.28722475708\n",
      "Epoch 7 step 869: training accuarcy: 0.9495\n",
      "Epoch 7 step 869: training loss: 1719.6237828335118\n",
      "Epoch 7 step 870: training accuarcy: 0.964\n",
      "Epoch 7 step 870: training loss: 1772.3170885335207\n",
      "Epoch 7 step 871: training accuarcy: 0.9605\n",
      "Epoch 7 step 871: training loss: 1721.1503894458833\n",
      "Epoch 7 step 872: training accuarcy: 0.961\n",
      "Epoch 7 step 872: training loss: 1840.1545081233958\n",
      "Epoch 7 step 873: training accuarcy: 0.96\n",
      "Epoch 7 step 873: training loss: 1283.8776360537156\n",
      "Epoch 7 step 874: training accuarcy: 0.973\n",
      "Epoch 7 step 874: training loss: 1719.9046050140028\n",
      "Epoch 7 step 875: training accuarcy: 0.9635\n",
      "Epoch 7 step 875: training loss: 1551.3451968521977\n",
      "Epoch 7 step 876: training accuarcy: 0.967\n",
      "Epoch 7 step 876: training loss: 1894.0340801365046\n",
      "Epoch 7 step 877: training accuarcy: 0.9585\n",
      "Epoch 7 step 877: training loss: 1536.0550363213802\n",
      "Epoch 7 step 878: training accuarcy: 0.9665\n",
      "Epoch 7 step 878: training loss: 1581.5155676890106\n",
      "Epoch 7 step 879: training accuarcy: 0.9655\n",
      "Epoch 7 step 879: training loss: 1523.3069904466602\n",
      "Epoch 7 step 880: training accuarcy: 0.964\n",
      "Epoch 7 step 880: training loss: 2129.0645295490103\n",
      "Epoch 7 step 881: training accuarcy: 0.9535\n",
      "Epoch 7 step 881: training loss: 1603.2264875722099\n",
      "Epoch 7 step 882: training accuarcy: 0.966\n",
      "Epoch 7 step 882: training loss: 1679.3212748191386\n",
      "Epoch 7 step 883: training accuarcy: 0.961\n",
      "Epoch 7 step 883: training loss: 1764.5740865573716\n",
      "Epoch 7 step 884: training accuarcy: 0.962\n",
      "Epoch 7 step 884: training loss: 1786.3969526077108\n",
      "Epoch 7 step 885: training accuarcy: 0.962\n",
      "Epoch 7 step 885: training loss: 1857.8322230403116\n",
      "Epoch 7 step 886: training accuarcy: 0.9590000000000001\n",
      "Epoch 7 step 886: training loss: 1671.748024268599\n",
      "Epoch 7 step 887: training accuarcy: 0.9645\n",
      "Epoch 7 step 887: training loss: 2033.0169936877876\n",
      "Epoch 7 step 888: training accuarcy: 0.9560000000000001\n",
      "Epoch 7 step 888: training loss: 2017.2799407784178\n",
      "Epoch 7 step 889: training accuarcy: 0.9545\n",
      "Epoch 7 step 889: training loss: 1676.9981668246655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 890: training accuarcy: 0.964\n",
      "Epoch 7 step 890: training loss: 1763.4725245047885\n",
      "Epoch 7 step 891: training accuarcy: 0.9615\n",
      "Epoch 7 step 891: training loss: 1621.3485586164197\n",
      "Epoch 7 step 892: training accuarcy: 0.963\n",
      "Epoch 7 step 892: training loss: 1506.0314564787445\n",
      "Epoch 7 step 893: training accuarcy: 0.967\n",
      "Epoch 7 step 893: training loss: 1546.3678086656262\n",
      "Epoch 7 step 894: training accuarcy: 0.968\n",
      "Epoch 7 step 894: training loss: 1654.6104709844178\n",
      "Epoch 7 step 895: training accuarcy: 0.9635\n",
      "Epoch 7 step 895: training loss: 1577.813318090757\n",
      "Epoch 7 step 896: training accuarcy: 0.9645\n",
      "Epoch 7 step 896: training loss: 1647.068517417925\n",
      "Epoch 7 step 897: training accuarcy: 0.963\n",
      "Epoch 7 step 897: training loss: 1725.6682201180063\n",
      "Epoch 7 step 898: training accuarcy: 0.963\n",
      "Epoch 7 step 898: training loss: 1651.8529143363435\n",
      "Epoch 7 step 899: training accuarcy: 0.964\n",
      "Epoch 7 step 899: training loss: 1625.0471581363179\n",
      "Epoch 7 step 900: training accuarcy: 0.9655\n",
      "Epoch 7 step 900: training loss: 1874.3811544176883\n",
      "Epoch 7 step 901: training accuarcy: 0.9585\n",
      "Epoch 7 step 901: training loss: 1829.704106727958\n",
      "Epoch 7 step 902: training accuarcy: 0.9615\n",
      "Epoch 7 step 902: training loss: 1581.7471960304454\n",
      "Epoch 7 step 903: training accuarcy: 0.9645\n",
      "Epoch 7 step 903: training loss: 1305.7319295668758\n",
      "Epoch 7 step 904: training accuarcy: 0.97\n",
      "Epoch 7 step 904: training loss: 1873.437602329958\n",
      "Epoch 7 step 905: training accuarcy: 0.96\n",
      "Epoch 7 step 905: training loss: 1766.2427494581195\n",
      "Epoch 7 step 906: training accuarcy: 0.9615\n",
      "Epoch 7 step 906: training loss: 1720.2903962823068\n",
      "Epoch 7 step 907: training accuarcy: 0.962\n",
      "Epoch 7 step 907: training loss: 1398.2176335092597\n",
      "Epoch 7 step 908: training accuarcy: 0.97\n",
      "Epoch 7 step 908: training loss: 1504.9155518973143\n",
      "Epoch 7 step 909: training accuarcy: 0.9685\n",
      "Epoch 7 step 909: training loss: 1783.9428504920008\n",
      "Epoch 7 step 910: training accuarcy: 0.96\n",
      "Epoch 7 step 910: training loss: 1877.9713546361218\n",
      "Epoch 7 step 911: training accuarcy: 0.9590000000000001\n",
      "Epoch 7 step 911: training loss: 1575.9775227956984\n",
      "Epoch 7 step 912: training accuarcy: 0.9655\n",
      "Epoch 7 step 912: training loss: 1719.0038157514505\n",
      "Epoch 7 step 913: training accuarcy: 0.963\n",
      "Epoch 7 step 913: training loss: 1700.509837875526\n",
      "Epoch 7 step 914: training accuarcy: 0.9615\n",
      "Epoch 7 step 914: training loss: 1884.5704229767387\n",
      "Epoch 7 step 915: training accuarcy: 0.9570000000000001\n",
      "Epoch 7 step 915: training loss: 2059.225485632966\n",
      "Epoch 7 step 916: training accuarcy: 0.9545\n",
      "Epoch 7 step 916: training loss: 1423.2088001770906\n",
      "Epoch 7 step 917: training accuarcy: 0.9695\n",
      "Epoch 7 step 917: training loss: 1462.358661235024\n",
      "Epoch 7 step 918: training accuarcy: 0.968\n",
      "Epoch 7 step 918: training loss: 1818.3748786801489\n",
      "Epoch 7 step 919: training accuarcy: 0.96\n",
      "Epoch 7 step 919: training loss: 1737.7826286573581\n",
      "Epoch 7 step 920: training accuarcy: 0.9612159329140462\n",
      "Epoch 7: train loss 1729.2566033869227, train accuarcy 0.9617905616760254\n",
      "Epoch 7: valid loss 1716.2921555391533, valid accuarcy 0.9618929624557495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [06:41<00:00, 50.26s/it]\n"
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=8,\n",
    "                loss_callback=simple_loss_callback,\n",
    "                log_dir=get_log_dir('kaggle', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:49:05.256831Z",
     "start_time": "2019-09-25T08:49:05.252830Z"
    }
   },
   "outputs": [],
   "source": [
    "del hrm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PRME FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:51:37.829497Z",
     "start_time": "2019-09-25T08:51:37.756469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchPrmeFM()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_model = TorchPrmeFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "prme_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:51:43.134627Z",
     "start_time": "2019-09-25T08:51:43.131627Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(prme_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T08:53:21.810554Z",
     "start_time": "2019-09-25T08:53:21.779552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1eddbc0c470>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_learner = FMLearner(prme_model, adam_opt, schedular, db)\n",
    "prme_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T09:00:35.468094Z",
     "start_time": "2019-09-25T08:53:37.924553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 49623.48128396946\n",
      "Epoch 0 step 1: training accuarcy: 0.105\n",
      "Epoch 0 step 1: training loss: 48873.34856646899\n",
      "Epoch 0 step 2: training accuarcy: 0.1245\n",
      "Epoch 0 step 2: training loss: 48772.25469438557\n",
      "Epoch 0 step 3: training accuarcy: 0.136\n",
      "Epoch 0 step 3: training loss: 48591.69769560142\n",
      "Epoch 0 step 4: training accuarcy: 0.1385\n",
      "Epoch 0 step 4: training loss: 48524.37249453151\n",
      "Epoch 0 step 5: training accuarcy: 0.14\n",
      "Epoch 0 step 5: training loss: 46739.499761136976\n",
      "Epoch 0 step 6: training accuarcy: 0.1675\n",
      "Epoch 0 step 6: training loss: 48115.74665151427\n",
      "Epoch 0 step 7: training accuarcy: 0.1395\n",
      "Epoch 0 step 7: training loss: 47197.856354077965\n",
      "Epoch 0 step 8: training accuarcy: 0.1555\n",
      "Epoch 0 step 8: training loss: 46640.16153866338\n",
      "Epoch 0 step 9: training accuarcy: 0.1555\n",
      "Epoch 0 step 9: training loss: 45342.490968963415\n",
      "Epoch 0 step 10: training accuarcy: 0.17\n",
      "Epoch 0 step 10: training loss: 46349.00888159786\n",
      "Epoch 0 step 11: training accuarcy: 0.1545\n",
      "Epoch 0 step 11: training loss: 45977.57827702407\n",
      "Epoch 0 step 12: training accuarcy: 0.156\n",
      "Epoch 0 step 12: training loss: 45806.21908633138\n",
      "Epoch 0 step 13: training accuarcy: 0.154\n",
      "Epoch 0 step 13: training loss: 45688.76054419848\n",
      "Epoch 0 step 14: training accuarcy: 0.1495\n",
      "Epoch 0 step 14: training loss: 45424.165801459545\n",
      "Epoch 0 step 15: training accuarcy: 0.1485\n",
      "Epoch 0 step 15: training loss: 44955.268208007714\n",
      "Epoch 0 step 16: training accuarcy: 0.1595\n",
      "Epoch 0 step 16: training loss: 44165.682872643905\n",
      "Epoch 0 step 17: training accuarcy: 0.1705\n",
      "Epoch 0 step 17: training loss: 44360.85079432056\n",
      "Epoch 0 step 18: training accuarcy: 0.155\n",
      "Epoch 0 step 18: training loss: 44938.75058221864\n",
      "Epoch 0 step 19: training accuarcy: 0.14300000000000002\n",
      "Epoch 0 step 19: training loss: 43648.97440904625\n",
      "Epoch 0 step 20: training accuarcy: 0.164\n",
      "Epoch 0 step 20: training loss: 43137.9182191699\n",
      "Epoch 0 step 21: training accuarcy: 0.169\n",
      "Epoch 0 step 21: training loss: 43141.645803546664\n",
      "Epoch 0 step 22: training accuarcy: 0.1715\n",
      "Epoch 0 step 22: training loss: 43431.35815143044\n",
      "Epoch 0 step 23: training accuarcy: 0.1555\n",
      "Epoch 0 step 23: training loss: 43341.03232183184\n",
      "Epoch 0 step 24: training accuarcy: 0.156\n",
      "Epoch 0 step 24: training loss: 42990.68598574763\n",
      "Epoch 0 step 25: training accuarcy: 0.1695\n",
      "Epoch 0 step 25: training loss: 42694.36304070288\n",
      "Epoch 0 step 26: training accuarcy: 0.171\n",
      "Epoch 0 step 26: training loss: 42497.20261937745\n",
      "Epoch 0 step 27: training accuarcy: 0.167\n",
      "Epoch 0 step 27: training loss: 42589.41818124542\n",
      "Epoch 0 step 28: training accuarcy: 0.17250000000000001\n",
      "Epoch 0 step 28: training loss: 42324.98608637209\n",
      "Epoch 0 step 29: training accuarcy: 0.167\n",
      "Epoch 0 step 29: training loss: 41734.523687191264\n",
      "Epoch 0 step 30: training accuarcy: 0.17250000000000001\n",
      "Epoch 0 step 30: training loss: 41835.62256909095\n",
      "Epoch 0 step 31: training accuarcy: 0.17550000000000002\n",
      "Epoch 0 step 31: training loss: 41784.96438762503\n",
      "Epoch 0 step 32: training accuarcy: 0.1765\n",
      "Epoch 0 step 32: training loss: 41601.37493729541\n",
      "Epoch 0 step 33: training accuarcy: 0.17300000000000001\n",
      "Epoch 0 step 33: training loss: 41103.23662285536\n",
      "Epoch 0 step 34: training accuarcy: 0.1815\n",
      "Epoch 0 step 34: training loss: 41279.28163366784\n",
      "Epoch 0 step 35: training accuarcy: 0.17500000000000002\n",
      "Epoch 0 step 35: training loss: 41278.01374399139\n",
      "Epoch 0 step 36: training accuarcy: 0.17300000000000001\n",
      "Epoch 0 step 36: training loss: 41037.36768571634\n",
      "Epoch 0 step 37: training accuarcy: 0.17500000000000002\n",
      "Epoch 0 step 37: training loss: 40725.952567557135\n",
      "Epoch 0 step 38: training accuarcy: 0.181\n",
      "Epoch 0 step 38: training loss: 41055.61120566768\n",
      "Epoch 0 step 39: training accuarcy: 0.1705\n",
      "Epoch 0 step 39: training loss: 40364.82263688224\n",
      "Epoch 0 step 40: training accuarcy: 0.1835\n",
      "Epoch 0 step 40: training loss: 40487.62312729213\n",
      "Epoch 0 step 41: training accuarcy: 0.17500000000000002\n",
      "Epoch 0 step 41: training loss: 40188.308899820506\n",
      "Epoch 0 step 42: training accuarcy: 0.18\n",
      "Epoch 0 step 42: training loss: 39996.17796311535\n",
      "Epoch 0 step 43: training accuarcy: 0.1785\n",
      "Epoch 0 step 43: training loss: 40555.39409224248\n",
      "Epoch 0 step 44: training accuarcy: 0.1675\n",
      "Epoch 0 step 44: training loss: 40060.94432434002\n",
      "Epoch 0 step 45: training accuarcy: 0.177\n",
      "Epoch 0 step 45: training loss: 39228.99929689483\n",
      "Epoch 0 step 46: training accuarcy: 0.1955\n",
      "Epoch 0 step 46: training loss: 40118.95231877017\n",
      "Epoch 0 step 47: training accuarcy: 0.171\n",
      "Epoch 0 step 47: training loss: 39894.078583517134\n",
      "Epoch 0 step 48: training accuarcy: 0.17300000000000001\n",
      "Epoch 0 step 48: training loss: 39505.228706759764\n",
      "Epoch 0 step 49: training accuarcy: 0.1835\n",
      "Epoch 0 step 49: training loss: 39881.60481229251\n",
      "Epoch 0 step 50: training accuarcy: 0.1705\n",
      "Epoch 0 step 50: training loss: 39549.02996870786\n",
      "Epoch 0 step 51: training accuarcy: 0.1765\n",
      "Epoch 0 step 51: training loss: 39144.08989896833\n",
      "Epoch 0 step 52: training accuarcy: 0.185\n",
      "Epoch 0 step 52: training loss: 38691.65167567373\n",
      "Epoch 0 step 53: training accuarcy: 0.186\n",
      "Epoch 0 step 53: training loss: 39663.244671951245\n",
      "Epoch 0 step 54: training accuarcy: 0.1695\n",
      "Epoch 0 step 54: training loss: 38764.35162212124\n",
      "Epoch 0 step 55: training accuarcy: 0.189\n",
      "Epoch 0 step 55: training loss: 38891.43185017971\n",
      "Epoch 0 step 56: training accuarcy: 0.1815\n",
      "Epoch 0 step 56: training loss: 39140.7475195002\n",
      "Epoch 0 step 57: training accuarcy: 0.178\n",
      "Epoch 0 step 57: training loss: 39143.21242180671\n",
      "Epoch 0 step 58: training accuarcy: 0.17350000000000002\n",
      "Epoch 0 step 58: training loss: 39224.117546391106\n",
      "Epoch 0 step 59: training accuarcy: 0.17550000000000002\n",
      "Epoch 0 step 59: training loss: 38620.83066254412\n",
      "Epoch 0 step 60: training accuarcy: 0.1825\n",
      "Epoch 0 step 60: training loss: 38840.26056338491\n",
      "Epoch 0 step 61: training accuarcy: 0.178\n",
      "Epoch 0 step 61: training loss: 37999.08942167196\n",
      "Epoch 0 step 62: training accuarcy: 0.1985\n",
      "Epoch 0 step 62: training loss: 38462.656995377736\n",
      "Epoch 0 step 63: training accuarcy: 0.1795\n",
      "Epoch 0 step 63: training loss: 38954.67067472744\n",
      "Epoch 0 step 64: training accuarcy: 0.17250000000000001\n",
      "Epoch 0 step 64: training loss: 39103.926935887015\n",
      "Epoch 0 step 65: training accuarcy: 0.1695\n",
      "Epoch 0 step 65: training loss: 38680.41949921145\n",
      "Epoch 0 step 66: training accuarcy: 0.17450000000000002\n",
      "Epoch 0 step 66: training loss: 38856.341405956904\n",
      "Epoch 0 step 67: training accuarcy: 0.17300000000000001\n",
      "Epoch 0 step 67: training loss: 39336.97256572337\n",
      "Epoch 0 step 68: training accuarcy: 0.1655\n",
      "Epoch 0 step 68: training loss: 37884.38151558193\n",
      "Epoch 0 step 69: training accuarcy: 0.187\n",
      "Epoch 0 step 69: training loss: 38524.446282019024\n",
      "Epoch 0 step 70: training accuarcy: 0.1805\n",
      "Epoch 0 step 70: training loss: 38628.916290418754\n",
      "Epoch 0 step 71: training accuarcy: 0.17300000000000001\n",
      "Epoch 0 step 71: training loss: 39094.406036899854\n",
      "Epoch 0 step 72: training accuarcy: 0.1605\n",
      "Epoch 0 step 72: training loss: 38305.43679327411\n",
      "Epoch 0 step 73: training accuarcy: 0.18\n",
      "Epoch 0 step 73: training loss: 38795.894541157395\n",
      "Epoch 0 step 74: training accuarcy: 0.167\n",
      "Epoch 0 step 74: training loss: 38219.533901633964\n",
      "Epoch 0 step 75: training accuarcy: 0.179\n",
      "Epoch 0 step 75: training loss: 38295.04642160455\n",
      "Epoch 0 step 76: training accuarcy: 0.179\n",
      "Epoch 0 step 76: training loss: 38652.335088720385\n",
      "Epoch 0 step 77: training accuarcy: 0.169\n",
      "Epoch 0 step 77: training loss: 37664.407704755045\n",
      "Epoch 0 step 78: training accuarcy: 0.1915\n",
      "Epoch 0 step 78: training loss: 37518.62471730869\n",
      "Epoch 0 step 79: training accuarcy: 0.193\n",
      "Epoch 0 step 79: training loss: 38120.38962709298\n",
      "Epoch 0 step 80: training accuarcy: 0.178\n",
      "Epoch 0 step 80: training loss: 38141.58367501145\n",
      "Epoch 0 step 81: training accuarcy: 0.176\n",
      "Epoch 0 step 81: training loss: 37906.91532911064\n",
      "Epoch 0 step 82: training accuarcy: 0.1805\n",
      "Epoch 0 step 82: training loss: 37792.39314600669\n",
      "Epoch 0 step 83: training accuarcy: 0.1855\n",
      "Epoch 0 step 83: training loss: 38184.93026423496\n",
      "Epoch 0 step 84: training accuarcy: 0.17550000000000002\n",
      "Epoch 0 step 84: training loss: 38633.342710068304\n",
      "Epoch 0 step 85: training accuarcy: 0.166\n",
      "Epoch 0 step 85: training loss: 37953.228375240134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 86: training accuarcy: 0.181\n",
      "Epoch 0 step 86: training loss: 37858.21036166166\n",
      "Epoch 0 step 87: training accuarcy: 0.1825\n",
      "Epoch 0 step 87: training loss: 37750.848116062116\n",
      "Epoch 0 step 88: training accuarcy: 0.184\n",
      "Epoch 0 step 88: training loss: 37668.99773688212\n",
      "Epoch 0 step 89: training accuarcy: 0.1845\n",
      "Epoch 0 step 89: training loss: 36749.47982047214\n",
      "Epoch 0 step 90: training accuarcy: 0.20500000000000002\n",
      "Epoch 0 step 90: training loss: 38048.29466173532\n",
      "Epoch 0 step 91: training accuarcy: 0.17500000000000002\n",
      "Epoch 0 step 91: training loss: 37478.72493630273\n",
      "Epoch 0 step 92: training accuarcy: 0.1875\n",
      "Epoch 0 step 92: training loss: 37673.83106061367\n",
      "Epoch 0 step 93: training accuarcy: 0.183\n",
      "Epoch 0 step 93: training loss: 38103.691997753005\n",
      "Epoch 0 step 94: training accuarcy: 0.17300000000000001\n",
      "Epoch 0 step 94: training loss: 37515.12181757482\n",
      "Epoch 0 step 95: training accuarcy: 0.187\n",
      "Epoch 0 step 95: training loss: 38546.88002983408\n",
      "Epoch 0 step 96: training accuarcy: 0.163\n",
      "Epoch 0 step 96: training loss: 37643.04063960015\n",
      "Epoch 0 step 97: training accuarcy: 0.184\n",
      "Epoch 0 step 97: training loss: 38097.96758999098\n",
      "Epoch 0 step 98: training accuarcy: 0.17300000000000001\n",
      "Epoch 0 step 98: training loss: 38153.57852687145\n",
      "Epoch 0 step 99: training accuarcy: 0.17200000000000001\n",
      "Epoch 0 step 99: training loss: 38049.394120159406\n",
      "Epoch 0 step 100: training accuarcy: 0.17550000000000002\n",
      "Epoch 0 step 100: training loss: 37200.73941957463\n",
      "Epoch 0 step 101: training accuarcy: 0.19\n",
      "Epoch 0 step 101: training loss: 37751.73334666753\n",
      "Epoch 0 step 102: training accuarcy: 0.1805\n",
      "Epoch 0 step 102: training loss: 37205.99810839596\n",
      "Epoch 0 step 103: training accuarcy: 0.193\n",
      "Epoch 0 step 103: training loss: 37597.51767404792\n",
      "Epoch 0 step 104: training accuarcy: 0.182\n",
      "Epoch 0 step 104: training loss: 38045.49970297687\n",
      "Epoch 0 step 105: training accuarcy: 0.1685\n",
      "Epoch 0 step 105: training loss: 37678.06516970185\n",
      "Epoch 0 step 106: training accuarcy: 0.182\n",
      "Epoch 0 step 106: training loss: 37966.16379581505\n",
      "Epoch 0 step 107: training accuarcy: 0.1775\n",
      "Epoch 0 step 107: training loss: 37672.981412016874\n",
      "Epoch 0 step 108: training accuarcy: 0.18\n",
      "Epoch 0 step 108: training loss: 37174.9525556498\n",
      "Epoch 0 step 109: training accuarcy: 0.187\n",
      "Epoch 0 step 109: training loss: 36939.693492951825\n",
      "Epoch 0 step 110: training accuarcy: 0.194\n",
      "Epoch 0 step 110: training loss: 37960.9175528059\n",
      "Epoch 0 step 111: training accuarcy: 0.168\n",
      "Epoch 0 step 111: training loss: 36965.07265870287\n",
      "Epoch 0 step 112: training accuarcy: 0.19\n",
      "Epoch 0 step 112: training loss: 35543.8422675332\n",
      "Epoch 0 step 113: training accuarcy: 0.224\n",
      "Epoch 0 step 113: training loss: 35910.95125067452\n",
      "Epoch 0 step 114: training accuarcy: 0.2125\n",
      "Epoch 0 step 114: training loss: 34033.297617412245\n",
      "Epoch 0 step 115: training accuarcy: 0.21855345911949686\n",
      "Epoch 0: train loss 40382.36182120465, train accuarcy 0.17297561466693878\n",
      "Epoch 0: valid loss 36540.36625406834, valid accuarcy 0.18448759615421295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████▏                                                                                                                                     | 1/8 [00:53<06:16, 53.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 34925.639471246526\n",
      "Epoch 1 step 116: training accuarcy: 0.234\n",
      "Epoch 1 step 116: training loss: 34041.26191688338\n",
      "Epoch 1 step 117: training accuarcy: 0.251\n",
      "Epoch 1 step 117: training loss: 33871.52605941463\n",
      "Epoch 1 step 118: training accuarcy: 0.2505\n",
      "Epoch 1 step 118: training loss: 32662.330786107646\n",
      "Epoch 1 step 119: training accuarcy: 0.2775\n",
      "Epoch 1 step 119: training loss: 31345.23160419582\n",
      "Epoch 1 step 120: training accuarcy: 0.305\n",
      "Epoch 1 step 120: training loss: 32140.35749981522\n",
      "Epoch 1 step 121: training accuarcy: 0.296\n",
      "Epoch 1 step 121: training loss: 30362.131924886664\n",
      "Epoch 1 step 122: training accuarcy: 0.33\n",
      "Epoch 1 step 122: training loss: 28139.78489340363\n",
      "Epoch 1 step 123: training accuarcy: 0.38\n",
      "Epoch 1 step 123: training loss: 27213.703842245042\n",
      "Epoch 1 step 124: training accuarcy: 0.4025\n",
      "Epoch 1 step 124: training loss: 27072.056154350335\n",
      "Epoch 1 step 125: training accuarcy: 0.4045\n",
      "Epoch 1 step 125: training loss: 27190.18588764609\n",
      "Epoch 1 step 126: training accuarcy: 0.404\n",
      "Epoch 1 step 126: training loss: 25520.77389315397\n",
      "Epoch 1 step 127: training accuarcy: 0.439\n",
      "Epoch 1 step 127: training loss: 24727.84278332411\n",
      "Epoch 1 step 128: training accuarcy: 0.453\n",
      "Epoch 1 step 128: training loss: 23225.774772460205\n",
      "Epoch 1 step 129: training accuarcy: 0.485\n",
      "Epoch 1 step 129: training loss: 23723.640570076153\n",
      "Epoch 1 step 130: training accuarcy: 0.47850000000000004\n",
      "Epoch 1 step 130: training loss: 23520.805494548564\n",
      "Epoch 1 step 131: training accuarcy: 0.47400000000000003\n",
      "Epoch 1 step 131: training loss: 22654.37704074625\n",
      "Epoch 1 step 132: training accuarcy: 0.491\n",
      "Epoch 1 step 132: training loss: 20655.129205233447\n",
      "Epoch 1 step 133: training accuarcy: 0.539\n",
      "Epoch 1 step 133: training loss: 18479.012458888235\n",
      "Epoch 1 step 134: training accuarcy: 0.592\n",
      "Epoch 1 step 134: training loss: 17579.349939737782\n",
      "Epoch 1 step 135: training accuarcy: 0.6125\n",
      "Epoch 1 step 135: training loss: 17208.761533894976\n",
      "Epoch 1 step 136: training accuarcy: 0.6225\n",
      "Epoch 1 step 136: training loss: 16562.329907585194\n",
      "Epoch 1 step 137: training accuarcy: 0.6365000000000001\n",
      "Epoch 1 step 137: training loss: 16739.35941652698\n",
      "Epoch 1 step 138: training accuarcy: 0.63\n",
      "Epoch 1 step 138: training loss: 17228.512138947783\n",
      "Epoch 1 step 139: training accuarcy: 0.6235\n",
      "Epoch 1 step 139: training loss: 15850.353458448417\n",
      "Epoch 1 step 140: training accuarcy: 0.652\n",
      "Epoch 1 step 140: training loss: 15679.862683886264\n",
      "Epoch 1 step 141: training accuarcy: 0.6565\n",
      "Epoch 1 step 141: training loss: 15579.6742842227\n",
      "Epoch 1 step 142: training accuarcy: 0.6595\n",
      "Epoch 1 step 142: training loss: 15074.84042806489\n",
      "Epoch 1 step 143: training accuarcy: 0.6695\n",
      "Epoch 1 step 143: training loss: 14795.201899705618\n",
      "Epoch 1 step 144: training accuarcy: 0.6745\n",
      "Epoch 1 step 144: training loss: 14996.75729182769\n",
      "Epoch 1 step 145: training accuarcy: 0.67\n",
      "Epoch 1 step 145: training loss: 15107.40893952662\n",
      "Epoch 1 step 146: training accuarcy: 0.671\n",
      "Epoch 1 step 146: training loss: 15260.143884060943\n",
      "Epoch 1 step 147: training accuarcy: 0.668\n",
      "Epoch 1 step 147: training loss: 13435.077232767211\n",
      "Epoch 1 step 148: training accuarcy: 0.7045\n",
      "Epoch 1 step 148: training loss: 14321.582297084778\n",
      "Epoch 1 step 149: training accuarcy: 0.685\n",
      "Epoch 1 step 149: training loss: 13674.845182087915\n",
      "Epoch 1 step 150: training accuarcy: 0.7005\n",
      "Epoch 1 step 150: training loss: 13635.275545034137\n",
      "Epoch 1 step 151: training accuarcy: 0.7025\n",
      "Epoch 1 step 151: training loss: 13512.510847972668\n",
      "Epoch 1 step 152: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 152: training loss: 13626.686097668602\n",
      "Epoch 1 step 153: training accuarcy: 0.7045\n",
      "Epoch 1 step 153: training loss: 14422.517117429557\n",
      "Epoch 1 step 154: training accuarcy: 0.683\n",
      "Epoch 1 step 154: training loss: 13594.463057786357\n",
      "Epoch 1 step 155: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 155: training loss: 13120.133089139954\n",
      "Epoch 1 step 156: training accuarcy: 0.7155\n",
      "Epoch 1 step 156: training loss: 13322.832059307793\n",
      "Epoch 1 step 157: training accuarcy: 0.707\n",
      "Epoch 1 step 157: training loss: 12846.492815378564\n",
      "Epoch 1 step 158: training accuarcy: 0.7165\n",
      "Epoch 1 step 158: training loss: 12526.998267904879\n",
      "Epoch 1 step 159: training accuarcy: 0.7255\n",
      "Epoch 1 step 159: training loss: 14331.735035348986\n",
      "Epoch 1 step 160: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 160: training loss: 13411.540429105855\n",
      "Epoch 1 step 161: training accuarcy: 0.708\n",
      "Epoch 1 step 161: training loss: 11857.161980805111\n",
      "Epoch 1 step 162: training accuarcy: 0.7385\n",
      "Epoch 1 step 162: training loss: 12778.946450296353\n",
      "Epoch 1 step 163: training accuarcy: 0.7195\n",
      "Epoch 1 step 163: training loss: 12563.726038720357\n",
      "Epoch 1 step 164: training accuarcy: 0.725\n",
      "Epoch 1 step 164: training loss: 11236.411495067134\n",
      "Epoch 1 step 165: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 165: training loss: 12664.029356895144\n",
      "Epoch 1 step 166: training accuarcy: 0.723\n",
      "Epoch 1 step 166: training loss: 12326.709793992239\n",
      "Epoch 1 step 167: training accuarcy: 0.7285\n",
      "Epoch 1 step 167: training loss: 12105.79408294312\n",
      "Epoch 1 step 168: training accuarcy: 0.7375\n",
      "Epoch 1 step 168: training loss: 12151.817176176282\n",
      "Epoch 1 step 169: training accuarcy: 0.734\n",
      "Epoch 1 step 169: training loss: 10983.485585853598\n",
      "Epoch 1 step 170: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 170: training loss: 13715.501824243313\n",
      "Epoch 1 step 171: training accuarcy: 0.6970000000000001\n",
      "Epoch 1 step 171: training loss: 11576.123596448708\n",
      "Epoch 1 step 172: training accuarcy: 0.7445\n",
      "Epoch 1 step 172: training loss: 11982.273607675756\n",
      "Epoch 1 step 173: training accuarcy: 0.7375\n",
      "Epoch 1 step 173: training loss: 11793.163095518774\n",
      "Epoch 1 step 174: training accuarcy: 0.74\n",
      "Epoch 1 step 174: training loss: 12919.757710618644\n",
      "Epoch 1 step 175: training accuarcy: 0.717\n",
      "Epoch 1 step 175: training loss: 11865.291257731524\n",
      "Epoch 1 step 176: training accuarcy: 0.7415\n",
      "Epoch 1 step 176: training loss: 12035.151340091972\n",
      "Epoch 1 step 177: training accuarcy: 0.7375\n",
      "Epoch 1 step 177: training loss: 11635.4821542541\n",
      "Epoch 1 step 178: training accuarcy: 0.7455\n",
      "Epoch 1 step 178: training loss: 12288.6799090534\n",
      "Epoch 1 step 179: training accuarcy: 0.7295\n",
      "Epoch 1 step 179: training loss: 12512.604288702225\n",
      "Epoch 1 step 180: training accuarcy: 0.7275\n",
      "Epoch 1 step 180: training loss: 11212.999727785631\n",
      "Epoch 1 step 181: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 181: training loss: 11467.817906457507\n",
      "Epoch 1 step 182: training accuarcy: 0.7495\n",
      "Epoch 1 step 182: training loss: 11892.594809723367\n",
      "Epoch 1 step 183: training accuarcy: 0.74\n",
      "Epoch 1 step 183: training loss: 11768.555237295792\n",
      "Epoch 1 step 184: training accuarcy: 0.741\n",
      "Epoch 1 step 184: training loss: 11016.037056742394\n",
      "Epoch 1 step 185: training accuarcy: 0.758\n",
      "Epoch 1 step 185: training loss: 11568.863902966412\n",
      "Epoch 1 step 186: training accuarcy: 0.7475\n",
      "Epoch 1 step 186: training loss: 11066.477843091967\n",
      "Epoch 1 step 187: training accuarcy: 0.757\n",
      "Epoch 1 step 187: training loss: 10360.56942597487\n",
      "Epoch 1 step 188: training accuarcy: 0.7685\n",
      "Epoch 1 step 188: training loss: 11778.426816145957\n",
      "Epoch 1 step 189: training accuarcy: 0.7415\n",
      "Epoch 1 step 189: training loss: 10742.28134345372\n",
      "Epoch 1 step 190: training accuarcy: 0.764\n",
      "Epoch 1 step 190: training loss: 11906.709137602606\n",
      "Epoch 1 step 191: training accuarcy: 0.7385\n",
      "Epoch 1 step 191: training loss: 11890.554298957186\n",
      "Epoch 1 step 192: training accuarcy: 0.7395\n",
      "Epoch 1 step 192: training loss: 11102.63528648031\n",
      "Epoch 1 step 193: training accuarcy: 0.758\n",
      "Epoch 1 step 193: training loss: 10835.447267682308\n",
      "Epoch 1 step 194: training accuarcy: 0.763\n",
      "Epoch 1 step 194: training loss: 11361.081806983233\n",
      "Epoch 1 step 195: training accuarcy: 0.7525000000000001\n",
      "Epoch 1 step 195: training loss: 10947.207053069746\n",
      "Epoch 1 step 196: training accuarcy: 0.761\n",
      "Epoch 1 step 196: training loss: 10383.657709280267\n",
      "Epoch 1 step 197: training accuarcy: 0.772\n",
      "Epoch 1 step 197: training loss: 11321.32102926665\n",
      "Epoch 1 step 198: training accuarcy: 0.7525000000000001\n",
      "Epoch 1 step 198: training loss: 11342.71689227928\n",
      "Epoch 1 step 199: training accuarcy: 0.751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 199: training loss: 11479.094147793567\n",
      "Epoch 1 step 200: training accuarcy: 0.7495\n",
      "Epoch 1 step 200: training loss: 11173.589863370657\n",
      "Epoch 1 step 201: training accuarcy: 0.7555000000000001\n",
      "Epoch 1 step 201: training loss: 11706.265441011805\n",
      "Epoch 1 step 202: training accuarcy: 0.742\n",
      "Epoch 1 step 202: training loss: 11495.80540972131\n",
      "Epoch 1 step 203: training accuarcy: 0.751\n",
      "Epoch 1 step 203: training loss: 10808.541132846634\n",
      "Epoch 1 step 204: training accuarcy: 0.7655000000000001\n",
      "Epoch 1 step 204: training loss: 11131.307686010918\n",
      "Epoch 1 step 205: training accuarcy: 0.7555000000000001\n",
      "Epoch 1 step 205: training loss: 10958.778693349352\n",
      "Epoch 1 step 206: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 206: training loss: 11245.899690338612\n",
      "Epoch 1 step 207: training accuarcy: 0.754\n",
      "Epoch 1 step 207: training loss: 9839.366040331755\n",
      "Epoch 1 step 208: training accuarcy: 0.786\n",
      "Epoch 1 step 208: training loss: 11590.077215328698\n",
      "Epoch 1 step 209: training accuarcy: 0.744\n",
      "Epoch 1 step 209: training loss: 10675.298272007281\n",
      "Epoch 1 step 210: training accuarcy: 0.7655000000000001\n",
      "Epoch 1 step 210: training loss: 10995.031013913693\n",
      "Epoch 1 step 211: training accuarcy: 0.757\n",
      "Epoch 1 step 211: training loss: 11397.090151245045\n",
      "Epoch 1 step 212: training accuarcy: 0.752\n",
      "Epoch 1 step 212: training loss: 12033.223525936148\n",
      "Epoch 1 step 213: training accuarcy: 0.735\n",
      "Epoch 1 step 213: training loss: 11038.389456957431\n",
      "Epoch 1 step 214: training accuarcy: 0.76\n",
      "Epoch 1 step 214: training loss: 10742.768991930587\n",
      "Epoch 1 step 215: training accuarcy: 0.763\n",
      "Epoch 1 step 215: training loss: 11496.01784264898\n",
      "Epoch 1 step 216: training accuarcy: 0.7485\n",
      "Epoch 1 step 216: training loss: 10495.810715106143\n",
      "Epoch 1 step 217: training accuarcy: 0.766\n",
      "Epoch 1 step 217: training loss: 11633.682372247602\n",
      "Epoch 1 step 218: training accuarcy: 0.743\n",
      "Epoch 1 step 218: training loss: 11218.848658866627\n",
      "Epoch 1 step 219: training accuarcy: 0.7555000000000001\n",
      "Epoch 1 step 219: training loss: 11083.248830282473\n",
      "Epoch 1 step 220: training accuarcy: 0.7575000000000001\n",
      "Epoch 1 step 220: training loss: 10410.589625168179\n",
      "Epoch 1 step 221: training accuarcy: 0.771\n",
      "Epoch 1 step 221: training loss: 10535.915924590136\n",
      "Epoch 1 step 222: training accuarcy: 0.7685\n",
      "Epoch 1 step 222: training loss: 10694.801051149314\n",
      "Epoch 1 step 223: training accuarcy: 0.766\n",
      "Epoch 1 step 223: training loss: 10673.906682135657\n",
      "Epoch 1 step 224: training accuarcy: 0.766\n",
      "Epoch 1 step 224: training loss: 11726.181403749453\n",
      "Epoch 1 step 225: training accuarcy: 0.743\n",
      "Epoch 1 step 225: training loss: 11629.14847478527\n",
      "Epoch 1 step 226: training accuarcy: 0.745\n",
      "Epoch 1 step 226: training loss: 11425.106098245571\n",
      "Epoch 1 step 227: training accuarcy: 0.75\n",
      "Epoch 1 step 227: training loss: 10468.359993034068\n",
      "Epoch 1 step 228: training accuarcy: 0.7695\n",
      "Epoch 1 step 228: training loss: 10601.219380766617\n",
      "Epoch 1 step 229: training accuarcy: 0.7705\n",
      "Epoch 1 step 229: training loss: 10135.466874308451\n",
      "Epoch 1 step 230: training accuarcy: 0.7678197064989518\n",
      "Epoch 1: train loss 14838.38006169204, train accuarcy 0.675815999507904\n",
      "Epoch 1: valid loss 10609.14536322197, valid accuarcy 0.7638893723487854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████▎                                                                                                                  | 2/8 [01:45<05:19, 53.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 230: training loss: 11290.368071192746\n",
      "Epoch 2 step 231: training accuarcy: 0.755\n",
      "Epoch 2 step 231: training loss: 10805.28259730721\n",
      "Epoch 2 step 232: training accuarcy: 0.765\n",
      "Epoch 2 step 232: training loss: 11621.15467087081\n",
      "Epoch 2 step 233: training accuarcy: 0.7455\n",
      "Epoch 2 step 233: training loss: 10831.275869539475\n",
      "Epoch 2 step 234: training accuarcy: 0.7625000000000001\n",
      "Epoch 2 step 234: training loss: 10767.109036515863\n",
      "Epoch 2 step 235: training accuarcy: 0.7665000000000001\n",
      "Epoch 2 step 235: training loss: 10521.411400122857\n",
      "Epoch 2 step 236: training accuarcy: 0.7705\n",
      "Epoch 2 step 236: training loss: 10686.232227857763\n",
      "Epoch 2 step 237: training accuarcy: 0.7645000000000001\n",
      "Epoch 2 step 237: training loss: 10899.325706242864\n",
      "Epoch 2 step 238: training accuarcy: 0.7625000000000001\n",
      "Epoch 2 step 238: training loss: 10453.899737061456\n",
      "Epoch 2 step 239: training accuarcy: 0.7715\n",
      "Epoch 2 step 239: training loss: 10177.603593739335\n",
      "Epoch 2 step 240: training accuarcy: 0.778\n",
      "Epoch 2 step 240: training loss: 10533.367651671451\n",
      "Epoch 2 step 241: training accuarcy: 0.7695\n",
      "Epoch 2 step 241: training loss: 10701.086531637233\n",
      "Epoch 2 step 242: training accuarcy: 0.764\n",
      "Epoch 2 step 242: training loss: 11045.697965686202\n",
      "Epoch 2 step 243: training accuarcy: 0.7585000000000001\n",
      "Epoch 2 step 243: training loss: 11207.107888367627\n",
      "Epoch 2 step 244: training accuarcy: 0.755\n",
      "Epoch 2 step 244: training loss: 10747.09482594178\n",
      "Epoch 2 step 245: training accuarcy: 0.763\n",
      "Epoch 2 step 245: training loss: 10883.541900758459\n",
      "Epoch 2 step 246: training accuarcy: 0.7605000000000001\n",
      "Epoch 2 step 246: training loss: 10416.311661293936\n",
      "Epoch 2 step 247: training accuarcy: 0.7735\n",
      "Epoch 2 step 247: training loss: 10421.243016822908\n",
      "Epoch 2 step 248: training accuarcy: 0.7705\n",
      "Epoch 2 step 248: training loss: 10661.915712850556\n",
      "Epoch 2 step 249: training accuarcy: 0.767\n",
      "Epoch 2 step 249: training loss: 11762.356882259763\n",
      "Epoch 2 step 250: training accuarcy: 0.7405\n",
      "Epoch 2 step 250: training loss: 10417.53171800467\n",
      "Epoch 2 step 251: training accuarcy: 0.7705\n",
      "Epoch 2 step 251: training loss: 10990.808224353816\n",
      "Epoch 2 step 252: training accuarcy: 0.759\n",
      "Epoch 2 step 252: training loss: 11288.253392457978\n",
      "Epoch 2 step 253: training accuarcy: 0.753\n",
      "Epoch 2 step 253: training loss: 11205.011706838655\n",
      "Epoch 2 step 254: training accuarcy: 0.755\n",
      "Epoch 2 step 254: training loss: 10503.168013033612\n",
      "Epoch 2 step 255: training accuarcy: 0.771\n",
      "Epoch 2 step 255: training loss: 9946.14336970754\n",
      "Epoch 2 step 256: training accuarcy: 0.7835\n",
      "Epoch 2 step 256: training loss: 10511.479066816799\n",
      "Epoch 2 step 257: training accuarcy: 0.77\n",
      "Epoch 2 step 257: training loss: 10966.839593383009\n",
      "Epoch 2 step 258: training accuarcy: 0.7585000000000001\n",
      "Epoch 2 step 258: training loss: 10809.656938279142\n",
      "Epoch 2 step 259: training accuarcy: 0.763\n",
      "Epoch 2 step 259: training loss: 10092.991236200047\n",
      "Epoch 2 step 260: training accuarcy: 0.781\n",
      "Epoch 2 step 260: training loss: 9809.184478299438\n",
      "Epoch 2 step 261: training accuarcy: 0.784\n",
      "Epoch 2 step 261: training loss: 10478.572554851333\n",
      "Epoch 2 step 262: training accuarcy: 0.771\n",
      "Epoch 2 step 262: training loss: 10478.946965446003\n",
      "Epoch 2 step 263: training accuarcy: 0.7695\n",
      "Epoch 2 step 263: training loss: 10163.053407662255\n",
      "Epoch 2 step 264: training accuarcy: 0.7765\n",
      "Epoch 2 step 264: training loss: 10451.826073468399\n",
      "Epoch 2 step 265: training accuarcy: 0.77\n",
      "Epoch 2 step 265: training loss: 10758.502595186585\n",
      "Epoch 2 step 266: training accuarcy: 0.766\n",
      "Epoch 2 step 266: training loss: 10224.96502073578\n",
      "Epoch 2 step 267: training accuarcy: 0.7775\n",
      "Epoch 2 step 267: training loss: 10228.283000322106\n",
      "Epoch 2 step 268: training accuarcy: 0.7725\n",
      "Epoch 2 step 268: training loss: 10322.672962404875\n",
      "Epoch 2 step 269: training accuarcy: 0.7735\n",
      "Epoch 2 step 269: training loss: 11355.075918990662\n",
      "Epoch 2 step 270: training accuarcy: 0.754\n",
      "Epoch 2 step 270: training loss: 10346.773699667157\n",
      "Epoch 2 step 271: training accuarcy: 0.7725\n",
      "Epoch 2 step 271: training loss: 10118.165896154878\n",
      "Epoch 2 step 272: training accuarcy: 0.7785\n",
      "Epoch 2 step 272: training loss: 10140.863522830732\n",
      "Epoch 2 step 273: training accuarcy: 0.7795\n",
      "Epoch 2 step 273: training loss: 10947.62067438409\n",
      "Epoch 2 step 274: training accuarcy: 0.76\n",
      "Epoch 2 step 274: training loss: 10784.286518984274\n",
      "Epoch 2 step 275: training accuarcy: 0.7635000000000001\n",
      "Epoch 2 step 275: training loss: 10642.339114442877\n",
      "Epoch 2 step 276: training accuarcy: 0.769\n",
      "Epoch 2 step 276: training loss: 10817.26517481146\n",
      "Epoch 2 step 277: training accuarcy: 0.761\n",
      "Epoch 2 step 277: training loss: 9935.57796921729\n",
      "Epoch 2 step 278: training accuarcy: 0.7815\n",
      "Epoch 2 step 278: training loss: 10417.162800746426\n",
      "Epoch 2 step 279: training accuarcy: 0.772\n",
      "Epoch 2 step 279: training loss: 10279.634587026569\n",
      "Epoch 2 step 280: training accuarcy: 0.7775\n",
      "Epoch 2 step 280: training loss: 10243.34262137314\n",
      "Epoch 2 step 281: training accuarcy: 0.7775\n",
      "Epoch 2 step 281: training loss: 10560.853569561597\n",
      "Epoch 2 step 282: training accuarcy: 0.7675000000000001\n",
      "Epoch 2 step 282: training loss: 10820.07921727328\n",
      "Epoch 2 step 283: training accuarcy: 0.7645000000000001\n",
      "Epoch 2 step 283: training loss: 10112.070840582475\n",
      "Epoch 2 step 284: training accuarcy: 0.7785\n",
      "Epoch 2 step 284: training loss: 11011.958680130727\n",
      "Epoch 2 step 285: training accuarcy: 0.759\n",
      "Epoch 2 step 285: training loss: 9630.277499514345\n",
      "Epoch 2 step 286: training accuarcy: 0.7895\n",
      "Epoch 2 step 286: training loss: 10606.951595716895\n",
      "Epoch 2 step 287: training accuarcy: 0.7655000000000001\n",
      "Epoch 2 step 287: training loss: 9721.80534822601\n",
      "Epoch 2 step 288: training accuarcy: 0.789\n",
      "Epoch 2 step 288: training loss: 10316.93025504344\n",
      "Epoch 2 step 289: training accuarcy: 0.774\n",
      "Epoch 2 step 289: training loss: 10218.398420064354\n",
      "Epoch 2 step 290: training accuarcy: 0.7785\n",
      "Epoch 2 step 290: training loss: 10036.779554579829\n",
      "Epoch 2 step 291: training accuarcy: 0.7815\n",
      "Epoch 2 step 291: training loss: 10540.181775255249\n",
      "Epoch 2 step 292: training accuarcy: 0.7695\n",
      "Epoch 2 step 292: training loss: 9969.173422394306\n",
      "Epoch 2 step 293: training accuarcy: 0.784\n",
      "Epoch 2 step 293: training loss: 10346.084523087002\n",
      "Epoch 2 step 294: training accuarcy: 0.773\n",
      "Epoch 2 step 294: training loss: 9933.306333314027\n",
      "Epoch 2 step 295: training accuarcy: 0.7835\n",
      "Epoch 2 step 295: training loss: 9325.652670150837\n",
      "Epoch 2 step 296: training accuarcy: 0.796\n",
      "Epoch 2 step 296: training loss: 9928.273500511881\n",
      "Epoch 2 step 297: training accuarcy: 0.7815\n",
      "Epoch 2 step 297: training loss: 9997.321635964643\n",
      "Epoch 2 step 298: training accuarcy: 0.7785\n",
      "Epoch 2 step 298: training loss: 10383.224620446415\n",
      "Epoch 2 step 299: training accuarcy: 0.7745\n",
      "Epoch 2 step 299: training loss: 10085.236070305364\n",
      "Epoch 2 step 300: training accuarcy: 0.7815\n",
      "Epoch 2 step 300: training loss: 9830.95316852211\n",
      "Epoch 2 step 301: training accuarcy: 0.7855\n",
      "Epoch 2 step 301: training loss: 10502.432356566795\n",
      "Epoch 2 step 302: training accuarcy: 0.7715\n",
      "Epoch 2 step 302: training loss: 9652.922684809899\n",
      "Epoch 2 step 303: training accuarcy: 0.788\n",
      "Epoch 2 step 303: training loss: 10021.851624972864\n",
      "Epoch 2 step 304: training accuarcy: 0.781\n",
      "Epoch 2 step 304: training loss: 9259.431638172493\n",
      "Epoch 2 step 305: training accuarcy: 0.7985\n",
      "Epoch 2 step 305: training loss: 10049.990272720086\n",
      "Epoch 2 step 306: training accuarcy: 0.7775\n",
      "Epoch 2 step 306: training loss: 10062.487403173503\n",
      "Epoch 2 step 307: training accuarcy: 0.7805\n",
      "Epoch 2 step 307: training loss: 10456.708897855198\n",
      "Epoch 2 step 308: training accuarcy: 0.77\n",
      "Epoch 2 step 308: training loss: 10458.616768351472\n",
      "Epoch 2 step 309: training accuarcy: 0.7695\n",
      "Epoch 2 step 309: training loss: 10076.412024322275\n",
      "Epoch 2 step 310: training accuarcy: 0.78\n",
      "Epoch 2 step 310: training loss: 9830.809928101477\n",
      "Epoch 2 step 311: training accuarcy: 0.7845\n",
      "Epoch 2 step 311: training loss: 10302.123296859318\n",
      "Epoch 2 step 312: training accuarcy: 0.7745\n",
      "Epoch 2 step 312: training loss: 9718.758877423745\n",
      "Epoch 2 step 313: training accuarcy: 0.787\n",
      "Epoch 2 step 313: training loss: 10547.395476103035\n",
      "Epoch 2 step 314: training accuarcy: 0.769\n",
      "Epoch 2 step 314: training loss: 9285.078442430642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 315: training accuarcy: 0.794\n",
      "Epoch 2 step 315: training loss: 10298.288840199795\n",
      "Epoch 2 step 316: training accuarcy: 0.7725\n",
      "Epoch 2 step 316: training loss: 9452.588437416618\n",
      "Epoch 2 step 317: training accuarcy: 0.7875\n",
      "Epoch 2 step 317: training loss: 9864.35393184143\n",
      "Epoch 2 step 318: training accuarcy: 0.7835\n",
      "Epoch 2 step 318: training loss: 9899.214060768061\n",
      "Epoch 2 step 319: training accuarcy: 0.7835\n",
      "Epoch 2 step 319: training loss: 10316.985984429879\n",
      "Epoch 2 step 320: training accuarcy: 0.7725\n",
      "Epoch 2 step 320: training loss: 9901.294115740879\n",
      "Epoch 2 step 321: training accuarcy: 0.7825\n",
      "Epoch 2 step 321: training loss: 9612.688519805328\n",
      "Epoch 2 step 322: training accuarcy: 0.7885\n",
      "Epoch 2 step 322: training loss: 11559.785857514089\n",
      "Epoch 2 step 323: training accuarcy: 0.747\n",
      "Epoch 2 step 323: training loss: 9672.506433319153\n",
      "Epoch 2 step 324: training accuarcy: 0.788\n",
      "Epoch 2 step 324: training loss: 10149.730167673606\n",
      "Epoch 2 step 325: training accuarcy: 0.778\n",
      "Epoch 2 step 325: training loss: 10100.59229960932\n",
      "Epoch 2 step 326: training accuarcy: 0.78\n",
      "Epoch 2 step 326: training loss: 9988.701401201179\n",
      "Epoch 2 step 327: training accuarcy: 0.782\n",
      "Epoch 2 step 327: training loss: 10245.273293078368\n",
      "Epoch 2 step 328: training accuarcy: 0.7745\n",
      "Epoch 2 step 328: training loss: 9631.41224400975\n",
      "Epoch 2 step 329: training accuarcy: 0.7885\n",
      "Epoch 2 step 329: training loss: 10542.920044865372\n",
      "Epoch 2 step 330: training accuarcy: 0.7705\n",
      "Epoch 2 step 330: training loss: 10142.685975270613\n",
      "Epoch 2 step 331: training accuarcy: 0.7775\n",
      "Epoch 2 step 331: training loss: 9561.845752487672\n",
      "Epoch 2 step 332: training accuarcy: 0.7915\n",
      "Epoch 2 step 332: training loss: 10301.655101973498\n",
      "Epoch 2 step 333: training accuarcy: 0.7745\n",
      "Epoch 2 step 333: training loss: 9784.501035120622\n",
      "Epoch 2 step 334: training accuarcy: 0.785\n",
      "Epoch 2 step 334: training loss: 10120.37260833612\n",
      "Epoch 2 step 335: training accuarcy: 0.778\n",
      "Epoch 2 step 335: training loss: 9576.388568841041\n",
      "Epoch 2 step 336: training accuarcy: 0.791\n",
      "Epoch 2 step 336: training loss: 9745.97682160154\n",
      "Epoch 2 step 337: training accuarcy: 0.783\n",
      "Epoch 2 step 337: training loss: 9649.908822458796\n",
      "Epoch 2 step 338: training accuarcy: 0.786\n",
      "Epoch 2 step 338: training loss: 9805.209928797534\n",
      "Epoch 2 step 339: training accuarcy: 0.7865\n",
      "Epoch 2 step 339: training loss: 9502.940481021578\n",
      "Epoch 2 step 340: training accuarcy: 0.792\n",
      "Epoch 2 step 340: training loss: 10269.218410998725\n",
      "Epoch 2 step 341: training accuarcy: 0.7745\n",
      "Epoch 2 step 341: training loss: 9480.970528794729\n",
      "Epoch 2 step 342: training accuarcy: 0.7915\n",
      "Epoch 2 step 342: training loss: 9492.046925974242\n",
      "Epoch 2 step 343: training accuarcy: 0.7925\n",
      "Epoch 2 step 343: training loss: 10230.238007272606\n",
      "Epoch 2 step 344: training accuarcy: 0.776\n",
      "Epoch 2 step 344: training loss: 8910.700828673267\n",
      "Epoch 2 step 345: training accuarcy: 0.7961215932914046\n",
      "Epoch 2: train loss 10291.486205090658, train accuarcy 0.7742959856987\n",
      "Epoch 2: valid loss 9119.063063514615, valid accuarcy 0.796968936920166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████▍                                                                                               | 3/8 [02:36<04:22, 52.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 345: training loss: 9648.772800201274\n",
      "Epoch 3 step 346: training accuarcy: 0.789\n",
      "Epoch 3 step 346: training loss: 9505.453760975674\n",
      "Epoch 3 step 347: training accuarcy: 0.7925\n",
      "Epoch 3 step 347: training loss: 8937.577117319735\n",
      "Epoch 3 step 348: training accuarcy: 0.804\n",
      "Epoch 3 step 348: training loss: 9025.758303160854\n",
      "Epoch 3 step 349: training accuarcy: 0.8045\n",
      "Epoch 3 step 349: training loss: 9669.322835405332\n",
      "Epoch 3 step 350: training accuarcy: 0.7895\n",
      "Epoch 3 step 350: training loss: 8844.754405579672\n",
      "Epoch 3 step 351: training accuarcy: 0.807\n",
      "Epoch 3 step 351: training loss: 9188.043403984879\n",
      "Epoch 3 step 352: training accuarcy: 0.8015\n",
      "Epoch 3 step 352: training loss: 9034.039798754213\n",
      "Epoch 3 step 353: training accuarcy: 0.803\n",
      "Epoch 3 step 353: training loss: 8955.573203928237\n",
      "Epoch 3 step 354: training accuarcy: 0.8045\n",
      "Epoch 3 step 354: training loss: 9382.390957622294\n",
      "Epoch 3 step 355: training accuarcy: 0.7935\n",
      "Epoch 3 step 355: training loss: 9148.139000125984\n",
      "Epoch 3 step 356: training accuarcy: 0.798\n",
      "Epoch 3 step 356: training loss: 9584.166457541756\n",
      "Epoch 3 step 357: training accuarcy: 0.79\n",
      "Epoch 3 step 357: training loss: 8685.211886710867\n",
      "Epoch 3 step 358: training accuarcy: 0.81\n",
      "Epoch 3 step 358: training loss: 8777.568492792647\n",
      "Epoch 3 step 359: training accuarcy: 0.8075\n",
      "Epoch 3 step 359: training loss: 9513.861387418001\n",
      "Epoch 3 step 360: training accuarcy: 0.793\n",
      "Epoch 3 step 360: training loss: 8772.860812499493\n",
      "Epoch 3 step 361: training accuarcy: 0.8105\n",
      "Epoch 3 step 361: training loss: 8292.006219532792\n",
      "Epoch 3 step 362: training accuarcy: 0.8195\n",
      "Epoch 3 step 362: training loss: 9722.611226386432\n",
      "Epoch 3 step 363: training accuarcy: 0.789\n",
      "Epoch 3 step 363: training loss: 9165.479404437348\n",
      "Epoch 3 step 364: training accuarcy: 0.799\n",
      "Epoch 3 step 364: training loss: 8847.02499631803\n",
      "Epoch 3 step 365: training accuarcy: 0.8045\n",
      "Epoch 3 step 365: training loss: 9456.452872661048\n",
      "Epoch 3 step 366: training accuarcy: 0.794\n",
      "Epoch 3 step 366: training loss: 9090.668230498371\n",
      "Epoch 3 step 367: training accuarcy: 0.8035\n",
      "Epoch 3 step 367: training loss: 8939.014038333715\n",
      "Epoch 3 step 368: training accuarcy: 0.805\n",
      "Epoch 3 step 368: training loss: 8681.353036958697\n",
      "Epoch 3 step 369: training accuarcy: 0.812\n",
      "Epoch 3 step 369: training loss: 9214.014873306114\n",
      "Epoch 3 step 370: training accuarcy: 0.7965\n",
      "Epoch 3 step 370: training loss: 9946.151880546213\n",
      "Epoch 3 step 371: training accuarcy: 0.784\n",
      "Epoch 3 step 371: training loss: 8952.711467031175\n",
      "Epoch 3 step 372: training accuarcy: 0.804\n",
      "Epoch 3 step 372: training loss: 8697.71414830886\n",
      "Epoch 3 step 373: training accuarcy: 0.8085\n",
      "Epoch 3 step 373: training loss: 9002.914703566963\n",
      "Epoch 3 step 374: training accuarcy: 0.8055\n",
      "Epoch 3 step 374: training loss: 8781.16385937244\n",
      "Epoch 3 step 375: training accuarcy: 0.8065\n",
      "Epoch 3 step 375: training loss: 8617.868605898353\n",
      "Epoch 3 step 376: training accuarcy: 0.8095\n",
      "Epoch 3 step 376: training loss: 9443.556024025042\n",
      "Epoch 3 step 377: training accuarcy: 0.7955\n",
      "Epoch 3 step 377: training loss: 8537.317501320911\n",
      "Epoch 3 step 378: training accuarcy: 0.8115\n",
      "Epoch 3 step 378: training loss: 8306.900168220576\n",
      "Epoch 3 step 379: training accuarcy: 0.8205\n",
      "Epoch 3 step 379: training loss: 9267.079681283156\n",
      "Epoch 3 step 380: training accuarcy: 0.798\n",
      "Epoch 3 step 380: training loss: 8366.682680781838\n",
      "Epoch 3 step 381: training accuarcy: 0.8155\n",
      "Epoch 3 step 381: training loss: 9416.882145419173\n",
      "Epoch 3 step 382: training accuarcy: 0.795\n",
      "Epoch 3 step 382: training loss: 8879.179814396153\n",
      "Epoch 3 step 383: training accuarcy: 0.8045\n",
      "Epoch 3 step 383: training loss: 9438.053355345426\n",
      "Epoch 3 step 384: training accuarcy: 0.794\n",
      "Epoch 3 step 384: training loss: 8770.119018210993\n",
      "Epoch 3 step 385: training accuarcy: 0.8075\n",
      "Epoch 3 step 385: training loss: 8889.684002766453\n",
      "Epoch 3 step 386: training accuarcy: 0.803\n",
      "Epoch 3 step 386: training loss: 9465.442304148086\n",
      "Epoch 3 step 387: training accuarcy: 0.7935\n",
      "Epoch 3 step 387: training loss: 9009.314327492966\n",
      "Epoch 3 step 388: training accuarcy: 0.803\n",
      "Epoch 3 step 388: training loss: 9257.192709370327\n",
      "Epoch 3 step 389: training accuarcy: 0.7975\n",
      "Epoch 3 step 389: training loss: 8629.740364450954\n",
      "Epoch 3 step 390: training accuarcy: 0.8135\n",
      "Epoch 3 step 390: training loss: 8474.729057985125\n",
      "Epoch 3 step 391: training accuarcy: 0.8160000000000001\n",
      "Epoch 3 step 391: training loss: 8546.760016536782\n",
      "Epoch 3 step 392: training accuarcy: 0.8130000000000001\n",
      "Epoch 3 step 392: training loss: 8314.918706611155\n",
      "Epoch 3 step 393: training accuarcy: 0.8180000000000001\n",
      "Epoch 3 step 393: training loss: 8947.332510730143\n",
      "Epoch 3 step 394: training accuarcy: 0.801\n",
      "Epoch 3 step 394: training loss: 8726.972074665127\n",
      "Epoch 3 step 395: training accuarcy: 0.8095\n",
      "Epoch 3 step 395: training loss: 8845.329263294283\n",
      "Epoch 3 step 396: training accuarcy: 0.8045\n",
      "Epoch 3 step 396: training loss: 8771.217916804311\n",
      "Epoch 3 step 397: training accuarcy: 0.8095\n",
      "Epoch 3 step 397: training loss: 9788.13761991232\n",
      "Epoch 3 step 398: training accuarcy: 0.7875\n",
      "Epoch 3 step 398: training loss: 9282.169970259109\n",
      "Epoch 3 step 399: training accuarcy: 0.7975\n",
      "Epoch 3 step 399: training loss: 9043.049354823544\n",
      "Epoch 3 step 400: training accuarcy: 0.8015\n",
      "Epoch 3 step 400: training loss: 9166.055930989512\n",
      "Epoch 3 step 401: training accuarcy: 0.8\n",
      "Epoch 3 step 401: training loss: 9097.30539032416\n",
      "Epoch 3 step 402: training accuarcy: 0.8025\n",
      "Epoch 3 step 402: training loss: 9253.92753104307\n",
      "Epoch 3 step 403: training accuarcy: 0.7985\n",
      "Epoch 3 step 403: training loss: 8986.257871028696\n",
      "Epoch 3 step 404: training accuarcy: 0.804\n",
      "Epoch 3 step 404: training loss: 8522.621207690308\n",
      "Epoch 3 step 405: training accuarcy: 0.812\n",
      "Epoch 3 step 405: training loss: 8424.772520165077\n",
      "Epoch 3 step 406: training accuarcy: 0.8170000000000001\n",
      "Epoch 3 step 406: training loss: 8999.40681084086\n",
      "Epoch 3 step 407: training accuarcy: 0.8045\n",
      "Epoch 3 step 407: training loss: 8563.502720230488\n",
      "Epoch 3 step 408: training accuarcy: 0.8130000000000001\n",
      "Epoch 3 step 408: training loss: 8771.639548235986\n",
      "Epoch 3 step 409: training accuarcy: 0.8075\n",
      "Epoch 3 step 409: training loss: 9441.141021405141\n",
      "Epoch 3 step 410: training accuarcy: 0.795\n",
      "Epoch 3 step 410: training loss: 8942.720945592366\n",
      "Epoch 3 step 411: training accuarcy: 0.8025\n",
      "Epoch 3 step 411: training loss: 8998.917105963412\n",
      "Epoch 3 step 412: training accuarcy: 0.806\n",
      "Epoch 3 step 412: training loss: 9345.521894374908\n",
      "Epoch 3 step 413: training accuarcy: 0.7945\n",
      "Epoch 3 step 413: training loss: 8358.309099002867\n",
      "Epoch 3 step 414: training accuarcy: 0.8185\n",
      "Epoch 3 step 414: training loss: 9524.177664728648\n",
      "Epoch 3 step 415: training accuarcy: 0.7925\n",
      "Epoch 3 step 415: training loss: 8570.221735477216\n",
      "Epoch 3 step 416: training accuarcy: 0.8115\n",
      "Epoch 3 step 416: training loss: 9415.219033599735\n",
      "Epoch 3 step 417: training accuarcy: 0.7925\n",
      "Epoch 3 step 417: training loss: 9004.19837141648\n",
      "Epoch 3 step 418: training accuarcy: 0.8025\n",
      "Epoch 3 step 418: training loss: 8787.23534316626\n",
      "Epoch 3 step 419: training accuarcy: 0.8085\n",
      "Epoch 3 step 419: training loss: 8250.633737550193\n",
      "Epoch 3 step 420: training accuarcy: 0.8185\n",
      "Epoch 3 step 420: training loss: 8854.016435453555\n",
      "Epoch 3 step 421: training accuarcy: 0.8085\n",
      "Epoch 3 step 421: training loss: 9022.963227701683\n",
      "Epoch 3 step 422: training accuarcy: 0.803\n",
      "Epoch 3 step 422: training loss: 8982.066025885098\n",
      "Epoch 3 step 423: training accuarcy: 0.8035\n",
      "Epoch 3 step 423: training loss: 9277.311979022948\n",
      "Epoch 3 step 424: training accuarcy: 0.7955\n",
      "Epoch 3 step 424: training loss: 9059.576641526903\n",
      "Epoch 3 step 425: training accuarcy: 0.803\n",
      "Epoch 3 step 425: training loss: 8543.15700469472\n",
      "Epoch 3 step 426: training accuarcy: 0.8145\n",
      "Epoch 3 step 426: training loss: 8226.257770053762\n",
      "Epoch 3 step 427: training accuarcy: 0.8170000000000001\n",
      "Epoch 3 step 427: training loss: 8362.814866422856\n",
      "Epoch 3 step 428: training accuarcy: 0.8155\n",
      "Epoch 3 step 428: training loss: 9099.658950462825\n",
      "Epoch 3 step 429: training accuarcy: 0.7995\n",
      "Epoch 3 step 429: training loss: 9536.616634345675\n",
      "Epoch 3 step 430: training accuarcy: 0.792\n",
      "Epoch 3 step 430: training loss: 9092.264830718843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 431: training accuarcy: 0.8015\n",
      "Epoch 3 step 431: training loss: 8200.444439336316\n",
      "Epoch 3 step 432: training accuarcy: 0.8205\n",
      "Epoch 3 step 432: training loss: 9690.686864334577\n",
      "Epoch 3 step 433: training accuarcy: 0.787\n",
      "Epoch 3 step 433: training loss: 9507.627282453941\n",
      "Epoch 3 step 434: training accuarcy: 0.792\n",
      "Epoch 3 step 434: training loss: 8057.69673004797\n",
      "Epoch 3 step 435: training accuarcy: 0.8220000000000001\n",
      "Epoch 3 step 435: training loss: 8700.223153193381\n",
      "Epoch 3 step 436: training accuarcy: 0.8085\n",
      "Epoch 3 step 436: training loss: 8674.689872765264\n",
      "Epoch 3 step 437: training accuarcy: 0.81\n",
      "Epoch 3 step 437: training loss: 9715.671093344867\n",
      "Epoch 3 step 438: training accuarcy: 0.787\n",
      "Epoch 3 step 438: training loss: 8989.820286512479\n",
      "Epoch 3 step 439: training accuarcy: 0.804\n",
      "Epoch 3 step 439: training loss: 8728.72080919239\n",
      "Epoch 3 step 440: training accuarcy: 0.81\n",
      "Epoch 3 step 440: training loss: 9510.875505533108\n",
      "Epoch 3 step 441: training accuarcy: 0.791\n",
      "Epoch 3 step 441: training loss: 8551.416451154093\n",
      "Epoch 3 step 442: training accuarcy: 0.8130000000000001\n",
      "Epoch 3 step 442: training loss: 9043.750629817325\n",
      "Epoch 3 step 443: training accuarcy: 0.804\n",
      "Epoch 3 step 443: training loss: 9807.048871685154\n",
      "Epoch 3 step 444: training accuarcy: 0.7865\n",
      "Epoch 3 step 444: training loss: 9162.672156888502\n",
      "Epoch 3 step 445: training accuarcy: 0.801\n",
      "Epoch 3 step 445: training loss: 9077.345047981698\n",
      "Epoch 3 step 446: training accuarcy: 0.802\n",
      "Epoch 3 step 446: training loss: 8948.572107529562\n",
      "Epoch 3 step 447: training accuarcy: 0.804\n",
      "Epoch 3 step 447: training loss: 9143.3047316745\n",
      "Epoch 3 step 448: training accuarcy: 0.7995\n",
      "Epoch 3 step 448: training loss: 8589.139166068973\n",
      "Epoch 3 step 449: training accuarcy: 0.8125\n",
      "Epoch 3 step 449: training loss: 8992.025525345063\n",
      "Epoch 3 step 450: training accuarcy: 0.8035\n",
      "Epoch 3 step 450: training loss: 8764.574239782492\n",
      "Epoch 3 step 451: training accuarcy: 0.81\n",
      "Epoch 3 step 451: training loss: 8864.604354184037\n",
      "Epoch 3 step 452: training accuarcy: 0.8045\n",
      "Epoch 3 step 452: training loss: 8545.661188196298\n",
      "Epoch 3 step 453: training accuarcy: 0.8125\n",
      "Epoch 3 step 453: training loss: 8882.02053075051\n",
      "Epoch 3 step 454: training accuarcy: 0.804\n",
      "Epoch 3 step 454: training loss: 8675.126008871217\n",
      "Epoch 3 step 455: training accuarcy: 0.811\n",
      "Epoch 3 step 455: training loss: 8385.061425167092\n",
      "Epoch 3 step 456: training accuarcy: 0.8180000000000001\n",
      "Epoch 3 step 456: training loss: 8729.505076694792\n",
      "Epoch 3 step 457: training accuarcy: 0.8085\n",
      "Epoch 3 step 457: training loss: 8239.397318852287\n",
      "Epoch 3 step 458: training accuarcy: 0.8225\n",
      "Epoch 3 step 458: training loss: 8289.272278884306\n",
      "Epoch 3 step 459: training accuarcy: 0.8175\n",
      "Epoch 3 step 459: training loss: 7360.363818376862\n",
      "Epoch 3 step 460: training accuarcy: 0.8301886792452831\n",
      "Epoch 3: train loss 8946.140970345781, train accuarcy 0.8042553067207336\n",
      "Epoch 3: valid loss 8509.433435333776, valid accuarcy 0.8104484677314758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 4/8 [03:28<03:29, 52.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 460: training loss: 8237.811526073083\n",
      "Epoch 4 step 461: training accuarcy: 0.8220000000000001\n",
      "Epoch 4 step 461: training loss: 8427.314448506182\n",
      "Epoch 4 step 462: training accuarcy: 0.8160000000000001\n",
      "Epoch 4 step 462: training loss: 8770.195692879603\n",
      "Epoch 4 step 463: training accuarcy: 0.811\n",
      "Epoch 4 step 463: training loss: 8638.207505642426\n",
      "Epoch 4 step 464: training accuarcy: 0.811\n",
      "Epoch 4 step 464: training loss: 8409.880123051233\n",
      "Epoch 4 step 465: training accuarcy: 0.8165\n",
      "Epoch 4 step 465: training loss: 8774.266469684946\n",
      "Epoch 4 step 466: training accuarcy: 0.807\n",
      "Epoch 4 step 466: training loss: 8354.925142639806\n",
      "Epoch 4 step 467: training accuarcy: 0.8185\n",
      "Epoch 4 step 467: training loss: 8245.9050386725\n",
      "Epoch 4 step 468: training accuarcy: 0.8195\n",
      "Epoch 4 step 468: training loss: 9033.2547676819\n",
      "Epoch 4 step 469: training accuarcy: 0.8005\n",
      "Epoch 4 step 469: training loss: 8212.921965537456\n",
      "Epoch 4 step 470: training accuarcy: 0.8180000000000001\n",
      "Epoch 4 step 470: training loss: 8237.688795333239\n",
      "Epoch 4 step 471: training accuarcy: 0.8200000000000001\n",
      "Epoch 4 step 471: training loss: 8932.458912987464\n",
      "Epoch 4 step 472: training accuarcy: 0.805\n",
      "Epoch 4 step 472: training loss: 8010.772447793123\n",
      "Epoch 4 step 473: training accuarcy: 0.8260000000000001\n",
      "Epoch 4 step 473: training loss: 8448.881477126666\n",
      "Epoch 4 step 474: training accuarcy: 0.8175\n",
      "Epoch 4 step 474: training loss: 8936.653497163778\n",
      "Epoch 4 step 475: training accuarcy: 0.804\n",
      "Epoch 4 step 475: training loss: 8202.49381277856\n",
      "Epoch 4 step 476: training accuarcy: 0.8195\n",
      "Epoch 4 step 476: training loss: 8221.151673055734\n",
      "Epoch 4 step 477: training accuarcy: 0.8210000000000001\n",
      "Epoch 4 step 477: training loss: 8631.339153727751\n",
      "Epoch 4 step 478: training accuarcy: 0.8105\n",
      "Epoch 4 step 478: training loss: 8054.393325004378\n",
      "Epoch 4 step 479: training accuarcy: 0.8240000000000001\n",
      "Epoch 4 step 479: training loss: 8420.641079116305\n",
      "Epoch 4 step 480: training accuarcy: 0.8165\n",
      "Epoch 4 step 480: training loss: 9433.483967102766\n",
      "Epoch 4 step 481: training accuarcy: 0.7935\n",
      "Epoch 4 step 481: training loss: 8104.288561592795\n",
      "Epoch 4 step 482: training accuarcy: 0.8190000000000001\n",
      "Epoch 4 step 482: training loss: 9338.694365776955\n",
      "Epoch 4 step 483: training accuarcy: 0.798\n",
      "Epoch 4 step 483: training loss: 7785.635041903131\n",
      "Epoch 4 step 484: training accuarcy: 0.8310000000000001\n",
      "Epoch 4 step 484: training loss: 8722.392076452179\n",
      "Epoch 4 step 485: training accuarcy: 0.808\n",
      "Epoch 4 step 485: training loss: 8291.15011943656\n",
      "Epoch 4 step 486: training accuarcy: 0.8175\n",
      "Epoch 4 step 486: training loss: 7784.573741457845\n",
      "Epoch 4 step 487: training accuarcy: 0.8290000000000001\n",
      "Epoch 4 step 487: training loss: 8151.663596739949\n",
      "Epoch 4 step 488: training accuarcy: 0.8225\n",
      "Epoch 4 step 488: training loss: 8535.90979670486\n",
      "Epoch 4 step 489: training accuarcy: 0.8140000000000001\n",
      "Epoch 4 step 489: training loss: 8931.244756972777\n",
      "Epoch 4 step 490: training accuarcy: 0.8055\n",
      "Epoch 4 step 490: training loss: 8486.915921855167\n",
      "Epoch 4 step 491: training accuarcy: 0.8150000000000001\n",
      "Epoch 4 step 491: training loss: 8427.5498769951\n",
      "Epoch 4 step 492: training accuarcy: 0.8165\n",
      "Epoch 4 step 492: training loss: 8582.006084152441\n",
      "Epoch 4 step 493: training accuarcy: 0.8140000000000001\n",
      "Epoch 4 step 493: training loss: 8599.734119504434\n",
      "Epoch 4 step 494: training accuarcy: 0.8140000000000001\n",
      "Epoch 4 step 494: training loss: 8569.073493743834\n",
      "Epoch 4 step 495: training accuarcy: 0.811\n",
      "Epoch 4 step 495: training loss: 8541.31414867946\n",
      "Epoch 4 step 496: training accuarcy: 0.8125\n",
      "Epoch 4 step 496: training loss: 9062.778821788579\n",
      "Epoch 4 step 497: training accuarcy: 0.8035\n",
      "Epoch 4 step 497: training loss: 8176.05356750938\n",
      "Epoch 4 step 498: training accuarcy: 0.8210000000000001\n",
      "Epoch 4 step 498: training loss: 7872.070536468107\n",
      "Epoch 4 step 499: training accuarcy: 0.8275\n",
      "Epoch 4 step 499: training loss: 8439.818503452532\n",
      "Epoch 4 step 500: training accuarcy: 0.8165\n",
      "Epoch 4 step 500: training loss: 8609.499090823525\n",
      "Epoch 4 step 501: training accuarcy: 0.8140000000000001\n",
      "Epoch 4 step 501: training loss: 8883.278406386258\n",
      "Epoch 4 step 502: training accuarcy: 0.806\n",
      "Epoch 4 step 502: training loss: 8221.899800543768\n",
      "Epoch 4 step 503: training accuarcy: 0.8225\n",
      "Epoch 4 step 503: training loss: 8310.637659299337\n",
      "Epoch 4 step 504: training accuarcy: 0.8190000000000001\n",
      "Epoch 4 step 504: training loss: 8474.677288625959\n",
      "Epoch 4 step 505: training accuarcy: 0.8165\n",
      "Epoch 4 step 505: training loss: 8681.382209886831\n",
      "Epoch 4 step 506: training accuarcy: 0.8105\n",
      "Epoch 4 step 506: training loss: 8451.511815689444\n",
      "Epoch 4 step 507: training accuarcy: 0.8155\n",
      "Epoch 4 step 507: training loss: 8514.984716368423\n",
      "Epoch 4 step 508: training accuarcy: 0.8165\n",
      "Epoch 4 step 508: training loss: 8420.934256696522\n",
      "Epoch 4 step 509: training accuarcy: 0.8160000000000001\n",
      "Epoch 4 step 509: training loss: 8188.232360556916\n",
      "Epoch 4 step 510: training accuarcy: 0.8190000000000001\n",
      "Epoch 4 step 510: training loss: 8981.40039176488\n",
      "Epoch 4 step 511: training accuarcy: 0.803\n",
      "Epoch 4 step 511: training loss: 7826.800613629812\n",
      "Epoch 4 step 512: training accuarcy: 0.8290000000000001\n",
      "Epoch 4 step 512: training loss: 7822.411307498793\n",
      "Epoch 4 step 513: training accuarcy: 0.8265\n",
      "Epoch 4 step 513: training loss: 9027.779515101807\n",
      "Epoch 4 step 514: training accuarcy: 0.8025\n",
      "Epoch 4 step 514: training loss: 8217.517729807358\n",
      "Epoch 4 step 515: training accuarcy: 0.8195\n",
      "Epoch 4 step 515: training loss: 8152.239862072934\n",
      "Epoch 4 step 516: training accuarcy: 0.8210000000000001\n",
      "Epoch 4 step 516: training loss: 8929.012131521064\n",
      "Epoch 4 step 517: training accuarcy: 0.805\n",
      "Epoch 4 step 517: training loss: 8648.440062207681\n",
      "Epoch 4 step 518: training accuarcy: 0.812\n",
      "Epoch 4 step 518: training loss: 8513.846082376203\n",
      "Epoch 4 step 519: training accuarcy: 0.8140000000000001\n",
      "Epoch 4 step 519: training loss: 8999.19816580033\n",
      "Epoch 4 step 520: training accuarcy: 0.802\n",
      "Epoch 4 step 520: training loss: 8780.462040479251\n",
      "Epoch 4 step 521: training accuarcy: 0.808\n",
      "Epoch 4 step 521: training loss: 8234.155823630212\n",
      "Epoch 4 step 522: training accuarcy: 0.8210000000000001\n",
      "Epoch 4 step 522: training loss: 8634.504690257105\n",
      "Epoch 4 step 523: training accuarcy: 0.8095\n",
      "Epoch 4 step 523: training loss: 9278.042985450149\n",
      "Epoch 4 step 524: training accuarcy: 0.7995\n",
      "Epoch 4 step 524: training loss: 8642.864124833919\n",
      "Epoch 4 step 525: training accuarcy: 0.8130000000000001\n",
      "Epoch 4 step 525: training loss: 8728.53978223295\n",
      "Epoch 4 step 526: training accuarcy: 0.8105\n",
      "Epoch 4 step 526: training loss: 8849.746087256068\n",
      "Epoch 4 step 527: training accuarcy: 0.8065\n",
      "Epoch 4 step 527: training loss: 8883.357173575854\n",
      "Epoch 4 step 528: training accuarcy: 0.8045\n",
      "Epoch 4 step 528: training loss: 8231.834043201363\n",
      "Epoch 4 step 529: training accuarcy: 0.8190000000000001\n",
      "Epoch 4 step 529: training loss: 9016.205074096779\n",
      "Epoch 4 step 530: training accuarcy: 0.803\n",
      "Epoch 4 step 530: training loss: 8679.670575587139\n",
      "Epoch 4 step 531: training accuarcy: 0.8125\n",
      "Epoch 4 step 531: training loss: 8695.917516214455\n",
      "Epoch 4 step 532: training accuarcy: 0.812\n",
      "Epoch 4 step 532: training loss: 8770.398699466963\n",
      "Epoch 4 step 533: training accuarcy: 0.8045\n",
      "Epoch 4 step 533: training loss: 9024.277240444364\n",
      "Epoch 4 step 534: training accuarcy: 0.801\n",
      "Epoch 4 step 534: training loss: 7805.825476554313\n",
      "Epoch 4 step 535: training accuarcy: 0.8295\n",
      "Epoch 4 step 535: training loss: 8339.651653712113\n",
      "Epoch 4 step 536: training accuarcy: 0.8160000000000001\n",
      "Epoch 4 step 536: training loss: 8721.72471929839\n",
      "Epoch 4 step 537: training accuarcy: 0.811\n",
      "Epoch 4 step 537: training loss: 7854.627126271239\n",
      "Epoch 4 step 538: training accuarcy: 0.8280000000000001\n",
      "Epoch 4 step 538: training loss: 8796.057088138059\n",
      "Epoch 4 step 539: training accuarcy: 0.8095\n",
      "Epoch 4 step 539: training loss: 10059.677890265104\n",
      "Epoch 4 step 540: training accuarcy: 0.7815\n",
      "Epoch 4 step 540: training loss: 8133.694059530158\n",
      "Epoch 4 step 541: training accuarcy: 0.8220000000000001\n",
      "Epoch 4 step 541: training loss: 8535.382320800867\n",
      "Epoch 4 step 542: training accuarcy: 0.8155\n",
      "Epoch 4 step 542: training loss: 8099.565179505254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 543: training accuarcy: 0.8245\n",
      "Epoch 4 step 543: training loss: 8377.269159285803\n",
      "Epoch 4 step 544: training accuarcy: 0.8175\n",
      "Epoch 4 step 544: training loss: 8359.64597723556\n",
      "Epoch 4 step 545: training accuarcy: 0.8160000000000001\n",
      "Epoch 4 step 545: training loss: 8521.126546952835\n",
      "Epoch 4 step 546: training accuarcy: 0.8130000000000001\n",
      "Epoch 4 step 546: training loss: 7913.738852417316\n",
      "Epoch 4 step 547: training accuarcy: 0.8280000000000001\n",
      "Epoch 4 step 547: training loss: 7635.569469836168\n",
      "Epoch 4 step 548: training accuarcy: 0.834\n",
      "Epoch 4 step 548: training loss: 8401.261196009706\n",
      "Epoch 4 step 549: training accuarcy: 0.8170000000000001\n",
      "Epoch 4 step 549: training loss: 8055.718845247195\n",
      "Epoch 4 step 550: training accuarcy: 0.8245\n",
      "Epoch 4 step 550: training loss: 8647.431939149043\n",
      "Epoch 4 step 551: training accuarcy: 0.8115\n",
      "Epoch 4 step 551: training loss: 7846.983036575802\n",
      "Epoch 4 step 552: training accuarcy: 0.8290000000000001\n",
      "Epoch 4 step 552: training loss: 7380.8308775665355\n",
      "Epoch 4 step 553: training accuarcy: 0.8375\n",
      "Epoch 4 step 553: training loss: 7613.354755888332\n",
      "Epoch 4 step 554: training accuarcy: 0.834\n",
      "Epoch 4 step 554: training loss: 8512.164074294618\n",
      "Epoch 4 step 555: training accuarcy: 0.8135\n",
      "Epoch 4 step 555: training loss: 7912.739222577156\n",
      "Epoch 4 step 556: training accuarcy: 0.8290000000000001\n",
      "Epoch 4 step 556: training loss: 8083.849280569705\n",
      "Epoch 4 step 557: training accuarcy: 0.8235\n",
      "Epoch 4 step 557: training loss: 8455.895002669775\n",
      "Epoch 4 step 558: training accuarcy: 0.8150000000000001\n",
      "Epoch 4 step 558: training loss: 7684.185232278151\n",
      "Epoch 4 step 559: training accuarcy: 0.833\n",
      "Epoch 4 step 559: training loss: 8427.246201413187\n",
      "Epoch 4 step 560: training accuarcy: 0.8160000000000001\n",
      "Epoch 4 step 560: training loss: 8013.942241192774\n",
      "Epoch 4 step 561: training accuarcy: 0.8240000000000001\n",
      "Epoch 4 step 561: training loss: 7891.289403777662\n",
      "Epoch 4 step 562: training accuarcy: 0.8295\n",
      "Epoch 4 step 562: training loss: 8200.434628165467\n",
      "Epoch 4 step 563: training accuarcy: 0.8200000000000001\n",
      "Epoch 4 step 563: training loss: 8030.55349346292\n",
      "Epoch 4 step 564: training accuarcy: 0.8270000000000001\n",
      "Epoch 4 step 564: training loss: 8377.196354245081\n",
      "Epoch 4 step 565: training accuarcy: 0.8185\n",
      "Epoch 4 step 565: training loss: 8145.750049346092\n",
      "Epoch 4 step 566: training accuarcy: 0.8215\n",
      "Epoch 4 step 566: training loss: 7853.807802053768\n",
      "Epoch 4 step 567: training accuarcy: 0.8275\n",
      "Epoch 4 step 567: training loss: 8150.8805971955335\n",
      "Epoch 4 step 568: training accuarcy: 0.8200000000000001\n",
      "Epoch 4 step 568: training loss: 8451.959653169728\n",
      "Epoch 4 step 569: training accuarcy: 0.8160000000000001\n",
      "Epoch 4 step 569: training loss: 7906.614359804765\n",
      "Epoch 4 step 570: training accuarcy: 0.8250000000000001\n",
      "Epoch 4 step 570: training loss: 8054.0111919743085\n",
      "Epoch 4 step 571: training accuarcy: 0.8235\n",
      "Epoch 4 step 571: training loss: 8029.014324853701\n",
      "Epoch 4 step 572: training accuarcy: 0.8250000000000001\n",
      "Epoch 4 step 572: training loss: 7567.543911383215\n",
      "Epoch 4 step 573: training accuarcy: 0.8355\n",
      "Epoch 4 step 573: training loss: 8020.206741857202\n",
      "Epoch 4 step 574: training accuarcy: 0.8230000000000001\n",
      "Epoch 4 step 574: training loss: 7753.467235563355\n",
      "Epoch 4 step 575: training accuarcy: 0.8244234800838575\n",
      "Epoch 4: train loss 8399.870038680156, train accuarcy 0.8166220784187317\n",
      "Epoch 4: valid loss 7804.664220746859, valid accuarcy 0.8264053463935852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 5/8 [04:22<02:38, 52.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 575: training loss: 8048.3859386953845\n",
      "Epoch 5 step 576: training accuarcy: 0.8255\n",
      "Epoch 5 step 576: training loss: 7981.55209021275\n",
      "Epoch 5 step 577: training accuarcy: 0.8230000000000001\n",
      "Epoch 5 step 577: training loss: 7369.0130576440015\n",
      "Epoch 5 step 578: training accuarcy: 0.8405\n",
      "Epoch 5 step 578: training loss: 7527.683939901322\n",
      "Epoch 5 step 579: training accuarcy: 0.8325\n",
      "Epoch 5 step 579: training loss: 7617.192998223291\n",
      "Epoch 5 step 580: training accuarcy: 0.8335\n",
      "Epoch 5 step 580: training loss: 8099.645400584147\n",
      "Epoch 5 step 581: training accuarcy: 0.8245\n",
      "Epoch 5 step 581: training loss: 8005.792638743365\n",
      "Epoch 5 step 582: training accuarcy: 0.8245\n",
      "Epoch 5 step 582: training loss: 7843.775268971102\n",
      "Epoch 5 step 583: training accuarcy: 0.8295\n",
      "Epoch 5 step 583: training loss: 7737.36214922021\n",
      "Epoch 5 step 584: training accuarcy: 0.8310000000000001\n",
      "Epoch 5 step 584: training loss: 7528.173772072148\n",
      "Epoch 5 step 585: training accuarcy: 0.8365\n",
      "Epoch 5 step 585: training loss: 7750.524308958308\n",
      "Epoch 5 step 586: training accuarcy: 0.8295\n",
      "Epoch 5 step 586: training loss: 8819.480553607467\n",
      "Epoch 5 step 587: training accuarcy: 0.8065\n",
      "Epoch 5 step 587: training loss: 7338.911004578026\n",
      "Epoch 5 step 588: training accuarcy: 0.841\n",
      "Epoch 5 step 588: training loss: 7731.5632374557745\n",
      "Epoch 5 step 589: training accuarcy: 0.8300000000000001\n",
      "Epoch 5 step 589: training loss: 7847.79112494359\n",
      "Epoch 5 step 590: training accuarcy: 0.8295\n",
      "Epoch 5 step 590: training loss: 7661.243212523274\n",
      "Epoch 5 step 591: training accuarcy: 0.8335\n",
      "Epoch 5 step 591: training loss: 8399.642933734534\n",
      "Epoch 5 step 592: training accuarcy: 0.8145\n",
      "Epoch 5 step 592: training loss: 6759.74395626747\n",
      "Epoch 5 step 593: training accuarcy: 0.8535\n",
      "Epoch 5 step 593: training loss: 7011.108360319275\n",
      "Epoch 5 step 594: training accuarcy: 0.8475\n",
      "Epoch 5 step 594: training loss: 7464.744911619467\n",
      "Epoch 5 step 595: training accuarcy: 0.8375\n",
      "Epoch 5 step 595: training loss: 7758.222762114045\n",
      "Epoch 5 step 596: training accuarcy: 0.8305\n",
      "Epoch 5 step 596: training loss: 8101.024721089803\n",
      "Epoch 5 step 597: training accuarcy: 0.8235\n",
      "Epoch 5 step 597: training loss: 7879.681229658422\n",
      "Epoch 5 step 598: training accuarcy: 0.8290000000000001\n",
      "Epoch 5 step 598: training loss: 6991.449811673266\n",
      "Epoch 5 step 599: training accuarcy: 0.846\n",
      "Epoch 5 step 599: training loss: 8117.496007879037\n",
      "Epoch 5 step 600: training accuarcy: 0.8205\n",
      "Epoch 5 step 600: training loss: 7781.814360033339\n",
      "Epoch 5 step 601: training accuarcy: 0.8315\n",
      "Epoch 5 step 601: training loss: 7652.193774513244\n",
      "Epoch 5 step 602: training accuarcy: 0.8325\n",
      "Epoch 5 step 602: training loss: 8190.948729631417\n",
      "Epoch 5 step 603: training accuarcy: 0.8220000000000001\n",
      "Epoch 5 step 603: training loss: 7955.261514207901\n",
      "Epoch 5 step 604: training accuarcy: 0.8260000000000001\n",
      "Epoch 5 step 604: training loss: 7587.275459505832\n",
      "Epoch 5 step 605: training accuarcy: 0.834\n",
      "Epoch 5 step 605: training loss: 8467.878264561235\n",
      "Epoch 5 step 606: training accuarcy: 0.8150000000000001\n",
      "Epoch 5 step 606: training loss: 7507.425465158458\n",
      "Epoch 5 step 607: training accuarcy: 0.836\n",
      "Epoch 5 step 607: training loss: 7252.592116843054\n",
      "Epoch 5 step 608: training accuarcy: 0.8425\n",
      "Epoch 5 step 608: training loss: 7845.50017592031\n",
      "Epoch 5 step 609: training accuarcy: 0.8285\n",
      "Epoch 5 step 609: training loss: 7840.568276549995\n",
      "Epoch 5 step 610: training accuarcy: 0.8285\n",
      "Epoch 5 step 610: training loss: 8196.204084483861\n",
      "Epoch 5 step 611: training accuarcy: 0.8210000000000001\n",
      "Epoch 5 step 611: training loss: 7824.417023149879\n",
      "Epoch 5 step 612: training accuarcy: 0.8300000000000001\n",
      "Epoch 5 step 612: training loss: 7939.173803273921\n",
      "Epoch 5 step 613: training accuarcy: 0.8255\n",
      "Epoch 5 step 613: training loss: 7342.200521340538\n",
      "Epoch 5 step 614: training accuarcy: 0.8385\n",
      "Epoch 5 step 614: training loss: 7208.364000836633\n",
      "Epoch 5 step 615: training accuarcy: 0.8415\n",
      "Epoch 5 step 615: training loss: 8374.867695388733\n",
      "Epoch 5 step 616: training accuarcy: 0.8180000000000001\n",
      "Epoch 5 step 616: training loss: 7223.07737322744\n",
      "Epoch 5 step 617: training accuarcy: 0.842\n",
      "Epoch 5 step 617: training loss: 7253.021077559761\n",
      "Epoch 5 step 618: training accuarcy: 0.84\n",
      "Epoch 5 step 618: training loss: 7294.546189618718\n",
      "Epoch 5 step 619: training accuarcy: 0.8415\n",
      "Epoch 5 step 619: training loss: 7325.666909161577\n",
      "Epoch 5 step 620: training accuarcy: 0.8385\n",
      "Epoch 5 step 620: training loss: 7802.691605662724\n",
      "Epoch 5 step 621: training accuarcy: 0.8290000000000001\n",
      "Epoch 5 step 621: training loss: 7396.049261560252\n",
      "Epoch 5 step 622: training accuarcy: 0.837\n",
      "Epoch 5 step 622: training loss: 6716.303037959841\n",
      "Epoch 5 step 623: training accuarcy: 0.855\n",
      "Epoch 5 step 623: training loss: 7965.456236694171\n",
      "Epoch 5 step 624: training accuarcy: 0.8280000000000001\n",
      "Epoch 5 step 624: training loss: 7525.914078963189\n",
      "Epoch 5 step 625: training accuarcy: 0.835\n",
      "Epoch 5 step 625: training loss: 7890.2421633014055\n",
      "Epoch 5 step 626: training accuarcy: 0.8280000000000001\n",
      "Epoch 5 step 626: training loss: 7367.708642685814\n",
      "Epoch 5 step 627: training accuarcy: 0.8375\n",
      "Epoch 5 step 627: training loss: 8101.0539777110425\n",
      "Epoch 5 step 628: training accuarcy: 0.8240000000000001\n",
      "Epoch 5 step 628: training loss: 7060.248609773769\n",
      "Epoch 5 step 629: training accuarcy: 0.8455\n",
      "Epoch 5 step 629: training loss: 7553.580814359669\n",
      "Epoch 5 step 630: training accuarcy: 0.8335\n",
      "Epoch 5 step 630: training loss: 7945.936224945086\n",
      "Epoch 5 step 631: training accuarcy: 0.8280000000000001\n",
      "Epoch 5 step 631: training loss: 7598.071932102649\n",
      "Epoch 5 step 632: training accuarcy: 0.8345\n",
      "Epoch 5 step 632: training loss: 7448.766788357348\n",
      "Epoch 5 step 633: training accuarcy: 0.838\n",
      "Epoch 5 step 633: training loss: 8422.259964671885\n",
      "Epoch 5 step 634: training accuarcy: 0.8170000000000001\n",
      "Epoch 5 step 634: training loss: 7767.201424417597\n",
      "Epoch 5 step 635: training accuarcy: 0.8290000000000001\n",
      "Epoch 5 step 635: training loss: 7223.752067145111\n",
      "Epoch 5 step 636: training accuarcy: 0.842\n",
      "Epoch 5 step 636: training loss: 6935.6414691445625\n",
      "Epoch 5 step 637: training accuarcy: 0.845\n",
      "Epoch 5 step 637: training loss: 7572.145073250277\n",
      "Epoch 5 step 638: training accuarcy: 0.834\n",
      "Epoch 5 step 638: training loss: 7047.13710661306\n",
      "Epoch 5 step 639: training accuarcy: 0.844\n",
      "Epoch 5 step 639: training loss: 7337.752477095046\n",
      "Epoch 5 step 640: training accuarcy: 0.84\n",
      "Epoch 5 step 640: training loss: 7973.188654819667\n",
      "Epoch 5 step 641: training accuarcy: 0.8230000000000001\n",
      "Epoch 5 step 641: training loss: 7301.2254176251745\n",
      "Epoch 5 step 642: training accuarcy: 0.84\n",
      "Epoch 5 step 642: training loss: 7315.873955416871\n",
      "Epoch 5 step 643: training accuarcy: 0.84\n",
      "Epoch 5 step 643: training loss: 7324.196821794451\n",
      "Epoch 5 step 644: training accuarcy: 0.8385\n",
      "Epoch 5 step 644: training loss: 7246.151612448004\n",
      "Epoch 5 step 645: training accuarcy: 0.842\n",
      "Epoch 5 step 645: training loss: 6770.810039916018\n",
      "Epoch 5 step 646: training accuarcy: 0.8515\n",
      "Epoch 5 step 646: training loss: 6728.504866334133\n",
      "Epoch 5 step 647: training accuarcy: 0.852\n",
      "Epoch 5 step 647: training loss: 7941.2558367942465\n",
      "Epoch 5 step 648: training accuarcy: 0.8280000000000001\n",
      "Epoch 5 step 648: training loss: 7530.3302886754755\n",
      "Epoch 5 step 649: training accuarcy: 0.835\n",
      "Epoch 5 step 649: training loss: 7633.105730888578\n",
      "Epoch 5 step 650: training accuarcy: 0.8355\n",
      "Epoch 5 step 650: training loss: 7438.440762213982\n",
      "Epoch 5 step 651: training accuarcy: 0.8365\n",
      "Epoch 5 step 651: training loss: 6910.653023338876\n",
      "Epoch 5 step 652: training accuarcy: 0.8495\n",
      "Epoch 5 step 652: training loss: 7294.177326935203\n",
      "Epoch 5 step 653: training accuarcy: 0.8425\n",
      "Epoch 5 step 653: training loss: 6955.55000943761\n",
      "Epoch 5 step 654: training accuarcy: 0.846\n",
      "Epoch 5 step 654: training loss: 7914.232103520958\n",
      "Epoch 5 step 655: training accuarcy: 0.8250000000000001\n",
      "Epoch 5 step 655: training loss: 6780.176578901932\n",
      "Epoch 5 step 656: training accuarcy: 0.8515\n",
      "Epoch 5 step 656: training loss: 6809.883230048787\n",
      "Epoch 5 step 657: training accuarcy: 0.8525\n",
      "Epoch 5 step 657: training loss: 6876.991521727713\n",
      "Epoch 5 step 658: training accuarcy: 0.848\n",
      "Epoch 5 step 658: training loss: 7549.927133589015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 659: training accuarcy: 0.8365\n",
      "Epoch 5 step 659: training loss: 7091.236351818684\n",
      "Epoch 5 step 660: training accuarcy: 0.8445\n",
      "Epoch 5 step 660: training loss: 6968.762373989677\n",
      "Epoch 5 step 661: training accuarcy: 0.8485\n",
      "Epoch 5 step 661: training loss: 7616.602389949764\n",
      "Epoch 5 step 662: training accuarcy: 0.8335\n",
      "Epoch 5 step 662: training loss: 7338.277634696622\n",
      "Epoch 5 step 663: training accuarcy: 0.841\n",
      "Epoch 5 step 663: training loss: 7622.008108916146\n",
      "Epoch 5 step 664: training accuarcy: 0.8320000000000001\n",
      "Epoch 5 step 664: training loss: 7514.980817169989\n",
      "Epoch 5 step 665: training accuarcy: 0.833\n",
      "Epoch 5 step 665: training loss: 7016.220763136868\n",
      "Epoch 5 step 666: training accuarcy: 0.8475\n",
      "Epoch 5 step 666: training loss: 7915.631726006357\n",
      "Epoch 5 step 667: training accuarcy: 0.8290000000000001\n",
      "Epoch 5 step 667: training loss: 7595.630386801875\n",
      "Epoch 5 step 668: training accuarcy: 0.8335\n",
      "Epoch 5 step 668: training loss: 7507.83812208345\n",
      "Epoch 5 step 669: training accuarcy: 0.8345\n",
      "Epoch 5 step 669: training loss: 6568.624085987431\n",
      "Epoch 5 step 670: training accuarcy: 0.856\n",
      "Epoch 5 step 670: training loss: 7220.732856108801\n",
      "Epoch 5 step 671: training accuarcy: 0.8435\n",
      "Epoch 5 step 671: training loss: 7644.887906084137\n",
      "Epoch 5 step 672: training accuarcy: 0.8325\n",
      "Epoch 5 step 672: training loss: 6858.475386006171\n",
      "Epoch 5 step 673: training accuarcy: 0.8495\n",
      "Epoch 5 step 673: training loss: 7454.064343536658\n",
      "Epoch 5 step 674: training accuarcy: 0.839\n",
      "Epoch 5 step 674: training loss: 6626.799065084423\n",
      "Epoch 5 step 675: training accuarcy: 0.8565\n",
      "Epoch 5 step 675: training loss: 6744.172101863119\n",
      "Epoch 5 step 676: training accuarcy: 0.854\n",
      "Epoch 5 step 676: training loss: 6744.123336775264\n",
      "Epoch 5 step 677: training accuarcy: 0.851\n",
      "Epoch 5 step 677: training loss: 6931.287520236722\n",
      "Epoch 5 step 678: training accuarcy: 0.849\n",
      "Epoch 5 step 678: training loss: 6222.908276629488\n",
      "Epoch 5 step 679: training accuarcy: 0.8655\n",
      "Epoch 5 step 679: training loss: 6645.790104981425\n",
      "Epoch 5 step 680: training accuarcy: 0.8565\n",
      "Epoch 5 step 680: training loss: 6929.9586517311545\n",
      "Epoch 5 step 681: training accuarcy: 0.849\n",
      "Epoch 5 step 681: training loss: 6745.5127739009295\n",
      "Epoch 5 step 682: training accuarcy: 0.854\n",
      "Epoch 5 step 682: training loss: 6250.883671168535\n",
      "Epoch 5 step 683: training accuarcy: 0.8655\n",
      "Epoch 5 step 683: training loss: 7063.102929579573\n",
      "Epoch 5 step 684: training accuarcy: 0.845\n",
      "Epoch 5 step 684: training loss: 7106.049028704711\n",
      "Epoch 5 step 685: training accuarcy: 0.846\n",
      "Epoch 5 step 685: training loss: 7961.432241904769\n",
      "Epoch 5 step 686: training accuarcy: 0.8270000000000001\n",
      "Epoch 5 step 686: training loss: 7227.375029361055\n",
      "Epoch 5 step 687: training accuarcy: 0.842\n",
      "Epoch 5 step 687: training loss: 7390.1597263243375\n",
      "Epoch 5 step 688: training accuarcy: 0.838\n",
      "Epoch 5 step 688: training loss: 6862.317130754933\n",
      "Epoch 5 step 689: training accuarcy: 0.847\n",
      "Epoch 5 step 689: training loss: 6669.64624327184\n",
      "Epoch 5 step 690: training accuarcy: 0.8485324947589099\n",
      "Epoch 5: train loss 7447.672010096189, train accuarcy 0.8371765613555908\n",
      "Epoch 5: valid loss 6897.375183234842, valid accuarcy 0.8464060425758362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 6/8 [05:14<01:45, 52.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 690: training loss: 7316.10506971373\n",
      "Epoch 6 step 691: training accuarcy: 0.839\n",
      "Epoch 6 step 691: training loss: 7704.352549892077\n",
      "Epoch 6 step 692: training accuarcy: 0.8295\n",
      "Epoch 6 step 692: training loss: 6760.760766218282\n",
      "Epoch 6 step 693: training accuarcy: 0.8535\n",
      "Epoch 6 step 693: training loss: 6345.649047497298\n",
      "Epoch 6 step 694: training accuarcy: 0.864\n",
      "Epoch 6 step 694: training loss: 7221.585073332963\n",
      "Epoch 6 step 695: training accuarcy: 0.8425\n",
      "Epoch 6 step 695: training loss: 6925.6153786296745\n",
      "Epoch 6 step 696: training accuarcy: 0.85\n",
      "Epoch 6 step 696: training loss: 7221.915969159363\n",
      "Epoch 6 step 697: training accuarcy: 0.8415\n",
      "Epoch 6 step 697: training loss: 6745.227587597992\n",
      "Epoch 6 step 698: training accuarcy: 0.8525\n",
      "Epoch 6 step 698: training loss: 6680.096702205277\n",
      "Epoch 6 step 699: training accuarcy: 0.8525\n",
      "Epoch 6 step 699: training loss: 6993.348739095867\n",
      "Epoch 6 step 700: training accuarcy: 0.8485\n",
      "Epoch 6 step 700: training loss: 6438.596506836595\n",
      "Epoch 6 step 701: training accuarcy: 0.859\n",
      "Epoch 6 step 701: training loss: 6844.636211355196\n",
      "Epoch 6 step 702: training accuarcy: 0.851\n",
      "Epoch 6 step 702: training loss: 6507.333241969661\n",
      "Epoch 6 step 703: training accuarcy: 0.8595\n",
      "Epoch 6 step 703: training loss: 6147.090564368541\n",
      "Epoch 6 step 704: training accuarcy: 0.863\n",
      "Epoch 6 step 704: training loss: 6372.953702060711\n",
      "Epoch 6 step 705: training accuarcy: 0.8625\n",
      "Epoch 6 step 705: training loss: 6387.223912943697\n",
      "Epoch 6 step 706: training accuarcy: 0.862\n",
      "Epoch 6 step 706: training loss: 6257.024325395868\n",
      "Epoch 6 step 707: training accuarcy: 0.8625\n",
      "Epoch 6 step 707: training loss: 6816.945299577552\n",
      "Epoch 6 step 708: training accuarcy: 0.8505\n",
      "Epoch 6 step 708: training loss: 6784.341182275143\n",
      "Epoch 6 step 709: training accuarcy: 0.852\n",
      "Epoch 6 step 709: training loss: 6375.472132291398\n",
      "Epoch 6 step 710: training accuarcy: 0.86\n",
      "Epoch 6 step 710: training loss: 6577.3205639710695\n",
      "Epoch 6 step 711: training accuarcy: 0.856\n",
      "Epoch 6 step 711: training loss: 5887.837937757352\n",
      "Epoch 6 step 712: training accuarcy: 0.871\n",
      "Epoch 6 step 712: training loss: 6682.2607645984335\n",
      "Epoch 6 step 713: training accuarcy: 0.854\n",
      "Epoch 6 step 713: training loss: 6308.923797939394\n",
      "Epoch 6 step 714: training accuarcy: 0.862\n",
      "Epoch 6 step 714: training loss: 6464.345201480485\n",
      "Epoch 6 step 715: training accuarcy: 0.8605\n",
      "Epoch 6 step 715: training loss: 7110.699854675792\n",
      "Epoch 6 step 716: training accuarcy: 0.8455\n",
      "Epoch 6 step 716: training loss: 6972.544770391002\n",
      "Epoch 6 step 717: training accuarcy: 0.848\n",
      "Epoch 6 step 717: training loss: 6074.833841775348\n",
      "Epoch 6 step 718: training accuarcy: 0.8675\n",
      "Epoch 6 step 718: training loss: 6556.447452737438\n",
      "Epoch 6 step 719: training accuarcy: 0.857\n",
      "Epoch 6 step 719: training loss: 6260.881975629709\n",
      "Epoch 6 step 720: training accuarcy: 0.863\n",
      "Epoch 6 step 720: training loss: 6529.845664012795\n",
      "Epoch 6 step 721: training accuarcy: 0.859\n",
      "Epoch 6 step 721: training loss: 6628.521079338104\n",
      "Epoch 6 step 722: training accuarcy: 0.853\n",
      "Epoch 6 step 722: training loss: 6644.247783388695\n",
      "Epoch 6 step 723: training accuarcy: 0.853\n",
      "Epoch 6 step 723: training loss: 6952.0392567108465\n",
      "Epoch 6 step 724: training accuarcy: 0.848\n",
      "Epoch 6 step 724: training loss: 6788.877840585062\n",
      "Epoch 6 step 725: training accuarcy: 0.854\n",
      "Epoch 6 step 725: training loss: 7021.525832413863\n",
      "Epoch 6 step 726: training accuarcy: 0.8465\n",
      "Epoch 6 step 726: training loss: 6649.429567094961\n",
      "Epoch 6 step 727: training accuarcy: 0.8555\n",
      "Epoch 6 step 727: training loss: 6462.569497198626\n",
      "Epoch 6 step 728: training accuarcy: 0.858\n",
      "Epoch 6 step 728: training loss: 6711.6331503887795\n",
      "Epoch 6 step 729: training accuarcy: 0.8525\n",
      "Epoch 6 step 729: training loss: 6760.46591207787\n",
      "Epoch 6 step 730: training accuarcy: 0.8525\n",
      "Epoch 6 step 730: training loss: 6424.030029127488\n",
      "Epoch 6 step 731: training accuarcy: 0.8575\n",
      "Epoch 6 step 731: training loss: 5357.5696311243355\n",
      "Epoch 6 step 732: training accuarcy: 0.8825000000000001\n",
      "Epoch 6 step 732: training loss: 6420.8667779112875\n",
      "Epoch 6 step 733: training accuarcy: 0.86\n",
      "Epoch 6 step 733: training loss: 6791.2974146239885\n",
      "Epoch 6 step 734: training accuarcy: 0.848\n",
      "Epoch 6 step 734: training loss: 6584.168203454934\n",
      "Epoch 6 step 735: training accuarcy: 0.857\n",
      "Epoch 6 step 735: training loss: 6269.3223676013395\n",
      "Epoch 6 step 736: training accuarcy: 0.8625\n",
      "Epoch 6 step 736: training loss: 6258.5993091327255\n",
      "Epoch 6 step 737: training accuarcy: 0.8635\n",
      "Epoch 6 step 737: training loss: 6103.845712675212\n",
      "Epoch 6 step 738: training accuarcy: 0.866\n",
      "Epoch 6 step 738: training loss: 6973.285357724719\n",
      "Epoch 6 step 739: training accuarcy: 0.8465\n",
      "Epoch 6 step 739: training loss: 6901.088286245298\n",
      "Epoch 6 step 740: training accuarcy: 0.8495\n",
      "Epoch 6 step 740: training loss: 6811.954654204933\n",
      "Epoch 6 step 741: training accuarcy: 0.853\n",
      "Epoch 6 step 741: training loss: 6261.861393842103\n",
      "Epoch 6 step 742: training accuarcy: 0.8625\n",
      "Epoch 6 step 742: training loss: 6104.012418668402\n",
      "Epoch 6 step 743: training accuarcy: 0.868\n",
      "Epoch 6 step 743: training loss: 6424.087252448567\n",
      "Epoch 6 step 744: training accuarcy: 0.86\n",
      "Epoch 6 step 744: training loss: 6487.715468270873\n",
      "Epoch 6 step 745: training accuarcy: 0.8575\n",
      "Epoch 6 step 745: training loss: 6794.154453720119\n",
      "Epoch 6 step 746: training accuarcy: 0.85\n",
      "Epoch 6 step 746: training loss: 6504.183641400641\n",
      "Epoch 6 step 747: training accuarcy: 0.86\n",
      "Epoch 6 step 747: training loss: 6609.246427385714\n",
      "Epoch 6 step 748: training accuarcy: 0.8545\n",
      "Epoch 6 step 748: training loss: 6488.27338203258\n",
      "Epoch 6 step 749: training accuarcy: 0.858\n",
      "Epoch 6 step 749: training loss: 6377.078121149429\n",
      "Epoch 6 step 750: training accuarcy: 0.8605\n",
      "Epoch 6 step 750: training loss: 6859.317821067063\n",
      "Epoch 6 step 751: training accuarcy: 0.8485\n",
      "Epoch 6 step 751: training loss: 6348.764619945343\n",
      "Epoch 6 step 752: training accuarcy: 0.8615\n",
      "Epoch 6 step 752: training loss: 6355.167523699554\n",
      "Epoch 6 step 753: training accuarcy: 0.8575\n",
      "Epoch 6 step 753: training loss: 6722.661618804386\n",
      "Epoch 6 step 754: training accuarcy: 0.854\n",
      "Epoch 6 step 754: training loss: 6909.714095005874\n",
      "Epoch 6 step 755: training accuarcy: 0.8475\n",
      "Epoch 6 step 755: training loss: 6369.366507528354\n",
      "Epoch 6 step 756: training accuarcy: 0.8615\n",
      "Epoch 6 step 756: training loss: 7368.314381993862\n",
      "Epoch 6 step 757: training accuarcy: 0.8385\n",
      "Epoch 6 step 757: training loss: 6624.266148697724\n",
      "Epoch 6 step 758: training accuarcy: 0.853\n",
      "Epoch 6 step 758: training loss: 6576.352266336131\n",
      "Epoch 6 step 759: training accuarcy: 0.8565\n",
      "Epoch 6 step 759: training loss: 7530.977095100885\n",
      "Epoch 6 step 760: training accuarcy: 0.835\n",
      "Epoch 6 step 760: training loss: 7156.91121227323\n",
      "Epoch 6 step 761: training accuarcy: 0.844\n",
      "Epoch 6 step 761: training loss: 7249.899730482779\n",
      "Epoch 6 step 762: training accuarcy: 0.8415\n",
      "Epoch 6 step 762: training loss: 6086.675120810449\n",
      "Epoch 6 step 763: training accuarcy: 0.865\n",
      "Epoch 6 step 763: training loss: 6307.012757238364\n",
      "Epoch 6 step 764: training accuarcy: 0.8625\n",
      "Epoch 6 step 764: training loss: 6345.884541034018\n",
      "Epoch 6 step 765: training accuarcy: 0.859\n",
      "Epoch 6 step 765: training loss: 6281.0802667180615\n",
      "Epoch 6 step 766: training accuarcy: 0.8615\n",
      "Epoch 6 step 766: training loss: 5676.041049858026\n",
      "Epoch 6 step 767: training accuarcy: 0.8765000000000001\n",
      "Epoch 6 step 767: training loss: 6660.384726033885\n",
      "Epoch 6 step 768: training accuarcy: 0.853\n",
      "Epoch 6 step 768: training loss: 6519.168317583033\n",
      "Epoch 6 step 769: training accuarcy: 0.8565\n",
      "Epoch 6 step 769: training loss: 6130.693758795191\n",
      "Epoch 6 step 770: training accuarcy: 0.8645\n",
      "Epoch 6 step 770: training loss: 6299.482234674287\n",
      "Epoch 6 step 771: training accuarcy: 0.8625\n",
      "Epoch 6 step 771: training loss: 6048.3369501879\n",
      "Epoch 6 step 772: training accuarcy: 0.867\n",
      "Epoch 6 step 772: training loss: 6642.608483176823\n",
      "Epoch 6 step 773: training accuarcy: 0.856\n",
      "Epoch 6 step 773: training loss: 6189.659670043554\n",
      "Epoch 6 step 774: training accuarcy: 0.863\n",
      "Epoch 6 step 774: training loss: 6694.53744905345\n",
      "Epoch 6 step 775: training accuarcy: 0.8555\n",
      "Epoch 6 step 775: training loss: 6575.680942606431\n",
      "Epoch 6 step 776: training accuarcy: 0.8575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 776: training loss: 6469.95161519717\n",
      "Epoch 6 step 777: training accuarcy: 0.854\n",
      "Epoch 6 step 777: training loss: 6351.394555798253\n",
      "Epoch 6 step 778: training accuarcy: 0.8605\n",
      "Epoch 6 step 778: training loss: 6599.207613894389\n",
      "Epoch 6 step 779: training accuarcy: 0.8585\n",
      "Epoch 6 step 779: training loss: 7156.851857184147\n",
      "Epoch 6 step 780: training accuarcy: 0.8435\n",
      "Epoch 6 step 780: training loss: 6219.492236798844\n",
      "Epoch 6 step 781: training accuarcy: 0.8605\n",
      "Epoch 6 step 781: training loss: 6077.428779418897\n",
      "Epoch 6 step 782: training accuarcy: 0.868\n",
      "Epoch 6 step 782: training loss: 6095.779268769322\n",
      "Epoch 6 step 783: training accuarcy: 0.869\n",
      "Epoch 6 step 783: training loss: 6352.14531418652\n",
      "Epoch 6 step 784: training accuarcy: 0.8615\n",
      "Epoch 6 step 784: training loss: 6533.739638352755\n",
      "Epoch 6 step 785: training accuarcy: 0.857\n",
      "Epoch 6 step 785: training loss: 6309.5478404581645\n",
      "Epoch 6 step 786: training accuarcy: 0.861\n",
      "Epoch 6 step 786: training loss: 6029.163394232861\n",
      "Epoch 6 step 787: training accuarcy: 0.8685\n",
      "Epoch 6 step 787: training loss: 6136.595468232934\n",
      "Epoch 6 step 788: training accuarcy: 0.862\n",
      "Epoch 6 step 788: training loss: 6671.146664283091\n",
      "Epoch 6 step 789: training accuarcy: 0.855\n",
      "Epoch 6 step 789: training loss: 6199.012854770736\n",
      "Epoch 6 step 790: training accuarcy: 0.864\n",
      "Epoch 6 step 790: training loss: 6005.050346635892\n",
      "Epoch 6 step 791: training accuarcy: 0.8675\n",
      "Epoch 6 step 791: training loss: 6297.50749411901\n",
      "Epoch 6 step 792: training accuarcy: 0.863\n",
      "Epoch 6 step 792: training loss: 6491.228038956134\n",
      "Epoch 6 step 793: training accuarcy: 0.859\n",
      "Epoch 6 step 793: training loss: 6078.691914213052\n",
      "Epoch 6 step 794: training accuarcy: 0.8695\n",
      "Epoch 6 step 794: training loss: 6476.329994804767\n",
      "Epoch 6 step 795: training accuarcy: 0.8565\n",
      "Epoch 6 step 795: training loss: 6349.397874747085\n",
      "Epoch 6 step 796: training accuarcy: 0.8625\n",
      "Epoch 6 step 796: training loss: 6078.549700061183\n",
      "Epoch 6 step 797: training accuarcy: 0.8655\n",
      "Epoch 6 step 797: training loss: 5819.515978905249\n",
      "Epoch 6 step 798: training accuarcy: 0.8745\n",
      "Epoch 6 step 798: training loss: 6117.686938119863\n",
      "Epoch 6 step 799: training accuarcy: 0.8675\n",
      "Epoch 6 step 799: training loss: 5831.435151736851\n",
      "Epoch 6 step 800: training accuarcy: 0.8715\n",
      "Epoch 6 step 800: training loss: 6911.899500195586\n",
      "Epoch 6 step 801: training accuarcy: 0.849\n",
      "Epoch 6 step 801: training loss: 5966.300955235012\n",
      "Epoch 6 step 802: training accuarcy: 0.8685\n",
      "Epoch 6 step 802: training loss: 6053.125960042021\n",
      "Epoch 6 step 803: training accuarcy: 0.8695\n",
      "Epoch 6 step 803: training loss: 6307.553616973939\n",
      "Epoch 6 step 804: training accuarcy: 0.8625\n",
      "Epoch 6 step 804: training loss: 5882.977326824685\n",
      "Epoch 6 step 805: training accuarcy: 0.8637316561844864\n",
      "Epoch 6: train loss 6506.398549488906, train accuarcy 0.8579146862030029\n",
      "Epoch 6: valid loss 6099.605433874241, valid accuarcy 0.8643666505813599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 7/8 [06:05<00:52, 52.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 805: training loss: 5837.822860693517\n",
      "Epoch 7 step 806: training accuarcy: 0.8725\n",
      "Epoch 7 step 806: training loss: 5872.779330019293\n",
      "Epoch 7 step 807: training accuarcy: 0.872\n",
      "Epoch 7 step 807: training loss: 6155.696513710112\n",
      "Epoch 7 step 808: training accuarcy: 0.866\n",
      "Epoch 7 step 808: training loss: 6186.359480544963\n",
      "Epoch 7 step 809: training accuarcy: 0.865\n",
      "Epoch 7 step 809: training loss: 6566.19268820615\n",
      "Epoch 7 step 810: training accuarcy: 0.855\n",
      "Epoch 7 step 810: training loss: 6582.039313615468\n",
      "Epoch 7 step 811: training accuarcy: 0.858\n",
      "Epoch 7 step 811: training loss: 5932.24287536907\n",
      "Epoch 7 step 812: training accuarcy: 0.8715\n",
      "Epoch 7 step 812: training loss: 5429.214777050631\n",
      "Epoch 7 step 813: training accuarcy: 0.881\n",
      "Epoch 7 step 813: training loss: 6130.576612448648\n",
      "Epoch 7 step 814: training accuarcy: 0.865\n",
      "Epoch 7 step 814: training loss: 6395.180063227235\n",
      "Epoch 7 step 815: training accuarcy: 0.861\n",
      "Epoch 7 step 815: training loss: 6585.61844887143\n",
      "Epoch 7 step 816: training accuarcy: 0.857\n",
      "Epoch 7 step 816: training loss: 5979.053980744804\n",
      "Epoch 7 step 817: training accuarcy: 0.868\n",
      "Epoch 7 step 817: training loss: 6288.006108215741\n",
      "Epoch 7 step 818: training accuarcy: 0.861\n",
      "Epoch 7 step 818: training loss: 5706.695406036764\n",
      "Epoch 7 step 819: training accuarcy: 0.8755000000000001\n",
      "Epoch 7 step 819: training loss: 6277.7496053738905\n",
      "Epoch 7 step 820: training accuarcy: 0.8635\n",
      "Epoch 7 step 820: training loss: 6212.91556821546\n",
      "Epoch 7 step 821: training accuarcy: 0.866\n",
      "Epoch 7 step 821: training loss: 6140.164043784374\n",
      "Epoch 7 step 822: training accuarcy: 0.8665\n",
      "Epoch 7 step 822: training loss: 6045.710198172451\n",
      "Epoch 7 step 823: training accuarcy: 0.871\n",
      "Epoch 7 step 823: training loss: 6279.628142697386\n",
      "Epoch 7 step 824: training accuarcy: 0.863\n",
      "Epoch 7 step 824: training loss: 5645.860574624589\n",
      "Epoch 7 step 825: training accuarcy: 0.876\n",
      "Epoch 7 step 825: training loss: 5689.80367775973\n",
      "Epoch 7 step 826: training accuarcy: 0.874\n",
      "Epoch 7 step 826: training loss: 4898.767590725615\n",
      "Epoch 7 step 827: training accuarcy: 0.892\n",
      "Epoch 7 step 827: training loss: 4882.920781123111\n",
      "Epoch 7 step 828: training accuarcy: 0.8925000000000001\n",
      "Epoch 7 step 828: training loss: 5298.779770639182\n",
      "Epoch 7 step 829: training accuarcy: 0.8825000000000001\n",
      "Epoch 7 step 829: training loss: 5426.890014809361\n",
      "Epoch 7 step 830: training accuarcy: 0.881\n",
      "Epoch 7 step 830: training loss: 5320.219383648375\n",
      "Epoch 7 step 831: training accuarcy: 0.884\n",
      "Epoch 7 step 831: training loss: 5952.333672618302\n",
      "Epoch 7 step 832: training accuarcy: 0.871\n",
      "Epoch 7 step 832: training loss: 5850.099223379384\n",
      "Epoch 7 step 833: training accuarcy: 0.8725\n",
      "Epoch 7 step 833: training loss: 5498.797697699872\n",
      "Epoch 7 step 834: training accuarcy: 0.881\n",
      "Epoch 7 step 834: training loss: 5435.2924676848\n",
      "Epoch 7 step 835: training accuarcy: 0.8805000000000001\n",
      "Epoch 7 step 835: training loss: 5782.003366217086\n",
      "Epoch 7 step 836: training accuarcy: 0.874\n",
      "Epoch 7 step 836: training loss: 5205.143836432454\n",
      "Epoch 7 step 837: training accuarcy: 0.8845000000000001\n",
      "Epoch 7 step 837: training loss: 5430.1706333929515\n",
      "Epoch 7 step 838: training accuarcy: 0.8815000000000001\n",
      "Epoch 7 step 838: training loss: 5581.759573510679\n",
      "Epoch 7 step 839: training accuarcy: 0.875\n",
      "Epoch 7 step 839: training loss: 5491.734639587374\n",
      "Epoch 7 step 840: training accuarcy: 0.881\n",
      "Epoch 7 step 840: training loss: 4785.225348086917\n",
      "Epoch 7 step 841: training accuarcy: 0.8945000000000001\n",
      "Epoch 7 step 841: training loss: 5797.7520018586865\n",
      "Epoch 7 step 842: training accuarcy: 0.8735\n",
      "Epoch 7 step 842: training loss: 5213.403156044575\n",
      "Epoch 7 step 843: training accuarcy: 0.8855000000000001\n",
      "Epoch 7 step 843: training loss: 5661.905467903602\n",
      "Epoch 7 step 844: training accuarcy: 0.873\n",
      "Epoch 7 step 844: training loss: 4950.8047258329325\n",
      "Epoch 7 step 845: training accuarcy: 0.8915000000000001\n",
      "Epoch 7 step 845: training loss: 5926.7697493180995\n",
      "Epoch 7 step 846: training accuarcy: 0.872\n",
      "Epoch 7 step 846: training loss: 5613.635560632422\n",
      "Epoch 7 step 847: training accuarcy: 0.877\n",
      "Epoch 7 step 847: training loss: 5049.877329113836\n",
      "Epoch 7 step 848: training accuarcy: 0.891\n",
      "Epoch 7 step 848: training loss: 4614.482548968676\n",
      "Epoch 7 step 849: training accuarcy: 0.8975\n",
      "Epoch 7 step 849: training loss: 5589.583624989846\n",
      "Epoch 7 step 850: training accuarcy: 0.8765000000000001\n",
      "Epoch 7 step 850: training loss: 5661.028795227787\n",
      "Epoch 7 step 851: training accuarcy: 0.8735\n",
      "Epoch 7 step 851: training loss: 5089.08329732739\n",
      "Epoch 7 step 852: training accuarcy: 0.889\n",
      "Epoch 7 step 852: training loss: 6110.83556518085\n",
      "Epoch 7 step 853: training accuarcy: 0.8675\n",
      "Epoch 7 step 853: training loss: 5751.08607286162\n",
      "Epoch 7 step 854: training accuarcy: 0.8755000000000001\n",
      "Epoch 7 step 854: training loss: 6030.402969155838\n",
      "Epoch 7 step 855: training accuarcy: 0.8675\n",
      "Epoch 7 step 855: training loss: 5803.417973563601\n",
      "Epoch 7 step 856: training accuarcy: 0.872\n",
      "Epoch 7 step 856: training loss: 5570.330650310127\n",
      "Epoch 7 step 857: training accuarcy: 0.876\n",
      "Epoch 7 step 857: training loss: 5480.451666514135\n",
      "Epoch 7 step 858: training accuarcy: 0.882\n",
      "Epoch 7 step 858: training loss: 5226.161097024161\n",
      "Epoch 7 step 859: training accuarcy: 0.886\n",
      "Epoch 7 step 859: training loss: 5592.014822929054\n",
      "Epoch 7 step 860: training accuarcy: 0.876\n",
      "Epoch 7 step 860: training loss: 5351.950620911204\n",
      "Epoch 7 step 861: training accuarcy: 0.8825000000000001\n",
      "Epoch 7 step 861: training loss: 5617.208406435178\n",
      "Epoch 7 step 862: training accuarcy: 0.8775000000000001\n",
      "Epoch 7 step 862: training loss: 5153.015041083569\n",
      "Epoch 7 step 863: training accuarcy: 0.889\n",
      "Epoch 7 step 863: training loss: 5159.051132627685\n",
      "Epoch 7 step 864: training accuarcy: 0.8885000000000001\n",
      "Epoch 7 step 864: training loss: 5619.316363871506\n",
      "Epoch 7 step 865: training accuarcy: 0.876\n",
      "Epoch 7 step 865: training loss: 5522.955204945397\n",
      "Epoch 7 step 866: training accuarcy: 0.88\n",
      "Epoch 7 step 866: training loss: 5679.466943688717\n",
      "Epoch 7 step 867: training accuarcy: 0.8765000000000001\n",
      "Epoch 7 step 867: training loss: 5605.571343408989\n",
      "Epoch 7 step 868: training accuarcy: 0.8775000000000001\n",
      "Epoch 7 step 868: training loss: 5271.122725959419\n",
      "Epoch 7 step 869: training accuarcy: 0.884\n",
      "Epoch 7 step 869: training loss: 5246.736411062536\n",
      "Epoch 7 step 870: training accuarcy: 0.885\n",
      "Epoch 7 step 870: training loss: 4608.988567925003\n",
      "Epoch 7 step 871: training accuarcy: 0.899\n",
      "Epoch 7 step 871: training loss: 6618.620389007902\n",
      "Epoch 7 step 872: training accuarcy: 0.855\n",
      "Epoch 7 step 872: training loss: 5923.344636686502\n",
      "Epoch 7 step 873: training accuarcy: 0.8715\n",
      "Epoch 7 step 873: training loss: 5486.029952815384\n",
      "Epoch 7 step 874: training accuarcy: 0.8805000000000001\n",
      "Epoch 7 step 874: training loss: 5203.189603689074\n",
      "Epoch 7 step 875: training accuarcy: 0.887\n",
      "Epoch 7 step 875: training loss: 5454.205470113083\n",
      "Epoch 7 step 876: training accuarcy: 0.882\n",
      "Epoch 7 step 876: training loss: 4846.124680080118\n",
      "Epoch 7 step 877: training accuarcy: 0.892\n",
      "Epoch 7 step 877: training loss: 5730.263858775853\n",
      "Epoch 7 step 878: training accuarcy: 0.8755000000000001\n",
      "Epoch 7 step 878: training loss: 5619.4519121755\n",
      "Epoch 7 step 879: training accuarcy: 0.8785000000000001\n",
      "Epoch 7 step 879: training loss: 5632.9934024885415\n",
      "Epoch 7 step 880: training accuarcy: 0.8785000000000001\n",
      "Epoch 7 step 880: training loss: 5361.751026223643\n",
      "Epoch 7 step 881: training accuarcy: 0.882\n",
      "Epoch 7 step 881: training loss: 5431.485624101852\n",
      "Epoch 7 step 882: training accuarcy: 0.877\n",
      "Epoch 7 step 882: training loss: 5636.713443899268\n",
      "Epoch 7 step 883: training accuarcy: 0.875\n",
      "Epoch 7 step 883: training loss: 5614.668562095082\n",
      "Epoch 7 step 884: training accuarcy: 0.877\n",
      "Epoch 7 step 884: training loss: 5721.187414041431\n",
      "Epoch 7 step 885: training accuarcy: 0.8735\n",
      "Epoch 7 step 885: training loss: 5383.21849095816\n",
      "Epoch 7 step 886: training accuarcy: 0.883\n",
      "Epoch 7 step 886: training loss: 5296.259248079378\n",
      "Epoch 7 step 887: training accuarcy: 0.8845000000000001\n",
      "Epoch 7 step 887: training loss: 5023.101993104348\n",
      "Epoch 7 step 888: training accuarcy: 0.889\n",
      "Epoch 7 step 888: training loss: 5295.2105688499805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 889: training accuarcy: 0.8845000000000001\n",
      "Epoch 7 step 889: training loss: 5906.359454037861\n",
      "Epoch 7 step 890: training accuarcy: 0.871\n",
      "Epoch 7 step 890: training loss: 5488.287287710072\n",
      "Epoch 7 step 891: training accuarcy: 0.8805000000000001\n",
      "Epoch 7 step 891: training loss: 5233.724942505251\n",
      "Epoch 7 step 892: training accuarcy: 0.883\n",
      "Epoch 7 step 892: training loss: 4602.219778548354\n",
      "Epoch 7 step 893: training accuarcy: 0.9\n",
      "Epoch 7 step 893: training loss: 4832.633448932175\n",
      "Epoch 7 step 894: training accuarcy: 0.8955000000000001\n",
      "Epoch 7 step 894: training loss: 5657.795660820183\n",
      "Epoch 7 step 895: training accuarcy: 0.8765000000000001\n",
      "Epoch 7 step 895: training loss: 4720.576335234174\n",
      "Epoch 7 step 896: training accuarcy: 0.8945000000000001\n",
      "Epoch 7 step 896: training loss: 5134.494897895243\n",
      "Epoch 7 step 897: training accuarcy: 0.8865000000000001\n",
      "Epoch 7 step 897: training loss: 5359.824407331974\n",
      "Epoch 7 step 898: training accuarcy: 0.8835000000000001\n",
      "Epoch 7 step 898: training loss: 4926.413687082286\n",
      "Epoch 7 step 899: training accuarcy: 0.892\n",
      "Epoch 7 step 899: training loss: 5154.414720921373\n",
      "Epoch 7 step 900: training accuarcy: 0.888\n",
      "Epoch 7 step 900: training loss: 4829.033703325266\n",
      "Epoch 7 step 901: training accuarcy: 0.8955000000000001\n",
      "Epoch 7 step 901: training loss: 4742.126050714038\n",
      "Epoch 7 step 902: training accuarcy: 0.897\n",
      "Epoch 7 step 902: training loss: 4283.5388043502335\n",
      "Epoch 7 step 903: training accuarcy: 0.906\n",
      "Epoch 7 step 903: training loss: 5304.371083195211\n",
      "Epoch 7 step 904: training accuarcy: 0.885\n",
      "Epoch 7 step 904: training loss: 4095.2195011079466\n",
      "Epoch 7 step 905: training accuarcy: 0.9105\n",
      "Epoch 7 step 905: training loss: 4860.471516060556\n",
      "Epoch 7 step 906: training accuarcy: 0.892\n",
      "Epoch 7 step 906: training loss: 5336.067028497588\n",
      "Epoch 7 step 907: training accuarcy: 0.884\n",
      "Epoch 7 step 907: training loss: 4786.9038048201555\n",
      "Epoch 7 step 908: training accuarcy: 0.895\n",
      "Epoch 7 step 908: training loss: 4715.433340952898\n",
      "Epoch 7 step 909: training accuarcy: 0.8965\n",
      "Epoch 7 step 909: training loss: 5178.0307858288625\n",
      "Epoch 7 step 910: training accuarcy: 0.887\n",
      "Epoch 7 step 910: training loss: 5361.5547429381795\n",
      "Epoch 7 step 911: training accuarcy: 0.8845000000000001\n",
      "Epoch 7 step 911: training loss: 4958.899456843997\n",
      "Epoch 7 step 912: training accuarcy: 0.892\n",
      "Epoch 7 step 912: training loss: 4362.877549886568\n",
      "Epoch 7 step 913: training accuarcy: 0.9065\n",
      "Epoch 7 step 913: training loss: 5082.891086565973\n",
      "Epoch 7 step 914: training accuarcy: 0.8895000000000001\n",
      "Epoch 7 step 914: training loss: 5043.007238488967\n",
      "Epoch 7 step 915: training accuarcy: 0.89\n",
      "Epoch 7 step 915: training loss: 5009.360093286424\n",
      "Epoch 7 step 916: training accuarcy: 0.89\n",
      "Epoch 7 step 916: training loss: 4720.039469091668\n",
      "Epoch 7 step 917: training accuarcy: 0.8975\n",
      "Epoch 7 step 917: training loss: 4994.357218746639\n",
      "Epoch 7 step 918: training accuarcy: 0.8905000000000001\n",
      "Epoch 7 step 918: training loss: 4804.152845008244\n",
      "Epoch 7 step 919: training accuarcy: 0.896\n",
      "Epoch 7 step 919: training loss: 4917.416179910977\n",
      "Epoch 7 step 920: training accuarcy: 0.8878406708595389\n",
      "Epoch 7: train loss 5443.723256551635, train accuarcy 0.8811598420143127\n",
      "Epoch 7: valid loss 4933.789000706446, valid accuarcy 0.8900506496429443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [06:57<00:00, 52.09s/it]\n"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=8, loss_callback=simple_loss_callback, log_dir=get_log_dir('kaggle', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T09:10:30.929752Z",
     "start_time": "2019-09-25T09:10:30.925751Z"
    }
   },
   "outputs": [],
   "source": [
    "del prme_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Trans FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T09:10:54.755083Z",
     "start_time": "2019-09-25T09:10:54.613083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTransFM()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model = TorchTransFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "trans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T09:11:01.177760Z",
     "start_time": "2019-09-25T09:11:01.173761Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(trans_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T09:11:06.088887Z",
     "start_time": "2019-09-25T09:11:06.051887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1eddbc084e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_learner = FMLearner(trans_model, adam_opt, schedular, db)\n",
    "trans_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T09:18:50.048401Z",
     "start_time": "2019-09-25T09:11:35.021383Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 12160.892751540743\n",
      "Epoch 0 step 1: training accuarcy: 0.9105\n",
      "Epoch 0 step 1: training loss: 11826.89675858812\n",
      "Epoch 0 step 2: training accuarcy: 0.927\n",
      "Epoch 0 step 2: training loss: 11452.118953841327\n",
      "Epoch 0 step 3: training accuarcy: 0.9470000000000001\n",
      "Epoch 0 step 3: training loss: 11068.639538373965\n",
      "Epoch 0 step 4: training accuarcy: 0.9550000000000001\n",
      "Epoch 0 step 4: training loss: 10724.853203650406\n",
      "Epoch 0 step 5: training accuarcy: 0.969\n",
      "Epoch 0 step 5: training loss: 10366.000922362471\n",
      "Epoch 0 step 6: training accuarcy: 0.971\n",
      "Epoch 0 step 6: training loss: 10050.857014314888\n",
      "Epoch 0 step 7: training accuarcy: 0.9695\n",
      "Epoch 0 step 7: training loss: 9782.412504553706\n",
      "Epoch 0 step 8: training accuarcy: 0.9605\n",
      "Epoch 0 step 8: training loss: 9429.218339358878\n",
      "Epoch 0 step 9: training accuarcy: 0.9695\n",
      "Epoch 0 step 9: training loss: 9150.083329365123\n",
      "Epoch 0 step 10: training accuarcy: 0.9655\n",
      "Epoch 0 step 10: training loss: 8803.925872109134\n",
      "Epoch 0 step 11: training accuarcy: 0.9735\n",
      "Epoch 0 step 11: training loss: 8543.353538244703\n",
      "Epoch 0 step 12: training accuarcy: 0.9665\n",
      "Epoch 0 step 12: training loss: 8273.990444487585\n",
      "Epoch 0 step 13: training accuarcy: 0.965\n",
      "Epoch 0 step 13: training loss: 7973.514577509442\n",
      "Epoch 0 step 14: training accuarcy: 0.9735\n",
      "Epoch 0 step 14: training loss: 7694.30073784117\n",
      "Epoch 0 step 15: training accuarcy: 0.9785\n",
      "Epoch 0 step 15: training loss: 7436.699495246393\n",
      "Epoch 0 step 16: training accuarcy: 0.9785\n",
      "Epoch 0 step 16: training loss: 7188.602976699378\n",
      "Epoch 0 step 17: training accuarcy: 0.978\n",
      "Epoch 0 step 17: training loss: 6937.481896052023\n",
      "Epoch 0 step 18: training accuarcy: 0.983\n",
      "Epoch 0 step 18: training loss: 6719.022167297091\n",
      "Epoch 0 step 19: training accuarcy: 0.978\n",
      "Epoch 0 step 19: training loss: 6480.816843999165\n",
      "Epoch 0 step 20: training accuarcy: 0.982\n",
      "Epoch 0 step 20: training loss: 6250.130015237263\n",
      "Epoch 0 step 21: training accuarcy: 0.9865\n",
      "Epoch 0 step 21: training loss: 6052.084067816939\n",
      "Epoch 0 step 22: training accuarcy: 0.984\n",
      "Epoch 0 step 22: training loss: 5832.474472795749\n",
      "Epoch 0 step 23: training accuarcy: 0.9865\n",
      "Epoch 0 step 23: training loss: 5634.475604631302\n",
      "Epoch 0 step 24: training accuarcy: 0.988\n",
      "Epoch 0 step 24: training loss: 5435.55833188793\n",
      "Epoch 0 step 25: training accuarcy: 0.9915\n",
      "Epoch 0 step 25: training loss: 5238.78119503129\n",
      "Epoch 0 step 26: training accuarcy: 0.9915\n",
      "Epoch 0 step 26: training loss: 5056.830804626499\n",
      "Epoch 0 step 27: training accuarcy: 0.99\n",
      "Epoch 0 step 27: training loss: 4879.312001686226\n",
      "Epoch 0 step 28: training accuarcy: 0.988\n",
      "Epoch 0 step 28: training loss: 4714.7418688159605\n",
      "Epoch 0 step 29: training accuarcy: 0.9865\n",
      "Epoch 0 step 29: training loss: 4541.101765467536\n",
      "Epoch 0 step 30: training accuarcy: 0.9915\n",
      "Epoch 0 step 30: training loss: 4375.1615237732385\n",
      "Epoch 0 step 31: training accuarcy: 0.991\n",
      "Epoch 0 step 31: training loss: 4221.525906426616\n",
      "Epoch 0 step 32: training accuarcy: 0.995\n",
      "Epoch 0 step 32: training loss: 4070.8424004228737\n",
      "Epoch 0 step 33: training accuarcy: 0.9915\n",
      "Epoch 0 step 33: training loss: 3926.089116560738\n",
      "Epoch 0 step 34: training accuarcy: 0.99\n",
      "Epoch 0 step 34: training loss: 3784.787712707578\n",
      "Epoch 0 step 35: training accuarcy: 0.992\n",
      "Epoch 0 step 35: training loss: 3636.5917588352117\n",
      "Epoch 0 step 36: training accuarcy: 0.992\n",
      "Epoch 0 step 36: training loss: 3503.349084256139\n",
      "Epoch 0 step 37: training accuarcy: 0.9955\n",
      "Epoch 0 step 37: training loss: 3386.6217415250308\n",
      "Epoch 0 step 38: training accuarcy: 0.989\n",
      "Epoch 0 step 38: training loss: 3254.4417129276376\n",
      "Epoch 0 step 39: training accuarcy: 0.9925\n",
      "Epoch 0 step 39: training loss: 3123.335178795892\n",
      "Epoch 0 step 40: training accuarcy: 0.9955\n",
      "Epoch 0 step 40: training loss: 3006.8722179866745\n",
      "Epoch 0 step 41: training accuarcy: 0.995\n",
      "Epoch 0 step 41: training loss: 2900.6103608437243\n",
      "Epoch 0 step 42: training accuarcy: 0.993\n",
      "Epoch 0 step 42: training loss: 2792.6743515322287\n",
      "Epoch 0 step 43: training accuarcy: 0.9935\n",
      "Epoch 0 step 43: training loss: 2679.6204515320474\n",
      "Epoch 0 step 44: training accuarcy: 0.997\n",
      "Epoch 0 step 44: training loss: 2589.451166666126\n",
      "Epoch 0 step 45: training accuarcy: 0.9915\n",
      "Epoch 0 step 45: training loss: 2483.243456300955\n",
      "Epoch 0 step 46: training accuarcy: 0.9955\n",
      "Epoch 0 step 46: training loss: 2391.9728152275156\n",
      "Epoch 0 step 47: training accuarcy: 0.9945\n",
      "Epoch 0 step 47: training loss: 2298.596081912663\n",
      "Epoch 0 step 48: training accuarcy: 0.9935\n",
      "Epoch 0 step 48: training loss: 2215.8204892261356\n",
      "Epoch 0 step 49: training accuarcy: 0.9945\n",
      "Epoch 0 step 49: training loss: 2119.560206693707\n",
      "Epoch 0 step 50: training accuarcy: 0.996\n",
      "Epoch 0 step 50: training loss: 2040.4961059218579\n",
      "Epoch 0 step 51: training accuarcy: 0.9955\n",
      "Epoch 0 step 51: training loss: 1967.573895532967\n",
      "Epoch 0 step 52: training accuarcy: 0.994\n",
      "Epoch 0 step 52: training loss: 1884.533723719006\n",
      "Epoch 0 step 53: training accuarcy: 0.997\n",
      "Epoch 0 step 53: training loss: 1818.5316029839378\n",
      "Epoch 0 step 54: training accuarcy: 0.994\n",
      "Epoch 0 step 54: training loss: 1746.863032861284\n",
      "Epoch 0 step 55: training accuarcy: 0.993\n",
      "Epoch 0 step 55: training loss: 1679.0388324882208\n",
      "Epoch 0 step 56: training accuarcy: 0.992\n",
      "Epoch 0 step 56: training loss: 1604.9679291387704\n",
      "Epoch 0 step 57: training accuarcy: 0.9975\n",
      "Epoch 0 step 57: training loss: 1547.7394654533264\n",
      "Epoch 0 step 58: training accuarcy: 0.9945\n",
      "Epoch 0 step 58: training loss: 1483.8708450578285\n",
      "Epoch 0 step 59: training accuarcy: 0.9965\n",
      "Epoch 0 step 59: training loss: 1421.4077680715627\n",
      "Epoch 0 step 60: training accuarcy: 0.996\n",
      "Epoch 0 step 60: training loss: 1367.7508316390729\n",
      "Epoch 0 step 61: training accuarcy: 0.9945\n",
      "Epoch 0 step 61: training loss: 1317.6755555365753\n",
      "Epoch 0 step 62: training accuarcy: 0.9945\n",
      "Epoch 0 step 62: training loss: 1261.6277784206725\n",
      "Epoch 0 step 63: training accuarcy: 0.995\n",
      "Epoch 0 step 63: training loss: 1208.884016567063\n",
      "Epoch 0 step 64: training accuarcy: 0.9955\n",
      "Epoch 0 step 64: training loss: 1164.9379265184543\n",
      "Epoch 0 step 65: training accuarcy: 0.9945\n",
      "Epoch 0 step 65: training loss: 1121.3633397541564\n",
      "Epoch 0 step 66: training accuarcy: 0.9935\n",
      "Epoch 0 step 66: training loss: 1070.3866053070478\n",
      "Epoch 0 step 67: training accuarcy: 0.996\n",
      "Epoch 0 step 67: training loss: 1025.0626705140335\n",
      "Epoch 0 step 68: training accuarcy: 0.995\n",
      "Epoch 0 step 68: training loss: 989.3617365545944\n",
      "Epoch 0 step 69: training accuarcy: 0.9935\n",
      "Epoch 0 step 69: training loss: 947.4975159725359\n",
      "Epoch 0 step 70: training accuarcy: 0.992\n",
      "Epoch 0 step 70: training loss: 901.7914909076816\n",
      "Epoch 0 step 71: training accuarcy: 0.9965\n",
      "Epoch 0 step 71: training loss: 870.9621879277413\n",
      "Epoch 0 step 72: training accuarcy: 0.9955\n",
      "Epoch 0 step 72: training loss: 838.3958185147306\n",
      "Epoch 0 step 73: training accuarcy: 0.995\n",
      "Epoch 0 step 73: training loss: 791.7054954254344\n",
      "Epoch 0 step 74: training accuarcy: 0.9965\n",
      "Epoch 0 step 74: training loss: 767.9954681210556\n",
      "Epoch 0 step 75: training accuarcy: 0.9945\n",
      "Epoch 0 step 75: training loss: 738.221107818546\n",
      "Epoch 0 step 76: training accuarcy: 0.9935\n",
      "Epoch 0 step 76: training loss: 706.6626297690075\n",
      "Epoch 0 step 77: training accuarcy: 0.992\n",
      "Epoch 0 step 77: training loss: 680.2583369337727\n",
      "Epoch 0 step 78: training accuarcy: 0.993\n",
      "Epoch 0 step 78: training loss: 645.9803264332512\n",
      "Epoch 0 step 79: training accuarcy: 0.9945\n",
      "Epoch 0 step 79: training loss: 617.7219725076685\n",
      "Epoch 0 step 80: training accuarcy: 0.9965\n",
      "Epoch 0 step 80: training loss: 595.5773853658872\n",
      "Epoch 0 step 81: training accuarcy: 0.9945\n",
      "Epoch 0 step 81: training loss: 567.7098213550076\n",
      "Epoch 0 step 82: training accuarcy: 0.9955\n",
      "Epoch 0 step 82: training loss: 550.7369708835406\n",
      "Epoch 0 step 83: training accuarcy: 0.994\n",
      "Epoch 0 step 83: training loss: 522.8856149100283\n",
      "Epoch 0 step 84: training accuarcy: 0.9945\n",
      "Epoch 0 step 84: training loss: 496.0260190908734\n",
      "Epoch 0 step 85: training accuarcy: 0.9975\n",
      "Epoch 0 step 85: training loss: 478.2790142956938\n",
      "Epoch 0 step 86: training accuarcy: 0.994\n",
      "Epoch 0 step 86: training loss: 470.17765494031664\n",
      "Epoch 0 step 87: training accuarcy: 0.993\n",
      "Epoch 0 step 87: training loss: 440.14905234447656\n",
      "Epoch 0 step 88: training accuarcy: 0.9935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 88: training loss: 419.4247955832205\n",
      "Epoch 0 step 89: training accuarcy: 0.997\n",
      "Epoch 0 step 89: training loss: 409.45225790489536\n",
      "Epoch 0 step 90: training accuarcy: 0.994\n",
      "Epoch 0 step 90: training loss: 384.0902134739546\n",
      "Epoch 0 step 91: training accuarcy: 0.996\n",
      "Epoch 0 step 91: training loss: 365.057570378091\n",
      "Epoch 0 step 92: training accuarcy: 0.9965\n",
      "Epoch 0 step 92: training loss: 349.4236759040039\n",
      "Epoch 0 step 93: training accuarcy: 0.997\n",
      "Epoch 0 step 93: training loss: 339.9839352012886\n",
      "Epoch 0 step 94: training accuarcy: 0.9965\n",
      "Epoch 0 step 94: training loss: 325.60015673319975\n",
      "Epoch 0 step 95: training accuarcy: 0.995\n",
      "Epoch 0 step 95: training loss: 320.0739945810385\n",
      "Epoch 0 step 96: training accuarcy: 0.993\n",
      "Epoch 0 step 96: training loss: 300.96693631516086\n",
      "Epoch 0 step 97: training accuarcy: 0.995\n",
      "Epoch 0 step 97: training loss: 283.46571088939487\n",
      "Epoch 0 step 98: training accuarcy: 0.997\n",
      "Epoch 0 step 98: training loss: 277.02326575982187\n",
      "Epoch 0 step 99: training accuarcy: 0.9955\n",
      "Epoch 0 step 99: training loss: 265.4143112240426\n",
      "Epoch 0 step 100: training accuarcy: 0.996\n",
      "Epoch 0 step 100: training loss: 247.01331809445338\n",
      "Epoch 0 step 101: training accuarcy: 0.9965\n",
      "Epoch 0 step 101: training loss: 239.9802499604421\n",
      "Epoch 0 step 102: training accuarcy: 0.996\n",
      "Epoch 0 step 102: training loss: 230.29618481041427\n",
      "Epoch 0 step 103: training accuarcy: 0.9955\n",
      "Epoch 0 step 103: training loss: 217.01065194876136\n",
      "Epoch 0 step 104: training accuarcy: 0.998\n",
      "Epoch 0 step 104: training loss: 211.87088404095104\n",
      "Epoch 0 step 105: training accuarcy: 0.9955\n",
      "Epoch 0 step 105: training loss: 199.92741544601876\n",
      "Epoch 0 step 106: training accuarcy: 0.9975\n",
      "Epoch 0 step 106: training loss: 190.74129730892062\n",
      "Epoch 0 step 107: training accuarcy: 0.997\n",
      "Epoch 0 step 107: training loss: 182.5712928888408\n",
      "Epoch 0 step 108: training accuarcy: 0.998\n",
      "Epoch 0 step 108: training loss: 180.64263496703802\n",
      "Epoch 0 step 109: training accuarcy: 0.9945\n",
      "Epoch 0 step 109: training loss: 179.6693712317027\n",
      "Epoch 0 step 110: training accuarcy: 0.9945\n",
      "Epoch 0 step 110: training loss: 170.2265083677687\n",
      "Epoch 0 step 111: training accuarcy: 0.996\n",
      "Epoch 0 step 111: training loss: 159.64132185334088\n",
      "Epoch 0 step 112: training accuarcy: 0.996\n",
      "Epoch 0 step 112: training loss: 154.87560303383293\n",
      "Epoch 0 step 113: training accuarcy: 0.997\n",
      "Epoch 0 step 113: training loss: 149.10526587165603\n",
      "Epoch 0 step 114: training accuarcy: 0.994\n",
      "Epoch 0 step 114: training loss: 142.3173221391467\n",
      "Epoch 0 step 115: training accuarcy: 0.9973794549266248\n",
      "Epoch 0: train loss 3006.147942774386, train accuarcy 0.9883856773376465\n",
      "Epoch 0: valid loss 128.3843254249123, valid accuarcy 0.9969033598899841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████▏                                                                                                                                     | 1/8 [00:54<06:19, 54.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 115: training loss: 134.75841000271495\n",
      "Epoch 1 step 116: training accuarcy: 0.9955\n",
      "Epoch 1 step 116: training loss: 120.88499411902873\n",
      "Epoch 1 step 117: training accuarcy: 0.999\n",
      "Epoch 1 step 117: training loss: 126.06403871832953\n",
      "Epoch 1 step 118: training accuarcy: 0.9955\n",
      "Epoch 1 step 118: training loss: 119.76052939137108\n",
      "Epoch 1 step 119: training accuarcy: 0.996\n",
      "Epoch 1 step 119: training loss: 110.92569184993953\n",
      "Epoch 1 step 120: training accuarcy: 0.9985\n",
      "Epoch 1 step 120: training loss: 108.791205074866\n",
      "Epoch 1 step 121: training accuarcy: 0.997\n",
      "Epoch 1 step 121: training loss: 102.83956020918073\n",
      "Epoch 1 step 122: training accuarcy: 0.9965\n",
      "Epoch 1 step 122: training loss: 103.65687674112475\n",
      "Epoch 1 step 123: training accuarcy: 0.9955\n",
      "Epoch 1 step 123: training loss: 100.94866732094351\n",
      "Epoch 1 step 124: training accuarcy: 0.9965\n",
      "Epoch 1 step 124: training loss: 96.67249741858993\n",
      "Epoch 1 step 125: training accuarcy: 0.994\n",
      "Epoch 1 step 125: training loss: 91.53581066885565\n",
      "Epoch 1 step 126: training accuarcy: 0.9965\n",
      "Epoch 1 step 126: training loss: 90.22597206050914\n",
      "Epoch 1 step 127: training accuarcy: 0.997\n",
      "Epoch 1 step 127: training loss: 91.18298557114383\n",
      "Epoch 1 step 128: training accuarcy: 0.9935\n",
      "Epoch 1 step 128: training loss: 84.74675135577705\n",
      "Epoch 1 step 129: training accuarcy: 0.9965\n",
      "Epoch 1 step 129: training loss: 76.52653709242611\n",
      "Epoch 1 step 130: training accuarcy: 0.9955\n",
      "Epoch 1 step 130: training loss: 85.78986703070883\n",
      "Epoch 1 step 131: training accuarcy: 0.9945\n",
      "Epoch 1 step 131: training loss: 74.96665255778439\n",
      "Epoch 1 step 132: training accuarcy: 0.9965\n",
      "Epoch 1 step 132: training loss: 64.45011017828685\n",
      "Epoch 1 step 133: training accuarcy: 0.9975\n",
      "Epoch 1 step 133: training loss: 74.768564770321\n",
      "Epoch 1 step 134: training accuarcy: 0.995\n",
      "Epoch 1 step 134: training loss: 60.89346279873911\n",
      "Epoch 1 step 135: training accuarcy: 0.9965\n",
      "Epoch 1 step 135: training loss: 64.53750130282745\n",
      "Epoch 1 step 136: training accuarcy: 0.9985\n",
      "Epoch 1 step 136: training loss: 69.04851595817594\n",
      "Epoch 1 step 137: training accuarcy: 0.9945\n",
      "Epoch 1 step 137: training loss: 60.91402477260783\n",
      "Epoch 1 step 138: training accuarcy: 0.9955\n",
      "Epoch 1 step 138: training loss: 62.82869823687506\n",
      "Epoch 1 step 139: training accuarcy: 0.9955\n",
      "Epoch 1 step 139: training loss: 56.24458694189184\n",
      "Epoch 1 step 140: training accuarcy: 0.9965\n",
      "Epoch 1 step 140: training loss: 56.69473405089142\n",
      "Epoch 1 step 141: training accuarcy: 0.9975\n",
      "Epoch 1 step 141: training loss: 63.16335688748824\n",
      "Epoch 1 step 142: training accuarcy: 0.9925\n",
      "Epoch 1 step 142: training loss: 49.37516900785825\n",
      "Epoch 1 step 143: training accuarcy: 0.9985\n",
      "Epoch 1 step 143: training loss: 49.24413263571069\n",
      "Epoch 1 step 144: training accuarcy: 0.998\n",
      "Epoch 1 step 144: training loss: 56.83171018893684\n",
      "Epoch 1 step 145: training accuarcy: 0.9935\n",
      "Epoch 1 step 145: training loss: 49.389968238188914\n",
      "Epoch 1 step 146: training accuarcy: 0.9975\n",
      "Epoch 1 step 146: training loss: 53.50389410009319\n",
      "Epoch 1 step 147: training accuarcy: 0.9965\n",
      "Epoch 1 step 147: training loss: 40.11743160314562\n",
      "Epoch 1 step 148: training accuarcy: 0.999\n",
      "Epoch 1 step 148: training loss: 40.372347326818186\n",
      "Epoch 1 step 149: training accuarcy: 0.998\n",
      "Epoch 1 step 149: training loss: 43.90285820816128\n",
      "Epoch 1 step 150: training accuarcy: 0.9985\n",
      "Epoch 1 step 150: training loss: 43.582121115471836\n",
      "Epoch 1 step 151: training accuarcy: 0.9955\n",
      "Epoch 1 step 151: training loss: 40.50527980940943\n",
      "Epoch 1 step 152: training accuarcy: 0.9965\n",
      "Epoch 1 step 152: training loss: 44.52823594852886\n",
      "Epoch 1 step 153: training accuarcy: 0.994\n",
      "Epoch 1 step 153: training loss: 45.53429362446048\n",
      "Epoch 1 step 154: training accuarcy: 0.9945\n",
      "Epoch 1 step 154: training loss: 46.809895338877766\n",
      "Epoch 1 step 155: training accuarcy: 0.9955\n",
      "Epoch 1 step 155: training loss: 43.86254652710757\n",
      "Epoch 1 step 156: training accuarcy: 0.9955\n",
      "Epoch 1 step 156: training loss: 33.68998571528248\n",
      "Epoch 1 step 157: training accuarcy: 0.998\n",
      "Epoch 1 step 157: training loss: 37.14259743351636\n",
      "Epoch 1 step 158: training accuarcy: 0.998\n",
      "Epoch 1 step 158: training loss: 40.18620243313829\n",
      "Epoch 1 step 159: training accuarcy: 0.9965\n",
      "Epoch 1 step 159: training loss: 42.360511555472016\n",
      "Epoch 1 step 160: training accuarcy: 0.995\n",
      "Epoch 1 step 160: training loss: 36.18477197961603\n",
      "Epoch 1 step 161: training accuarcy: 0.9965\n",
      "Epoch 1 step 161: training loss: 31.938444603236945\n",
      "Epoch 1 step 162: training accuarcy: 0.9975\n",
      "Epoch 1 step 162: training loss: 38.63701361783636\n",
      "Epoch 1 step 163: training accuarcy: 0.993\n",
      "Epoch 1 step 163: training loss: 32.611832916226035\n",
      "Epoch 1 step 164: training accuarcy: 0.9975\n",
      "Epoch 1 step 164: training loss: 35.79459903065254\n",
      "Epoch 1 step 165: training accuarcy: 0.9965\n",
      "Epoch 1 step 165: training loss: 37.2528404428745\n",
      "Epoch 1 step 166: training accuarcy: 0.9965\n",
      "Epoch 1 step 166: training loss: 34.35826859114317\n",
      "Epoch 1 step 167: training accuarcy: 0.9955\n",
      "Epoch 1 step 167: training loss: 34.66468773401661\n",
      "Epoch 1 step 168: training accuarcy: 0.9955\n",
      "Epoch 1 step 168: training loss: 39.28406004085842\n",
      "Epoch 1 step 169: training accuarcy: 0.9965\n",
      "Epoch 1 step 169: training loss: 32.485447637339846\n",
      "Epoch 1 step 170: training accuarcy: 0.9955\n",
      "Epoch 1 step 170: training loss: 27.252090315419018\n",
      "Epoch 1 step 171: training accuarcy: 0.997\n",
      "Epoch 1 step 171: training loss: 37.92387172921251\n",
      "Epoch 1 step 172: training accuarcy: 0.993\n",
      "Epoch 1 step 172: training loss: 36.13289167766837\n",
      "Epoch 1 step 173: training accuarcy: 0.995\n",
      "Epoch 1 step 173: training loss: 32.38059077573256\n",
      "Epoch 1 step 174: training accuarcy: 0.997\n",
      "Epoch 1 step 174: training loss: 31.69099702139917\n",
      "Epoch 1 step 175: training accuarcy: 0.997\n",
      "Epoch 1 step 175: training loss: 26.991348829376292\n",
      "Epoch 1 step 176: training accuarcy: 0.9965\n",
      "Epoch 1 step 176: training loss: 27.145987080328986\n",
      "Epoch 1 step 177: training accuarcy: 0.998\n",
      "Epoch 1 step 177: training loss: 30.05416731286454\n",
      "Epoch 1 step 178: training accuarcy: 0.997\n",
      "Epoch 1 step 178: training loss: 23.940150477868023\n",
      "Epoch 1 step 179: training accuarcy: 0.998\n",
      "Epoch 1 step 179: training loss: 33.21791413632192\n",
      "Epoch 1 step 180: training accuarcy: 0.9945\n",
      "Epoch 1 step 180: training loss: 26.28391989007269\n",
      "Epoch 1 step 181: training accuarcy: 0.997\n",
      "Epoch 1 step 181: training loss: 26.969233163221055\n",
      "Epoch 1 step 182: training accuarcy: 0.9965\n",
      "Epoch 1 step 182: training loss: 24.940387710335948\n",
      "Epoch 1 step 183: training accuarcy: 0.9985\n",
      "Epoch 1 step 183: training loss: 24.135933625024833\n",
      "Epoch 1 step 184: training accuarcy: 0.9975\n",
      "Epoch 1 step 184: training loss: 28.59055514240309\n",
      "Epoch 1 step 185: training accuarcy: 0.997\n",
      "Epoch 1 step 185: training loss: 30.809016849466612\n",
      "Epoch 1 step 186: training accuarcy: 0.9945\n",
      "Epoch 1 step 186: training loss: 25.714323113290448\n",
      "Epoch 1 step 187: training accuarcy: 0.996\n",
      "Epoch 1 step 187: training loss: 34.03406738714522\n",
      "Epoch 1 step 188: training accuarcy: 0.9935\n",
      "Epoch 1 step 188: training loss: 24.654171089834197\n",
      "Epoch 1 step 189: training accuarcy: 0.9975\n",
      "Epoch 1 step 189: training loss: 27.464000621494407\n",
      "Epoch 1 step 190: training accuarcy: 0.9965\n",
      "Epoch 1 step 190: training loss: 27.11125531269981\n",
      "Epoch 1 step 191: training accuarcy: 0.995\n",
      "Epoch 1 step 191: training loss: 30.179369144836656\n",
      "Epoch 1 step 192: training accuarcy: 0.9965\n",
      "Epoch 1 step 192: training loss: 32.51851617074367\n",
      "Epoch 1 step 193: training accuarcy: 0.9935\n",
      "Epoch 1 step 193: training loss: 20.718331086121882\n",
      "Epoch 1 step 194: training accuarcy: 0.998\n",
      "Epoch 1 step 194: training loss: 31.02367851066625\n",
      "Epoch 1 step 195: training accuarcy: 0.9945\n",
      "Epoch 1 step 195: training loss: 22.202662776060865\n",
      "Epoch 1 step 196: training accuarcy: 0.9975\n",
      "Epoch 1 step 196: training loss: 23.356766816219192\n",
      "Epoch 1 step 197: training accuarcy: 0.9985\n",
      "Epoch 1 step 197: training loss: 22.67227846232519\n",
      "Epoch 1 step 198: training accuarcy: 0.997\n",
      "Epoch 1 step 198: training loss: 23.609774597486805\n",
      "Epoch 1 step 199: training accuarcy: 0.999\n",
      "Epoch 1 step 199: training loss: 28.943815899993872\n",
      "Epoch 1 step 200: training accuarcy: 0.9965\n",
      "Epoch 1 step 200: training loss: 25.819923442441052\n",
      "Epoch 1 step 201: training accuarcy: 0.9965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 201: training loss: 27.415942534775645\n",
      "Epoch 1 step 202: training accuarcy: 0.996\n",
      "Epoch 1 step 202: training loss: 33.26499650317609\n",
      "Epoch 1 step 203: training accuarcy: 0.9935\n",
      "Epoch 1 step 203: training loss: 20.51639396315599\n",
      "Epoch 1 step 204: training accuarcy: 0.9985\n",
      "Epoch 1 step 204: training loss: 27.252504396786893\n",
      "Epoch 1 step 205: training accuarcy: 0.9975\n",
      "Epoch 1 step 205: training loss: 26.84765713427013\n",
      "Epoch 1 step 206: training accuarcy: 0.9965\n",
      "Epoch 1 step 206: training loss: 23.812342544387416\n",
      "Epoch 1 step 207: training accuarcy: 0.998\n",
      "Epoch 1 step 207: training loss: 22.504232696832574\n",
      "Epoch 1 step 208: training accuarcy: 0.997\n",
      "Epoch 1 step 208: training loss: 23.393757102226985\n",
      "Epoch 1 step 209: training accuarcy: 0.9975\n",
      "Epoch 1 step 209: training loss: 24.505789238167655\n",
      "Epoch 1 step 210: training accuarcy: 0.9975\n",
      "Epoch 1 step 210: training loss: 17.956525918513268\n",
      "Epoch 1 step 211: training accuarcy: 0.9985\n",
      "Epoch 1 step 211: training loss: 20.550947727674313\n",
      "Epoch 1 step 212: training accuarcy: 0.998\n",
      "Epoch 1 step 212: training loss: 18.345259851584636\n",
      "Epoch 1 step 213: training accuarcy: 0.9985\n",
      "Epoch 1 step 213: training loss: 24.139902384874947\n",
      "Epoch 1 step 214: training accuarcy: 0.997\n",
      "Epoch 1 step 214: training loss: 19.311046241392923\n",
      "Epoch 1 step 215: training accuarcy: 0.9985\n",
      "Epoch 1 step 215: training loss: 16.78072015231806\n",
      "Epoch 1 step 216: training accuarcy: 0.999\n",
      "Epoch 1 step 216: training loss: 26.939099176434006\n",
      "Epoch 1 step 217: training accuarcy: 0.9955\n",
      "Epoch 1 step 217: training loss: 19.633445051790474\n",
      "Epoch 1 step 218: training accuarcy: 0.998\n",
      "Epoch 1 step 218: training loss: 17.94472455598067\n",
      "Epoch 1 step 219: training accuarcy: 0.9975\n",
      "Epoch 1 step 219: training loss: 22.40850464849171\n",
      "Epoch 1 step 220: training accuarcy: 0.997\n",
      "Epoch 1 step 220: training loss: 31.427536753544132\n",
      "Epoch 1 step 221: training accuarcy: 0.994\n",
      "Epoch 1 step 221: training loss: 20.820431515609542\n",
      "Epoch 1 step 222: training accuarcy: 0.998\n",
      "Epoch 1 step 222: training loss: 26.908453574886092\n",
      "Epoch 1 step 223: training accuarcy: 0.9965\n",
      "Epoch 1 step 223: training loss: 21.75698275651736\n",
      "Epoch 1 step 224: training accuarcy: 0.9975\n",
      "Epoch 1 step 224: training loss: 25.518553971892327\n",
      "Epoch 1 step 225: training accuarcy: 0.9975\n",
      "Epoch 1 step 225: training loss: 18.79131834632953\n",
      "Epoch 1 step 226: training accuarcy: 0.998\n",
      "Epoch 1 step 226: training loss: 27.145183261870866\n",
      "Epoch 1 step 227: training accuarcy: 0.996\n",
      "Epoch 1 step 227: training loss: 20.13862311016426\n",
      "Epoch 1 step 228: training accuarcy: 0.9975\n",
      "Epoch 1 step 228: training loss: 22.483662513160503\n",
      "Epoch 1 step 229: training accuarcy: 0.997\n",
      "Epoch 1 step 229: training loss: 20.768461595033223\n",
      "Epoch 1 step 230: training accuarcy: 0.9979035639412999\n",
      "Epoch 1: train loss 43.8759716510895, train accuarcy 0.9958972930908203\n",
      "Epoch 1: valid loss 19.207010098632193, valid accuarcy 0.9967576265335083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████▎                                                                                                                  | 2/8 [01:48<05:24, 54.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 230: training loss: 23.696668504157245\n",
      "Epoch 2 step 231: training accuarcy: 0.996\n",
      "Epoch 2 step 231: training loss: 16.01251041171815\n",
      "Epoch 2 step 232: training accuarcy: 0.998\n",
      "Epoch 2 step 232: training loss: 26.5951790573777\n",
      "Epoch 2 step 233: training accuarcy: 0.9955\n",
      "Epoch 2 step 233: training loss: 21.912780447681346\n",
      "Epoch 2 step 234: training accuarcy: 0.997\n",
      "Epoch 2 step 234: training loss: 21.183639375905738\n",
      "Epoch 2 step 235: training accuarcy: 0.9975\n",
      "Epoch 2 step 235: training loss: 20.11676697159342\n",
      "Epoch 2 step 236: training accuarcy: 0.997\n",
      "Epoch 2 step 236: training loss: 16.99453036072121\n",
      "Epoch 2 step 237: training accuarcy: 0.9985\n",
      "Epoch 2 step 237: training loss: 26.19903958947762\n",
      "Epoch 2 step 238: training accuarcy: 0.9945\n",
      "Epoch 2 step 238: training loss: 29.23084383604337\n",
      "Epoch 2 step 239: training accuarcy: 0.9935\n",
      "Epoch 2 step 239: training loss: 21.79047052358284\n",
      "Epoch 2 step 240: training accuarcy: 0.996\n",
      "Epoch 2 step 240: training loss: 22.763243784627267\n",
      "Epoch 2 step 241: training accuarcy: 0.9955\n",
      "Epoch 2 step 241: training loss: 26.524423553407825\n",
      "Epoch 2 step 242: training accuarcy: 0.9955\n",
      "Epoch 2 step 242: training loss: 25.98020658777883\n",
      "Epoch 2 step 243: training accuarcy: 0.9955\n",
      "Epoch 2 step 243: training loss: 20.071853043819587\n",
      "Epoch 2 step 244: training accuarcy: 0.9975\n",
      "Epoch 2 step 244: training loss: 20.226466585323795\n",
      "Epoch 2 step 245: training accuarcy: 0.998\n",
      "Epoch 2 step 245: training loss: 25.498917165927708\n",
      "Epoch 2 step 246: training accuarcy: 0.995\n",
      "Epoch 2 step 246: training loss: 22.689941087749265\n",
      "Epoch 2 step 247: training accuarcy: 0.996\n",
      "Epoch 2 step 247: training loss: 23.39969207614385\n",
      "Epoch 2 step 248: training accuarcy: 0.9955\n",
      "Epoch 2 step 248: training loss: 20.01046124330818\n",
      "Epoch 2 step 249: training accuarcy: 0.9975\n",
      "Epoch 2 step 249: training loss: 19.669075173531287\n",
      "Epoch 2 step 250: training accuarcy: 0.998\n",
      "Epoch 2 step 250: training loss: 17.982167278231714\n",
      "Epoch 2 step 251: training accuarcy: 0.997\n",
      "Epoch 2 step 251: training loss: 23.77718325198607\n",
      "Epoch 2 step 252: training accuarcy: 0.9965\n",
      "Epoch 2 step 252: training loss: 27.290793262044662\n",
      "Epoch 2 step 253: training accuarcy: 0.9955\n",
      "Epoch 2 step 253: training loss: 24.03098381216556\n",
      "Epoch 2 step 254: training accuarcy: 0.9965\n",
      "Epoch 2 step 254: training loss: 19.706586754779345\n",
      "Epoch 2 step 255: training accuarcy: 0.997\n",
      "Epoch 2 step 255: training loss: 19.198226558848347\n",
      "Epoch 2 step 256: training accuarcy: 0.998\n",
      "Epoch 2 step 256: training loss: 19.077431937032898\n",
      "Epoch 2 step 257: training accuarcy: 0.9965\n",
      "Epoch 2 step 257: training loss: 21.79620867278845\n",
      "Epoch 2 step 258: training accuarcy: 0.9965\n",
      "Epoch 2 step 258: training loss: 21.572353135725184\n",
      "Epoch 2 step 259: training accuarcy: 0.996\n",
      "Epoch 2 step 259: training loss: 25.046022300996945\n",
      "Epoch 2 step 260: training accuarcy: 0.9935\n",
      "Epoch 2 step 260: training loss: 21.29557596666948\n",
      "Epoch 2 step 261: training accuarcy: 0.997\n",
      "Epoch 2 step 261: training loss: 26.388459201657287\n",
      "Epoch 2 step 262: training accuarcy: 0.994\n",
      "Epoch 2 step 262: training loss: 25.77561506065743\n",
      "Epoch 2 step 263: training accuarcy: 0.9935\n",
      "Epoch 2 step 263: training loss: 22.836384507124876\n",
      "Epoch 2 step 264: training accuarcy: 0.997\n",
      "Epoch 2 step 264: training loss: 23.384808302150812\n",
      "Epoch 2 step 265: training accuarcy: 0.995\n",
      "Epoch 2 step 265: training loss: 14.380981591715022\n",
      "Epoch 2 step 266: training accuarcy: 0.998\n",
      "Epoch 2 step 266: training loss: 16.86382259899971\n",
      "Epoch 2 step 267: training accuarcy: 0.9975\n",
      "Epoch 2 step 267: training loss: 21.647472979128906\n",
      "Epoch 2 step 268: training accuarcy: 0.9965\n",
      "Epoch 2 step 268: training loss: 18.634391307245995\n",
      "Epoch 2 step 269: training accuarcy: 0.997\n",
      "Epoch 2 step 269: training loss: 23.36187342749567\n",
      "Epoch 2 step 270: training accuarcy: 0.9965\n",
      "Epoch 2 step 270: training loss: 22.029550479190494\n",
      "Epoch 2 step 271: training accuarcy: 0.997\n",
      "Epoch 2 step 271: training loss: 19.19524601613887\n",
      "Epoch 2 step 272: training accuarcy: 0.9965\n",
      "Epoch 2 step 272: training loss: 19.2582167082237\n",
      "Epoch 2 step 273: training accuarcy: 0.9965\n",
      "Epoch 2 step 273: training loss: 16.03680063825977\n",
      "Epoch 2 step 274: training accuarcy: 0.998\n",
      "Epoch 2 step 274: training loss: 21.50694981737091\n",
      "Epoch 2 step 275: training accuarcy: 0.9965\n",
      "Epoch 2 step 275: training loss: 18.265756599159598\n",
      "Epoch 2 step 276: training accuarcy: 0.9965\n",
      "Epoch 2 step 276: training loss: 14.18110634177567\n",
      "Epoch 2 step 277: training accuarcy: 0.999\n",
      "Epoch 2 step 277: training loss: 19.588694215742958\n",
      "Epoch 2 step 278: training accuarcy: 0.9955\n",
      "Epoch 2 step 278: training loss: 19.22835696860568\n",
      "Epoch 2 step 279: training accuarcy: 0.996\n",
      "Epoch 2 step 279: training loss: 16.74906077658519\n",
      "Epoch 2 step 280: training accuarcy: 0.9975\n",
      "Epoch 2 step 280: training loss: 21.039794494798645\n",
      "Epoch 2 step 281: training accuarcy: 0.996\n",
      "Epoch 2 step 281: training loss: 14.649650346763405\n",
      "Epoch 2 step 282: training accuarcy: 0.999\n",
      "Epoch 2 step 282: training loss: 21.0957402932169\n",
      "Epoch 2 step 283: training accuarcy: 0.996\n",
      "Epoch 2 step 283: training loss: 20.85736158624427\n",
      "Epoch 2 step 284: training accuarcy: 0.9955\n",
      "Epoch 2 step 284: training loss: 15.070264067036895\n",
      "Epoch 2 step 285: training accuarcy: 0.9985\n",
      "Epoch 2 step 285: training loss: 17.81947025216424\n",
      "Epoch 2 step 286: training accuarcy: 0.997\n",
      "Epoch 2 step 286: training loss: 20.947052555940154\n",
      "Epoch 2 step 287: training accuarcy: 0.9975\n",
      "Epoch 2 step 287: training loss: 21.843311354571668\n",
      "Epoch 2 step 288: training accuarcy: 0.9955\n",
      "Epoch 2 step 288: training loss: 22.811034342986503\n",
      "Epoch 2 step 289: training accuarcy: 0.9965\n",
      "Epoch 2 step 289: training loss: 16.01529215414925\n",
      "Epoch 2 step 290: training accuarcy: 0.9985\n",
      "Epoch 2 step 290: training loss: 23.616522860450097\n",
      "Epoch 2 step 291: training accuarcy: 0.995\n",
      "Epoch 2 step 291: training loss: 17.768571005988207\n",
      "Epoch 2 step 292: training accuarcy: 0.995\n",
      "Epoch 2 step 292: training loss: 20.66979141487732\n",
      "Epoch 2 step 293: training accuarcy: 0.9965\n",
      "Epoch 2 step 293: training loss: 12.959115804994905\n",
      "Epoch 2 step 294: training accuarcy: 0.998\n",
      "Epoch 2 step 294: training loss: 17.663365337711767\n",
      "Epoch 2 step 295: training accuarcy: 0.997\n",
      "Epoch 2 step 295: training loss: 20.075619413289015\n",
      "Epoch 2 step 296: training accuarcy: 0.996\n",
      "Epoch 2 step 296: training loss: 24.266105503806067\n",
      "Epoch 2 step 297: training accuarcy: 0.995\n",
      "Epoch 2 step 297: training loss: 10.439569087210302\n",
      "Epoch 2 step 298: training accuarcy: 0.999\n",
      "Epoch 2 step 298: training loss: 17.241759064003425\n",
      "Epoch 2 step 299: training accuarcy: 0.997\n",
      "Epoch 2 step 299: training loss: 19.00489103562442\n",
      "Epoch 2 step 300: training accuarcy: 0.9965\n",
      "Epoch 2 step 300: training loss: 18.80427347689775\n",
      "Epoch 2 step 301: training accuarcy: 0.9975\n",
      "Epoch 2 step 301: training loss: 16.00463826494366\n",
      "Epoch 2 step 302: training accuarcy: 0.997\n",
      "Epoch 2 step 302: training loss: 13.427566838260606\n",
      "Epoch 2 step 303: training accuarcy: 0.9985\n",
      "Epoch 2 step 303: training loss: 19.71848124027315\n",
      "Epoch 2 step 304: training accuarcy: 0.9955\n",
      "Epoch 2 step 304: training loss: 16.97303446270338\n",
      "Epoch 2 step 305: training accuarcy: 0.9965\n",
      "Epoch 2 step 305: training loss: 16.49178325499586\n",
      "Epoch 2 step 306: training accuarcy: 0.9975\n",
      "Epoch 2 step 306: training loss: 19.97469066299765\n",
      "Epoch 2 step 307: training accuarcy: 0.9965\n",
      "Epoch 2 step 307: training loss: 21.137612110701063\n",
      "Epoch 2 step 308: training accuarcy: 0.9965\n",
      "Epoch 2 step 308: training loss: 14.557685686520482\n",
      "Epoch 2 step 309: training accuarcy: 0.9965\n",
      "Epoch 2 step 309: training loss: 19.601168763966196\n",
      "Epoch 2 step 310: training accuarcy: 0.9955\n",
      "Epoch 2 step 310: training loss: 16.89565606490013\n",
      "Epoch 2 step 311: training accuarcy: 0.9965\n",
      "Epoch 2 step 311: training loss: 27.701756116786193\n",
      "Epoch 2 step 312: training accuarcy: 0.9935\n",
      "Epoch 2 step 312: training loss: 21.463966608430585\n",
      "Epoch 2 step 313: training accuarcy: 0.9955\n",
      "Epoch 2 step 313: training loss: 13.4127247350905\n",
      "Epoch 2 step 314: training accuarcy: 0.9975\n",
      "Epoch 2 step 314: training loss: 18.86013021224635\n",
      "Epoch 2 step 315: training accuarcy: 0.9965\n",
      "Epoch 2 step 315: training loss: 16.62914742455057\n",
      "Epoch 2 step 316: training accuarcy: 0.9985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 316: training loss: 17.564355024726385\n",
      "Epoch 2 step 317: training accuarcy: 0.9965\n",
      "Epoch 2 step 317: training loss: 15.781898837689436\n",
      "Epoch 2 step 318: training accuarcy: 0.998\n",
      "Epoch 2 step 318: training loss: 13.061112121113469\n",
      "Epoch 2 step 319: training accuarcy: 0.9985\n",
      "Epoch 2 step 319: training loss: 13.51125755391138\n",
      "Epoch 2 step 320: training accuarcy: 0.9995\n",
      "Epoch 2 step 320: training loss: 13.189374778880525\n",
      "Epoch 2 step 321: training accuarcy: 0.998\n",
      "Epoch 2 step 321: training loss: 17.53214680866388\n",
      "Epoch 2 step 322: training accuarcy: 0.9955\n",
      "Epoch 2 step 322: training loss: 20.003611710108245\n",
      "Epoch 2 step 323: training accuarcy: 0.997\n",
      "Epoch 2 step 323: training loss: 14.419862343372825\n",
      "Epoch 2 step 324: training accuarcy: 0.9985\n",
      "Epoch 2 step 324: training loss: 20.52028249668736\n",
      "Epoch 2 step 325: training accuarcy: 0.995\n",
      "Epoch 2 step 325: training loss: 20.66502917385579\n",
      "Epoch 2 step 326: training accuarcy: 0.9955\n",
      "Epoch 2 step 326: training loss: 16.822440156408817\n",
      "Epoch 2 step 327: training accuarcy: 0.9965\n",
      "Epoch 2 step 327: training loss: 22.227598987095057\n",
      "Epoch 2 step 328: training accuarcy: 0.9945\n",
      "Epoch 2 step 328: training loss: 18.320294679977195\n",
      "Epoch 2 step 329: training accuarcy: 0.998\n",
      "Epoch 2 step 329: training loss: 18.446544442199123\n",
      "Epoch 2 step 330: training accuarcy: 0.9965\n",
      "Epoch 2 step 330: training loss: 17.43804590834765\n",
      "Epoch 2 step 331: training accuarcy: 0.9965\n",
      "Epoch 2 step 331: training loss: 14.116241262508394\n",
      "Epoch 2 step 332: training accuarcy: 0.998\n",
      "Epoch 2 step 332: training loss: 21.000821887077727\n",
      "Epoch 2 step 333: training accuarcy: 0.9955\n",
      "Epoch 2 step 333: training loss: 15.970635670910394\n",
      "Epoch 2 step 334: training accuarcy: 0.997\n",
      "Epoch 2 step 334: training loss: 21.983519972077428\n",
      "Epoch 2 step 335: training accuarcy: 0.996\n",
      "Epoch 2 step 335: training loss: 13.451056447056764\n",
      "Epoch 2 step 336: training accuarcy: 0.9985\n",
      "Epoch 2 step 336: training loss: 12.597330562015586\n",
      "Epoch 2 step 337: training accuarcy: 0.9985\n",
      "Epoch 2 step 337: training loss: 16.788804313703327\n",
      "Epoch 2 step 338: training accuarcy: 0.9975\n",
      "Epoch 2 step 338: training loss: 14.332148584347955\n",
      "Epoch 2 step 339: training accuarcy: 0.998\n",
      "Epoch 2 step 339: training loss: 17.800319830914304\n",
      "Epoch 2 step 340: training accuarcy: 0.9975\n",
      "Epoch 2 step 340: training loss: 15.397906467778109\n",
      "Epoch 2 step 341: training accuarcy: 0.998\n",
      "Epoch 2 step 341: training loss: 15.154579081656772\n",
      "Epoch 2 step 342: training accuarcy: 0.9975\n",
      "Epoch 2 step 342: training loss: 14.35928698303863\n",
      "Epoch 2 step 343: training accuarcy: 0.9965\n",
      "Epoch 2 step 343: training loss: 20.25662659478092\n",
      "Epoch 2 step 344: training accuarcy: 0.996\n",
      "Epoch 2 step 344: training loss: 15.851814472070462\n",
      "Epoch 2 step 345: training accuarcy: 0.9968553459119497\n",
      "Epoch 2: train loss 19.362846390447288, train accuarcy 0.9959636926651001\n",
      "Epoch 2: valid loss 15.095700040391147, valid accuarcy 0.9967940449714661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████▍                                                                                               | 3/8 [02:42<04:30, 54.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 345: training loss: 22.957445217862325\n",
      "Epoch 3 step 346: training accuarcy: 0.9965\n",
      "Epoch 3 step 346: training loss: 15.333211023165891\n",
      "Epoch 3 step 347: training accuarcy: 0.997\n",
      "Epoch 3 step 347: training loss: 18.339112317332983\n",
      "Epoch 3 step 348: training accuarcy: 0.9955\n",
      "Epoch 3 step 348: training loss: 18.60946384432682\n",
      "Epoch 3 step 349: training accuarcy: 0.9965\n",
      "Epoch 3 step 349: training loss: 15.857088066065002\n",
      "Epoch 3 step 350: training accuarcy: 0.997\n",
      "Epoch 3 step 350: training loss: 10.649299193211723\n",
      "Epoch 3 step 351: training accuarcy: 0.998\n",
      "Epoch 3 step 351: training loss: 13.240375579864569\n",
      "Epoch 3 step 352: training accuarcy: 0.998\n",
      "Epoch 3 step 352: training loss: 13.211386367132787\n",
      "Epoch 3 step 353: training accuarcy: 0.998\n",
      "Epoch 3 step 353: training loss: 14.444709297678244\n",
      "Epoch 3 step 354: training accuarcy: 0.997\n",
      "Epoch 3 step 354: training loss: 18.518381252044602\n",
      "Epoch 3 step 355: training accuarcy: 0.9985\n",
      "Epoch 3 step 355: training loss: 16.781256758960033\n",
      "Epoch 3 step 356: training accuarcy: 0.9955\n",
      "Epoch 3 step 356: training loss: 9.045586297027082\n",
      "Epoch 3 step 357: training accuarcy: 0.9995\n",
      "Epoch 3 step 357: training loss: 15.6463359324174\n",
      "Epoch 3 step 358: training accuarcy: 0.998\n",
      "Epoch 3 step 358: training loss: 15.144690298336156\n",
      "Epoch 3 step 359: training accuarcy: 0.996\n",
      "Epoch 3 step 359: training loss: 20.11799593569471\n",
      "Epoch 3 step 360: training accuarcy: 0.996\n",
      "Epoch 3 step 360: training loss: 13.481506387770764\n",
      "Epoch 3 step 361: training accuarcy: 0.998\n",
      "Epoch 3 step 361: training loss: 13.15060846596961\n",
      "Epoch 3 step 362: training accuarcy: 0.9975\n",
      "Epoch 3 step 362: training loss: 18.862182956026196\n",
      "Epoch 3 step 363: training accuarcy: 0.9975\n",
      "Epoch 3 step 363: training loss: 15.994957419647353\n",
      "Epoch 3 step 364: training accuarcy: 0.9975\n",
      "Epoch 3 step 364: training loss: 15.725292721279184\n",
      "Epoch 3 step 365: training accuarcy: 0.998\n",
      "Epoch 3 step 365: training loss: 15.396640507956397\n",
      "Epoch 3 step 366: training accuarcy: 0.997\n",
      "Epoch 3 step 366: training loss: 18.912959967489048\n",
      "Epoch 3 step 367: training accuarcy: 0.9955\n",
      "Epoch 3 step 367: training loss: 15.49887473804577\n",
      "Epoch 3 step 368: training accuarcy: 0.9975\n",
      "Epoch 3 step 368: training loss: 13.20224805450249\n",
      "Epoch 3 step 369: training accuarcy: 0.9975\n",
      "Epoch 3 step 369: training loss: 14.285387332371798\n",
      "Epoch 3 step 370: training accuarcy: 0.9975\n",
      "Epoch 3 step 370: training loss: 14.374716257503685\n",
      "Epoch 3 step 371: training accuarcy: 0.998\n",
      "Epoch 3 step 371: training loss: 15.530075207899564\n",
      "Epoch 3 step 372: training accuarcy: 0.9975\n",
      "Epoch 3 step 372: training loss: 12.832274763614393\n",
      "Epoch 3 step 373: training accuarcy: 0.9975\n",
      "Epoch 3 step 373: training loss: 22.267251458438654\n",
      "Epoch 3 step 374: training accuarcy: 0.995\n",
      "Epoch 3 step 374: training loss: 19.616026478232335\n",
      "Epoch 3 step 375: training accuarcy: 0.997\n",
      "Epoch 3 step 375: training loss: 23.61940530538779\n",
      "Epoch 3 step 376: training accuarcy: 0.995\n",
      "Epoch 3 step 376: training loss: 12.359346173586799\n",
      "Epoch 3 step 377: training accuarcy: 0.999\n",
      "Epoch 3 step 377: training loss: 18.48179616124442\n",
      "Epoch 3 step 378: training accuarcy: 0.997\n",
      "Epoch 3 step 378: training loss: 14.863242830536958\n",
      "Epoch 3 step 379: training accuarcy: 0.996\n",
      "Epoch 3 step 379: training loss: 13.647919453205805\n",
      "Epoch 3 step 380: training accuarcy: 0.997\n",
      "Epoch 3 step 380: training loss: 14.174963324136709\n",
      "Epoch 3 step 381: training accuarcy: 0.998\n",
      "Epoch 3 step 381: training loss: 10.428939865689909\n",
      "Epoch 3 step 382: training accuarcy: 0.9985\n",
      "Epoch 3 step 382: training loss: 17.713452236673678\n",
      "Epoch 3 step 383: training accuarcy: 0.9985\n",
      "Epoch 3 step 383: training loss: 20.858761680043102\n",
      "Epoch 3 step 384: training accuarcy: 0.994\n",
      "Epoch 3 step 384: training loss: 12.565161160680539\n",
      "Epoch 3 step 385: training accuarcy: 0.999\n",
      "Epoch 3 step 385: training loss: 16.729629615690776\n",
      "Epoch 3 step 386: training accuarcy: 0.9965\n",
      "Epoch 3 step 386: training loss: 16.731904005162157\n",
      "Epoch 3 step 387: training accuarcy: 0.997\n",
      "Epoch 3 step 387: training loss: 19.443426226904077\n",
      "Epoch 3 step 388: training accuarcy: 0.9955\n",
      "Epoch 3 step 388: training loss: 11.08544772679388\n",
      "Epoch 3 step 389: training accuarcy: 0.999\n",
      "Epoch 3 step 389: training loss: 19.472548042099938\n",
      "Epoch 3 step 390: training accuarcy: 0.9965\n",
      "Epoch 3 step 390: training loss: 15.985558324616328\n",
      "Epoch 3 step 391: training accuarcy: 0.998\n",
      "Epoch 3 step 391: training loss: 14.902961056237258\n",
      "Epoch 3 step 392: training accuarcy: 0.9975\n",
      "Epoch 3 step 392: training loss: 11.8123592543013\n",
      "Epoch 3 step 393: training accuarcy: 0.998\n",
      "Epoch 3 step 393: training loss: 22.518096040742645\n",
      "Epoch 3 step 394: training accuarcy: 0.995\n",
      "Epoch 3 step 394: training loss: 13.609139296923228\n",
      "Epoch 3 step 395: training accuarcy: 0.999\n",
      "Epoch 3 step 395: training loss: 13.348826152748666\n",
      "Epoch 3 step 396: training accuarcy: 0.997\n",
      "Epoch 3 step 396: training loss: 14.060462455202655\n",
      "Epoch 3 step 397: training accuarcy: 0.9975\n",
      "Epoch 3 step 397: training loss: 17.89065625146072\n",
      "Epoch 3 step 398: training accuarcy: 0.997\n",
      "Epoch 3 step 398: training loss: 16.734103590380077\n",
      "Epoch 3 step 399: training accuarcy: 0.996\n",
      "Epoch 3 step 399: training loss: 10.725385558338168\n",
      "Epoch 3 step 400: training accuarcy: 0.998\n",
      "Epoch 3 step 400: training loss: 8.955602765940688\n",
      "Epoch 3 step 401: training accuarcy: 0.9985\n",
      "Epoch 3 step 401: training loss: 18.31884741802211\n",
      "Epoch 3 step 402: training accuarcy: 0.9965\n",
      "Epoch 3 step 402: training loss: 15.688905787904808\n",
      "Epoch 3 step 403: training accuarcy: 0.9965\n",
      "Epoch 3 step 403: training loss: 15.216305560645164\n",
      "Epoch 3 step 404: training accuarcy: 0.9975\n",
      "Epoch 3 step 404: training loss: 16.057991205319386\n",
      "Epoch 3 step 405: training accuarcy: 0.997\n",
      "Epoch 3 step 405: training loss: 20.080482553311494\n",
      "Epoch 3 step 406: training accuarcy: 0.9955\n",
      "Epoch 3 step 406: training loss: 10.907390395290586\n",
      "Epoch 3 step 407: training accuarcy: 0.9985\n",
      "Epoch 3 step 407: training loss: 16.235108076096978\n",
      "Epoch 3 step 408: training accuarcy: 0.9975\n",
      "Epoch 3 step 408: training loss: 14.915048073593395\n",
      "Epoch 3 step 409: training accuarcy: 0.997\n",
      "Epoch 3 step 409: training loss: 17.009708145020216\n",
      "Epoch 3 step 410: training accuarcy: 0.996\n",
      "Epoch 3 step 410: training loss: 25.592356380132323\n",
      "Epoch 3 step 411: training accuarcy: 0.994\n",
      "Epoch 3 step 411: training loss: 14.388013587045242\n",
      "Epoch 3 step 412: training accuarcy: 0.998\n",
      "Epoch 3 step 412: training loss: 14.12850057044566\n",
      "Epoch 3 step 413: training accuarcy: 0.998\n",
      "Epoch 3 step 413: training loss: 14.718348455641273\n",
      "Epoch 3 step 414: training accuarcy: 0.998\n",
      "Epoch 3 step 414: training loss: 16.9731980556482\n",
      "Epoch 3 step 415: training accuarcy: 0.997\n",
      "Epoch 3 step 415: training loss: 9.936008301640024\n",
      "Epoch 3 step 416: training accuarcy: 0.999\n",
      "Epoch 3 step 416: training loss: 16.539586265729213\n",
      "Epoch 3 step 417: training accuarcy: 0.996\n",
      "Epoch 3 step 417: training loss: 16.26614799963325\n",
      "Epoch 3 step 418: training accuarcy: 0.996\n",
      "Epoch 3 step 418: training loss: 13.392860405148065\n",
      "Epoch 3 step 419: training accuarcy: 0.9965\n",
      "Epoch 3 step 419: training loss: 18.914919447944644\n",
      "Epoch 3 step 420: training accuarcy: 0.9945\n",
      "Epoch 3 step 420: training loss: 11.5458192488569\n",
      "Epoch 3 step 421: training accuarcy: 0.9985\n",
      "Epoch 3 step 421: training loss: 13.54044681787189\n",
      "Epoch 3 step 422: training accuarcy: 0.998\n",
      "Epoch 3 step 422: training loss: 12.548023779632453\n",
      "Epoch 3 step 423: training accuarcy: 0.999\n",
      "Epoch 3 step 423: training loss: 13.87571495217975\n",
      "Epoch 3 step 424: training accuarcy: 0.997\n",
      "Epoch 3 step 424: training loss: 10.734850216157417\n",
      "Epoch 3 step 425: training accuarcy: 0.9985\n",
      "Epoch 3 step 425: training loss: 17.023237490571418\n",
      "Epoch 3 step 426: training accuarcy: 0.996\n",
      "Epoch 3 step 426: training loss: 14.876942073945967\n",
      "Epoch 3 step 427: training accuarcy: 0.9975\n",
      "Epoch 3 step 427: training loss: 17.492567264488287\n",
      "Epoch 3 step 428: training accuarcy: 0.996\n",
      "Epoch 3 step 428: training loss: 20.536632951155656\n",
      "Epoch 3 step 429: training accuarcy: 0.996\n",
      "Epoch 3 step 429: training loss: 13.937975664630024\n",
      "Epoch 3 step 430: training accuarcy: 0.9975\n",
      "Epoch 3 step 430: training loss: 15.744313422107869\n",
      "Epoch 3 step 431: training accuarcy: 0.9975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 431: training loss: 18.471679663611994\n",
      "Epoch 3 step 432: training accuarcy: 0.9955\n",
      "Epoch 3 step 432: training loss: 13.902432050858323\n",
      "Epoch 3 step 433: training accuarcy: 0.997\n",
      "Epoch 3 step 433: training loss: 15.36232814972551\n",
      "Epoch 3 step 434: training accuarcy: 0.997\n",
      "Epoch 3 step 434: training loss: 19.8493471590699\n",
      "Epoch 3 step 435: training accuarcy: 0.9965\n",
      "Epoch 3 step 435: training loss: 19.88166707452678\n",
      "Epoch 3 step 436: training accuarcy: 0.9945\n",
      "Epoch 3 step 436: training loss: 22.450678243303667\n",
      "Epoch 3 step 437: training accuarcy: 0.9945\n",
      "Epoch 3 step 437: training loss: 15.729347025802378\n",
      "Epoch 3 step 438: training accuarcy: 0.996\n",
      "Epoch 3 step 438: training loss: 16.50592281861075\n",
      "Epoch 3 step 439: training accuarcy: 0.9965\n",
      "Epoch 3 step 439: training loss: 21.19923625162519\n",
      "Epoch 3 step 440: training accuarcy: 0.996\n",
      "Epoch 3 step 440: training loss: 12.177797676011838\n",
      "Epoch 3 step 441: training accuarcy: 0.997\n",
      "Epoch 3 step 441: training loss: 15.476896954496812\n",
      "Epoch 3 step 442: training accuarcy: 0.9975\n",
      "Epoch 3 step 442: training loss: 14.838764950856152\n",
      "Epoch 3 step 443: training accuarcy: 0.9965\n",
      "Epoch 3 step 443: training loss: 12.626674436519364\n",
      "Epoch 3 step 444: training accuarcy: 0.9985\n",
      "Epoch 3 step 444: training loss: 19.85243092596899\n",
      "Epoch 3 step 445: training accuarcy: 0.997\n",
      "Epoch 3 step 445: training loss: 17.099441556171474\n",
      "Epoch 3 step 446: training accuarcy: 0.9965\n",
      "Epoch 3 step 446: training loss: 12.260620469533722\n",
      "Epoch 3 step 447: training accuarcy: 0.9985\n",
      "Epoch 3 step 447: training loss: 17.900771394952294\n",
      "Epoch 3 step 448: training accuarcy: 0.996\n",
      "Epoch 3 step 448: training loss: 9.830390088092628\n",
      "Epoch 3 step 449: training accuarcy: 0.9975\n",
      "Epoch 3 step 449: training loss: 15.166874032415933\n",
      "Epoch 3 step 450: training accuarcy: 0.997\n",
      "Epoch 3 step 450: training loss: 13.461845951360104\n",
      "Epoch 3 step 451: training accuarcy: 0.9965\n",
      "Epoch 3 step 451: training loss: 9.043668750752989\n",
      "Epoch 3 step 452: training accuarcy: 0.999\n",
      "Epoch 3 step 452: training loss: 16.369799964046585\n",
      "Epoch 3 step 453: training accuarcy: 0.9975\n",
      "Epoch 3 step 453: training loss: 13.551109741877596\n",
      "Epoch 3 step 454: training accuarcy: 0.9975\n",
      "Epoch 3 step 454: training loss: 12.691017262662003\n",
      "Epoch 3 step 455: training accuarcy: 0.998\n",
      "Epoch 3 step 455: training loss: 7.43555112405243\n",
      "Epoch 3 step 456: training accuarcy: 0.999\n",
      "Epoch 3 step 456: training loss: 11.473409536311527\n",
      "Epoch 3 step 457: training accuarcy: 0.9975\n",
      "Epoch 3 step 457: training loss: 15.333829564713854\n",
      "Epoch 3 step 458: training accuarcy: 0.997\n",
      "Epoch 3 step 458: training loss: 13.663925309409475\n",
      "Epoch 3 step 459: training accuarcy: 0.999\n",
      "Epoch 3 step 459: training loss: 19.2017015400891\n",
      "Epoch 3 step 460: training accuarcy: 0.9937106918238994\n",
      "Epoch 3: train loss 15.562537781615944, train accuarcy 0.9962733387947083\n",
      "Epoch 3: valid loss 12.367367277585576, valid accuarcy 0.997559130191803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 4/8 [03:36<03:36, 54.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 460: training loss: 10.450345995625021\n",
      "Epoch 4 step 461: training accuarcy: 0.999\n",
      "Epoch 4 step 461: training loss: 11.66767661944162\n",
      "Epoch 4 step 462: training accuarcy: 0.998\n",
      "Epoch 4 step 462: training loss: 12.28997468312343\n",
      "Epoch 4 step 463: training accuarcy: 0.9975\n",
      "Epoch 4 step 463: training loss: 22.64343817790436\n",
      "Epoch 4 step 464: training accuarcy: 0.9945\n",
      "Epoch 4 step 464: training loss: 14.033151677612175\n",
      "Epoch 4 step 465: training accuarcy: 0.9975\n",
      "Epoch 4 step 465: training loss: 17.135309632435636\n",
      "Epoch 4 step 466: training accuarcy: 0.9965\n",
      "Epoch 4 step 466: training loss: 12.324289428796831\n",
      "Epoch 4 step 467: training accuarcy: 0.9975\n",
      "Epoch 4 step 467: training loss: 13.394186597488726\n",
      "Epoch 4 step 468: training accuarcy: 0.9975\n",
      "Epoch 4 step 468: training loss: 16.345894170673056\n",
      "Epoch 4 step 469: training accuarcy: 0.997\n",
      "Epoch 4 step 469: training loss: 13.879221847536742\n",
      "Epoch 4 step 470: training accuarcy: 0.9975\n",
      "Epoch 4 step 470: training loss: 12.156325136634981\n",
      "Epoch 4 step 471: training accuarcy: 0.9985\n",
      "Epoch 4 step 471: training loss: 16.762862956390997\n",
      "Epoch 4 step 472: training accuarcy: 0.995\n",
      "Epoch 4 step 472: training loss: 12.939430317721603\n",
      "Epoch 4 step 473: training accuarcy: 0.998\n",
      "Epoch 4 step 473: training loss: 14.769397325323261\n",
      "Epoch 4 step 474: training accuarcy: 0.996\n",
      "Epoch 4 step 474: training loss: 14.182763351477\n",
      "Epoch 4 step 475: training accuarcy: 0.9965\n",
      "Epoch 4 step 475: training loss: 19.0114576611604\n",
      "Epoch 4 step 476: training accuarcy: 0.997\n",
      "Epoch 4 step 476: training loss: 17.289969495630405\n",
      "Epoch 4 step 477: training accuarcy: 0.997\n",
      "Epoch 4 step 477: training loss: 17.068625792142836\n",
      "Epoch 4 step 478: training accuarcy: 0.997\n",
      "Epoch 4 step 478: training loss: 12.481907397758208\n",
      "Epoch 4 step 479: training accuarcy: 0.998\n",
      "Epoch 4 step 479: training loss: 13.428210185438319\n",
      "Epoch 4 step 480: training accuarcy: 0.997\n",
      "Epoch 4 step 480: training loss: 15.134033561075181\n",
      "Epoch 4 step 481: training accuarcy: 0.996\n",
      "Epoch 4 step 481: training loss: 17.126986355859614\n",
      "Epoch 4 step 482: training accuarcy: 0.9965\n",
      "Epoch 4 step 482: training loss: 18.62047342801308\n",
      "Epoch 4 step 483: training accuarcy: 0.996\n",
      "Epoch 4 step 483: training loss: 11.30042502451578\n",
      "Epoch 4 step 484: training accuarcy: 0.998\n",
      "Epoch 4 step 484: training loss: 6.6427275763755285\n",
      "Epoch 4 step 485: training accuarcy: 0.999\n",
      "Epoch 4 step 485: training loss: 17.531984824070673\n",
      "Epoch 4 step 486: training accuarcy: 0.997\n",
      "Epoch 4 step 486: training loss: 16.82509649794716\n",
      "Epoch 4 step 487: training accuarcy: 0.996\n",
      "Epoch 4 step 487: training loss: 12.947028112181272\n",
      "Epoch 4 step 488: training accuarcy: 0.9975\n",
      "Epoch 4 step 488: training loss: 18.91371114333775\n",
      "Epoch 4 step 489: training accuarcy: 0.995\n",
      "Epoch 4 step 489: training loss: 18.121102880110616\n",
      "Epoch 4 step 490: training accuarcy: 0.996\n",
      "Epoch 4 step 490: training loss: 12.226008775261626\n",
      "Epoch 4 step 491: training accuarcy: 0.9975\n",
      "Epoch 4 step 491: training loss: 12.288428007560785\n",
      "Epoch 4 step 492: training accuarcy: 0.998\n",
      "Epoch 4 step 492: training loss: 15.159846706206826\n",
      "Epoch 4 step 493: training accuarcy: 0.9965\n",
      "Epoch 4 step 493: training loss: 12.726339063970887\n",
      "Epoch 4 step 494: training accuarcy: 0.999\n",
      "Epoch 4 step 494: training loss: 15.581162365107685\n",
      "Epoch 4 step 495: training accuarcy: 0.9965\n",
      "Epoch 4 step 495: training loss: 13.668138783437927\n",
      "Epoch 4 step 496: training accuarcy: 0.9985\n",
      "Epoch 4 step 496: training loss: 13.772815719452343\n",
      "Epoch 4 step 497: training accuarcy: 0.997\n",
      "Epoch 4 step 497: training loss: 26.900875290424374\n",
      "Epoch 4 step 498: training accuarcy: 0.992\n",
      "Epoch 4 step 498: training loss: 20.13764133782776\n",
      "Epoch 4 step 499: training accuarcy: 0.9945\n",
      "Epoch 4 step 499: training loss: 11.963256924171572\n",
      "Epoch 4 step 500: training accuarcy: 0.997\n",
      "Epoch 4 step 500: training loss: 14.125637590313747\n",
      "Epoch 4 step 501: training accuarcy: 0.9975\n",
      "Epoch 4 step 501: training loss: 13.91466240681046\n",
      "Epoch 4 step 502: training accuarcy: 0.9965\n",
      "Epoch 4 step 502: training loss: 16.77904261837769\n",
      "Epoch 4 step 503: training accuarcy: 0.996\n",
      "Epoch 4 step 503: training loss: 13.241293475563209\n",
      "Epoch 4 step 504: training accuarcy: 0.9975\n",
      "Epoch 4 step 504: training loss: 14.850446173778511\n",
      "Epoch 4 step 505: training accuarcy: 0.997\n",
      "Epoch 4 step 505: training loss: 10.499041971486308\n",
      "Epoch 4 step 506: training accuarcy: 0.997\n",
      "Epoch 4 step 506: training loss: 16.601840403644427\n",
      "Epoch 4 step 507: training accuarcy: 0.9965\n",
      "Epoch 4 step 507: training loss: 18.180785645640995\n",
      "Epoch 4 step 508: training accuarcy: 0.997\n",
      "Epoch 4 step 508: training loss: 11.83493631554119\n",
      "Epoch 4 step 509: training accuarcy: 0.9975\n",
      "Epoch 4 step 509: training loss: 12.6743768567692\n",
      "Epoch 4 step 510: training accuarcy: 0.997\n",
      "Epoch 4 step 510: training loss: 18.580356669265576\n",
      "Epoch 4 step 511: training accuarcy: 0.995\n",
      "Epoch 4 step 511: training loss: 15.939443091375338\n",
      "Epoch 4 step 512: training accuarcy: 0.9975\n",
      "Epoch 4 step 512: training loss: 14.711347502434347\n",
      "Epoch 4 step 513: training accuarcy: 0.997\n",
      "Epoch 4 step 513: training loss: 15.909771955162807\n",
      "Epoch 4 step 514: training accuarcy: 0.9965\n",
      "Epoch 4 step 514: training loss: 17.066902904531712\n",
      "Epoch 4 step 515: training accuarcy: 0.996\n",
      "Epoch 4 step 515: training loss: 12.744798399337864\n",
      "Epoch 4 step 516: training accuarcy: 0.9965\n",
      "Epoch 4 step 516: training loss: 17.514770949938764\n",
      "Epoch 4 step 517: training accuarcy: 0.9945\n",
      "Epoch 4 step 517: training loss: 17.443714282551674\n",
      "Epoch 4 step 518: training accuarcy: 0.9965\n",
      "Epoch 4 step 518: training loss: 17.142796682053106\n",
      "Epoch 4 step 519: training accuarcy: 0.996\n",
      "Epoch 4 step 519: training loss: 12.189128704084775\n",
      "Epoch 4 step 520: training accuarcy: 0.997\n",
      "Epoch 4 step 520: training loss: 12.943774787680338\n",
      "Epoch 4 step 521: training accuarcy: 0.9975\n",
      "Epoch 4 step 521: training loss: 10.95076113036233\n",
      "Epoch 4 step 522: training accuarcy: 0.998\n",
      "Epoch 4 step 522: training loss: 10.366021172692287\n",
      "Epoch 4 step 523: training accuarcy: 0.999\n",
      "Epoch 4 step 523: training loss: 13.180609807038477\n",
      "Epoch 4 step 524: training accuarcy: 0.9965\n",
      "Epoch 4 step 524: training loss: 8.43470023157899\n",
      "Epoch 4 step 525: training accuarcy: 0.999\n",
      "Epoch 4 step 525: training loss: 10.581870412685806\n",
      "Epoch 4 step 526: training accuarcy: 0.9985\n",
      "Epoch 4 step 526: training loss: 10.43238479758793\n",
      "Epoch 4 step 527: training accuarcy: 0.9985\n",
      "Epoch 4 step 527: training loss: 13.755096333648334\n",
      "Epoch 4 step 528: training accuarcy: 0.997\n",
      "Epoch 4 step 528: training loss: 10.75234885820441\n",
      "Epoch 4 step 529: training accuarcy: 0.998\n",
      "Epoch 4 step 529: training loss: 16.65627542914613\n",
      "Epoch 4 step 530: training accuarcy: 0.9965\n",
      "Epoch 4 step 530: training loss: 12.965229358124349\n",
      "Epoch 4 step 531: training accuarcy: 0.997\n",
      "Epoch 4 step 531: training loss: 10.407445799364007\n",
      "Epoch 4 step 532: training accuarcy: 0.9985\n",
      "Epoch 4 step 532: training loss: 15.488990529597103\n",
      "Epoch 4 step 533: training accuarcy: 0.996\n",
      "Epoch 4 step 533: training loss: 13.516706300689616\n",
      "Epoch 4 step 534: training accuarcy: 0.997\n",
      "Epoch 4 step 534: training loss: 14.993375078795616\n",
      "Epoch 4 step 535: training accuarcy: 0.998\n",
      "Epoch 4 step 535: training loss: 10.527794392270721\n",
      "Epoch 4 step 536: training accuarcy: 0.998\n",
      "Epoch 4 step 536: training loss: 11.234380443359187\n",
      "Epoch 4 step 537: training accuarcy: 0.998\n",
      "Epoch 4 step 537: training loss: 18.684949958014478\n",
      "Epoch 4 step 538: training accuarcy: 0.9955\n",
      "Epoch 4 step 538: training loss: 15.412368695645146\n",
      "Epoch 4 step 539: training accuarcy: 0.9965\n",
      "Epoch 4 step 539: training loss: 10.865570232640493\n",
      "Epoch 4 step 540: training accuarcy: 0.998\n",
      "Epoch 4 step 540: training loss: 12.065248591567162\n",
      "Epoch 4 step 541: training accuarcy: 0.9985\n",
      "Epoch 4 step 541: training loss: 10.737929759675588\n",
      "Epoch 4 step 542: training accuarcy: 0.998\n",
      "Epoch 4 step 542: training loss: 11.517310574841463\n",
      "Epoch 4 step 543: training accuarcy: 0.997\n",
      "Epoch 4 step 543: training loss: 15.685287360309335\n",
      "Epoch 4 step 544: training accuarcy: 0.9955\n",
      "Epoch 4 step 544: training loss: 10.393641966708556\n",
      "Epoch 4 step 545: training accuarcy: 0.998\n",
      "Epoch 4 step 545: training loss: 10.368953378197057\n",
      "Epoch 4 step 546: training accuarcy: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 546: training loss: 9.847628001273176\n",
      "Epoch 4 step 547: training accuarcy: 0.999\n",
      "Epoch 4 step 547: training loss: 11.501630238850812\n",
      "Epoch 4 step 548: training accuarcy: 0.9975\n",
      "Epoch 4 step 548: training loss: 10.896522424236881\n",
      "Epoch 4 step 549: training accuarcy: 0.998\n",
      "Epoch 4 step 549: training loss: 18.65840783476295\n",
      "Epoch 4 step 550: training accuarcy: 0.995\n",
      "Epoch 4 step 550: training loss: 10.160125829414529\n",
      "Epoch 4 step 551: training accuarcy: 0.9985\n",
      "Epoch 4 step 551: training loss: 9.331417401275367\n",
      "Epoch 4 step 552: training accuarcy: 0.998\n",
      "Epoch 4 step 552: training loss: 17.455808247641073\n",
      "Epoch 4 step 553: training accuarcy: 0.996\n",
      "Epoch 4 step 553: training loss: 14.614633861415621\n",
      "Epoch 4 step 554: training accuarcy: 0.9975\n",
      "Epoch 4 step 554: training loss: 11.375860254885536\n",
      "Epoch 4 step 555: training accuarcy: 0.998\n",
      "Epoch 4 step 555: training loss: 11.449497135326567\n",
      "Epoch 4 step 556: training accuarcy: 0.9975\n",
      "Epoch 4 step 556: training loss: 16.621826118474377\n",
      "Epoch 4 step 557: training accuarcy: 0.997\n",
      "Epoch 4 step 557: training loss: 10.815409662162446\n",
      "Epoch 4 step 558: training accuarcy: 0.9975\n",
      "Epoch 4 step 558: training loss: 8.947367970686605\n",
      "Epoch 4 step 559: training accuarcy: 0.9985\n",
      "Epoch 4 step 559: training loss: 10.801372435182229\n",
      "Epoch 4 step 560: training accuarcy: 0.9975\n",
      "Epoch 4 step 560: training loss: 11.297968793665403\n",
      "Epoch 4 step 561: training accuarcy: 0.999\n",
      "Epoch 4 step 561: training loss: 7.589116325551036\n",
      "Epoch 4 step 562: training accuarcy: 0.9985\n",
      "Epoch 4 step 562: training loss: 11.315319842559896\n",
      "Epoch 4 step 563: training accuarcy: 0.9975\n",
      "Epoch 4 step 563: training loss: 10.685138693493293\n",
      "Epoch 4 step 564: training accuarcy: 0.9985\n",
      "Epoch 4 step 564: training loss: 15.88093486299967\n",
      "Epoch 4 step 565: training accuarcy: 0.997\n",
      "Epoch 4 step 565: training loss: 13.416798506557972\n",
      "Epoch 4 step 566: training accuarcy: 0.9965\n",
      "Epoch 4 step 566: training loss: 14.929253973667572\n",
      "Epoch 4 step 567: training accuarcy: 0.9965\n",
      "Epoch 4 step 567: training loss: 13.26133599658745\n",
      "Epoch 4 step 568: training accuarcy: 0.997\n",
      "Epoch 4 step 568: training loss: 8.760292479237664\n",
      "Epoch 4 step 569: training accuarcy: 0.998\n",
      "Epoch 4 step 569: training loss: 16.267379074896663\n",
      "Epoch 4 step 570: training accuarcy: 0.997\n",
      "Epoch 4 step 570: training loss: 10.550838287033155\n",
      "Epoch 4 step 571: training accuarcy: 0.999\n",
      "Epoch 4 step 571: training loss: 13.635505665455234\n",
      "Epoch 4 step 572: training accuarcy: 0.9965\n",
      "Epoch 4 step 572: training loss: 8.090521021315576\n",
      "Epoch 4 step 573: training accuarcy: 0.999\n",
      "Epoch 4 step 573: training loss: 19.042539163415146\n",
      "Epoch 4 step 574: training accuarcy: 0.9955\n",
      "Epoch 4 step 574: training loss: 18.522228736445154\n",
      "Epoch 4 step 575: training accuarcy: 0.9958071278825996\n",
      "Epoch 4: train loss 13.847211475172, train accuarcy 0.996495246887207\n",
      "Epoch 4: valid loss 11.417747960996573, valid accuarcy 0.9973040819168091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 5/8 [04:32<02:43, 54.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 575: training loss: 11.827505599010632\n",
      "Epoch 5 step 576: training accuarcy: 0.997\n",
      "Epoch 5 step 576: training loss: 8.79841962290871\n",
      "Epoch 5 step 577: training accuarcy: 0.998\n",
      "Epoch 5 step 577: training loss: 13.491281032007034\n",
      "Epoch 5 step 578: training accuarcy: 0.997\n",
      "Epoch 5 step 578: training loss: 10.41342948073753\n",
      "Epoch 5 step 579: training accuarcy: 0.998\n",
      "Epoch 5 step 579: training loss: 11.927098503925944\n",
      "Epoch 5 step 580: training accuarcy: 0.9975\n",
      "Epoch 5 step 580: training loss: 9.228367727888811\n",
      "Epoch 5 step 581: training accuarcy: 0.9995\n",
      "Epoch 5 step 581: training loss: 5.465866344807413\n",
      "Epoch 5 step 582: training accuarcy: 0.9995\n",
      "Epoch 5 step 582: training loss: 11.030014408158738\n",
      "Epoch 5 step 583: training accuarcy: 0.9985\n",
      "Epoch 5 step 583: training loss: 15.632966828937723\n",
      "Epoch 5 step 584: training accuarcy: 0.9955\n",
      "Epoch 5 step 584: training loss: 12.507493763672509\n",
      "Epoch 5 step 585: training accuarcy: 0.998\n",
      "Epoch 5 step 585: training loss: 7.080830484996995\n",
      "Epoch 5 step 586: training accuarcy: 0.999\n",
      "Epoch 5 step 586: training loss: 12.661040913630256\n",
      "Epoch 5 step 587: training accuarcy: 0.997\n",
      "Epoch 5 step 587: training loss: 13.871232940391913\n",
      "Epoch 5 step 588: training accuarcy: 0.996\n",
      "Epoch 5 step 588: training loss: 13.359643583430456\n",
      "Epoch 5 step 589: training accuarcy: 0.996\n",
      "Epoch 5 step 589: training loss: 13.234108018520192\n",
      "Epoch 5 step 590: training accuarcy: 0.9965\n",
      "Epoch 5 step 590: training loss: 14.419572352253965\n",
      "Epoch 5 step 591: training accuarcy: 0.996\n",
      "Epoch 5 step 591: training loss: 19.26101770674136\n",
      "Epoch 5 step 592: training accuarcy: 0.9935\n",
      "Epoch 5 step 592: training loss: 12.306360041921806\n",
      "Epoch 5 step 593: training accuarcy: 0.998\n",
      "Epoch 5 step 593: training loss: 12.030058177767202\n",
      "Epoch 5 step 594: training accuarcy: 0.9965\n",
      "Epoch 5 step 594: training loss: 11.776462546220639\n",
      "Epoch 5 step 595: training accuarcy: 0.9975\n",
      "Epoch 5 step 595: training loss: 10.408863812460908\n",
      "Epoch 5 step 596: training accuarcy: 0.998\n",
      "Epoch 5 step 596: training loss: 13.817467969793825\n",
      "Epoch 5 step 597: training accuarcy: 0.9975\n",
      "Epoch 5 step 597: training loss: 12.33545578104368\n",
      "Epoch 5 step 598: training accuarcy: 0.9975\n",
      "Epoch 5 step 598: training loss: 10.97649944727665\n",
      "Epoch 5 step 599: training accuarcy: 0.9975\n",
      "Epoch 5 step 599: training loss: 22.570062024380604\n",
      "Epoch 5 step 600: training accuarcy: 0.995\n",
      "Epoch 5 step 600: training loss: 6.587112415419256\n",
      "Epoch 5 step 601: training accuarcy: 0.9995\n",
      "Epoch 5 step 601: training loss: 6.419911085990914\n",
      "Epoch 5 step 602: training accuarcy: 0.9995\n",
      "Epoch 5 step 602: training loss: 13.01072182313227\n",
      "Epoch 5 step 603: training accuarcy: 0.9975\n",
      "Epoch 5 step 603: training loss: 13.224190028910453\n",
      "Epoch 5 step 604: training accuarcy: 0.997\n",
      "Epoch 5 step 604: training loss: 17.028660530811962\n",
      "Epoch 5 step 605: training accuarcy: 0.996\n",
      "Epoch 5 step 605: training loss: 14.588391859475172\n",
      "Epoch 5 step 606: training accuarcy: 0.997\n",
      "Epoch 5 step 606: training loss: 10.506540564181334\n",
      "Epoch 5 step 607: training accuarcy: 0.998\n",
      "Epoch 5 step 607: training loss: 12.992062881311728\n",
      "Epoch 5 step 608: training accuarcy: 0.998\n",
      "Epoch 5 step 608: training loss: 12.776812474213889\n",
      "Epoch 5 step 609: training accuarcy: 0.997\n",
      "Epoch 5 step 609: training loss: 19.313638810997187\n",
      "Epoch 5 step 610: training accuarcy: 0.9955\n",
      "Epoch 5 step 610: training loss: 14.796138504560417\n",
      "Epoch 5 step 611: training accuarcy: 0.9965\n",
      "Epoch 5 step 611: training loss: 10.763664882753424\n",
      "Epoch 5 step 612: training accuarcy: 0.9975\n",
      "Epoch 5 step 612: training loss: 10.93318576670533\n",
      "Epoch 5 step 613: training accuarcy: 0.9985\n",
      "Epoch 5 step 613: training loss: 8.52432111046582\n",
      "Epoch 5 step 614: training accuarcy: 0.9985\n",
      "Epoch 5 step 614: training loss: 13.41064557195354\n",
      "Epoch 5 step 615: training accuarcy: 0.9965\n",
      "Epoch 5 step 615: training loss: 8.07236648676256\n",
      "Epoch 5 step 616: training accuarcy: 0.998\n",
      "Epoch 5 step 616: training loss: 12.306771296866154\n",
      "Epoch 5 step 617: training accuarcy: 0.9975\n",
      "Epoch 5 step 617: training loss: 17.255251340452997\n",
      "Epoch 5 step 618: training accuarcy: 0.9955\n",
      "Epoch 5 step 618: training loss: 9.5392973384284\n",
      "Epoch 5 step 619: training accuarcy: 1.0\n",
      "Epoch 5 step 619: training loss: 11.885855451965217\n",
      "Epoch 5 step 620: training accuarcy: 0.9965\n",
      "Epoch 5 step 620: training loss: 17.41356489789455\n",
      "Epoch 5 step 621: training accuarcy: 0.9965\n",
      "Epoch 5 step 621: training loss: 8.71285713278658\n",
      "Epoch 5 step 622: training accuarcy: 0.9985\n",
      "Epoch 5 step 622: training loss: 11.922639181436955\n",
      "Epoch 5 step 623: training accuarcy: 0.997\n",
      "Epoch 5 step 623: training loss: 8.71950200486413\n",
      "Epoch 5 step 624: training accuarcy: 0.999\n",
      "Epoch 5 step 624: training loss: 12.268387496944136\n",
      "Epoch 5 step 625: training accuarcy: 0.997\n",
      "Epoch 5 step 625: training loss: 12.635445798454452\n",
      "Epoch 5 step 626: training accuarcy: 0.997\n",
      "Epoch 5 step 626: training loss: 11.875349115935894\n",
      "Epoch 5 step 627: training accuarcy: 0.997\n",
      "Epoch 5 step 627: training loss: 12.154872943189943\n",
      "Epoch 5 step 628: training accuarcy: 0.9975\n",
      "Epoch 5 step 628: training loss: 12.925499174517563\n",
      "Epoch 5 step 629: training accuarcy: 0.997\n",
      "Epoch 5 step 629: training loss: 11.059980119388433\n",
      "Epoch 5 step 630: training accuarcy: 0.998\n",
      "Epoch 5 step 630: training loss: 10.1906110675629\n",
      "Epoch 5 step 631: training accuarcy: 0.9985\n",
      "Epoch 5 step 631: training loss: 13.344551336556965\n",
      "Epoch 5 step 632: training accuarcy: 0.997\n",
      "Epoch 5 step 632: training loss: 8.563218125235764\n",
      "Epoch 5 step 633: training accuarcy: 0.9985\n",
      "Epoch 5 step 633: training loss: 14.77118129937737\n",
      "Epoch 5 step 634: training accuarcy: 0.996\n",
      "Epoch 5 step 634: training loss: 13.598635442230176\n",
      "Epoch 5 step 635: training accuarcy: 0.9975\n",
      "Epoch 5 step 635: training loss: 7.843909303752892\n",
      "Epoch 5 step 636: training accuarcy: 0.999\n",
      "Epoch 5 step 636: training loss: 17.791375918148432\n",
      "Epoch 5 step 637: training accuarcy: 0.995\n",
      "Epoch 5 step 637: training loss: 19.626313838147407\n",
      "Epoch 5 step 638: training accuarcy: 0.995\n",
      "Epoch 5 step 638: training loss: 11.648421581287272\n",
      "Epoch 5 step 639: training accuarcy: 0.9975\n",
      "Epoch 5 step 639: training loss: 12.885904410783885\n",
      "Epoch 5 step 640: training accuarcy: 0.997\n",
      "Epoch 5 step 640: training loss: 9.909883821113814\n",
      "Epoch 5 step 641: training accuarcy: 0.9985\n",
      "Epoch 5 step 641: training loss: 16.081060381943818\n",
      "Epoch 5 step 642: training accuarcy: 0.996\n",
      "Epoch 5 step 642: training loss: 14.935007380236566\n",
      "Epoch 5 step 643: training accuarcy: 0.9965\n",
      "Epoch 5 step 643: training loss: 9.467089682163087\n",
      "Epoch 5 step 644: training accuarcy: 0.9985\n",
      "Epoch 5 step 644: training loss: 12.25837744516284\n",
      "Epoch 5 step 645: training accuarcy: 0.997\n",
      "Epoch 5 step 645: training loss: 8.657514894499952\n",
      "Epoch 5 step 646: training accuarcy: 0.999\n",
      "Epoch 5 step 646: training loss: 13.509186301538726\n",
      "Epoch 5 step 647: training accuarcy: 0.996\n",
      "Epoch 5 step 647: training loss: 17.07167190562125\n",
      "Epoch 5 step 648: training accuarcy: 0.9965\n",
      "Epoch 5 step 648: training loss: 12.612165032685496\n",
      "Epoch 5 step 649: training accuarcy: 0.9985\n",
      "Epoch 5 step 649: training loss: 11.998127336670992\n",
      "Epoch 5 step 650: training accuarcy: 0.997\n",
      "Epoch 5 step 650: training loss: 11.235897640787902\n",
      "Epoch 5 step 651: training accuarcy: 0.998\n",
      "Epoch 5 step 651: training loss: 11.995595263491245\n",
      "Epoch 5 step 652: training accuarcy: 0.997\n",
      "Epoch 5 step 652: training loss: 14.812184622099922\n",
      "Epoch 5 step 653: training accuarcy: 0.9955\n",
      "Epoch 5 step 653: training loss: 12.355117882876254\n",
      "Epoch 5 step 654: training accuarcy: 0.9975\n",
      "Epoch 5 step 654: training loss: 9.03277454918323\n",
      "Epoch 5 step 655: training accuarcy: 0.998\n",
      "Epoch 5 step 655: training loss: 15.498751533445256\n",
      "Epoch 5 step 656: training accuarcy: 0.996\n",
      "Epoch 5 step 656: training loss: 12.458844367805083\n",
      "Epoch 5 step 657: training accuarcy: 0.998\n",
      "Epoch 5 step 657: training loss: 14.20181587803958\n",
      "Epoch 5 step 658: training accuarcy: 0.9955\n",
      "Epoch 5 step 658: training loss: 7.368149220314999\n",
      "Epoch 5 step 659: training accuarcy: 0.9985\n",
      "Epoch 5 step 659: training loss: 14.122262139246297\n",
      "Epoch 5 step 660: training accuarcy: 0.9955\n",
      "Epoch 5 step 660: training loss: 13.62151015257507\n",
      "Epoch 5 step 661: training accuarcy: 0.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 661: training loss: 11.908288856982491\n",
      "Epoch 5 step 662: training accuarcy: 0.9975\n",
      "Epoch 5 step 662: training loss: 10.59144157680076\n",
      "Epoch 5 step 663: training accuarcy: 0.998\n",
      "Epoch 5 step 663: training loss: 12.678673590565639\n",
      "Epoch 5 step 664: training accuarcy: 0.997\n",
      "Epoch 5 step 664: training loss: 9.254611380187452\n",
      "Epoch 5 step 665: training accuarcy: 0.998\n",
      "Epoch 5 step 665: training loss: 12.4185995295822\n",
      "Epoch 5 step 666: training accuarcy: 0.998\n",
      "Epoch 5 step 666: training loss: 9.079012747341952\n",
      "Epoch 5 step 667: training accuarcy: 0.999\n",
      "Epoch 5 step 667: training loss: 10.611931041343794\n",
      "Epoch 5 step 668: training accuarcy: 0.999\n",
      "Epoch 5 step 668: training loss: 9.253949743320115\n",
      "Epoch 5 step 669: training accuarcy: 0.9985\n",
      "Epoch 5 step 669: training loss: 12.877358301284028\n",
      "Epoch 5 step 670: training accuarcy: 0.998\n",
      "Epoch 5 step 670: training loss: 10.828314647330476\n",
      "Epoch 5 step 671: training accuarcy: 0.9975\n",
      "Epoch 5 step 671: training loss: 15.306212779338422\n",
      "Epoch 5 step 672: training accuarcy: 0.9965\n",
      "Epoch 5 step 672: training loss: 7.098878336112119\n",
      "Epoch 5 step 673: training accuarcy: 0.999\n",
      "Epoch 5 step 673: training loss: 13.58645582571818\n",
      "Epoch 5 step 674: training accuarcy: 0.9965\n",
      "Epoch 5 step 674: training loss: 13.783485829733078\n",
      "Epoch 5 step 675: training accuarcy: 0.9975\n",
      "Epoch 5 step 675: training loss: 7.9802288903172185\n",
      "Epoch 5 step 676: training accuarcy: 0.9985\n",
      "Epoch 5 step 676: training loss: 7.744042370330514\n",
      "Epoch 5 step 677: training accuarcy: 0.998\n",
      "Epoch 5 step 677: training loss: 11.34849463948692\n",
      "Epoch 5 step 678: training accuarcy: 0.9975\n",
      "Epoch 5 step 678: training loss: 15.132350458292354\n",
      "Epoch 5 step 679: training accuarcy: 0.997\n",
      "Epoch 5 step 679: training loss: 15.64568117965425\n",
      "Epoch 5 step 680: training accuarcy: 0.996\n",
      "Epoch 5 step 680: training loss: 16.473498486572982\n",
      "Epoch 5 step 681: training accuarcy: 0.996\n",
      "Epoch 5 step 681: training loss: 10.23350562761619\n",
      "Epoch 5 step 682: training accuarcy: 0.998\n",
      "Epoch 5 step 682: training loss: 9.069892765966527\n",
      "Epoch 5 step 683: training accuarcy: 0.998\n",
      "Epoch 5 step 683: training loss: 11.916080405807273\n",
      "Epoch 5 step 684: training accuarcy: 0.997\n",
      "Epoch 5 step 684: training loss: 8.523714341309303\n",
      "Epoch 5 step 685: training accuarcy: 0.9995\n",
      "Epoch 5 step 685: training loss: 12.669909849191518\n",
      "Epoch 5 step 686: training accuarcy: 0.997\n",
      "Epoch 5 step 686: training loss: 12.132703232792862\n",
      "Epoch 5 step 687: training accuarcy: 0.997\n",
      "Epoch 5 step 687: training loss: 9.475630715252668\n",
      "Epoch 5 step 688: training accuarcy: 0.9985\n",
      "Epoch 5 step 688: training loss: 9.662565857603314\n",
      "Epoch 5 step 689: training accuarcy: 0.9975\n",
      "Epoch 5 step 689: training loss: 14.204166611787914\n",
      "Epoch 5 step 690: training accuarcy: 0.9963312368972747\n",
      "Epoch 5: train loss 12.165317796311854, train accuarcy 0.9965604543685913\n",
      "Epoch 5: valid loss 9.187911735992628, valid accuarcy 0.9981055855751038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 6/8 [05:26<01:49, 54.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 690: training loss: 10.855657683567959\n",
      "Epoch 6 step 691: training accuarcy: 0.998\n",
      "Epoch 6 step 691: training loss: 13.012175625384549\n",
      "Epoch 6 step 692: training accuarcy: 0.998\n",
      "Epoch 6 step 692: training loss: 14.83792795160522\n",
      "Epoch 6 step 693: training accuarcy: 0.9965\n",
      "Epoch 6 step 693: training loss: 8.138868891935669\n",
      "Epoch 6 step 694: training accuarcy: 0.9985\n",
      "Epoch 6 step 694: training loss: 7.550348414864541\n",
      "Epoch 6 step 695: training accuarcy: 0.9995\n",
      "Epoch 6 step 695: training loss: 8.024254106722353\n",
      "Epoch 6 step 696: training accuarcy: 0.999\n",
      "Epoch 6 step 696: training loss: 16.46897493976397\n",
      "Epoch 6 step 697: training accuarcy: 0.994\n",
      "Epoch 6 step 697: training loss: 9.118939766827395\n",
      "Epoch 6 step 698: training accuarcy: 0.9975\n",
      "Epoch 6 step 698: training loss: 7.4071646726278155\n",
      "Epoch 6 step 699: training accuarcy: 0.9985\n",
      "Epoch 6 step 699: training loss: 9.816571057192608\n",
      "Epoch 6 step 700: training accuarcy: 0.998\n",
      "Epoch 6 step 700: training loss: 13.328985520347434\n",
      "Epoch 6 step 701: training accuarcy: 0.9965\n",
      "Epoch 6 step 701: training loss: 11.8114466923418\n",
      "Epoch 6 step 702: training accuarcy: 0.998\n",
      "Epoch 6 step 702: training loss: 12.515010937208359\n",
      "Epoch 6 step 703: training accuarcy: 0.998\n",
      "Epoch 6 step 703: training loss: 10.541019621719071\n",
      "Epoch 6 step 704: training accuarcy: 0.998\n",
      "Epoch 6 step 704: training loss: 14.94681899804274\n",
      "Epoch 6 step 705: training accuarcy: 0.996\n",
      "Epoch 6 step 705: training loss: 12.415508712358085\n",
      "Epoch 6 step 706: training accuarcy: 0.997\n",
      "Epoch 6 step 706: training loss: 9.672918615483285\n",
      "Epoch 6 step 707: training accuarcy: 0.9975\n",
      "Epoch 6 step 707: training loss: 11.483548882532878\n",
      "Epoch 6 step 708: training accuarcy: 0.997\n",
      "Epoch 6 step 708: training loss: 14.27740024098275\n",
      "Epoch 6 step 709: training accuarcy: 0.997\n",
      "Epoch 6 step 709: training loss: 6.956083256141736\n",
      "Epoch 6 step 710: training accuarcy: 0.9995\n",
      "Epoch 6 step 710: training loss: 14.678468677943382\n",
      "Epoch 6 step 711: training accuarcy: 0.996\n",
      "Epoch 6 step 711: training loss: 7.715112562260449\n",
      "Epoch 6 step 712: training accuarcy: 0.999\n",
      "Epoch 6 step 712: training loss: 9.35300705196733\n",
      "Epoch 6 step 713: training accuarcy: 0.9985\n",
      "Epoch 6 step 713: training loss: 7.9804881595814\n",
      "Epoch 6 step 714: training accuarcy: 0.9985\n",
      "Epoch 6 step 714: training loss: 13.398332830250293\n",
      "Epoch 6 step 715: training accuarcy: 0.9975\n",
      "Epoch 6 step 715: training loss: 11.15400554231191\n",
      "Epoch 6 step 716: training accuarcy: 0.9975\n",
      "Epoch 6 step 716: training loss: 15.837105008823155\n",
      "Epoch 6 step 717: training accuarcy: 0.996\n",
      "Epoch 6 step 717: training loss: 11.572970049317249\n",
      "Epoch 6 step 718: training accuarcy: 0.997\n",
      "Epoch 6 step 718: training loss: 7.810496957731364\n",
      "Epoch 6 step 719: training accuarcy: 0.999\n",
      "Epoch 6 step 719: training loss: 6.90000047142985\n",
      "Epoch 6 step 720: training accuarcy: 0.9985\n",
      "Epoch 6 step 720: training loss: 6.716274389278238\n",
      "Epoch 6 step 721: training accuarcy: 0.999\n",
      "Epoch 6 step 721: training loss: 15.360884033624476\n",
      "Epoch 6 step 722: training accuarcy: 0.997\n",
      "Epoch 6 step 722: training loss: 11.535005201037746\n",
      "Epoch 6 step 723: training accuarcy: 0.997\n",
      "Epoch 6 step 723: training loss: 9.327493066897928\n",
      "Epoch 6 step 724: training accuarcy: 0.9985\n",
      "Epoch 6 step 724: training loss: 12.00909810035649\n",
      "Epoch 6 step 725: training accuarcy: 0.9965\n",
      "Epoch 6 step 725: training loss: 12.803874545125494\n",
      "Epoch 6 step 726: training accuarcy: 0.997\n",
      "Epoch 6 step 726: training loss: 7.589143478974376\n",
      "Epoch 6 step 727: training accuarcy: 0.998\n",
      "Epoch 6 step 727: training loss: 11.167657818373764\n",
      "Epoch 6 step 728: training accuarcy: 0.9985\n",
      "Epoch 6 step 728: training loss: 9.727145009970688\n",
      "Epoch 6 step 729: training accuarcy: 0.9985\n",
      "Epoch 6 step 729: training loss: 5.0750649441000935\n",
      "Epoch 6 step 730: training accuarcy: 0.999\n",
      "Epoch 6 step 730: training loss: 9.660557222757948\n",
      "Epoch 6 step 731: training accuarcy: 0.9985\n",
      "Epoch 6 step 731: training loss: 12.76788460417266\n",
      "Epoch 6 step 732: training accuarcy: 0.998\n",
      "Epoch 6 step 732: training loss: 14.308491203233855\n",
      "Epoch 6 step 733: training accuarcy: 0.997\n",
      "Epoch 6 step 733: training loss: 10.547958086093713\n",
      "Epoch 6 step 734: training accuarcy: 0.997\n",
      "Epoch 6 step 734: training loss: 14.654131340106096\n",
      "Epoch 6 step 735: training accuarcy: 0.9955\n",
      "Epoch 6 step 735: training loss: 13.662522692312576\n",
      "Epoch 6 step 736: training accuarcy: 0.996\n",
      "Epoch 6 step 736: training loss: 13.623332443453386\n",
      "Epoch 6 step 737: training accuarcy: 0.9965\n",
      "Epoch 6 step 737: training loss: 12.276007655611936\n",
      "Epoch 6 step 738: training accuarcy: 0.9985\n",
      "Epoch 6 step 738: training loss: 12.971240968603507\n",
      "Epoch 6 step 739: training accuarcy: 0.9975\n",
      "Epoch 6 step 739: training loss: 6.950564055291941\n",
      "Epoch 6 step 740: training accuarcy: 0.9985\n",
      "Epoch 6 step 740: training loss: 9.494317382654874\n",
      "Epoch 6 step 741: training accuarcy: 0.9985\n",
      "Epoch 6 step 741: training loss: 8.756497333286845\n",
      "Epoch 6 step 742: training accuarcy: 0.998\n",
      "Epoch 6 step 742: training loss: 13.157411472159616\n",
      "Epoch 6 step 743: training accuarcy: 0.9965\n",
      "Epoch 6 step 743: training loss: 6.183177944592678\n",
      "Epoch 6 step 744: training accuarcy: 0.999\n",
      "Epoch 6 step 744: training loss: 7.853361392420949\n",
      "Epoch 6 step 745: training accuarcy: 0.9985\n",
      "Epoch 6 step 745: training loss: 5.3708516115122915\n",
      "Epoch 6 step 746: training accuarcy: 1.0\n",
      "Epoch 6 step 746: training loss: 11.154209496852122\n",
      "Epoch 6 step 747: training accuarcy: 0.998\n",
      "Epoch 6 step 747: training loss: 11.765485548265262\n",
      "Epoch 6 step 748: training accuarcy: 0.9975\n",
      "Epoch 6 step 748: training loss: 13.937319177606419\n",
      "Epoch 6 step 749: training accuarcy: 0.9975\n",
      "Epoch 6 step 749: training loss: 8.322721292433423\n",
      "Epoch 6 step 750: training accuarcy: 0.998\n",
      "Epoch 6 step 750: training loss: 9.91638438299551\n",
      "Epoch 6 step 751: training accuarcy: 0.999\n",
      "Epoch 6 step 751: training loss: 10.393169717984575\n",
      "Epoch 6 step 752: training accuarcy: 0.997\n",
      "Epoch 6 step 752: training loss: 10.189792092459403\n",
      "Epoch 6 step 753: training accuarcy: 0.998\n",
      "Epoch 6 step 753: training loss: 11.04439188827884\n",
      "Epoch 6 step 754: training accuarcy: 0.998\n",
      "Epoch 6 step 754: training loss: 10.066105061952705\n",
      "Epoch 6 step 755: training accuarcy: 0.9985\n",
      "Epoch 6 step 755: training loss: 13.036616715639964\n",
      "Epoch 6 step 756: training accuarcy: 0.997\n",
      "Epoch 6 step 756: training loss: 10.464214617386126\n",
      "Epoch 6 step 757: training accuarcy: 0.996\n",
      "Epoch 6 step 757: training loss: 10.193459697846503\n",
      "Epoch 6 step 758: training accuarcy: 0.998\n",
      "Epoch 6 step 758: training loss: 14.604341003798035\n",
      "Epoch 6 step 759: training accuarcy: 0.996\n",
      "Epoch 6 step 759: training loss: 12.678435643031765\n",
      "Epoch 6 step 760: training accuarcy: 0.997\n",
      "Epoch 6 step 760: training loss: 11.977201577047126\n",
      "Epoch 6 step 761: training accuarcy: 0.997\n",
      "Epoch 6 step 761: training loss: 10.54972571226686\n",
      "Epoch 6 step 762: training accuarcy: 0.9975\n",
      "Epoch 6 step 762: training loss: 12.32936469733058\n",
      "Epoch 6 step 763: training accuarcy: 0.9965\n",
      "Epoch 6 step 763: training loss: 9.263824625117241\n",
      "Epoch 6 step 764: training accuarcy: 0.998\n",
      "Epoch 6 step 764: training loss: 17.222293613732518\n",
      "Epoch 6 step 765: training accuarcy: 0.995\n",
      "Epoch 6 step 765: training loss: 17.330302512849556\n",
      "Epoch 6 step 766: training accuarcy: 0.996\n",
      "Epoch 6 step 766: training loss: 14.100719337527629\n",
      "Epoch 6 step 767: training accuarcy: 0.9965\n",
      "Epoch 6 step 767: training loss: 7.7449077706222065\n",
      "Epoch 6 step 768: training accuarcy: 0.999\n",
      "Epoch 6 step 768: training loss: 7.126949350385223\n",
      "Epoch 6 step 769: training accuarcy: 0.9985\n",
      "Epoch 6 step 769: training loss: 4.665240441976388\n",
      "Epoch 6 step 770: training accuarcy: 1.0\n",
      "Epoch 6 step 770: training loss: 11.562854389231157\n",
      "Epoch 6 step 771: training accuarcy: 0.9975\n",
      "Epoch 6 step 771: training loss: 12.867594055903929\n",
      "Epoch 6 step 772: training accuarcy: 0.9965\n",
      "Epoch 6 step 772: training loss: 9.129645634198955\n",
      "Epoch 6 step 773: training accuarcy: 0.998\n",
      "Epoch 6 step 773: training loss: 15.257278830265545\n",
      "Epoch 6 step 774: training accuarcy: 0.996\n",
      "Epoch 6 step 774: training loss: 12.623633169142035\n",
      "Epoch 6 step 775: training accuarcy: 0.9955\n",
      "Epoch 6 step 775: training loss: 14.547376706822087\n",
      "Epoch 6 step 776: training accuarcy: 0.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 776: training loss: 9.151883625292346\n",
      "Epoch 6 step 777: training accuarcy: 0.999\n",
      "Epoch 6 step 777: training loss: 11.358224378553803\n",
      "Epoch 6 step 778: training accuarcy: 0.997\n",
      "Epoch 6 step 778: training loss: 8.861355969446159\n",
      "Epoch 6 step 779: training accuarcy: 0.998\n",
      "Epoch 6 step 779: training loss: 11.73925168249439\n",
      "Epoch 6 step 780: training accuarcy: 0.9975\n",
      "Epoch 6 step 780: training loss: 16.24820744735153\n",
      "Epoch 6 step 781: training accuarcy: 0.998\n",
      "Epoch 6 step 781: training loss: 10.629463208345193\n",
      "Epoch 6 step 782: training accuarcy: 0.9975\n",
      "Epoch 6 step 782: training loss: 14.186852483782042\n",
      "Epoch 6 step 783: training accuarcy: 0.995\n",
      "Epoch 6 step 783: training loss: 12.68431564714567\n",
      "Epoch 6 step 784: training accuarcy: 0.9975\n",
      "Epoch 6 step 784: training loss: 10.347415631869952\n",
      "Epoch 6 step 785: training accuarcy: 0.998\n",
      "Epoch 6 step 785: training loss: 7.437622268654307\n",
      "Epoch 6 step 786: training accuarcy: 0.9995\n",
      "Epoch 6 step 786: training loss: 17.04763048066891\n",
      "Epoch 6 step 787: training accuarcy: 0.995\n",
      "Epoch 6 step 787: training loss: 10.229052086660715\n",
      "Epoch 6 step 788: training accuarcy: 0.9975\n",
      "Epoch 6 step 788: training loss: 8.800960550944122\n",
      "Epoch 6 step 789: training accuarcy: 0.9975\n",
      "Epoch 6 step 789: training loss: 7.008094308871465\n",
      "Epoch 6 step 790: training accuarcy: 0.998\n",
      "Epoch 6 step 790: training loss: 6.440036563895087\n",
      "Epoch 6 step 791: training accuarcy: 0.999\n",
      "Epoch 6 step 791: training loss: 11.693729942833087\n",
      "Epoch 6 step 792: training accuarcy: 0.9975\n",
      "Epoch 6 step 792: training loss: 10.560031394055459\n",
      "Epoch 6 step 793: training accuarcy: 0.998\n",
      "Epoch 6 step 793: training loss: 10.547433582405388\n",
      "Epoch 6 step 794: training accuarcy: 0.9985\n",
      "Epoch 6 step 794: training loss: 11.93777733322638\n",
      "Epoch 6 step 795: training accuarcy: 0.9965\n",
      "Epoch 6 step 795: training loss: 10.70969278930205\n",
      "Epoch 6 step 796: training accuarcy: 0.9975\n",
      "Epoch 6 step 796: training loss: 15.716952024805328\n",
      "Epoch 6 step 797: training accuarcy: 0.996\n",
      "Epoch 6 step 797: training loss: 5.500812989919306\n",
      "Epoch 6 step 798: training accuarcy: 0.999\n",
      "Epoch 6 step 798: training loss: 12.728756131534293\n",
      "Epoch 6 step 799: training accuarcy: 0.9965\n",
      "Epoch 6 step 799: training loss: 10.25627231814542\n",
      "Epoch 6 step 800: training accuarcy: 0.996\n",
      "Epoch 6 step 800: training loss: 7.212869098381563\n",
      "Epoch 6 step 801: training accuarcy: 0.9995\n",
      "Epoch 6 step 801: training loss: 10.505142599607469\n",
      "Epoch 6 step 802: training accuarcy: 0.9975\n",
      "Epoch 6 step 802: training loss: 10.758470570035376\n",
      "Epoch 6 step 803: training accuarcy: 0.9965\n",
      "Epoch 6 step 803: training loss: 9.3930098238767\n",
      "Epoch 6 step 804: training accuarcy: 0.9975\n",
      "Epoch 6 step 804: training loss: 12.758505054447264\n",
      "Epoch 6 step 805: training accuarcy: 0.9973794549266248\n",
      "Epoch 6: train loss 10.97082557080671, train accuarcy 0.9969789385795593\n",
      "Epoch 6: valid loss 10.483157009195178, valid accuarcy 0.9970855116844177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 7/8 [06:20<00:54, 54.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 805: training loss: 7.11458540766523\n",
      "Epoch 7 step 806: training accuarcy: 0.999\n",
      "Epoch 7 step 806: training loss: 13.44953423471516\n",
      "Epoch 7 step 807: training accuarcy: 0.9975\n",
      "Epoch 7 step 807: training loss: 13.380260211985263\n",
      "Epoch 7 step 808: training accuarcy: 0.9955\n",
      "Epoch 7 step 808: training loss: 6.594813322440129\n",
      "Epoch 7 step 809: training accuarcy: 0.9995\n",
      "Epoch 7 step 809: training loss: 10.091206933802434\n",
      "Epoch 7 step 810: training accuarcy: 0.998\n",
      "Epoch 7 step 810: training loss: 10.236788300835059\n",
      "Epoch 7 step 811: training accuarcy: 0.9975\n",
      "Epoch 7 step 811: training loss: 9.90358998516715\n",
      "Epoch 7 step 812: training accuarcy: 0.999\n",
      "Epoch 7 step 812: training loss: 14.426919063718644\n",
      "Epoch 7 step 813: training accuarcy: 0.9965\n",
      "Epoch 7 step 813: training loss: 7.373284149902315\n",
      "Epoch 7 step 814: training accuarcy: 0.999\n",
      "Epoch 7 step 814: training loss: 8.71742184036349\n",
      "Epoch 7 step 815: training accuarcy: 0.9985\n",
      "Epoch 7 step 815: training loss: 5.483928684422076\n",
      "Epoch 7 step 816: training accuarcy: 1.0\n",
      "Epoch 7 step 816: training loss: 12.295510195237966\n",
      "Epoch 7 step 817: training accuarcy: 0.9965\n",
      "Epoch 7 step 817: training loss: 15.856367956991448\n",
      "Epoch 7 step 818: training accuarcy: 0.996\n",
      "Epoch 7 step 818: training loss: 5.172160317696964\n",
      "Epoch 7 step 819: training accuarcy: 0.999\n",
      "Epoch 7 step 819: training loss: 9.876893648966876\n",
      "Epoch 7 step 820: training accuarcy: 0.9985\n",
      "Epoch 7 step 820: training loss: 13.042059096686309\n",
      "Epoch 7 step 821: training accuarcy: 0.9985\n",
      "Epoch 7 step 821: training loss: 10.781396614445681\n",
      "Epoch 7 step 822: training accuarcy: 0.998\n",
      "Epoch 7 step 822: training loss: 15.624378140665927\n",
      "Epoch 7 step 823: training accuarcy: 0.997\n",
      "Epoch 7 step 823: training loss: 7.110924507159961\n",
      "Epoch 7 step 824: training accuarcy: 0.9985\n",
      "Epoch 7 step 824: training loss: 13.552759389858636\n",
      "Epoch 7 step 825: training accuarcy: 0.997\n",
      "Epoch 7 step 825: training loss: 8.555597122559135\n",
      "Epoch 7 step 826: training accuarcy: 0.9975\n",
      "Epoch 7 step 826: training loss: 15.116325165208437\n",
      "Epoch 7 step 827: training accuarcy: 0.9965\n",
      "Epoch 7 step 827: training loss: 5.408367519782879\n",
      "Epoch 7 step 828: training accuarcy: 0.999\n",
      "Epoch 7 step 828: training loss: 8.573351855202421\n",
      "Epoch 7 step 829: training accuarcy: 0.998\n",
      "Epoch 7 step 829: training loss: 12.508969630004922\n",
      "Epoch 7 step 830: training accuarcy: 0.9975\n",
      "Epoch 7 step 830: training loss: 9.520446024820533\n",
      "Epoch 7 step 831: training accuarcy: 0.998\n",
      "Epoch 7 step 831: training loss: 6.510292843413844\n",
      "Epoch 7 step 832: training accuarcy: 0.999\n",
      "Epoch 7 step 832: training loss: 11.523640653689833\n",
      "Epoch 7 step 833: training accuarcy: 0.9975\n",
      "Epoch 7 step 833: training loss: 11.04340851906988\n",
      "Epoch 7 step 834: training accuarcy: 0.998\n",
      "Epoch 7 step 834: training loss: 20.755086557767335\n",
      "Epoch 7 step 835: training accuarcy: 0.9965\n",
      "Epoch 7 step 835: training loss: 12.467460553422471\n",
      "Epoch 7 step 836: training accuarcy: 0.996\n",
      "Epoch 7 step 836: training loss: 6.8136853607914425\n",
      "Epoch 7 step 837: training accuarcy: 0.9985\n",
      "Epoch 7 step 837: training loss: 8.130344389745202\n",
      "Epoch 7 step 838: training accuarcy: 0.999\n",
      "Epoch 7 step 838: training loss: 10.170235834534694\n",
      "Epoch 7 step 839: training accuarcy: 0.9975\n",
      "Epoch 7 step 839: training loss: 11.894071549749825\n",
      "Epoch 7 step 840: training accuarcy: 0.9965\n",
      "Epoch 7 step 840: training loss: 7.199705418553657\n",
      "Epoch 7 step 841: training accuarcy: 0.9985\n",
      "Epoch 7 step 841: training loss: 13.573117386188095\n",
      "Epoch 7 step 842: training accuarcy: 0.997\n",
      "Epoch 7 step 842: training loss: 11.21668390328166\n",
      "Epoch 7 step 843: training accuarcy: 0.998\n",
      "Epoch 7 step 843: training loss: 10.647087143055709\n",
      "Epoch 7 step 844: training accuarcy: 0.9975\n",
      "Epoch 7 step 844: training loss: 9.47491497375645\n",
      "Epoch 7 step 845: training accuarcy: 0.997\n",
      "Epoch 7 step 845: training loss: 9.538323906976693\n",
      "Epoch 7 step 846: training accuarcy: 0.998\n",
      "Epoch 7 step 846: training loss: 13.524690425591933\n",
      "Epoch 7 step 847: training accuarcy: 0.995\n",
      "Epoch 7 step 847: training loss: 16.981518150183298\n",
      "Epoch 7 step 848: training accuarcy: 0.9955\n",
      "Epoch 7 step 848: training loss: 7.319103948921564\n",
      "Epoch 7 step 849: training accuarcy: 0.9975\n",
      "Epoch 7 step 849: training loss: 11.164014994655895\n",
      "Epoch 7 step 850: training accuarcy: 0.9985\n",
      "Epoch 7 step 850: training loss: 10.414358005441418\n",
      "Epoch 7 step 851: training accuarcy: 0.9965\n",
      "Epoch 7 step 851: training loss: 9.918594955833745\n",
      "Epoch 7 step 852: training accuarcy: 0.998\n",
      "Epoch 7 step 852: training loss: 12.261195962924427\n",
      "Epoch 7 step 853: training accuarcy: 0.997\n",
      "Epoch 7 step 853: training loss: 13.425730586613467\n",
      "Epoch 7 step 854: training accuarcy: 0.9975\n",
      "Epoch 7 step 854: training loss: 10.57570950119105\n",
      "Epoch 7 step 855: training accuarcy: 0.9975\n",
      "Epoch 7 step 855: training loss: 11.069062185626885\n",
      "Epoch 7 step 856: training accuarcy: 0.997\n",
      "Epoch 7 step 856: training loss: 18.160364093911177\n",
      "Epoch 7 step 857: training accuarcy: 0.996\n",
      "Epoch 7 step 857: training loss: 9.561666431085188\n",
      "Epoch 7 step 858: training accuarcy: 0.997\n",
      "Epoch 7 step 858: training loss: 7.342670771146053\n",
      "Epoch 7 step 859: training accuarcy: 0.9975\n",
      "Epoch 7 step 859: training loss: 9.742670339962595\n",
      "Epoch 7 step 860: training accuarcy: 0.997\n",
      "Epoch 7 step 860: training loss: 8.566838903410012\n",
      "Epoch 7 step 861: training accuarcy: 0.9985\n",
      "Epoch 7 step 861: training loss: 6.358990168031187\n",
      "Epoch 7 step 862: training accuarcy: 0.9985\n",
      "Epoch 7 step 862: training loss: 9.041797358567145\n",
      "Epoch 7 step 863: training accuarcy: 0.998\n",
      "Epoch 7 step 863: training loss: 14.213124059061185\n",
      "Epoch 7 step 864: training accuarcy: 0.997\n",
      "Epoch 7 step 864: training loss: 11.082483022958462\n",
      "Epoch 7 step 865: training accuarcy: 0.9975\n",
      "Epoch 7 step 865: training loss: 8.247413408643814\n",
      "Epoch 7 step 866: training accuarcy: 0.999\n",
      "Epoch 7 step 866: training loss: 12.078108081284071\n",
      "Epoch 7 step 867: training accuarcy: 0.9975\n",
      "Epoch 7 step 867: training loss: 10.596590871682833\n",
      "Epoch 7 step 868: training accuarcy: 0.9975\n",
      "Epoch 7 step 868: training loss: 6.934880757429806\n",
      "Epoch 7 step 869: training accuarcy: 0.999\n",
      "Epoch 7 step 869: training loss: 9.460873099555522\n",
      "Epoch 7 step 870: training accuarcy: 0.9975\n",
      "Epoch 7 step 870: training loss: 4.0704813505078326\n",
      "Epoch 7 step 871: training accuarcy: 0.9995\n",
      "Epoch 7 step 871: training loss: 13.966295128539105\n",
      "Epoch 7 step 872: training accuarcy: 0.9975\n",
      "Epoch 7 step 872: training loss: 13.10326463272261\n",
      "Epoch 7 step 873: training accuarcy: 0.997\n",
      "Epoch 7 step 873: training loss: 8.275268257084537\n",
      "Epoch 7 step 874: training accuarcy: 0.999\n",
      "Epoch 7 step 874: training loss: 12.53892320358397\n",
      "Epoch 7 step 875: training accuarcy: 0.997\n",
      "Epoch 7 step 875: training loss: 8.466276377740042\n",
      "Epoch 7 step 876: training accuarcy: 0.9995\n",
      "Epoch 7 step 876: training loss: 7.133187009760844\n",
      "Epoch 7 step 877: training accuarcy: 0.9975\n",
      "Epoch 7 step 877: training loss: 15.235156620673107\n",
      "Epoch 7 step 878: training accuarcy: 0.9975\n",
      "Epoch 7 step 878: training loss: 9.658387083500935\n",
      "Epoch 7 step 879: training accuarcy: 0.9985\n",
      "Epoch 7 step 879: training loss: 6.554369409725832\n",
      "Epoch 7 step 880: training accuarcy: 0.999\n",
      "Epoch 7 step 880: training loss: 8.035872292255059\n",
      "Epoch 7 step 881: training accuarcy: 0.999\n",
      "Epoch 7 step 881: training loss: 9.798329733377217\n",
      "Epoch 7 step 882: training accuarcy: 0.998\n",
      "Epoch 7 step 882: training loss: 15.321819607314765\n",
      "Epoch 7 step 883: training accuarcy: 0.9965\n",
      "Epoch 7 step 883: training loss: 8.190620433302405\n",
      "Epoch 7 step 884: training accuarcy: 0.9985\n",
      "Epoch 7 step 884: training loss: 10.910099693209549\n",
      "Epoch 7 step 885: training accuarcy: 0.9975\n",
      "Epoch 7 step 885: training loss: 12.460511141696731\n",
      "Epoch 7 step 886: training accuarcy: 0.9975\n",
      "Epoch 7 step 886: training loss: 8.004801190764159\n",
      "Epoch 7 step 887: training accuarcy: 0.999\n",
      "Epoch 7 step 887: training loss: 12.967581443499824\n",
      "Epoch 7 step 888: training accuarcy: 0.9975\n",
      "Epoch 7 step 888: training loss: 8.800963431588444\n",
      "Epoch 7 step 889: training accuarcy: 0.997\n",
      "Epoch 7 step 889: training loss: 15.2904457395851\n",
      "Epoch 7 step 890: training accuarcy: 0.9965\n",
      "Epoch 7 step 890: training loss: 13.09674906121127\n",
      "Epoch 7 step 891: training accuarcy: 0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 891: training loss: 12.622982812292847\n",
      "Epoch 7 step 892: training accuarcy: 0.997\n",
      "Epoch 7 step 892: training loss: 13.019636853345306\n",
      "Epoch 7 step 893: training accuarcy: 0.997\n",
      "Epoch 7 step 893: training loss: 7.6545537319614825\n",
      "Epoch 7 step 894: training accuarcy: 0.9975\n",
      "Epoch 7 step 894: training loss: 6.861583488200544\n",
      "Epoch 7 step 895: training accuarcy: 0.999\n",
      "Epoch 7 step 895: training loss: 9.571752318098444\n",
      "Epoch 7 step 896: training accuarcy: 0.9985\n",
      "Epoch 7 step 896: training loss: 15.39481397733446\n",
      "Epoch 7 step 897: training accuarcy: 0.995\n",
      "Epoch 7 step 897: training loss: 10.93101306955965\n",
      "Epoch 7 step 898: training accuarcy: 0.998\n",
      "Epoch 7 step 898: training loss: 6.983164075279247\n",
      "Epoch 7 step 899: training accuarcy: 0.999\n",
      "Epoch 7 step 899: training loss: 10.972670067328423\n",
      "Epoch 7 step 900: training accuarcy: 0.9975\n",
      "Epoch 7 step 900: training loss: 7.1518080557438175\n",
      "Epoch 7 step 901: training accuarcy: 0.9985\n",
      "Epoch 7 step 901: training loss: 5.350563701460016\n",
      "Epoch 7 step 902: training accuarcy: 0.9995\n",
      "Epoch 7 step 902: training loss: 12.348799446668238\n",
      "Epoch 7 step 903: training accuarcy: 0.997\n",
      "Epoch 7 step 903: training loss: 13.20694576237152\n",
      "Epoch 7 step 904: training accuarcy: 0.997\n",
      "Epoch 7 step 904: training loss: 9.706071934853002\n",
      "Epoch 7 step 905: training accuarcy: 0.9975\n",
      "Epoch 7 step 905: training loss: 11.78868105069528\n",
      "Epoch 7 step 906: training accuarcy: 0.9975\n",
      "Epoch 7 step 906: training loss: 8.446949656901628\n",
      "Epoch 7 step 907: training accuarcy: 0.9975\n",
      "Epoch 7 step 907: training loss: 12.73294423810058\n",
      "Epoch 7 step 908: training accuarcy: 0.9965\n",
      "Epoch 7 step 908: training loss: 11.362236242181101\n",
      "Epoch 7 step 909: training accuarcy: 0.997\n",
      "Epoch 7 step 909: training loss: 14.70149527025868\n",
      "Epoch 7 step 910: training accuarcy: 0.9965\n",
      "Epoch 7 step 910: training loss: 14.819905337329095\n",
      "Epoch 7 step 911: training accuarcy: 0.997\n",
      "Epoch 7 step 911: training loss: 13.504357075600732\n",
      "Epoch 7 step 912: training accuarcy: 0.9975\n",
      "Epoch 7 step 912: training loss: 11.996825151373912\n",
      "Epoch 7 step 913: training accuarcy: 0.996\n",
      "Epoch 7 step 913: training loss: 12.21244215849651\n",
      "Epoch 7 step 914: training accuarcy: 0.9975\n",
      "Epoch 7 step 914: training loss: 7.941804188192784\n",
      "Epoch 7 step 915: training accuarcy: 0.9985\n",
      "Epoch 7 step 915: training loss: 5.964595553896155\n",
      "Epoch 7 step 916: training accuarcy: 0.999\n",
      "Epoch 7 step 916: training loss: 6.940618167575957\n",
      "Epoch 7 step 917: training accuarcy: 0.999\n",
      "Epoch 7 step 917: training loss: 7.411838864555932\n",
      "Epoch 7 step 918: training accuarcy: 0.999\n",
      "Epoch 7 step 918: training loss: 11.985160926623502\n",
      "Epoch 7 step 919: training accuarcy: 0.9975\n",
      "Epoch 7 step 919: training loss: 10.24357662023412\n",
      "Epoch 7 step 920: training accuarcy: 0.9984276729559749\n",
      "Epoch 7: train loss 10.560674269879744, train accuarcy 0.9967600703239441\n",
      "Epoch 7: valid loss 7.916754974241444, valid accuarcy 0.9981784224510193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [07:15<00:00, 54.35s/it]\n"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=8,\n",
    "                  loss_callback=trans_loss_callback,\n",
    "                  log_dir=get_log_dir('kaggle', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
