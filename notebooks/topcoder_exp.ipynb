{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:01:30.581717Z",
     "start_time": "2019-09-25T14:01:30.456659Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:11.453093Z",
     "start_time": "2019-10-17T13:40:11.447095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\python\\recommender\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:12.744093Z",
     "start_time": "2019-10-17T13:40:12.119109Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import get_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:14.350091Z",
     "start_time": "2019-10-17T13:40:12.745093Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import SeqTopcoder\n",
    "from models import FMLearner, TorchFM, TorchHrmFM, TorchPrmeFM, TorchTransFM, TransProbFM\n",
    "from models.fm_learner import simple_loss, trans_loss, simple_weight_loss, trans_weight_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:14.557092Z",
     "start_time": "2019-10-17T13:40:14.351091Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = T.cuda.current_device()\n",
    "BATCH = 2000\n",
    "SHUFFLE = True\n",
    "WORKERS = 0\n",
    "NEG_SAMPLE = 5\n",
    "REGS_PATH = Path(\"./inputs/topcoder/regs.csv\")\n",
    "CHAG_PATH = Path(\"./inputs/topcoder/challenge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:17.328091Z",
     "start_time": "2019-10-17T13:40:14.926093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset in inputs\\topcoder\\regs.csv\n",
      "Original regs shape: (610025, 3)\n",
      "Original registants size: 60017\n",
      "Original challenges size: 39916\n",
      "Filter dataframe shape: (544568, 3)\n",
      "Index(['challengeId', 'period', 'date', 'prizes', 'technologies', 'platforms'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datasets.torch_topcoder.SeqTopcoder at 0x1dd0946fb88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = SeqTopcoder(regs_path=REGS_PATH, chag_path=CHAG_PATH)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:17.334092Z",
     "start_time": "2019-10-17T13:40:17.330093Z"
    }
   },
   "outputs": [],
   "source": [
    "db.config_db(batch_size=BATCH,\n",
    "             shuffle=SHUFFLE,\n",
    "             num_workers=WORKERS,\n",
    "             device=DEVICE,\n",
    "             neg_sample=NEG_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:17.585094Z",
     "start_time": "2019-10-17T13:40:17.582106Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1\n",
    "INIT_SCALE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:18.314092Z",
     "start_time": "2019-10-17T13:40:18.312092Z"
    }
   },
   "outputs": [],
   "source": [
    "# regst setting\n",
    "LINEAR_REG = 1\n",
    "EMB_REG = 1\n",
    "TRANS_REG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:19.691094Z",
     "start_time": "2019-10-17T13:40:19.687094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_loss at 0x000001DD093D2AF8>, 1, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_loss_callback = partial(simple_loss, LINEAR_REG, EMB_REG)\n",
    "simple_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:19.972093Z",
     "start_time": "2019-10-17T13:40:19.967095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_loss at 0x000001DD094671F8>, 1, 1, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_loss_callback = partial(trans_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:20.219097Z",
     "start_time": "2019-10-17T13:40:20.214093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_weight_loss at 0x000001DD09467288>, 1, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weight_loss_callback = partial(simple_weight_loss, LINEAR_REG, EMB_REG)\n",
    "simple_weight_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:20.476094Z",
     "start_time": "2019-10-17T13:40:20.470093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_weight_loss at 0x000001DD09467318>, 1, 1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_weight_loss_callback = partial(trans_weight_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_weight_loss_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "#### Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:23.514092Z",
     "start_time": "2019-10-17T13:40:23.510093Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1\n",
    "INIT_SCALE = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:24.418091Z",
     "start_time": "2019-10-17T13:40:24.414092Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DECAY_FREQ = 1000\n",
    "DECAY_GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:33:38.947942Z",
     "start_time": "2019-10-17T07:33:38.692942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchFM()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model = TorchFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:37.388025Z",
     "start_time": "2019-10-09T11:41:37.384028Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(fm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:38.693328Z",
     "start_time": "2019-10-09T11:41:38.648293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1939e327ba8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_learner = FMLearner(fm_model, adam_opt, schedular, db)\n",
    "fm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:25.856807Z",
     "start_time": "2019-10-07T13:48:25.853806Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:26:24.690026Z",
     "start_time": "2019-10-09T11:26:24.687055Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='seq',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:40.324650Z",
     "start_time": "2019-10-09T11:41:40.320652Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='seq',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:19.805716Z",
     "start_time": "2019-10-09T11:26:29.768670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 46546.154640854336\n",
      "Epoch 0 step 1: training accuarcy: 0.36760000000000004\n",
      "Epoch 0 step 1: training loss: 44140.504792487875\n",
      "Epoch 0 step 2: training accuarcy: 0.4319\n",
      "Epoch 0 step 2: training loss: 41675.07565998279\n",
      "Epoch 0 step 3: training accuarcy: 0.5108\n",
      "Epoch 0 step 3: training loss: 39450.99958670329\n",
      "Epoch 0 step 4: training accuarcy: 0.5817\n",
      "Epoch 0 step 4: training loss: 37480.27146773438\n",
      "Epoch 0 step 5: training accuarcy: 0.6623\n",
      "Epoch 0 step 5: training loss: 35875.01403513422\n",
      "Epoch 0 step 6: training accuarcy: 0.7083\n",
      "Epoch 0 step 6: training loss: 34544.81596775044\n",
      "Epoch 0 step 7: training accuarcy: 0.7407\n",
      "Epoch 0 step 7: training loss: 33181.54690224211\n",
      "Epoch 0 step 8: training accuarcy: 0.7913\n",
      "Epoch 0 step 8: training loss: 32084.20438950817\n",
      "Epoch 0 step 9: training accuarcy: 0.8089000000000001\n",
      "Epoch 0 step 9: training loss: 31093.71356452987\n",
      "Epoch 0 step 10: training accuarcy: 0.8297\n",
      "Epoch 0 step 10: training loss: 30231.732255741055\n",
      "Epoch 0 step 11: training accuarcy: 0.8453\n",
      "Epoch 0 step 11: training loss: 29520.315584328186\n",
      "Epoch 0 step 12: training accuarcy: 0.8402000000000001\n",
      "Epoch 0 step 12: training loss: 28884.354913167757\n",
      "Epoch 0 step 13: training accuarcy: 0.8398\n",
      "Epoch 0 step 13: training loss: 27905.745133619086\n",
      "Epoch 0 step 14: training accuarcy: 0.8664000000000001\n",
      "Epoch 0 step 14: training loss: 27171.315296167573\n",
      "Epoch 0 step 15: training accuarcy: 0.8752000000000001\n",
      "Epoch 0 step 15: training loss: 26567.28917460179\n",
      "Epoch 0 step 16: training accuarcy: 0.871\n",
      "Epoch 0 step 16: training loss: 25935.438804682497\n",
      "Epoch 0 step 17: training accuarcy: 0.8761\n",
      "Epoch 0 step 17: training loss: 25063.242458599463\n",
      "Epoch 0 step 18: training accuarcy: 0.9003\n",
      "Epoch 0 step 18: training loss: 24662.890787161403\n",
      "Epoch 0 step 19: training accuarcy: 0.8833000000000001\n",
      "Epoch 0 step 19: training loss: 24150.429451553682\n",
      "Epoch 0 step 20: training accuarcy: 0.8882\n",
      "Epoch 0 step 20: training loss: 23338.00173500339\n",
      "Epoch 0 step 21: training accuarcy: 0.9049\n",
      "Epoch 0 step 21: training loss: 22915.09829797726\n",
      "Epoch 0 step 22: training accuarcy: 0.8966000000000001\n",
      "Epoch 0 step 22: training loss: 22218.21218858137\n",
      "Epoch 0 step 23: training accuarcy: 0.9137000000000001\n",
      "Epoch 0 step 23: training loss: 21826.613483607995\n",
      "Epoch 0 step 24: training accuarcy: 0.9023\n",
      "Epoch 0 step 24: training loss: 21293.742965627564\n",
      "Epoch 0 step 25: training accuarcy: 0.9092\n",
      "Epoch 0 step 25: training loss: 20760.689774483093\n",
      "Epoch 0 step 26: training accuarcy: 0.9136000000000001\n",
      "Epoch 0 step 26: training loss: 20263.351203293496\n",
      "Epoch 0 step 27: training accuarcy: 0.9203\n",
      "Epoch 0 step 27: training loss: 19720.54019230831\n",
      "Epoch 0 step 28: training accuarcy: 0.9247000000000001\n",
      "Epoch 0 step 28: training loss: 19490.808730795514\n",
      "Epoch 0 step 29: training accuarcy: 0.9159\n",
      "Epoch 0 step 29: training loss: 18836.18159112953\n",
      "Epoch 0 step 30: training accuarcy: 0.9322\n",
      "Epoch 0 step 30: training loss: 18458.397152266683\n",
      "Epoch 0 step 31: training accuarcy: 0.9288000000000001\n",
      "Epoch 0 step 31: training loss: 18164.47932100472\n",
      "Epoch 0 step 32: training accuarcy: 0.9165000000000001\n",
      "Epoch 0 step 32: training loss: 17732.237286048134\n",
      "Epoch 0 step 33: training accuarcy: 0.9246000000000001\n",
      "Epoch 0 step 33: training loss: 17333.765721345455\n",
      "Epoch 0 step 34: training accuarcy: 0.9298000000000001\n",
      "Epoch 0 step 34: training loss: 16927.19440547125\n",
      "Epoch 0 step 35: training accuarcy: 0.9311\n",
      "Epoch 0 step 35: training loss: 16573.69296637209\n",
      "Epoch 0 step 36: training accuarcy: 0.9331\n",
      "Epoch 0 step 36: training loss: 16151.36839337222\n",
      "Epoch 0 step 37: training accuarcy: 0.9404\n",
      "Epoch 0 step 37: training loss: 15708.833770822308\n",
      "Epoch 0 step 38: training accuarcy: 0.9489000000000001\n",
      "Epoch 0 step 38: training loss: 15564.171523687444\n",
      "Epoch 0 step 39: training accuarcy: 0.9311\n",
      "Epoch 0 step 39: training loss: 15121.363922667972\n",
      "Epoch 0 step 40: training accuarcy: 0.9445\n",
      "Epoch 0 step 40: training loss: 14819.06057104719\n",
      "Epoch 0 step 41: training accuarcy: 0.9409000000000001\n",
      "Epoch 0 step 41: training loss: 14503.302605305325\n",
      "Epoch 0 step 42: training accuarcy: 0.9477000000000001\n",
      "Epoch 0 step 42: training loss: 14188.560423289051\n",
      "Epoch 0 step 43: training accuarcy: 0.9499000000000001\n",
      "Epoch 0 step 43: training loss: 13981.649054906664\n",
      "Epoch 0 step 44: training accuarcy: 0.9471\n",
      "Epoch 0 step 44: training loss: 13697.81775616869\n",
      "Epoch 0 step 45: training accuarcy: 0.9496\n",
      "Epoch 0 step 45: training loss: 13390.965856939478\n",
      "Epoch 0 step 46: training accuarcy: 0.9445\n",
      "Epoch 0 step 46: training loss: 13072.525224967161\n",
      "Epoch 0 step 47: training accuarcy: 0.9561000000000001\n",
      "Epoch 0 step 47: training loss: 12863.386391707756\n",
      "Epoch 0 step 48: training accuarcy: 0.9534\n",
      "Epoch 0 step 48: training loss: 12517.594751065391\n",
      "Epoch 0 step 49: training accuarcy: 0.9599000000000001\n",
      "Epoch 0 step 49: training loss: 12277.26593375544\n",
      "Epoch 0 step 50: training accuarcy: 0.9607\n",
      "Epoch 0 step 50: training loss: 12023.484709600978\n",
      "Epoch 0 step 51: training accuarcy: 0.9603\n",
      "Epoch 0 step 51: training loss: 11871.820783373429\n",
      "Epoch 0 step 52: training accuarcy: 0.9522\n",
      "Epoch 0 step 52: training loss: 11665.736990592095\n",
      "Epoch 0 step 53: training accuarcy: 0.9529000000000001\n",
      "Epoch 0 step 53: training loss: 11418.759232387505\n",
      "Epoch 0 step 54: training accuarcy: 0.9574\n",
      "Epoch 0 step 54: training loss: 11149.857302320948\n",
      "Epoch 0 step 55: training accuarcy: 0.9602\n",
      "Epoch 0 step 55: training loss: 10964.472566560516\n",
      "Epoch 0 step 56: training accuarcy: 0.9602\n",
      "Epoch 0 step 56: training loss: 10831.934317620402\n",
      "Epoch 0 step 57: training accuarcy: 0.9546\n",
      "Epoch 0 step 57: training loss: 10655.3275359064\n",
      "Epoch 0 step 58: training accuarcy: 0.9558000000000001\n",
      "Epoch 0 step 58: training loss: 10409.00993191277\n",
      "Epoch 0 step 59: training accuarcy: 0.9606\n",
      "Epoch 0 step 59: training loss: 10190.126736025262\n",
      "Epoch 0 step 60: training accuarcy: 0.9647\n",
      "Epoch 0 step 60: training loss: 10040.85403106833\n",
      "Epoch 0 step 61: training accuarcy: 0.9598000000000001\n",
      "Epoch 0 step 61: training loss: 9859.918656030142\n",
      "Epoch 0 step 62: training accuarcy: 0.9586\n",
      "Epoch 0 step 62: training loss: 9623.71167509555\n",
      "Epoch 0 step 63: training accuarcy: 0.9632000000000001\n",
      "Epoch 0 step 63: training loss: 9491.337163915134\n",
      "Epoch 0 step 64: training accuarcy: 0.9679000000000001\n",
      "Epoch 0 step 64: training loss: 9257.10484107236\n",
      "Epoch 0 step 65: training accuarcy: 0.9707\n",
      "Epoch 0 step 65: training loss: 9100.489427533488\n",
      "Epoch 0 step 66: training accuarcy: 0.9677\n",
      "Epoch 0 step 66: training loss: 9061.441009053218\n",
      "Epoch 0 step 67: training accuarcy: 0.9622\n",
      "Epoch 0 step 67: training loss: 8865.98514064014\n",
      "Epoch 0 step 68: training accuarcy: 0.9627\n",
      "Epoch 0 step 68: training loss: 8671.758932109413\n",
      "Epoch 0 step 69: training accuarcy: 0.9698\n",
      "Epoch 0 step 69: training loss: 8527.474412722964\n",
      "Epoch 0 step 70: training accuarcy: 0.9688\n",
      "Epoch 0 step 70: training loss: 8360.770416182342\n",
      "Epoch 0 step 71: training accuarcy: 0.9711000000000001\n",
      "Epoch 0 step 71: training loss: 8270.345836047847\n",
      "Epoch 0 step 72: training accuarcy: 0.9677\n",
      "Epoch 0 step 72: training loss: 8154.891683617339\n",
      "Epoch 0 step 73: training accuarcy: 0.9698\n",
      "Epoch 0 step 73: training loss: 8056.6704549640335\n",
      "Epoch 0 step 74: training accuarcy: 0.9666\n",
      "Epoch 0 step 74: training loss: 7895.8409671925565\n",
      "Epoch 0 step 75: training accuarcy: 0.9681000000000001\n",
      "Epoch 0 step 75: training loss: 7669.371218413247\n",
      "Epoch 0 step 76: training accuarcy: 0.9766\n",
      "Epoch 0 step 76: training loss: 7695.470488851995\n",
      "Epoch 0 step 77: training accuarcy: 0.9668\n",
      "Epoch 0 step 77: training loss: 7537.177961282316\n",
      "Epoch 0 step 78: training accuarcy: 0.9702000000000001\n",
      "Epoch 0 step 78: training loss: 7424.542323741191\n",
      "Epoch 0 step 79: training accuarcy: 0.9705\n",
      "Epoch 0 step 79: training loss: 7258.05852192447\n",
      "Epoch 0 step 80: training accuarcy: 0.9749000000000001\n",
      "Epoch 0 step 80: training loss: 7123.409775129712\n",
      "Epoch 0 step 81: training accuarcy: 0.9774\n",
      "Epoch 0 step 81: training loss: 7149.951348710446\n",
      "Epoch 0 step 82: training accuarcy: 0.9692000000000001\n",
      "Epoch 0 step 82: training loss: 6994.272865394437\n",
      "Epoch 0 step 83: training accuarcy: 0.9710000000000001\n",
      "Epoch 0 step 83: training loss: 6872.508768189162\n",
      "Epoch 0 step 84: training accuarcy: 0.9709000000000001\n",
      "Epoch 0 step 84: training loss: 6749.962539922964\n",
      "Epoch 0 step 85: training accuarcy: 0.9766\n",
      "Epoch 0 step 85: training loss: 6750.392220424044\n",
      "Epoch 0 step 86: training accuarcy: 0.9708\n",
      "Epoch 0 step 86: training loss: 6635.5681847329215\n",
      "Epoch 0 step 87: training accuarcy: 0.9713\n",
      "Epoch 0 step 87: training loss: 6420.73477351625\n",
      "Epoch 0 step 88: training accuarcy: 0.9803000000000001\n",
      "Epoch 0 step 88: training loss: 6430.323353674918\n",
      "Epoch 0 step 89: training accuarcy: 0.9726\n",
      "Epoch 0 step 89: training loss: 6281.644858462743\n",
      "Epoch 0 step 90: training accuarcy: 0.9779\n",
      "Epoch 0 step 90: training loss: 6135.84968099944\n",
      "Epoch 0 step 91: training accuarcy: 0.9788\n",
      "Epoch 0 step 91: training loss: 6184.901493894874\n",
      "Epoch 0 step 92: training accuarcy: 0.9747\n",
      "Epoch 0 step 92: training loss: 6092.918878513035\n",
      "Epoch 0 step 93: training accuarcy: 0.9746\n",
      "Epoch 0 step 93: training loss: 5974.349425806837\n",
      "Epoch 0 step 94: training accuarcy: 0.9783000000000001\n",
      "Epoch 0 step 94: training loss: 5904.541500218362\n",
      "Epoch 0 step 95: training accuarcy: 0.9772000000000001\n",
      "Epoch 0 step 95: training loss: 5847.7118776709285\n",
      "Epoch 0 step 96: training accuarcy: 0.9732000000000001\n",
      "Epoch 0 step 96: training loss: 5775.329463710406\n",
      "Epoch 0 step 97: training accuarcy: 0.9745\n",
      "Epoch 0 step 97: training loss: 5614.9177729564535\n",
      "Epoch 0 step 98: training accuarcy: 0.9806\n",
      "Epoch 0 step 98: training loss: 5633.57121793618\n",
      "Epoch 0 step 99: training accuarcy: 0.9728\n",
      "Epoch 0 step 99: training loss: 5540.982463589767\n",
      "Epoch 0 step 100: training accuarcy: 0.9762000000000001\n",
      "Epoch 0 step 100: training loss: 5435.732534662382\n",
      "Epoch 0 step 101: training accuarcy: 0.9811000000000001\n",
      "Epoch 0 step 101: training loss: 5372.06910012488\n",
      "Epoch 0 step 102: training accuarcy: 0.9809\n",
      "Epoch 0 step 102: training loss: 5383.64370447561\n",
      "Epoch 0 step 103: training accuarcy: 0.9714\n",
      "Epoch 0 step 103: training loss: 5256.06309815715\n",
      "Epoch 0 step 104: training accuarcy: 0.9756\n",
      "Epoch 0 step 104: training loss: 5188.328238497314\n",
      "Epoch 0 step 105: training accuarcy: 0.9777\n",
      "Epoch 0 step 105: training loss: 5102.182009835425\n",
      "Epoch 0 step 106: training accuarcy: 0.9812000000000001\n",
      "Epoch 0 step 106: training loss: 5001.069988832107\n",
      "Epoch 0 step 107: training accuarcy: 0.9841000000000001\n",
      "Epoch 0 step 107: training loss: 5025.377163797662\n",
      "Epoch 0 step 108: training accuarcy: 0.9767\n",
      "Epoch 0 step 108: training loss: 4933.676857219854\n",
      "Epoch 0 step 109: training accuarcy: 0.9796\n",
      "Epoch 0 step 109: training loss: 4887.736419090107\n",
      "Epoch 0 step 110: training accuarcy: 0.9780000000000001\n",
      "Epoch 0 step 110: training loss: 4817.331432497175\n",
      "Epoch 0 step 111: training accuarcy: 0.9814\n",
      "Epoch 0 step 111: training loss: 4756.09957185815\n",
      "Epoch 0 step 112: training accuarcy: 0.9830000000000001\n",
      "Epoch 0 step 112: training loss: 4624.585910760974\n",
      "Epoch 0 step 113: training accuarcy: 0.9857\n",
      "Epoch 0 step 113: training loss: 4547.303001172361\n",
      "Epoch 0 step 114: training accuarcy: 0.9888\n",
      "Epoch 0 step 114: training loss: 4630.402063804201\n",
      "Epoch 0 step 115: training accuarcy: 0.9783000000000001\n",
      "Epoch 0 step 115: training loss: 4511.174952655375\n",
      "Epoch 0 step 116: training accuarcy: 0.9826\n",
      "Epoch 0 step 116: training loss: 4494.662926032201\n",
      "Epoch 0 step 117: training accuarcy: 0.9816\n",
      "Epoch 0 step 117: training loss: 4505.873360218991\n",
      "Epoch 0 step 118: training accuarcy: 0.9788\n",
      "Epoch 0 step 118: training loss: 4380.222573111397\n",
      "Epoch 0 step 119: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 119: training loss: 4340.90247958169\n",
      "Epoch 0 step 120: training accuarcy: 0.9813000000000001\n",
      "Epoch 0 step 120: training loss: 4310.211258663809\n",
      "Epoch 0 step 121: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 121: training loss: 4212.357510685064\n",
      "Epoch 0 step 122: training accuarcy: 0.9834\n",
      "Epoch 0 step 122: training loss: 4273.310672992935\n",
      "Epoch 0 step 123: training accuarcy: 0.9777\n",
      "Epoch 0 step 123: training loss: 4158.980775446714\n",
      "Epoch 0 step 124: training accuarcy: 0.9826\n",
      "Epoch 0 step 124: training loss: 4069.474364180968\n",
      "Epoch 0 step 125: training accuarcy: 0.9861000000000001\n",
      "Epoch 0 step 125: training loss: 4032.709004971245\n",
      "Epoch 0 step 126: training accuarcy: 0.9864\n",
      "Epoch 0 step 126: training loss: 4068.9175125624083\n",
      "Epoch 0 step 127: training accuarcy: 0.9791000000000001\n",
      "Epoch 0 step 127: training loss: 4021.514799095072\n",
      "Epoch 0 step 128: training accuarcy: 0.9767\n",
      "Epoch 0 step 128: training loss: 3897.2253326967566\n",
      "Epoch 0 step 129: training accuarcy: 0.9870000000000001\n",
      "Epoch 0 step 129: training loss: 3934.829396123743\n",
      "Epoch 0 step 130: training accuarcy: 0.9807\n",
      "Epoch 0 step 130: training loss: 3871.6727901103254\n",
      "Epoch 0 step 131: training accuarcy: 0.9812000000000001\n",
      "Epoch 0 step 131: training loss: 3776.247236149556\n",
      "Epoch 0 step 132: training accuarcy: 0.9855\n",
      "Epoch 0 step 132: training loss: 3742.22935360402\n",
      "Epoch 0 step 133: training accuarcy: 0.9833000000000001\n",
      "Epoch 0 step 133: training loss: 3747.3275383411055\n",
      "Epoch 0 step 134: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 134: training loss: 3702.493218446947\n",
      "Epoch 0 step 135: training accuarcy: 0.9821000000000001\n",
      "Epoch 0 step 135: training loss: 3634.5809583590785\n",
      "Epoch 0 step 136: training accuarcy: 0.9848\n",
      "Epoch 0 step 136: training loss: 3577.2521160519977\n",
      "Epoch 0 step 137: training accuarcy: 0.9869\n",
      "Epoch 0 step 137: training loss: 3614.7615144124775\n",
      "Epoch 0 step 138: training accuarcy: 0.9828\n",
      "Epoch 0 step 138: training loss: 3516.497603060079\n",
      "Epoch 0 step 139: training accuarcy: 0.9879\n",
      "Epoch 0 step 139: training loss: 3486.2250470360614\n",
      "Epoch 0 step 140: training accuarcy: 0.9858\n",
      "Epoch 0 step 140: training loss: 3470.5107341025646\n",
      "Epoch 0 step 141: training accuarcy: 0.9846\n",
      "Epoch 0 step 141: training loss: 3408.3581450087686\n",
      "Epoch 0 step 142: training accuarcy: 0.9847\n",
      "Epoch 0 step 142: training loss: 3387.5121543324985\n",
      "Epoch 0 step 143: training accuarcy: 0.9882000000000001\n",
      "Epoch 0 step 143: training loss: 3368.030965761244\n",
      "Epoch 0 step 144: training accuarcy: 0.9861000000000001\n",
      "Epoch 0 step 144: training loss: 3317.2066941266335\n",
      "Epoch 0 step 145: training accuarcy: 0.9838\n",
      "Epoch 0 step 145: training loss: 3369.8291854380127\n",
      "Epoch 0 step 146: training accuarcy: 0.9804\n",
      "Epoch 0 step 146: training loss: 3257.679084437724\n",
      "Epoch 0 step 147: training accuarcy: 0.9858\n",
      "Epoch 0 step 147: training loss: 3237.699959532336\n",
      "Epoch 0 step 148: training accuarcy: 0.9860000000000001\n",
      "Epoch 0 step 148: training loss: 3163.171903962855\n",
      "Epoch 0 step 149: training accuarcy: 0.9854\n",
      "Epoch 0 step 149: training loss: 3136.522401951986\n",
      "Epoch 0 step 150: training accuarcy: 0.9866\n",
      "Epoch 0 step 150: training loss: 3080.362407065442\n",
      "Epoch 0 step 151: training accuarcy: 0.9874\n",
      "Epoch 0 step 151: training loss: 3044.605643573601\n",
      "Epoch 0 step 152: training accuarcy: 0.9869\n",
      "Epoch 0 step 152: training loss: 3040.24860202671\n",
      "Epoch 0 step 153: training accuarcy: 0.9883000000000001\n",
      "Epoch 0 step 153: training loss: 3030.757933014829\n",
      "Epoch 0 step 154: training accuarcy: 0.9866\n",
      "Epoch 0 step 154: training loss: 2996.7602216914643\n",
      "Epoch 0 step 155: training accuarcy: 0.9871000000000001\n",
      "Epoch 0 step 155: training loss: 2996.6829223189598\n",
      "Epoch 0 step 156: training accuarcy: 0.9867\n",
      "Epoch 0 step 156: training loss: 2953.0852356220576\n",
      "Epoch 0 step 157: training accuarcy: 0.9863000000000001\n",
      "Epoch 0 step 157: training loss: 2913.206685985385\n",
      "Epoch 0 step 158: training accuarcy: 0.9853000000000001\n",
      "Epoch 0 step 158: training loss: 2893.467817648129\n",
      "Epoch 0 step 159: training accuarcy: 0.9851000000000001\n",
      "Epoch 0 step 159: training loss: 2840.0471484972936\n",
      "Epoch 0 step 160: training accuarcy: 0.9865\n",
      "Epoch 0 step 160: training loss: 2858.0709469488747\n",
      "Epoch 0 step 161: training accuarcy: 0.9849\n",
      "Epoch 0 step 161: training loss: 2754.3068372058665\n",
      "Epoch 0 step 162: training accuarcy: 0.9890000000000001\n",
      "Epoch 0 step 162: training loss: 2700.817522507656\n",
      "Epoch 0 step 163: training accuarcy: 0.9912000000000001\n",
      "Epoch 0 step 163: training loss: 2742.381666573158\n",
      "Epoch 0 step 164: training accuarcy: 0.9862000000000001\n",
      "Epoch 0 step 164: training loss: 2759.401268134354\n",
      "Epoch 0 step 165: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 165: training loss: 2721.381621159014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 166: training accuarcy: 0.9862000000000001\n",
      "Epoch 0 step 166: training loss: 2696.7118351185077\n",
      "Epoch 0 step 167: training accuarcy: 0.9857\n",
      "Epoch 0 step 167: training loss: 2681.0778855298404\n",
      "Epoch 0 step 168: training accuarcy: 0.9868\n",
      "Epoch 0 step 168: training loss: 2626.978286697358\n",
      "Epoch 0 step 169: training accuarcy: 0.9872000000000001\n",
      "Epoch 0 step 169: training loss: 2614.225238317762\n",
      "Epoch 0 step 170: training accuarcy: 0.9849\n",
      "Epoch 0 step 170: training loss: 2572.3588066466223\n",
      "Epoch 0 step 171: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 171: training loss: 2562.2526673500697\n",
      "Epoch 0 step 172: training accuarcy: 0.9870000000000001\n",
      "Epoch 0 step 172: training loss: 2502.9905367897736\n",
      "Epoch 0 step 173: training accuarcy: 0.9887\n",
      "Epoch 0 step 173: training loss: 2456.2448976557566\n",
      "Epoch 0 step 174: training accuarcy: 0.9894000000000001\n",
      "Epoch 0 step 174: training loss: 2546.7299974681673\n",
      "Epoch 0 step 175: training accuarcy: 0.9840000000000001\n",
      "Epoch 0 step 175: training loss: 2446.250919528472\n",
      "Epoch 0 step 176: training accuarcy: 0.9862000000000001\n",
      "Epoch 0 step 176: training loss: 2417.4392507389334\n",
      "Epoch 0 step 177: training accuarcy: 0.9887\n",
      "Epoch 0 step 177: training loss: 2379.888968613569\n",
      "Epoch 0 step 178: training accuarcy: 0.9885\n",
      "Epoch 0 step 178: training loss: 2395.066967219495\n",
      "Epoch 0 step 179: training accuarcy: 0.9890000000000001\n",
      "Epoch 0 step 179: training loss: 2403.815887845468\n",
      "Epoch 0 step 180: training accuarcy: 0.9844\n",
      "Epoch 0 step 180: training loss: 2298.8957178466585\n",
      "Epoch 0 step 181: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 181: training loss: 2306.3661220142376\n",
      "Epoch 0 step 182: training accuarcy: 0.9899\n",
      "Epoch 0 step 182: training loss: 2330.535158785832\n",
      "Epoch 0 step 183: training accuarcy: 0.9852000000000001\n",
      "Epoch 0 step 183: training loss: 2263.0311468628606\n",
      "Epoch 0 step 184: training accuarcy: 0.9901000000000001\n",
      "Epoch 0 step 184: training loss: 2224.7376392453198\n",
      "Epoch 0 step 185: training accuarcy: 0.9935\n",
      "Epoch 0 step 185: training loss: 2217.062941420424\n",
      "Epoch 0 step 186: training accuarcy: 0.9894000000000001\n",
      "Epoch 0 step 186: training loss: 2186.2503864825867\n",
      "Epoch 0 step 187: training accuarcy: 0.9896\n",
      "Epoch 0 step 187: training loss: 2181.0959518249674\n",
      "Epoch 0 step 188: training accuarcy: 0.9889\n",
      "Epoch 0 step 188: training loss: 2176.1340617859287\n",
      "Epoch 0 step 189: training accuarcy: 0.9893000000000001\n",
      "Epoch 0 step 189: training loss: 2225.3244411126116\n",
      "Epoch 0 step 190: training accuarcy: 0.9846\n",
      "Epoch 0 step 190: training loss: 2141.478094766137\n",
      "Epoch 0 step 191: training accuarcy: 0.9894000000000001\n",
      "Epoch 0 step 191: training loss: 2081.4668135512243\n",
      "Epoch 0 step 192: training accuarcy: 0.9918\n",
      "Epoch 0 step 192: training loss: 2057.6409301536446\n",
      "Epoch 0 step 193: training accuarcy: 0.9898\n",
      "Epoch 0 step 193: training loss: 2064.249503288501\n",
      "Epoch 0 step 194: training accuarcy: 0.9893000000000001\n",
      "Epoch 0 step 194: training loss: 1975.0453244800215\n",
      "Epoch 0 step 195: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 195: training loss: 2049.4297634784016\n",
      "Epoch 0 step 196: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 196: training loss: 2009.3606467826532\n",
      "Epoch 0 step 197: training accuarcy: 0.9899\n",
      "Epoch 0 step 197: training loss: 2003.8950953143947\n",
      "Epoch 0 step 198: training accuarcy: 0.9905\n",
      "Epoch 0 step 198: training loss: 2005.1547377373786\n",
      "Epoch 0 step 199: training accuarcy: 0.9870000000000001\n",
      "Epoch 0 step 199: training loss: 1970.0508469378685\n",
      "Epoch 0 step 200: training accuarcy: 0.9909\n",
      "Epoch 0 step 200: training loss: 1916.9482702173018\n",
      "Epoch 0 step 201: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 201: training loss: 1943.0323041380998\n",
      "Epoch 0 step 202: training accuarcy: 0.9878\n",
      "Epoch 0 step 202: training loss: 1953.074549548568\n",
      "Epoch 0 step 203: training accuarcy: 0.9896\n",
      "Epoch 0 step 203: training loss: 1926.129135107347\n",
      "Epoch 0 step 204: training accuarcy: 0.9886\n",
      "Epoch 0 step 204: training loss: 1915.289626201195\n",
      "Epoch 0 step 205: training accuarcy: 0.9845\n",
      "Epoch 0 step 205: training loss: 1864.1003447325543\n",
      "Epoch 0 step 206: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 206: training loss: 1860.302959444375\n",
      "Epoch 0 step 207: training accuarcy: 0.9892000000000001\n",
      "Epoch 0 step 207: training loss: 1881.5090925206052\n",
      "Epoch 0 step 208: training accuarcy: 0.9875\n",
      "Epoch 0 step 208: training loss: 1812.192555939457\n",
      "Epoch 0 step 209: training accuarcy: 0.9899\n",
      "Epoch 0 step 209: training loss: 1796.8013236150189\n",
      "Epoch 0 step 210: training accuarcy: 0.993\n",
      "Epoch 0 step 210: training loss: 1779.9852605129802\n",
      "Epoch 0 step 211: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 211: training loss: 1787.3225024390888\n",
      "Epoch 0 step 212: training accuarcy: 0.9884000000000001\n",
      "Epoch 0 step 212: training loss: 1741.5176462584536\n",
      "Epoch 0 step 213: training accuarcy: 0.9902000000000001\n",
      "Epoch 0 step 213: training loss: 1721.9866731163413\n",
      "Epoch 0 step 214: training accuarcy: 0.9918\n",
      "Epoch 0 step 214: training loss: 1712.6917498373055\n",
      "Epoch 0 step 215: training accuarcy: 0.9923000000000001\n",
      "Epoch 0 step 215: training loss: 1679.0101311076692\n",
      "Epoch 0 step 216: training accuarcy: 0.9917\n",
      "Epoch 0 step 216: training loss: 1675.4583372385694\n",
      "Epoch 0 step 217: training accuarcy: 0.9899\n",
      "Epoch 0 step 217: training loss: 1718.1300047546674\n",
      "Epoch 0 step 218: training accuarcy: 0.9889\n",
      "Epoch 0 step 218: training loss: 1658.091293695234\n",
      "Epoch 0 step 219: training accuarcy: 0.9901000000000001\n",
      "Epoch 0 step 219: training loss: 1641.7008259206032\n",
      "Epoch 0 step 220: training accuarcy: 0.9925\n",
      "Epoch 0 step 220: training loss: 1621.5263222894866\n",
      "Epoch 0 step 221: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 221: training loss: 1623.1098969268685\n",
      "Epoch 0 step 222: training accuarcy: 0.9904000000000001\n",
      "Epoch 0 step 222: training loss: 1567.7158222942608\n",
      "Epoch 0 step 223: training accuarcy: 0.9926\n",
      "Epoch 0 step 223: training loss: 1603.0899142155113\n",
      "Epoch 0 step 224: training accuarcy: 0.9889\n",
      "Epoch 0 step 224: training loss: 1596.094053113762\n",
      "Epoch 0 step 225: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 225: training loss: 1583.5453593512916\n",
      "Epoch 0 step 226: training accuarcy: 0.9893000000000001\n",
      "Epoch 0 step 226: training loss: 1557.5326706955984\n",
      "Epoch 0 step 227: training accuarcy: 0.9911000000000001\n",
      "Epoch 0 step 227: training loss: 1528.2863355406867\n",
      "Epoch 0 step 228: training accuarcy: 0.9928\n",
      "Epoch 0 step 228: training loss: 1557.586241784738\n",
      "Epoch 0 step 229: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 229: training loss: 1497.6474161888839\n",
      "Epoch 0 step 230: training accuarcy: 0.9915\n",
      "Epoch 0 step 230: training loss: 1499.9791637595413\n",
      "Epoch 0 step 231: training accuarcy: 0.9924000000000001\n",
      "Epoch 0 step 231: training loss: 1541.292554034927\n",
      "Epoch 0 step 232: training accuarcy: 0.9881000000000001\n",
      "Epoch 0 step 232: training loss: 1443.6259035933963\n",
      "Epoch 0 step 233: training accuarcy: 0.9941000000000001\n",
      "Epoch 0 step 233: training loss: 1462.7369213467791\n",
      "Epoch 0 step 234: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 234: training loss: 1442.406035412787\n",
      "Epoch 0 step 235: training accuarcy: 0.9912000000000001\n",
      "Epoch 0 step 235: training loss: 1460.7337371363137\n",
      "Epoch 0 step 236: training accuarcy: 0.9880000000000001\n",
      "Epoch 0 step 236: training loss: 1452.5161493224402\n",
      "Epoch 0 step 237: training accuarcy: 0.9895\n",
      "Epoch 0 step 237: training loss: 1444.3908702270548\n",
      "Epoch 0 step 238: training accuarcy: 0.9891000000000001\n",
      "Epoch 0 step 238: training loss: 1412.7633974061278\n",
      "Epoch 0 step 239: training accuarcy: 0.9925\n",
      "Epoch 0 step 239: training loss: 1423.4700414432675\n",
      "Epoch 0 step 240: training accuarcy: 0.9889\n",
      "Epoch 0 step 240: training loss: 1377.2120521737422\n",
      "Epoch 0 step 241: training accuarcy: 0.9927\n",
      "Epoch 0 step 241: training loss: 1371.2294890755677\n",
      "Epoch 0 step 242: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 242: training loss: 1346.5086628707631\n",
      "Epoch 0 step 243: training accuarcy: 0.9919\n",
      "Epoch 0 step 243: training loss: 1345.4425230710006\n",
      "Epoch 0 step 244: training accuarcy: 0.9917\n",
      "Epoch 0 step 244: training loss: 1357.8805676183938\n",
      "Epoch 0 step 245: training accuarcy: 0.9912000000000001\n",
      "Epoch 0 step 245: training loss: 1360.0220524423014\n",
      "Epoch 0 step 246: training accuarcy: 0.9890000000000001\n",
      "Epoch 0 step 246: training loss: 1299.316307948798\n",
      "Epoch 0 step 247: training accuarcy: 0.9935\n",
      "Epoch 0 step 247: training loss: 1306.382292371756\n",
      "Epoch 0 step 248: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 248: training loss: 1297.280236087457\n",
      "Epoch 0 step 249: training accuarcy: 0.9924000000000001\n",
      "Epoch 0 step 249: training loss: 1317.9026961631646\n",
      "Epoch 0 step 250: training accuarcy: 0.9892000000000001\n",
      "Epoch 0 step 250: training loss: 1303.8867386063735\n",
      "Epoch 0 step 251: training accuarcy: 0.9895\n",
      "Epoch 0 step 251: training loss: 1272.722001783706\n",
      "Epoch 0 step 252: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 252: training loss: 1238.0084262240712\n",
      "Epoch 0 step 253: training accuarcy: 0.994\n",
      "Epoch 0 step 253: training loss: 1257.2068688782333\n",
      "Epoch 0 step 254: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 254: training loss: 1250.4342858559987\n",
      "Epoch 0 step 255: training accuarcy: 0.9927\n",
      "Epoch 0 step 255: training loss: 1207.3746565724925\n",
      "Epoch 0 step 256: training accuarcy: 0.9941000000000001\n",
      "Epoch 0 step 256: training loss: 1200.0420141717118\n",
      "Epoch 0 step 257: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 257: training loss: 1219.2398452935556\n",
      "Epoch 0 step 258: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 258: training loss: 1233.5203263910478\n",
      "Epoch 0 step 259: training accuarcy: 0.9903000000000001\n",
      "Epoch 0 step 259: training loss: 1215.3067919145192\n",
      "Epoch 0 step 260: training accuarcy: 0.9916\n",
      "Epoch 0 step 260: training loss: 1197.2055246704967\n",
      "Epoch 0 step 261: training accuarcy: 0.9919\n",
      "Epoch 0 step 261: training loss: 1155.3120774584265\n",
      "Epoch 0 step 262: training accuarcy: 0.9941000000000001\n",
      "Epoch 0 step 262: training loss: 1064.7214108383223\n",
      "Epoch 0 step 263: training accuarcy: 0.9933333333333333\n",
      "Epoch 0: train loss 7588.774127760777, train accuarcy 0.942011296749115\n",
      "Epoch 0: valid loss 1541.5706435176821, valid accuarcy 0.9870830178260803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▍                                                                                                                                         | 1/5 [04:56<19:45, 296.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1141.8540969200897\n",
      "Epoch 1 step 264: training accuarcy: 0.9913000000000001\n",
      "Epoch 1 step 264: training loss: 1119.3933070159264\n",
      "Epoch 1 step 265: training accuarcy: 0.995\n",
      "Epoch 1 step 265: training loss: 1100.372395073889\n",
      "Epoch 1 step 266: training accuarcy: 0.9957\n",
      "Epoch 1 step 266: training loss: 1091.5917523604219\n",
      "Epoch 1 step 267: training accuarcy: 0.995\n",
      "Epoch 1 step 267: training loss: 1053.180513373251\n",
      "Epoch 1 step 268: training accuarcy: 0.9955\n",
      "Epoch 1 step 268: training loss: 1078.1888165328292\n",
      "Epoch 1 step 269: training accuarcy: 0.9936\n",
      "Epoch 1 step 269: training loss: 1075.3834084892223\n",
      "Epoch 1 step 270: training accuarcy: 0.9936\n",
      "Epoch 1 step 270: training loss: 1027.8584271406962\n",
      "Epoch 1 step 271: training accuarcy: 0.9966\n",
      "Epoch 1 step 271: training loss: 1046.4623134771666\n",
      "Epoch 1 step 272: training accuarcy: 0.9955\n",
      "Epoch 1 step 272: training loss: 1028.8387499158332\n",
      "Epoch 1 step 273: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 273: training loss: 1031.2907730820255\n",
      "Epoch 1 step 274: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 274: training loss: 1030.7934998697203\n",
      "Epoch 1 step 275: training accuarcy: 0.9946\n",
      "Epoch 1 step 275: training loss: 999.572254856207\n",
      "Epoch 1 step 276: training accuarcy: 0.9955\n",
      "Epoch 1 step 276: training loss: 1038.0148568033374\n",
      "Epoch 1 step 277: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 277: training loss: 1006.7016423513958\n",
      "Epoch 1 step 278: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 278: training loss: 964.9087828717451\n",
      "Epoch 1 step 279: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 279: training loss: 997.5648413466465\n",
      "Epoch 1 step 280: training accuarcy: 0.995\n",
      "Epoch 1 step 280: training loss: 977.3380236985763\n",
      "Epoch 1 step 281: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 281: training loss: 989.8550228780382\n",
      "Epoch 1 step 282: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 282: training loss: 977.9734003668892\n",
      "Epoch 1 step 283: training accuarcy: 0.9937\n",
      "Epoch 1 step 283: training loss: 929.6690847040342\n",
      "Epoch 1 step 284: training accuarcy: 0.997\n",
      "Epoch 1 step 284: training loss: 951.4429066391147\n",
      "Epoch 1 step 285: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 285: training loss: 939.2104839247002\n",
      "Epoch 1 step 286: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 286: training loss: 946.6793850745621\n",
      "Epoch 1 step 287: training accuarcy: 0.9949\n",
      "Epoch 1 step 287: training loss: 925.7881074865579\n",
      "Epoch 1 step 288: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 288: training loss: 920.7339944524034\n",
      "Epoch 1 step 289: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 289: training loss: 909.7788890561646\n",
      "Epoch 1 step 290: training accuarcy: 0.9977\n",
      "Epoch 1 step 290: training loss: 882.828168141316\n",
      "Epoch 1 step 291: training accuarcy: 0.9967\n",
      "Epoch 1 step 291: training loss: 915.3541116928759\n",
      "Epoch 1 step 292: training accuarcy: 0.9947\n",
      "Epoch 1 step 292: training loss: 910.473462104249\n",
      "Epoch 1 step 293: training accuarcy: 0.996\n",
      "Epoch 1 step 293: training loss: 919.4437959090785\n",
      "Epoch 1 step 294: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 294: training loss: 906.959069044718\n",
      "Epoch 1 step 295: training accuarcy: 0.9945\n",
      "Epoch 1 step 295: training loss: 888.4814519045469\n",
      "Epoch 1 step 296: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 296: training loss: 853.0178556075159\n",
      "Epoch 1 step 297: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 297: training loss: 887.488244109936\n",
      "Epoch 1 step 298: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 298: training loss: 862.8841872824332\n",
      "Epoch 1 step 299: training accuarcy: 0.996\n",
      "Epoch 1 step 299: training loss: 851.340266182408\n",
      "Epoch 1 step 300: training accuarcy: 0.996\n",
      "Epoch 1 step 300: training loss: 843.150227060905\n",
      "Epoch 1 step 301: training accuarcy: 0.9948\n",
      "Epoch 1 step 301: training loss: 838.9449130164927\n",
      "Epoch 1 step 302: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 302: training loss: 849.4182359218621\n",
      "Epoch 1 step 303: training accuarcy: 0.9955\n",
      "Epoch 1 step 303: training loss: 844.2450362236561\n",
      "Epoch 1 step 304: training accuarcy: 0.9956\n",
      "Epoch 1 step 304: training loss: 819.0923855789015\n",
      "Epoch 1 step 305: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 305: training loss: 872.9130145069477\n",
      "Epoch 1 step 306: training accuarcy: 0.994\n",
      "Epoch 1 step 306: training loss: 860.4435258873254\n",
      "Epoch 1 step 307: training accuarcy: 0.9939\n",
      "Epoch 1 step 307: training loss: 849.8864812523889\n",
      "Epoch 1 step 308: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 308: training loss: 845.8738228553676\n",
      "Epoch 1 step 309: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 309: training loss: 805.3308452672231\n",
      "Epoch 1 step 310: training accuarcy: 0.9976\n",
      "Epoch 1 step 310: training loss: 826.5595357748984\n",
      "Epoch 1 step 311: training accuarcy: 0.9945\n",
      "Epoch 1 step 311: training loss: 823.7916550409263\n",
      "Epoch 1 step 312: training accuarcy: 0.9931000000000001\n",
      "Epoch 1 step 312: training loss: 857.5012412636297\n",
      "Epoch 1 step 313: training accuarcy: 0.9925\n",
      "Epoch 1 step 313: training loss: 828.9584189178823\n",
      "Epoch 1 step 314: training accuarcy: 0.9929\n",
      "Epoch 1 step 314: training loss: 803.9898601109333\n",
      "Epoch 1 step 315: training accuarcy: 0.9945\n",
      "Epoch 1 step 315: training loss: 791.817758538654\n",
      "Epoch 1 step 316: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 316: training loss: 769.812205357412\n",
      "Epoch 1 step 317: training accuarcy: 0.9958\n",
      "Epoch 1 step 317: training loss: 782.7129571521039\n",
      "Epoch 1 step 318: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 318: training loss: 772.5075862098603\n",
      "Epoch 1 step 319: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 319: training loss: 778.9520033726551\n",
      "Epoch 1 step 320: training accuarcy: 0.995\n",
      "Epoch 1 step 320: training loss: 804.6366967664569\n",
      "Epoch 1 step 321: training accuarcy: 0.9919\n",
      "Epoch 1 step 321: training loss: 780.4553606430212\n",
      "Epoch 1 step 322: training accuarcy: 0.9948\n",
      "Epoch 1 step 322: training loss: 771.7453731109072\n",
      "Epoch 1 step 323: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 323: training loss: 774.1553000795341\n",
      "Epoch 1 step 324: training accuarcy: 0.9948\n",
      "Epoch 1 step 324: training loss: 763.8615039295962\n",
      "Epoch 1 step 325: training accuarcy: 0.9938\n",
      "Epoch 1 step 325: training loss: 714.2365685790187\n",
      "Epoch 1 step 326: training accuarcy: 0.9978\n",
      "Epoch 1 step 326: training loss: 766.3116019377119\n",
      "Epoch 1 step 327: training accuarcy: 0.9948\n",
      "Epoch 1 step 327: training loss: 720.1209616239474\n",
      "Epoch 1 step 328: training accuarcy: 0.9957\n",
      "Epoch 1 step 328: training loss: 720.9535450457668\n",
      "Epoch 1 step 329: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 329: training loss: 740.4600060987863\n",
      "Epoch 1 step 330: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 330: training loss: 708.6014037625894\n",
      "Epoch 1 step 331: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 331: training loss: 727.8590152121931\n",
      "Epoch 1 step 332: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 332: training loss: 722.9378067085224\n",
      "Epoch 1 step 333: training accuarcy: 0.9956\n",
      "Epoch 1 step 333: training loss: 709.0735065785293\n",
      "Epoch 1 step 334: training accuarcy: 0.9958\n",
      "Epoch 1 step 334: training loss: 706.419156308354\n",
      "Epoch 1 step 335: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 335: training loss: 733.351458787662\n",
      "Epoch 1 step 336: training accuarcy: 0.9926\n",
      "Epoch 1 step 336: training loss: 717.3641267230178\n",
      "Epoch 1 step 337: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 337: training loss: 727.3740092526369\n",
      "Epoch 1 step 338: training accuarcy: 0.9955\n",
      "Epoch 1 step 338: training loss: 668.9260129236533\n",
      "Epoch 1 step 339: training accuarcy: 0.9959\n",
      "Epoch 1 step 339: training loss: 705.4940512079634\n",
      "Epoch 1 step 340: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 340: training loss: 667.5488895763041\n",
      "Epoch 1 step 341: training accuarcy: 0.996\n",
      "Epoch 1 step 341: training loss: 754.6366288541248\n",
      "Epoch 1 step 342: training accuarcy: 0.993\n",
      "Epoch 1 step 342: training loss: 669.9458578027321\n",
      "Epoch 1 step 343: training accuarcy: 0.9977\n",
      "Epoch 1 step 343: training loss: 706.4281923478466\n",
      "Epoch 1 step 344: training accuarcy: 0.9936\n",
      "Epoch 1 step 344: training loss: 651.8229565575164\n",
      "Epoch 1 step 345: training accuarcy: 0.9956\n",
      "Epoch 1 step 345: training loss: 667.7384451014498\n",
      "Epoch 1 step 346: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 346: training loss: 668.925549671285\n",
      "Epoch 1 step 347: training accuarcy: 0.996\n",
      "Epoch 1 step 347: training loss: 694.9472886853931\n",
      "Epoch 1 step 348: training accuarcy: 0.995\n",
      "Epoch 1 step 348: training loss: 651.7745569783302\n",
      "Epoch 1 step 349: training accuarcy: 0.9955\n",
      "Epoch 1 step 349: training loss: 712.7055659411735\n",
      "Epoch 1 step 350: training accuarcy: 0.992\n",
      "Epoch 1 step 350: training loss: 690.7257632154144\n",
      "Epoch 1 step 351: training accuarcy: 0.9955\n",
      "Epoch 1 step 351: training loss: 655.5577172238508\n",
      "Epoch 1 step 352: training accuarcy: 0.9949\n",
      "Epoch 1 step 352: training loss: 635.6901486220396\n",
      "Epoch 1 step 353: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 353: training loss: 657.709630081275\n",
      "Epoch 1 step 354: training accuarcy: 0.994\n",
      "Epoch 1 step 354: training loss: 695.7517271916912\n",
      "Epoch 1 step 355: training accuarcy: 0.9926\n",
      "Epoch 1 step 355: training loss: 667.6055843756192\n",
      "Epoch 1 step 356: training accuarcy: 0.9937\n",
      "Epoch 1 step 356: training loss: 616.1819211805923\n",
      "Epoch 1 step 357: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 357: training loss: 629.5965841198239\n",
      "Epoch 1 step 358: training accuarcy: 0.9956\n",
      "Epoch 1 step 358: training loss: 641.9806790896936\n",
      "Epoch 1 step 359: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 359: training loss: 630.3832346574939\n",
      "Epoch 1 step 360: training accuarcy: 0.9968\n",
      "Epoch 1 step 360: training loss: 645.3348530469721\n",
      "Epoch 1 step 361: training accuarcy: 0.9947\n",
      "Epoch 1 step 361: training loss: 641.606781312468\n",
      "Epoch 1 step 362: training accuarcy: 0.9949\n",
      "Epoch 1 step 362: training loss: 628.634386814967\n",
      "Epoch 1 step 363: training accuarcy: 0.9937\n",
      "Epoch 1 step 363: training loss: 647.6135522874164\n",
      "Epoch 1 step 364: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 364: training loss: 651.1458391078343\n",
      "Epoch 1 step 365: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 365: training loss: 624.7879553727598\n",
      "Epoch 1 step 366: training accuarcy: 0.9957\n",
      "Epoch 1 step 366: training loss: 592.1564282727539\n",
      "Epoch 1 step 367: training accuarcy: 0.9982000000000001\n",
      "Epoch 1 step 367: training loss: 616.3829093356981\n",
      "Epoch 1 step 368: training accuarcy: 0.9965\n",
      "Epoch 1 step 368: training loss: 618.714284615713\n",
      "Epoch 1 step 369: training accuarcy: 0.9968\n",
      "Epoch 1 step 369: training loss: 599.5811229316146\n",
      "Epoch 1 step 370: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 370: training loss: 679.6775353743474\n",
      "Epoch 1 step 371: training accuarcy: 0.9922000000000001\n",
      "Epoch 1 step 371: training loss: 602.497609058744\n",
      "Epoch 1 step 372: training accuarcy: 0.9957\n",
      "Epoch 1 step 372: training loss: 584.4807945613352\n",
      "Epoch 1 step 373: training accuarcy: 0.9967\n",
      "Epoch 1 step 373: training loss: 595.4670136879513\n",
      "Epoch 1 step 374: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 374: training loss: 583.1051682855029\n",
      "Epoch 1 step 375: training accuarcy: 0.9968\n",
      "Epoch 1 step 375: training loss: 610.7595973342144\n",
      "Epoch 1 step 376: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 376: training loss: 621.6511852039318\n",
      "Epoch 1 step 377: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 377: training loss: 616.7121495203905\n",
      "Epoch 1 step 378: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 378: training loss: 599.7542341935605\n",
      "Epoch 1 step 379: training accuarcy: 0.9946\n",
      "Epoch 1 step 379: training loss: 599.541630009884\n",
      "Epoch 1 step 380: training accuarcy: 0.9955\n",
      "Epoch 1 step 380: training loss: 617.7029454091708\n",
      "Epoch 1 step 381: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 381: training loss: 589.1483998642248\n",
      "Epoch 1 step 382: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 382: training loss: 619.3888409607\n",
      "Epoch 1 step 383: training accuarcy: 0.9937\n",
      "Epoch 1 step 383: training loss: 622.7175346646333\n",
      "Epoch 1 step 384: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 384: training loss: 592.6579235738227\n",
      "Epoch 1 step 385: training accuarcy: 0.9955\n",
      "Epoch 1 step 385: training loss: 573.7641887990625\n",
      "Epoch 1 step 386: training accuarcy: 0.9959\n",
      "Epoch 1 step 386: training loss: 545.5840475922041\n",
      "Epoch 1 step 387: training accuarcy: 0.9978\n",
      "Epoch 1 step 387: training loss: 619.4360633424777\n",
      "Epoch 1 step 388: training accuarcy: 0.9906\n",
      "Epoch 1 step 388: training loss: 580.2825698618292\n",
      "Epoch 1 step 389: training accuarcy: 0.9945\n",
      "Epoch 1 step 389: training loss: 586.7252229883876\n",
      "Epoch 1 step 390: training accuarcy: 0.9937\n",
      "Epoch 1 step 390: training loss: 595.1130807627646\n",
      "Epoch 1 step 391: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 391: training loss: 539.4743275131603\n",
      "Epoch 1 step 392: training accuarcy: 0.998\n",
      "Epoch 1 step 392: training loss: 512.5580005204263\n",
      "Epoch 1 step 393: training accuarcy: 0.9976\n",
      "Epoch 1 step 393: training loss: 568.3956336411952\n",
      "Epoch 1 step 394: training accuarcy: 0.9936\n",
      "Epoch 1 step 394: training loss: 566.4877310539696\n",
      "Epoch 1 step 395: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 395: training loss: 550.0467858526949\n",
      "Epoch 1 step 396: training accuarcy: 0.995\n",
      "Epoch 1 step 396: training loss: 548.9915979962209\n",
      "Epoch 1 step 397: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 397: training loss: 528.9474451809597\n",
      "Epoch 1 step 398: training accuarcy: 0.9969\n",
      "Epoch 1 step 398: training loss: 529.5244174072932\n",
      "Epoch 1 step 399: training accuarcy: 0.9971000000000001\n",
      "Epoch 1 step 399: training loss: 584.966418004688\n",
      "Epoch 1 step 400: training accuarcy: 0.9928\n",
      "Epoch 1 step 400: training loss: 576.8214502334914\n",
      "Epoch 1 step 401: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 401: training loss: 539.7327019697296\n",
      "Epoch 1 step 402: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 402: training loss: 561.9549524337348\n",
      "Epoch 1 step 403: training accuarcy: 0.9946\n",
      "Epoch 1 step 403: training loss: 557.2449175185666\n",
      "Epoch 1 step 404: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 404: training loss: 541.6929702402024\n",
      "Epoch 1 step 405: training accuarcy: 0.9966\n",
      "Epoch 1 step 405: training loss: 558.6442070266561\n",
      "Epoch 1 step 406: training accuarcy: 0.994\n",
      "Epoch 1 step 406: training loss: 557.6917092993017\n",
      "Epoch 1 step 407: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 407: training loss: 534.3993165435072\n",
      "Epoch 1 step 408: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 408: training loss: 529.7956832494089\n",
      "Epoch 1 step 409: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 409: training loss: 562.8359852656044\n",
      "Epoch 1 step 410: training accuarcy: 0.9945\n",
      "Epoch 1 step 410: training loss: 540.2349964463953\n",
      "Epoch 1 step 411: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 411: training loss: 543.8685513332017\n",
      "Epoch 1 step 412: training accuarcy: 0.9946\n",
      "Epoch 1 step 412: training loss: 535.3000306921547\n",
      "Epoch 1 step 413: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 413: training loss: 534.0050679008648\n",
      "Epoch 1 step 414: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 414: training loss: 553.1099678152905\n",
      "Epoch 1 step 415: training accuarcy: 0.9948\n",
      "Epoch 1 step 415: training loss: 515.7013627822898\n",
      "Epoch 1 step 416: training accuarcy: 0.9946\n",
      "Epoch 1 step 416: training loss: 510.49603384839247\n",
      "Epoch 1 step 417: training accuarcy: 0.9966\n",
      "Epoch 1 step 417: training loss: 511.6905968120963\n",
      "Epoch 1 step 418: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 418: training loss: 505.4809991986483\n",
      "Epoch 1 step 419: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 419: training loss: 576.2570176794898\n",
      "Epoch 1 step 420: training accuarcy: 0.994\n",
      "Epoch 1 step 420: training loss: 498.7614944586058\n",
      "Epoch 1 step 421: training accuarcy: 0.997\n",
      "Epoch 1 step 421: training loss: 535.5090485578606\n",
      "Epoch 1 step 422: training accuarcy: 0.9956\n",
      "Epoch 1 step 422: training loss: 518.5730091879955\n",
      "Epoch 1 step 423: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 423: training loss: 490.2746726092519\n",
      "Epoch 1 step 424: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 424: training loss: 499.8794313127031\n",
      "Epoch 1 step 425: training accuarcy: 0.9959\n",
      "Epoch 1 step 425: training loss: 507.3600808489392\n",
      "Epoch 1 step 426: training accuarcy: 0.9947\n",
      "Epoch 1 step 426: training loss: 482.5441341402169\n",
      "Epoch 1 step 427: training accuarcy: 0.9959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 427: training loss: 481.791156457174\n",
      "Epoch 1 step 428: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 428: training loss: 493.6798024119568\n",
      "Epoch 1 step 429: training accuarcy: 0.9966\n",
      "Epoch 1 step 429: training loss: 496.97522602462345\n",
      "Epoch 1 step 430: training accuarcy: 0.9955\n",
      "Epoch 1 step 430: training loss: 526.967835088629\n",
      "Epoch 1 step 431: training accuarcy: 0.9927\n",
      "Epoch 1 step 431: training loss: 482.9606008390582\n",
      "Epoch 1 step 432: training accuarcy: 0.9969\n",
      "Epoch 1 step 432: training loss: 496.5840363391192\n",
      "Epoch 1 step 433: training accuarcy: 0.9956\n",
      "Epoch 1 step 433: training loss: 501.66837869207257\n",
      "Epoch 1 step 434: training accuarcy: 0.9955\n",
      "Epoch 1 step 434: training loss: 558.4313912829242\n",
      "Epoch 1 step 435: training accuarcy: 0.991\n",
      "Epoch 1 step 435: training loss: 498.7550573230224\n",
      "Epoch 1 step 436: training accuarcy: 0.9966\n",
      "Epoch 1 step 436: training loss: 484.0770827239389\n",
      "Epoch 1 step 437: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 437: training loss: 513.1064759525857\n",
      "Epoch 1 step 438: training accuarcy: 0.9938\n",
      "Epoch 1 step 438: training loss: 477.02155064826934\n",
      "Epoch 1 step 439: training accuarcy: 0.9967\n",
      "Epoch 1 step 439: training loss: 494.20547749543044\n",
      "Epoch 1 step 440: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 440: training loss: 468.3725418311261\n",
      "Epoch 1 step 441: training accuarcy: 0.9975\n",
      "Epoch 1 step 441: training loss: 489.3756137802477\n",
      "Epoch 1 step 442: training accuarcy: 0.9948\n",
      "Epoch 1 step 442: training loss: 468.50547401082514\n",
      "Epoch 1 step 443: training accuarcy: 0.9976\n",
      "Epoch 1 step 443: training loss: 481.55606105227935\n",
      "Epoch 1 step 444: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 444: training loss: 459.46324147663165\n",
      "Epoch 1 step 445: training accuarcy: 0.997\n",
      "Epoch 1 step 445: training loss: 464.18534040173756\n",
      "Epoch 1 step 446: training accuarcy: 0.9959\n",
      "Epoch 1 step 446: training loss: 472.8009125481773\n",
      "Epoch 1 step 447: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 447: training loss: 473.65364384589435\n",
      "Epoch 1 step 448: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 448: training loss: 465.50365368339686\n",
      "Epoch 1 step 449: training accuarcy: 0.996\n",
      "Epoch 1 step 449: training loss: 464.847173900539\n",
      "Epoch 1 step 450: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 450: training loss: 474.0402165381368\n",
      "Epoch 1 step 451: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 451: training loss: 506.98958439959836\n",
      "Epoch 1 step 452: training accuarcy: 0.9948\n",
      "Epoch 1 step 452: training loss: 472.2789822601302\n",
      "Epoch 1 step 453: training accuarcy: 0.9967\n",
      "Epoch 1 step 453: training loss: 436.07901863334075\n",
      "Epoch 1 step 454: training accuarcy: 0.9977\n",
      "Epoch 1 step 454: training loss: 464.98112552065913\n",
      "Epoch 1 step 455: training accuarcy: 0.9965\n",
      "Epoch 1 step 455: training loss: 463.3130403238203\n",
      "Epoch 1 step 456: training accuarcy: 0.9965\n",
      "Epoch 1 step 456: training loss: 478.6772271316259\n",
      "Epoch 1 step 457: training accuarcy: 0.9949\n",
      "Epoch 1 step 457: training loss: 468.39188859246224\n",
      "Epoch 1 step 458: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 458: training loss: 481.7023964040579\n",
      "Epoch 1 step 459: training accuarcy: 0.9955\n",
      "Epoch 1 step 459: training loss: 418.8294737999759\n",
      "Epoch 1 step 460: training accuarcy: 0.9976\n",
      "Epoch 1 step 460: training loss: 458.0887214394792\n",
      "Epoch 1 step 461: training accuarcy: 0.9946\n",
      "Epoch 1 step 461: training loss: 488.712958752535\n",
      "Epoch 1 step 462: training accuarcy: 0.9947\n",
      "Epoch 1 step 462: training loss: 451.0889440263099\n",
      "Epoch 1 step 463: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 463: training loss: 451.86792737116366\n",
      "Epoch 1 step 464: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 464: training loss: 472.2896934987312\n",
      "Epoch 1 step 465: training accuarcy: 0.9967\n",
      "Epoch 1 step 465: training loss: 499.5934583151535\n",
      "Epoch 1 step 466: training accuarcy: 0.9948\n",
      "Epoch 1 step 466: training loss: 473.51500879741707\n",
      "Epoch 1 step 467: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 467: training loss: 488.4397321819383\n",
      "Epoch 1 step 468: training accuarcy: 0.9958\n",
      "Epoch 1 step 468: training loss: 442.26615689478194\n",
      "Epoch 1 step 469: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 469: training loss: 447.75627189268675\n",
      "Epoch 1 step 470: training accuarcy: 0.9968\n",
      "Epoch 1 step 470: training loss: 458.756195495865\n",
      "Epoch 1 step 471: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 471: training loss: 445.73284618177337\n",
      "Epoch 1 step 472: training accuarcy: 0.9983000000000001\n",
      "Epoch 1 step 472: training loss: 444.97751128963847\n",
      "Epoch 1 step 473: training accuarcy: 0.995\n",
      "Epoch 1 step 473: training loss: 454.97464354472066\n",
      "Epoch 1 step 474: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 474: training loss: 431.99185937694097\n",
      "Epoch 1 step 475: training accuarcy: 0.9955\n",
      "Epoch 1 step 475: training loss: 431.3595969258089\n",
      "Epoch 1 step 476: training accuarcy: 0.9965\n",
      "Epoch 1 step 476: training loss: 438.25590877391056\n",
      "Epoch 1 step 477: training accuarcy: 0.9956\n",
      "Epoch 1 step 477: training loss: 417.9945836289743\n",
      "Epoch 1 step 478: training accuarcy: 0.9979\n",
      "Epoch 1 step 478: training loss: 466.37711564964104\n",
      "Epoch 1 step 479: training accuarcy: 0.9958\n",
      "Epoch 1 step 479: training loss: 474.6519835355888\n",
      "Epoch 1 step 480: training accuarcy: 0.9938\n",
      "Epoch 1 step 480: training loss: 444.09494422817727\n",
      "Epoch 1 step 481: training accuarcy: 0.9959\n",
      "Epoch 1 step 481: training loss: 401.93323260304135\n",
      "Epoch 1 step 482: training accuarcy: 0.9978\n",
      "Epoch 1 step 482: training loss: 455.8807651773336\n",
      "Epoch 1 step 483: training accuarcy: 0.9967\n",
      "Epoch 1 step 483: training loss: 415.01752042507917\n",
      "Epoch 1 step 484: training accuarcy: 0.9967\n",
      "Epoch 1 step 484: training loss: 435.50874916464164\n",
      "Epoch 1 step 485: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 485: training loss: 422.8759349159179\n",
      "Epoch 1 step 486: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 486: training loss: 443.706740193898\n",
      "Epoch 1 step 487: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 487: training loss: 437.93121595994234\n",
      "Epoch 1 step 488: training accuarcy: 0.9959\n",
      "Epoch 1 step 488: training loss: 440.147625760705\n",
      "Epoch 1 step 489: training accuarcy: 0.9945\n",
      "Epoch 1 step 489: training loss: 438.37188016527557\n",
      "Epoch 1 step 490: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 490: training loss: 427.0531057359364\n",
      "Epoch 1 step 491: training accuarcy: 0.9975\n",
      "Epoch 1 step 491: training loss: 441.56226942228795\n",
      "Epoch 1 step 492: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 492: training loss: 466.23147158325776\n",
      "Epoch 1 step 493: training accuarcy: 0.9958\n",
      "Epoch 1 step 493: training loss: 401.5138615029424\n",
      "Epoch 1 step 494: training accuarcy: 0.998\n",
      "Epoch 1 step 494: training loss: 433.8744011728088\n",
      "Epoch 1 step 495: training accuarcy: 0.9965\n",
      "Epoch 1 step 495: training loss: 439.1194523001107\n",
      "Epoch 1 step 496: training accuarcy: 0.9967\n",
      "Epoch 1 step 496: training loss: 449.2886990484467\n",
      "Epoch 1 step 497: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 497: training loss: 455.0526007238744\n",
      "Epoch 1 step 498: training accuarcy: 0.9946\n",
      "Epoch 1 step 498: training loss: 466.2495364993873\n",
      "Epoch 1 step 499: training accuarcy: 0.9936\n",
      "Epoch 1 step 499: training loss: 448.54188291873766\n",
      "Epoch 1 step 500: training accuarcy: 0.9926\n",
      "Epoch 1 step 500: training loss: 434.4289988940816\n",
      "Epoch 1 step 501: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 501: training loss: 411.09487081389386\n",
      "Epoch 1 step 502: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 502: training loss: 420.54142623182423\n",
      "Epoch 1 step 503: training accuarcy: 0.9967\n",
      "Epoch 1 step 503: training loss: 426.2665852668441\n",
      "Epoch 1 step 504: training accuarcy: 0.9967\n",
      "Epoch 1 step 504: training loss: 395.92506068179296\n",
      "Epoch 1 step 505: training accuarcy: 0.9971000000000001\n",
      "Epoch 1 step 505: training loss: 422.1887324740164\n",
      "Epoch 1 step 506: training accuarcy: 0.9976\n",
      "Epoch 1 step 506: training loss: 430.7447251917735\n",
      "Epoch 1 step 507: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 507: training loss: 440.9722836084977\n",
      "Epoch 1 step 508: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 508: training loss: 447.07499649794653\n",
      "Epoch 1 step 509: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 509: training loss: 444.0392063827576\n",
      "Epoch 1 step 510: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 510: training loss: 445.97621883856783\n",
      "Epoch 1 step 511: training accuarcy: 0.9935\n",
      "Epoch 1 step 511: training loss: 408.6690696548517\n",
      "Epoch 1 step 512: training accuarcy: 0.997\n",
      "Epoch 1 step 512: training loss: 451.5340753109575\n",
      "Epoch 1 step 513: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 513: training loss: 406.97472861717534\n",
      "Epoch 1 step 514: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 514: training loss: 410.8431999635772\n",
      "Epoch 1 step 515: training accuarcy: 0.9959\n",
      "Epoch 1 step 515: training loss: 413.0774699028161\n",
      "Epoch 1 step 516: training accuarcy: 0.9968\n",
      "Epoch 1 step 516: training loss: 405.5417411382557\n",
      "Epoch 1 step 517: training accuarcy: 0.9971000000000001\n",
      "Epoch 1 step 517: training loss: 427.15928973223015\n",
      "Epoch 1 step 518: training accuarcy: 0.9957\n",
      "Epoch 1 step 518: training loss: 406.32419004660034\n",
      "Epoch 1 step 519: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 519: training loss: 413.04127009837214\n",
      "Epoch 1 step 520: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 520: training loss: 412.44680023155246\n",
      "Epoch 1 step 521: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 521: training loss: 406.91240302679216\n",
      "Epoch 1 step 522: training accuarcy: 0.9965\n",
      "Epoch 1 step 522: training loss: 389.6707242744376\n",
      "Epoch 1 step 523: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 523: training loss: 405.5664837330436\n",
      "Epoch 1 step 524: training accuarcy: 0.9968\n",
      "Epoch 1 step 524: training loss: 397.4477996468483\n",
      "Epoch 1 step 525: training accuarcy: 0.9973000000000001\n",
      "Epoch 1 step 525: training loss: 340.4436111627402\n",
      "Epoch 1 step 526: training accuarcy: 0.9966666666666667\n",
      "Epoch 1: train loss 621.6931889058443, train accuarcy 0.9897975921630859\n",
      "Epoch 1: valid loss 727.7551166987381, valid accuarcy 0.9922782182693481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████████████████████████████▊                                                                                                       | 2/5 [09:46<14:43, 294.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 383.67212274899856\n",
      "Epoch 2 step 527: training accuarcy: 0.9966\n",
      "Epoch 2 step 527: training loss: 384.8049289700171\n",
      "Epoch 2 step 528: training accuarcy: 0.9978\n",
      "Epoch 2 step 528: training loss: 356.31919884529856\n",
      "Epoch 2 step 529: training accuarcy: 0.9978\n",
      "Epoch 2 step 529: training loss: 356.761232820384\n",
      "Epoch 2 step 530: training accuarcy: 0.999\n",
      "Epoch 2 step 530: training loss: 362.7373632910302\n",
      "Epoch 2 step 531: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 531: training loss: 409.8708334306421\n",
      "Epoch 2 step 532: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 532: training loss: 370.6629892123538\n",
      "Epoch 2 step 533: training accuarcy: 0.9965\n",
      "Epoch 2 step 533: training loss: 355.17098244522646\n",
      "Epoch 2 step 534: training accuarcy: 0.999\n",
      "Epoch 2 step 534: training loss: 378.93532783609265\n",
      "Epoch 2 step 535: training accuarcy: 0.9966\n",
      "Epoch 2 step 535: training loss: 339.88251935452354\n",
      "Epoch 2 step 536: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 536: training loss: 367.0548180342944\n",
      "Epoch 2 step 537: training accuarcy: 0.9996\n",
      "Epoch 2 step 537: training loss: 371.08341750030286\n",
      "Epoch 2 step 538: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 538: training loss: 391.7499176977963\n",
      "Epoch 2 step 539: training accuarcy: 0.9975\n",
      "Epoch 2 step 539: training loss: 359.8930280941354\n",
      "Epoch 2 step 540: training accuarcy: 0.9987\n",
      "Epoch 2 step 540: training loss: 393.08508090112423\n",
      "Epoch 2 step 541: training accuarcy: 0.9978\n",
      "Epoch 2 step 541: training loss: 370.3265336623399\n",
      "Epoch 2 step 542: training accuarcy: 0.9976\n",
      "Epoch 2 step 542: training loss: 371.0167451738488\n",
      "Epoch 2 step 543: training accuarcy: 0.9977\n",
      "Epoch 2 step 543: training loss: 358.2590293014148\n",
      "Epoch 2 step 544: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 544: training loss: 354.7116501426177\n",
      "Epoch 2 step 545: training accuarcy: 0.9978\n",
      "Epoch 2 step 545: training loss: 386.72488043662895\n",
      "Epoch 2 step 546: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 546: training loss: 354.0927187404975\n",
      "Epoch 2 step 547: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 547: training loss: 358.0693629588526\n",
      "Epoch 2 step 548: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 548: training loss: 345.785810889547\n",
      "Epoch 2 step 549: training accuarcy: 0.9991000000000001\n",
      "Epoch 2 step 549: training loss: 366.8371913332817\n",
      "Epoch 2 step 550: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 550: training loss: 358.046824763467\n",
      "Epoch 2 step 551: training accuarcy: 0.9979\n",
      "Epoch 2 step 551: training loss: 382.0166893576918\n",
      "Epoch 2 step 552: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 552: training loss: 373.6764051015116\n",
      "Epoch 2 step 553: training accuarcy: 0.996\n",
      "Epoch 2 step 553: training loss: 374.3137338054959\n",
      "Epoch 2 step 554: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 554: training loss: 358.5423605798652\n",
      "Epoch 2 step 555: training accuarcy: 0.9986\n",
      "Epoch 2 step 555: training loss: 363.27722702822905\n",
      "Epoch 2 step 556: training accuarcy: 0.9979\n",
      "Epoch 2 step 556: training loss: 375.19048032181644\n",
      "Epoch 2 step 557: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 557: training loss: 376.35054418433504\n",
      "Epoch 2 step 558: training accuarcy: 0.9969\n",
      "Epoch 2 step 558: training loss: 364.5917799597546\n",
      "Epoch 2 step 559: training accuarcy: 0.997\n",
      "Epoch 2 step 559: training loss: 377.49228124121896\n",
      "Epoch 2 step 560: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 560: training loss: 343.7059458924774\n",
      "Epoch 2 step 561: training accuarcy: 0.999\n",
      "Epoch 2 step 561: training loss: 377.3798867043789\n",
      "Epoch 2 step 562: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 562: training loss: 343.68167147062223\n",
      "Epoch 2 step 563: training accuarcy: 0.998\n",
      "Epoch 2 step 563: training loss: 360.30006774202513\n",
      "Epoch 2 step 564: training accuarcy: 0.9978\n",
      "Epoch 2 step 564: training loss: 352.9811638129926\n",
      "Epoch 2 step 565: training accuarcy: 0.9978\n",
      "Epoch 2 step 565: training loss: 356.28254703009884\n",
      "Epoch 2 step 566: training accuarcy: 0.9977\n",
      "Epoch 2 step 566: training loss: 367.8802407886041\n",
      "Epoch 2 step 567: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 567: training loss: 344.2162036331962\n",
      "Epoch 2 step 568: training accuarcy: 0.9992000000000001\n",
      "Epoch 2 step 568: training loss: 368.8813451366322\n",
      "Epoch 2 step 569: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 569: training loss: 324.3280475075035\n",
      "Epoch 2 step 570: training accuarcy: 0.9991000000000001\n",
      "Epoch 2 step 570: training loss: 396.2827879701968\n",
      "Epoch 2 step 571: training accuarcy: 0.9968\n",
      "Epoch 2 step 571: training loss: 361.71093942272154\n",
      "Epoch 2 step 572: training accuarcy: 0.9977\n",
      "Epoch 2 step 572: training loss: 362.38791172855974\n",
      "Epoch 2 step 573: training accuarcy: 0.9968\n",
      "Epoch 2 step 573: training loss: 331.7720972610663\n",
      "Epoch 2 step 574: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 574: training loss: 367.5233362856871\n",
      "Epoch 2 step 575: training accuarcy: 0.9967\n",
      "Epoch 2 step 575: training loss: 370.47064525448985\n",
      "Epoch 2 step 576: training accuarcy: 0.9967\n",
      "Epoch 2 step 576: training loss: 384.7079172333103\n",
      "Epoch 2 step 577: training accuarcy: 0.9941000000000001\n",
      "Epoch 2 step 577: training loss: 363.77183003314616\n",
      "Epoch 2 step 578: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 578: training loss: 360.45922702610756\n",
      "Epoch 2 step 579: training accuarcy: 0.9984000000000001\n",
      "Epoch 2 step 579: training loss: 360.7458437191511\n",
      "Epoch 2 step 580: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 580: training loss: 348.3126320080865\n",
      "Epoch 2 step 581: training accuarcy: 0.9977\n",
      "Epoch 2 step 581: training loss: 369.732953604175\n",
      "Epoch 2 step 582: training accuarcy: 0.9979\n",
      "Epoch 2 step 582: training loss: 344.6878195994676\n",
      "Epoch 2 step 583: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 583: training loss: 377.4115929532494\n",
      "Epoch 2 step 584: training accuarcy: 0.9965\n",
      "Epoch 2 step 584: training loss: 398.9636556550332\n",
      "Epoch 2 step 585: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 585: training loss: 365.46618652807337\n",
      "Epoch 2 step 586: training accuarcy: 0.9967\n",
      "Epoch 2 step 586: training loss: 342.7412948322632\n",
      "Epoch 2 step 587: training accuarcy: 0.998\n",
      "Epoch 2 step 587: training loss: 361.53337176196175\n",
      "Epoch 2 step 588: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 588: training loss: 377.3810550737307\n",
      "Epoch 2 step 589: training accuarcy: 0.9965\n",
      "Epoch 2 step 589: training loss: 336.6361612240375\n",
      "Epoch 2 step 590: training accuarcy: 0.9985\n",
      "Epoch 2 step 590: training loss: 367.9470000391867\n",
      "Epoch 2 step 591: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 591: training loss: 353.5276041963622\n",
      "Epoch 2 step 592: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 592: training loss: 336.4697801831603\n",
      "Epoch 2 step 593: training accuarcy: 0.9986\n",
      "Epoch 2 step 593: training loss: 335.1241363827919\n",
      "Epoch 2 step 594: training accuarcy: 0.9992000000000001\n",
      "Epoch 2 step 594: training loss: 376.4041706874642\n",
      "Epoch 2 step 595: training accuarcy: 0.9968\n",
      "Epoch 2 step 595: training loss: 351.20085928554806\n",
      "Epoch 2 step 596: training accuarcy: 0.9965\n",
      "Epoch 2 step 596: training loss: 359.3888810092476\n",
      "Epoch 2 step 597: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 597: training loss: 337.15438965992905\n",
      "Epoch 2 step 598: training accuarcy: 0.9984000000000001\n",
      "Epoch 2 step 598: training loss: 340.7751973150144\n",
      "Epoch 2 step 599: training accuarcy: 0.9977\n",
      "Epoch 2 step 599: training loss: 337.9178402028988\n",
      "Epoch 2 step 600: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 600: training loss: 379.41080183861254\n",
      "Epoch 2 step 601: training accuarcy: 0.9965\n",
      "Epoch 2 step 601: training loss: 359.85471834783596\n",
      "Epoch 2 step 602: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 602: training loss: 366.71100822624965\n",
      "Epoch 2 step 603: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 603: training loss: 365.0055615732794\n",
      "Epoch 2 step 604: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 604: training loss: 352.4230398306701\n",
      "Epoch 2 step 605: training accuarcy: 0.9978\n",
      "Epoch 2 step 605: training loss: 358.33843887028877\n",
      "Epoch 2 step 606: training accuarcy: 0.9978\n",
      "Epoch 2 step 606: training loss: 357.5174482604878\n",
      "Epoch 2 step 607: training accuarcy: 0.9979\n",
      "Epoch 2 step 607: training loss: 358.6218510251732\n",
      "Epoch 2 step 608: training accuarcy: 0.9975\n",
      "Epoch 2 step 608: training loss: 361.3149624561489\n",
      "Epoch 2 step 609: training accuarcy: 0.9968\n",
      "Epoch 2 step 609: training loss: 377.2613636394208\n",
      "Epoch 2 step 610: training accuarcy: 0.9969\n",
      "Epoch 2 step 610: training loss: 354.86207737352083\n",
      "Epoch 2 step 611: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 611: training loss: 385.3298007830897\n",
      "Epoch 2 step 612: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 612: training loss: 338.9122788196206\n",
      "Epoch 2 step 613: training accuarcy: 0.9976\n",
      "Epoch 2 step 613: training loss: 371.9828201566735\n",
      "Epoch 2 step 614: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 614: training loss: 351.38334072019495\n",
      "Epoch 2 step 615: training accuarcy: 0.9979\n",
      "Epoch 2 step 615: training loss: 351.75857508924406\n",
      "Epoch 2 step 616: training accuarcy: 0.9977\n",
      "Epoch 2 step 616: training loss: 381.56188285866347\n",
      "Epoch 2 step 617: training accuarcy: 0.9968\n",
      "Epoch 2 step 617: training loss: 362.6179404059551\n",
      "Epoch 2 step 618: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 618: training loss: 338.4652591841173\n",
      "Epoch 2 step 619: training accuarcy: 0.9977\n",
      "Epoch 2 step 619: training loss: 368.4567131681408\n",
      "Epoch 2 step 620: training accuarcy: 0.9965\n",
      "Epoch 2 step 620: training loss: 361.2430724056792\n",
      "Epoch 2 step 621: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 621: training loss: 379.6275678429901\n",
      "Epoch 2 step 622: training accuarcy: 0.9956\n",
      "Epoch 2 step 622: training loss: 353.75629651016305\n",
      "Epoch 2 step 623: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 623: training loss: 374.0740467924328\n",
      "Epoch 2 step 624: training accuarcy: 0.9969\n",
      "Epoch 2 step 624: training loss: 354.88263673478593\n",
      "Epoch 2 step 625: training accuarcy: 0.9992000000000001\n",
      "Epoch 2 step 625: training loss: 384.56685138746445\n",
      "Epoch 2 step 626: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 626: training loss: 363.460376412968\n",
      "Epoch 2 step 627: training accuarcy: 0.9965\n",
      "Epoch 2 step 627: training loss: 348.2096215927206\n",
      "Epoch 2 step 628: training accuarcy: 0.9979\n",
      "Epoch 2 step 628: training loss: 364.83539064214494\n",
      "Epoch 2 step 629: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 629: training loss: 363.6592723875462\n",
      "Epoch 2 step 630: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 630: training loss: 370.383697529124\n",
      "Epoch 2 step 631: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 631: training loss: 379.4740846139017\n",
      "Epoch 2 step 632: training accuarcy: 0.9955\n",
      "Epoch 2 step 632: training loss: 360.1349731238254\n",
      "Epoch 2 step 633: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 633: training loss: 346.67115760619834\n",
      "Epoch 2 step 634: training accuarcy: 0.9978\n",
      "Epoch 2 step 634: training loss: 375.8318307421523\n",
      "Epoch 2 step 635: training accuarcy: 0.9947\n",
      "Epoch 2 step 635: training loss: 333.58470533839824\n",
      "Epoch 2 step 636: training accuarcy: 0.9975\n",
      "Epoch 2 step 636: training loss: 330.69455548473843\n",
      "Epoch 2 step 637: training accuarcy: 0.9975\n",
      "Epoch 2 step 637: training loss: 394.5084889018067\n",
      "Epoch 2 step 638: training accuarcy: 0.9941000000000001\n",
      "Epoch 2 step 638: training loss: 329.41403760503556\n",
      "Epoch 2 step 639: training accuarcy: 0.9977\n",
      "Epoch 2 step 639: training loss: 361.5276467477304\n",
      "Epoch 2 step 640: training accuarcy: 0.998\n",
      "Epoch 2 step 640: training loss: 357.53557152001804\n",
      "Epoch 2 step 641: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 641: training loss: 337.8011282586017\n",
      "Epoch 2 step 642: training accuarcy: 0.9987\n",
      "Epoch 2 step 642: training loss: 397.2371577270547\n",
      "Epoch 2 step 643: training accuarcy: 0.9959\n",
      "Epoch 2 step 643: training loss: 367.50955967588254\n",
      "Epoch 2 step 644: training accuarcy: 0.9967\n",
      "Epoch 2 step 644: training loss: 342.2903952736709\n",
      "Epoch 2 step 645: training accuarcy: 0.9978\n",
      "Epoch 2 step 645: training loss: 379.09047522794094\n",
      "Epoch 2 step 646: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 646: training loss: 358.0343522125755\n",
      "Epoch 2 step 647: training accuarcy: 0.9968\n",
      "Epoch 2 step 647: training loss: 347.01637193317407\n",
      "Epoch 2 step 648: training accuarcy: 0.9966\n",
      "Epoch 2 step 648: training loss: 318.8431520998342\n",
      "Epoch 2 step 649: training accuarcy: 0.9975\n",
      "Epoch 2 step 649: training loss: 341.5988737790746\n",
      "Epoch 2 step 650: training accuarcy: 0.998\n",
      "Epoch 2 step 650: training loss: 354.63488170962535\n",
      "Epoch 2 step 651: training accuarcy: 0.9968\n",
      "Epoch 2 step 651: training loss: 353.888431964632\n",
      "Epoch 2 step 652: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 652: training loss: 358.8014540066648\n",
      "Epoch 2 step 653: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 653: training loss: 350.4446174479548\n",
      "Epoch 2 step 654: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 654: training loss: 364.82570030384943\n",
      "Epoch 2 step 655: training accuarcy: 0.998\n",
      "Epoch 2 step 655: training loss: 331.93993865177913\n",
      "Epoch 2 step 656: training accuarcy: 0.9988\n",
      "Epoch 2 step 656: training loss: 328.0148760652769\n",
      "Epoch 2 step 657: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 657: training loss: 330.13652584867225\n",
      "Epoch 2 step 658: training accuarcy: 0.9979\n",
      "Epoch 2 step 658: training loss: 362.0568249032648\n",
      "Epoch 2 step 659: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 659: training loss: 336.54481920751766\n",
      "Epoch 2 step 660: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 660: training loss: 330.06189186857097\n",
      "Epoch 2 step 661: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 661: training loss: 356.6659034374406\n",
      "Epoch 2 step 662: training accuarcy: 0.9976\n",
      "Epoch 2 step 662: training loss: 359.35041742209205\n",
      "Epoch 2 step 663: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 663: training loss: 355.8676030211391\n",
      "Epoch 2 step 664: training accuarcy: 0.997\n",
      "Epoch 2 step 664: training loss: 314.5755082774908\n",
      "Epoch 2 step 665: training accuarcy: 0.9992000000000001\n",
      "Epoch 2 step 665: training loss: 363.7677740320702\n",
      "Epoch 2 step 666: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 666: training loss: 330.24852192892627\n",
      "Epoch 2 step 667: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 667: training loss: 333.3938140417955\n",
      "Epoch 2 step 668: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 668: training loss: 376.0100193604327\n",
      "Epoch 2 step 669: training accuarcy: 0.9956\n",
      "Epoch 2 step 669: training loss: 383.5658761358915\n",
      "Epoch 2 step 670: training accuarcy: 0.9957\n",
      "Epoch 2 step 670: training loss: 338.20838045826\n",
      "Epoch 2 step 671: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 671: training loss: 361.2538602095847\n",
      "Epoch 2 step 672: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 672: training loss: 365.11334876911474\n",
      "Epoch 2 step 673: training accuarcy: 0.9958\n",
      "Epoch 2 step 673: training loss: 344.755005567836\n",
      "Epoch 2 step 674: training accuarcy: 0.9966\n",
      "Epoch 2 step 674: training loss: 372.31105450493976\n",
      "Epoch 2 step 675: training accuarcy: 0.9957\n",
      "Epoch 2 step 675: training loss: 397.21334721083855\n",
      "Epoch 2 step 676: training accuarcy: 0.9957\n",
      "Epoch 2 step 676: training loss: 344.4193342291922\n",
      "Epoch 2 step 677: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 677: training loss: 306.6249585706901\n",
      "Epoch 2 step 678: training accuarcy: 0.9997\n",
      "Epoch 2 step 678: training loss: 357.2143380967941\n",
      "Epoch 2 step 679: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 679: training loss: 369.21508167425947\n",
      "Epoch 2 step 680: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 680: training loss: 365.66073298632966\n",
      "Epoch 2 step 681: training accuarcy: 0.997\n",
      "Epoch 2 step 681: training loss: 339.4179774802659\n",
      "Epoch 2 step 682: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 682: training loss: 328.7522289053637\n",
      "Epoch 2 step 683: training accuarcy: 0.9979\n",
      "Epoch 2 step 683: training loss: 378.9641090195603\n",
      "Epoch 2 step 684: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 684: training loss: 362.1808103676728\n",
      "Epoch 2 step 685: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 685: training loss: 359.57152711057097\n",
      "Epoch 2 step 686: training accuarcy: 0.9948\n",
      "Epoch 2 step 686: training loss: 370.0959265566074\n",
      "Epoch 2 step 687: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 687: training loss: 342.33541911580363\n",
      "Epoch 2 step 688: training accuarcy: 0.9979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 688: training loss: 375.2483641722556\n",
      "Epoch 2 step 689: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 689: training loss: 342.9448350660174\n",
      "Epoch 2 step 690: training accuarcy: 0.9979\n",
      "Epoch 2 step 690: training loss: 359.4577075171537\n",
      "Epoch 2 step 691: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 691: training loss: 330.59003584137616\n",
      "Epoch 2 step 692: training accuarcy: 0.9986\n",
      "Epoch 2 step 692: training loss: 340.5669023189479\n",
      "Epoch 2 step 693: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 693: training loss: 350.6996320764523\n",
      "Epoch 2 step 694: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 694: training loss: 374.09681592900597\n",
      "Epoch 2 step 695: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 695: training loss: 336.74055270371014\n",
      "Epoch 2 step 696: training accuarcy: 0.9966\n",
      "Epoch 2 step 696: training loss: 363.0581063136573\n",
      "Epoch 2 step 697: training accuarcy: 0.996\n",
      "Epoch 2 step 697: training loss: 353.8461701288368\n",
      "Epoch 2 step 698: training accuarcy: 0.998\n",
      "Epoch 2 step 698: training loss: 346.48174880757233\n",
      "Epoch 2 step 699: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 699: training loss: 366.3953028332457\n",
      "Epoch 2 step 700: training accuarcy: 0.9966\n",
      "Epoch 2 step 700: training loss: 353.2986712959754\n",
      "Epoch 2 step 701: training accuarcy: 0.9967\n",
      "Epoch 2 step 701: training loss: 337.9109375619919\n",
      "Epoch 2 step 702: training accuarcy: 0.9966\n",
      "Epoch 2 step 702: training loss: 321.97590800902543\n",
      "Epoch 2 step 703: training accuarcy: 0.9988\n",
      "Epoch 2 step 703: training loss: 355.4922886022997\n",
      "Epoch 2 step 704: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 704: training loss: 351.9492961681575\n",
      "Epoch 2 step 705: training accuarcy: 0.9965\n",
      "Epoch 2 step 705: training loss: 376.5694435666845\n",
      "Epoch 2 step 706: training accuarcy: 0.9958\n",
      "Epoch 2 step 706: training loss: 366.60685949482047\n",
      "Epoch 2 step 707: training accuarcy: 0.996\n",
      "Epoch 2 step 707: training loss: 343.000173114528\n",
      "Epoch 2 step 708: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 708: training loss: 328.47254214016095\n",
      "Epoch 2 step 709: training accuarcy: 0.9967\n",
      "Epoch 2 step 709: training loss: 363.06228456629435\n",
      "Epoch 2 step 710: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 710: training loss: 351.9054243140592\n",
      "Epoch 2 step 711: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 711: training loss: 350.3911392662476\n",
      "Epoch 2 step 712: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 712: training loss: 354.84423391094674\n",
      "Epoch 2 step 713: training accuarcy: 0.9975\n",
      "Epoch 2 step 713: training loss: 342.9516403980697\n",
      "Epoch 2 step 714: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 714: training loss: 354.1496678180624\n",
      "Epoch 2 step 715: training accuarcy: 0.9975\n",
      "Epoch 2 step 715: training loss: 341.16230344486524\n",
      "Epoch 2 step 716: training accuarcy: 0.9977\n",
      "Epoch 2 step 716: training loss: 330.37194068005397\n",
      "Epoch 2 step 717: training accuarcy: 0.9977\n",
      "Epoch 2 step 717: training loss: 341.4238000784554\n",
      "Epoch 2 step 718: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 718: training loss: 341.0250937381714\n",
      "Epoch 2 step 719: training accuarcy: 0.9979\n",
      "Epoch 2 step 719: training loss: 363.44580514563717\n",
      "Epoch 2 step 720: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 720: training loss: 317.0743763460424\n",
      "Epoch 2 step 721: training accuarcy: 0.9988\n",
      "Epoch 2 step 721: training loss: 342.442113927565\n",
      "Epoch 2 step 722: training accuarcy: 0.9978\n",
      "Epoch 2 step 722: training loss: 320.76303873016457\n",
      "Epoch 2 step 723: training accuarcy: 0.9976\n",
      "Epoch 2 step 723: training loss: 349.0991043096379\n",
      "Epoch 2 step 724: training accuarcy: 0.9976\n",
      "Epoch 2 step 724: training loss: 349.6538493531317\n",
      "Epoch 2 step 725: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 725: training loss: 330.9070277904176\n",
      "Epoch 2 step 726: training accuarcy: 0.9978\n",
      "Epoch 2 step 726: training loss: 341.3221729675764\n",
      "Epoch 2 step 727: training accuarcy: 0.9966\n",
      "Epoch 2 step 727: training loss: 369.49201546375286\n",
      "Epoch 2 step 728: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 728: training loss: 356.0413004996992\n",
      "Epoch 2 step 729: training accuarcy: 0.9957\n",
      "Epoch 2 step 729: training loss: 348.36775007659935\n",
      "Epoch 2 step 730: training accuarcy: 0.9975\n",
      "Epoch 2 step 730: training loss: 362.52593861837727\n",
      "Epoch 2 step 731: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 731: training loss: 320.6515702698697\n",
      "Epoch 2 step 732: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 732: training loss: 343.479713642736\n",
      "Epoch 2 step 733: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 733: training loss: 387.159942808804\n",
      "Epoch 2 step 734: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 734: training loss: 323.57117883929396\n",
      "Epoch 2 step 735: training accuarcy: 0.9985\n",
      "Epoch 2 step 735: training loss: 367.5442672599605\n",
      "Epoch 2 step 736: training accuarcy: 0.9959\n",
      "Epoch 2 step 736: training loss: 361.906422090398\n",
      "Epoch 2 step 737: training accuarcy: 0.9956\n",
      "Epoch 2 step 737: training loss: 350.28769813446024\n",
      "Epoch 2 step 738: training accuarcy: 0.9966\n",
      "Epoch 2 step 738: training loss: 316.608898539856\n",
      "Epoch 2 step 739: training accuarcy: 0.998\n",
      "Epoch 2 step 739: training loss: 319.8835822319743\n",
      "Epoch 2 step 740: training accuarcy: 0.9969\n",
      "Epoch 2 step 740: training loss: 337.7134805816683\n",
      "Epoch 2 step 741: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 741: training loss: 330.6158029176834\n",
      "Epoch 2 step 742: training accuarcy: 0.9977\n",
      "Epoch 2 step 742: training loss: 347.04980849561105\n",
      "Epoch 2 step 743: training accuarcy: 0.9966\n",
      "Epoch 2 step 743: training loss: 341.30567266294366\n",
      "Epoch 2 step 744: training accuarcy: 0.9978\n",
      "Epoch 2 step 744: training loss: 336.51740425019\n",
      "Epoch 2 step 745: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 745: training loss: 340.619660640746\n",
      "Epoch 2 step 746: training accuarcy: 0.9966\n",
      "Epoch 2 step 746: training loss: 314.6267441734062\n",
      "Epoch 2 step 747: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 747: training loss: 322.24350643773823\n",
      "Epoch 2 step 748: training accuarcy: 0.9986\n",
      "Epoch 2 step 748: training loss: 338.91178978385017\n",
      "Epoch 2 step 749: training accuarcy: 0.9967\n",
      "Epoch 2 step 749: training loss: 338.04749513926294\n",
      "Epoch 2 step 750: training accuarcy: 0.9976\n",
      "Epoch 2 step 750: training loss: 325.5071169604344\n",
      "Epoch 2 step 751: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 751: training loss: 370.8038343550178\n",
      "Epoch 2 step 752: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 752: training loss: 358.71709865387425\n",
      "Epoch 2 step 753: training accuarcy: 0.9958\n",
      "Epoch 2 step 753: training loss: 336.97550154774984\n",
      "Epoch 2 step 754: training accuarcy: 0.9967\n",
      "Epoch 2 step 754: training loss: 307.56948044235315\n",
      "Epoch 2 step 755: training accuarcy: 0.9979\n",
      "Epoch 2 step 755: training loss: 343.3394035979005\n",
      "Epoch 2 step 756: training accuarcy: 0.9978\n",
      "Epoch 2 step 756: training loss: 373.9033261952111\n",
      "Epoch 2 step 757: training accuarcy: 0.9958\n",
      "Epoch 2 step 757: training loss: 346.9314333634575\n",
      "Epoch 2 step 758: training accuarcy: 0.997\n",
      "Epoch 2 step 758: training loss: 322.245233588344\n",
      "Epoch 2 step 759: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 759: training loss: 320.85327715662623\n",
      "Epoch 2 step 760: training accuarcy: 0.9978\n",
      "Epoch 2 step 760: training loss: 319.86422597763516\n",
      "Epoch 2 step 761: training accuarcy: 0.9978\n",
      "Epoch 2 step 761: training loss: 322.3973425019395\n",
      "Epoch 2 step 762: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 762: training loss: 338.74485204193\n",
      "Epoch 2 step 763: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 763: training loss: 296.91274880164235\n",
      "Epoch 2 step 764: training accuarcy: 0.9993000000000001\n",
      "Epoch 2 step 764: training loss: 339.5134393094062\n",
      "Epoch 2 step 765: training accuarcy: 0.9969\n",
      "Epoch 2 step 765: training loss: 340.0097827744453\n",
      "Epoch 2 step 766: training accuarcy: 0.9976\n",
      "Epoch 2 step 766: training loss: 342.34396381551403\n",
      "Epoch 2 step 767: training accuarcy: 0.9968\n",
      "Epoch 2 step 767: training loss: 315.157275263374\n",
      "Epoch 2 step 768: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 768: training loss: 338.3196822784192\n",
      "Epoch 2 step 769: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 769: training loss: 345.69685066353344\n",
      "Epoch 2 step 770: training accuarcy: 0.9955\n",
      "Epoch 2 step 770: training loss: 331.6722159311154\n",
      "Epoch 2 step 771: training accuarcy: 0.9975\n",
      "Epoch 2 step 771: training loss: 318.5717784066992\n",
      "Epoch 2 step 772: training accuarcy: 0.9989\n",
      "Epoch 2 step 772: training loss: 304.64454724768274\n",
      "Epoch 2 step 773: training accuarcy: 0.9993000000000001\n",
      "Epoch 2 step 773: training loss: 339.63084325082866\n",
      "Epoch 2 step 774: training accuarcy: 0.9955\n",
      "Epoch 2 step 774: training loss: 341.6755270532709\n",
      "Epoch 2 step 775: training accuarcy: 0.997\n",
      "Epoch 2 step 775: training loss: 301.7743740648524\n",
      "Epoch 2 step 776: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 776: training loss: 326.94688439260335\n",
      "Epoch 2 step 777: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 777: training loss: 318.78065681837705\n",
      "Epoch 2 step 778: training accuarcy: 0.997\n",
      "Epoch 2 step 778: training loss: 343.08533286276105\n",
      "Epoch 2 step 779: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 779: training loss: 319.9862566148262\n",
      "Epoch 2 step 780: training accuarcy: 0.9975\n",
      "Epoch 2 step 780: training loss: 340.50833114911086\n",
      "Epoch 2 step 781: training accuarcy: 0.9968\n",
      "Epoch 2 step 781: training loss: 356.4449644833875\n",
      "Epoch 2 step 782: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 782: training loss: 325.2431180090905\n",
      "Epoch 2 step 783: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 783: training loss: 355.9937899533502\n",
      "Epoch 2 step 784: training accuarcy: 0.997\n",
      "Epoch 2 step 784: training loss: 354.0272731925885\n",
      "Epoch 2 step 785: training accuarcy: 0.9955\n",
      "Epoch 2 step 785: training loss: 312.1987452069394\n",
      "Epoch 2 step 786: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 786: training loss: 339.13717831656425\n",
      "Epoch 2 step 787: training accuarcy: 0.9967\n",
      "Epoch 2 step 787: training loss: 353.4691181950666\n",
      "Epoch 2 step 788: training accuarcy: 0.995\n",
      "Epoch 2 step 788: training loss: 267.00680214411557\n",
      "Epoch 2 step 789: training accuarcy: 0.9956410256410256\n",
      "Epoch 2: train loss 352.318687755944, train accuarcy 0.993584930896759\n",
      "Epoch 2: valid loss 626.6299229773125, valid accuarcy 0.9942793846130371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 3/5 [14:28<09:41, 290.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 296.23032878304855\n",
      "Epoch 3 step 790: training accuarcy: 0.999\n",
      "Epoch 3 step 790: training loss: 300.7896643394858\n",
      "Epoch 3 step 791: training accuarcy: 0.9983000000000001\n",
      "Epoch 3 step 791: training loss: 293.4835120560487\n",
      "Epoch 3 step 792: training accuarcy: 0.9989\n",
      "Epoch 3 step 792: training loss: 285.5181151599255\n",
      "Epoch 3 step 793: training accuarcy: 0.9991000000000001\n",
      "Epoch 3 step 793: training loss: 302.478868166378\n",
      "Epoch 3 step 794: training accuarcy: 0.9982000000000001\n",
      "Epoch 3 step 794: training loss: 316.27082484827275\n",
      "Epoch 3 step 795: training accuarcy: 0.9989\n",
      "Epoch 3 step 795: training loss: 293.5194634860173\n",
      "Epoch 3 step 796: training accuarcy: 0.9977\n",
      "Epoch 3 step 796: training loss: 298.44195453152906\n",
      "Epoch 3 step 797: training accuarcy: 0.9985\n",
      "Epoch 3 step 797: training loss: 274.5905129792551\n",
      "Epoch 3 step 798: training accuarcy: 0.9989\n",
      "Epoch 3 step 798: training loss: 313.0679676110951\n",
      "Epoch 3 step 799: training accuarcy: 0.9989\n",
      "Epoch 3 step 799: training loss: 289.94282451757886\n",
      "Epoch 3 step 800: training accuarcy: 0.9988\n",
      "Epoch 3 step 800: training loss: 304.66987975262606\n",
      "Epoch 3 step 801: training accuarcy: 0.997\n",
      "Epoch 3 step 801: training loss: 290.99150960606505\n",
      "Epoch 3 step 802: training accuarcy: 0.9979\n",
      "Epoch 3 step 802: training loss: 303.28746394201846\n",
      "Epoch 3 step 803: training accuarcy: 0.9979\n",
      "Epoch 3 step 803: training loss: 278.6161234543749\n",
      "Epoch 3 step 804: training accuarcy: 0.9991000000000001\n",
      "Epoch 3 step 804: training loss: 307.3240067918505\n",
      "Epoch 3 step 805: training accuarcy: 0.9979\n",
      "Epoch 3 step 805: training loss: 294.73003690929716\n",
      "Epoch 3 step 806: training accuarcy: 0.9971000000000001\n",
      "Epoch 3 step 806: training loss: 306.44739375162226\n",
      "Epoch 3 step 807: training accuarcy: 0.9977\n",
      "Epoch 3 step 807: training loss: 302.0016683125134\n",
      "Epoch 3 step 808: training accuarcy: 0.9975\n",
      "Epoch 3 step 808: training loss: 281.5326337209339\n",
      "Epoch 3 step 809: training accuarcy: 0.9986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-30a929e26ac3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m fm_learner.fit(epoch=5,\n\u001b[1;32m----> 2\u001b[1;33m                log_dir=get_log_dir('simple_topcoder', 'fm'))\n\u001b[0m",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, epoch, log_dir)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mschedular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muser_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\datasets\\torch_topcoder.py\u001b[0m in \u001b[0;36m_seq_collate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    201\u001b[0m             chag_df[chag_df['period'] == per].sample(n=per_counts[per],\n\u001b[0;32m    202\u001b[0m                                                      replace=True)\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mper\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mper_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         ]\n\u001b[0;32m    205\u001b[0m         \u001b[0mneg_feat_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\datasets\\torch_topcoder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    201\u001b[0m             chag_df[chag_df['period'] == per].sample(n=per_counts[per],\n\u001b[0;32m    202\u001b[0m                                                      replace=True)\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mper\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mper_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         ]\n\u001b[0;32m    205\u001b[0m         \u001b[0mneg_feat_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2916\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2918\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2920\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2967\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take\u001b[1;34m(self, indices, axis, is_copy)\u001b[0m\n\u001b[0;32m   3357\u001b[0m         new_data = self._data.take(indices,\n\u001b[0;32m   3358\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3359\u001b[1;33m                                    verify=True)\n\u001b[0m\u001b[0;32m   3360\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[1;32m-> 1350\u001b[1;33m                                     axis=axis, allow_dups=True)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1237\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[0mnew_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slice_take_blocks_ax0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_or_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rebuild_blknos_and_blklocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_rebuild_blknos_and_blklocs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0mnew_blknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[0mnew_blklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_blknos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=5,\n",
    "               log_dir=get_log_dir('simple_topcoder', 'fm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:56:07.718333Z",
     "start_time": "2019-10-09T11:41:44.391956Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 96099.51260676965\n",
      "Epoch 0 step 1: training accuarcy: 0.29510000000000003\n",
      "Epoch 0 step 1: training loss: 88335.88527898806\n",
      "Epoch 0 step 2: training accuarcy: 0.3618\n",
      "Epoch 0 step 2: training loss: 82955.90096143921\n",
      "Epoch 0 step 3: training accuarcy: 0.41050000000000003\n",
      "Epoch 0 step 3: training loss: 75841.65049929358\n",
      "Epoch 0 step 4: training accuarcy: 0.47500000000000003\n",
      "Epoch 0 step 4: training loss: 69900.84115374796\n",
      "Epoch 0 step 5: training accuarcy: 0.5316000000000001\n",
      "Epoch 0 step 5: training loss: 66515.16308581155\n",
      "Epoch 0 step 6: training accuarcy: 0.5718\n",
      "Epoch 0 step 6: training loss: 62505.37561871747\n",
      "Epoch 0 step 7: training accuarcy: 0.6056\n",
      "Epoch 0 step 7: training loss: 57537.948671329985\n",
      "Epoch 0 step 8: training accuarcy: 0.6546000000000001\n",
      "Epoch 0 step 8: training loss: 56306.74673244863\n",
      "Epoch 0 step 9: training accuarcy: 0.6639\n",
      "Epoch 0 step 9: training loss: 54004.51565162408\n",
      "Epoch 0 step 10: training accuarcy: 0.6841\n",
      "Epoch 0 step 10: training loss: 52092.79156344346\n",
      "Epoch 0 step 11: training accuarcy: 0.6947\n",
      "Epoch 0 step 11: training loss: 50497.44627196995\n",
      "Epoch 0 step 12: training accuarcy: 0.7148\n",
      "Epoch 0 step 12: training loss: 47546.291487897244\n",
      "Epoch 0 step 13: training accuarcy: 0.7436\n",
      "Epoch 0 step 13: training loss: 45964.037996165964\n",
      "Epoch 0 step 14: training accuarcy: 0.7632\n",
      "Epoch 0 step 14: training loss: 44430.30105638853\n",
      "Epoch 0 step 15: training accuarcy: 0.7721\n",
      "Epoch 0 step 15: training loss: 40924.010835627276\n",
      "Epoch 0 step 16: training accuarcy: 0.8109000000000001\n",
      "Epoch 0 step 16: training loss: 41229.720291221936\n",
      "Epoch 0 step 17: training accuarcy: 0.8054\n",
      "Epoch 0 step 17: training loss: 39660.46386698006\n",
      "Epoch 0 step 18: training accuarcy: 0.8180000000000001\n",
      "Epoch 0 step 18: training loss: 38270.73735652237\n",
      "Epoch 0 step 19: training accuarcy: 0.8273\n",
      "Epoch 0 step 19: training loss: 38662.085452056665\n",
      "Epoch 0 step 20: training accuarcy: 0.8112\n",
      "Epoch 0 step 20: training loss: 36002.56431866517\n",
      "Epoch 0 step 21: training accuarcy: 0.8411000000000001\n",
      "Epoch 0 step 21: training loss: 36748.02245214078\n",
      "Epoch 0 step 22: training accuarcy: 0.8440000000000001\n",
      "Epoch 0 step 22: training loss: 34816.30376877714\n",
      "Epoch 0 step 23: training accuarcy: 0.8547\n",
      "Epoch 0 step 23: training loss: 34413.769964374034\n",
      "Epoch 0 step 24: training accuarcy: 0.8544\n",
      "Epoch 0 step 24: training loss: 33353.930288466916\n",
      "Epoch 0 step 25: training accuarcy: 0.8564\n",
      "Epoch 0 step 25: training loss: 32390.97325353021\n",
      "Epoch 0 step 26: training accuarcy: 0.8659\n",
      "Epoch 0 step 26: training loss: 31799.210569694238\n",
      "Epoch 0 step 27: training accuarcy: 0.8725\n",
      "Epoch 0 step 27: training loss: 32223.745278635073\n",
      "Epoch 0 step 28: training accuarcy: 0.8617\n",
      "Epoch 0 step 28: training loss: 30613.122838401832\n",
      "Epoch 0 step 29: training accuarcy: 0.8781\n",
      "Epoch 0 step 29: training loss: 30037.00062051333\n",
      "Epoch 0 step 30: training accuarcy: 0.8855000000000001\n",
      "Epoch 0 step 30: training loss: 28916.69939003615\n",
      "Epoch 0 step 31: training accuarcy: 0.89\n",
      "Epoch 0 step 31: training loss: 29597.94337084506\n",
      "Epoch 0 step 32: training accuarcy: 0.8803000000000001\n",
      "Epoch 0 step 32: training loss: 29590.242222751673\n",
      "Epoch 0 step 33: training accuarcy: 0.8709\n",
      "Epoch 0 step 33: training loss: 28097.505616760987\n",
      "Epoch 0 step 34: training accuarcy: 0.8925000000000001\n",
      "Epoch 0 step 34: training loss: 28150.805828712546\n",
      "Epoch 0 step 35: training accuarcy: 0.889\n",
      "Epoch 0 step 35: training loss: 27233.659450903477\n",
      "Epoch 0 step 36: training accuarcy: 0.8976000000000001\n",
      "Epoch 0 step 36: training loss: 26624.133039187334\n",
      "Epoch 0 step 37: training accuarcy: 0.8973000000000001\n",
      "Epoch 0 step 37: training loss: 26503.81546120587\n",
      "Epoch 0 step 38: training accuarcy: 0.9001\n",
      "Epoch 0 step 38: training loss: 26488.246382310874\n",
      "Epoch 0 step 39: training accuarcy: 0.8926000000000001\n",
      "Epoch 0 step 39: training loss: 26020.141105323157\n",
      "Epoch 0 step 40: training accuarcy: 0.9004000000000001\n",
      "Epoch 0 step 40: training loss: 25317.482695624214\n",
      "Epoch 0 step 41: training accuarcy: 0.9106000000000001\n",
      "Epoch 0 step 41: training loss: 24997.46986938546\n",
      "Epoch 0 step 42: training accuarcy: 0.9088\n",
      "Epoch 0 step 42: training loss: 25720.19103453218\n",
      "Epoch 0 step 43: training accuarcy: 0.8967\n",
      "Epoch 0 step 43: training loss: 25692.01407790656\n",
      "Epoch 0 step 44: training accuarcy: 0.8949\n",
      "Epoch 0 step 44: training loss: 24794.65014836484\n",
      "Epoch 0 step 45: training accuarcy: 0.8985000000000001\n",
      "Epoch 0 step 45: training loss: 23879.490042990743\n",
      "Epoch 0 step 46: training accuarcy: 0.9071\n",
      "Epoch 0 step 46: training loss: 23098.69881980529\n",
      "Epoch 0 step 47: training accuarcy: 0.9193\n",
      "Epoch 0 step 47: training loss: 23372.88277172666\n",
      "Epoch 0 step 48: training accuarcy: 0.9117000000000001\n",
      "Epoch 0 step 48: training loss: 23253.937540642422\n",
      "Epoch 0 step 49: training accuarcy: 0.9098\n",
      "Epoch 0 step 49: training loss: 22841.621651341033\n",
      "Epoch 0 step 50: training accuarcy: 0.9114000000000001\n",
      "Epoch 0 step 50: training loss: 23357.760056842657\n",
      "Epoch 0 step 51: training accuarcy: 0.9076000000000001\n",
      "Epoch 0 step 51: training loss: 22356.596470092525\n",
      "Epoch 0 step 52: training accuarcy: 0.9185000000000001\n",
      "Epoch 0 step 52: training loss: 23360.514251395638\n",
      "Epoch 0 step 53: training accuarcy: 0.9006000000000001\n",
      "Epoch 0 step 53: training loss: 22326.663776895988\n",
      "Epoch 0 step 54: training accuarcy: 0.912\n",
      "Epoch 0 step 54: training loss: 21689.281521883106\n",
      "Epoch 0 step 55: training accuarcy: 0.9178000000000001\n",
      "Epoch 0 step 55: training loss: 21097.615564544372\n",
      "Epoch 0 step 56: training accuarcy: 0.9246000000000001\n",
      "Epoch 0 step 56: training loss: 22708.324555639578\n",
      "Epoch 0 step 57: training accuarcy: 0.9002\n",
      "Epoch 0 step 57: training loss: 20469.958688770454\n",
      "Epoch 0 step 58: training accuarcy: 0.9324\n",
      "Epoch 0 step 58: training loss: 21619.449955774486\n",
      "Epoch 0 step 59: training accuarcy: 0.9098\n",
      "Epoch 0 step 59: training loss: 20692.84302621839\n",
      "Epoch 0 step 60: training accuarcy: 0.93\n",
      "Epoch 0 step 60: training loss: 20359.31372995396\n",
      "Epoch 0 step 61: training accuarcy: 0.932\n",
      "Epoch 0 step 61: training loss: 20765.642503152034\n",
      "Epoch 0 step 62: training accuarcy: 0.9161\n",
      "Epoch 0 step 62: training loss: 20042.27445190447\n",
      "Epoch 0 step 63: training accuarcy: 0.9319000000000001\n",
      "Epoch 0 step 63: training loss: 20133.863765588285\n",
      "Epoch 0 step 64: training accuarcy: 0.9292\n",
      "Epoch 0 step 64: training loss: 20029.099930118544\n",
      "Epoch 0 step 65: training accuarcy: 0.9254\n",
      "Epoch 0 step 65: training loss: 19944.906890262784\n",
      "Epoch 0 step 66: training accuarcy: 0.9293\n",
      "Epoch 0 step 66: training loss: 19440.507952469794\n",
      "Epoch 0 step 67: training accuarcy: 0.9282\n",
      "Epoch 0 step 67: training loss: 19584.844403267394\n",
      "Epoch 0 step 68: training accuarcy: 0.9298000000000001\n",
      "Epoch 0 step 68: training loss: 19609.24674330537\n",
      "Epoch 0 step 69: training accuarcy: 0.9264\n",
      "Epoch 0 step 69: training loss: 18712.44428658543\n",
      "Epoch 0 step 70: training accuarcy: 0.9388000000000001\n",
      "Epoch 0 step 70: training loss: 18612.018486475565\n",
      "Epoch 0 step 71: training accuarcy: 0.9361\n",
      "Epoch 0 step 71: training loss: 19170.023134561176\n",
      "Epoch 0 step 72: training accuarcy: 0.9288000000000001\n",
      "Epoch 0 step 72: training loss: 18988.16521152961\n",
      "Epoch 0 step 73: training accuarcy: 0.9284\n",
      "Epoch 0 step 73: training loss: 18498.454600846162\n",
      "Epoch 0 step 74: training accuarcy: 0.9376000000000001\n",
      "Epoch 0 step 74: training loss: 17761.195573976598\n",
      "Epoch 0 step 75: training accuarcy: 0.9457000000000001\n",
      "Epoch 0 step 75: training loss: 17561.367685931862\n",
      "Epoch 0 step 76: training accuarcy: 0.9436\n",
      "Epoch 0 step 76: training loss: 17814.685101782987\n",
      "Epoch 0 step 77: training accuarcy: 0.9378000000000001\n",
      "Epoch 0 step 77: training loss: 18242.89641170801\n",
      "Epoch 0 step 78: training accuarcy: 0.9383\n",
      "Epoch 0 step 78: training loss: 17366.3679513772\n",
      "Epoch 0 step 79: training accuarcy: 0.9452\n",
      "Epoch 0 step 79: training loss: 16763.451861627756\n",
      "Epoch 0 step 80: training accuarcy: 0.9473\n",
      "Epoch 0 step 80: training loss: 17898.783786915672\n",
      "Epoch 0 step 81: training accuarcy: 0.9351\n",
      "Epoch 0 step 81: training loss: 17485.37705481673\n",
      "Epoch 0 step 82: training accuarcy: 0.9401\n",
      "Epoch 0 step 82: training loss: 17368.558014366747\n",
      "Epoch 0 step 83: training accuarcy: 0.9436\n",
      "Epoch 0 step 83: training loss: 17305.680949592283\n",
      "Epoch 0 step 84: training accuarcy: 0.9396\n",
      "Epoch 0 step 84: training loss: 17522.18548765844\n",
      "Epoch 0 step 85: training accuarcy: 0.9384\n",
      "Epoch 0 step 85: training loss: 17190.615107079884\n",
      "Epoch 0 step 86: training accuarcy: 0.9427000000000001\n",
      "Epoch 0 step 86: training loss: 17452.658316554072\n",
      "Epoch 0 step 87: training accuarcy: 0.9365\n",
      "Epoch 0 step 87: training loss: 16615.43538274709\n",
      "Epoch 0 step 88: training accuarcy: 0.9475\n",
      "Epoch 0 step 88: training loss: 17267.05967346716\n",
      "Epoch 0 step 89: training accuarcy: 0.9379000000000001\n",
      "Epoch 0 step 89: training loss: 17218.19447258259\n",
      "Epoch 0 step 90: training accuarcy: 0.9376000000000001\n",
      "Epoch 0 step 90: training loss: 17251.018886259422\n",
      "Epoch 0 step 91: training accuarcy: 0.9377000000000001\n",
      "Epoch 0 step 91: training loss: 16880.034898250327\n",
      "Epoch 0 step 92: training accuarcy: 0.9408000000000001\n",
      "Epoch 0 step 92: training loss: 17046.815792650184\n",
      "Epoch 0 step 93: training accuarcy: 0.9420000000000001\n",
      "Epoch 0 step 93: training loss: 16981.366729458292\n",
      "Epoch 0 step 94: training accuarcy: 0.9410000000000001\n",
      "Epoch 0 step 94: training loss: 16582.176131456225\n",
      "Epoch 0 step 95: training accuarcy: 0.9408000000000001\n",
      "Epoch 0 step 95: training loss: 16524.09528657326\n",
      "Epoch 0 step 96: training accuarcy: 0.9478000000000001\n",
      "Epoch 0 step 96: training loss: 16392.848802019846\n",
      "Epoch 0 step 97: training accuarcy: 0.9455\n",
      "Epoch 0 step 97: training loss: 15938.563089904595\n",
      "Epoch 0 step 98: training accuarcy: 0.9471\n",
      "Epoch 0 step 98: training loss: 16142.910312748667\n",
      "Epoch 0 step 99: training accuarcy: 0.9462\n",
      "Epoch 0 step 99: training loss: 15464.642470222421\n",
      "Epoch 0 step 100: training accuarcy: 0.9518000000000001\n",
      "Epoch 0 step 100: training loss: 15721.953138874771\n",
      "Epoch 0 step 101: training accuarcy: 0.9537\n",
      "Epoch 0 step 101: training loss: 15250.317636316146\n",
      "Epoch 0 step 102: training accuarcy: 0.9604\n",
      "Epoch 0 step 102: training loss: 15546.220233197277\n",
      "Epoch 0 step 103: training accuarcy: 0.9526\n",
      "Epoch 0 step 103: training loss: 15057.185484768532\n",
      "Epoch 0 step 104: training accuarcy: 0.9601000000000001\n",
      "Epoch 0 step 104: training loss: 15200.358839278993\n",
      "Epoch 0 step 105: training accuarcy: 0.9554\n",
      "Epoch 0 step 105: training loss: 15433.620181630098\n",
      "Epoch 0 step 106: training accuarcy: 0.9503\n",
      "Epoch 0 step 106: training loss: 15788.511535800599\n",
      "Epoch 0 step 107: training accuarcy: 0.9467000000000001\n",
      "Epoch 0 step 107: training loss: 15105.331458631723\n",
      "Epoch 0 step 108: training accuarcy: 0.9563\n",
      "Epoch 0 step 108: training loss: 15421.752858195536\n",
      "Epoch 0 step 109: training accuarcy: 0.9479000000000001\n",
      "Epoch 0 step 109: training loss: 15026.99867882596\n",
      "Epoch 0 step 110: training accuarcy: 0.9565\n",
      "Epoch 0 step 110: training loss: 15712.21122219024\n",
      "Epoch 0 step 111: training accuarcy: 0.9456\n",
      "Epoch 0 step 111: training loss: 15294.682989028199\n",
      "Epoch 0 step 112: training accuarcy: 0.9476\n",
      "Epoch 0 step 112: training loss: 14891.876549584158\n",
      "Epoch 0 step 113: training accuarcy: 0.9552\n",
      "Epoch 0 step 113: training loss: 15719.811242344418\n",
      "Epoch 0 step 114: training accuarcy: 0.9438000000000001\n",
      "Epoch 0 step 114: training loss: 15071.44911293159\n",
      "Epoch 0 step 115: training accuarcy: 0.9514\n",
      "Epoch 0 step 115: training loss: 15042.630888428414\n",
      "Epoch 0 step 116: training accuarcy: 0.9532\n",
      "Epoch 0 step 116: training loss: 15188.284222677748\n",
      "Epoch 0 step 117: training accuarcy: 0.9491\n",
      "Epoch 0 step 117: training loss: 15180.24757263358\n",
      "Epoch 0 step 118: training accuarcy: 0.9485\n",
      "Epoch 0 step 118: training loss: 14923.722516492588\n",
      "Epoch 0 step 119: training accuarcy: 0.9528000000000001\n",
      "Epoch 0 step 119: training loss: 14722.222945991332\n",
      "Epoch 0 step 120: training accuarcy: 0.9555\n",
      "Epoch 0 step 120: training loss: 14603.571899187427\n",
      "Epoch 0 step 121: training accuarcy: 0.9522\n",
      "Epoch 0 step 121: training loss: 14645.952224493853\n",
      "Epoch 0 step 122: training accuarcy: 0.9552\n",
      "Epoch 0 step 122: training loss: 15137.655515681376\n",
      "Epoch 0 step 123: training accuarcy: 0.9490000000000001\n",
      "Epoch 0 step 123: training loss: 14161.958357887328\n",
      "Epoch 0 step 124: training accuarcy: 0.9603\n",
      "Epoch 0 step 124: training loss: 14009.2638075811\n",
      "Epoch 0 step 125: training accuarcy: 0.9597\n",
      "Epoch 0 step 125: training loss: 14522.44045160238\n",
      "Epoch 0 step 126: training accuarcy: 0.9550000000000001\n",
      "Epoch 0 step 126: training loss: 13956.606421151764\n",
      "Epoch 0 step 127: training accuarcy: 0.9611000000000001\n",
      "Epoch 0 step 127: training loss: 14093.17414597288\n",
      "Epoch 0 step 128: training accuarcy: 0.9612\n",
      "Epoch 0 step 128: training loss: 14459.995997556238\n",
      "Epoch 0 step 129: training accuarcy: 0.9556\n",
      "Epoch 0 step 129: training loss: 14143.107892010754\n",
      "Epoch 0 step 130: training accuarcy: 0.9584\n",
      "Epoch 0 step 130: training loss: 13671.56552596375\n",
      "Epoch 0 step 131: training accuarcy: 0.9626\n",
      "Epoch 0 step 131: training loss: 14453.469142539747\n",
      "Epoch 0 step 132: training accuarcy: 0.9524\n",
      "Epoch 0 step 132: training loss: 14165.397096180011\n",
      "Epoch 0 step 133: training accuarcy: 0.9574\n",
      "Epoch 0 step 133: training loss: 14192.278586091621\n",
      "Epoch 0 step 134: training accuarcy: 0.9567\n",
      "Epoch 0 step 134: training loss: 13915.658593014956\n",
      "Epoch 0 step 135: training accuarcy: 0.9600000000000001\n",
      "Epoch 0 step 135: training loss: 13349.218114891977\n",
      "Epoch 0 step 136: training accuarcy: 0.9676\n",
      "Epoch 0 step 136: training loss: 14225.006677352874\n",
      "Epoch 0 step 137: training accuarcy: 0.9557\n",
      "Epoch 0 step 137: training loss: 13990.560980495658\n",
      "Epoch 0 step 138: training accuarcy: 0.9577\n",
      "Epoch 0 step 138: training loss: 13638.016521656995\n",
      "Epoch 0 step 139: training accuarcy: 0.9616\n",
      "Epoch 0 step 139: training loss: 13536.911959251898\n",
      "Epoch 0 step 140: training accuarcy: 0.9658\n",
      "Epoch 0 step 140: training loss: 13903.48800547942\n",
      "Epoch 0 step 141: training accuarcy: 0.9569000000000001\n",
      "Epoch 0 step 141: training loss: 12972.741660654992\n",
      "Epoch 0 step 142: training accuarcy: 0.9695\n",
      "Epoch 0 step 142: training loss: 13223.448164209352\n",
      "Epoch 0 step 143: training accuarcy: 0.9628000000000001\n",
      "Epoch 0 step 143: training loss: 13551.705939639953\n",
      "Epoch 0 step 144: training accuarcy: 0.9634\n",
      "Epoch 0 step 144: training loss: 12835.0353162918\n",
      "Epoch 0 step 145: training accuarcy: 0.9668\n",
      "Epoch 0 step 145: training loss: 13143.301310964816\n",
      "Epoch 0 step 146: training accuarcy: 0.9649000000000001\n",
      "Epoch 0 step 146: training loss: 13661.443958433163\n",
      "Epoch 0 step 147: training accuarcy: 0.9620000000000001\n",
      "Epoch 0 step 147: training loss: 13109.87439975031\n",
      "Epoch 0 step 148: training accuarcy: 0.9656\n",
      "Epoch 0 step 148: training loss: 13265.77477425618\n",
      "Epoch 0 step 149: training accuarcy: 0.9637\n",
      "Epoch 0 step 149: training loss: 13666.992427817057\n",
      "Epoch 0 step 150: training accuarcy: 0.9546\n",
      "Epoch 0 step 150: training loss: 13664.489506068508\n",
      "Epoch 0 step 151: training accuarcy: 0.9576\n",
      "Epoch 0 step 151: training loss: 13181.564582939647\n",
      "Epoch 0 step 152: training accuarcy: 0.9586\n",
      "Epoch 0 step 152: training loss: 13493.625794894691\n",
      "Epoch 0 step 153: training accuarcy: 0.9598000000000001\n",
      "Epoch 0 step 153: training loss: 13241.214839970417\n",
      "Epoch 0 step 154: training accuarcy: 0.9604\n",
      "Epoch 0 step 154: training loss: 12612.68920207381\n",
      "Epoch 0 step 155: training accuarcy: 0.9710000000000001\n",
      "Epoch 0 step 155: training loss: 12495.329823242391\n",
      "Epoch 0 step 156: training accuarcy: 0.9700000000000001\n",
      "Epoch 0 step 156: training loss: 13115.609290902139\n",
      "Epoch 0 step 157: training accuarcy: 0.9592\n",
      "Epoch 0 step 157: training loss: 12729.939956163074\n",
      "Epoch 0 step 158: training accuarcy: 0.9657\n",
      "Epoch 0 step 158: training loss: 12791.807752566143\n",
      "Epoch 0 step 159: training accuarcy: 0.9656\n",
      "Epoch 0 step 159: training loss: 12706.58528443638\n",
      "Epoch 0 step 160: training accuarcy: 0.9681000000000001\n",
      "Epoch 0 step 160: training loss: 12981.718605545873\n",
      "Epoch 0 step 161: training accuarcy: 0.9651000000000001\n",
      "Epoch 0 step 161: training loss: 12518.647232851972\n",
      "Epoch 0 step 162: training accuarcy: 0.9665\n",
      "Epoch 0 step 162: training loss: 12884.350636051544\n",
      "Epoch 0 step 163: training accuarcy: 0.9632000000000001\n",
      "Epoch 0 step 163: training loss: 12605.36906451976\n",
      "Epoch 0 step 164: training accuarcy: 0.9649000000000001\n",
      "Epoch 0 step 164: training loss: 12195.54440208216\n",
      "Epoch 0 step 165: training accuarcy: 0.9711000000000001\n",
      "Epoch 0 step 165: training loss: 12576.814590101403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 166: training accuarcy: 0.9649000000000001\n",
      "Epoch 0 step 166: training loss: 12025.231994584245\n",
      "Epoch 0 step 167: training accuarcy: 0.9740000000000001\n",
      "Epoch 0 step 167: training loss: 12975.623940105927\n",
      "Epoch 0 step 168: training accuarcy: 0.9597\n",
      "Epoch 0 step 168: training loss: 12528.237092144336\n",
      "Epoch 0 step 169: training accuarcy: 0.9664\n",
      "Epoch 0 step 169: training loss: 12116.927013772427\n",
      "Epoch 0 step 170: training accuarcy: 0.9694\n",
      "Epoch 0 step 170: training loss: 12190.54159559381\n",
      "Epoch 0 step 171: training accuarcy: 0.9705\n",
      "Epoch 0 step 171: training loss: 11859.6976563459\n",
      "Epoch 0 step 172: training accuarcy: 0.9723\n",
      "Epoch 0 step 172: training loss: 12199.927979699658\n",
      "Epoch 0 step 173: training accuarcy: 0.9687\n",
      "Epoch 0 step 173: training loss: 12409.036370485526\n",
      "Epoch 0 step 174: training accuarcy: 0.9647\n",
      "Epoch 0 step 174: training loss: 11917.692596284536\n",
      "Epoch 0 step 175: training accuarcy: 0.9752000000000001\n",
      "Epoch 0 step 175: training loss: 12528.170015238262\n",
      "Epoch 0 step 176: training accuarcy: 0.9663\n",
      "Epoch 0 step 176: training loss: 12419.433897772411\n",
      "Epoch 0 step 177: training accuarcy: 0.9627\n",
      "Epoch 0 step 177: training loss: 12372.508188206912\n",
      "Epoch 0 step 178: training accuarcy: 0.9646\n",
      "Epoch 0 step 178: training loss: 12037.425587410624\n",
      "Epoch 0 step 179: training accuarcy: 0.9695\n",
      "Epoch 0 step 179: training loss: 12214.717546438593\n",
      "Epoch 0 step 180: training accuarcy: 0.9673\n",
      "Epoch 0 step 180: training loss: 12006.220994774696\n",
      "Epoch 0 step 181: training accuarcy: 0.9687\n",
      "Epoch 0 step 181: training loss: 11765.084526515055\n",
      "Epoch 0 step 182: training accuarcy: 0.9700000000000001\n",
      "Epoch 0 step 182: training loss: 11564.1940765397\n",
      "Epoch 0 step 183: training accuarcy: 0.9731000000000001\n",
      "Epoch 0 step 183: training loss: 12130.032757621046\n",
      "Epoch 0 step 184: training accuarcy: 0.9646\n",
      "Epoch 0 step 184: training loss: 11858.035322307744\n",
      "Epoch 0 step 185: training accuarcy: 0.9684\n",
      "Epoch 0 step 185: training loss: 11542.629519501435\n",
      "Epoch 0 step 186: training accuarcy: 0.9737\n",
      "Epoch 0 step 186: training loss: 11603.873981131283\n",
      "Epoch 0 step 187: training accuarcy: 0.9700000000000001\n",
      "Epoch 0 step 187: training loss: 11566.553030727833\n",
      "Epoch 0 step 188: training accuarcy: 0.9724\n",
      "Epoch 0 step 188: training loss: 11470.838870963382\n",
      "Epoch 0 step 189: training accuarcy: 0.9723\n",
      "Epoch 0 step 189: training loss: 12271.066339657078\n",
      "Epoch 0 step 190: training accuarcy: 0.9603\n",
      "Epoch 0 step 190: training loss: 11508.779428013382\n",
      "Epoch 0 step 191: training accuarcy: 0.9769000000000001\n",
      "Epoch 0 step 191: training loss: 11413.224444810152\n",
      "Epoch 0 step 192: training accuarcy: 0.9732000000000001\n",
      "Epoch 0 step 192: training loss: 11511.259473806453\n",
      "Epoch 0 step 193: training accuarcy: 0.9698\n",
      "Epoch 0 step 193: training loss: 11238.234921292575\n",
      "Epoch 0 step 194: training accuarcy: 0.9746\n",
      "Epoch 0 step 194: training loss: 11891.929029493533\n",
      "Epoch 0 step 195: training accuarcy: 0.9641000000000001\n",
      "Epoch 0 step 195: training loss: 11735.260678061117\n",
      "Epoch 0 step 196: training accuarcy: 0.9687\n",
      "Epoch 0 step 196: training loss: 11554.809953225105\n",
      "Epoch 0 step 197: training accuarcy: 0.9711000000000001\n",
      "Epoch 0 step 197: training loss: 11279.135287598985\n",
      "Epoch 0 step 198: training accuarcy: 0.9731000000000001\n",
      "Epoch 0 step 198: training loss: 11067.754269954658\n",
      "Epoch 0 step 199: training accuarcy: 0.9763000000000001\n",
      "Epoch 0 step 199: training loss: 11429.263487858741\n",
      "Epoch 0 step 200: training accuarcy: 0.9693\n",
      "Epoch 0 step 200: training loss: 11390.260001822182\n",
      "Epoch 0 step 201: training accuarcy: 0.9706\n",
      "Epoch 0 step 201: training loss: 11669.929721883165\n",
      "Epoch 0 step 202: training accuarcy: 0.9672000000000001\n",
      "Epoch 0 step 202: training loss: 11372.54907509447\n",
      "Epoch 0 step 203: training accuarcy: 0.9707\n",
      "Epoch 0 step 203: training loss: 11546.01089018879\n",
      "Epoch 0 step 204: training accuarcy: 0.9711000000000001\n",
      "Epoch 0 step 204: training loss: 11012.79408319137\n",
      "Epoch 0 step 205: training accuarcy: 0.9751000000000001\n",
      "Epoch 0 step 205: training loss: 11303.053260536684\n",
      "Epoch 0 step 206: training accuarcy: 0.9696\n",
      "Epoch 0 step 206: training loss: 10659.71616331544\n",
      "Epoch 0 step 207: training accuarcy: 0.9785\n",
      "Epoch 0 step 207: training loss: 10769.516128772602\n",
      "Epoch 0 step 208: training accuarcy: 0.9769000000000001\n",
      "Epoch 0 step 208: training loss: 11065.56634820042\n",
      "Epoch 0 step 209: training accuarcy: 0.9712000000000001\n",
      "Epoch 0 step 209: training loss: 11093.818176822124\n",
      "Epoch 0 step 210: training accuarcy: 0.9703\n",
      "Epoch 0 step 210: training loss: 11052.878490730827\n",
      "Epoch 0 step 211: training accuarcy: 0.9733\n",
      "Epoch 0 step 211: training loss: 10733.324894337024\n",
      "Epoch 0 step 212: training accuarcy: 0.9774\n",
      "Epoch 0 step 212: training loss: 10671.935809319406\n",
      "Epoch 0 step 213: training accuarcy: 0.9776\n",
      "Epoch 0 step 213: training loss: 11136.504808698895\n",
      "Epoch 0 step 214: training accuarcy: 0.9742000000000001\n",
      "Epoch 0 step 214: training loss: 10498.837992165045\n",
      "Epoch 0 step 215: training accuarcy: 0.9784\n",
      "Epoch 0 step 215: training loss: 10680.466843025632\n",
      "Epoch 0 step 216: training accuarcy: 0.9762000000000001\n",
      "Epoch 0 step 216: training loss: 10939.72038022563\n",
      "Epoch 0 step 217: training accuarcy: 0.9750000000000001\n",
      "Epoch 0 step 217: training loss: 11328.31088957916\n",
      "Epoch 0 step 218: training accuarcy: 0.9665\n",
      "Epoch 0 step 218: training loss: 10591.302725271662\n",
      "Epoch 0 step 219: training accuarcy: 0.9795\n",
      "Epoch 0 step 219: training loss: 11012.654055277975\n",
      "Epoch 0 step 220: training accuarcy: 0.9716\n",
      "Epoch 0 step 220: training loss: 10720.71278778989\n",
      "Epoch 0 step 221: training accuarcy: 0.9732000000000001\n",
      "Epoch 0 step 221: training loss: 10602.37240817212\n",
      "Epoch 0 step 222: training accuarcy: 0.9767\n",
      "Epoch 0 step 222: training loss: 10909.567224610862\n",
      "Epoch 0 step 223: training accuarcy: 0.9688\n",
      "Epoch 0 step 223: training loss: 10672.273837644836\n",
      "Epoch 0 step 224: training accuarcy: 0.9763000000000001\n",
      "Epoch 0 step 224: training loss: 10426.512844546935\n",
      "Epoch 0 step 225: training accuarcy: 0.9769000000000001\n",
      "Epoch 0 step 225: training loss: 10207.451875273588\n",
      "Epoch 0 step 226: training accuarcy: 0.9794\n",
      "Epoch 0 step 226: training loss: 10589.67952646756\n",
      "Epoch 0 step 227: training accuarcy: 0.9756\n",
      "Epoch 0 step 227: training loss: 10465.71676126993\n",
      "Epoch 0 step 228: training accuarcy: 0.9770000000000001\n",
      "Epoch 0 step 228: training loss: 10467.50865536282\n",
      "Epoch 0 step 229: training accuarcy: 0.9768\n",
      "Epoch 0 step 229: training loss: 10542.935256357523\n",
      "Epoch 0 step 230: training accuarcy: 0.9730000000000001\n",
      "Epoch 0 step 230: training loss: 10554.70579242522\n",
      "Epoch 0 step 231: training accuarcy: 0.9744\n",
      "Epoch 0 step 231: training loss: 10433.059968583104\n",
      "Epoch 0 step 232: training accuarcy: 0.9770000000000001\n",
      "Epoch 0 step 232: training loss: 10508.198805289281\n",
      "Epoch 0 step 233: training accuarcy: 0.9736\n",
      "Epoch 0 step 233: training loss: 10021.950350062953\n",
      "Epoch 0 step 234: training accuarcy: 0.9797\n",
      "Epoch 0 step 234: training loss: 10374.736424283268\n",
      "Epoch 0 step 235: training accuarcy: 0.9740000000000001\n",
      "Epoch 0 step 235: training loss: 10146.324856118088\n",
      "Epoch 0 step 236: training accuarcy: 0.9776\n",
      "Epoch 0 step 236: training loss: 10361.999331314177\n",
      "Epoch 0 step 237: training accuarcy: 0.9750000000000001\n",
      "Epoch 0 step 237: training loss: 10114.320540083108\n",
      "Epoch 0 step 238: training accuarcy: 0.9776\n",
      "Epoch 0 step 238: training loss: 10492.605423289802\n",
      "Epoch 0 step 239: training accuarcy: 0.9705\n",
      "Epoch 0 step 239: training loss: 10368.608010637743\n",
      "Epoch 0 step 240: training accuarcy: 0.9723\n",
      "Epoch 0 step 240: training loss: 9943.705751937892\n",
      "Epoch 0 step 241: training accuarcy: 0.9776\n",
      "Epoch 0 step 241: training loss: 10127.286638427426\n",
      "Epoch 0 step 242: training accuarcy: 0.9768\n",
      "Epoch 0 step 242: training loss: 9777.243165415666\n",
      "Epoch 0 step 243: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 243: training loss: 10276.156721707011\n",
      "Epoch 0 step 244: training accuarcy: 0.9773000000000001\n",
      "Epoch 0 step 244: training loss: 10034.01735636959\n",
      "Epoch 0 step 245: training accuarcy: 0.9753000000000001\n",
      "Epoch 0 step 245: training loss: 10085.943107265768\n",
      "Epoch 0 step 246: training accuarcy: 0.9780000000000001\n",
      "Epoch 0 step 246: training loss: 9771.859453765355\n",
      "Epoch 0 step 247: training accuarcy: 0.9782000000000001\n",
      "Epoch 0 step 247: training loss: 9721.948464146415\n",
      "Epoch 0 step 248: training accuarcy: 0.9806\n",
      "Epoch 0 step 248: training loss: 9823.732362416913\n",
      "Epoch 0 step 249: training accuarcy: 0.9808\n",
      "Epoch 0 step 249: training loss: 9744.033195292506\n",
      "Epoch 0 step 250: training accuarcy: 0.9796\n",
      "Epoch 0 step 250: training loss: 9772.963041826892\n",
      "Epoch 0 step 251: training accuarcy: 0.9773000000000001\n",
      "Epoch 0 step 251: training loss: 10060.710117865334\n",
      "Epoch 0 step 252: training accuarcy: 0.9766\n",
      "Epoch 0 step 252: training loss: 9693.226185739817\n",
      "Epoch 0 step 253: training accuarcy: 0.9778\n",
      "Epoch 0 step 253: training loss: 9905.918822931282\n",
      "Epoch 0 step 254: training accuarcy: 0.9758\n",
      "Epoch 0 step 254: training loss: 9470.095188920608\n",
      "Epoch 0 step 255: training accuarcy: 0.9821000000000001\n",
      "Epoch 0 step 255: training loss: 9617.223845037699\n",
      "Epoch 0 step 256: training accuarcy: 0.9786\n",
      "Epoch 0 step 256: training loss: 9699.16240197113\n",
      "Epoch 0 step 257: training accuarcy: 0.9782000000000001\n",
      "Epoch 0 step 257: training loss: 9336.635739774216\n",
      "Epoch 0 step 258: training accuarcy: 0.9818\n",
      "Epoch 0 step 258: training loss: 9404.36891035516\n",
      "Epoch 0 step 259: training accuarcy: 0.9843000000000001\n",
      "Epoch 0 step 259: training loss: 9799.416559065854\n",
      "Epoch 0 step 260: training accuarcy: 0.9776\n",
      "Epoch 0 step 260: training loss: 9458.733748646097\n",
      "Epoch 0 step 261: training accuarcy: 0.9801000000000001\n",
      "Epoch 0 step 261: training loss: 9256.330211831288\n",
      "Epoch 0 step 262: training accuarcy: 0.9843000000000001\n",
      "Epoch 0 step 262: training loss: 8292.971848743331\n",
      "Epoch 0 step 263: training accuarcy: 0.9794871794871794\n",
      "Epoch 0: train loss 18720.79175424917, train accuarcy 0.9062932133674622\n",
      "Epoch 0: valid loss 11510.802419596064, valid accuarcy 0.9607641696929932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|█████████████████████████████████████████████████████████▎                                                                                                                  | 1/3 [04:50<09:41, 290.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 8943.806104887635\n",
      "Epoch 1 step 264: training accuarcy: 0.9852000000000001\n",
      "Epoch 1 step 264: training loss: 8739.16893308108\n",
      "Epoch 1 step 265: training accuarcy: 0.9881000000000001\n",
      "Epoch 1 step 265: training loss: 8939.825616020917\n",
      "Epoch 1 step 266: training accuarcy: 0.9856\n",
      "Epoch 1 step 266: training loss: 8735.961095957951\n",
      "Epoch 1 step 267: training accuarcy: 0.9875\n",
      "Epoch 1 step 267: training loss: 8881.61574397262\n",
      "Epoch 1 step 268: training accuarcy: 0.9870000000000001\n",
      "Epoch 1 step 268: training loss: 8927.158958352806\n",
      "Epoch 1 step 269: training accuarcy: 0.9848\n",
      "Epoch 1 step 269: training loss: 8834.69927256897\n",
      "Epoch 1 step 270: training accuarcy: 0.9830000000000001\n",
      "Epoch 1 step 270: training loss: 8841.092411649668\n",
      "Epoch 1 step 271: training accuarcy: 0.9851000000000001\n",
      "Epoch 1 step 271: training loss: 8631.794868342997\n",
      "Epoch 1 step 272: training accuarcy: 0.9862000000000001\n",
      "Epoch 1 step 272: training loss: 8458.694663804956\n",
      "Epoch 1 step 273: training accuarcy: 0.9899\n",
      "Epoch 1 step 273: training loss: 8402.805924040107\n",
      "Epoch 1 step 274: training accuarcy: 0.9888\n",
      "Epoch 1 step 274: training loss: 8561.221411214618\n",
      "Epoch 1 step 275: training accuarcy: 0.9900000000000001\n",
      "Epoch 1 step 275: training loss: 8616.648463012181\n",
      "Epoch 1 step 276: training accuarcy: 0.9886\n",
      "Epoch 1 step 276: training loss: 8549.2860729817\n",
      "Epoch 1 step 277: training accuarcy: 0.9881000000000001\n",
      "Epoch 1 step 277: training loss: 8517.731821443747\n",
      "Epoch 1 step 278: training accuarcy: 0.9865\n",
      "Epoch 1 step 278: training loss: 8620.158624188134\n",
      "Epoch 1 step 279: training accuarcy: 0.9865\n",
      "Epoch 1 step 279: training loss: 8252.224077047504\n",
      "Epoch 1 step 280: training accuarcy: 0.9899\n",
      "Epoch 1 step 280: training loss: 8321.355522748572\n",
      "Epoch 1 step 281: training accuarcy: 0.9881000000000001\n",
      "Epoch 1 step 281: training loss: 8546.927612961941\n",
      "Epoch 1 step 282: training accuarcy: 0.9841000000000001\n",
      "Epoch 1 step 282: training loss: 8082.438600832669\n",
      "Epoch 1 step 283: training accuarcy: 0.9908\n",
      "Epoch 1 step 283: training loss: 8328.178234232171\n",
      "Epoch 1 step 284: training accuarcy: 0.9884000000000001\n",
      "Epoch 1 step 284: training loss: 8255.70390190783\n",
      "Epoch 1 step 285: training accuarcy: 0.9884000000000001\n",
      "Epoch 1 step 285: training loss: 8028.33617262362\n",
      "Epoch 1 step 286: training accuarcy: 0.9915\n",
      "Epoch 1 step 286: training loss: 8124.581471358369\n",
      "Epoch 1 step 287: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 287: training loss: 8266.034317614765\n",
      "Epoch 1 step 288: training accuarcy: 0.9854\n",
      "Epoch 1 step 288: training loss: 8208.738095550096\n",
      "Epoch 1 step 289: training accuarcy: 0.9876\n",
      "Epoch 1 step 289: training loss: 8530.001724007407\n",
      "Epoch 1 step 290: training accuarcy: 0.9819\n",
      "Epoch 1 step 290: training loss: 8551.172267036209\n",
      "Epoch 1 step 291: training accuarcy: 0.9822000000000001\n",
      "Epoch 1 step 291: training loss: 8322.754656292538\n",
      "Epoch 1 step 292: training accuarcy: 0.9865\n",
      "Epoch 1 step 292: training loss: 8114.007475052497\n",
      "Epoch 1 step 293: training accuarcy: 0.9864\n",
      "Epoch 1 step 293: training loss: 7986.890944535151\n",
      "Epoch 1 step 294: training accuarcy: 0.9907\n",
      "Epoch 1 step 294: training loss: 8341.10623887905\n",
      "Epoch 1 step 295: training accuarcy: 0.9868\n",
      "Epoch 1 step 295: training loss: 8551.565963001542\n",
      "Epoch 1 step 296: training accuarcy: 0.9824\n",
      "Epoch 1 step 296: training loss: 8027.46692676193\n",
      "Epoch 1 step 297: training accuarcy: 0.9898\n",
      "Epoch 1 step 297: training loss: 8371.83554750462\n",
      "Epoch 1 step 298: training accuarcy: 0.9847\n",
      "Epoch 1 step 298: training loss: 7986.819186736468\n",
      "Epoch 1 step 299: training accuarcy: 0.9884000000000001\n",
      "Epoch 1 step 299: training loss: 8365.537747217091\n",
      "Epoch 1 step 300: training accuarcy: 0.9850000000000001\n",
      "Epoch 1 step 300: training loss: 8057.062598130377\n",
      "Epoch 1 step 301: training accuarcy: 0.9872000000000001\n",
      "Epoch 1 step 301: training loss: 8185.100411781701\n",
      "Epoch 1 step 302: training accuarcy: 0.9879\n",
      "Epoch 1 step 302: training loss: 7894.208844132833\n",
      "Epoch 1 step 303: training accuarcy: 0.991\n",
      "Epoch 1 step 303: training loss: 8060.78326461716\n",
      "Epoch 1 step 304: training accuarcy: 0.9875\n",
      "Epoch 1 step 304: training loss: 7963.359951127811\n",
      "Epoch 1 step 305: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 305: training loss: 8165.718719998613\n",
      "Epoch 1 step 306: training accuarcy: 0.9871000000000001\n",
      "Epoch 1 step 306: training loss: 7658.488026731717\n",
      "Epoch 1 step 307: training accuarcy: 0.9913000000000001\n",
      "Epoch 1 step 307: training loss: 7871.680790631116\n",
      "Epoch 1 step 308: training accuarcy: 0.9899\n",
      "Epoch 1 step 308: training loss: 7863.706967783619\n",
      "Epoch 1 step 309: training accuarcy: 0.9882000000000001\n",
      "Epoch 1 step 309: training loss: 7908.104044718812\n",
      "Epoch 1 step 310: training accuarcy: 0.9873000000000001\n",
      "Epoch 1 step 310: training loss: 7878.129297672805\n",
      "Epoch 1 step 311: training accuarcy: 0.9854\n",
      "Epoch 1 step 311: training loss: 8001.051849734138\n",
      "Epoch 1 step 312: training accuarcy: 0.9855\n",
      "Epoch 1 step 312: training loss: 7868.503315830117\n",
      "Epoch 1 step 313: training accuarcy: 0.9878\n",
      "Epoch 1 step 313: training loss: 7786.796071054669\n",
      "Epoch 1 step 314: training accuarcy: 0.9896\n",
      "Epoch 1 step 314: training loss: 7643.549038541394\n",
      "Epoch 1 step 315: training accuarcy: 0.9902000000000001\n",
      "Epoch 1 step 315: training loss: 7632.387119456839\n",
      "Epoch 1 step 316: training accuarcy: 0.9909\n",
      "Epoch 1 step 316: training loss: 7757.823738250553\n",
      "Epoch 1 step 317: training accuarcy: 0.9874\n",
      "Epoch 1 step 317: training loss: 7530.324928563196\n",
      "Epoch 1 step 318: training accuarcy: 0.9905\n",
      "Epoch 1 step 318: training loss: 7833.050914958493\n",
      "Epoch 1 step 319: training accuarcy: 0.9861000000000001\n",
      "Epoch 1 step 319: training loss: 7967.89112088583\n",
      "Epoch 1 step 320: training accuarcy: 0.9830000000000001\n",
      "Epoch 1 step 320: training loss: 7440.191969947069\n",
      "Epoch 1 step 321: training accuarcy: 0.9927\n",
      "Epoch 1 step 321: training loss: 7590.587452404397\n",
      "Epoch 1 step 322: training accuarcy: 0.9892000000000001\n",
      "Epoch 1 step 322: training loss: 7880.152276257269\n",
      "Epoch 1 step 323: training accuarcy: 0.9855\n",
      "Epoch 1 step 323: training loss: 7478.8084202793825\n",
      "Epoch 1 step 324: training accuarcy: 0.9909\n",
      "Epoch 1 step 324: training loss: 7614.649082601036\n",
      "Epoch 1 step 325: training accuarcy: 0.9887\n",
      "Epoch 1 step 325: training loss: 7520.66241818933\n",
      "Epoch 1 step 326: training accuarcy: 0.9893000000000001\n",
      "Epoch 1 step 326: training loss: 7636.248693801268\n",
      "Epoch 1 step 327: training accuarcy: 0.9876\n",
      "Epoch 1 step 327: training loss: 7464.985464839806\n",
      "Epoch 1 step 328: training accuarcy: 0.9894000000000001\n",
      "Epoch 1 step 328: training loss: 7384.339921762844\n",
      "Epoch 1 step 329: training accuarcy: 0.9887\n",
      "Epoch 1 step 329: training loss: 7628.045786429377\n",
      "Epoch 1 step 330: training accuarcy: 0.9877\n",
      "Epoch 1 step 330: training loss: 7491.583125376788\n",
      "Epoch 1 step 331: training accuarcy: 0.9885\n",
      "Epoch 1 step 331: training loss: 7454.485612507786\n",
      "Epoch 1 step 332: training accuarcy: 0.9885\n",
      "Epoch 1 step 332: training loss: 7528.108617049505\n",
      "Epoch 1 step 333: training accuarcy: 0.9887\n",
      "Epoch 1 step 333: training loss: 7875.882799837177\n",
      "Epoch 1 step 334: training accuarcy: 0.9866\n",
      "Epoch 1 step 334: training loss: 7098.958339808454\n",
      "Epoch 1 step 335: training accuarcy: 0.9921000000000001\n",
      "Epoch 1 step 335: training loss: 7419.216220309522\n",
      "Epoch 1 step 336: training accuarcy: 0.9897\n",
      "Epoch 1 step 336: training loss: 7536.887717229649\n",
      "Epoch 1 step 337: training accuarcy: 0.9874\n",
      "Epoch 1 step 337: training loss: 7385.824560541427\n",
      "Epoch 1 step 338: training accuarcy: 0.9879\n",
      "Epoch 1 step 338: training loss: 7459.6131161836765\n",
      "Epoch 1 step 339: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 339: training loss: 7270.044771342775\n",
      "Epoch 1 step 340: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 340: training loss: 7394.158599178054\n",
      "Epoch 1 step 341: training accuarcy: 0.9862000000000001\n",
      "Epoch 1 step 341: training loss: 7289.869284869177\n",
      "Epoch 1 step 342: training accuarcy: 0.9898\n",
      "Epoch 1 step 342: training loss: 7400.381993459224\n",
      "Epoch 1 step 343: training accuarcy: 0.9878\n",
      "Epoch 1 step 343: training loss: 6968.9743304231215\n",
      "Epoch 1 step 344: training accuarcy: 0.9917\n",
      "Epoch 1 step 344: training loss: 7075.69334786834\n",
      "Epoch 1 step 345: training accuarcy: 0.9926\n",
      "Epoch 1 step 345: training loss: 7097.7550662350495\n",
      "Epoch 1 step 346: training accuarcy: 0.9922000000000001\n",
      "Epoch 1 step 346: training loss: 6987.1913263054275\n",
      "Epoch 1 step 347: training accuarcy: 0.9923000000000001\n",
      "Epoch 1 step 347: training loss: 7495.912534489609\n",
      "Epoch 1 step 348: training accuarcy: 0.9863000000000001\n",
      "Epoch 1 step 348: training loss: 6922.838613224958\n",
      "Epoch 1 step 349: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 349: training loss: 7211.283547732277\n",
      "Epoch 1 step 350: training accuarcy: 0.9882000000000001\n",
      "Epoch 1 step 350: training loss: 7211.562581088768\n",
      "Epoch 1 step 351: training accuarcy: 0.9895\n",
      "Epoch 1 step 351: training loss: 7260.450980638821\n",
      "Epoch 1 step 352: training accuarcy: 0.9869\n",
      "Epoch 1 step 352: training loss: 7056.225038647559\n",
      "Epoch 1 step 353: training accuarcy: 0.9892000000000001\n",
      "Epoch 1 step 353: training loss: 7011.345909196959\n",
      "Epoch 1 step 354: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 354: training loss: 7170.815583432084\n",
      "Epoch 1 step 355: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 355: training loss: 6887.737366423443\n",
      "Epoch 1 step 356: training accuarcy: 0.992\n",
      "Epoch 1 step 356: training loss: 7014.451934659821\n",
      "Epoch 1 step 357: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 357: training loss: 6852.215916342452\n",
      "Epoch 1 step 358: training accuarcy: 0.9919\n",
      "Epoch 1 step 358: training loss: 7060.7496278493145\n",
      "Epoch 1 step 359: training accuarcy: 0.9865\n",
      "Epoch 1 step 359: training loss: 7266.947190659256\n",
      "Epoch 1 step 360: training accuarcy: 0.9861000000000001\n",
      "Epoch 1 step 360: training loss: 7224.854970274842\n",
      "Epoch 1 step 361: training accuarcy: 0.9865\n",
      "Epoch 1 step 361: training loss: 6935.314243321259\n",
      "Epoch 1 step 362: training accuarcy: 0.9884000000000001\n",
      "Epoch 1 step 362: training loss: 6983.246257017316\n",
      "Epoch 1 step 363: training accuarcy: 0.9889\n",
      "Epoch 1 step 363: training loss: 7048.5098295541\n",
      "Epoch 1 step 364: training accuarcy: 0.9876\n",
      "Epoch 1 step 364: training loss: 7254.0574878783855\n",
      "Epoch 1 step 365: training accuarcy: 0.9861000000000001\n",
      "Epoch 1 step 365: training loss: 6633.353305420405\n",
      "Epoch 1 step 366: training accuarcy: 0.9926\n",
      "Epoch 1 step 366: training loss: 7121.776968674159\n",
      "Epoch 1 step 367: training accuarcy: 0.9851000000000001\n",
      "Epoch 1 step 367: training loss: 6823.482437240352\n",
      "Epoch 1 step 368: training accuarcy: 0.9887\n",
      "Epoch 1 step 368: training loss: 6843.778996361816\n",
      "Epoch 1 step 369: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 369: training loss: 6999.43969377324\n",
      "Epoch 1 step 370: training accuarcy: 0.9871000000000001\n",
      "Epoch 1 step 370: training loss: 6813.888043831478\n",
      "Epoch 1 step 371: training accuarcy: 0.9919\n",
      "Epoch 1 step 371: training loss: 6911.693708871885\n",
      "Epoch 1 step 372: training accuarcy: 0.9875\n",
      "Epoch 1 step 372: training loss: 6745.030027848807\n",
      "Epoch 1 step 373: training accuarcy: 0.991\n",
      "Epoch 1 step 373: training loss: 6811.1819439535475\n",
      "Epoch 1 step 374: training accuarcy: 0.9889\n",
      "Epoch 1 step 374: training loss: 6836.55276648917\n",
      "Epoch 1 step 375: training accuarcy: 0.9878\n",
      "Epoch 1 step 375: training loss: 6814.959647450711\n",
      "Epoch 1 step 376: training accuarcy: 0.9873000000000001\n",
      "Epoch 1 step 376: training loss: 7064.115027876311\n",
      "Epoch 1 step 377: training accuarcy: 0.9838\n",
      "Epoch 1 step 377: training loss: 6856.284307473134\n",
      "Epoch 1 step 378: training accuarcy: 0.9870000000000001\n",
      "Epoch 1 step 378: training loss: 6815.129527788584\n",
      "Epoch 1 step 379: training accuarcy: 0.9868\n",
      "Epoch 1 step 379: training loss: 6764.460147745344\n",
      "Epoch 1 step 380: training accuarcy: 0.9895\n",
      "Epoch 1 step 380: training loss: 6454.708665916857\n",
      "Epoch 1 step 381: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 381: training loss: 6442.8861151389965\n",
      "Epoch 1 step 382: training accuarcy: 0.9917\n",
      "Epoch 1 step 382: training loss: 6646.221136068353\n",
      "Epoch 1 step 383: training accuarcy: 0.9900000000000001\n",
      "Epoch 1 step 383: training loss: 6450.281183386941\n",
      "Epoch 1 step 384: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 384: training loss: 6578.4118428508955\n",
      "Epoch 1 step 385: training accuarcy: 0.9915\n",
      "Epoch 1 step 385: training loss: 6702.572354340387\n",
      "Epoch 1 step 386: training accuarcy: 0.9889\n",
      "Epoch 1 step 386: training loss: 6700.153514695225\n",
      "Epoch 1 step 387: training accuarcy: 0.9888\n",
      "Epoch 1 step 387: training loss: 6666.74789524879\n",
      "Epoch 1 step 388: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 388: training loss: 6776.2099465626525\n",
      "Epoch 1 step 389: training accuarcy: 0.9871000000000001\n",
      "Epoch 1 step 389: training loss: 6453.651853456294\n",
      "Epoch 1 step 390: training accuarcy: 0.9909\n",
      "Epoch 1 step 390: training loss: 6600.250126948434\n",
      "Epoch 1 step 391: training accuarcy: 0.9878\n",
      "Epoch 1 step 391: training loss: 6638.055043040886\n",
      "Epoch 1 step 392: training accuarcy: 0.9876\n",
      "Epoch 1 step 392: training loss: 6419.640900457287\n",
      "Epoch 1 step 393: training accuarcy: 0.9916\n",
      "Epoch 1 step 393: training loss: 6526.807323501766\n",
      "Epoch 1 step 394: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 394: training loss: 6562.388735992496\n",
      "Epoch 1 step 395: training accuarcy: 0.9874\n",
      "Epoch 1 step 395: training loss: 6268.091653686759\n",
      "Epoch 1 step 396: training accuarcy: 0.9937\n",
      "Epoch 1 step 396: training loss: 6323.369231700078\n",
      "Epoch 1 step 397: training accuarcy: 0.9906\n",
      "Epoch 1 step 397: training loss: 6260.028536117499\n",
      "Epoch 1 step 398: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 398: training loss: 6566.485962743731\n",
      "Epoch 1 step 399: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 399: training loss: 6197.05261278635\n",
      "Epoch 1 step 400: training accuarcy: 0.9915\n",
      "Epoch 1 step 400: training loss: 6385.975661587736\n",
      "Epoch 1 step 401: training accuarcy: 0.9890000000000001\n",
      "Epoch 1 step 401: training loss: 6418.361382527368\n",
      "Epoch 1 step 402: training accuarcy: 0.9888\n",
      "Epoch 1 step 402: training loss: 6291.7502856095825\n",
      "Epoch 1 step 403: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 403: training loss: 6384.55997246207\n",
      "Epoch 1 step 404: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 404: training loss: 6321.75280316118\n",
      "Epoch 1 step 405: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 405: training loss: 6308.718225447948\n",
      "Epoch 1 step 406: training accuarcy: 0.9888\n",
      "Epoch 1 step 406: training loss: 6209.758087908938\n",
      "Epoch 1 step 407: training accuarcy: 0.9897\n",
      "Epoch 1 step 407: training loss: 6421.9587483281975\n",
      "Epoch 1 step 408: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 408: training loss: 6190.770627794197\n",
      "Epoch 1 step 409: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 409: training loss: 6011.552830212262\n",
      "Epoch 1 step 410: training accuarcy: 0.9919\n",
      "Epoch 1 step 410: training loss: 6353.578292123686\n",
      "Epoch 1 step 411: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 411: training loss: 6068.41859954842\n",
      "Epoch 1 step 412: training accuarcy: 0.9919\n",
      "Epoch 1 step 412: training loss: 6362.193412165918\n",
      "Epoch 1 step 413: training accuarcy: 0.9894000000000001\n",
      "Epoch 1 step 413: training loss: 6335.22644147479\n",
      "Epoch 1 step 414: training accuarcy: 0.9854\n",
      "Epoch 1 step 414: training loss: 6204.615801384204\n",
      "Epoch 1 step 415: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 415: training loss: 6047.600539132025\n",
      "Epoch 1 step 416: training accuarcy: 0.992\n",
      "Epoch 1 step 416: training loss: 5956.504991906599\n",
      "Epoch 1 step 417: training accuarcy: 0.9929\n",
      "Epoch 1 step 417: training loss: 6092.912753834333\n",
      "Epoch 1 step 418: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 418: training loss: 5891.060833625375\n",
      "Epoch 1 step 419: training accuarcy: 0.9946\n",
      "Epoch 1 step 419: training loss: 6070.074859342089\n",
      "Epoch 1 step 420: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 420: training loss: 6036.406360582217\n",
      "Epoch 1 step 421: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 421: training loss: 5898.498952559178\n",
      "Epoch 1 step 422: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 422: training loss: 6126.560106065228\n",
      "Epoch 1 step 423: training accuarcy: 0.9893000000000001\n",
      "Epoch 1 step 423: training loss: 6231.997441955531\n",
      "Epoch 1 step 424: training accuarcy: 0.9861000000000001\n",
      "Epoch 1 step 424: training loss: 5937.703923899526\n",
      "Epoch 1 step 425: training accuarcy: 0.9914000000000001\n",
      "Epoch 1 step 425: training loss: 5960.532924343954\n",
      "Epoch 1 step 426: training accuarcy: 0.9896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 426: training loss: 6221.554330038238\n",
      "Epoch 1 step 427: training accuarcy: 0.9876\n",
      "Epoch 1 step 427: training loss: 5927.4489242996915\n",
      "Epoch 1 step 428: training accuarcy: 0.9934000000000001\n",
      "Epoch 1 step 428: training loss: 6041.6842210320665\n",
      "Epoch 1 step 429: training accuarcy: 0.9885\n",
      "Epoch 1 step 429: training loss: 5944.180503888112\n",
      "Epoch 1 step 430: training accuarcy: 0.9895\n",
      "Epoch 1 step 430: training loss: 5981.051606968576\n",
      "Epoch 1 step 431: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 431: training loss: 5886.426304914507\n",
      "Epoch 1 step 432: training accuarcy: 0.9914000000000001\n",
      "Epoch 1 step 432: training loss: 5959.966622372545\n",
      "Epoch 1 step 433: training accuarcy: 0.9907\n",
      "Epoch 1 step 433: training loss: 5846.8005121209135\n",
      "Epoch 1 step 434: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 434: training loss: 5879.018690676422\n",
      "Epoch 1 step 435: training accuarcy: 0.9896\n",
      "Epoch 1 step 435: training loss: 5900.992912393967\n",
      "Epoch 1 step 436: training accuarcy: 0.9900000000000001\n",
      "Epoch 1 step 436: training loss: 5711.353033907936\n",
      "Epoch 1 step 437: training accuarcy: 0.9921000000000001\n",
      "Epoch 1 step 437: training loss: 5756.4644723899255\n",
      "Epoch 1 step 438: training accuarcy: 0.9934000000000001\n",
      "Epoch 1 step 438: training loss: 6034.09066728977\n",
      "Epoch 1 step 439: training accuarcy: 0.9886\n",
      "Epoch 1 step 439: training loss: 6132.924847906665\n",
      "Epoch 1 step 440: training accuarcy: 0.9865\n",
      "Epoch 1 step 440: training loss: 5836.943321873535\n",
      "Epoch 1 step 441: training accuarcy: 0.9909\n",
      "Epoch 1 step 441: training loss: 5791.994040068501\n",
      "Epoch 1 step 442: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 442: training loss: 5698.032796405522\n",
      "Epoch 1 step 443: training accuarcy: 0.9906\n",
      "Epoch 1 step 443: training loss: 5738.904096738552\n",
      "Epoch 1 step 444: training accuarcy: 0.9887\n",
      "Epoch 1 step 444: training loss: 5808.117280423676\n",
      "Epoch 1 step 445: training accuarcy: 0.9877\n",
      "Epoch 1 step 445: training loss: 5720.944239178067\n",
      "Epoch 1 step 446: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 446: training loss: 5655.1905669443795\n",
      "Epoch 1 step 447: training accuarcy: 0.9926\n",
      "Epoch 1 step 447: training loss: 5858.105963965609\n",
      "Epoch 1 step 448: training accuarcy: 0.9886\n",
      "Epoch 1 step 448: training loss: 5662.32375448088\n",
      "Epoch 1 step 449: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 449: training loss: 5800.204731668591\n",
      "Epoch 1 step 450: training accuarcy: 0.9887\n",
      "Epoch 1 step 450: training loss: 5648.419396526001\n",
      "Epoch 1 step 451: training accuarcy: 0.9905\n",
      "Epoch 1 step 451: training loss: 5788.2222634778445\n",
      "Epoch 1 step 452: training accuarcy: 0.9899\n",
      "Epoch 1 step 452: training loss: 5685.70576620097\n",
      "Epoch 1 step 453: training accuarcy: 0.9893000000000001\n",
      "Epoch 1 step 453: training loss: 5588.854349743072\n",
      "Epoch 1 step 454: training accuarcy: 0.9907\n",
      "Epoch 1 step 454: training loss: 5447.183874701358\n",
      "Epoch 1 step 455: training accuarcy: 0.9925\n",
      "Epoch 1 step 455: training loss: 5683.269178725956\n",
      "Epoch 1 step 456: training accuarcy: 0.9902000000000001\n",
      "Epoch 1 step 456: training loss: 5477.978339449386\n",
      "Epoch 1 step 457: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 457: training loss: 5722.73578096058\n",
      "Epoch 1 step 458: training accuarcy: 0.9882000000000001\n",
      "Epoch 1 step 458: training loss: 5608.905541424537\n",
      "Epoch 1 step 459: training accuarcy: 0.9900000000000001\n",
      "Epoch 1 step 459: training loss: 5655.056793975122\n",
      "Epoch 1 step 460: training accuarcy: 0.9893000000000001\n",
      "Epoch 1 step 460: training loss: 5887.567994239271\n",
      "Epoch 1 step 461: training accuarcy: 0.9852000000000001\n",
      "Epoch 1 step 461: training loss: 5521.258913597797\n",
      "Epoch 1 step 462: training accuarcy: 0.9918\n",
      "Epoch 1 step 462: training loss: 5580.315748799157\n",
      "Epoch 1 step 463: training accuarcy: 0.9909\n",
      "Epoch 1 step 463: training loss: 5600.52323444183\n",
      "Epoch 1 step 464: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 464: training loss: 5663.864984108074\n",
      "Epoch 1 step 465: training accuarcy: 0.9894000000000001\n",
      "Epoch 1 step 465: training loss: 5723.606887683758\n",
      "Epoch 1 step 466: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 466: training loss: 5480.0751693848415\n",
      "Epoch 1 step 467: training accuarcy: 0.9928\n",
      "Epoch 1 step 467: training loss: 5810.051582856529\n",
      "Epoch 1 step 468: training accuarcy: 0.9877\n",
      "Epoch 1 step 468: training loss: 5487.972953431334\n",
      "Epoch 1 step 469: training accuarcy: 0.9895\n",
      "Epoch 1 step 469: training loss: 5656.834195844168\n",
      "Epoch 1 step 470: training accuarcy: 0.9875\n",
      "Epoch 1 step 470: training loss: 5629.311735662933\n",
      "Epoch 1 step 471: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 471: training loss: 5270.228391131032\n",
      "Epoch 1 step 472: training accuarcy: 0.9927\n",
      "Epoch 1 step 472: training loss: 5390.2585246205745\n",
      "Epoch 1 step 473: training accuarcy: 0.9915\n",
      "Epoch 1 step 473: training loss: 5519.719131952854\n",
      "Epoch 1 step 474: training accuarcy: 0.9885\n",
      "Epoch 1 step 474: training loss: 5484.736298527824\n",
      "Epoch 1 step 475: training accuarcy: 0.9895\n",
      "Epoch 1 step 475: training loss: 5510.416364660758\n",
      "Epoch 1 step 476: training accuarcy: 0.9889\n",
      "Epoch 1 step 476: training loss: 5391.405810846619\n",
      "Epoch 1 step 477: training accuarcy: 0.9913000000000001\n",
      "Epoch 1 step 477: training loss: 5637.897956212426\n",
      "Epoch 1 step 478: training accuarcy: 0.9867\n",
      "Epoch 1 step 478: training loss: 5325.631868816194\n",
      "Epoch 1 step 479: training accuarcy: 0.9938\n",
      "Epoch 1 step 479: training loss: 5226.129669859205\n",
      "Epoch 1 step 480: training accuarcy: 0.9928\n",
      "Epoch 1 step 480: training loss: 5306.417235241644\n",
      "Epoch 1 step 481: training accuarcy: 0.9914000000000001\n",
      "Epoch 1 step 481: training loss: 5362.74047373792\n",
      "Epoch 1 step 482: training accuarcy: 0.9899\n",
      "Epoch 1 step 482: training loss: 5331.445173597955\n",
      "Epoch 1 step 483: training accuarcy: 0.9918\n",
      "Epoch 1 step 483: training loss: 5178.624495409098\n",
      "Epoch 1 step 484: training accuarcy: 0.9909\n",
      "Epoch 1 step 484: training loss: 5486.197405881222\n",
      "Epoch 1 step 485: training accuarcy: 0.9897\n",
      "Epoch 1 step 485: training loss: 5437.9321169707655\n",
      "Epoch 1 step 486: training accuarcy: 0.9871000000000001\n",
      "Epoch 1 step 486: training loss: 5391.045121864234\n",
      "Epoch 1 step 487: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 487: training loss: 5165.059331979584\n",
      "Epoch 1 step 488: training accuarcy: 0.9918\n",
      "Epoch 1 step 488: training loss: 5109.925926399835\n",
      "Epoch 1 step 489: training accuarcy: 0.9935\n",
      "Epoch 1 step 489: training loss: 5287.315389598602\n",
      "Epoch 1 step 490: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 490: training loss: 5222.090289984806\n",
      "Epoch 1 step 491: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 491: training loss: 5149.230994236997\n",
      "Epoch 1 step 492: training accuarcy: 0.9904000000000001\n",
      "Epoch 1 step 492: training loss: 5210.075858357756\n",
      "Epoch 1 step 493: training accuarcy: 0.9895\n",
      "Epoch 1 step 493: training loss: 5083.095688674087\n",
      "Epoch 1 step 494: training accuarcy: 0.9923000000000001\n",
      "Epoch 1 step 494: training loss: 5301.7753774775265\n",
      "Epoch 1 step 495: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 495: training loss: 5053.201538496582\n",
      "Epoch 1 step 496: training accuarcy: 0.9936\n",
      "Epoch 1 step 496: training loss: 5298.378687050765\n",
      "Epoch 1 step 497: training accuarcy: 0.991\n",
      "Epoch 1 step 497: training loss: 5038.256271901444\n",
      "Epoch 1 step 498: training accuarcy: 0.9929\n",
      "Epoch 1 step 498: training loss: 5339.472228112268\n",
      "Epoch 1 step 499: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 499: training loss: 5502.234093548709\n",
      "Epoch 1 step 500: training accuarcy: 0.9888\n",
      "Epoch 1 step 500: training loss: 4852.196573524754\n",
      "Epoch 1 step 501: training accuarcy: 0.9968\n",
      "Epoch 1 step 501: training loss: 5010.28854159548\n",
      "Epoch 1 step 502: training accuarcy: 0.9936\n",
      "Epoch 1 step 502: training loss: 4953.03485751679\n",
      "Epoch 1 step 503: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 503: training loss: 4914.034184399153\n",
      "Epoch 1 step 504: training accuarcy: 0.9928\n",
      "Epoch 1 step 504: training loss: 4979.198309321041\n",
      "Epoch 1 step 505: training accuarcy: 0.9919\n",
      "Epoch 1 step 505: training loss: 5060.0250912299025\n",
      "Epoch 1 step 506: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 506: training loss: 5122.462022046057\n",
      "Epoch 1 step 507: training accuarcy: 0.9916\n",
      "Epoch 1 step 507: training loss: 5062.974255037603\n",
      "Epoch 1 step 508: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 508: training loss: 5268.513102068115\n",
      "Epoch 1 step 509: training accuarcy: 0.9874\n",
      "Epoch 1 step 509: training loss: 5116.304354988928\n",
      "Epoch 1 step 510: training accuarcy: 0.991\n",
      "Epoch 1 step 510: training loss: 4981.044080838401\n",
      "Epoch 1 step 511: training accuarcy: 0.9926\n",
      "Epoch 1 step 511: training loss: 4986.915260795151\n",
      "Epoch 1 step 512: training accuarcy: 0.9909\n",
      "Epoch 1 step 512: training loss: 5060.415814811108\n",
      "Epoch 1 step 513: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 513: training loss: 4990.116700843382\n",
      "Epoch 1 step 514: training accuarcy: 0.9916\n",
      "Epoch 1 step 514: training loss: 4760.047024217125\n",
      "Epoch 1 step 515: training accuarcy: 0.9938\n",
      "Epoch 1 step 515: training loss: 5149.57028470459\n",
      "Epoch 1 step 516: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 516: training loss: 5023.456092177116\n",
      "Epoch 1 step 517: training accuarcy: 0.9908\n",
      "Epoch 1 step 517: training loss: 5183.479453972701\n",
      "Epoch 1 step 518: training accuarcy: 0.9881000000000001\n",
      "Epoch 1 step 518: training loss: 5038.692499720786\n",
      "Epoch 1 step 519: training accuarcy: 0.9892000000000001\n",
      "Epoch 1 step 519: training loss: 5030.880027989722\n",
      "Epoch 1 step 520: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 520: training loss: 4986.236115476854\n",
      "Epoch 1 step 521: training accuarcy: 0.992\n",
      "Epoch 1 step 521: training loss: 4921.557711966993\n",
      "Epoch 1 step 522: training accuarcy: 0.993\n",
      "Epoch 1 step 522: training loss: 4829.256577622125\n",
      "Epoch 1 step 523: training accuarcy: 0.992\n",
      "Epoch 1 step 523: training loss: 5016.442766075729\n",
      "Epoch 1 step 524: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 524: training loss: 4978.944549427739\n",
      "Epoch 1 step 525: training accuarcy: 0.991\n",
      "Epoch 1 step 525: training loss: 4131.729062578906\n",
      "Epoch 1 step 526: training accuarcy: 0.9941025641025641\n",
      "Epoch 1: train loss 6595.833687502627, train accuarcy 0.9858560562133789\n",
      "Epoch 1: valid loss 6667.3031028699115, valid accuarcy 0.9780675172805786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 2/3 [09:35<04:49, 289.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 4345.961915111372\n",
      "Epoch 2 step 527: training accuarcy: 0.9978\n",
      "Epoch 2 step 527: training loss: 4260.850086770885\n",
      "Epoch 2 step 528: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 528: training loss: 4487.605909708647\n",
      "Epoch 2 step 529: training accuarcy: 0.9956\n",
      "Epoch 2 step 529: training loss: 4215.843818764394\n",
      "Epoch 2 step 530: training accuarcy: 0.9966\n",
      "Epoch 2 step 530: training loss: 4288.54462929906\n",
      "Epoch 2 step 531: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 531: training loss: 4238.23354934376\n",
      "Epoch 2 step 532: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 532: training loss: 4265.6264095508295\n",
      "Epoch 2 step 533: training accuarcy: 0.997\n",
      "Epoch 2 step 533: training loss: 4356.499136132832\n",
      "Epoch 2 step 534: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 534: training loss: 4433.148706856522\n",
      "Epoch 2 step 535: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 535: training loss: 4427.379311821591\n",
      "Epoch 2 step 536: training accuarcy: 0.9939\n",
      "Epoch 2 step 536: training loss: 4169.88670738351\n",
      "Epoch 2 step 537: training accuarcy: 0.997\n",
      "Epoch 2 step 537: training loss: 4305.927888942322\n",
      "Epoch 2 step 538: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 538: training loss: 4243.584449965448\n",
      "Epoch 2 step 539: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 539: training loss: 4157.81447246271\n",
      "Epoch 2 step 540: training accuarcy: 0.9979\n",
      "Epoch 2 step 540: training loss: 4240.708558308206\n",
      "Epoch 2 step 541: training accuarcy: 0.9965\n",
      "Epoch 2 step 541: training loss: 4248.280102968247\n",
      "Epoch 2 step 542: training accuarcy: 0.9949\n",
      "Epoch 2 step 542: training loss: 4201.3912768384225\n",
      "Epoch 2 step 543: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 543: training loss: 4223.545373308863\n",
      "Epoch 2 step 544: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 544: training loss: 4353.972185098846\n",
      "Epoch 2 step 545: training accuarcy: 0.9948\n",
      "Epoch 2 step 545: training loss: 4164.550310099451\n",
      "Epoch 2 step 546: training accuarcy: 0.9975\n",
      "Epoch 2 step 546: training loss: 4240.265889723425\n",
      "Epoch 2 step 547: training accuarcy: 0.995\n",
      "Epoch 2 step 547: training loss: 4316.3984927175825\n",
      "Epoch 2 step 548: training accuarcy: 0.9959\n",
      "Epoch 2 step 548: training loss: 4133.295903149377\n",
      "Epoch 2 step 549: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 549: training loss: 4032.0519258399268\n",
      "Epoch 2 step 550: training accuarcy: 0.9979\n",
      "Epoch 2 step 550: training loss: 4180.8870767930175\n",
      "Epoch 2 step 551: training accuarcy: 0.9956\n",
      "Epoch 2 step 551: training loss: 4102.518201880825\n",
      "Epoch 2 step 552: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 552: training loss: 4226.553528364369\n",
      "Epoch 2 step 553: training accuarcy: 0.9943000000000001\n",
      "Epoch 2 step 553: training loss: 4237.412583125778\n",
      "Epoch 2 step 554: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 554: training loss: 4074.3700405321297\n",
      "Epoch 2 step 555: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 555: training loss: 4139.496359936687\n",
      "Epoch 2 step 556: training accuarcy: 0.995\n",
      "Epoch 2 step 556: training loss: 4046.125957638458\n",
      "Epoch 2 step 557: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 557: training loss: 4114.500145402349\n",
      "Epoch 2 step 558: training accuarcy: 0.9967\n",
      "Epoch 2 step 558: training loss: 4033.0356502250415\n",
      "Epoch 2 step 559: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 559: training loss: 4118.117838887245\n",
      "Epoch 2 step 560: training accuarcy: 0.9956\n",
      "Epoch 2 step 560: training loss: 3932.852525492859\n",
      "Epoch 2 step 561: training accuarcy: 0.9968\n",
      "Epoch 2 step 561: training loss: 4109.313059112459\n",
      "Epoch 2 step 562: training accuarcy: 0.9935\n",
      "Epoch 2 step 562: training loss: 4022.0651228246634\n",
      "Epoch 2 step 563: training accuarcy: 0.9979\n",
      "Epoch 2 step 563: training loss: 4055.768335108992\n",
      "Epoch 2 step 564: training accuarcy: 0.9957\n",
      "Epoch 2 step 564: training loss: 4059.752241568733\n",
      "Epoch 2 step 565: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 565: training loss: 4131.947744123897\n",
      "Epoch 2 step 566: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 566: training loss: 3983.73439121904\n",
      "Epoch 2 step 567: training accuarcy: 0.9967\n",
      "Epoch 2 step 567: training loss: 4036.516729908665\n",
      "Epoch 2 step 568: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 568: training loss: 3989.980658840961\n",
      "Epoch 2 step 569: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 569: training loss: 3866.7778529416946\n",
      "Epoch 2 step 570: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 570: training loss: 3981.205173160471\n",
      "Epoch 2 step 571: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 571: training loss: 4023.1129320080427\n",
      "Epoch 2 step 572: training accuarcy: 0.9956\n",
      "Epoch 2 step 572: training loss: 3870.5668843986523\n",
      "Epoch 2 step 573: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 573: training loss: 3869.0171739113716\n",
      "Epoch 2 step 574: training accuarcy: 0.9978\n",
      "Epoch 2 step 574: training loss: 4028.7174755186875\n",
      "Epoch 2 step 575: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 575: training loss: 3875.5957005181926\n",
      "Epoch 2 step 576: training accuarcy: 0.9967\n",
      "Epoch 2 step 576: training loss: 3923.0124991388875\n",
      "Epoch 2 step 577: training accuarcy: 0.9956\n",
      "Epoch 2 step 577: training loss: 3971.4240621052295\n",
      "Epoch 2 step 578: training accuarcy: 0.9958\n",
      "Epoch 2 step 578: training loss: 3878.930307762113\n",
      "Epoch 2 step 579: training accuarcy: 0.9957\n",
      "Epoch 2 step 579: training loss: 3943.518950384016\n",
      "Epoch 2 step 580: training accuarcy: 0.9958\n",
      "Epoch 2 step 580: training loss: 3858.808031683253\n",
      "Epoch 2 step 581: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 581: training loss: 3881.0651118358346\n",
      "Epoch 2 step 582: training accuarcy: 0.9956\n",
      "Epoch 2 step 582: training loss: 3777.6766500359436\n",
      "Epoch 2 step 583: training accuarcy: 0.997\n",
      "Epoch 2 step 583: training loss: 3922.5318743971793\n",
      "Epoch 2 step 584: training accuarcy: 0.9978\n",
      "Epoch 2 step 584: training loss: 3831.6070174670413\n",
      "Epoch 2 step 585: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 585: training loss: 3902.2188885451897\n",
      "Epoch 2 step 586: training accuarcy: 0.996\n",
      "Epoch 2 step 586: training loss: 3830.2312544843853\n",
      "Epoch 2 step 587: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 587: training loss: 3958.6899656625496\n",
      "Epoch 2 step 588: training accuarcy: 0.995\n",
      "Epoch 2 step 588: training loss: 3945.0867238765063\n",
      "Epoch 2 step 589: training accuarcy: 0.9948\n",
      "Epoch 2 step 589: training loss: 3825.192352615844\n",
      "Epoch 2 step 590: training accuarcy: 0.9966\n",
      "Epoch 2 step 590: training loss: 3877.3101839249875\n",
      "Epoch 2 step 591: training accuarcy: 0.9967\n",
      "Epoch 2 step 591: training loss: 3794.1576236589945\n",
      "Epoch 2 step 592: training accuarcy: 0.9958\n",
      "Epoch 2 step 592: training loss: 3851.3969711649906\n",
      "Epoch 2 step 593: training accuarcy: 0.995\n",
      "Epoch 2 step 593: training loss: 4071.1782271968013\n",
      "Epoch 2 step 594: training accuarcy: 0.9927\n",
      "Epoch 2 step 594: training loss: 3907.049762606766\n",
      "Epoch 2 step 595: training accuarcy: 0.9949\n",
      "Epoch 2 step 595: training loss: 3779.6760663532546\n",
      "Epoch 2 step 596: training accuarcy: 0.9959\n",
      "Epoch 2 step 596: training loss: 3777.6950815941773\n",
      "Epoch 2 step 597: training accuarcy: 0.997\n",
      "Epoch 2 step 597: training loss: 3710.3198316333996\n",
      "Epoch 2 step 598: training accuarcy: 0.9985\n",
      "Epoch 2 step 598: training loss: 3609.4950684578016\n",
      "Epoch 2 step 599: training accuarcy: 0.998\n",
      "Epoch 2 step 599: training loss: 3944.3719237686632\n",
      "Epoch 2 step 600: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 600: training loss: 3721.5577199746645\n",
      "Epoch 2 step 601: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 601: training loss: 3776.8497570017807\n",
      "Epoch 2 step 602: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 602: training loss: 3720.627148811125\n",
      "Epoch 2 step 603: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 603: training loss: 3708.707838011487\n",
      "Epoch 2 step 604: training accuarcy: 0.9967\n",
      "Epoch 2 step 604: training loss: 3710.9908876132895\n",
      "Epoch 2 step 605: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 605: training loss: 3650.483637184607\n",
      "Epoch 2 step 606: training accuarcy: 0.9966\n",
      "Epoch 2 step 606: training loss: 3734.467857911567\n",
      "Epoch 2 step 607: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 607: training loss: 3852.975857816791\n",
      "Epoch 2 step 608: training accuarcy: 0.9965\n",
      "Epoch 2 step 608: training loss: 3714.3946581420864\n",
      "Epoch 2 step 609: training accuarcy: 0.9955\n",
      "Epoch 2 step 609: training loss: 3764.9458870785325\n",
      "Epoch 2 step 610: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 610: training loss: 3666.9206516563445\n",
      "Epoch 2 step 611: training accuarcy: 0.9966\n",
      "Epoch 2 step 611: training loss: 3649.9610777944317\n",
      "Epoch 2 step 612: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 612: training loss: 3654.3432675779686\n",
      "Epoch 2 step 613: training accuarcy: 0.9958\n",
      "Epoch 2 step 613: training loss: 3608.0745050062524\n",
      "Epoch 2 step 614: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 614: training loss: 3623.1259730719275\n",
      "Epoch 2 step 615: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 615: training loss: 3706.5483294964233\n",
      "Epoch 2 step 616: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 616: training loss: 3736.1308040295894\n",
      "Epoch 2 step 617: training accuarcy: 0.9948\n",
      "Epoch 2 step 617: training loss: 3551.5897887793517\n",
      "Epoch 2 step 618: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 618: training loss: 3660.6962631584443\n",
      "Epoch 2 step 619: training accuarcy: 0.9955\n",
      "Epoch 2 step 619: training loss: 3881.1845887031814\n",
      "Epoch 2 step 620: training accuarcy: 0.9921000000000001\n",
      "Epoch 2 step 620: training loss: 3747.924949606191\n",
      "Epoch 2 step 621: training accuarcy: 0.9948\n",
      "Epoch 2 step 621: training loss: 3560.2043207271904\n",
      "Epoch 2 step 622: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 622: training loss: 3703.6872166569697\n",
      "Epoch 2 step 623: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 623: training loss: 3722.4903845168124\n",
      "Epoch 2 step 624: training accuarcy: 0.9956\n",
      "Epoch 2 step 624: training loss: 3643.5276636101094\n",
      "Epoch 2 step 625: training accuarcy: 0.9956\n",
      "Epoch 2 step 625: training loss: 3510.0782723338725\n",
      "Epoch 2 step 626: training accuarcy: 0.9975\n",
      "Epoch 2 step 626: training loss: 3614.1337283198636\n",
      "Epoch 2 step 627: training accuarcy: 0.9966\n",
      "Epoch 2 step 627: training loss: 3482.2418181877983\n",
      "Epoch 2 step 628: training accuarcy: 0.9984000000000001\n",
      "Epoch 2 step 628: training loss: 3731.5328900969353\n",
      "Epoch 2 step 629: training accuarcy: 0.9958\n",
      "Epoch 2 step 629: training loss: 3460.9138447103182\n",
      "Epoch 2 step 630: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 630: training loss: 3700.806968871475\n",
      "Epoch 2 step 631: training accuarcy: 0.9935\n",
      "Epoch 2 step 631: training loss: 3617.792945048167\n",
      "Epoch 2 step 632: training accuarcy: 0.9947\n",
      "Epoch 2 step 632: training loss: 3371.7156067846213\n",
      "Epoch 2 step 633: training accuarcy: 0.9979\n",
      "Epoch 2 step 633: training loss: 3295.358433819355\n",
      "Epoch 2 step 634: training accuarcy: 0.9988\n",
      "Epoch 2 step 634: training loss: 3669.1243670458125\n",
      "Epoch 2 step 635: training accuarcy: 0.9943000000000001\n",
      "Epoch 2 step 635: training loss: 3512.8424528648848\n",
      "Epoch 2 step 636: training accuarcy: 0.9965\n",
      "Epoch 2 step 636: training loss: 3415.4135158155086\n",
      "Epoch 2 step 637: training accuarcy: 0.9968\n",
      "Epoch 2 step 637: training loss: 3484.4077262912588\n",
      "Epoch 2 step 638: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 638: training loss: 3873.8263282902412\n",
      "Epoch 2 step 639: training accuarcy: 0.9935\n",
      "Epoch 2 step 639: training loss: 3529.474362040521\n",
      "Epoch 2 step 640: training accuarcy: 0.9955\n",
      "Epoch 2 step 640: training loss: 3406.751883550567\n",
      "Epoch 2 step 641: training accuarcy: 0.996\n",
      "Epoch 2 step 641: training loss: 3516.2288304938847\n",
      "Epoch 2 step 642: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 642: training loss: 3445.1570302471146\n",
      "Epoch 2 step 643: training accuarcy: 0.9958\n",
      "Epoch 2 step 643: training loss: 3423.211883640824\n",
      "Epoch 2 step 644: training accuarcy: 0.9965\n",
      "Epoch 2 step 644: training loss: 3488.5223587784117\n",
      "Epoch 2 step 645: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 645: training loss: 3472.0917183869697\n",
      "Epoch 2 step 646: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 646: training loss: 3555.0830821640743\n",
      "Epoch 2 step 647: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 647: training loss: 3342.9680744358066\n",
      "Epoch 2 step 648: training accuarcy: 0.9975\n",
      "Epoch 2 step 648: training loss: 3522.515065834003\n",
      "Epoch 2 step 649: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 649: training loss: 3424.4938224011235\n",
      "Epoch 2 step 650: training accuarcy: 0.9967\n",
      "Epoch 2 step 650: training loss: 3519.645680722427\n",
      "Epoch 2 step 651: training accuarcy: 0.9945\n",
      "Epoch 2 step 651: training loss: 3499.201104581739\n",
      "Epoch 2 step 652: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 652: training loss: 3408.289518720034\n",
      "Epoch 2 step 653: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 653: training loss: 3579.072491323386\n",
      "Epoch 2 step 654: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 654: training loss: 3352.9786941563752\n",
      "Epoch 2 step 655: training accuarcy: 0.997\n",
      "Epoch 2 step 655: training loss: 3481.5606847119034\n",
      "Epoch 2 step 656: training accuarcy: 0.9958\n",
      "Epoch 2 step 656: training loss: 3380.5383284537056\n",
      "Epoch 2 step 657: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 657: training loss: 3403.579598612594\n",
      "Epoch 2 step 658: training accuarcy: 0.997\n",
      "Epoch 2 step 658: training loss: 3510.25587203297\n",
      "Epoch 2 step 659: training accuarcy: 0.9931000000000001\n",
      "Epoch 2 step 659: training loss: 3444.068833405552\n",
      "Epoch 2 step 660: training accuarcy: 0.9948\n",
      "Epoch 2 step 660: training loss: 3458.7467871288263\n",
      "Epoch 2 step 661: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 661: training loss: 3396.8472755312187\n",
      "Epoch 2 step 662: training accuarcy: 0.9957\n",
      "Epoch 2 step 662: training loss: 3503.2260850646044\n",
      "Epoch 2 step 663: training accuarcy: 0.9938\n",
      "Epoch 2 step 663: training loss: 3344.497704772469\n",
      "Epoch 2 step 664: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 664: training loss: 3340.7904072800175\n",
      "Epoch 2 step 665: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 665: training loss: 3404.231450756593\n",
      "Epoch 2 step 666: training accuarcy: 0.9957\n",
      "Epoch 2 step 666: training loss: 3504.112403652807\n",
      "Epoch 2 step 667: training accuarcy: 0.9932000000000001\n",
      "Epoch 2 step 667: training loss: 3375.0835479681627\n",
      "Epoch 2 step 668: training accuarcy: 0.9947\n",
      "Epoch 2 step 668: training loss: 3392.776511447315\n",
      "Epoch 2 step 669: training accuarcy: 0.9955\n",
      "Epoch 2 step 669: training loss: 3372.4222073203136\n",
      "Epoch 2 step 670: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 670: training loss: 3414.0028973082267\n",
      "Epoch 2 step 671: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 671: training loss: 3416.7312155073714\n",
      "Epoch 2 step 672: training accuarcy: 0.9967\n",
      "Epoch 2 step 672: training loss: 3231.7943513619025\n",
      "Epoch 2 step 673: training accuarcy: 0.9975\n",
      "Epoch 2 step 673: training loss: 3286.4109096689904\n",
      "Epoch 2 step 674: training accuarcy: 0.9967\n",
      "Epoch 2 step 674: training loss: 3244.477930530732\n",
      "Epoch 2 step 675: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 675: training loss: 3244.1651869085845\n",
      "Epoch 2 step 676: training accuarcy: 0.9969\n",
      "Epoch 2 step 676: training loss: 3324.831176426674\n",
      "Epoch 2 step 677: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 677: training loss: 3241.755697195716\n",
      "Epoch 2 step 678: training accuarcy: 0.9957\n",
      "Epoch 2 step 678: training loss: 3501.632548846551\n",
      "Epoch 2 step 679: training accuarcy: 0.9929\n",
      "Epoch 2 step 679: training loss: 3289.947324990967\n",
      "Epoch 2 step 680: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 680: training loss: 3306.833962760761\n",
      "Epoch 2 step 681: training accuarcy: 0.9976\n",
      "Epoch 2 step 681: training loss: 3309.754312875615\n",
      "Epoch 2 step 682: training accuarcy: 0.9949\n",
      "Epoch 2 step 682: training loss: 3282.992945026115\n",
      "Epoch 2 step 683: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 683: training loss: 3362.9064888764174\n",
      "Epoch 2 step 684: training accuarcy: 0.9948\n",
      "Epoch 2 step 684: training loss: 3228.989364680623\n",
      "Epoch 2 step 685: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 685: training loss: 3221.0310390393793\n",
      "Epoch 2 step 686: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 686: training loss: 3348.618608588694\n",
      "Epoch 2 step 687: training accuarcy: 0.9969\n",
      "Epoch 2 step 687: training loss: 3089.7805719110943\n",
      "Epoch 2 step 688: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 688: training loss: 3350.435075723738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 689: training accuarcy: 0.9943000000000001\n",
      "Epoch 2 step 689: training loss: 3416.9060598718124\n",
      "Epoch 2 step 690: training accuarcy: 0.9939\n",
      "Epoch 2 step 690: training loss: 3396.018059200225\n",
      "Epoch 2 step 691: training accuarcy: 0.994\n",
      "Epoch 2 step 691: training loss: 3282.170893567474\n",
      "Epoch 2 step 692: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 692: training loss: 3317.985513265985\n",
      "Epoch 2 step 693: training accuarcy: 0.9928\n",
      "Epoch 2 step 693: training loss: 3234.8365542682873\n",
      "Epoch 2 step 694: training accuarcy: 0.9958\n",
      "Epoch 2 step 694: training loss: 3220.3541611956457\n",
      "Epoch 2 step 695: training accuarcy: 0.9939\n",
      "Epoch 2 step 695: training loss: 3209.0935824468315\n",
      "Epoch 2 step 696: training accuarcy: 0.995\n",
      "Epoch 2 step 696: training loss: 3167.241672729309\n",
      "Epoch 2 step 697: training accuarcy: 0.997\n",
      "Epoch 2 step 697: training loss: 3224.179567806962\n",
      "Epoch 2 step 698: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 698: training loss: 3485.1692240131806\n",
      "Epoch 2 step 699: training accuarcy: 0.9928\n",
      "Epoch 2 step 699: training loss: 3149.800559159925\n",
      "Epoch 2 step 700: training accuarcy: 0.9958\n",
      "Epoch 2 step 700: training loss: 3143.2942593202324\n",
      "Epoch 2 step 701: training accuarcy: 0.997\n",
      "Epoch 2 step 701: training loss: 3208.0985726330355\n",
      "Epoch 2 step 702: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 702: training loss: 3286.2961750475074\n",
      "Epoch 2 step 703: training accuarcy: 0.9936\n",
      "Epoch 2 step 703: training loss: 2994.381216610832\n",
      "Epoch 2 step 704: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 704: training loss: 3236.486995839174\n",
      "Epoch 2 step 705: training accuarcy: 0.9947\n",
      "Epoch 2 step 705: training loss: 3284.2242745958984\n",
      "Epoch 2 step 706: training accuarcy: 0.9949\n",
      "Epoch 2 step 706: training loss: 3286.9843572882887\n",
      "Epoch 2 step 707: training accuarcy: 0.9933000000000001\n",
      "Epoch 2 step 707: training loss: 3130.92780938732\n",
      "Epoch 2 step 708: training accuarcy: 0.9959\n",
      "Epoch 2 step 708: training loss: 3021.3947492678203\n",
      "Epoch 2 step 709: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 709: training loss: 3096.6816916307057\n",
      "Epoch 2 step 710: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 710: training loss: 3098.8612251488485\n",
      "Epoch 2 step 711: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 711: training loss: 3267.8262553777968\n",
      "Epoch 2 step 712: training accuarcy: 0.9936\n",
      "Epoch 2 step 712: training loss: 3109.992747905695\n",
      "Epoch 2 step 713: training accuarcy: 0.9957\n",
      "Epoch 2 step 713: training loss: 3139.2470495461284\n",
      "Epoch 2 step 714: training accuarcy: 0.995\n",
      "Epoch 2 step 714: training loss: 3243.01896371966\n",
      "Epoch 2 step 715: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 715: training loss: 3396.7960417148324\n",
      "Epoch 2 step 716: training accuarcy: 0.9915\n",
      "Epoch 2 step 716: training loss: 3131.7242452746095\n",
      "Epoch 2 step 717: training accuarcy: 0.9966\n",
      "Epoch 2 step 717: training loss: 3076.732594363521\n",
      "Epoch 2 step 718: training accuarcy: 0.9957\n",
      "Epoch 2 step 718: training loss: 3322.2491576660104\n",
      "Epoch 2 step 719: training accuarcy: 0.9925\n",
      "Epoch 2 step 719: training loss: 3308.419466418408\n",
      "Epoch 2 step 720: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 720: training loss: 3072.5599835144067\n",
      "Epoch 2 step 721: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 721: training loss: 3082.14002523912\n",
      "Epoch 2 step 722: training accuarcy: 0.997\n",
      "Epoch 2 step 722: training loss: 3174.7889780111377\n",
      "Epoch 2 step 723: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 723: training loss: 2936.6709965948066\n",
      "Epoch 2 step 724: training accuarcy: 0.9966\n",
      "Epoch 2 step 724: training loss: 3089.5762986643026\n",
      "Epoch 2 step 725: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 725: training loss: 3103.2966236576235\n",
      "Epoch 2 step 726: training accuarcy: 0.9947\n",
      "Epoch 2 step 726: training loss: 3016.9078374420974\n",
      "Epoch 2 step 727: training accuarcy: 0.9965\n",
      "Epoch 2 step 727: training loss: 2959.9121376825697\n",
      "Epoch 2 step 728: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 728: training loss: 2873.6039966444127\n",
      "Epoch 2 step 729: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 729: training loss: 3300.7557898239284\n",
      "Epoch 2 step 730: training accuarcy: 0.9914000000000001\n",
      "Epoch 2 step 730: training loss: 3027.0748532367274\n",
      "Epoch 2 step 731: training accuarcy: 0.9956\n",
      "Epoch 2 step 731: training loss: 2991.3518057419965\n",
      "Epoch 2 step 732: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 732: training loss: 3028.635707455238\n",
      "Epoch 2 step 733: training accuarcy: 0.996\n",
      "Epoch 2 step 733: training loss: 2975.4510342990166\n",
      "Epoch 2 step 734: training accuarcy: 0.997\n",
      "Epoch 2 step 734: training loss: 2995.689429654468\n",
      "Epoch 2 step 735: training accuarcy: 0.9947\n",
      "Epoch 2 step 735: training loss: 3223.512886042687\n",
      "Epoch 2 step 736: training accuarcy: 0.9939\n",
      "Epoch 2 step 736: training loss: 3097.6381497977973\n",
      "Epoch 2 step 737: training accuarcy: 0.9945\n",
      "Epoch 2 step 737: training loss: 3055.4525002959226\n",
      "Epoch 2 step 738: training accuarcy: 0.9947\n",
      "Epoch 2 step 738: training loss: 3056.018853263244\n",
      "Epoch 2 step 739: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 739: training loss: 3063.2797278738713\n",
      "Epoch 2 step 740: training accuarcy: 0.9948\n",
      "Epoch 2 step 740: training loss: 2968.9021892808846\n",
      "Epoch 2 step 741: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 741: training loss: 3123.029885641755\n",
      "Epoch 2 step 742: training accuarcy: 0.9934000000000001\n",
      "Epoch 2 step 742: training loss: 3219.6261242838027\n",
      "Epoch 2 step 743: training accuarcy: 0.9931000000000001\n",
      "Epoch 2 step 743: training loss: 2961.4681745776843\n",
      "Epoch 2 step 744: training accuarcy: 0.9947\n",
      "Epoch 2 step 744: training loss: 3205.1517761558534\n",
      "Epoch 2 step 745: training accuarcy: 0.9928\n",
      "Epoch 2 step 745: training loss: 3224.17834648957\n",
      "Epoch 2 step 746: training accuarcy: 0.9939\n",
      "Epoch 2 step 746: training loss: 3258.139322190708\n",
      "Epoch 2 step 747: training accuarcy: 0.9909\n",
      "Epoch 2 step 747: training loss: 2877.0700505356676\n",
      "Epoch 2 step 748: training accuarcy: 0.9977\n",
      "Epoch 2 step 748: training loss: 3107.9320572671168\n",
      "Epoch 2 step 749: training accuarcy: 0.9938\n",
      "Epoch 2 step 749: training loss: 3085.005247547433\n",
      "Epoch 2 step 750: training accuarcy: 0.994\n",
      "Epoch 2 step 750: training loss: 2871.782426942408\n",
      "Epoch 2 step 751: training accuarcy: 0.9959\n",
      "Epoch 2 step 751: training loss: 3053.708780896621\n",
      "Epoch 2 step 752: training accuarcy: 0.9945\n",
      "Epoch 2 step 752: training loss: 2939.708846653879\n",
      "Epoch 2 step 753: training accuarcy: 0.9955\n",
      "Epoch 2 step 753: training loss: 3101.334915069524\n",
      "Epoch 2 step 754: training accuarcy: 0.9937\n",
      "Epoch 2 step 754: training loss: 2889.677767050205\n",
      "Epoch 2 step 755: training accuarcy: 0.9957\n",
      "Epoch 2 step 755: training loss: 3106.94055189997\n",
      "Epoch 2 step 756: training accuarcy: 0.9934000000000001\n",
      "Epoch 2 step 756: training loss: 2807.2557873074684\n",
      "Epoch 2 step 757: training accuarcy: 0.9977\n",
      "Epoch 2 step 757: training loss: 2922.9605749069397\n",
      "Epoch 2 step 758: training accuarcy: 0.9945\n",
      "Epoch 2 step 758: training loss: 3021.051618569402\n",
      "Epoch 2 step 759: training accuarcy: 0.9931000000000001\n",
      "Epoch 2 step 759: training loss: 2845.13526048805\n",
      "Epoch 2 step 760: training accuarcy: 0.9965\n",
      "Epoch 2 step 760: training loss: 2835.6248344463256\n",
      "Epoch 2 step 761: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 761: training loss: 2935.7164943341013\n",
      "Epoch 2 step 762: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 762: training loss: 2803.169595133986\n",
      "Epoch 2 step 763: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 763: training loss: 2984.060494066367\n",
      "Epoch 2 step 764: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 764: training loss: 2836.3816725333654\n",
      "Epoch 2 step 765: training accuarcy: 0.997\n",
      "Epoch 2 step 765: training loss: 2881.9041257716262\n",
      "Epoch 2 step 766: training accuarcy: 0.9959\n",
      "Epoch 2 step 766: training loss: 3122.216476310705\n",
      "Epoch 2 step 767: training accuarcy: 0.9916\n",
      "Epoch 2 step 767: training loss: 3115.7819896079022\n",
      "Epoch 2 step 768: training accuarcy: 0.9928\n",
      "Epoch 2 step 768: training loss: 2987.204187997322\n",
      "Epoch 2 step 769: training accuarcy: 0.9948\n",
      "Epoch 2 step 769: training loss: 2790.5135685728346\n",
      "Epoch 2 step 770: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 770: training loss: 2972.701783000728\n",
      "Epoch 2 step 771: training accuarcy: 0.9948\n",
      "Epoch 2 step 771: training loss: 2848.5194950118935\n",
      "Epoch 2 step 772: training accuarcy: 0.9959\n",
      "Epoch 2 step 772: training loss: 3065.9653765286075\n",
      "Epoch 2 step 773: training accuarcy: 0.9932000000000001\n",
      "Epoch 2 step 773: training loss: 2794.284189296995\n",
      "Epoch 2 step 774: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 774: training loss: 2850.787944047416\n",
      "Epoch 2 step 775: training accuarcy: 0.9957\n",
      "Epoch 2 step 775: training loss: 2878.8481362079683\n",
      "Epoch 2 step 776: training accuarcy: 0.9948\n",
      "Epoch 2 step 776: training loss: 2875.5821691032265\n",
      "Epoch 2 step 777: training accuarcy: 0.996\n",
      "Epoch 2 step 777: training loss: 2978.1554223886587\n",
      "Epoch 2 step 778: training accuarcy: 0.9922000000000001\n",
      "Epoch 2 step 778: training loss: 2909.6197048614304\n",
      "Epoch 2 step 779: training accuarcy: 0.9947\n",
      "Epoch 2 step 779: training loss: 2917.244620465904\n",
      "Epoch 2 step 780: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 780: training loss: 2909.17975647773\n",
      "Epoch 2 step 781: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 781: training loss: 2828.4751487965264\n",
      "Epoch 2 step 782: training accuarcy: 0.9949\n",
      "Epoch 2 step 782: training loss: 2916.3683263783887\n",
      "Epoch 2 step 783: training accuarcy: 0.9936\n",
      "Epoch 2 step 783: training loss: 2964.695481609655\n",
      "Epoch 2 step 784: training accuarcy: 0.994\n",
      "Epoch 2 step 784: training loss: 2830.4699081228987\n",
      "Epoch 2 step 785: training accuarcy: 0.9947\n",
      "Epoch 2 step 785: training loss: 2847.413845458139\n",
      "Epoch 2 step 786: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 786: training loss: 2887.9039055105495\n",
      "Epoch 2 step 787: training accuarcy: 0.9949\n",
      "Epoch 2 step 787: training loss: 2805.0505817426233\n",
      "Epoch 2 step 788: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 788: training loss: 2335.5849611159742\n",
      "Epoch 2 step 789: training accuarcy: 0.9953846153846154\n",
      "Epoch 2: train loss 3487.6549919167196, train accuarcy 0.9944347739219666\n",
      "Epoch 2: valid loss 4571.206134291568, valid accuarcy 0.9847785830497742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [14:23<00:00, 288.59s/it]"
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=3, \n",
    "               log_dir=get_log_dir(\"weight_topcoder\", \"fm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:29.079257Z",
     "start_time": "2019-10-09T11:41:29.070256Z"
    }
   },
   "outputs": [],
   "source": [
    "del fm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HRM FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:33:53.799943Z",
     "start_time": "2019-10-17T07:33:53.556954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchHrmFM()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_model = TorchHrmFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "hrm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:33:54.256966Z",
     "start_time": "2019-10-17T07:33:54.252941Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(hrm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:33:56.898945Z",
     "start_time": "2019-10-17T07:33:54.732943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x2516d99b848>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_learner = FMLearner(hrm_model, adam_opt, schedular, db)\n",
    "hrm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:33:58.592947Z",
     "start_time": "2019-10-17T07:33:58.588971Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='base',\n",
    "                   valid_col='base',\n",
    "                   test_col='base',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:58:47.213679Z",
     "start_time": "2019-10-09T11:58:47.209675Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='seq',\n",
    "                    valid_col='seq',\n",
    "                    test_col='seq',\n",
    "                    loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:24:23.644511Z",
     "start_time": "2019-10-09T12:24:23.641537Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='seq',\n",
    "                    valid_col='seq',\n",
    "                    test_col='seq',\n",
    "                    loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:41:02.489942Z",
     "start_time": "2019-10-17T07:34:03.900945Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 42094.005158331034\n",
      "Epoch 0 step 1: training accuarcy: 0.532\n",
      "Epoch 0 step 1: training loss: 40873.383274417065\n",
      "Epoch 0 step 2: training accuarcy: 0.5275\n",
      "Epoch 0 step 2: training loss: 40043.158675224884\n",
      "Epoch 0 step 3: training accuarcy: 0.5115000000000001\n",
      "Epoch 0 step 3: training loss: 38481.62659565021\n",
      "Epoch 0 step 4: training accuarcy: 0.531\n",
      "Epoch 0 step 4: training loss: 37545.26783184933\n",
      "Epoch 0 step 5: training accuarcy: 0.5425\n",
      "Epoch 0 step 5: training loss: 36458.21108693169\n",
      "Epoch 0 step 6: training accuarcy: 0.5265\n",
      "Epoch 0 step 6: training loss: 35192.893540318415\n",
      "Epoch 0 step 7: training accuarcy: 0.539\n",
      "Epoch 0 step 7: training loss: 34100.40943591181\n",
      "Epoch 0 step 8: training accuarcy: 0.5335\n",
      "Epoch 0 step 8: training loss: 33409.8551280519\n",
      "Epoch 0 step 9: training accuarcy: 0.522\n",
      "Epoch 0 step 9: training loss: 32133.455023909646\n",
      "Epoch 0 step 10: training accuarcy: 0.5275\n",
      "Epoch 0 step 10: training loss: 31459.402333177084\n",
      "Epoch 0 step 11: training accuarcy: 0.522\n",
      "Epoch 0 step 11: training loss: 30428.28045633508\n",
      "Epoch 0 step 12: training accuarcy: 0.5245\n",
      "Epoch 0 step 12: training loss: 29450.228390618624\n",
      "Epoch 0 step 13: training accuarcy: 0.5325\n",
      "Epoch 0 step 13: training loss: 28792.607446704063\n",
      "Epoch 0 step 14: training accuarcy: 0.5305\n",
      "Epoch 0 step 14: training loss: 27866.917593654827\n",
      "Epoch 0 step 15: training accuarcy: 0.528\n",
      "Epoch 0 step 15: training loss: 27048.121374060815\n",
      "Epoch 0 step 16: training accuarcy: 0.5215\n",
      "Epoch 0 step 16: training loss: 26287.0509392105\n",
      "Epoch 0 step 17: training accuarcy: 0.523\n",
      "Epoch 0 step 17: training loss: 25400.993588567217\n",
      "Epoch 0 step 18: training accuarcy: 0.5285\n",
      "Epoch 0 step 18: training loss: 24560.161246208427\n",
      "Epoch 0 step 19: training accuarcy: 0.5395\n",
      "Epoch 0 step 19: training loss: 23584.432182777575\n",
      "Epoch 0 step 20: training accuarcy: 0.5495\n",
      "Epoch 0 step 20: training loss: 23016.321132468594\n",
      "Epoch 0 step 21: training accuarcy: 0.5355\n",
      "Epoch 0 step 21: training loss: 22202.002758536324\n",
      "Epoch 0 step 22: training accuarcy: 0.5455\n",
      "Epoch 0 step 22: training loss: 21418.30588132302\n",
      "Epoch 0 step 23: training accuarcy: 0.5465\n",
      "Epoch 0 step 23: training loss: 20893.229626540433\n",
      "Epoch 0 step 24: training accuarcy: 0.539\n",
      "Epoch 0 step 24: training loss: 20193.956694936835\n",
      "Epoch 0 step 25: training accuarcy: 0.5285\n",
      "Epoch 0 step 25: training loss: 19750.00883335407\n",
      "Epoch 0 step 26: training accuarcy: 0.534\n",
      "Epoch 0 step 26: training loss: 18995.67574886468\n",
      "Epoch 0 step 27: training accuarcy: 0.5385\n",
      "Epoch 0 step 27: training loss: 18590.24004446042\n",
      "Epoch 0 step 28: training accuarcy: 0.5285\n",
      "Epoch 0 step 28: training loss: 17789.530319611127\n",
      "Epoch 0 step 29: training accuarcy: 0.5465\n",
      "Epoch 0 step 29: training loss: 17245.850843854827\n",
      "Epoch 0 step 30: training accuarcy: 0.5365\n",
      "Epoch 0 step 30: training loss: 16690.733232999053\n",
      "Epoch 0 step 31: training accuarcy: 0.545\n",
      "Epoch 0 step 31: training loss: 16106.580369534164\n",
      "Epoch 0 step 32: training accuarcy: 0.5630000000000001\n",
      "Epoch 0 step 32: training loss: 15718.978577302714\n",
      "Epoch 0 step 33: training accuarcy: 0.5275\n",
      "Epoch 0 step 33: training loss: 15219.708278834296\n",
      "Epoch 0 step 34: training accuarcy: 0.5255\n",
      "Epoch 0 step 34: training loss: 14791.23182565843\n",
      "Epoch 0 step 35: training accuarcy: 0.538\n",
      "Epoch 0 step 35: training loss: 14285.708247888013\n",
      "Epoch 0 step 36: training accuarcy: 0.5405\n",
      "Epoch 0 step 36: training loss: 13905.691495445157\n",
      "Epoch 0 step 37: training accuarcy: 0.5315\n",
      "Epoch 0 step 37: training loss: 13405.427825508641\n",
      "Epoch 0 step 38: training accuarcy: 0.5255\n",
      "Epoch 0 step 38: training loss: 12892.35848281562\n",
      "Epoch 0 step 39: training accuarcy: 0.525\n",
      "Epoch 0 step 39: training loss: 12473.416260806254\n",
      "Epoch 0 step 40: training accuarcy: 0.538\n",
      "Epoch 0 step 40: training loss: 12024.045447708762\n",
      "Epoch 0 step 41: training accuarcy: 0.5505\n",
      "Epoch 0 step 41: training loss: 11681.378879126694\n",
      "Epoch 0 step 42: training accuarcy: 0.5455\n",
      "Epoch 0 step 42: training loss: 11261.74573408735\n",
      "Epoch 0 step 43: training accuarcy: 0.5485\n",
      "Epoch 0 step 43: training loss: 10854.702341094155\n",
      "Epoch 0 step 44: training accuarcy: 0.543\n",
      "Epoch 0 step 44: training loss: 10576.686970180846\n",
      "Epoch 0 step 45: training accuarcy: 0.529\n",
      "Epoch 0 step 45: training loss: 10225.509766310613\n",
      "Epoch 0 step 46: training accuarcy: 0.56\n",
      "Epoch 0 step 46: training loss: 9969.832442224288\n",
      "Epoch 0 step 47: training accuarcy: 0.5465\n",
      "Epoch 0 step 47: training loss: 9561.846723710145\n",
      "Epoch 0 step 48: training accuarcy: 0.5495\n",
      "Epoch 0 step 48: training loss: 9289.859441470946\n",
      "Epoch 0 step 49: training accuarcy: 0.544\n",
      "Epoch 0 step 49: training loss: 8977.738521254838\n",
      "Epoch 0 step 50: training accuarcy: 0.5555\n",
      "Epoch 0 step 50: training loss: 8863.48077485432\n",
      "Epoch 0 step 51: training accuarcy: 0.5335\n",
      "Epoch 0 step 51: training loss: 8459.279620088646\n",
      "Epoch 0 step 52: training accuarcy: 0.5485\n",
      "Epoch 0 step 52: training loss: 8162.9181039732175\n",
      "Epoch 0 step 53: training accuarcy: 0.558\n",
      "Epoch 0 step 53: training loss: 8004.362422975056\n",
      "Epoch 0 step 54: training accuarcy: 0.5285\n",
      "Epoch 0 step 54: training loss: 7632.913611496245\n",
      "Epoch 0 step 55: training accuarcy: 0.5425\n",
      "Epoch 0 step 55: training loss: 7525.0954477717305\n",
      "Epoch 0 step 56: training accuarcy: 0.5405\n",
      "Epoch 0 step 56: training loss: 7146.131517079362\n",
      "Epoch 0 step 57: training accuarcy: 0.551\n",
      "Epoch 0 step 57: training loss: 6938.249473208928\n",
      "Epoch 0 step 58: training accuarcy: 0.5545\n",
      "Epoch 0 step 58: training loss: 6772.756112262322\n",
      "Epoch 0 step 59: training accuarcy: 0.551\n",
      "Epoch 0 step 59: training loss: 6560.176280162632\n",
      "Epoch 0 step 60: training accuarcy: 0.541\n",
      "Epoch 0 step 60: training loss: 6406.241531222887\n",
      "Epoch 0 step 61: training accuarcy: 0.5335\n",
      "Epoch 0 step 61: training loss: 6196.814244116553\n",
      "Epoch 0 step 62: training accuarcy: 0.5535\n",
      "Epoch 0 step 62: training loss: 5965.732381336799\n",
      "Epoch 0 step 63: training accuarcy: 0.557\n",
      "Epoch 0 step 63: training loss: 5900.565579388776\n",
      "Epoch 0 step 64: training accuarcy: 0.5505\n",
      "Epoch 0 step 64: training loss: 5613.585270954452\n",
      "Epoch 0 step 65: training accuarcy: 0.5565\n",
      "Epoch 0 step 65: training loss: 5567.436876694651\n",
      "Epoch 0 step 66: training accuarcy: 0.5415\n",
      "Epoch 0 step 66: training loss: 5288.795760847905\n",
      "Epoch 0 step 67: training accuarcy: 0.5685\n",
      "Epoch 0 step 67: training loss: 5137.63642767439\n",
      "Epoch 0 step 68: training accuarcy: 0.5670000000000001\n",
      "Epoch 0 step 68: training loss: 5002.015945715915\n",
      "Epoch 0 step 69: training accuarcy: 0.5715\n",
      "Epoch 0 step 69: training loss: 4916.5164285596165\n",
      "Epoch 0 step 70: training accuarcy: 0.5575\n",
      "Epoch 0 step 70: training loss: 4772.713837045026\n",
      "Epoch 0 step 71: training accuarcy: 0.5650000000000001\n",
      "Epoch 0 step 71: training loss: 4628.224114140532\n",
      "Epoch 0 step 72: training accuarcy: 0.5455\n",
      "Epoch 0 step 72: training loss: 4502.989114924887\n",
      "Epoch 0 step 73: training accuarcy: 0.5660000000000001\n",
      "Epoch 0 step 73: training loss: 4363.6993565537105\n",
      "Epoch 0 step 74: training accuarcy: 0.555\n",
      "Epoch 0 step 74: training loss: 4256.2756785803085\n",
      "Epoch 0 step 75: training accuarcy: 0.5680000000000001\n",
      "Epoch 0 step 75: training loss: 4122.2017747772625\n",
      "Epoch 0 step 76: training accuarcy: 0.5720000000000001\n",
      "Epoch 0 step 76: training loss: 4033.1345197104615\n",
      "Epoch 0 step 77: training accuarcy: 0.5855\n",
      "Epoch 0 step 77: training loss: 3894.156215847877\n",
      "Epoch 0 step 78: training accuarcy: 0.591\n",
      "Epoch 0 step 78: training loss: 3831.8867101447904\n",
      "Epoch 0 step 79: training accuarcy: 0.5795\n",
      "Epoch 0 step 79: training loss: 3772.734408253023\n",
      "Epoch 0 step 80: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 80: training loss: 3634.8901845497103\n",
      "Epoch 0 step 81: training accuarcy: 0.58\n",
      "Epoch 0 step 81: training loss: 3560.754553419695\n",
      "Epoch 0 step 82: training accuarcy: 0.5585\n",
      "Epoch 0 step 82: training loss: 3456.361542646664\n",
      "Epoch 0 step 83: training accuarcy: 0.5855\n",
      "Epoch 0 step 83: training loss: 3377.089656884139\n",
      "Epoch 0 step 84: training accuarcy: 0.5795\n",
      "Epoch 0 step 84: training loss: 3289.6144051184947\n",
      "Epoch 0 step 85: training accuarcy: 0.594\n",
      "Epoch 0 step 85: training loss: 3201.282297523834\n",
      "Epoch 0 step 86: training accuarcy: 0.582\n",
      "Epoch 0 step 86: training loss: 3163.552585634167\n",
      "Epoch 0 step 87: training accuarcy: 0.5985\n",
      "Epoch 0 step 87: training loss: 3072.4604964998043\n",
      "Epoch 0 step 88: training accuarcy: 0.581\n",
      "Epoch 0 step 88: training loss: 2989.4740981003306\n",
      "Epoch 0 step 89: training accuarcy: 0.6155\n",
      "Epoch 0 step 89: training loss: 2933.367148501142\n",
      "Epoch 0 step 90: training accuarcy: 0.5915\n",
      "Epoch 0 step 90: training loss: 2874.1293374088277\n",
      "Epoch 0 step 91: training accuarcy: 0.6055\n",
      "Epoch 0 step 91: training loss: 2812.540848027739\n",
      "Epoch 0 step 92: training accuarcy: 0.592\n",
      "Epoch 0 step 92: training loss: 2751.150148255042\n",
      "Epoch 0 step 93: training accuarcy: 0.6055\n",
      "Epoch 0 step 93: training loss: 2695.644881397222\n",
      "Epoch 0 step 94: training accuarcy: 0.593\n",
      "Epoch 0 step 94: training loss: 2650.328720972755\n",
      "Epoch 0 step 95: training accuarcy: 0.612\n",
      "Epoch 0 step 95: training loss: 2572.746281498863\n",
      "Epoch 0 step 96: training accuarcy: 0.61\n",
      "Epoch 0 step 96: training loss: 2525.312479115464\n",
      "Epoch 0 step 97: training accuarcy: 0.626\n",
      "Epoch 0 step 97: training loss: 2489.7176667469676\n",
      "Epoch 0 step 98: training accuarcy: 0.615\n",
      "Epoch 0 step 98: training loss: 2415.6328074409244\n",
      "Epoch 0 step 99: training accuarcy: 0.6085\n",
      "Epoch 0 step 99: training loss: 2366.6036166420167\n",
      "Epoch 0 step 100: training accuarcy: 0.625\n",
      "Epoch 0 step 100: training loss: 2355.680762212029\n",
      "Epoch 0 step 101: training accuarcy: 0.619\n",
      "Epoch 0 step 101: training loss: 2281.9288892499126\n",
      "Epoch 0 step 102: training accuarcy: 0.6215\n",
      "Epoch 0 step 102: training loss: 2262.5164714711896\n",
      "Epoch 0 step 103: training accuarcy: 0.624\n",
      "Epoch 0 step 103: training loss: 2241.8933545137334\n",
      "Epoch 0 step 104: training accuarcy: 0.6315000000000001\n",
      "Epoch 0 step 104: training loss: 2169.5231640790225\n",
      "Epoch 0 step 105: training accuarcy: 0.6315000000000001\n",
      "Epoch 0 step 105: training loss: 2153.777612839558\n",
      "Epoch 0 step 106: training accuarcy: 0.6355000000000001\n",
      "Epoch 0 step 106: training loss: 2136.641190619148\n",
      "Epoch 0 step 107: training accuarcy: 0.609\n",
      "Epoch 0 step 107: training loss: 2085.3372293676957\n",
      "Epoch 0 step 108: training accuarcy: 0.6295000000000001\n",
      "Epoch 0 step 108: training loss: 2033.2300181385147\n",
      "Epoch 0 step 109: training accuarcy: 0.614\n",
      "Epoch 0 step 109: training loss: 2038.1194132291844\n",
      "Epoch 0 step 110: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 110: training loss: 1991.1561521425197\n",
      "Epoch 0 step 111: training accuarcy: 0.634\n",
      "Epoch 0 step 111: training loss: 1926.456074161589\n",
      "Epoch 0 step 112: training accuarcy: 0.654\n",
      "Epoch 0 step 112: training loss: 1936.96702452279\n",
      "Epoch 0 step 113: training accuarcy: 0.6335000000000001\n",
      "Epoch 0 step 113: training loss: 1890.4055553429425\n",
      "Epoch 0 step 114: training accuarcy: 0.6475\n",
      "Epoch 0 step 114: training loss: 1858.3772514276006\n",
      "Epoch 0 step 115: training accuarcy: 0.6505\n",
      "Epoch 0 step 115: training loss: 1852.0074768136906\n",
      "Epoch 0 step 116: training accuarcy: 0.635\n",
      "Epoch 0 step 116: training loss: 1817.472065277123\n",
      "Epoch 0 step 117: training accuarcy: 0.63\n",
      "Epoch 0 step 117: training loss: 1788.3570453431366\n",
      "Epoch 0 step 118: training accuarcy: 0.645\n",
      "Epoch 0 step 118: training loss: 1809.9732437001708\n",
      "Epoch 0 step 119: training accuarcy: 0.629\n",
      "Epoch 0 step 119: training loss: 1753.933702428506\n",
      "Epoch 0 step 120: training accuarcy: 0.669\n",
      "Epoch 0 step 120: training loss: 1711.3048138802153\n",
      "Epoch 0 step 121: training accuarcy: 0.653\n",
      "Epoch 0 step 121: training loss: 1697.6088474869734\n",
      "Epoch 0 step 122: training accuarcy: 0.67\n",
      "Epoch 0 step 122: training loss: 1661.896077456041\n",
      "Epoch 0 step 123: training accuarcy: 0.675\n",
      "Epoch 0 step 123: training loss: 1683.023072497701\n",
      "Epoch 0 step 124: training accuarcy: 0.6635\n",
      "Epoch 0 step 124: training loss: 1652.4739931304698\n",
      "Epoch 0 step 125: training accuarcy: 0.6685\n",
      "Epoch 0 step 125: training loss: 1616.9556832013873\n",
      "Epoch 0 step 126: training accuarcy: 0.679\n",
      "Epoch 0 step 126: training loss: 1601.3858218091996\n",
      "Epoch 0 step 127: training accuarcy: 0.663\n",
      "Epoch 0 step 127: training loss: 1590.0400430969742\n",
      "Epoch 0 step 128: training accuarcy: 0.674\n",
      "Epoch 0 step 128: training loss: 1597.1030896341538\n",
      "Epoch 0 step 129: training accuarcy: 0.67\n",
      "Epoch 0 step 129: training loss: 1592.64801932366\n",
      "Epoch 0 step 130: training accuarcy: 0.67\n",
      "Epoch 0 step 130: training loss: 1539.4498079799905\n",
      "Epoch 0 step 131: training accuarcy: 0.677\n",
      "Epoch 0 step 131: training loss: 1560.3923474312892\n",
      "Epoch 0 step 132: training accuarcy: 0.663\n",
      "Epoch 0 step 132: training loss: 1523.003745992714\n",
      "Epoch 0 step 133: training accuarcy: 0.682\n",
      "Epoch 0 step 133: training loss: 1499.7957470508172\n",
      "Epoch 0 step 134: training accuarcy: 0.6915\n",
      "Epoch 0 step 134: training loss: 1490.566596933073\n",
      "Epoch 0 step 135: training accuarcy: 0.6825\n",
      "Epoch 0 step 135: training loss: 1476.2405843163474\n",
      "Epoch 0 step 136: training accuarcy: 0.6795\n",
      "Epoch 0 step 136: training loss: 1473.6328964018037\n",
      "Epoch 0 step 137: training accuarcy: 0.6935\n",
      "Epoch 0 step 137: training loss: 1468.0936737338188\n",
      "Epoch 0 step 138: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 138: training loss: 1452.19024946135\n",
      "Epoch 0 step 139: training accuarcy: 0.6900000000000001\n",
      "Epoch 0 step 139: training loss: 1439.6328811110154\n",
      "Epoch 0 step 140: training accuarcy: 0.687\n",
      "Epoch 0 step 140: training loss: 1432.2838269786334\n",
      "Epoch 0 step 141: training accuarcy: 0.684\n",
      "Epoch 0 step 141: training loss: 1435.8376045485536\n",
      "Epoch 0 step 142: training accuarcy: 0.6765\n",
      "Epoch 0 step 142: training loss: 1433.8026334328563\n",
      "Epoch 0 step 143: training accuarcy: 0.6930000000000001\n",
      "Epoch 0 step 143: training loss: 1422.5926631734762\n",
      "Epoch 0 step 144: training accuarcy: 0.7025\n",
      "Epoch 0 step 144: training loss: 1425.7236900425946\n",
      "Epoch 0 step 145: training accuarcy: 0.6745\n",
      "Epoch 0 step 145: training loss: 1364.9470300487017\n",
      "Epoch 0 step 146: training accuarcy: 0.7065\n",
      "Epoch 0 step 146: training loss: 1417.2806438144762\n",
      "Epoch 0 step 147: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 147: training loss: 1390.3864684135292\n",
      "Epoch 0 step 148: training accuarcy: 0.6825\n",
      "Epoch 0 step 148: training loss: 1384.5413570254182\n",
      "Epoch 0 step 149: training accuarcy: 0.6945\n",
      "Epoch 0 step 149: training loss: 1363.3079375062362\n",
      "Epoch 0 step 150: training accuarcy: 0.6925\n",
      "Epoch 0 step 150: training loss: 1367.83692892516\n",
      "Epoch 0 step 151: training accuarcy: 0.71\n",
      "Epoch 0 step 151: training loss: 1380.7999510488864\n",
      "Epoch 0 step 152: training accuarcy: 0.6855\n",
      "Epoch 0 step 152: training loss: 1328.5465209577103\n",
      "Epoch 0 step 153: training accuarcy: 0.7105\n",
      "Epoch 0 step 153: training loss: 1359.1454415936062\n",
      "Epoch 0 step 154: training accuarcy: 0.708\n",
      "Epoch 0 step 154: training loss: 1342.3482416460342\n",
      "Epoch 0 step 155: training accuarcy: 0.715\n",
      "Epoch 0 step 155: training loss: 1320.4467861613373\n",
      "Epoch 0 step 156: training accuarcy: 0.707\n",
      "Epoch 0 step 156: training loss: 1318.9408333875665\n",
      "Epoch 0 step 157: training accuarcy: 0.7010000000000001\n",
      "Epoch 0 step 157: training loss: 1318.6693954984878\n",
      "Epoch 0 step 158: training accuarcy: 0.709\n",
      "Epoch 0 step 158: training loss: 1309.8964110191207\n",
      "Epoch 0 step 159: training accuarcy: 0.714\n",
      "Epoch 0 step 159: training loss: 1288.2067682605834\n",
      "Epoch 0 step 160: training accuarcy: 0.709\n",
      "Epoch 0 step 160: training loss: 1290.6023086285634\n",
      "Epoch 0 step 161: training accuarcy: 0.713\n",
      "Epoch 0 step 161: training loss: 1327.8633398654765\n",
      "Epoch 0 step 162: training accuarcy: 0.7085\n",
      "Epoch 0 step 162: training loss: 1297.2180374602535\n",
      "Epoch 0 step 163: training accuarcy: 0.705\n",
      "Epoch 0 step 163: training loss: 1305.5834179711733\n",
      "Epoch 0 step 164: training accuarcy: 0.712\n",
      "Epoch 0 step 164: training loss: 1305.5079780512356\n",
      "Epoch 0 step 165: training accuarcy: 0.6950000000000001\n",
      "Epoch 0 step 165: training loss: 1257.1718874427122\n",
      "Epoch 0 step 166: training accuarcy: 0.7305\n",
      "Epoch 0 step 166: training loss: 1295.7545027499027\n",
      "Epoch 0 step 167: training accuarcy: 0.7095\n",
      "Epoch 0 step 167: training loss: 1286.714595484381\n",
      "Epoch 0 step 168: training accuarcy: 0.7055\n",
      "Epoch 0 step 168: training loss: 1255.502574983335\n",
      "Epoch 0 step 169: training accuarcy: 0.7135\n",
      "Epoch 0 step 169: training loss: 1276.19558249928\n",
      "Epoch 0 step 170: training accuarcy: 0.7085\n",
      "Epoch 0 step 170: training loss: 1239.1669860591603\n",
      "Epoch 0 step 171: training accuarcy: 0.7235\n",
      "Epoch 0 step 171: training loss: 1238.882031178933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 172: training accuarcy: 0.724\n",
      "Epoch 0 step 172: training loss: 1255.015799419085\n",
      "Epoch 0 step 173: training accuarcy: 0.718\n",
      "Epoch 0 step 173: training loss: 1243.445423480644\n",
      "Epoch 0 step 174: training accuarcy: 0.7265\n",
      "Epoch 0 step 174: training loss: 1241.8855561575454\n",
      "Epoch 0 step 175: training accuarcy: 0.724\n",
      "Epoch 0 step 175: training loss: 1240.0318690759455\n",
      "Epoch 0 step 176: training accuarcy: 0.7165\n",
      "Epoch 0 step 176: training loss: 1231.1232859525599\n",
      "Epoch 0 step 177: training accuarcy: 0.726\n",
      "Epoch 0 step 177: training loss: 1222.4003862039606\n",
      "Epoch 0 step 178: training accuarcy: 0.7425\n",
      "Epoch 0 step 178: training loss: 1280.6408639733775\n",
      "Epoch 0 step 179: training accuarcy: 0.7205\n",
      "Epoch 0 step 179: training loss: 1266.1483716507191\n",
      "Epoch 0 step 180: training accuarcy: 0.7155\n",
      "Epoch 0 step 180: training loss: 1246.5356428763448\n",
      "Epoch 0 step 181: training accuarcy: 0.7255\n",
      "Epoch 0 step 181: training loss: 1209.8314671368735\n",
      "Epoch 0 step 182: training accuarcy: 0.737\n",
      "Epoch 0 step 182: training loss: 1250.7014549830183\n",
      "Epoch 0 step 183: training accuarcy: 0.723\n",
      "Epoch 0 step 183: training loss: 1226.536665127215\n",
      "Epoch 0 step 184: training accuarcy: 0.7115\n",
      "Epoch 0 step 184: training loss: 1234.063726359071\n",
      "Epoch 0 step 185: training accuarcy: 0.7335\n",
      "Epoch 0 step 185: training loss: 1219.2435354094632\n",
      "Epoch 0 step 186: training accuarcy: 0.7275\n",
      "Epoch 0 step 186: training loss: 1214.2394076559915\n",
      "Epoch 0 step 187: training accuarcy: 0.735\n",
      "Epoch 0 step 187: training loss: 1209.7083933335864\n",
      "Epoch 0 step 188: training accuarcy: 0.7385\n",
      "Epoch 0 step 188: training loss: 1273.2908163201441\n",
      "Epoch 0 step 189: training accuarcy: 0.7315\n",
      "Epoch 0 step 189: training loss: 1213.2993039437717\n",
      "Epoch 0 step 190: training accuarcy: 0.7225\n",
      "Epoch 0 step 190: training loss: 1195.9499998462875\n",
      "Epoch 0 step 191: training accuarcy: 0.738\n",
      "Epoch 0 step 191: training loss: 1203.0336497215976\n",
      "Epoch 0 step 192: training accuarcy: 0.739\n",
      "Epoch 0 step 192: training loss: 1227.186383430057\n",
      "Epoch 0 step 193: training accuarcy: 0.725\n",
      "Epoch 0 step 193: training loss: 1201.594133665051\n",
      "Epoch 0 step 194: training accuarcy: 0.7395\n",
      "Epoch 0 step 194: training loss: 1223.8991194183507\n",
      "Epoch 0 step 195: training accuarcy: 0.735\n",
      "Epoch 0 step 195: training loss: 1187.0558697579556\n",
      "Epoch 0 step 196: training accuarcy: 0.757\n",
      "Epoch 0 step 196: training loss: 1205.6812871025336\n",
      "Epoch 0 step 197: training accuarcy: 0.746\n",
      "Epoch 0 step 197: training loss: 1211.74995477926\n",
      "Epoch 0 step 198: training accuarcy: 0.7275\n",
      "Epoch 0 step 198: training loss: 1185.1015220700026\n",
      "Epoch 0 step 199: training accuarcy: 0.7475\n",
      "Epoch 0 step 199: training loss: 1206.6068673168884\n",
      "Epoch 0 step 200: training accuarcy: 0.739\n",
      "Epoch 0 step 200: training loss: 1170.782014832765\n",
      "Epoch 0 step 201: training accuarcy: 0.753\n",
      "Epoch 0 step 201: training loss: 1196.3709794476754\n",
      "Epoch 0 step 202: training accuarcy: 0.7335\n",
      "Epoch 0 step 202: training loss: 1196.5488050085312\n",
      "Epoch 0 step 203: training accuarcy: 0.7485\n",
      "Epoch 0 step 203: training loss: 1192.5459196714532\n",
      "Epoch 0 step 204: training accuarcy: 0.7515000000000001\n",
      "Epoch 0 step 204: training loss: 1209.9095439010484\n",
      "Epoch 0 step 205: training accuarcy: 0.7465\n",
      "Epoch 0 step 205: training loss: 1197.8717897783583\n",
      "Epoch 0 step 206: training accuarcy: 0.7355\n",
      "Epoch 0 step 206: training loss: 1174.1301770998477\n",
      "Epoch 0 step 207: training accuarcy: 0.737\n",
      "Epoch 0 step 207: training loss: 1180.762465916968\n",
      "Epoch 0 step 208: training accuarcy: 0.748\n",
      "Epoch 0 step 208: training loss: 1183.3867553606167\n",
      "Epoch 0 step 209: training accuarcy: 0.742\n",
      "Epoch 0 step 209: training loss: 1184.759297900213\n",
      "Epoch 0 step 210: training accuarcy: 0.7425\n",
      "Epoch 0 step 210: training loss: 1170.8316190773462\n",
      "Epoch 0 step 211: training accuarcy: 0.755\n",
      "Epoch 0 step 211: training loss: 1172.1328498935034\n",
      "Epoch 0 step 212: training accuarcy: 0.7565000000000001\n",
      "Epoch 0 step 212: training loss: 1163.020635683556\n",
      "Epoch 0 step 213: training accuarcy: 0.759\n",
      "Epoch 0 step 213: training loss: 1189.4876937960446\n",
      "Epoch 0 step 214: training accuarcy: 0.7485\n",
      "Epoch 0 step 214: training loss: 1164.6200931371384\n",
      "Epoch 0 step 215: training accuarcy: 0.758\n",
      "Epoch 0 step 215: training loss: 1182.1743850055034\n",
      "Epoch 0 step 216: training accuarcy: 0.747\n",
      "Epoch 0 step 216: training loss: 1180.599240757383\n",
      "Epoch 0 step 217: training accuarcy: 0.7375\n",
      "Epoch 0 step 217: training loss: 1177.9381530257758\n",
      "Epoch 0 step 218: training accuarcy: 0.734\n",
      "Epoch 0 step 218: training loss: 1167.0932165632355\n",
      "Epoch 0 step 219: training accuarcy: 0.7575000000000001\n",
      "Epoch 0 step 219: training loss: 1170.3956536614126\n",
      "Epoch 0 step 220: training accuarcy: 0.7635000000000001\n",
      "Epoch 0 step 220: training loss: 1184.1911127573637\n",
      "Epoch 0 step 221: training accuarcy: 0.739\n",
      "Epoch 0 step 221: training loss: 1174.7439283544293\n",
      "Epoch 0 step 222: training accuarcy: 0.755\n",
      "Epoch 0 step 222: training loss: 1187.1751864299322\n",
      "Epoch 0 step 223: training accuarcy: 0.737\n",
      "Epoch 0 step 223: training loss: 1168.5479797568237\n",
      "Epoch 0 step 224: training accuarcy: 0.7615000000000001\n",
      "Epoch 0 step 224: training loss: 1180.1421961006056\n",
      "Epoch 0 step 225: training accuarcy: 0.7545000000000001\n",
      "Epoch 0 step 225: training loss: 1128.8036453102377\n",
      "Epoch 0 step 226: training accuarcy: 0.7605000000000001\n",
      "Epoch 0 step 226: training loss: 1171.4282555819393\n",
      "Epoch 0 step 227: training accuarcy: 0.7435\n",
      "Epoch 0 step 227: training loss: 1141.990521509612\n",
      "Epoch 0 step 228: training accuarcy: 0.764\n",
      "Epoch 0 step 228: training loss: 1142.261119861749\n",
      "Epoch 0 step 229: training accuarcy: 0.759\n",
      "Epoch 0 step 229: training loss: 1179.944483254088\n",
      "Epoch 0 step 230: training accuarcy: 0.747\n",
      "Epoch 0 step 230: training loss: 1156.5810242461348\n",
      "Epoch 0 step 231: training accuarcy: 0.76\n",
      "Epoch 0 step 231: training loss: 1151.0405207487547\n",
      "Epoch 0 step 232: training accuarcy: 0.768\n",
      "Epoch 0 step 232: training loss: 1131.6458431667602\n",
      "Epoch 0 step 233: training accuarcy: 0.771\n",
      "Epoch 0 step 233: training loss: 1177.0689651314217\n",
      "Epoch 0 step 234: training accuarcy: 0.7425\n",
      "Epoch 0 step 234: training loss: 1155.1663203727671\n",
      "Epoch 0 step 235: training accuarcy: 0.747\n",
      "Epoch 0 step 235: training loss: 1126.774158758157\n",
      "Epoch 0 step 236: training accuarcy: 0.76\n",
      "Epoch 0 step 236: training loss: 1151.5757162567497\n",
      "Epoch 0 step 237: training accuarcy: 0.7545000000000001\n",
      "Epoch 0 step 237: training loss: 1179.4520040606928\n",
      "Epoch 0 step 238: training accuarcy: 0.745\n",
      "Epoch 0 step 238: training loss: 1136.7556115493635\n",
      "Epoch 0 step 239: training accuarcy: 0.7615000000000001\n",
      "Epoch 0 step 239: training loss: 1111.9708077275789\n",
      "Epoch 0 step 240: training accuarcy: 0.775\n",
      "Epoch 0 step 240: training loss: 1157.4969252536862\n",
      "Epoch 0 step 241: training accuarcy: 0.7655000000000001\n",
      "Epoch 0 step 241: training loss: 1167.4369429337376\n",
      "Epoch 0 step 242: training accuarcy: 0.7605000000000001\n",
      "Epoch 0 step 242: training loss: 1151.5006711621743\n",
      "Epoch 0 step 243: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 243: training loss: 1117.2786503175482\n",
      "Epoch 0 step 244: training accuarcy: 0.768\n",
      "Epoch 0 step 244: training loss: 1190.6437125169646\n",
      "Epoch 0 step 245: training accuarcy: 0.7615000000000001\n",
      "Epoch 0 step 245: training loss: 1156.1328244775034\n",
      "Epoch 0 step 246: training accuarcy: 0.745\n",
      "Epoch 0 step 246: training loss: 1163.000400769478\n",
      "Epoch 0 step 247: training accuarcy: 0.761\n",
      "Epoch 0 step 247: training loss: 1158.8434609320593\n",
      "Epoch 0 step 248: training accuarcy: 0.7465\n",
      "Epoch 0 step 248: training loss: 1165.4871850330726\n",
      "Epoch 0 step 249: training accuarcy: 0.746\n",
      "Epoch 0 step 249: training loss: 1143.5287514482015\n",
      "Epoch 0 step 250: training accuarcy: 0.7605000000000001\n",
      "Epoch 0 step 250: training loss: 1119.6017451547373\n",
      "Epoch 0 step 251: training accuarcy: 0.77\n",
      "Epoch 0 step 251: training loss: 1159.1506516316172\n",
      "Epoch 0 step 252: training accuarcy: 0.75\n",
      "Epoch 0 step 252: training loss: 1146.4186253162302\n",
      "Epoch 0 step 253: training accuarcy: 0.761\n",
      "Epoch 0 step 253: training loss: 1132.1190870673267\n",
      "Epoch 0 step 254: training accuarcy: 0.7655000000000001\n",
      "Epoch 0 step 254: training loss: 1167.9426538642792\n",
      "Epoch 0 step 255: training accuarcy: 0.7475\n",
      "Epoch 0 step 255: training loss: 1151.7030247416542\n",
      "Epoch 0 step 256: training accuarcy: 0.751\n",
      "Epoch 0 step 256: training loss: 1181.8635354003202\n",
      "Epoch 0 step 257: training accuarcy: 0.7395\n",
      "Epoch 0 step 257: training loss: 1166.53368106909\n",
      "Epoch 0 step 258: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 258: training loss: 1126.2945352588888\n",
      "Epoch 0 step 259: training accuarcy: 0.765\n",
      "Epoch 0 step 259: training loss: 1138.9558823780617\n",
      "Epoch 0 step 260: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 260: training loss: 1162.48548838082\n",
      "Epoch 0 step 261: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 261: training loss: 1154.5879302035491\n",
      "Epoch 0 step 262: training accuarcy: 0.7715\n",
      "Epoch 0 step 262: training loss: 469.8548283717866\n",
      "Epoch 0 step 263: training accuarcy: 0.7653846153846153\n",
      "Epoch 0: train loss 5892.473445062888, train accuarcy 0.6273751258850098\n",
      "Epoch 0: valid loss 1125.2355265512006, valid accuarcy 0.759753406047821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▍                                                                                                                                         | 1/5 [01:54<07:36, 114.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1161.0205454811971\n",
      "Epoch 1 step 264: training accuarcy: 0.759\n",
      "Epoch 1 step 264: training loss: 1114.0463723028427\n",
      "Epoch 1 step 265: training accuarcy: 0.7685\n",
      "Epoch 1 step 265: training loss: 1137.5079738431443\n",
      "Epoch 1 step 266: training accuarcy: 0.771\n",
      "Epoch 1 step 266: training loss: 1146.1399610397661\n",
      "Epoch 1 step 267: training accuarcy: 0.762\n",
      "Epoch 1 step 267: training loss: 1140.6525136081366\n",
      "Epoch 1 step 268: training accuarcy: 0.772\n",
      "Epoch 1 step 268: training loss: 1142.4505502583102\n",
      "Epoch 1 step 269: training accuarcy: 0.791\n",
      "Epoch 1 step 269: training loss: 1133.9076002596264\n",
      "Epoch 1 step 270: training accuarcy: 0.76\n",
      "Epoch 1 step 270: training loss: 1134.2826896525069\n",
      "Epoch 1 step 271: training accuarcy: 0.769\n",
      "Epoch 1 step 271: training loss: 1135.9700961490314\n",
      "Epoch 1 step 272: training accuarcy: 0.776\n",
      "Epoch 1 step 272: training loss: 1128.2721628699794\n",
      "Epoch 1 step 273: training accuarcy: 0.772\n",
      "Epoch 1 step 273: training loss: 1142.825720269585\n",
      "Epoch 1 step 274: training accuarcy: 0.766\n",
      "Epoch 1 step 274: training loss: 1155.7046770559696\n",
      "Epoch 1 step 275: training accuarcy: 0.749\n",
      "Epoch 1 step 275: training loss: 1101.5006558898822\n",
      "Epoch 1 step 276: training accuarcy: 0.773\n",
      "Epoch 1 step 276: training loss: 1131.6027813494275\n",
      "Epoch 1 step 277: training accuarcy: 0.7765\n",
      "Epoch 1 step 277: training loss: 1098.5798369463084\n",
      "Epoch 1 step 278: training accuarcy: 0.7935\n",
      "Epoch 1 step 278: training loss: 1141.236361297069\n",
      "Epoch 1 step 279: training accuarcy: 0.774\n",
      "Epoch 1 step 279: training loss: 1126.050817692143\n",
      "Epoch 1 step 280: training accuarcy: 0.767\n",
      "Epoch 1 step 280: training loss: 1119.5646811162571\n",
      "Epoch 1 step 281: training accuarcy: 0.7695\n",
      "Epoch 1 step 281: training loss: 1155.5798419979892\n",
      "Epoch 1 step 282: training accuarcy: 0.747\n",
      "Epoch 1 step 282: training loss: 1110.1935025263251\n",
      "Epoch 1 step 283: training accuarcy: 0.781\n",
      "Epoch 1 step 283: training loss: 1165.753674093396\n",
      "Epoch 1 step 284: training accuarcy: 0.753\n",
      "Epoch 1 step 284: training loss: 1109.3379836681854\n",
      "Epoch 1 step 285: training accuarcy: 0.7625000000000001\n",
      "Epoch 1 step 285: training loss: 1101.8137313449731\n",
      "Epoch 1 step 286: training accuarcy: 0.7705\n",
      "Epoch 1 step 286: training loss: 1143.1671954169738\n",
      "Epoch 1 step 287: training accuarcy: 0.764\n",
      "Epoch 1 step 287: training loss: 1146.028165814605\n",
      "Epoch 1 step 288: training accuarcy: 0.7525000000000001\n",
      "Epoch 1 step 288: training loss: 1118.444621754857\n",
      "Epoch 1 step 289: training accuarcy: 0.7455\n",
      "Epoch 1 step 289: training loss: 1139.7019556831822\n",
      "Epoch 1 step 290: training accuarcy: 0.761\n",
      "Epoch 1 step 290: training loss: 1139.0242644789944\n",
      "Epoch 1 step 291: training accuarcy: 0.7595000000000001\n",
      "Epoch 1 step 291: training loss: 1136.49854353275\n",
      "Epoch 1 step 292: training accuarcy: 0.768\n",
      "Epoch 1 step 292: training loss: 1123.1596657423663\n",
      "Epoch 1 step 293: training accuarcy: 0.7675000000000001\n",
      "Epoch 1 step 293: training loss: 1146.3483917943101\n",
      "Epoch 1 step 294: training accuarcy: 0.7535000000000001\n",
      "Epoch 1 step 294: training loss: 1119.3237004167058\n",
      "Epoch 1 step 295: training accuarcy: 0.773\n",
      "Epoch 1 step 295: training loss: 1179.4135905176854\n",
      "Epoch 1 step 296: training accuarcy: 0.74\n",
      "Epoch 1 step 296: training loss: 1142.8725544900676\n",
      "Epoch 1 step 297: training accuarcy: 0.7605000000000001\n",
      "Epoch 1 step 297: training loss: 1143.59093416143\n",
      "Epoch 1 step 298: training accuarcy: 0.788\n",
      "Epoch 1 step 298: training loss: 1114.3239011057012\n",
      "Epoch 1 step 299: training accuarcy: 0.775\n",
      "Epoch 1 step 299: training loss: 1139.206752195212\n",
      "Epoch 1 step 300: training accuarcy: 0.7695\n",
      "Epoch 1 step 300: training loss: 1117.9654820605597\n",
      "Epoch 1 step 301: training accuarcy: 0.7715\n",
      "Epoch 1 step 301: training loss: 1143.6049303169923\n",
      "Epoch 1 step 302: training accuarcy: 0.7595000000000001\n",
      "Epoch 1 step 302: training loss: 1152.823339512518\n",
      "Epoch 1 step 303: training accuarcy: 0.764\n",
      "Epoch 1 step 303: training loss: 1146.5240803775946\n",
      "Epoch 1 step 304: training accuarcy: 0.7685\n",
      "Epoch 1 step 304: training loss: 1149.3053418145232\n",
      "Epoch 1 step 305: training accuarcy: 0.7565000000000001\n",
      "Epoch 1 step 305: training loss: 1112.310646074195\n",
      "Epoch 1 step 306: training accuarcy: 0.7655000000000001\n",
      "Epoch 1 step 306: training loss: 1130.5971295799284\n",
      "Epoch 1 step 307: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 307: training loss: 1129.0610796784822\n",
      "Epoch 1 step 308: training accuarcy: 0.777\n",
      "Epoch 1 step 308: training loss: 1119.8430981273252\n",
      "Epoch 1 step 309: training accuarcy: 0.778\n",
      "Epoch 1 step 309: training loss: 1118.3511668581652\n",
      "Epoch 1 step 310: training accuarcy: 0.7765\n",
      "Epoch 1 step 310: training loss: 1129.2945721798276\n",
      "Epoch 1 step 311: training accuarcy: 0.782\n",
      "Epoch 1 step 311: training loss: 1128.5969948003572\n",
      "Epoch 1 step 312: training accuarcy: 0.7595000000000001\n",
      "Epoch 1 step 312: training loss: 1147.553157419118\n",
      "Epoch 1 step 313: training accuarcy: 0.7785\n",
      "Epoch 1 step 313: training loss: 1115.539471367732\n",
      "Epoch 1 step 314: training accuarcy: 0.784\n",
      "Epoch 1 step 314: training loss: 1139.5356431965529\n",
      "Epoch 1 step 315: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 315: training loss: 1135.184140464832\n",
      "Epoch 1 step 316: training accuarcy: 0.7685\n",
      "Epoch 1 step 316: training loss: 1154.4546248453764\n",
      "Epoch 1 step 317: training accuarcy: 0.7515000000000001\n",
      "Epoch 1 step 317: training loss: 1127.3215422555586\n",
      "Epoch 1 step 318: training accuarcy: 0.7775\n",
      "Epoch 1 step 318: training loss: 1121.141246458146\n",
      "Epoch 1 step 319: training accuarcy: 0.778\n",
      "Epoch 1 step 319: training loss: 1120.1368183146112\n",
      "Epoch 1 step 320: training accuarcy: 0.7825\n",
      "Epoch 1 step 320: training loss: 1143.9377383107928\n",
      "Epoch 1 step 321: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 321: training loss: 1142.973094583485\n",
      "Epoch 1 step 322: training accuarcy: 0.76\n",
      "Epoch 1 step 322: training loss: 1121.3550048967497\n",
      "Epoch 1 step 323: training accuarcy: 0.763\n",
      "Epoch 1 step 323: training loss: 1115.4577216609857\n",
      "Epoch 1 step 324: training accuarcy: 0.772\n",
      "Epoch 1 step 324: training loss: 1134.4930310229283\n",
      "Epoch 1 step 325: training accuarcy: 0.7765\n",
      "Epoch 1 step 325: training loss: 1131.4231079795543\n",
      "Epoch 1 step 326: training accuarcy: 0.762\n",
      "Epoch 1 step 326: training loss: 1135.5543795818735\n",
      "Epoch 1 step 327: training accuarcy: 0.765\n",
      "Epoch 1 step 327: training loss: 1100.6353194062974\n",
      "Epoch 1 step 328: training accuarcy: 0.7885\n",
      "Epoch 1 step 328: training loss: 1121.2821297418827\n",
      "Epoch 1 step 329: training accuarcy: 0.768\n",
      "Epoch 1 step 329: training loss: 1153.4713027216096\n",
      "Epoch 1 step 330: training accuarcy: 0.771\n",
      "Epoch 1 step 330: training loss: 1105.3523691062358\n",
      "Epoch 1 step 331: training accuarcy: 0.7705\n",
      "Epoch 1 step 331: training loss: 1113.3278408933338\n",
      "Epoch 1 step 332: training accuarcy: 0.7855\n",
      "Epoch 1 step 332: training loss: 1132.458247139632\n",
      "Epoch 1 step 333: training accuarcy: 0.774\n",
      "Epoch 1 step 333: training loss: 1122.2356120414072\n",
      "Epoch 1 step 334: training accuarcy: 0.775\n",
      "Epoch 1 step 334: training loss: 1152.604219588413\n",
      "Epoch 1 step 335: training accuarcy: 0.766\n",
      "Epoch 1 step 335: training loss: 1126.3824095148714\n",
      "Epoch 1 step 336: training accuarcy: 0.77\n",
      "Epoch 1 step 336: training loss: 1126.0692730142794\n",
      "Epoch 1 step 337: training accuarcy: 0.763\n",
      "Epoch 1 step 337: training loss: 1112.9896542198858\n",
      "Epoch 1 step 338: training accuarcy: 0.784\n",
      "Epoch 1 step 338: training loss: 1152.3640150337897\n",
      "Epoch 1 step 339: training accuarcy: 0.782\n",
      "Epoch 1 step 339: training loss: 1120.3342270506569\n",
      "Epoch 1 step 340: training accuarcy: 0.783\n",
      "Epoch 1 step 340: training loss: 1093.7850558696366\n",
      "Epoch 1 step 341: training accuarcy: 0.798\n",
      "Epoch 1 step 341: training loss: 1110.461351915692\n",
      "Epoch 1 step 342: training accuarcy: 0.7675000000000001\n",
      "Epoch 1 step 342: training loss: 1113.8125446870536\n",
      "Epoch 1 step 343: training accuarcy: 0.785\n",
      "Epoch 1 step 343: training loss: 1115.5996204036337\n",
      "Epoch 1 step 344: training accuarcy: 0.782\n",
      "Epoch 1 step 344: training loss: 1117.9279814447427\n",
      "Epoch 1 step 345: training accuarcy: 0.779\n",
      "Epoch 1 step 345: training loss: 1149.2734471748508\n",
      "Epoch 1 step 346: training accuarcy: 0.7705\n",
      "Epoch 1 step 346: training loss: 1136.8497879424945\n",
      "Epoch 1 step 347: training accuarcy: 0.769\n",
      "Epoch 1 step 347: training loss: 1107.5552629353979\n",
      "Epoch 1 step 348: training accuarcy: 0.7775\n",
      "Epoch 1 step 348: training loss: 1099.3761273661003\n",
      "Epoch 1 step 349: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 349: training loss: 1097.5685879811447\n",
      "Epoch 1 step 350: training accuarcy: 0.7795\n",
      "Epoch 1 step 350: training loss: 1164.7169298518459\n",
      "Epoch 1 step 351: training accuarcy: 0.778\n",
      "Epoch 1 step 351: training loss: 1113.4064613422665\n",
      "Epoch 1 step 352: training accuarcy: 0.7805\n",
      "Epoch 1 step 352: training loss: 1114.2799472315533\n",
      "Epoch 1 step 353: training accuarcy: 0.78\n",
      "Epoch 1 step 353: training loss: 1146.8521080022651\n",
      "Epoch 1 step 354: training accuarcy: 0.7565000000000001\n",
      "Epoch 1 step 354: training loss: 1113.7520051495983\n",
      "Epoch 1 step 355: training accuarcy: 0.782\n",
      "Epoch 1 step 355: training loss: 1104.1089715288342\n",
      "Epoch 1 step 356: training accuarcy: 0.7825\n",
      "Epoch 1 step 356: training loss: 1120.4308437553068\n",
      "Epoch 1 step 357: training accuarcy: 0.7735\n",
      "Epoch 1 step 357: training loss: 1119.3510733591352\n",
      "Epoch 1 step 358: training accuarcy: 0.767\n",
      "Epoch 1 step 358: training loss: 1109.0072905547622\n",
      "Epoch 1 step 359: training accuarcy: 0.7855\n",
      "Epoch 1 step 359: training loss: 1089.1642659376187\n",
      "Epoch 1 step 360: training accuarcy: 0.788\n",
      "Epoch 1 step 360: training loss: 1114.1576966375685\n",
      "Epoch 1 step 361: training accuarcy: 0.777\n",
      "Epoch 1 step 361: training loss: 1130.6912442459031\n",
      "Epoch 1 step 362: training accuarcy: 0.784\n",
      "Epoch 1 step 362: training loss: 1110.8296878663923\n",
      "Epoch 1 step 363: training accuarcy: 0.772\n",
      "Epoch 1 step 363: training loss: 1133.965685630584\n",
      "Epoch 1 step 364: training accuarcy: 0.77\n",
      "Epoch 1 step 364: training loss: 1138.0269263538892\n",
      "Epoch 1 step 365: training accuarcy: 0.7715\n",
      "Epoch 1 step 365: training loss: 1095.9594675867093\n",
      "Epoch 1 step 366: training accuarcy: 0.7875\n",
      "Epoch 1 step 366: training loss: 1112.2959608600966\n",
      "Epoch 1 step 367: training accuarcy: 0.7795\n",
      "Epoch 1 step 367: training loss: 1134.0957918058007\n",
      "Epoch 1 step 368: training accuarcy: 0.775\n",
      "Epoch 1 step 368: training loss: 1096.7663458459715\n",
      "Epoch 1 step 369: training accuarcy: 0.7835\n",
      "Epoch 1 step 369: training loss: 1090.0584354423888\n",
      "Epoch 1 step 370: training accuarcy: 0.7845\n",
      "Epoch 1 step 370: training loss: 1115.0287407571934\n",
      "Epoch 1 step 371: training accuarcy: 0.7725\n",
      "Epoch 1 step 371: training loss: 1133.749940259399\n",
      "Epoch 1 step 372: training accuarcy: 0.769\n",
      "Epoch 1 step 372: training loss: 1090.1849980356126\n",
      "Epoch 1 step 373: training accuarcy: 0.788\n",
      "Epoch 1 step 373: training loss: 1096.5995944041579\n",
      "Epoch 1 step 374: training accuarcy: 0.793\n",
      "Epoch 1 step 374: training loss: 1085.3104709383902\n",
      "Epoch 1 step 375: training accuarcy: 0.7795\n",
      "Epoch 1 step 375: training loss: 1118.9189486597202\n",
      "Epoch 1 step 376: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 376: training loss: 1111.0434686664025\n",
      "Epoch 1 step 377: training accuarcy: 0.769\n",
      "Epoch 1 step 377: training loss: 1135.0954028670258\n",
      "Epoch 1 step 378: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 378: training loss: 1119.63440172577\n",
      "Epoch 1 step 379: training accuarcy: 0.7825\n",
      "Epoch 1 step 379: training loss: 1110.5731151994362\n",
      "Epoch 1 step 380: training accuarcy: 0.7785\n",
      "Epoch 1 step 380: training loss: 1120.820180388337\n",
      "Epoch 1 step 381: training accuarcy: 0.79\n",
      "Epoch 1 step 381: training loss: 1129.5962408100047\n",
      "Epoch 1 step 382: training accuarcy: 0.776\n",
      "Epoch 1 step 382: training loss: 1104.9489365421161\n",
      "Epoch 1 step 383: training accuarcy: 0.781\n",
      "Epoch 1 step 383: training loss: 1127.4479093749437\n",
      "Epoch 1 step 384: training accuarcy: 0.7805\n",
      "Epoch 1 step 384: training loss: 1109.492874984582\n",
      "Epoch 1 step 385: training accuarcy: 0.7715\n",
      "Epoch 1 step 385: training loss: 1147.5615865333289\n",
      "Epoch 1 step 386: training accuarcy: 0.774\n",
      "Epoch 1 step 386: training loss: 1089.3090947023727\n",
      "Epoch 1 step 387: training accuarcy: 0.8005\n",
      "Epoch 1 step 387: training loss: 1108.874214824493\n",
      "Epoch 1 step 388: training accuarcy: 0.78\n",
      "Epoch 1 step 388: training loss: 1112.7489576846542\n",
      "Epoch 1 step 389: training accuarcy: 0.773\n",
      "Epoch 1 step 389: training loss: 1117.8882864904845\n",
      "Epoch 1 step 390: training accuarcy: 0.773\n",
      "Epoch 1 step 390: training loss: 1113.1534958070672\n",
      "Epoch 1 step 391: training accuarcy: 0.7825\n",
      "Epoch 1 step 391: training loss: 1136.0584455962562\n",
      "Epoch 1 step 392: training accuarcy: 0.7645000000000001\n",
      "Epoch 1 step 392: training loss: 1114.3654746886373\n",
      "Epoch 1 step 393: training accuarcy: 0.7955\n",
      "Epoch 1 step 393: training loss: 1093.6858899024546\n",
      "Epoch 1 step 394: training accuarcy: 0.7895\n",
      "Epoch 1 step 394: training loss: 1076.4000636494923\n",
      "Epoch 1 step 395: training accuarcy: 0.796\n",
      "Epoch 1 step 395: training loss: 1113.4833437053378\n",
      "Epoch 1 step 396: training accuarcy: 0.7815\n",
      "Epoch 1 step 396: training loss: 1155.3011577789107\n",
      "Epoch 1 step 397: training accuarcy: 0.786\n",
      "Epoch 1 step 397: training loss: 1120.7456669850378\n",
      "Epoch 1 step 398: training accuarcy: 0.7765\n",
      "Epoch 1 step 398: training loss: 1133.459385953699\n",
      "Epoch 1 step 399: training accuarcy: 0.79\n",
      "Epoch 1 step 399: training loss: 1100.3956774136961\n",
      "Epoch 1 step 400: training accuarcy: 0.7905\n",
      "Epoch 1 step 400: training loss: 1092.4732339178222\n",
      "Epoch 1 step 401: training accuarcy: 0.797\n",
      "Epoch 1 step 401: training loss: 1107.9403446398992\n",
      "Epoch 1 step 402: training accuarcy: 0.7855\n",
      "Epoch 1 step 402: training loss: 1113.7548327322786\n",
      "Epoch 1 step 403: training accuarcy: 0.7845\n",
      "Epoch 1 step 403: training loss: 1133.1021125752757\n",
      "Epoch 1 step 404: training accuarcy: 0.7865\n",
      "Epoch 1 step 404: training loss: 1091.4082563830018\n",
      "Epoch 1 step 405: training accuarcy: 0.7975\n",
      "Epoch 1 step 405: training loss: 1103.444821342808\n",
      "Epoch 1 step 406: training accuarcy: 0.7905\n",
      "Epoch 1 step 406: training loss: 1120.87448261316\n",
      "Epoch 1 step 407: training accuarcy: 0.7975\n",
      "Epoch 1 step 407: training loss: 1113.621370795283\n",
      "Epoch 1 step 408: training accuarcy: 0.7865\n",
      "Epoch 1 step 408: training loss: 1102.2403597294456\n",
      "Epoch 1 step 409: training accuarcy: 0.7865\n",
      "Epoch 1 step 409: training loss: 1088.4764781521312\n",
      "Epoch 1 step 410: training accuarcy: 0.7995\n",
      "Epoch 1 step 410: training loss: 1117.8100809559962\n",
      "Epoch 1 step 411: training accuarcy: 0.8025\n",
      "Epoch 1 step 411: training loss: 1087.1457991579157\n",
      "Epoch 1 step 412: training accuarcy: 0.8055\n",
      "Epoch 1 step 412: training loss: 1126.232329207053\n",
      "Epoch 1 step 413: training accuarcy: 0.7695\n",
      "Epoch 1 step 413: training loss: 1109.9568888697922\n",
      "Epoch 1 step 414: training accuarcy: 0.788\n",
      "Epoch 1 step 414: training loss: 1106.7024650428177\n",
      "Epoch 1 step 415: training accuarcy: 0.767\n",
      "Epoch 1 step 415: training loss: 1094.1846909591536\n",
      "Epoch 1 step 416: training accuarcy: 0.796\n",
      "Epoch 1 step 416: training loss: 1102.3914718192711\n",
      "Epoch 1 step 417: training accuarcy: 0.787\n",
      "Epoch 1 step 417: training loss: 1062.045800122659\n",
      "Epoch 1 step 418: training accuarcy: 0.788\n",
      "Epoch 1 step 418: training loss: 1130.191425956841\n",
      "Epoch 1 step 419: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 419: training loss: 1094.779838891703\n",
      "Epoch 1 step 420: training accuarcy: 0.779\n",
      "Epoch 1 step 420: training loss: 1109.0699008943607\n",
      "Epoch 1 step 421: training accuarcy: 0.78\n",
      "Epoch 1 step 421: training loss: 1114.9848710314432\n",
      "Epoch 1 step 422: training accuarcy: 0.785\n",
      "Epoch 1 step 422: training loss: 1071.2773209831385\n",
      "Epoch 1 step 423: training accuarcy: 0.798\n",
      "Epoch 1 step 423: training loss: 1117.4878185574787\n",
      "Epoch 1 step 424: training accuarcy: 0.8\n",
      "Epoch 1 step 424: training loss: 1062.057301274341\n",
      "Epoch 1 step 425: training accuarcy: 0.798\n",
      "Epoch 1 step 425: training loss: 1151.66316695254\n",
      "Epoch 1 step 426: training accuarcy: 0.7925\n",
      "Epoch 1 step 426: training loss: 1089.1182481426008\n",
      "Epoch 1 step 427: training accuarcy: 0.792\n",
      "Epoch 1 step 427: training loss: 1121.4658275224342\n",
      "Epoch 1 step 428: training accuarcy: 0.7785\n",
      "Epoch 1 step 428: training loss: 1088.4486250500072\n",
      "Epoch 1 step 429: training accuarcy: 0.7925\n",
      "Epoch 1 step 429: training loss: 1099.182175554503\n",
      "Epoch 1 step 430: training accuarcy: 0.7885\n",
      "Epoch 1 step 430: training loss: 1110.9744718736627\n",
      "Epoch 1 step 431: training accuarcy: 0.7765\n",
      "Epoch 1 step 431: training loss: 1105.751646885225\n",
      "Epoch 1 step 432: training accuarcy: 0.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 432: training loss: 1116.0131972071235\n",
      "Epoch 1 step 433: training accuarcy: 0.7835\n",
      "Epoch 1 step 433: training loss: 1117.701232790347\n",
      "Epoch 1 step 434: training accuarcy: 0.793\n",
      "Epoch 1 step 434: training loss: 1091.7669693749392\n",
      "Epoch 1 step 435: training accuarcy: 0.7945\n",
      "Epoch 1 step 435: training loss: 1081.775015947967\n",
      "Epoch 1 step 436: training accuarcy: 0.799\n",
      "Epoch 1 step 436: training loss: 1101.5096819631713\n",
      "Epoch 1 step 437: training accuarcy: 0.792\n",
      "Epoch 1 step 437: training loss: 1094.4633121175946\n",
      "Epoch 1 step 438: training accuarcy: 0.804\n",
      "Epoch 1 step 438: training loss: 1104.0837338073957\n",
      "Epoch 1 step 439: training accuarcy: 0.793\n",
      "Epoch 1 step 439: training loss: 1077.202240319614\n",
      "Epoch 1 step 440: training accuarcy: 0.798\n",
      "Epoch 1 step 440: training loss: 1128.966430646535\n",
      "Epoch 1 step 441: training accuarcy: 0.7755\n",
      "Epoch 1 step 441: training loss: 1111.4319274603372\n",
      "Epoch 1 step 442: training accuarcy: 0.794\n",
      "Epoch 1 step 442: training loss: 1099.694271727312\n",
      "Epoch 1 step 443: training accuarcy: 0.7995\n",
      "Epoch 1 step 443: training loss: 1069.0321847767136\n",
      "Epoch 1 step 444: training accuarcy: 0.8005\n",
      "Epoch 1 step 444: training loss: 1110.543210953955\n",
      "Epoch 1 step 445: training accuarcy: 0.794\n",
      "Epoch 1 step 445: training loss: 1073.0722676128162\n",
      "Epoch 1 step 446: training accuarcy: 0.804\n",
      "Epoch 1 step 446: training loss: 1073.3543407775303\n",
      "Epoch 1 step 447: training accuarcy: 0.809\n",
      "Epoch 1 step 447: training loss: 1138.5565485379045\n",
      "Epoch 1 step 448: training accuarcy: 0.7775\n",
      "Epoch 1 step 448: training loss: 1110.2779481761488\n",
      "Epoch 1 step 449: training accuarcy: 0.7815\n",
      "Epoch 1 step 449: training loss: 1101.781900415634\n",
      "Epoch 1 step 450: training accuarcy: 0.7905\n",
      "Epoch 1 step 450: training loss: 1107.976415497789\n",
      "Epoch 1 step 451: training accuarcy: 0.787\n",
      "Epoch 1 step 451: training loss: 1103.009512760029\n",
      "Epoch 1 step 452: training accuarcy: 0.7945\n",
      "Epoch 1 step 452: training loss: 1114.8133603434533\n",
      "Epoch 1 step 453: training accuarcy: 0.791\n",
      "Epoch 1 step 453: training loss: 1108.00011188313\n",
      "Epoch 1 step 454: training accuarcy: 0.8005\n",
      "Epoch 1 step 454: training loss: 1128.1447623055444\n",
      "Epoch 1 step 455: training accuarcy: 0.791\n",
      "Epoch 1 step 455: training loss: 1103.1257341881162\n",
      "Epoch 1 step 456: training accuarcy: 0.8135\n",
      "Epoch 1 step 456: training loss: 1115.2564984686467\n",
      "Epoch 1 step 457: training accuarcy: 0.795\n",
      "Epoch 1 step 457: training loss: 1115.4315793628855\n",
      "Epoch 1 step 458: training accuarcy: 0.7965\n",
      "Epoch 1 step 458: training loss: 1080.6912775898918\n",
      "Epoch 1 step 459: training accuarcy: 0.801\n",
      "Epoch 1 step 459: training loss: 1105.1396227992273\n",
      "Epoch 1 step 460: training accuarcy: 0.789\n",
      "Epoch 1 step 460: training loss: 1093.1423607718705\n",
      "Epoch 1 step 461: training accuarcy: 0.798\n",
      "Epoch 1 step 461: training loss: 1091.7070997228889\n",
      "Epoch 1 step 462: training accuarcy: 0.7955\n",
      "Epoch 1 step 462: training loss: 1101.9761314788302\n",
      "Epoch 1 step 463: training accuarcy: 0.7765\n",
      "Epoch 1 step 463: training loss: 1114.7773116526039\n",
      "Epoch 1 step 464: training accuarcy: 0.797\n",
      "Epoch 1 step 464: training loss: 1101.9389860543138\n",
      "Epoch 1 step 465: training accuarcy: 0.799\n",
      "Epoch 1 step 465: training loss: 1102.527973556889\n",
      "Epoch 1 step 466: training accuarcy: 0.798\n",
      "Epoch 1 step 466: training loss: 1101.3970985896376\n",
      "Epoch 1 step 467: training accuarcy: 0.788\n",
      "Epoch 1 step 467: training loss: 1109.9972944690182\n",
      "Epoch 1 step 468: training accuarcy: 0.791\n",
      "Epoch 1 step 468: training loss: 1076.7338634457196\n",
      "Epoch 1 step 469: training accuarcy: 0.8035\n",
      "Epoch 1 step 469: training loss: 1097.3854570016144\n",
      "Epoch 1 step 470: training accuarcy: 0.803\n",
      "Epoch 1 step 470: training loss: 1091.1402898411907\n",
      "Epoch 1 step 471: training accuarcy: 0.7945\n",
      "Epoch 1 step 471: training loss: 1109.8498818599844\n",
      "Epoch 1 step 472: training accuarcy: 0.7955\n",
      "Epoch 1 step 472: training loss: 1118.5017354838774\n",
      "Epoch 1 step 473: training accuarcy: 0.793\n",
      "Epoch 1 step 473: training loss: 1064.3744818971609\n",
      "Epoch 1 step 474: training accuarcy: 0.7925\n",
      "Epoch 1 step 474: training loss: 1130.8821794421065\n",
      "Epoch 1 step 475: training accuarcy: 0.784\n",
      "Epoch 1 step 475: training loss: 1079.220325908983\n",
      "Epoch 1 step 476: training accuarcy: 0.798\n",
      "Epoch 1 step 476: training loss: 1114.4935527269552\n",
      "Epoch 1 step 477: training accuarcy: 0.7805\n",
      "Epoch 1 step 477: training loss: 1093.7085038817725\n",
      "Epoch 1 step 478: training accuarcy: 0.7945\n",
      "Epoch 1 step 478: training loss: 1087.4441247018135\n",
      "Epoch 1 step 479: training accuarcy: 0.797\n",
      "Epoch 1 step 479: training loss: 1113.7508387773828\n",
      "Epoch 1 step 480: training accuarcy: 0.793\n",
      "Epoch 1 step 480: training loss: 1130.3227211979852\n",
      "Epoch 1 step 481: training accuarcy: 0.7905\n",
      "Epoch 1 step 481: training loss: 1082.99817191063\n",
      "Epoch 1 step 482: training accuarcy: 0.7975\n",
      "Epoch 1 step 482: training loss: 1090.925615973949\n",
      "Epoch 1 step 483: training accuarcy: 0.7995\n",
      "Epoch 1 step 483: training loss: 1084.910736153201\n",
      "Epoch 1 step 484: training accuarcy: 0.791\n",
      "Epoch 1 step 484: training loss: 1109.138884859017\n",
      "Epoch 1 step 485: training accuarcy: 0.7765\n",
      "Epoch 1 step 485: training loss: 1111.701656236578\n",
      "Epoch 1 step 486: training accuarcy: 0.779\n",
      "Epoch 1 step 486: training loss: 1065.7028364967855\n",
      "Epoch 1 step 487: training accuarcy: 0.8015\n",
      "Epoch 1 step 487: training loss: 1105.9044101937077\n",
      "Epoch 1 step 488: training accuarcy: 0.7905\n",
      "Epoch 1 step 488: training loss: 1100.5836667303208\n",
      "Epoch 1 step 489: training accuarcy: 0.7895\n",
      "Epoch 1 step 489: training loss: 1136.187596026488\n",
      "Epoch 1 step 490: training accuarcy: 0.7935\n",
      "Epoch 1 step 490: training loss: 1094.1563047518475\n",
      "Epoch 1 step 491: training accuarcy: 0.798\n",
      "Epoch 1 step 491: training loss: 1094.1763335356216\n",
      "Epoch 1 step 492: training accuarcy: 0.7965\n",
      "Epoch 1 step 492: training loss: 1114.1952081384788\n",
      "Epoch 1 step 493: training accuarcy: 0.795\n",
      "Epoch 1 step 493: training loss: 1111.5106420620214\n",
      "Epoch 1 step 494: training accuarcy: 0.8025\n",
      "Epoch 1 step 494: training loss: 1087.325490258102\n",
      "Epoch 1 step 495: training accuarcy: 0.803\n",
      "Epoch 1 step 495: training loss: 1111.9092375178936\n",
      "Epoch 1 step 496: training accuarcy: 0.7945\n",
      "Epoch 1 step 496: training loss: 1080.2116882237437\n",
      "Epoch 1 step 497: training accuarcy: 0.805\n",
      "Epoch 1 step 497: training loss: 1064.2059424560275\n",
      "Epoch 1 step 498: training accuarcy: 0.8075\n",
      "Epoch 1 step 498: training loss: 1074.472919282318\n",
      "Epoch 1 step 499: training accuarcy: 0.799\n",
      "Epoch 1 step 499: training loss: 1103.9604237306637\n",
      "Epoch 1 step 500: training accuarcy: 0.784\n",
      "Epoch 1 step 500: training loss: 1119.5242688934218\n",
      "Epoch 1 step 501: training accuarcy: 0.7855\n",
      "Epoch 1 step 501: training loss: 1089.5941488023527\n",
      "Epoch 1 step 502: training accuarcy: 0.7895\n",
      "Epoch 1 step 502: training loss: 1052.1594231715885\n",
      "Epoch 1 step 503: training accuarcy: 0.8185\n",
      "Epoch 1 step 503: training loss: 1075.1382189013566\n",
      "Epoch 1 step 504: training accuarcy: 0.8115\n",
      "Epoch 1 step 504: training loss: 1081.1035879631092\n",
      "Epoch 1 step 505: training accuarcy: 0.8035\n",
      "Epoch 1 step 505: training loss: 1087.723712135125\n",
      "Epoch 1 step 506: training accuarcy: 0.7955\n",
      "Epoch 1 step 506: training loss: 1102.0687060553264\n",
      "Epoch 1 step 507: training accuarcy: 0.7875\n",
      "Epoch 1 step 507: training loss: 1095.3994628097062\n",
      "Epoch 1 step 508: training accuarcy: 0.8065\n",
      "Epoch 1 step 508: training loss: 1102.1549782683317\n",
      "Epoch 1 step 509: training accuarcy: 0.7875\n",
      "Epoch 1 step 509: training loss: 1105.3844688425197\n",
      "Epoch 1 step 510: training accuarcy: 0.788\n",
      "Epoch 1 step 510: training loss: 1108.768714517057\n",
      "Epoch 1 step 511: training accuarcy: 0.7865\n",
      "Epoch 1 step 511: training loss: 1078.2206553536805\n",
      "Epoch 1 step 512: training accuarcy: 0.8075\n",
      "Epoch 1 step 512: training loss: 1085.8695145384922\n",
      "Epoch 1 step 513: training accuarcy: 0.793\n",
      "Epoch 1 step 513: training loss: 1079.0537295450633\n",
      "Epoch 1 step 514: training accuarcy: 0.795\n",
      "Epoch 1 step 514: training loss: 1087.9350200774536\n",
      "Epoch 1 step 515: training accuarcy: 0.791\n",
      "Epoch 1 step 515: training loss: 1059.2257435237238\n",
      "Epoch 1 step 516: training accuarcy: 0.8105\n",
      "Epoch 1 step 516: training loss: 1084.2192377138288\n",
      "Epoch 1 step 517: training accuarcy: 0.801\n",
      "Epoch 1 step 517: training loss: 1070.5051242233053\n",
      "Epoch 1 step 518: training accuarcy: 0.8195\n",
      "Epoch 1 step 518: training loss: 1135.9179649419323\n",
      "Epoch 1 step 519: training accuarcy: 0.792\n",
      "Epoch 1 step 519: training loss: 1128.490766696197\n",
      "Epoch 1 step 520: training accuarcy: 0.7795\n",
      "Epoch 1 step 520: training loss: 1042.6733306086112\n",
      "Epoch 1 step 521: training accuarcy: 0.808\n",
      "Epoch 1 step 521: training loss: 1112.2321477091696\n",
      "Epoch 1 step 522: training accuarcy: 0.801\n",
      "Epoch 1 step 522: training loss: 1096.221698273474\n",
      "Epoch 1 step 523: training accuarcy: 0.8095\n",
      "Epoch 1 step 523: training loss: 1097.0082712633246\n",
      "Epoch 1 step 524: training accuarcy: 0.793\n",
      "Epoch 1 step 524: training loss: 1120.6692701334132\n",
      "Epoch 1 step 525: training accuarcy: 0.779\n",
      "Epoch 1 step 525: training loss: 448.86326563268653\n",
      "Epoch 1 step 526: training accuarcy: 0.7935897435897435\n",
      "Epoch 1: train loss 1110.3558790838717, train accuarcy 0.7341272830963135\n",
      "Epoch 1: valid loss 1076.2211097409024, valid accuarcy 0.7942187190055847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████████████████████████████▊                                                                                                       | 2/5 [03:42<05:37, 112.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 1100.5477466753318\n",
      "Epoch 2 step 527: training accuarcy: 0.7845\n",
      "Epoch 2 step 527: training loss: 1067.7745364657915\n",
      "Epoch 2 step 528: training accuarcy: 0.8045\n",
      "Epoch 2 step 528: training loss: 1088.5497564537507\n",
      "Epoch 2 step 529: training accuarcy: 0.8005\n",
      "Epoch 2 step 529: training loss: 1080.0682961145353\n",
      "Epoch 2 step 530: training accuarcy: 0.8165\n",
      "Epoch 2 step 530: training loss: 1100.9836258816654\n",
      "Epoch 2 step 531: training accuarcy: 0.7985\n",
      "Epoch 2 step 531: training loss: 1085.9675617762662\n",
      "Epoch 2 step 532: training accuarcy: 0.8095\n",
      "Epoch 2 step 532: training loss: 1126.0007000331427\n",
      "Epoch 2 step 533: training accuarcy: 0.7805\n",
      "Epoch 2 step 533: training loss: 1109.2915892906092\n",
      "Epoch 2 step 534: training accuarcy: 0.7775\n",
      "Epoch 2 step 534: training loss: 1113.6083832316397\n",
      "Epoch 2 step 535: training accuarcy: 0.7795\n",
      "Epoch 2 step 535: training loss: 1087.0495635407558\n",
      "Epoch 2 step 536: training accuarcy: 0.794\n",
      "Epoch 2 step 536: training loss: 1070.3469977304158\n",
      "Epoch 2 step 537: training accuarcy: 0.808\n",
      "Epoch 2 step 537: training loss: 1112.7224724260038\n",
      "Epoch 2 step 538: training accuarcy: 0.785\n",
      "Epoch 2 step 538: training loss: 1097.6886019553667\n",
      "Epoch 2 step 539: training accuarcy: 0.7945\n",
      "Epoch 2 step 539: training loss: 1075.1911630515497\n",
      "Epoch 2 step 540: training accuarcy: 0.809\n",
      "Epoch 2 step 540: training loss: 1106.7323365387733\n",
      "Epoch 2 step 541: training accuarcy: 0.791\n",
      "Epoch 2 step 541: training loss: 1094.7790653428342\n",
      "Epoch 2 step 542: training accuarcy: 0.8015\n",
      "Epoch 2 step 542: training loss: 1097.2027098006952\n",
      "Epoch 2 step 543: training accuarcy: 0.786\n",
      "Epoch 2 step 543: training loss: 1085.3734662327172\n",
      "Epoch 2 step 544: training accuarcy: 0.809\n",
      "Epoch 2 step 544: training loss: 1091.3071656096135\n",
      "Epoch 2 step 545: training accuarcy: 0.8025\n",
      "Epoch 2 step 545: training loss: 1076.429746156497\n",
      "Epoch 2 step 546: training accuarcy: 0.808\n",
      "Epoch 2 step 546: training loss: 1098.8636428632626\n",
      "Epoch 2 step 547: training accuarcy: 0.796\n",
      "Epoch 2 step 547: training loss: 1124.7645499043974\n",
      "Epoch 2 step 548: training accuarcy: 0.778\n",
      "Epoch 2 step 548: training loss: 1073.657058729816\n",
      "Epoch 2 step 549: training accuarcy: 0.7885\n",
      "Epoch 2 step 549: training loss: 1078.8725645829452\n",
      "Epoch 2 step 550: training accuarcy: 0.801\n",
      "Epoch 2 step 550: training loss: 1079.6600830852742\n",
      "Epoch 2 step 551: training accuarcy: 0.796\n",
      "Epoch 2 step 551: training loss: 1077.0083519033826\n",
      "Epoch 2 step 552: training accuarcy: 0.8025\n",
      "Epoch 2 step 552: training loss: 1101.330171899004\n",
      "Epoch 2 step 553: training accuarcy: 0.8015\n",
      "Epoch 2 step 553: training loss: 1083.3878631606276\n",
      "Epoch 2 step 554: training accuarcy: 0.795\n",
      "Epoch 2 step 554: training loss: 1043.9643490197175\n",
      "Epoch 2 step 555: training accuarcy: 0.8155\n",
      "Epoch 2 step 555: training loss: 1100.595312200042\n",
      "Epoch 2 step 556: training accuarcy: 0.8055\n",
      "Epoch 2 step 556: training loss: 1090.250273603107\n",
      "Epoch 2 step 557: training accuarcy: 0.806\n",
      "Epoch 2 step 557: training loss: 1096.6846335399684\n",
      "Epoch 2 step 558: training accuarcy: 0.801\n",
      "Epoch 2 step 558: training loss: 1098.6562145560838\n",
      "Epoch 2 step 559: training accuarcy: 0.8\n",
      "Epoch 2 step 559: training loss: 1070.938720142902\n",
      "Epoch 2 step 560: training accuarcy: 0.8085\n",
      "Epoch 2 step 560: training loss: 1084.1365190576234\n",
      "Epoch 2 step 561: training accuarcy: 0.8\n",
      "Epoch 2 step 561: training loss: 1088.4173047315066\n",
      "Epoch 2 step 562: training accuarcy: 0.7915\n",
      "Epoch 2 step 562: training loss: 1064.2895436319202\n",
      "Epoch 2 step 563: training accuarcy: 0.8065\n",
      "Epoch 2 step 563: training loss: 1075.3318792063023\n",
      "Epoch 2 step 564: training accuarcy: 0.7985\n",
      "Epoch 2 step 564: training loss: 1106.7385841038924\n",
      "Epoch 2 step 565: training accuarcy: 0.787\n",
      "Epoch 2 step 565: training loss: 1071.3809756178932\n",
      "Epoch 2 step 566: training accuarcy: 0.804\n",
      "Epoch 2 step 566: training loss: 1093.585692385168\n",
      "Epoch 2 step 567: training accuarcy: 0.7935\n",
      "Epoch 2 step 567: training loss: 1111.1178295899792\n",
      "Epoch 2 step 568: training accuarcy: 0.7945\n",
      "Epoch 2 step 568: training loss: 1073.628993325946\n",
      "Epoch 2 step 569: training accuarcy: 0.798\n",
      "Epoch 2 step 569: training loss: 1090.7489413061255\n",
      "Epoch 2 step 570: training accuarcy: 0.796\n",
      "Epoch 2 step 570: training loss: 1085.5438991851506\n",
      "Epoch 2 step 571: training accuarcy: 0.8015\n",
      "Epoch 2 step 571: training loss: 1080.9890366087118\n",
      "Epoch 2 step 572: training accuarcy: 0.7945\n",
      "Epoch 2 step 572: training loss: 1127.368813848668\n",
      "Epoch 2 step 573: training accuarcy: 0.784\n",
      "Epoch 2 step 573: training loss: 1110.9907949761973\n",
      "Epoch 2 step 574: training accuarcy: 0.79\n",
      "Epoch 2 step 574: training loss: 1092.1403170531055\n",
      "Epoch 2 step 575: training accuarcy: 0.807\n",
      "Epoch 2 step 575: training loss: 1048.6982688092346\n",
      "Epoch 2 step 576: training accuarcy: 0.812\n",
      "Epoch 2 step 576: training loss: 1075.6308955663296\n",
      "Epoch 2 step 577: training accuarcy: 0.8015\n",
      "Epoch 2 step 577: training loss: 1096.8413129249557\n",
      "Epoch 2 step 578: training accuarcy: 0.777\n",
      "Epoch 2 step 578: training loss: 1081.347799848771\n",
      "Epoch 2 step 579: training accuarcy: 0.7945\n",
      "Epoch 2 step 579: training loss: 1095.8619127408501\n",
      "Epoch 2 step 580: training accuarcy: 0.7885\n",
      "Epoch 2 step 580: training loss: 1073.319404945448\n",
      "Epoch 2 step 581: training accuarcy: 0.811\n",
      "Epoch 2 step 581: training loss: 1093.253528707763\n",
      "Epoch 2 step 582: training accuarcy: 0.794\n",
      "Epoch 2 step 582: training loss: 1072.977095814355\n",
      "Epoch 2 step 583: training accuarcy: 0.8045\n",
      "Epoch 2 step 583: training loss: 1068.659554506359\n",
      "Epoch 2 step 584: training accuarcy: 0.7965\n",
      "Epoch 2 step 584: training loss: 1064.9909270415499\n",
      "Epoch 2 step 585: training accuarcy: 0.812\n",
      "Epoch 2 step 585: training loss: 1050.6563496147655\n",
      "Epoch 2 step 586: training accuarcy: 0.799\n",
      "Epoch 2 step 586: training loss: 1085.9723226785675\n",
      "Epoch 2 step 587: training accuarcy: 0.7935\n",
      "Epoch 2 step 587: training loss: 1102.6881607948164\n",
      "Epoch 2 step 588: training accuarcy: 0.807\n",
      "Epoch 2 step 588: training loss: 1099.0736577427147\n",
      "Epoch 2 step 589: training accuarcy: 0.7945\n",
      "Epoch 2 step 589: training loss: 1119.7714719701135\n",
      "Epoch 2 step 590: training accuarcy: 0.7855\n",
      "Epoch 2 step 590: training loss: 1071.8853471608938\n",
      "Epoch 2 step 591: training accuarcy: 0.8025\n",
      "Epoch 2 step 591: training loss: 1058.695137753278\n",
      "Epoch 2 step 592: training accuarcy: 0.8105\n",
      "Epoch 2 step 592: training loss: 1090.6146546088062\n",
      "Epoch 2 step 593: training accuarcy: 0.789\n",
      "Epoch 2 step 593: training loss: 1083.9755894178618\n",
      "Epoch 2 step 594: training accuarcy: 0.79\n",
      "Epoch 2 step 594: training loss: 1085.4657187519065\n",
      "Epoch 2 step 595: training accuarcy: 0.8105\n",
      "Epoch 2 step 595: training loss: 1072.6456364881417\n",
      "Epoch 2 step 596: training accuarcy: 0.793\n",
      "Epoch 2 step 596: training loss: 1091.1168520961676\n",
      "Epoch 2 step 597: training accuarcy: 0.791\n",
      "Epoch 2 step 597: training loss: 1081.5776046881867\n",
      "Epoch 2 step 598: training accuarcy: 0.789\n",
      "Epoch 2 step 598: training loss: 1083.274203884594\n",
      "Epoch 2 step 599: training accuarcy: 0.798\n",
      "Epoch 2 step 599: training loss: 1099.9880847554155\n",
      "Epoch 2 step 600: training accuarcy: 0.779\n",
      "Epoch 2 step 600: training loss: 1069.3897650542426\n",
      "Epoch 2 step 601: training accuarcy: 0.807\n",
      "Epoch 2 step 601: training loss: 1091.423261042988\n",
      "Epoch 2 step 602: training accuarcy: 0.7965\n",
      "Epoch 2 step 602: training loss: 1101.414465129352\n",
      "Epoch 2 step 603: training accuarcy: 0.801\n",
      "Epoch 2 step 603: training loss: 1109.9504475978497\n",
      "Epoch 2 step 604: training accuarcy: 0.8005\n",
      "Epoch 2 step 604: training loss: 1083.0052329263458\n",
      "Epoch 2 step 605: training accuarcy: 0.791\n",
      "Epoch 2 step 605: training loss: 1079.0672189705274\n",
      "Epoch 2 step 606: training accuarcy: 0.796\n",
      "Epoch 2 step 606: training loss: 1105.5497257586876\n",
      "Epoch 2 step 607: training accuarcy: 0.7835\n",
      "Epoch 2 step 607: training loss: 1066.2159984750697\n",
      "Epoch 2 step 608: training accuarcy: 0.8055\n",
      "Epoch 2 step 608: training loss: 1058.8100646148862\n",
      "Epoch 2 step 609: training accuarcy: 0.81\n",
      "Epoch 2 step 609: training loss: 1049.9525786408358\n",
      "Epoch 2 step 610: training accuarcy: 0.8035\n",
      "Epoch 2 step 610: training loss: 1090.259136390091\n",
      "Epoch 2 step 611: training accuarcy: 0.794\n",
      "Epoch 2 step 611: training loss: 1074.886720445282\n",
      "Epoch 2 step 612: training accuarcy: 0.8105\n",
      "Epoch 2 step 612: training loss: 1083.9167258167931\n",
      "Epoch 2 step 613: training accuarcy: 0.798\n",
      "Epoch 2 step 613: training loss: 1090.8498545572381\n",
      "Epoch 2 step 614: training accuarcy: 0.8045\n",
      "Epoch 2 step 614: training loss: 1103.8423035146034\n",
      "Epoch 2 step 615: training accuarcy: 0.8\n",
      "Epoch 2 step 615: training loss: 1085.0453815935373\n",
      "Epoch 2 step 616: training accuarcy: 0.8025\n",
      "Epoch 2 step 616: training loss: 1106.3903744330821\n",
      "Epoch 2 step 617: training accuarcy: 0.7795\n",
      "Epoch 2 step 617: training loss: 1086.2564680016233\n",
      "Epoch 2 step 618: training accuarcy: 0.7995\n",
      "Epoch 2 step 618: training loss: 1061.5928422435034\n",
      "Epoch 2 step 619: training accuarcy: 0.805\n",
      "Epoch 2 step 619: training loss: 1080.7288888635467\n",
      "Epoch 2 step 620: training accuarcy: 0.792\n",
      "Epoch 2 step 620: training loss: 1076.923531199897\n",
      "Epoch 2 step 621: training accuarcy: 0.8005\n",
      "Epoch 2 step 621: training loss: 1069.438626342401\n",
      "Epoch 2 step 622: training accuarcy: 0.794\n",
      "Epoch 2 step 622: training loss: 1080.1885781035955\n",
      "Epoch 2 step 623: training accuarcy: 0.808\n",
      "Epoch 2 step 623: training loss: 1064.5235210060002\n",
      "Epoch 2 step 624: training accuarcy: 0.8065\n",
      "Epoch 2 step 624: training loss: 1071.8794626425622\n",
      "Epoch 2 step 625: training accuarcy: 0.794\n",
      "Epoch 2 step 625: training loss: 1077.9503590553345\n",
      "Epoch 2 step 626: training accuarcy: 0.8005\n",
      "Epoch 2 step 626: training loss: 1066.7343521783619\n",
      "Epoch 2 step 627: training accuarcy: 0.8025\n",
      "Epoch 2 step 627: training loss: 1075.9447473789876\n",
      "Epoch 2 step 628: training accuarcy: 0.7945\n",
      "Epoch 2 step 628: training loss: 1089.772260115419\n",
      "Epoch 2 step 629: training accuarcy: 0.8065\n",
      "Epoch 2 step 629: training loss: 1083.2680710649424\n",
      "Epoch 2 step 630: training accuarcy: 0.789\n",
      "Epoch 2 step 630: training loss: 1071.5938479804975\n",
      "Epoch 2 step 631: training accuarcy: 0.808\n",
      "Epoch 2 step 631: training loss: 1089.5044193238825\n",
      "Epoch 2 step 632: training accuarcy: 0.785\n",
      "Epoch 2 step 632: training loss: 1084.1139216668348\n",
      "Epoch 2 step 633: training accuarcy: 0.796\n",
      "Epoch 2 step 633: training loss: 1060.851786308775\n",
      "Epoch 2 step 634: training accuarcy: 0.812\n",
      "Epoch 2 step 634: training loss: 1114.7821676819897\n",
      "Epoch 2 step 635: training accuarcy: 0.803\n",
      "Epoch 2 step 635: training loss: 1063.3812030798338\n",
      "Epoch 2 step 636: training accuarcy: 0.8065\n",
      "Epoch 2 step 636: training loss: 1053.3865724763707\n",
      "Epoch 2 step 637: training accuarcy: 0.7985\n",
      "Epoch 2 step 637: training loss: 1051.9653138995473\n",
      "Epoch 2 step 638: training accuarcy: 0.7995\n",
      "Epoch 2 step 638: training loss: 1101.5463362267722\n",
      "Epoch 2 step 639: training accuarcy: 0.7975\n",
      "Epoch 2 step 639: training loss: 1075.242953564178\n",
      "Epoch 2 step 640: training accuarcy: 0.8145\n",
      "Epoch 2 step 640: training loss: 1092.3492820957908\n",
      "Epoch 2 step 641: training accuarcy: 0.8075\n",
      "Epoch 2 step 641: training loss: 1062.0314423456068\n",
      "Epoch 2 step 642: training accuarcy: 0.8\n",
      "Epoch 2 step 642: training loss: 1070.5718986188401\n",
      "Epoch 2 step 643: training accuarcy: 0.807\n",
      "Epoch 2 step 643: training loss: 1091.8144474311048\n",
      "Epoch 2 step 644: training accuarcy: 0.789\n",
      "Epoch 2 step 644: training loss: 1060.5238344302024\n",
      "Epoch 2 step 645: training accuarcy: 0.7945\n",
      "Epoch 2 step 645: training loss: 1079.5557149595707\n",
      "Epoch 2 step 646: training accuarcy: 0.802\n",
      "Epoch 2 step 646: training loss: 1076.085144408389\n",
      "Epoch 2 step 647: training accuarcy: 0.792\n",
      "Epoch 2 step 647: training loss: 1077.2499578261873\n",
      "Epoch 2 step 648: training accuarcy: 0.7945\n",
      "Epoch 2 step 648: training loss: 1062.3912650200418\n",
      "Epoch 2 step 649: training accuarcy: 0.8005\n",
      "Epoch 2 step 649: training loss: 1040.4580067393679\n",
      "Epoch 2 step 650: training accuarcy: 0.8145\n",
      "Epoch 2 step 650: training loss: 1076.2244810999484\n",
      "Epoch 2 step 651: training accuarcy: 0.802\n",
      "Epoch 2 step 651: training loss: 1092.5891645689173\n",
      "Epoch 2 step 652: training accuarcy: 0.796\n",
      "Epoch 2 step 652: training loss: 1028.2289793779364\n",
      "Epoch 2 step 653: training accuarcy: 0.8160000000000001\n",
      "Epoch 2 step 653: training loss: 1090.6555057143992\n",
      "Epoch 2 step 654: training accuarcy: 0.786\n",
      "Epoch 2 step 654: training loss: 1095.2820783053119\n",
      "Epoch 2 step 655: training accuarcy: 0.8035\n",
      "Epoch 2 step 655: training loss: 1062.7602317695619\n",
      "Epoch 2 step 656: training accuarcy: 0.8055\n",
      "Epoch 2 step 656: training loss: 1074.718663273529\n",
      "Epoch 2 step 657: training accuarcy: 0.794\n",
      "Epoch 2 step 657: training loss: 1093.530677305169\n",
      "Epoch 2 step 658: training accuarcy: 0.7945\n",
      "Epoch 2 step 658: training loss: 1079.4363321405272\n",
      "Epoch 2 step 659: training accuarcy: 0.8\n",
      "Epoch 2 step 659: training loss: 1048.409017709612\n",
      "Epoch 2 step 660: training accuarcy: 0.8095\n",
      "Epoch 2 step 660: training loss: 1082.3550775467609\n",
      "Epoch 2 step 661: training accuarcy: 0.8025\n",
      "Epoch 2 step 661: training loss: 1086.9155877317144\n",
      "Epoch 2 step 662: training accuarcy: 0.8065\n",
      "Epoch 2 step 662: training loss: 1089.0515022636905\n",
      "Epoch 2 step 663: training accuarcy: 0.8065\n",
      "Epoch 2 step 663: training loss: 1073.9814666735247\n",
      "Epoch 2 step 664: training accuarcy: 0.7965\n",
      "Epoch 2 step 664: training loss: 1070.5068792082238\n",
      "Epoch 2 step 665: training accuarcy: 0.7995\n",
      "Epoch 2 step 665: training loss: 1077.373563683935\n",
      "Epoch 2 step 666: training accuarcy: 0.7985\n",
      "Epoch 2 step 666: training loss: 1051.70885168001\n",
      "Epoch 2 step 667: training accuarcy: 0.8055\n",
      "Epoch 2 step 667: training loss: 1069.0774380912726\n",
      "Epoch 2 step 668: training accuarcy: 0.804\n",
      "Epoch 2 step 668: training loss: 1089.375277422808\n",
      "Epoch 2 step 669: training accuarcy: 0.795\n",
      "Epoch 2 step 669: training loss: 1083.9443064296572\n",
      "Epoch 2 step 670: training accuarcy: 0.806\n",
      "Epoch 2 step 670: training loss: 1067.6462402370819\n",
      "Epoch 2 step 671: training accuarcy: 0.782\n",
      "Epoch 2 step 671: training loss: 1038.7514427820213\n",
      "Epoch 2 step 672: training accuarcy: 0.8310000000000001\n",
      "Epoch 2 step 672: training loss: 1091.2862785683994\n",
      "Epoch 2 step 673: training accuarcy: 0.7905\n",
      "Epoch 2 step 673: training loss: 1073.165985802955\n",
      "Epoch 2 step 674: training accuarcy: 0.7935\n",
      "Epoch 2 step 674: training loss: 1089.4666049811751\n",
      "Epoch 2 step 675: training accuarcy: 0.7785\n",
      "Epoch 2 step 675: training loss: 1064.8123411506408\n",
      "Epoch 2 step 676: training accuarcy: 0.807\n",
      "Epoch 2 step 676: training loss: 1059.8591840151817\n",
      "Epoch 2 step 677: training accuarcy: 0.8045\n",
      "Epoch 2 step 677: training loss: 1071.8656360576535\n",
      "Epoch 2 step 678: training accuarcy: 0.797\n",
      "Epoch 2 step 678: training loss: 1070.1643545488298\n",
      "Epoch 2 step 679: training accuarcy: 0.799\n",
      "Epoch 2 step 679: training loss: 1106.8921830805357\n",
      "Epoch 2 step 680: training accuarcy: 0.7855\n",
      "Epoch 2 step 680: training loss: 1093.928665775692\n",
      "Epoch 2 step 681: training accuarcy: 0.7915\n",
      "Epoch 2 step 681: training loss: 1065.2119233002059\n",
      "Epoch 2 step 682: training accuarcy: 0.794\n",
      "Epoch 2 step 682: training loss: 1089.972681765644\n",
      "Epoch 2 step 683: training accuarcy: 0.7935\n",
      "Epoch 2 step 683: training loss: 1050.5821322340864\n",
      "Epoch 2 step 684: training accuarcy: 0.809\n",
      "Epoch 2 step 684: training loss: 1089.1317396302672\n",
      "Epoch 2 step 685: training accuarcy: 0.7835\n",
      "Epoch 2 step 685: training loss: 1072.6148423555344\n",
      "Epoch 2 step 686: training accuarcy: 0.796\n",
      "Epoch 2 step 686: training loss: 1109.153347321147\n",
      "Epoch 2 step 687: training accuarcy: 0.7895\n",
      "Epoch 2 step 687: training loss: 1085.324250169461\n",
      "Epoch 2 step 688: training accuarcy: 0.792\n",
      "Epoch 2 step 688: training loss: 1043.7277607270455\n",
      "Epoch 2 step 689: training accuarcy: 0.8140000000000001\n",
      "Epoch 2 step 689: training loss: 1095.910063406284\n",
      "Epoch 2 step 690: training accuarcy: 0.7875\n",
      "Epoch 2 step 690: training loss: 1089.6418024915913\n",
      "Epoch 2 step 691: training accuarcy: 0.8085\n",
      "Epoch 2 step 691: training loss: 1064.069497329461\n",
      "Epoch 2 step 692: training accuarcy: 0.799\n",
      "Epoch 2 step 692: training loss: 1048.5206902376535\n",
      "Epoch 2 step 693: training accuarcy: 0.812\n",
      "Epoch 2 step 693: training loss: 1118.1233464891657\n",
      "Epoch 2 step 694: training accuarcy: 0.784\n",
      "Epoch 2 step 694: training loss: 1107.494086338417\n",
      "Epoch 2 step 695: training accuarcy: 0.7915\n",
      "Epoch 2 step 695: training loss: 1055.5690650348222\n",
      "Epoch 2 step 696: training accuarcy: 0.809\n",
      "Epoch 2 step 696: training loss: 1089.6657990974531\n",
      "Epoch 2 step 697: training accuarcy: 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 697: training loss: 1062.2989079397992\n",
      "Epoch 2 step 698: training accuarcy: 0.806\n",
      "Epoch 2 step 698: training loss: 1095.2612995913194\n",
      "Epoch 2 step 699: training accuarcy: 0.7935\n",
      "Epoch 2 step 699: training loss: 1065.0421402633988\n",
      "Epoch 2 step 700: training accuarcy: 0.806\n",
      "Epoch 2 step 700: training loss: 1039.120796714582\n",
      "Epoch 2 step 701: training accuarcy: 0.804\n",
      "Epoch 2 step 701: training loss: 1052.8962720861578\n",
      "Epoch 2 step 702: training accuarcy: 0.8135\n",
      "Epoch 2 step 702: training loss: 1053.4870635410366\n",
      "Epoch 2 step 703: training accuarcy: 0.81\n",
      "Epoch 2 step 703: training loss: 1048.8249097773485\n",
      "Epoch 2 step 704: training accuarcy: 0.802\n",
      "Epoch 2 step 704: training loss: 1060.499785374857\n",
      "Epoch 2 step 705: training accuarcy: 0.798\n",
      "Epoch 2 step 705: training loss: 1039.5388385102847\n",
      "Epoch 2 step 706: training accuarcy: 0.8045\n",
      "Epoch 2 step 706: training loss: 1079.1362478928888\n",
      "Epoch 2 step 707: training accuarcy: 0.7995\n",
      "Epoch 2 step 707: training loss: 1106.8067398898415\n",
      "Epoch 2 step 708: training accuarcy: 0.7885\n",
      "Epoch 2 step 708: training loss: 1084.2176102802175\n",
      "Epoch 2 step 709: training accuarcy: 0.7955\n",
      "Epoch 2 step 709: training loss: 1067.7785037716662\n",
      "Epoch 2 step 710: training accuarcy: 0.799\n",
      "Epoch 2 step 710: training loss: 1033.604507261147\n",
      "Epoch 2 step 711: training accuarcy: 0.8180000000000001\n",
      "Epoch 2 step 711: training loss: 1119.1870438400988\n",
      "Epoch 2 step 712: training accuarcy: 0.7755\n",
      "Epoch 2 step 712: training loss: 1123.9592947223591\n",
      "Epoch 2 step 713: training accuarcy: 0.7935\n",
      "Epoch 2 step 713: training loss: 1068.833801225673\n",
      "Epoch 2 step 714: training accuarcy: 0.7995\n",
      "Epoch 2 step 714: training loss: 1064.2164957077048\n",
      "Epoch 2 step 715: training accuarcy: 0.796\n",
      "Epoch 2 step 715: training loss: 1053.3674626724676\n",
      "Epoch 2 step 716: training accuarcy: 0.804\n",
      "Epoch 2 step 716: training loss: 1059.2411629300084\n",
      "Epoch 2 step 717: training accuarcy: 0.8115\n",
      "Epoch 2 step 717: training loss: 1087.1221993995764\n",
      "Epoch 2 step 718: training accuarcy: 0.805\n",
      "Epoch 2 step 718: training loss: 1075.7870091055868\n",
      "Epoch 2 step 719: training accuarcy: 0.8005\n",
      "Epoch 2 step 719: training loss: 1070.383264940043\n",
      "Epoch 2 step 720: training accuarcy: 0.793\n",
      "Epoch 2 step 720: training loss: 1059.1836157000107\n",
      "Epoch 2 step 721: training accuarcy: 0.8015\n",
      "Epoch 2 step 721: training loss: 1043.6783600000292\n",
      "Epoch 2 step 722: training accuarcy: 0.8175\n",
      "Epoch 2 step 722: training loss: 1054.3251071089378\n",
      "Epoch 2 step 723: training accuarcy: 0.8140000000000001\n",
      "Epoch 2 step 723: training loss: 1066.8281358693375\n",
      "Epoch 2 step 724: training accuarcy: 0.8115\n",
      "Epoch 2 step 724: training loss: 1080.028081055629\n",
      "Epoch 2 step 725: training accuarcy: 0.7935\n",
      "Epoch 2 step 725: training loss: 1070.6483578499538\n",
      "Epoch 2 step 726: training accuarcy: 0.787\n",
      "Epoch 2 step 726: training loss: 1071.3173471423581\n",
      "Epoch 2 step 727: training accuarcy: 0.8045\n",
      "Epoch 2 step 727: training loss: 1075.1601720491735\n",
      "Epoch 2 step 728: training accuarcy: 0.807\n",
      "Epoch 2 step 728: training loss: 1067.524900275714\n",
      "Epoch 2 step 729: training accuarcy: 0.7935\n",
      "Epoch 2 step 729: training loss: 1038.9034488894195\n",
      "Epoch 2 step 730: training accuarcy: 0.809\n",
      "Epoch 2 step 730: training loss: 1081.999969593359\n",
      "Epoch 2 step 731: training accuarcy: 0.802\n",
      "Epoch 2 step 731: training loss: 1076.4190553316446\n",
      "Epoch 2 step 732: training accuarcy: 0.7995\n",
      "Epoch 2 step 732: training loss: 1041.635041676582\n",
      "Epoch 2 step 733: training accuarcy: 0.7965\n",
      "Epoch 2 step 733: training loss: 1076.7572722257296\n",
      "Epoch 2 step 734: training accuarcy: 0.8015\n",
      "Epoch 2 step 734: training loss: 1039.9786884035866\n",
      "Epoch 2 step 735: training accuarcy: 0.801\n",
      "Epoch 2 step 735: training loss: 1068.5611378958288\n",
      "Epoch 2 step 736: training accuarcy: 0.798\n",
      "Epoch 2 step 736: training loss: 1058.6539683880496\n",
      "Epoch 2 step 737: training accuarcy: 0.7955\n",
      "Epoch 2 step 737: training loss: 1061.0329082904027\n",
      "Epoch 2 step 738: training accuarcy: 0.7945\n",
      "Epoch 2 step 738: training loss: 1068.131535935205\n",
      "Epoch 2 step 739: training accuarcy: 0.8045\n",
      "Epoch 2 step 739: training loss: 1060.5914795681538\n",
      "Epoch 2 step 740: training accuarcy: 0.801\n",
      "Epoch 2 step 740: training loss: 1039.5823680631402\n",
      "Epoch 2 step 741: training accuarcy: 0.8115\n",
      "Epoch 2 step 741: training loss: 1015.2883470612694\n",
      "Epoch 2 step 742: training accuarcy: 0.812\n",
      "Epoch 2 step 742: training loss: 1052.6376801800657\n",
      "Epoch 2 step 743: training accuarcy: 0.8145\n",
      "Epoch 2 step 743: training loss: 1075.054563597182\n",
      "Epoch 2 step 744: training accuarcy: 0.802\n",
      "Epoch 2 step 744: training loss: 1078.557264718514\n",
      "Epoch 2 step 745: training accuarcy: 0.8015\n",
      "Epoch 2 step 745: training loss: 1084.0630345352752\n",
      "Epoch 2 step 746: training accuarcy: 0.7955\n",
      "Epoch 2 step 746: training loss: 1062.3142030900017\n",
      "Epoch 2 step 747: training accuarcy: 0.7995\n",
      "Epoch 2 step 747: training loss: 1054.4102597587364\n",
      "Epoch 2 step 748: training accuarcy: 0.809\n",
      "Epoch 2 step 748: training loss: 1035.3491227681297\n",
      "Epoch 2 step 749: training accuarcy: 0.811\n",
      "Epoch 2 step 749: training loss: 1053.474061718843\n",
      "Epoch 2 step 750: training accuarcy: 0.8015\n",
      "Epoch 2 step 750: training loss: 1076.4940208633823\n",
      "Epoch 2 step 751: training accuarcy: 0.81\n",
      "Epoch 2 step 751: training loss: 1058.4317205835546\n",
      "Epoch 2 step 752: training accuarcy: 0.7965\n",
      "Epoch 2 step 752: training loss: 1101.9941222946422\n",
      "Epoch 2 step 753: training accuarcy: 0.789\n",
      "Epoch 2 step 753: training loss: 1045.7887153123759\n",
      "Epoch 2 step 754: training accuarcy: 0.806\n",
      "Epoch 2 step 754: training loss: 1095.0908025418835\n",
      "Epoch 2 step 755: training accuarcy: 0.782\n",
      "Epoch 2 step 755: training loss: 1048.6195143834907\n",
      "Epoch 2 step 756: training accuarcy: 0.801\n",
      "Epoch 2 step 756: training loss: 1083.1629989144155\n",
      "Epoch 2 step 757: training accuarcy: 0.792\n",
      "Epoch 2 step 757: training loss: 1049.2971617734613\n",
      "Epoch 2 step 758: training accuarcy: 0.8075\n",
      "Epoch 2 step 758: training loss: 1073.2506737493363\n",
      "Epoch 2 step 759: training accuarcy: 0.7925\n",
      "Epoch 2 step 759: training loss: 1079.6098060328604\n",
      "Epoch 2 step 760: training accuarcy: 0.7955\n",
      "Epoch 2 step 760: training loss: 1030.2056048956372\n",
      "Epoch 2 step 761: training accuarcy: 0.809\n",
      "Epoch 2 step 761: training loss: 1083.9530479758462\n",
      "Epoch 2 step 762: training accuarcy: 0.797\n",
      "Epoch 2 step 762: training loss: 1070.5514598132306\n",
      "Epoch 2 step 763: training accuarcy: 0.798\n",
      "Epoch 2 step 763: training loss: 1074.3898083555068\n",
      "Epoch 2 step 764: training accuarcy: 0.799\n",
      "Epoch 2 step 764: training loss: 1044.3238495118435\n",
      "Epoch 2 step 765: training accuarcy: 0.7905\n",
      "Epoch 2 step 765: training loss: 1059.5038726100518\n",
      "Epoch 2 step 766: training accuarcy: 0.797\n",
      "Epoch 2 step 766: training loss: 1067.3646341466188\n",
      "Epoch 2 step 767: training accuarcy: 0.805\n",
      "Epoch 2 step 767: training loss: 1059.2806232049456\n",
      "Epoch 2 step 768: training accuarcy: 0.809\n",
      "Epoch 2 step 768: training loss: 1024.133439521498\n",
      "Epoch 2 step 769: training accuarcy: 0.8190000000000001\n",
      "Epoch 2 step 769: training loss: 1109.6455144533354\n",
      "Epoch 2 step 770: training accuarcy: 0.789\n",
      "Epoch 2 step 770: training loss: 1040.3150953912948\n",
      "Epoch 2 step 771: training accuarcy: 0.8155\n",
      "Epoch 2 step 771: training loss: 1058.1767262371045\n",
      "Epoch 2 step 772: training accuarcy: 0.8085\n",
      "Epoch 2 step 772: training loss: 1051.6811612210674\n",
      "Epoch 2 step 773: training accuarcy: 0.8085\n",
      "Epoch 2 step 773: training loss: 1103.6103696421892\n",
      "Epoch 2 step 774: training accuarcy: 0.799\n",
      "Epoch 2 step 774: training loss: 1072.7594479457573\n",
      "Epoch 2 step 775: training accuarcy: 0.8185\n",
      "Epoch 2 step 775: training loss: 1022.9010811770765\n",
      "Epoch 2 step 776: training accuarcy: 0.8095\n",
      "Epoch 2 step 776: training loss: 1100.468657159247\n",
      "Epoch 2 step 777: training accuarcy: 0.7835\n",
      "Epoch 2 step 777: training loss: 1049.089276475685\n",
      "Epoch 2 step 778: training accuarcy: 0.8015\n",
      "Epoch 2 step 778: training loss: 1085.875244799482\n",
      "Epoch 2 step 779: training accuarcy: 0.788\n",
      "Epoch 2 step 779: training loss: 1121.035791780722\n",
      "Epoch 2 step 780: training accuarcy: 0.7925\n",
      "Epoch 2 step 780: training loss: 1050.1815086708602\n",
      "Epoch 2 step 781: training accuarcy: 0.8035\n",
      "Epoch 2 step 781: training loss: 1054.428253407716\n",
      "Epoch 2 step 782: training accuarcy: 0.793\n",
      "Epoch 2 step 782: training loss: 1079.1820215599441\n",
      "Epoch 2 step 783: training accuarcy: 0.787\n",
      "Epoch 2 step 783: training loss: 1101.2999568464577\n",
      "Epoch 2 step 784: training accuarcy: 0.7955\n",
      "Epoch 2 step 784: training loss: 1025.104959846994\n",
      "Epoch 2 step 785: training accuarcy: 0.8105\n",
      "Epoch 2 step 785: training loss: 1049.0850036393247\n",
      "Epoch 2 step 786: training accuarcy: 0.808\n",
      "Epoch 2 step 786: training loss: 1059.864703419138\n",
      "Epoch 2 step 787: training accuarcy: 0.806\n",
      "Epoch 2 step 787: training loss: 1078.2572851929394\n",
      "Epoch 2 step 788: training accuarcy: 0.7775\n",
      "Epoch 2 step 788: training loss: 435.6522121681433\n",
      "Epoch 2 step 789: training accuarcy: 0.7987179487179488\n",
      "Epoch 2: train loss 1073.846571830047, train accuarcy 0.7454384565353394\n",
      "Epoch 2: valid loss 1057.5251058916351, valid accuarcy 0.7956337332725525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 3/5 [05:30<03:42, 111.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 1066.2613758246357\n",
      "Epoch 3 step 790: training accuarcy: 0.801\n",
      "Epoch 3 step 790: training loss: 1074.6224072993346\n",
      "Epoch 3 step 791: training accuarcy: 0.7935\n",
      "Epoch 3 step 791: training loss: 1060.1022044013143\n",
      "Epoch 3 step 792: training accuarcy: 0.8170000000000001\n",
      "Epoch 3 step 792: training loss: 1034.7502471715661\n",
      "Epoch 3 step 793: training accuarcy: 0.8170000000000001\n",
      "Epoch 3 step 793: training loss: 1047.3258581745852\n",
      "Epoch 3 step 794: training accuarcy: 0.8170000000000001\n",
      "Epoch 3 step 794: training loss: 1072.122306892444\n",
      "Epoch 3 step 795: training accuarcy: 0.7915\n",
      "Epoch 3 step 795: training loss: 1078.9307834525773\n",
      "Epoch 3 step 796: training accuarcy: 0.799\n",
      "Epoch 3 step 796: training loss: 1103.5424686332083\n",
      "Epoch 3 step 797: training accuarcy: 0.777\n",
      "Epoch 3 step 797: training loss: 1033.3652683804855\n",
      "Epoch 3 step 798: training accuarcy: 0.8130000000000001\n",
      "Epoch 3 step 798: training loss: 1071.1659759830459\n",
      "Epoch 3 step 799: training accuarcy: 0.7935\n",
      "Epoch 3 step 799: training loss: 1065.04471022999\n",
      "Epoch 3 step 800: training accuarcy: 0.81\n",
      "Epoch 3 step 800: training loss: 1077.5004229194226\n",
      "Epoch 3 step 801: training accuarcy: 0.789\n",
      "Epoch 3 step 801: training loss: 1066.4142563242426\n",
      "Epoch 3 step 802: training accuarcy: 0.803\n",
      "Epoch 3 step 802: training loss: 1067.3727160885603\n",
      "Epoch 3 step 803: training accuarcy: 0.7935\n",
      "Epoch 3 step 803: training loss: 1061.9795741281896\n",
      "Epoch 3 step 804: training accuarcy: 0.8005\n",
      "Epoch 3 step 804: training loss: 1060.538528084923\n",
      "Epoch 3 step 805: training accuarcy: 0.797\n",
      "Epoch 3 step 805: training loss: 1079.3308668884888\n",
      "Epoch 3 step 806: training accuarcy: 0.8\n",
      "Epoch 3 step 806: training loss: 1103.1468879770632\n",
      "Epoch 3 step 807: training accuarcy: 0.7955\n",
      "Epoch 3 step 807: training loss: 1066.9707221543038\n",
      "Epoch 3 step 808: training accuarcy: 0.791\n",
      "Epoch 3 step 808: training loss: 1047.3774776176801\n",
      "Epoch 3 step 809: training accuarcy: 0.812\n",
      "Epoch 3 step 809: training loss: 1050.8431229303935\n",
      "Epoch 3 step 810: training accuarcy: 0.8095\n",
      "Epoch 3 step 810: training loss: 1047.8594712766837\n",
      "Epoch 3 step 811: training accuarcy: 0.8015\n",
      "Epoch 3 step 811: training loss: 1040.726554065141\n",
      "Epoch 3 step 812: training accuarcy: 0.809\n",
      "Epoch 3 step 812: training loss: 1051.1037360071145\n",
      "Epoch 3 step 813: training accuarcy: 0.807\n",
      "Epoch 3 step 813: training loss: 1055.0724161246449\n",
      "Epoch 3 step 814: training accuarcy: 0.8\n",
      "Epoch 3 step 814: training loss: 1068.3499208772946\n",
      "Epoch 3 step 815: training accuarcy: 0.794\n",
      "Epoch 3 step 815: training loss: 1117.3067295900705\n",
      "Epoch 3 step 816: training accuarcy: 0.7825\n",
      "Epoch 3 step 816: training loss: 1041.8585286572097\n",
      "Epoch 3 step 817: training accuarcy: 0.8055\n",
      "Epoch 3 step 817: training loss: 1058.0030868410938\n",
      "Epoch 3 step 818: training accuarcy: 0.798\n",
      "Epoch 3 step 818: training loss: 1087.1691747495843\n",
      "Epoch 3 step 819: training accuarcy: 0.804\n",
      "Epoch 3 step 819: training loss: 1056.6726785249916\n",
      "Epoch 3 step 820: training accuarcy: 0.7965\n",
      "Epoch 3 step 820: training loss: 1068.202475776064\n",
      "Epoch 3 step 821: training accuarcy: 0.796\n",
      "Epoch 3 step 821: training loss: 1059.1842087825974\n",
      "Epoch 3 step 822: training accuarcy: 0.8075\n",
      "Epoch 3 step 822: training loss: 1096.9984908162696\n",
      "Epoch 3 step 823: training accuarcy: 0.793\n",
      "Epoch 3 step 823: training loss: 1080.4341555784845\n",
      "Epoch 3 step 824: training accuarcy: 0.7975\n",
      "Epoch 3 step 824: training loss: 1068.4896815485008\n",
      "Epoch 3 step 825: training accuarcy: 0.796\n",
      "Epoch 3 step 825: training loss: 1109.3219988653311\n",
      "Epoch 3 step 826: training accuarcy: 0.789\n",
      "Epoch 3 step 826: training loss: 1009.1744934625998\n",
      "Epoch 3 step 827: training accuarcy: 0.8290000000000001\n",
      "Epoch 3 step 827: training loss: 1056.5386727700593\n",
      "Epoch 3 step 828: training accuarcy: 0.8075\n",
      "Epoch 3 step 828: training loss: 1061.0788938708897\n",
      "Epoch 3 step 829: training accuarcy: 0.7935\n",
      "Epoch 3 step 829: training loss: 1085.127352440199\n",
      "Epoch 3 step 830: training accuarcy: 0.8075\n",
      "Epoch 3 step 830: training loss: 1050.4715160148821\n",
      "Epoch 3 step 831: training accuarcy: 0.806\n",
      "Epoch 3 step 831: training loss: 1088.1597496346797\n",
      "Epoch 3 step 832: training accuarcy: 0.8035\n",
      "Epoch 3 step 832: training loss: 1084.596217950594\n",
      "Epoch 3 step 833: training accuarcy: 0.7875\n",
      "Epoch 3 step 833: training loss: 1067.182869073566\n",
      "Epoch 3 step 834: training accuarcy: 0.81\n",
      "Epoch 3 step 834: training loss: 1087.309830422057\n",
      "Epoch 3 step 835: training accuarcy: 0.7925\n",
      "Epoch 3 step 835: training loss: 1045.7836239652968\n",
      "Epoch 3 step 836: training accuarcy: 0.8135\n",
      "Epoch 3 step 836: training loss: 1066.0427048771958\n",
      "Epoch 3 step 837: training accuarcy: 0.8\n",
      "Epoch 3 step 837: training loss: 1054.6332766214198\n",
      "Epoch 3 step 838: training accuarcy: 0.808\n",
      "Epoch 3 step 838: training loss: 1051.0746100942492\n",
      "Epoch 3 step 839: training accuarcy: 0.8\n",
      "Epoch 3 step 839: training loss: 1047.151823952668\n",
      "Epoch 3 step 840: training accuarcy: 0.804\n",
      "Epoch 3 step 840: training loss: 1047.0764872527075\n",
      "Epoch 3 step 841: training accuarcy: 0.808\n",
      "Epoch 3 step 841: training loss: 1064.729481401846\n",
      "Epoch 3 step 842: training accuarcy: 0.8005\n",
      "Epoch 3 step 842: training loss: 1036.8119027560115\n",
      "Epoch 3 step 843: training accuarcy: 0.808\n",
      "Epoch 3 step 843: training loss: 1091.323135727638\n",
      "Epoch 3 step 844: training accuarcy: 0.789\n",
      "Epoch 3 step 844: training loss: 1056.5791351818284\n",
      "Epoch 3 step 845: training accuarcy: 0.8055\n",
      "Epoch 3 step 845: training loss: 1060.668044098511\n",
      "Epoch 3 step 846: training accuarcy: 0.794\n",
      "Epoch 3 step 846: training loss: 1081.4752158269546\n",
      "Epoch 3 step 847: training accuarcy: 0.801\n",
      "Epoch 3 step 847: training loss: 1057.923499039899\n",
      "Epoch 3 step 848: training accuarcy: 0.8\n",
      "Epoch 3 step 848: training loss: 1062.03350311061\n",
      "Epoch 3 step 849: training accuarcy: 0.8045\n",
      "Epoch 3 step 849: training loss: 1028.9877872468219\n",
      "Epoch 3 step 850: training accuarcy: 0.8085\n",
      "Epoch 3 step 850: training loss: 1062.5472516986265\n",
      "Epoch 3 step 851: training accuarcy: 0.8015\n",
      "Epoch 3 step 851: training loss: 1050.4178858806595\n",
      "Epoch 3 step 852: training accuarcy: 0.794\n",
      "Epoch 3 step 852: training loss: 1061.7156097729899\n",
      "Epoch 3 step 853: training accuarcy: 0.8115\n",
      "Epoch 3 step 853: training loss: 1085.2504306777641\n",
      "Epoch 3 step 854: training accuarcy: 0.8015\n",
      "Epoch 3 step 854: training loss: 1037.036953299651\n",
      "Epoch 3 step 855: training accuarcy: 0.804\n",
      "Epoch 3 step 855: training loss: 1085.6869476541624\n",
      "Epoch 3 step 856: training accuarcy: 0.7935\n",
      "Epoch 3 step 856: training loss: 1061.6766004193528\n",
      "Epoch 3 step 857: training accuarcy: 0.807\n",
      "Epoch 3 step 857: training loss: 1054.1466536227413\n",
      "Epoch 3 step 858: training accuarcy: 0.804\n",
      "Epoch 3 step 858: training loss: 1079.0413304169488\n",
      "Epoch 3 step 859: training accuarcy: 0.7975\n",
      "Epoch 3 step 859: training loss: 1076.3479876355236\n",
      "Epoch 3 step 860: training accuarcy: 0.803\n",
      "Epoch 3 step 860: training loss: 1058.1433563111661\n",
      "Epoch 3 step 861: training accuarcy: 0.7985\n",
      "Epoch 3 step 861: training loss: 1032.7251246357664\n",
      "Epoch 3 step 862: training accuarcy: 0.8055\n",
      "Epoch 3 step 862: training loss: 1070.931824348576\n",
      "Epoch 3 step 863: training accuarcy: 0.809\n",
      "Epoch 3 step 863: training loss: 1079.4942546267748\n",
      "Epoch 3 step 864: training accuarcy: 0.792\n",
      "Epoch 3 step 864: training loss: 1049.618965252328\n",
      "Epoch 3 step 865: training accuarcy: 0.807\n",
      "Epoch 3 step 865: training loss: 1061.608773027338\n",
      "Epoch 3 step 866: training accuarcy: 0.7995\n",
      "Epoch 3 step 866: training loss: 1072.0129011666118\n",
      "Epoch 3 step 867: training accuarcy: 0.798\n",
      "Epoch 3 step 867: training loss: 1055.310218219713\n",
      "Epoch 3 step 868: training accuarcy: 0.7975\n",
      "Epoch 3 step 868: training loss: 1064.5947077156093\n",
      "Epoch 3 step 869: training accuarcy: 0.806\n",
      "Epoch 3 step 869: training loss: 1026.2482330798275\n",
      "Epoch 3 step 870: training accuarcy: 0.8155\n",
      "Epoch 3 step 870: training loss: 1055.4282443765546\n",
      "Epoch 3 step 871: training accuarcy: 0.808\n",
      "Epoch 3 step 871: training loss: 1066.6565162850318\n",
      "Epoch 3 step 872: training accuarcy: 0.8075\n",
      "Epoch 3 step 872: training loss: 1074.0203576996428\n",
      "Epoch 3 step 873: training accuarcy: 0.8035\n",
      "Epoch 3 step 873: training loss: 1040.3344549585004\n",
      "Epoch 3 step 874: training accuarcy: 0.808\n",
      "Epoch 3 step 874: training loss: 1052.2474378795196\n",
      "Epoch 3 step 875: training accuarcy: 0.7975\n",
      "Epoch 3 step 875: training loss: 1065.814488061769\n",
      "Epoch 3 step 876: training accuarcy: 0.8095\n",
      "Epoch 3 step 876: training loss: 1051.848574208058\n",
      "Epoch 3 step 877: training accuarcy: 0.801\n",
      "Epoch 3 step 877: training loss: 1070.4260811904799\n",
      "Epoch 3 step 878: training accuarcy: 0.799\n",
      "Epoch 3 step 878: training loss: 1048.0687076842098\n",
      "Epoch 3 step 879: training accuarcy: 0.8035\n",
      "Epoch 3 step 879: training loss: 1063.63440397915\n",
      "Epoch 3 step 880: training accuarcy: 0.792\n",
      "Epoch 3 step 880: training loss: 1069.19973382994\n",
      "Epoch 3 step 881: training accuarcy: 0.792\n",
      "Epoch 3 step 881: training loss: 1053.9252898755922\n",
      "Epoch 3 step 882: training accuarcy: 0.8025\n",
      "Epoch 3 step 882: training loss: 1103.2747113821374\n",
      "Epoch 3 step 883: training accuarcy: 0.8\n",
      "Epoch 3 step 883: training loss: 1056.5325627847321\n",
      "Epoch 3 step 884: training accuarcy: 0.802\n",
      "Epoch 3 step 884: training loss: 1047.6333806310915\n",
      "Epoch 3 step 885: training accuarcy: 0.791\n",
      "Epoch 3 step 885: training loss: 1061.8881660341365\n",
      "Epoch 3 step 886: training accuarcy: 0.798\n",
      "Epoch 3 step 886: training loss: 1071.2967278107208\n",
      "Epoch 3 step 887: training accuarcy: 0.8165\n",
      "Epoch 3 step 887: training loss: 1063.5829812224342\n",
      "Epoch 3 step 888: training accuarcy: 0.802\n",
      "Epoch 3 step 888: training loss: 1076.4580395738435\n",
      "Epoch 3 step 889: training accuarcy: 0.7985\n",
      "Epoch 3 step 889: training loss: 1071.3313673682871\n",
      "Epoch 3 step 890: training accuarcy: 0.785\n",
      "Epoch 3 step 890: training loss: 1055.6917727487396\n",
      "Epoch 3 step 891: training accuarcy: 0.8225\n",
      "Epoch 3 step 891: training loss: 1056.677585201219\n",
      "Epoch 3 step 892: training accuarcy: 0.81\n",
      "Epoch 3 step 892: training loss: 1081.747657598054\n",
      "Epoch 3 step 893: training accuarcy: 0.799\n",
      "Epoch 3 step 893: training loss: 1072.4760002616931\n",
      "Epoch 3 step 894: training accuarcy: 0.7935\n",
      "Epoch 3 step 894: training loss: 1052.798644660185\n",
      "Epoch 3 step 895: training accuarcy: 0.7885\n",
      "Epoch 3 step 895: training loss: 1086.9270963343156\n",
      "Epoch 3 step 896: training accuarcy: 0.793\n",
      "Epoch 3 step 896: training loss: 1037.7690260317952\n",
      "Epoch 3 step 897: training accuarcy: 0.805\n",
      "Epoch 3 step 897: training loss: 1049.1069372383058\n",
      "Epoch 3 step 898: training accuarcy: 0.811\n",
      "Epoch 3 step 898: training loss: 1032.8397462747255\n",
      "Epoch 3 step 899: training accuarcy: 0.809\n",
      "Epoch 3 step 899: training loss: 1097.8912453006942\n",
      "Epoch 3 step 900: training accuarcy: 0.7875\n",
      "Epoch 3 step 900: training loss: 1104.543733504366\n",
      "Epoch 3 step 901: training accuarcy: 0.795\n",
      "Epoch 3 step 901: training loss: 1004.1788337943867\n",
      "Epoch 3 step 902: training accuarcy: 0.8260000000000001\n",
      "Epoch 3 step 902: training loss: 1021.3425798835226\n",
      "Epoch 3 step 903: training accuarcy: 0.81\n",
      "Epoch 3 step 903: training loss: 1074.443053943584\n",
      "Epoch 3 step 904: training accuarcy: 0.801\n",
      "Epoch 3 step 904: training loss: 1081.4151643269174\n",
      "Epoch 3 step 905: training accuarcy: 0.7935\n",
      "Epoch 3 step 905: training loss: 1084.3263838777427\n",
      "Epoch 3 step 906: training accuarcy: 0.7945\n",
      "Epoch 3 step 906: training loss: 1036.7033105868425\n",
      "Epoch 3 step 907: training accuarcy: 0.8035\n",
      "Epoch 3 step 907: training loss: 1073.81253204646\n",
      "Epoch 3 step 908: training accuarcy: 0.7945\n",
      "Epoch 3 step 908: training loss: 1071.0352683842507\n",
      "Epoch 3 step 909: training accuarcy: 0.8130000000000001\n",
      "Epoch 3 step 909: training loss: 1031.8899095193528\n",
      "Epoch 3 step 910: training accuarcy: 0.8125\n",
      "Epoch 3 step 910: training loss: 1067.8897300947915\n",
      "Epoch 3 step 911: training accuarcy: 0.7945\n",
      "Epoch 3 step 911: training loss: 1086.224874502567\n",
      "Epoch 3 step 912: training accuarcy: 0.8125\n",
      "Epoch 3 step 912: training loss: 1053.1584074595276\n",
      "Epoch 3 step 913: training accuarcy: 0.806\n",
      "Epoch 3 step 913: training loss: 1054.0653085121075\n",
      "Epoch 3 step 914: training accuarcy: 0.8005\n",
      "Epoch 3 step 914: training loss: 1083.489187299263\n",
      "Epoch 3 step 915: training accuarcy: 0.7925\n",
      "Epoch 3 step 915: training loss: 1028.836675901044\n",
      "Epoch 3 step 916: training accuarcy: 0.805\n",
      "Epoch 3 step 916: training loss: 1067.4702127287285\n",
      "Epoch 3 step 917: training accuarcy: 0.7985\n",
      "Epoch 3 step 917: training loss: 1033.2049339619196\n",
      "Epoch 3 step 918: training accuarcy: 0.801\n",
      "Epoch 3 step 918: training loss: 1044.6416670929605\n",
      "Epoch 3 step 919: training accuarcy: 0.807\n",
      "Epoch 3 step 919: training loss: 1067.8544238841876\n",
      "Epoch 3 step 920: training accuarcy: 0.797\n",
      "Epoch 3 step 920: training loss: 1080.7214291871078\n",
      "Epoch 3 step 921: training accuarcy: 0.7945\n",
      "Epoch 3 step 921: training loss: 1029.722644241454\n",
      "Epoch 3 step 922: training accuarcy: 0.806\n",
      "Epoch 3 step 922: training loss: 1041.7729952647333\n",
      "Epoch 3 step 923: training accuarcy: 0.806\n",
      "Epoch 3 step 923: training loss: 1044.1604165143335\n",
      "Epoch 3 step 924: training accuarcy: 0.7985\n",
      "Epoch 3 step 924: training loss: 1084.6007066202703\n",
      "Epoch 3 step 925: training accuarcy: 0.805\n",
      "Epoch 3 step 925: training loss: 1047.9390520000675\n",
      "Epoch 3 step 926: training accuarcy: 0.8075\n",
      "Epoch 3 step 926: training loss: 1088.8996879303645\n",
      "Epoch 3 step 927: training accuarcy: 0.783\n",
      "Epoch 3 step 927: training loss: 1043.6229593980188\n",
      "Epoch 3 step 928: training accuarcy: 0.8105\n",
      "Epoch 3 step 928: training loss: 1075.368349652582\n",
      "Epoch 3 step 929: training accuarcy: 0.8015\n",
      "Epoch 3 step 929: training loss: 1019.7277102997054\n",
      "Epoch 3 step 930: training accuarcy: 0.811\n",
      "Epoch 3 step 930: training loss: 1055.6940444371592\n",
      "Epoch 3 step 931: training accuarcy: 0.8170000000000001\n",
      "Epoch 3 step 931: training loss: 1052.1724508777838\n",
      "Epoch 3 step 932: training accuarcy: 0.801\n",
      "Epoch 3 step 932: training loss: 1034.35608997181\n",
      "Epoch 3 step 933: training accuarcy: 0.8140000000000001\n",
      "Epoch 3 step 933: training loss: 1080.5274252355057\n",
      "Epoch 3 step 934: training accuarcy: 0.8075\n",
      "Epoch 3 step 934: training loss: 1082.4975597772805\n",
      "Epoch 3 step 935: training accuarcy: 0.807\n",
      "Epoch 3 step 935: training loss: 1043.1587401795655\n",
      "Epoch 3 step 936: training accuarcy: 0.7975\n",
      "Epoch 3 step 936: training loss: 1068.2827645320117\n",
      "Epoch 3 step 937: training accuarcy: 0.796\n",
      "Epoch 3 step 937: training loss: 1088.3168710685434\n",
      "Epoch 3 step 938: training accuarcy: 0.8180000000000001\n",
      "Epoch 3 step 938: training loss: 1084.560282451757\n",
      "Epoch 3 step 939: training accuarcy: 0.7905\n",
      "Epoch 3 step 939: training loss: 1076.8499492922083\n",
      "Epoch 3 step 940: training accuarcy: 0.787\n",
      "Epoch 3 step 940: training loss: 1061.8615157132413\n",
      "Epoch 3 step 941: training accuarcy: 0.805\n",
      "Epoch 3 step 941: training loss: 1044.6042009108196\n",
      "Epoch 3 step 942: training accuarcy: 0.8165\n",
      "Epoch 3 step 942: training loss: 1053.6922372431275\n",
      "Epoch 3 step 943: training accuarcy: 0.8065\n",
      "Epoch 3 step 943: training loss: 1074.2297429397336\n",
      "Epoch 3 step 944: training accuarcy: 0.808\n",
      "Epoch 3 step 944: training loss: 1052.8719818044963\n",
      "Epoch 3 step 945: training accuarcy: 0.8055\n",
      "Epoch 3 step 945: training loss: 1053.6086093943297\n",
      "Epoch 3 step 946: training accuarcy: 0.8115\n",
      "Epoch 3 step 946: training loss: 1018.9007658753005\n",
      "Epoch 3 step 947: training accuarcy: 0.8195\n",
      "Epoch 3 step 947: training loss: 1038.5943462508842\n",
      "Epoch 3 step 948: training accuarcy: 0.7975\n",
      "Epoch 3 step 948: training loss: 1018.4371811212377\n",
      "Epoch 3 step 949: training accuarcy: 0.8175\n",
      "Epoch 3 step 949: training loss: 1075.761825179015\n",
      "Epoch 3 step 950: training accuarcy: 0.789\n",
      "Epoch 3 step 950: training loss: 1056.2472401103314\n",
      "Epoch 3 step 951: training accuarcy: 0.7935\n",
      "Epoch 3 step 951: training loss: 1049.6452198470959\n",
      "Epoch 3 step 952: training accuarcy: 0.8150000000000001\n",
      "Epoch 3 step 952: training loss: 1036.2353241107853\n",
      "Epoch 3 step 953: training accuarcy: 0.805\n",
      "Epoch 3 step 953: training loss: 1051.2982318362644\n",
      "Epoch 3 step 954: training accuarcy: 0.8095\n",
      "Epoch 3 step 954: training loss: 1071.6324997368238\n",
      "Epoch 3 step 955: training accuarcy: 0.8\n",
      "Epoch 3 step 955: training loss: 1049.9518581786074\n",
      "Epoch 3 step 956: training accuarcy: 0.7955\n",
      "Epoch 3 step 956: training loss: 1062.405558348081\n",
      "Epoch 3 step 957: training accuarcy: 0.794\n",
      "Epoch 3 step 957: training loss: 1056.7487080865722\n",
      "Epoch 3 step 958: training accuarcy: 0.7865\n",
      "Epoch 3 step 958: training loss: 1070.8658270598303\n",
      "Epoch 3 step 959: training accuarcy: 0.8025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 959: training loss: 1054.0427978074881\n",
      "Epoch 3 step 960: training accuarcy: 0.795\n",
      "Epoch 3 step 960: training loss: 1021.0338259692319\n",
      "Epoch 3 step 961: training accuarcy: 0.799\n",
      "Epoch 3 step 961: training loss: 1046.5826495096674\n",
      "Epoch 3 step 962: training accuarcy: 0.811\n",
      "Epoch 3 step 962: training loss: 1054.2052319673983\n",
      "Epoch 3 step 963: training accuarcy: 0.8210000000000001\n",
      "Epoch 3 step 963: training loss: 1122.0936364683673\n",
      "Epoch 3 step 964: training accuarcy: 0.788\n",
      "Epoch 3 step 964: training loss: 1052.0740740937154\n",
      "Epoch 3 step 965: training accuarcy: 0.8055\n",
      "Epoch 3 step 965: training loss: 1050.083900173126\n",
      "Epoch 3 step 966: training accuarcy: 0.7985\n",
      "Epoch 3 step 966: training loss: 1076.7226337107818\n",
      "Epoch 3 step 967: training accuarcy: 0.794\n",
      "Epoch 3 step 967: training loss: 1053.1820373913324\n",
      "Epoch 3 step 968: training accuarcy: 0.8075\n",
      "Epoch 3 step 968: training loss: 1073.2273617563637\n",
      "Epoch 3 step 969: training accuarcy: 0.792\n",
      "Epoch 3 step 969: training loss: 1029.252562554182\n",
      "Epoch 3 step 970: training accuarcy: 0.8045\n",
      "Epoch 3 step 970: training loss: 1050.2244156956763\n",
      "Epoch 3 step 971: training accuarcy: 0.8055\n",
      "Epoch 3 step 971: training loss: 1033.0311308922176\n",
      "Epoch 3 step 972: training accuarcy: 0.8095\n",
      "Epoch 3 step 972: training loss: 1064.848208606727\n",
      "Epoch 3 step 973: training accuarcy: 0.798\n",
      "Epoch 3 step 973: training loss: 1115.99202001998\n",
      "Epoch 3 step 974: training accuarcy: 0.7925\n",
      "Epoch 3 step 974: training loss: 1074.4742999121784\n",
      "Epoch 3 step 975: training accuarcy: 0.79\n",
      "Epoch 3 step 975: training loss: 1042.1407860629188\n",
      "Epoch 3 step 976: training accuarcy: 0.8105\n",
      "Epoch 3 step 976: training loss: 1048.3343989555815\n",
      "Epoch 3 step 977: training accuarcy: 0.807\n",
      "Epoch 3 step 977: training loss: 1035.2271544071225\n",
      "Epoch 3 step 978: training accuarcy: 0.8160000000000001\n",
      "Epoch 3 step 978: training loss: 1039.1174018741851\n",
      "Epoch 3 step 979: training accuarcy: 0.8150000000000001\n",
      "Epoch 3 step 979: training loss: 1073.2672594064882\n",
      "Epoch 3 step 980: training accuarcy: 0.8055\n",
      "Epoch 3 step 980: training loss: 1048.0422203095168\n",
      "Epoch 3 step 981: training accuarcy: 0.808\n",
      "Epoch 3 step 981: training loss: 1099.3187629623042\n",
      "Epoch 3 step 982: training accuarcy: 0.791\n",
      "Epoch 3 step 982: training loss: 1011.707060530947\n",
      "Epoch 3 step 983: training accuarcy: 0.8180000000000001\n",
      "Epoch 3 step 983: training loss: 1029.5852135486518\n",
      "Epoch 3 step 984: training accuarcy: 0.8200000000000001\n",
      "Epoch 3 step 984: training loss: 1055.6072817008053\n",
      "Epoch 3 step 985: training accuarcy: 0.8095\n",
      "Epoch 3 step 985: training loss: 1057.6928059665202\n",
      "Epoch 3 step 986: training accuarcy: 0.7985\n",
      "Epoch 3 step 986: training loss: 1052.1746662287787\n",
      "Epoch 3 step 987: training accuarcy: 0.801\n",
      "Epoch 3 step 987: training loss: 1027.6072313301365\n",
      "Epoch 3 step 988: training accuarcy: 0.8175\n",
      "Epoch 3 step 988: training loss: 1092.183129884795\n",
      "Epoch 3 step 989: training accuarcy: 0.797\n",
      "Epoch 3 step 989: training loss: 1066.4444650869339\n",
      "Epoch 3 step 990: training accuarcy: 0.802\n",
      "Epoch 3 step 990: training loss: 1048.2279268253326\n",
      "Epoch 3 step 991: training accuarcy: 0.8\n",
      "Epoch 3 step 991: training loss: 1070.557428261875\n",
      "Epoch 3 step 992: training accuarcy: 0.806\n",
      "Epoch 3 step 992: training loss: 1090.3405896606514\n",
      "Epoch 3 step 993: training accuarcy: 0.7995\n",
      "Epoch 3 step 993: training loss: 1038.237877268702\n",
      "Epoch 3 step 994: training accuarcy: 0.8130000000000001\n",
      "Epoch 3 step 994: training loss: 1084.8145626952175\n",
      "Epoch 3 step 995: training accuarcy: 0.799\n",
      "Epoch 3 step 995: training loss: 1079.9358088668423\n",
      "Epoch 3 step 996: training accuarcy: 0.805\n",
      "Epoch 3 step 996: training loss: 1042.1988400282687\n",
      "Epoch 3 step 997: training accuarcy: 0.803\n",
      "Epoch 3 step 997: training loss: 1047.4459394176229\n",
      "Epoch 3 step 998: training accuarcy: 0.8065\n",
      "Epoch 3 step 998: training loss: 1071.4848668798866\n",
      "Epoch 3 step 999: training accuarcy: 0.797\n",
      "Epoch 3 step 999: training loss: 1046.8830568672104\n",
      "Epoch 3 step 1000: training accuarcy: 0.8140000000000001\n",
      "Epoch 3 step 1000: training loss: 1034.178221714261\n",
      "Epoch 3 step 1001: training accuarcy: 0.8160000000000001\n",
      "Epoch 3 step 1001: training loss: 1020.2758215137685\n",
      "Epoch 3 step 1002: training accuarcy: 0.8150000000000001\n",
      "Epoch 3 step 1002: training loss: 1092.644390050576\n",
      "Epoch 3 step 1003: training accuarcy: 0.7875\n",
      "Epoch 3 step 1003: training loss: 1034.9869850455862\n",
      "Epoch 3 step 1004: training accuarcy: 0.8160000000000001\n",
      "Epoch 3 step 1004: training loss: 1024.0902653137014\n",
      "Epoch 3 step 1005: training accuarcy: 0.8065\n",
      "Epoch 3 step 1005: training loss: 1077.4829953478097\n",
      "Epoch 3 step 1006: training accuarcy: 0.797\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e40b5332b155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m hrm_learner.fit(epoch=5,\n\u001b[1;32m----> 2\u001b[1;33m                 log_dir=get_log_dir('topcoder', 'hrm'))\n\u001b[0m",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, epoch, log_dir)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mschedular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muser_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricky\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricky\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricky\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\datasets\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricky\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1426\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricky\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2159\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2161\u001b[0m     \u001b[1;31m# raise_missing is included for compat with the parent class signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricky\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_loc\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricky\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   2913\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2915\u001b[1;33m             \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2916\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2917\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricky\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mfast_xs\u001b[1;34m(self, loc)\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m             \u001b[1;31m# Such assignment may incorrectly coerce NaT to None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=3,\n",
    "                log_dir=get_log_dir('topcoder', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:39:42.508036Z",
     "start_time": "2019-10-09T12:24:27.256918Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 193365.16163974453\n",
      "Epoch 0 step 1: training accuarcy: 0.51\n",
      "Epoch 0 step 1: training loss: 185119.86205282228\n",
      "Epoch 0 step 2: training accuarcy: 0.5207\n",
      "Epoch 0 step 2: training loss: 189420.40994518064\n",
      "Epoch 0 step 3: training accuarcy: 0.5195000000000001\n",
      "Epoch 0 step 3: training loss: 179850.18585890112\n",
      "Epoch 0 step 4: training accuarcy: 0.5253\n",
      "Epoch 0 step 4: training loss: 176592.3830378908\n",
      "Epoch 0 step 5: training accuarcy: 0.5118\n",
      "Epoch 0 step 5: training loss: 171351.80347387\n",
      "Epoch 0 step 6: training accuarcy: 0.5213\n",
      "Epoch 0 step 6: training loss: 164100.8087699164\n",
      "Epoch 0 step 7: training accuarcy: 0.5284\n",
      "Epoch 0 step 7: training loss: 161553.85251411583\n",
      "Epoch 0 step 8: training accuarcy: 0.5347000000000001\n",
      "Epoch 0 step 8: training loss: 162316.5572234422\n",
      "Epoch 0 step 9: training accuarcy: 0.5323\n",
      "Epoch 0 step 9: training loss: 153082.98121592737\n",
      "Epoch 0 step 10: training accuarcy: 0.522\n",
      "Epoch 0 step 10: training loss: 150206.3848270157\n",
      "Epoch 0 step 11: training accuarcy: 0.5258\n",
      "Epoch 0 step 11: training loss: 144639.407001255\n",
      "Epoch 0 step 12: training accuarcy: 0.5371\n",
      "Epoch 0 step 12: training loss: 145414.24965864274\n",
      "Epoch 0 step 13: training accuarcy: 0.5306000000000001\n",
      "Epoch 0 step 13: training loss: 141295.10355182062\n",
      "Epoch 0 step 14: training accuarcy: 0.527\n",
      "Epoch 0 step 14: training loss: 135046.33918794277\n",
      "Epoch 0 step 15: training accuarcy: 0.5378000000000001\n",
      "Epoch 0 step 15: training loss: 134339.02288177123\n",
      "Epoch 0 step 16: training accuarcy: 0.5233\n",
      "Epoch 0 step 16: training loss: 131281.90260255878\n",
      "Epoch 0 step 17: training accuarcy: 0.5215000000000001\n",
      "Epoch 0 step 17: training loss: 127101.5524351792\n",
      "Epoch 0 step 18: training accuarcy: 0.5173\n",
      "Epoch 0 step 18: training loss: 126965.93325599973\n",
      "Epoch 0 step 19: training accuarcy: 0.5211\n",
      "Epoch 0 step 19: training loss: 119878.51462630939\n",
      "Epoch 0 step 20: training accuarcy: 0.5352\n",
      "Epoch 0 step 20: training loss: 119982.03796053487\n",
      "Epoch 0 step 21: training accuarcy: 0.5158\n",
      "Epoch 0 step 21: training loss: 114535.9586199146\n",
      "Epoch 0 step 22: training accuarcy: 0.5259\n",
      "Epoch 0 step 22: training loss: 111771.03850666934\n",
      "Epoch 0 step 23: training accuarcy: 0.5242\n",
      "Epoch 0 step 23: training loss: 111137.40762711813\n",
      "Epoch 0 step 24: training accuarcy: 0.527\n",
      "Epoch 0 step 24: training loss: 107410.92474167107\n",
      "Epoch 0 step 25: training accuarcy: 0.5292\n",
      "Epoch 0 step 25: training loss: 103893.81330921061\n",
      "Epoch 0 step 26: training accuarcy: 0.5234\n",
      "Epoch 0 step 26: training loss: 99039.4704165088\n",
      "Epoch 0 step 27: training accuarcy: 0.547\n",
      "Epoch 0 step 27: training loss: 97368.56665713676\n",
      "Epoch 0 step 28: training accuarcy: 0.5392\n",
      "Epoch 0 step 28: training loss: 97689.92931006211\n",
      "Epoch 0 step 29: training accuarcy: 0.5216000000000001\n",
      "Epoch 0 step 29: training loss: 92562.97555925263\n",
      "Epoch 0 step 30: training accuarcy: 0.5386000000000001\n",
      "Epoch 0 step 30: training loss: 91093.86903852389\n",
      "Epoch 0 step 31: training accuarcy: 0.5352\n",
      "Epoch 0 step 31: training loss: 87765.49222048059\n",
      "Epoch 0 step 32: training accuarcy: 0.5469\n",
      "Epoch 0 step 32: training loss: 89542.59140375617\n",
      "Epoch 0 step 33: training accuarcy: 0.5313\n",
      "Epoch 0 step 33: training loss: 86627.06900394175\n",
      "Epoch 0 step 34: training accuarcy: 0.5325\n",
      "Epoch 0 step 34: training loss: 82551.08588498089\n",
      "Epoch 0 step 35: training accuarcy: 0.5324\n",
      "Epoch 0 step 35: training loss: 79838.5569073899\n",
      "Epoch 0 step 36: training accuarcy: 0.5395\n",
      "Epoch 0 step 36: training loss: 80650.7825718748\n",
      "Epoch 0 step 37: training accuarcy: 0.5424\n",
      "Epoch 0 step 37: training loss: 76755.96957680708\n",
      "Epoch 0 step 38: training accuarcy: 0.5308\n",
      "Epoch 0 step 38: training loss: 74575.61042324468\n",
      "Epoch 0 step 39: training accuarcy: 0.5419\n",
      "Epoch 0 step 39: training loss: 73003.01111989663\n",
      "Epoch 0 step 40: training accuarcy: 0.549\n",
      "Epoch 0 step 40: training loss: 72182.6130454413\n",
      "Epoch 0 step 41: training accuarcy: 0.5475\n",
      "Epoch 0 step 41: training loss: 71740.61104428832\n",
      "Epoch 0 step 42: training accuarcy: 0.5361\n",
      "Epoch 0 step 42: training loss: 69055.97206726775\n",
      "Epoch 0 step 43: training accuarcy: 0.5478000000000001\n",
      "Epoch 0 step 43: training loss: 71443.60865343726\n",
      "Epoch 0 step 44: training accuarcy: 0.5313\n",
      "Epoch 0 step 44: training loss: 67496.54850426059\n",
      "Epoch 0 step 45: training accuarcy: 0.5394\n",
      "Epoch 0 step 45: training loss: 66884.09848947471\n",
      "Epoch 0 step 46: training accuarcy: 0.5416000000000001\n",
      "Epoch 0 step 46: training loss: 65354.551555240425\n",
      "Epoch 0 step 47: training accuarcy: 0.547\n",
      "Epoch 0 step 47: training loss: 63372.73146955027\n",
      "Epoch 0 step 48: training accuarcy: 0.5575\n",
      "Epoch 0 step 48: training loss: 62309.72307750742\n",
      "Epoch 0 step 49: training accuarcy: 0.556\n",
      "Epoch 0 step 49: training loss: 62484.846466148396\n",
      "Epoch 0 step 50: training accuarcy: 0.5453\n",
      "Epoch 0 step 50: training loss: 62004.870543039775\n",
      "Epoch 0 step 51: training accuarcy: 0.5468000000000001\n",
      "Epoch 0 step 51: training loss: 60214.28097059796\n",
      "Epoch 0 step 52: training accuarcy: 0.5491\n",
      "Epoch 0 step 52: training loss: 59073.11136021465\n",
      "Epoch 0 step 53: training accuarcy: 0.5472\n",
      "Epoch 0 step 53: training loss: 57907.096257366095\n",
      "Epoch 0 step 54: training accuarcy: 0.5667\n",
      "Epoch 0 step 54: training loss: 57446.971101294985\n",
      "Epoch 0 step 55: training accuarcy: 0.5592\n",
      "Epoch 0 step 55: training loss: 57187.69814665722\n",
      "Epoch 0 step 56: training accuarcy: 0.5515\n",
      "Epoch 0 step 56: training loss: 55681.3639589378\n",
      "Epoch 0 step 57: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 57: training loss: 55098.648536720626\n",
      "Epoch 0 step 58: training accuarcy: 0.5528000000000001\n",
      "Epoch 0 step 58: training loss: 55576.07604314837\n",
      "Epoch 0 step 59: training accuarcy: 0.5503\n",
      "Epoch 0 step 59: training loss: 53351.653025195046\n",
      "Epoch 0 step 60: training accuarcy: 0.5694\n",
      "Epoch 0 step 60: training loss: 53476.73057687022\n",
      "Epoch 0 step 61: training accuarcy: 0.5725\n",
      "Epoch 0 step 61: training loss: 53182.46572757854\n",
      "Epoch 0 step 62: training accuarcy: 0.5567\n",
      "Epoch 0 step 62: training loss: 51921.74304342278\n",
      "Epoch 0 step 63: training accuarcy: 0.5738\n",
      "Epoch 0 step 63: training loss: 52452.13308390027\n",
      "Epoch 0 step 64: training accuarcy: 0.553\n",
      "Epoch 0 step 64: training loss: 51308.65159587097\n",
      "Epoch 0 step 65: training accuarcy: 0.5650000000000001\n",
      "Epoch 0 step 65: training loss: 50963.134148719284\n",
      "Epoch 0 step 66: training accuarcy: 0.5646\n",
      "Epoch 0 step 66: training loss: 50695.183484107365\n",
      "Epoch 0 step 67: training accuarcy: 0.5653\n",
      "Epoch 0 step 67: training loss: 49738.203942428416\n",
      "Epoch 0 step 68: training accuarcy: 0.5779000000000001\n",
      "Epoch 0 step 68: training loss: 50297.04415502014\n",
      "Epoch 0 step 69: training accuarcy: 0.5671\n",
      "Epoch 0 step 69: training loss: 49365.82384885503\n",
      "Epoch 0 step 70: training accuarcy: 0.5762\n",
      "Epoch 0 step 70: training loss: 48793.720091769836\n",
      "Epoch 0 step 71: training accuarcy: 0.5826\n",
      "Epoch 0 step 71: training loss: 47683.69072467066\n",
      "Epoch 0 step 72: training accuarcy: 0.5914\n",
      "Epoch 0 step 72: training loss: 47894.8782217871\n",
      "Epoch 0 step 73: training accuarcy: 0.5839\n",
      "Epoch 0 step 73: training loss: 47544.03427265578\n",
      "Epoch 0 step 74: training accuarcy: 0.5862\n",
      "Epoch 0 step 74: training loss: 47294.23433352224\n",
      "Epoch 0 step 75: training accuarcy: 0.5873\n",
      "Epoch 0 step 75: training loss: 47614.946790266375\n",
      "Epoch 0 step 76: training accuarcy: 0.5866\n",
      "Epoch 0 step 76: training loss: 47897.67473745393\n",
      "Epoch 0 step 77: training accuarcy: 0.5831000000000001\n",
      "Epoch 0 step 77: training loss: 46492.498989672546\n",
      "Epoch 0 step 78: training accuarcy: 0.5958\n",
      "Epoch 0 step 78: training loss: 46802.10184603045\n",
      "Epoch 0 step 79: training accuarcy: 0.5884\n",
      "Epoch 0 step 79: training loss: 46621.560306314226\n",
      "Epoch 0 step 80: training accuarcy: 0.5846\n",
      "Epoch 0 step 80: training loss: 45886.56656124981\n",
      "Epoch 0 step 81: training accuarcy: 0.5963\n",
      "Epoch 0 step 81: training loss: 46624.26668270258\n",
      "Epoch 0 step 82: training accuarcy: 0.5804\n",
      "Epoch 0 step 82: training loss: 45886.41567402624\n",
      "Epoch 0 step 83: training accuarcy: 0.6001000000000001\n",
      "Epoch 0 step 83: training loss: 45748.32500895973\n",
      "Epoch 0 step 84: training accuarcy: 0.5881000000000001\n",
      "Epoch 0 step 84: training loss: 45492.795963150034\n",
      "Epoch 0 step 85: training accuarcy: 0.5875\n",
      "Epoch 0 step 85: training loss: 45236.212746289246\n",
      "Epoch 0 step 86: training accuarcy: 0.5819\n",
      "Epoch 0 step 86: training loss: 44889.39903795921\n",
      "Epoch 0 step 87: training accuarcy: 0.5967\n",
      "Epoch 0 step 87: training loss: 44594.90324295683\n",
      "Epoch 0 step 88: training accuarcy: 0.6064\n",
      "Epoch 0 step 88: training loss: 44248.53584450766\n",
      "Epoch 0 step 89: training accuarcy: 0.607\n",
      "Epoch 0 step 89: training loss: 44968.39339135873\n",
      "Epoch 0 step 90: training accuarcy: 0.5903\n",
      "Epoch 0 step 90: training loss: 44462.97636799871\n",
      "Epoch 0 step 91: training accuarcy: 0.5972000000000001\n",
      "Epoch 0 step 91: training loss: 43692.35513216111\n",
      "Epoch 0 step 92: training accuarcy: 0.6045\n",
      "Epoch 0 step 92: training loss: 43768.13933397746\n",
      "Epoch 0 step 93: training accuarcy: 0.6159\n",
      "Epoch 0 step 93: training loss: 44247.50242781735\n",
      "Epoch 0 step 94: training accuarcy: 0.5985\n",
      "Epoch 0 step 94: training loss: 43860.23705441014\n",
      "Epoch 0 step 95: training accuarcy: 0.612\n",
      "Epoch 0 step 95: training loss: 44254.405241494256\n",
      "Epoch 0 step 96: training accuarcy: 0.5932000000000001\n",
      "Epoch 0 step 96: training loss: 43674.00069915353\n",
      "Epoch 0 step 97: training accuarcy: 0.6089\n",
      "Epoch 0 step 97: training loss: 42896.64588761967\n",
      "Epoch 0 step 98: training accuarcy: 0.6168\n",
      "Epoch 0 step 98: training loss: 42969.35864003167\n",
      "Epoch 0 step 99: training accuarcy: 0.6207\n",
      "Epoch 0 step 99: training loss: 42837.57999000067\n",
      "Epoch 0 step 100: training accuarcy: 0.6102000000000001\n",
      "Epoch 0 step 100: training loss: 42651.02559994623\n",
      "Epoch 0 step 101: training accuarcy: 0.6238\n",
      "Epoch 0 step 101: training loss: 42833.840299424584\n",
      "Epoch 0 step 102: training accuarcy: 0.616\n",
      "Epoch 0 step 102: training loss: 42675.25066872553\n",
      "Epoch 0 step 103: training accuarcy: 0.6203000000000001\n",
      "Epoch 0 step 103: training loss: 42656.57484858908\n",
      "Epoch 0 step 104: training accuarcy: 0.6095\n",
      "Epoch 0 step 104: training loss: 42185.881449090084\n",
      "Epoch 0 step 105: training accuarcy: 0.6265000000000001\n",
      "Epoch 0 step 105: training loss: 42648.305081872124\n",
      "Epoch 0 step 106: training accuarcy: 0.6086\n",
      "Epoch 0 step 106: training loss: 41451.634755201856\n",
      "Epoch 0 step 107: training accuarcy: 0.6297\n",
      "Epoch 0 step 107: training loss: 41891.54073461717\n",
      "Epoch 0 step 108: training accuarcy: 0.623\n",
      "Epoch 0 step 108: training loss: 42133.13205485652\n",
      "Epoch 0 step 109: training accuarcy: 0.6185\n",
      "Epoch 0 step 109: training loss: 41417.558175373204\n",
      "Epoch 0 step 110: training accuarcy: 0.6346\n",
      "Epoch 0 step 110: training loss: 41498.88263415163\n",
      "Epoch 0 step 111: training accuarcy: 0.6303000000000001\n",
      "Epoch 0 step 111: training loss: 41915.51315287541\n",
      "Epoch 0 step 112: training accuarcy: 0.6187\n",
      "Epoch 0 step 112: training loss: 41507.584704272056\n",
      "Epoch 0 step 113: training accuarcy: 0.6309\n",
      "Epoch 0 step 113: training loss: 41963.67200764065\n",
      "Epoch 0 step 114: training accuarcy: 0.6189\n",
      "Epoch 0 step 114: training loss: 41226.71086686451\n",
      "Epoch 0 step 115: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 115: training loss: 41041.51766209875\n",
      "Epoch 0 step 116: training accuarcy: 0.6415000000000001\n",
      "Epoch 0 step 116: training loss: 41108.55217214449\n",
      "Epoch 0 step 117: training accuarcy: 0.6372\n",
      "Epoch 0 step 117: training loss: 41296.46640927435\n",
      "Epoch 0 step 118: training accuarcy: 0.6278\n",
      "Epoch 0 step 118: training loss: 41879.50548272974\n",
      "Epoch 0 step 119: training accuarcy: 0.6225\n",
      "Epoch 0 step 119: training loss: 41402.41689481221\n",
      "Epoch 0 step 120: training accuarcy: 0.6334000000000001\n",
      "Epoch 0 step 120: training loss: 41187.50437681527\n",
      "Epoch 0 step 121: training accuarcy: 0.6337\n",
      "Epoch 0 step 121: training loss: 40769.51412673939\n",
      "Epoch 0 step 122: training accuarcy: 0.6346\n",
      "Epoch 0 step 122: training loss: 40724.69916503954\n",
      "Epoch 0 step 123: training accuarcy: 0.6389\n",
      "Epoch 0 step 123: training loss: 40074.516892071246\n",
      "Epoch 0 step 124: training accuarcy: 0.6501\n",
      "Epoch 0 step 124: training loss: 40321.97791010557\n",
      "Epoch 0 step 125: training accuarcy: 0.6486000000000001\n",
      "Epoch 0 step 125: training loss: 40049.24204191955\n",
      "Epoch 0 step 126: training accuarcy: 0.6483\n",
      "Epoch 0 step 126: training loss: 40112.32568264703\n",
      "Epoch 0 step 127: training accuarcy: 0.6495000000000001\n",
      "Epoch 0 step 127: training loss: 39663.56236194968\n",
      "Epoch 0 step 128: training accuarcy: 0.6618\n",
      "Epoch 0 step 128: training loss: 39905.95570731075\n",
      "Epoch 0 step 129: training accuarcy: 0.6477\n",
      "Epoch 0 step 129: training loss: 39886.04678616509\n",
      "Epoch 0 step 130: training accuarcy: 0.6557000000000001\n",
      "Epoch 0 step 130: training loss: 40020.16563802789\n",
      "Epoch 0 step 131: training accuarcy: 0.655\n",
      "Epoch 0 step 131: training loss: 39625.68408617367\n",
      "Epoch 0 step 132: training accuarcy: 0.65\n",
      "Epoch 0 step 132: training loss: 39705.92042242375\n",
      "Epoch 0 step 133: training accuarcy: 0.6605000000000001\n",
      "Epoch 0 step 133: training loss: 39490.04742371687\n",
      "Epoch 0 step 134: training accuarcy: 0.6538\n",
      "Epoch 0 step 134: training loss: 39557.696141043176\n",
      "Epoch 0 step 135: training accuarcy: 0.6652\n",
      "Epoch 0 step 135: training loss: 39610.512137629565\n",
      "Epoch 0 step 136: training accuarcy: 0.6487\n",
      "Epoch 0 step 136: training loss: 39764.356623355976\n",
      "Epoch 0 step 137: training accuarcy: 0.65\n",
      "Epoch 0 step 137: training loss: 39399.646686360386\n",
      "Epoch 0 step 138: training accuarcy: 0.6584\n",
      "Epoch 0 step 138: training loss: 38824.642481342016\n",
      "Epoch 0 step 139: training accuarcy: 0.6729\n",
      "Epoch 0 step 139: training loss: 38837.01806262264\n",
      "Epoch 0 step 140: training accuarcy: 0.6698000000000001\n",
      "Epoch 0 step 140: training loss: 38834.93104748565\n",
      "Epoch 0 step 141: training accuarcy: 0.6696000000000001\n",
      "Epoch 0 step 141: training loss: 38912.460544360816\n",
      "Epoch 0 step 142: training accuarcy: 0.6696000000000001\n",
      "Epoch 0 step 142: training loss: 38680.60371682092\n",
      "Epoch 0 step 143: training accuarcy: 0.6661\n",
      "Epoch 0 step 143: training loss: 38702.238929985455\n",
      "Epoch 0 step 144: training accuarcy: 0.6687000000000001\n",
      "Epoch 0 step 144: training loss: 38849.27618623343\n",
      "Epoch 0 step 145: training accuarcy: 0.6633\n",
      "Epoch 0 step 145: training loss: 39188.26484068709\n",
      "Epoch 0 step 146: training accuarcy: 0.6588\n",
      "Epoch 0 step 146: training loss: 38622.890297750986\n",
      "Epoch 0 step 147: training accuarcy: 0.671\n",
      "Epoch 0 step 147: training loss: 38405.99375159457\n",
      "Epoch 0 step 148: training accuarcy: 0.6759000000000001\n",
      "Epoch 0 step 148: training loss: 38453.17007851935\n",
      "Epoch 0 step 149: training accuarcy: 0.6781\n",
      "Epoch 0 step 149: training loss: 38872.21685071643\n",
      "Epoch 0 step 150: training accuarcy: 0.6695\n",
      "Epoch 0 step 150: training loss: 38739.66316213471\n",
      "Epoch 0 step 151: training accuarcy: 0.6663\n",
      "Epoch 0 step 151: training loss: 38343.395765643\n",
      "Epoch 0 step 152: training accuarcy: 0.6712\n",
      "Epoch 0 step 152: training loss: 37979.318837618805\n",
      "Epoch 0 step 153: training accuarcy: 0.6875\n",
      "Epoch 0 step 153: training loss: 38156.486212527176\n",
      "Epoch 0 step 154: training accuarcy: 0.6744\n",
      "Epoch 0 step 154: training loss: 38491.53359396441\n",
      "Epoch 0 step 155: training accuarcy: 0.6762\n",
      "Epoch 0 step 155: training loss: 37920.01717030075\n",
      "Epoch 0 step 156: training accuarcy: 0.685\n",
      "Epoch 0 step 156: training loss: 37981.37228142651\n",
      "Epoch 0 step 157: training accuarcy: 0.683\n",
      "Epoch 0 step 157: training loss: 37904.011466645585\n",
      "Epoch 0 step 158: training accuarcy: 0.678\n",
      "Epoch 0 step 158: training loss: 37718.649433871855\n",
      "Epoch 0 step 159: training accuarcy: 0.6871\n",
      "Epoch 0 step 159: training loss: 37479.22279795881\n",
      "Epoch 0 step 160: training accuarcy: 0.6885\n",
      "Epoch 0 step 160: training loss: 37601.55259248121\n",
      "Epoch 0 step 161: training accuarcy: 0.6826\n",
      "Epoch 0 step 161: training loss: 38085.27381216524\n",
      "Epoch 0 step 162: training accuarcy: 0.6687000000000001\n",
      "Epoch 0 step 162: training loss: 38039.10241673222\n",
      "Epoch 0 step 163: training accuarcy: 0.6852\n",
      "Epoch 0 step 163: training loss: 37862.9427208481\n",
      "Epoch 0 step 164: training accuarcy: 0.6809000000000001\n",
      "Epoch 0 step 164: training loss: 37749.71662737567\n",
      "Epoch 0 step 165: training accuarcy: 0.6844\n",
      "Epoch 0 step 165: training loss: 37164.49553803697\n",
      "Epoch 0 step 166: training accuarcy: 0.6961\n",
      "Epoch 0 step 166: training loss: 37449.77239260851\n",
      "Epoch 0 step 167: training accuarcy: 0.6874\n",
      "Epoch 0 step 167: training loss: 37659.393278794065\n",
      "Epoch 0 step 168: training accuarcy: 0.6875\n",
      "Epoch 0 step 168: training loss: 37239.63905204004\n",
      "Epoch 0 step 169: training accuarcy: 0.6829000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 169: training loss: 37197.22417312359\n",
      "Epoch 0 step 170: training accuarcy: 0.6948000000000001\n",
      "Epoch 0 step 170: training loss: 37271.32799917286\n",
      "Epoch 0 step 171: training accuarcy: 0.6865\n",
      "Epoch 0 step 171: training loss: 38013.06343429402\n",
      "Epoch 0 step 172: training accuarcy: 0.6849000000000001\n",
      "Epoch 0 step 172: training loss: 37532.25739286332\n",
      "Epoch 0 step 173: training accuarcy: 0.6824\n",
      "Epoch 0 step 173: training loss: 37072.33954684183\n",
      "Epoch 0 step 174: training accuarcy: 0.6904\n",
      "Epoch 0 step 174: training loss: 36897.478919225105\n",
      "Epoch 0 step 175: training accuarcy: 0.6941\n",
      "Epoch 0 step 175: training loss: 36839.429808392735\n",
      "Epoch 0 step 176: training accuarcy: 0.6974\n",
      "Epoch 0 step 176: training loss: 36865.56303404251\n",
      "Epoch 0 step 177: training accuarcy: 0.6892\n",
      "Epoch 0 step 177: training loss: 37232.450899596624\n",
      "Epoch 0 step 178: training accuarcy: 0.6881\n",
      "Epoch 0 step 178: training loss: 36279.362217262074\n",
      "Epoch 0 step 179: training accuarcy: 0.7096\n",
      "Epoch 0 step 179: training loss: 37100.95838853873\n",
      "Epoch 0 step 180: training accuarcy: 0.6922\n",
      "Epoch 0 step 180: training loss: 36831.99898559486\n",
      "Epoch 0 step 181: training accuarcy: 0.7022\n",
      "Epoch 0 step 181: training loss: 36853.10260686503\n",
      "Epoch 0 step 182: training accuarcy: 0.6942\n",
      "Epoch 0 step 182: training loss: 36625.08324917516\n",
      "Epoch 0 step 183: training accuarcy: 0.6989000000000001\n",
      "Epoch 0 step 183: training loss: 36402.67693204575\n",
      "Epoch 0 step 184: training accuarcy: 0.7065\n",
      "Epoch 0 step 184: training loss: 36386.27030161718\n",
      "Epoch 0 step 185: training accuarcy: 0.7014\n",
      "Epoch 0 step 185: training loss: 36597.94718139978\n",
      "Epoch 0 step 186: training accuarcy: 0.6997\n",
      "Epoch 0 step 186: training loss: 37097.61758564309\n",
      "Epoch 0 step 187: training accuarcy: 0.6864\n",
      "Epoch 0 step 187: training loss: 36830.25099546311\n",
      "Epoch 0 step 188: training accuarcy: 0.6964\n",
      "Epoch 0 step 188: training loss: 35935.56804394504\n",
      "Epoch 0 step 189: training accuarcy: 0.7081000000000001\n",
      "Epoch 0 step 189: training loss: 36162.000293282006\n",
      "Epoch 0 step 190: training accuarcy: 0.7022\n",
      "Epoch 0 step 190: training loss: 36242.13291677952\n",
      "Epoch 0 step 191: training accuarcy: 0.7071000000000001\n",
      "Epoch 0 step 191: training loss: 36725.14780642603\n",
      "Epoch 0 step 192: training accuarcy: 0.7014\n",
      "Epoch 0 step 192: training loss: 36571.533839624586\n",
      "Epoch 0 step 193: training accuarcy: 0.6979000000000001\n",
      "Epoch 0 step 193: training loss: 36252.634259176426\n",
      "Epoch 0 step 194: training accuarcy: 0.7061000000000001\n",
      "Epoch 0 step 194: training loss: 36257.994766221935\n",
      "Epoch 0 step 195: training accuarcy: 0.7035\n",
      "Epoch 0 step 195: training loss: 35966.98966892451\n",
      "Epoch 0 step 196: training accuarcy: 0.7067\n",
      "Epoch 0 step 196: training loss: 36017.82349128469\n",
      "Epoch 0 step 197: training accuarcy: 0.7056\n",
      "Epoch 0 step 197: training loss: 36356.7010422043\n",
      "Epoch 0 step 198: training accuarcy: 0.7035\n",
      "Epoch 0 step 198: training loss: 35675.668082417265\n",
      "Epoch 0 step 199: training accuarcy: 0.7174\n",
      "Epoch 0 step 199: training loss: 36115.203165077604\n",
      "Epoch 0 step 200: training accuarcy: 0.6965\n",
      "Epoch 0 step 200: training loss: 36083.36830895803\n",
      "Epoch 0 step 201: training accuarcy: 0.7058\n",
      "Epoch 0 step 201: training loss: 35874.982346960096\n",
      "Epoch 0 step 202: training accuarcy: 0.7063\n",
      "Epoch 0 step 202: training loss: 35996.52356107235\n",
      "Epoch 0 step 203: training accuarcy: 0.7020000000000001\n",
      "Epoch 0 step 203: training loss: 36285.48536118589\n",
      "Epoch 0 step 204: training accuarcy: 0.7058\n",
      "Epoch 0 step 204: training loss: 35830.093796765614\n",
      "Epoch 0 step 205: training accuarcy: 0.7090000000000001\n",
      "Epoch 0 step 205: training loss: 35973.99167644843\n",
      "Epoch 0 step 206: training accuarcy: 0.7047\n",
      "Epoch 0 step 206: training loss: 35379.650118237616\n",
      "Epoch 0 step 207: training accuarcy: 0.7239\n",
      "Epoch 0 step 207: training loss: 35818.87913388075\n",
      "Epoch 0 step 208: training accuarcy: 0.7152000000000001\n",
      "Epoch 0 step 208: training loss: 34963.551392016365\n",
      "Epoch 0 step 209: training accuarcy: 0.7253000000000001\n",
      "Epoch 0 step 209: training loss: 35463.48597475151\n",
      "Epoch 0 step 210: training accuarcy: 0.7098\n",
      "Epoch 0 step 210: training loss: 35152.59618966754\n",
      "Epoch 0 step 211: training accuarcy: 0.7118\n",
      "Epoch 0 step 211: training loss: 35448.428277139814\n",
      "Epoch 0 step 212: training accuarcy: 0.7165\n",
      "Epoch 0 step 212: training loss: 35229.46708689221\n",
      "Epoch 0 step 213: training accuarcy: 0.7118\n",
      "Epoch 0 step 213: training loss: 35509.76467399786\n",
      "Epoch 0 step 214: training accuarcy: 0.7205\n",
      "Epoch 0 step 214: training loss: 35984.482258246826\n",
      "Epoch 0 step 215: training accuarcy: 0.7034\n",
      "Epoch 0 step 215: training loss: 35460.55754635639\n",
      "Epoch 0 step 216: training accuarcy: 0.7199\n",
      "Epoch 0 step 216: training loss: 35591.21015305679\n",
      "Epoch 0 step 217: training accuarcy: 0.7188\n",
      "Epoch 0 step 217: training loss: 35356.71136722554\n",
      "Epoch 0 step 218: training accuarcy: 0.7101000000000001\n",
      "Epoch 0 step 218: training loss: 35248.732488283276\n",
      "Epoch 0 step 219: training accuarcy: 0.7187\n",
      "Epoch 0 step 219: training loss: 35249.81769069251\n",
      "Epoch 0 step 220: training accuarcy: 0.7219\n",
      "Epoch 0 step 220: training loss: 34993.213837316536\n",
      "Epoch 0 step 221: training accuarcy: 0.7162000000000001\n",
      "Epoch 0 step 221: training loss: 34986.43602715645\n",
      "Epoch 0 step 222: training accuarcy: 0.7241000000000001\n",
      "Epoch 0 step 222: training loss: 34863.558679947775\n",
      "Epoch 0 step 223: training accuarcy: 0.7252000000000001\n",
      "Epoch 0 step 223: training loss: 35340.2167809559\n",
      "Epoch 0 step 224: training accuarcy: 0.7184\n",
      "Epoch 0 step 224: training loss: 35016.86586821722\n",
      "Epoch 0 step 225: training accuarcy: 0.7206\n",
      "Epoch 0 step 225: training loss: 34995.72357523601\n",
      "Epoch 0 step 226: training accuarcy: 0.7311000000000001\n",
      "Epoch 0 step 226: training loss: 34821.15321005895\n",
      "Epoch 0 step 227: training accuarcy: 0.7250000000000001\n",
      "Epoch 0 step 227: training loss: 35183.5636525012\n",
      "Epoch 0 step 228: training accuarcy: 0.7198\n",
      "Epoch 0 step 228: training loss: 35137.72738377773\n",
      "Epoch 0 step 229: training accuarcy: 0.7297\n",
      "Epoch 0 step 229: training loss: 35069.935788089824\n",
      "Epoch 0 step 230: training accuarcy: 0.7229\n",
      "Epoch 0 step 230: training loss: 34309.64009126546\n",
      "Epoch 0 step 231: training accuarcy: 0.7258\n",
      "Epoch 0 step 231: training loss: 34957.74452757635\n",
      "Epoch 0 step 232: training accuarcy: 0.7159\n",
      "Epoch 0 step 232: training loss: 34447.535412812285\n",
      "Epoch 0 step 233: training accuarcy: 0.7247\n",
      "Epoch 0 step 233: training loss: 34936.56444875852\n",
      "Epoch 0 step 234: training accuarcy: 0.7203\n",
      "Epoch 0 step 234: training loss: 34241.0661770185\n",
      "Epoch 0 step 235: training accuarcy: 0.727\n",
      "Epoch 0 step 235: training loss: 34668.49831956962\n",
      "Epoch 0 step 236: training accuarcy: 0.7225\n",
      "Epoch 0 step 236: training loss: 34572.54377675141\n",
      "Epoch 0 step 237: training accuarcy: 0.7247\n",
      "Epoch 0 step 237: training loss: 34746.18165536891\n",
      "Epoch 0 step 238: training accuarcy: 0.7216\n",
      "Epoch 0 step 238: training loss: 34957.41972071829\n",
      "Epoch 0 step 239: training accuarcy: 0.7182000000000001\n",
      "Epoch 0 step 239: training loss: 35169.91435184982\n",
      "Epoch 0 step 240: training accuarcy: 0.7206\n",
      "Epoch 0 step 240: training loss: 34708.416601433855\n",
      "Epoch 0 step 241: training accuarcy: 0.7219\n",
      "Epoch 0 step 241: training loss: 34897.30972102288\n",
      "Epoch 0 step 242: training accuarcy: 0.7145\n",
      "Epoch 0 step 242: training loss: 34467.124104514805\n",
      "Epoch 0 step 243: training accuarcy: 0.7242000000000001\n",
      "Epoch 0 step 243: training loss: 34294.283932403814\n",
      "Epoch 0 step 244: training accuarcy: 0.7263000000000001\n",
      "Epoch 0 step 244: training loss: 33967.64283128842\n",
      "Epoch 0 step 245: training accuarcy: 0.7353000000000001\n",
      "Epoch 0 step 245: training loss: 34128.75063798379\n",
      "Epoch 0 step 246: training accuarcy: 0.7279\n",
      "Epoch 0 step 246: training loss: 34471.19088489437\n",
      "Epoch 0 step 247: training accuarcy: 0.7248\n",
      "Epoch 0 step 247: training loss: 33934.62552101892\n",
      "Epoch 0 step 248: training accuarcy: 0.7378\n",
      "Epoch 0 step 248: training loss: 33843.055729428794\n",
      "Epoch 0 step 249: training accuarcy: 0.7381000000000001\n",
      "Epoch 0 step 249: training loss: 34483.14589502088\n",
      "Epoch 0 step 250: training accuarcy: 0.7250000000000001\n",
      "Epoch 0 step 250: training loss: 33921.281117615516\n",
      "Epoch 0 step 251: training accuarcy: 0.7295\n",
      "Epoch 0 step 251: training loss: 34146.1995418\n",
      "Epoch 0 step 252: training accuarcy: 0.7222000000000001\n",
      "Epoch 0 step 252: training loss: 34278.952959497314\n",
      "Epoch 0 step 253: training accuarcy: 0.7257\n",
      "Epoch 0 step 253: training loss: 33501.52451137232\n",
      "Epoch 0 step 254: training accuarcy: 0.7494000000000001\n",
      "Epoch 0 step 254: training loss: 33864.68539032806\n",
      "Epoch 0 step 255: training accuarcy: 0.7278\n",
      "Epoch 0 step 255: training loss: 34020.38493017855\n",
      "Epoch 0 step 256: training accuarcy: 0.7354\n",
      "Epoch 0 step 256: training loss: 34341.72044269684\n",
      "Epoch 0 step 257: training accuarcy: 0.7245\n",
      "Epoch 0 step 257: training loss: 33554.52557347421\n",
      "Epoch 0 step 258: training accuarcy: 0.7383000000000001\n",
      "Epoch 0 step 258: training loss: 33889.85953733517\n",
      "Epoch 0 step 259: training accuarcy: 0.736\n",
      "Epoch 0 step 259: training loss: 34254.14105188148\n",
      "Epoch 0 step 260: training accuarcy: 0.7288\n",
      "Epoch 0 step 260: training loss: 34180.93643714745\n",
      "Epoch 0 step 261: training accuarcy: 0.7249\n",
      "Epoch 0 step 261: training loss: 33865.30062586484\n",
      "Epoch 0 step 262: training accuarcy: 0.7386\n",
      "Epoch 0 step 262: training loss: 17068.280397425377\n",
      "Epoch 0 step 263: training accuarcy: 0.7035897435897436\n",
      "Epoch 0: train loss 53804.09029535396, train accuarcy 0.6310576796531677\n",
      "Epoch 0: valid loss 32219.746296435827, valid accuarcy 0.7518095374107361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [05:04<10:08, 304.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 33232.64051768092\n",
      "Epoch 1 step 264: training accuarcy: 0.7549\n",
      "Epoch 1 step 264: training loss: 32550.221703703206\n",
      "Epoch 1 step 265: training accuarcy: 0.7577\n",
      "Epoch 1 step 265: training loss: 33045.24654042096\n",
      "Epoch 1 step 266: training accuarcy: 0.756\n",
      "Epoch 1 step 266: training loss: 32446.43821047878\n",
      "Epoch 1 step 267: training accuarcy: 0.763\n",
      "Epoch 1 step 267: training loss: 32815.873558117186\n",
      "Epoch 1 step 268: training accuarcy: 0.7507\n",
      "Epoch 1 step 268: training loss: 33085.9551245544\n",
      "Epoch 1 step 269: training accuarcy: 0.7499\n",
      "Epoch 1 step 269: training loss: 32525.589556083534\n",
      "Epoch 1 step 270: training accuarcy: 0.7584000000000001\n",
      "Epoch 1 step 270: training loss: 33522.12690631198\n",
      "Epoch 1 step 271: training accuarcy: 0.7422000000000001\n",
      "Epoch 1 step 271: training loss: 32712.234827701042\n",
      "Epoch 1 step 272: training accuarcy: 0.7588\n",
      "Epoch 1 step 272: training loss: 32848.03136463337\n",
      "Epoch 1 step 273: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 273: training loss: 32438.109261423837\n",
      "Epoch 1 step 274: training accuarcy: 0.764\n",
      "Epoch 1 step 274: training loss: 32326.68082628503\n",
      "Epoch 1 step 275: training accuarcy: 0.7589\n",
      "Epoch 1 step 275: training loss: 32714.730519611898\n",
      "Epoch 1 step 276: training accuarcy: 0.753\n",
      "Epoch 1 step 276: training loss: 32781.46014026087\n",
      "Epoch 1 step 277: training accuarcy: 0.7547\n",
      "Epoch 1 step 277: training loss: 32710.062666459504\n",
      "Epoch 1 step 278: training accuarcy: 0.7477\n",
      "Epoch 1 step 278: training loss: 32777.24265549321\n",
      "Epoch 1 step 279: training accuarcy: 0.7542\n",
      "Epoch 1 step 279: training loss: 33034.572193548636\n",
      "Epoch 1 step 280: training accuarcy: 0.754\n",
      "Epoch 1 step 280: training loss: 32184.979440527415\n",
      "Epoch 1 step 281: training accuarcy: 0.7667\n",
      "Epoch 1 step 281: training loss: 32500.732316935966\n",
      "Epoch 1 step 282: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 282: training loss: 32801.344761694105\n",
      "Epoch 1 step 283: training accuarcy: 0.7509\n",
      "Epoch 1 step 283: training loss: 32421.99898550783\n",
      "Epoch 1 step 284: training accuarcy: 0.7637\n",
      "Epoch 1 step 284: training loss: 32623.88849869165\n",
      "Epoch 1 step 285: training accuarcy: 0.7511\n",
      "Epoch 1 step 285: training loss: 32625.768870462463\n",
      "Epoch 1 step 286: training accuarcy: 0.7524000000000001\n",
      "Epoch 1 step 286: training loss: 32472.385506414146\n",
      "Epoch 1 step 287: training accuarcy: 0.7586\n",
      "Epoch 1 step 287: training loss: 32410.306529012545\n",
      "Epoch 1 step 288: training accuarcy: 0.7618\n",
      "Epoch 1 step 288: training loss: 32281.626598752933\n",
      "Epoch 1 step 289: training accuarcy: 0.7597\n",
      "Epoch 1 step 289: training loss: 32431.91071622417\n",
      "Epoch 1 step 290: training accuarcy: 0.7461\n",
      "Epoch 1 step 290: training loss: 32415.44152479219\n",
      "Epoch 1 step 291: training accuarcy: 0.755\n",
      "Epoch 1 step 291: training loss: 32133.183084253695\n",
      "Epoch 1 step 292: training accuarcy: 0.7637\n",
      "Epoch 1 step 292: training loss: 32202.37825502399\n",
      "Epoch 1 step 293: training accuarcy: 0.7592\n",
      "Epoch 1 step 293: training loss: 32149.828259541286\n",
      "Epoch 1 step 294: training accuarcy: 0.7603000000000001\n",
      "Epoch 1 step 294: training loss: 32142.236458521013\n",
      "Epoch 1 step 295: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 295: training loss: 32810.51110371607\n",
      "Epoch 1 step 296: training accuarcy: 0.7548\n",
      "Epoch 1 step 296: training loss: 32609.915803462707\n",
      "Epoch 1 step 297: training accuarcy: 0.7519\n",
      "Epoch 1 step 297: training loss: 32398.983750882286\n",
      "Epoch 1 step 298: training accuarcy: 0.7583000000000001\n",
      "Epoch 1 step 298: training loss: 32634.584070323603\n",
      "Epoch 1 step 299: training accuarcy: 0.7559\n",
      "Epoch 1 step 299: training loss: 32095.92971117602\n",
      "Epoch 1 step 300: training accuarcy: 0.7611\n",
      "Epoch 1 step 300: training loss: 32036.828925822756\n",
      "Epoch 1 step 301: training accuarcy: 0.7674000000000001\n",
      "Epoch 1 step 301: training loss: 32357.262575967245\n",
      "Epoch 1 step 302: training accuarcy: 0.753\n",
      "Epoch 1 step 302: training loss: 32653.39880757966\n",
      "Epoch 1 step 303: training accuarcy: 0.7598\n",
      "Epoch 1 step 303: training loss: 31746.143656092292\n",
      "Epoch 1 step 304: training accuarcy: 0.7626000000000001\n",
      "Epoch 1 step 304: training loss: 32887.23617950478\n",
      "Epoch 1 step 305: training accuarcy: 0.7481\n",
      "Epoch 1 step 305: training loss: 32957.94534597535\n",
      "Epoch 1 step 306: training accuarcy: 0.7434000000000001\n",
      "Epoch 1 step 306: training loss: 32125.76097023602\n",
      "Epoch 1 step 307: training accuarcy: 0.7637\n",
      "Epoch 1 step 307: training loss: 31790.33154205657\n",
      "Epoch 1 step 308: training accuarcy: 0.7708\n",
      "Epoch 1 step 308: training loss: 32223.472628073803\n",
      "Epoch 1 step 309: training accuarcy: 0.759\n",
      "Epoch 1 step 309: training loss: 32752.66387678003\n",
      "Epoch 1 step 310: training accuarcy: 0.7487\n",
      "Epoch 1 step 310: training loss: 32492.584041923572\n",
      "Epoch 1 step 311: training accuarcy: 0.7517\n",
      "Epoch 1 step 311: training loss: 32298.572653205832\n",
      "Epoch 1 step 312: training accuarcy: 0.7569\n",
      "Epoch 1 step 312: training loss: 31887.53380642345\n",
      "Epoch 1 step 313: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 313: training loss: 32276.071421960896\n",
      "Epoch 1 step 314: training accuarcy: 0.7558\n",
      "Epoch 1 step 314: training loss: 32239.843426922438\n",
      "Epoch 1 step 315: training accuarcy: 0.755\n",
      "Epoch 1 step 315: training loss: 32307.765107586187\n",
      "Epoch 1 step 316: training accuarcy: 0.754\n",
      "Epoch 1 step 316: training loss: 32348.144215344444\n",
      "Epoch 1 step 317: training accuarcy: 0.7532\n",
      "Epoch 1 step 317: training loss: 32271.32956229514\n",
      "Epoch 1 step 318: training accuarcy: 0.7535000000000001\n",
      "Epoch 1 step 318: training loss: 32246.90469019874\n",
      "Epoch 1 step 319: training accuarcy: 0.7571\n",
      "Epoch 1 step 319: training loss: 31532.897504623757\n",
      "Epoch 1 step 320: training accuarcy: 0.7652\n",
      "Epoch 1 step 320: training loss: 31827.16260635207\n",
      "Epoch 1 step 321: training accuarcy: 0.7619\n",
      "Epoch 1 step 321: training loss: 31956.12593010076\n",
      "Epoch 1 step 322: training accuarcy: 0.7643000000000001\n",
      "Epoch 1 step 322: training loss: 32017.55271222202\n",
      "Epoch 1 step 323: training accuarcy: 0.766\n",
      "Epoch 1 step 323: training loss: 31736.473004625055\n",
      "Epoch 1 step 324: training accuarcy: 0.7647\n",
      "Epoch 1 step 324: training loss: 31709.06598605278\n",
      "Epoch 1 step 325: training accuarcy: 0.7621\n",
      "Epoch 1 step 325: training loss: 31924.006125437798\n",
      "Epoch 1 step 326: training accuarcy: 0.7616\n",
      "Epoch 1 step 326: training loss: 31618.512693530876\n",
      "Epoch 1 step 327: training accuarcy: 0.7711\n",
      "Epoch 1 step 327: training loss: 31713.122684935006\n",
      "Epoch 1 step 328: training accuarcy: 0.7616\n",
      "Epoch 1 step 328: training loss: 32333.604517920678\n",
      "Epoch 1 step 329: training accuarcy: 0.7551\n",
      "Epoch 1 step 329: training loss: 31384.69985166389\n",
      "Epoch 1 step 330: training accuarcy: 0.7686000000000001\n",
      "Epoch 1 step 330: training loss: 31299.53629864839\n",
      "Epoch 1 step 331: training accuarcy: 0.7707\n",
      "Epoch 1 step 331: training loss: 31678.95570333552\n",
      "Epoch 1 step 332: training accuarcy: 0.7681\n",
      "Epoch 1 step 332: training loss: 31771.000761021674\n",
      "Epoch 1 step 333: training accuarcy: 0.7628\n",
      "Epoch 1 step 333: training loss: 31744.03933102204\n",
      "Epoch 1 step 334: training accuarcy: 0.7616\n",
      "Epoch 1 step 334: training loss: 32130.11635831442\n",
      "Epoch 1 step 335: training accuarcy: 0.7551\n",
      "Epoch 1 step 335: training loss: 31866.733389249763\n",
      "Epoch 1 step 336: training accuarcy: 0.7612\n",
      "Epoch 1 step 336: training loss: 32278.305861882473\n",
      "Epoch 1 step 337: training accuarcy: 0.7529\n",
      "Epoch 1 step 337: training loss: 32022.283973499394\n",
      "Epoch 1 step 338: training accuarcy: 0.7542\n",
      "Epoch 1 step 338: training loss: 31666.63837453708\n",
      "Epoch 1 step 339: training accuarcy: 0.7574000000000001\n",
      "Epoch 1 step 339: training loss: 31643.697942267954\n",
      "Epoch 1 step 340: training accuarcy: 0.7603000000000001\n",
      "Epoch 1 step 340: training loss: 31551.943370669123\n",
      "Epoch 1 step 341: training accuarcy: 0.7616\n",
      "Epoch 1 step 341: training loss: 31622.181495846664\n",
      "Epoch 1 step 342: training accuarcy: 0.7619\n",
      "Epoch 1 step 342: training loss: 31760.474914751445\n",
      "Epoch 1 step 343: training accuarcy: 0.7581\n",
      "Epoch 1 step 343: training loss: 31193.410981377125\n",
      "Epoch 1 step 344: training accuarcy: 0.7715000000000001\n",
      "Epoch 1 step 344: training loss: 32015.038855736468\n",
      "Epoch 1 step 345: training accuarcy: 0.7626000000000001\n",
      "Epoch 1 step 345: training loss: 31664.908712051132\n",
      "Epoch 1 step 346: training accuarcy: 0.7588\n",
      "Epoch 1 step 346: training loss: 32194.906405058213\n",
      "Epoch 1 step 347: training accuarcy: 0.7536\n",
      "Epoch 1 step 347: training loss: 31560.79408243436\n",
      "Epoch 1 step 348: training accuarcy: 0.7566\n",
      "Epoch 1 step 348: training loss: 31812.411681668676\n",
      "Epoch 1 step 349: training accuarcy: 0.7591\n",
      "Epoch 1 step 349: training loss: 31729.45014434647\n",
      "Epoch 1 step 350: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 350: training loss: 32174.51372787211\n",
      "Epoch 1 step 351: training accuarcy: 0.7611\n",
      "Epoch 1 step 351: training loss: 31700.859085089607\n",
      "Epoch 1 step 352: training accuarcy: 0.761\n",
      "Epoch 1 step 352: training loss: 31729.56458239881\n",
      "Epoch 1 step 353: training accuarcy: 0.7597\n",
      "Epoch 1 step 353: training loss: 31176.129505494973\n",
      "Epoch 1 step 354: training accuarcy: 0.7673\n",
      "Epoch 1 step 354: training loss: 31678.289222722444\n",
      "Epoch 1 step 355: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 355: training loss: 31425.195905576504\n",
      "Epoch 1 step 356: training accuarcy: 0.7606\n",
      "Epoch 1 step 356: training loss: 31710.545952590895\n",
      "Epoch 1 step 357: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 357: training loss: 32208.94566137876\n",
      "Epoch 1 step 358: training accuarcy: 0.7568\n",
      "Epoch 1 step 358: training loss: 31558.03434089813\n",
      "Epoch 1 step 359: training accuarcy: 0.7641\n",
      "Epoch 1 step 359: training loss: 31494.04929353452\n",
      "Epoch 1 step 360: training accuarcy: 0.7598\n",
      "Epoch 1 step 360: training loss: 31711.352987032096\n",
      "Epoch 1 step 361: training accuarcy: 0.7616\n",
      "Epoch 1 step 361: training loss: 31581.124297565824\n",
      "Epoch 1 step 362: training accuarcy: 0.759\n",
      "Epoch 1 step 362: training loss: 31421.383612980997\n",
      "Epoch 1 step 363: training accuarcy: 0.7668\n",
      "Epoch 1 step 363: training loss: 31744.584091739405\n",
      "Epoch 1 step 364: training accuarcy: 0.7523000000000001\n",
      "Epoch 1 step 364: training loss: 31259.674818120893\n",
      "Epoch 1 step 365: training accuarcy: 0.7568\n",
      "Epoch 1 step 365: training loss: 31614.04772607352\n",
      "Epoch 1 step 366: training accuarcy: 0.7598\n",
      "Epoch 1 step 366: training loss: 31524.46534086342\n",
      "Epoch 1 step 367: training accuarcy: 0.7636000000000001\n",
      "Epoch 1 step 367: training loss: 31058.27363574502\n",
      "Epoch 1 step 368: training accuarcy: 0.7726000000000001\n",
      "Epoch 1 step 368: training loss: 31499.721411458868\n",
      "Epoch 1 step 369: training accuarcy: 0.7555000000000001\n",
      "Epoch 1 step 369: training loss: 31010.89614317582\n",
      "Epoch 1 step 370: training accuarcy: 0.7673\n",
      "Epoch 1 step 370: training loss: 31361.096852028666\n",
      "Epoch 1 step 371: training accuarcy: 0.7698\n",
      "Epoch 1 step 371: training loss: 31239.171433093397\n",
      "Epoch 1 step 372: training accuarcy: 0.7632\n",
      "Epoch 1 step 372: training loss: 31953.91437300783\n",
      "Epoch 1 step 373: training accuarcy: 0.7584000000000001\n",
      "Epoch 1 step 373: training loss: 31379.350071597048\n",
      "Epoch 1 step 374: training accuarcy: 0.7698\n",
      "Epoch 1 step 374: training loss: 31106.89145195963\n",
      "Epoch 1 step 375: training accuarcy: 0.766\n",
      "Epoch 1 step 375: training loss: 31394.73361193596\n",
      "Epoch 1 step 376: training accuarcy: 0.7602\n",
      "Epoch 1 step 376: training loss: 31151.93889376422\n",
      "Epoch 1 step 377: training accuarcy: 0.7696000000000001\n",
      "Epoch 1 step 377: training loss: 31725.718980787737\n",
      "Epoch 1 step 378: training accuarcy: 0.7589\n",
      "Epoch 1 step 378: training loss: 31566.270594937203\n",
      "Epoch 1 step 379: training accuarcy: 0.7576\n",
      "Epoch 1 step 379: training loss: 30687.59255314443\n",
      "Epoch 1 step 380: training accuarcy: 0.7735000000000001\n",
      "Epoch 1 step 380: training loss: 31723.811903180675\n",
      "Epoch 1 step 381: training accuarcy: 0.7583000000000001\n",
      "Epoch 1 step 381: training loss: 31377.963611127576\n",
      "Epoch 1 step 382: training accuarcy: 0.7692\n",
      "Epoch 1 step 382: training loss: 31580.93331720585\n",
      "Epoch 1 step 383: training accuarcy: 0.7579\n",
      "Epoch 1 step 383: training loss: 31534.66692024107\n",
      "Epoch 1 step 384: training accuarcy: 0.7602\n",
      "Epoch 1 step 384: training loss: 30678.69974812981\n",
      "Epoch 1 step 385: training accuarcy: 0.7745000000000001\n",
      "Epoch 1 step 385: training loss: 31531.23150585139\n",
      "Epoch 1 step 386: training accuarcy: 0.7557\n",
      "Epoch 1 step 386: training loss: 31377.615544637414\n",
      "Epoch 1 step 387: training accuarcy: 0.76\n",
      "Epoch 1 step 387: training loss: 31314.950385229422\n",
      "Epoch 1 step 388: training accuarcy: 0.7695000000000001\n",
      "Epoch 1 step 388: training loss: 31281.926670015666\n",
      "Epoch 1 step 389: training accuarcy: 0.7599\n",
      "Epoch 1 step 389: training loss: 31356.682704669234\n",
      "Epoch 1 step 390: training accuarcy: 0.7564000000000001\n",
      "Epoch 1 step 390: training loss: 31234.72517855964\n",
      "Epoch 1 step 391: training accuarcy: 0.7671\n",
      "Epoch 1 step 391: training loss: 30852.17194270255\n",
      "Epoch 1 step 392: training accuarcy: 0.7717\n",
      "Epoch 1 step 392: training loss: 31407.587375048268\n",
      "Epoch 1 step 393: training accuarcy: 0.7631\n",
      "Epoch 1 step 393: training loss: 31418.511798527823\n",
      "Epoch 1 step 394: training accuarcy: 0.7586\n",
      "Epoch 1 step 394: training loss: 31514.425379820597\n",
      "Epoch 1 step 395: training accuarcy: 0.7639\n",
      "Epoch 1 step 395: training loss: 31569.229775034466\n",
      "Epoch 1 step 396: training accuarcy: 0.755\n",
      "Epoch 1 step 396: training loss: 31207.870646881536\n",
      "Epoch 1 step 397: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 397: training loss: 31221.885827494727\n",
      "Epoch 1 step 398: training accuarcy: 0.756\n",
      "Epoch 1 step 398: training loss: 31107.00899907674\n",
      "Epoch 1 step 399: training accuarcy: 0.7685000000000001\n",
      "Epoch 1 step 399: training loss: 31052.854619237463\n",
      "Epoch 1 step 400: training accuarcy: 0.764\n",
      "Epoch 1 step 400: training loss: 31125.98208065675\n",
      "Epoch 1 step 401: training accuarcy: 0.7687\n",
      "Epoch 1 step 401: training loss: 31382.03960597923\n",
      "Epoch 1 step 402: training accuarcy: 0.7589\n",
      "Epoch 1 step 402: training loss: 30880.918554051314\n",
      "Epoch 1 step 403: training accuarcy: 0.7715000000000001\n",
      "Epoch 1 step 403: training loss: 31613.873508981036\n",
      "Epoch 1 step 404: training accuarcy: 0.7691\n",
      "Epoch 1 step 404: training loss: 31489.398562282844\n",
      "Epoch 1 step 405: training accuarcy: 0.7617\n",
      "Epoch 1 step 405: training loss: 31195.61334809867\n",
      "Epoch 1 step 406: training accuarcy: 0.7689\n",
      "Epoch 1 step 406: training loss: 31468.537604666984\n",
      "Epoch 1 step 407: training accuarcy: 0.7587\n",
      "Epoch 1 step 407: training loss: 30977.100641356526\n",
      "Epoch 1 step 408: training accuarcy: 0.7625000000000001\n",
      "Epoch 1 step 408: training loss: 30942.77682963838\n",
      "Epoch 1 step 409: training accuarcy: 0.7623000000000001\n",
      "Epoch 1 step 409: training loss: 31305.955361758373\n",
      "Epoch 1 step 410: training accuarcy: 0.7663000000000001\n",
      "Epoch 1 step 410: training loss: 30485.349009571164\n",
      "Epoch 1 step 411: training accuarcy: 0.7746000000000001\n",
      "Epoch 1 step 411: training loss: 30974.52671877022\n",
      "Epoch 1 step 412: training accuarcy: 0.7699\n",
      "Epoch 1 step 412: training loss: 31379.357903761495\n",
      "Epoch 1 step 413: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 413: training loss: 31537.872759823164\n",
      "Epoch 1 step 414: training accuarcy: 0.7648\n",
      "Epoch 1 step 414: training loss: 30565.808989369092\n",
      "Epoch 1 step 415: training accuarcy: 0.7727\n",
      "Epoch 1 step 415: training loss: 31063.562765507428\n",
      "Epoch 1 step 416: training accuarcy: 0.761\n",
      "Epoch 1 step 416: training loss: 30995.418105416487\n",
      "Epoch 1 step 417: training accuarcy: 0.7652\n",
      "Epoch 1 step 417: training loss: 30685.60967283623\n",
      "Epoch 1 step 418: training accuarcy: 0.7811\n",
      "Epoch 1 step 418: training loss: 30746.85361680783\n",
      "Epoch 1 step 419: training accuarcy: 0.773\n",
      "Epoch 1 step 419: training loss: 30786.69762689219\n",
      "Epoch 1 step 420: training accuarcy: 0.779\n",
      "Epoch 1 step 420: training loss: 31221.5897492686\n",
      "Epoch 1 step 421: training accuarcy: 0.7666000000000001\n",
      "Epoch 1 step 421: training loss: 30652.186061481174\n",
      "Epoch 1 step 422: training accuarcy: 0.7698\n",
      "Epoch 1 step 422: training loss: 30726.35386600817\n",
      "Epoch 1 step 423: training accuarcy: 0.7692\n",
      "Epoch 1 step 423: training loss: 31182.487390992454\n",
      "Epoch 1 step 424: training accuarcy: 0.761\n",
      "Epoch 1 step 424: training loss: 31205.40822959424\n",
      "Epoch 1 step 425: training accuarcy: 0.7578\n",
      "Epoch 1 step 425: training loss: 30845.759116453595\n",
      "Epoch 1 step 426: training accuarcy: 0.7799\n",
      "Epoch 1 step 426: training loss: 31712.512698770137\n",
      "Epoch 1 step 427: training accuarcy: 0.7479\n",
      "Epoch 1 step 427: training loss: 30511.92565827003\n",
      "Epoch 1 step 428: training accuarcy: 0.7715000000000001\n",
      "Epoch 1 step 428: training loss: 30776.138689504784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 429: training accuarcy: 0.7694000000000001\n",
      "Epoch 1 step 429: training loss: 31576.346707594264\n",
      "Epoch 1 step 430: training accuarcy: 0.7592\n",
      "Epoch 1 step 430: training loss: 31232.155290046936\n",
      "Epoch 1 step 431: training accuarcy: 0.7638\n",
      "Epoch 1 step 431: training loss: 31054.72281708138\n",
      "Epoch 1 step 432: training accuarcy: 0.7674000000000001\n",
      "Epoch 1 step 432: training loss: 30774.843264047104\n",
      "Epoch 1 step 433: training accuarcy: 0.7667\n",
      "Epoch 1 step 433: training loss: 30815.03723026877\n",
      "Epoch 1 step 434: training accuarcy: 0.7647\n",
      "Epoch 1 step 434: training loss: 30262.529443554227\n",
      "Epoch 1 step 435: training accuarcy: 0.7756000000000001\n",
      "Epoch 1 step 435: training loss: 30784.972607255135\n",
      "Epoch 1 step 436: training accuarcy: 0.769\n",
      "Epoch 1 step 436: training loss: 31040.70105093181\n",
      "Epoch 1 step 437: training accuarcy: 0.7639\n",
      "Epoch 1 step 437: training loss: 30499.637345536426\n",
      "Epoch 1 step 438: training accuarcy: 0.765\n",
      "Epoch 1 step 438: training loss: 30841.3660873036\n",
      "Epoch 1 step 439: training accuarcy: 0.7707\n",
      "Epoch 1 step 439: training loss: 30267.391535003673\n",
      "Epoch 1 step 440: training accuarcy: 0.7847000000000001\n",
      "Epoch 1 step 440: training loss: 30382.236977774584\n",
      "Epoch 1 step 441: training accuarcy: 0.7723\n",
      "Epoch 1 step 441: training loss: 31159.844297741256\n",
      "Epoch 1 step 442: training accuarcy: 0.7679\n",
      "Epoch 1 step 442: training loss: 30329.59088668149\n",
      "Epoch 1 step 443: training accuarcy: 0.769\n",
      "Epoch 1 step 443: training loss: 30723.102829506213\n",
      "Epoch 1 step 444: training accuarcy: 0.7641\n",
      "Epoch 1 step 444: training loss: 30532.69233711214\n",
      "Epoch 1 step 445: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 445: training loss: 30699.779613326173\n",
      "Epoch 1 step 446: training accuarcy: 0.7697\n",
      "Epoch 1 step 446: training loss: 30280.358940608865\n",
      "Epoch 1 step 447: training accuarcy: 0.78\n",
      "Epoch 1 step 447: training loss: 30392.028768327535\n",
      "Epoch 1 step 448: training accuarcy: 0.7755000000000001\n",
      "Epoch 1 step 448: training loss: 30695.219019504846\n",
      "Epoch 1 step 449: training accuarcy: 0.7731\n",
      "Epoch 1 step 449: training loss: 30196.882568575984\n",
      "Epoch 1 step 450: training accuarcy: 0.7729\n",
      "Epoch 1 step 450: training loss: 30788.94533554171\n",
      "Epoch 1 step 451: training accuarcy: 0.7651\n",
      "Epoch 1 step 451: training loss: 30714.31262146528\n",
      "Epoch 1 step 452: training accuarcy: 0.7708\n",
      "Epoch 1 step 452: training loss: 30532.429557966338\n",
      "Epoch 1 step 453: training accuarcy: 0.7697\n",
      "Epoch 1 step 453: training loss: 30652.193955050803\n",
      "Epoch 1 step 454: training accuarcy: 0.7677\n",
      "Epoch 1 step 454: training loss: 30917.751680833706\n",
      "Epoch 1 step 455: training accuarcy: 0.7669\n",
      "Epoch 1 step 455: training loss: 30818.7373468453\n",
      "Epoch 1 step 456: training accuarcy: 0.7648\n",
      "Epoch 1 step 456: training loss: 30577.233767858474\n",
      "Epoch 1 step 457: training accuarcy: 0.769\n",
      "Epoch 1 step 457: training loss: 30444.12135944697\n",
      "Epoch 1 step 458: training accuarcy: 0.7731\n",
      "Epoch 1 step 458: training loss: 30409.2201706168\n",
      "Epoch 1 step 459: training accuarcy: 0.7759\n",
      "Epoch 1 step 459: training loss: 30556.886715626984\n",
      "Epoch 1 step 460: training accuarcy: 0.7683\n",
      "Epoch 1 step 460: training loss: 30386.63804449131\n",
      "Epoch 1 step 461: training accuarcy: 0.7747\n",
      "Epoch 1 step 461: training loss: 30305.148046035567\n",
      "Epoch 1 step 462: training accuarcy: 0.7686000000000001\n",
      "Epoch 1 step 462: training loss: 30348.922135911384\n",
      "Epoch 1 step 463: training accuarcy: 0.7763\n",
      "Epoch 1 step 463: training loss: 30509.21013050514\n",
      "Epoch 1 step 464: training accuarcy: 0.7663000000000001\n",
      "Epoch 1 step 464: training loss: 30587.5847518721\n",
      "Epoch 1 step 465: training accuarcy: 0.762\n",
      "Epoch 1 step 465: training loss: 30230.99828171007\n",
      "Epoch 1 step 466: training accuarcy: 0.7726000000000001\n",
      "Epoch 1 step 466: training loss: 30536.249750471296\n",
      "Epoch 1 step 467: training accuarcy: 0.7787000000000001\n",
      "Epoch 1 step 467: training loss: 30686.588086274845\n",
      "Epoch 1 step 468: training accuarcy: 0.7679\n",
      "Epoch 1 step 468: training loss: 30002.03097074968\n",
      "Epoch 1 step 469: training accuarcy: 0.7753\n",
      "Epoch 1 step 469: training loss: 30392.535026608573\n",
      "Epoch 1 step 470: training accuarcy: 0.7729\n",
      "Epoch 1 step 470: training loss: 30836.90114340756\n",
      "Epoch 1 step 471: training accuarcy: 0.7587\n",
      "Epoch 1 step 471: training loss: 30082.809883708873\n",
      "Epoch 1 step 472: training accuarcy: 0.7735000000000001\n",
      "Epoch 1 step 472: training loss: 30396.83523516943\n",
      "Epoch 1 step 473: training accuarcy: 0.7719\n",
      "Epoch 1 step 473: training loss: 30771.28427944635\n",
      "Epoch 1 step 474: training accuarcy: 0.7669\n",
      "Epoch 1 step 474: training loss: 30882.27545266177\n",
      "Epoch 1 step 475: training accuarcy: 0.7578\n",
      "Epoch 1 step 475: training loss: 30729.180178959883\n",
      "Epoch 1 step 476: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 476: training loss: 31417.09069917605\n",
      "Epoch 1 step 477: training accuarcy: 0.7589\n",
      "Epoch 1 step 477: training loss: 29985.772560471418\n",
      "Epoch 1 step 478: training accuarcy: 0.7781\n",
      "Epoch 1 step 478: training loss: 30428.630019516626\n",
      "Epoch 1 step 479: training accuarcy: 0.7644000000000001\n",
      "Epoch 1 step 479: training loss: 30021.358985297276\n",
      "Epoch 1 step 480: training accuarcy: 0.7839\n",
      "Epoch 1 step 480: training loss: 30518.129797998437\n",
      "Epoch 1 step 481: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 481: training loss: 30102.619016039396\n",
      "Epoch 1 step 482: training accuarcy: 0.7749\n",
      "Epoch 1 step 482: training loss: 30640.631277020842\n",
      "Epoch 1 step 483: training accuarcy: 0.7672\n",
      "Epoch 1 step 483: training loss: 30582.081468472497\n",
      "Epoch 1 step 484: training accuarcy: 0.7689\n",
      "Epoch 1 step 484: training loss: 30712.367867200443\n",
      "Epoch 1 step 485: training accuarcy: 0.7631\n",
      "Epoch 1 step 485: training loss: 30522.13727158122\n",
      "Epoch 1 step 486: training accuarcy: 0.7702\n",
      "Epoch 1 step 486: training loss: 30415.34179197301\n",
      "Epoch 1 step 487: training accuarcy: 0.7729\n",
      "Epoch 1 step 487: training loss: 30627.437591016766\n",
      "Epoch 1 step 488: training accuarcy: 0.769\n",
      "Epoch 1 step 488: training loss: 30259.193470072343\n",
      "Epoch 1 step 489: training accuarcy: 0.773\n",
      "Epoch 1 step 489: training loss: 29826.206431885632\n",
      "Epoch 1 step 490: training accuarcy: 0.7752\n",
      "Epoch 1 step 490: training loss: 30373.234578079144\n",
      "Epoch 1 step 491: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 491: training loss: 30076.757735048155\n",
      "Epoch 1 step 492: training accuarcy: 0.78\n",
      "Epoch 1 step 492: training loss: 30278.694868233295\n",
      "Epoch 1 step 493: training accuarcy: 0.7769\n",
      "Epoch 1 step 493: training loss: 30169.81773775445\n",
      "Epoch 1 step 494: training accuarcy: 0.7686000000000001\n",
      "Epoch 1 step 494: training loss: 29864.934555699492\n",
      "Epoch 1 step 495: training accuarcy: 0.7817000000000001\n",
      "Epoch 1 step 495: training loss: 30225.517097026466\n",
      "Epoch 1 step 496: training accuarcy: 0.7738\n",
      "Epoch 1 step 496: training loss: 30258.10792005444\n",
      "Epoch 1 step 497: training accuarcy: 0.7754000000000001\n",
      "Epoch 1 step 497: training loss: 29890.021164098733\n",
      "Epoch 1 step 498: training accuarcy: 0.786\n",
      "Epoch 1 step 498: training loss: 30290.506509567425\n",
      "Epoch 1 step 499: training accuarcy: 0.77\n",
      "Epoch 1 step 499: training loss: 29937.46273652114\n",
      "Epoch 1 step 500: training accuarcy: 0.7759\n",
      "Epoch 1 step 500: training loss: 31125.13714004818\n",
      "Epoch 1 step 501: training accuarcy: 0.7634000000000001\n",
      "Epoch 1 step 501: training loss: 30054.354080233024\n",
      "Epoch 1 step 502: training accuarcy: 0.7732\n",
      "Epoch 1 step 502: training loss: 30558.47607828644\n",
      "Epoch 1 step 503: training accuarcy: 0.7659\n",
      "Epoch 1 step 503: training loss: 29755.516404817714\n",
      "Epoch 1 step 504: training accuarcy: 0.7791\n",
      "Epoch 1 step 504: training loss: 30007.321599230847\n",
      "Epoch 1 step 505: training accuarcy: 0.7799\n",
      "Epoch 1 step 505: training loss: 30031.21894262087\n",
      "Epoch 1 step 506: training accuarcy: 0.7761\n",
      "Epoch 1 step 506: training loss: 29874.64737304374\n",
      "Epoch 1 step 507: training accuarcy: 0.777\n",
      "Epoch 1 step 507: training loss: 30332.660131711455\n",
      "Epoch 1 step 508: training accuarcy: 0.7737\n",
      "Epoch 1 step 508: training loss: 30321.42983256355\n",
      "Epoch 1 step 509: training accuarcy: 0.7677\n",
      "Epoch 1 step 509: training loss: 30389.08744728059\n",
      "Epoch 1 step 510: training accuarcy: 0.7726000000000001\n",
      "Epoch 1 step 510: training loss: 30486.7307010176\n",
      "Epoch 1 step 511: training accuarcy: 0.7622\n",
      "Epoch 1 step 511: training loss: 30167.813032167265\n",
      "Epoch 1 step 512: training accuarcy: 0.7677\n",
      "Epoch 1 step 512: training loss: 30231.842841948674\n",
      "Epoch 1 step 513: training accuarcy: 0.7727\n",
      "Epoch 1 step 513: training loss: 30298.75123263509\n",
      "Epoch 1 step 514: training accuarcy: 0.7773\n",
      "Epoch 1 step 514: training loss: 30291.555082274008\n",
      "Epoch 1 step 515: training accuarcy: 0.7682\n",
      "Epoch 1 step 515: training loss: 29753.810008661727\n",
      "Epoch 1 step 516: training accuarcy: 0.7805000000000001\n",
      "Epoch 1 step 516: training loss: 30225.881385176515\n",
      "Epoch 1 step 517: training accuarcy: 0.7747\n",
      "Epoch 1 step 517: training loss: 29906.184142027236\n",
      "Epoch 1 step 518: training accuarcy: 0.774\n",
      "Epoch 1 step 518: training loss: 29605.025573338407\n",
      "Epoch 1 step 519: training accuarcy: 0.7803\n",
      "Epoch 1 step 519: training loss: 30821.358642994215\n",
      "Epoch 1 step 520: training accuarcy: 0.7602\n",
      "Epoch 1 step 520: training loss: 29736.963569083877\n",
      "Epoch 1 step 521: training accuarcy: 0.7771\n",
      "Epoch 1 step 521: training loss: 29822.423405976493\n",
      "Epoch 1 step 522: training accuarcy: 0.7794000000000001\n",
      "Epoch 1 step 522: training loss: 30586.114514511897\n",
      "Epoch 1 step 523: training accuarcy: 0.7671\n",
      "Epoch 1 step 523: training loss: 30009.887945643848\n",
      "Epoch 1 step 524: training accuarcy: 0.7738\n",
      "Epoch 1 step 524: training loss: 30476.615864347776\n",
      "Epoch 1 step 525: training accuarcy: 0.7753\n",
      "Epoch 1 step 525: training loss: 13834.57346807209\n",
      "Epoch 1 step 526: training accuarcy: 0.7774358974358975\n",
      "Epoch 1: train loss 31221.192366396055, train accuarcy 0.7492716312408447\n",
      "Epoch 1: valid loss 28757.06106598878, valid accuarcy 0.7869622111320496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [10:13<05:05, 305.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 29444.827389827682\n",
      "Epoch 2 step 527: training accuarcy: 0.7862\n",
      "Epoch 2 step 527: training loss: 28523.125714488488\n",
      "Epoch 2 step 528: training accuarcy: 0.8022\n",
      "Epoch 2 step 528: training loss: 29318.442319866088\n",
      "Epoch 2 step 529: training accuarcy: 0.7877000000000001\n",
      "Epoch 2 step 529: training loss: 29288.732426444403\n",
      "Epoch 2 step 530: training accuarcy: 0.7866000000000001\n",
      "Epoch 2 step 530: training loss: 28845.008535668574\n",
      "Epoch 2 step 531: training accuarcy: 0.7902\n",
      "Epoch 2 step 531: training loss: 29109.83316338543\n",
      "Epoch 2 step 532: training accuarcy: 0.7919\n",
      "Epoch 2 step 532: training loss: 29101.957448656427\n",
      "Epoch 2 step 533: training accuarcy: 0.79\n",
      "Epoch 2 step 533: training loss: 29421.874596014342\n",
      "Epoch 2 step 534: training accuarcy: 0.7853\n",
      "Epoch 2 step 534: training loss: 29064.878122278165\n",
      "Epoch 2 step 535: training accuarcy: 0.7874\n",
      "Epoch 2 step 535: training loss: 29181.4835376725\n",
      "Epoch 2 step 536: training accuarcy: 0.7942\n",
      "Epoch 2 step 536: training loss: 29453.53374325736\n",
      "Epoch 2 step 537: training accuarcy: 0.7805000000000001\n",
      "Epoch 2 step 537: training loss: 29878.02809153224\n",
      "Epoch 2 step 538: training accuarcy: 0.7834\n",
      "Epoch 2 step 538: training loss: 29364.450210509647\n",
      "Epoch 2 step 539: training accuarcy: 0.7886000000000001\n",
      "Epoch 2 step 539: training loss: 29063.07266618019\n",
      "Epoch 2 step 540: training accuarcy: 0.7896000000000001\n",
      "Epoch 2 step 540: training loss: 29185.05185825834\n",
      "Epoch 2 step 541: training accuarcy: 0.7881\n",
      "Epoch 2 step 541: training loss: 29090.051557095296\n",
      "Epoch 2 step 542: training accuarcy: 0.7922\n",
      "Epoch 2 step 542: training loss: 29402.42274992395\n",
      "Epoch 2 step 543: training accuarcy: 0.7823\n",
      "Epoch 2 step 543: training loss: 29239.86144547651\n",
      "Epoch 2 step 544: training accuarcy: 0.7948000000000001\n",
      "Epoch 2 step 544: training loss: 29822.61274772259\n",
      "Epoch 2 step 545: training accuarcy: 0.7758\n",
      "Epoch 2 step 545: training loss: 29580.487478265546\n",
      "Epoch 2 step 546: training accuarcy: 0.7872\n",
      "Epoch 2 step 546: training loss: 29696.443725420406\n",
      "Epoch 2 step 547: training accuarcy: 0.7811\n",
      "Epoch 2 step 547: training loss: 29324.56057661057\n",
      "Epoch 2 step 548: training accuarcy: 0.7854\n",
      "Epoch 2 step 548: training loss: 29409.46637440659\n",
      "Epoch 2 step 549: training accuarcy: 0.7813\n",
      "Epoch 2 step 549: training loss: 29072.908467389243\n",
      "Epoch 2 step 550: training accuarcy: 0.7909\n",
      "Epoch 2 step 550: training loss: 28985.58704631179\n",
      "Epoch 2 step 551: training accuarcy: 0.788\n",
      "Epoch 2 step 551: training loss: 29386.975939971904\n",
      "Epoch 2 step 552: training accuarcy: 0.7821\n",
      "Epoch 2 step 552: training loss: 29699.026946826583\n",
      "Epoch 2 step 553: training accuarcy: 0.7777000000000001\n",
      "Epoch 2 step 553: training loss: 29766.73525826648\n",
      "Epoch 2 step 554: training accuarcy: 0.783\n",
      "Epoch 2 step 554: training loss: 29514.416579993056\n",
      "Epoch 2 step 555: training accuarcy: 0.7902\n",
      "Epoch 2 step 555: training loss: 29359.54013128171\n",
      "Epoch 2 step 556: training accuarcy: 0.7856000000000001\n",
      "Epoch 2 step 556: training loss: 29332.708258187016\n",
      "Epoch 2 step 557: training accuarcy: 0.786\n",
      "Epoch 2 step 557: training loss: 28989.724934936043\n",
      "Epoch 2 step 558: training accuarcy: 0.7895000000000001\n",
      "Epoch 2 step 558: training loss: 28970.488273861185\n",
      "Epoch 2 step 559: training accuarcy: 0.7955\n",
      "Epoch 2 step 559: training loss: 29317.68700580615\n",
      "Epoch 2 step 560: training accuarcy: 0.7942\n",
      "Epoch 2 step 560: training loss: 29525.59420805888\n",
      "Epoch 2 step 561: training accuarcy: 0.7822\n",
      "Epoch 2 step 561: training loss: 29077.119154715725\n",
      "Epoch 2 step 562: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 562: training loss: 29037.698445381186\n",
      "Epoch 2 step 563: training accuarcy: 0.7945\n",
      "Epoch 2 step 563: training loss: 29194.61885738949\n",
      "Epoch 2 step 564: training accuarcy: 0.7905000000000001\n",
      "Epoch 2 step 564: training loss: 29203.167281447048\n",
      "Epoch 2 step 565: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 565: training loss: 28902.256859953817\n",
      "Epoch 2 step 566: training accuarcy: 0.7896000000000001\n",
      "Epoch 2 step 566: training loss: 29818.504248136556\n",
      "Epoch 2 step 567: training accuarcy: 0.7768\n",
      "Epoch 2 step 567: training loss: 29163.08278269532\n",
      "Epoch 2 step 568: training accuarcy: 0.7863\n",
      "Epoch 2 step 568: training loss: 28862.51744139387\n",
      "Epoch 2 step 569: training accuarcy: 0.7994\n",
      "Epoch 2 step 569: training loss: 29266.87527688684\n",
      "Epoch 2 step 570: training accuarcy: 0.7854\n",
      "Epoch 2 step 570: training loss: 29590.033780018297\n",
      "Epoch 2 step 571: training accuarcy: 0.7912\n",
      "Epoch 2 step 571: training loss: 29263.30625840515\n",
      "Epoch 2 step 572: training accuarcy: 0.7814\n",
      "Epoch 2 step 572: training loss: 29318.87829814509\n",
      "Epoch 2 step 573: training accuarcy: 0.7803\n",
      "Epoch 2 step 573: training loss: 29823.69713776901\n",
      "Epoch 2 step 574: training accuarcy: 0.7783\n",
      "Epoch 2 step 574: training loss: 29174.78097912875\n",
      "Epoch 2 step 575: training accuarcy: 0.7898000000000001\n",
      "Epoch 2 step 575: training loss: 29080.14253402895\n",
      "Epoch 2 step 576: training accuarcy: 0.7889\n",
      "Epoch 2 step 576: training loss: 29166.45118001599\n",
      "Epoch 2 step 577: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 577: training loss: 29869.5542285227\n",
      "Epoch 2 step 578: training accuarcy: 0.7785000000000001\n",
      "Epoch 2 step 578: training loss: 29309.871692875466\n",
      "Epoch 2 step 579: training accuarcy: 0.7818\n",
      "Epoch 2 step 579: training loss: 29017.131526877478\n",
      "Epoch 2 step 580: training accuarcy: 0.7906000000000001\n",
      "Epoch 2 step 580: training loss: 29242.479782615395\n",
      "Epoch 2 step 581: training accuarcy: 0.7818\n",
      "Epoch 2 step 581: training loss: 29468.993074662445\n",
      "Epoch 2 step 582: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 582: training loss: 29358.616156332006\n",
      "Epoch 2 step 583: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 583: training loss: 29157.314818471612\n",
      "Epoch 2 step 584: training accuarcy: 0.7853\n",
      "Epoch 2 step 584: training loss: 29425.503962265822\n",
      "Epoch 2 step 585: training accuarcy: 0.7779\n",
      "Epoch 2 step 585: training loss: 28944.832611514812\n",
      "Epoch 2 step 586: training accuarcy: 0.7811\n",
      "Epoch 2 step 586: training loss: 29672.840451559314\n",
      "Epoch 2 step 587: training accuarcy: 0.7829\n",
      "Epoch 2 step 587: training loss: 29568.8348934746\n",
      "Epoch 2 step 588: training accuarcy: 0.7797000000000001\n",
      "Epoch 2 step 588: training loss: 29555.95698196651\n",
      "Epoch 2 step 589: training accuarcy: 0.7779\n",
      "Epoch 2 step 589: training loss: 28955.891374639265\n",
      "Epoch 2 step 590: training accuarcy: 0.7913\n",
      "Epoch 2 step 590: training loss: 29417.595100384737\n",
      "Epoch 2 step 591: training accuarcy: 0.7838\n",
      "Epoch 2 step 591: training loss: 29026.675532994454\n",
      "Epoch 2 step 592: training accuarcy: 0.7852\n",
      "Epoch 2 step 592: training loss: 28817.48163878658\n",
      "Epoch 2 step 593: training accuarcy: 0.7861\n",
      "Epoch 2 step 593: training loss: 28825.52660954916\n",
      "Epoch 2 step 594: training accuarcy: 0.7944\n",
      "Epoch 2 step 594: training loss: 29199.067921380036\n",
      "Epoch 2 step 595: training accuarcy: 0.7803\n",
      "Epoch 2 step 595: training loss: 29311.894953481373\n",
      "Epoch 2 step 596: training accuarcy: 0.7812\n",
      "Epoch 2 step 596: training loss: 29041.751685752275\n",
      "Epoch 2 step 597: training accuarcy: 0.7841\n",
      "Epoch 2 step 597: training loss: 29458.40383998506\n",
      "Epoch 2 step 598: training accuarcy: 0.79\n",
      "Epoch 2 step 598: training loss: 29262.51077433983\n",
      "Epoch 2 step 599: training accuarcy: 0.7744000000000001\n",
      "Epoch 2 step 599: training loss: 28820.153754717518\n",
      "Epoch 2 step 600: training accuarcy: 0.7882\n",
      "Epoch 2 step 600: training loss: 29630.482056725687\n",
      "Epoch 2 step 601: training accuarcy: 0.7763\n",
      "Epoch 2 step 601: training loss: 29381.401671161304\n",
      "Epoch 2 step 602: training accuarcy: 0.7834\n",
      "Epoch 2 step 602: training loss: 29078.169890520305\n",
      "Epoch 2 step 603: training accuarcy: 0.7849\n",
      "Epoch 2 step 603: training loss: 29152.520053204484\n",
      "Epoch 2 step 604: training accuarcy: 0.7845000000000001\n",
      "Epoch 2 step 604: training loss: 29613.227843468852\n",
      "Epoch 2 step 605: training accuarcy: 0.7785000000000001\n",
      "Epoch 2 step 605: training loss: 29207.367798351494\n",
      "Epoch 2 step 606: training accuarcy: 0.7871\n",
      "Epoch 2 step 606: training loss: 29047.818569122064\n",
      "Epoch 2 step 607: training accuarcy: 0.7833\n",
      "Epoch 2 step 607: training loss: 29749.007707717854\n",
      "Epoch 2 step 608: training accuarcy: 0.7689\n",
      "Epoch 2 step 608: training loss: 29166.480280921758\n",
      "Epoch 2 step 609: training accuarcy: 0.7785000000000001\n",
      "Epoch 2 step 609: training loss: 29393.46329736679\n",
      "Epoch 2 step 610: training accuarcy: 0.7835000000000001\n",
      "Epoch 2 step 610: training loss: 29467.41881039033\n",
      "Epoch 2 step 611: training accuarcy: 0.778\n",
      "Epoch 2 step 611: training loss: 29608.2089999082\n",
      "Epoch 2 step 612: training accuarcy: 0.7785000000000001\n",
      "Epoch 2 step 612: training loss: 29824.281984113375\n",
      "Epoch 2 step 613: training accuarcy: 0.7704000000000001\n",
      "Epoch 2 step 613: training loss: 28741.93647236604\n",
      "Epoch 2 step 614: training accuarcy: 0.786\n",
      "Epoch 2 step 614: training loss: 29166.909862043896\n",
      "Epoch 2 step 615: training accuarcy: 0.7808\n",
      "Epoch 2 step 615: training loss: 29332.23251420685\n",
      "Epoch 2 step 616: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 616: training loss: 29218.20570179459\n",
      "Epoch 2 step 617: training accuarcy: 0.7796000000000001\n",
      "Epoch 2 step 617: training loss: 29429.351695108195\n",
      "Epoch 2 step 618: training accuarcy: 0.7812\n",
      "Epoch 2 step 618: training loss: 28766.813896309814\n",
      "Epoch 2 step 619: training accuarcy: 0.7888000000000001\n",
      "Epoch 2 step 619: training loss: 29345.246333567924\n",
      "Epoch 2 step 620: training accuarcy: 0.7878000000000001\n",
      "Epoch 2 step 620: training loss: 29123.18777994977\n",
      "Epoch 2 step 621: training accuarcy: 0.7844\n",
      "Epoch 2 step 621: training loss: 29523.13598342963\n",
      "Epoch 2 step 622: training accuarcy: 0.776\n",
      "Epoch 2 step 622: training loss: 28523.815924780592\n",
      "Epoch 2 step 623: training accuarcy: 0.7991\n",
      "Epoch 2 step 623: training loss: 29010.160017725913\n",
      "Epoch 2 step 624: training accuarcy: 0.7834\n",
      "Epoch 2 step 624: training loss: 29601.254947516758\n",
      "Epoch 2 step 625: training accuarcy: 0.7771\n",
      "Epoch 2 step 625: training loss: 29261.51096835529\n",
      "Epoch 2 step 626: training accuarcy: 0.7859\n",
      "Epoch 2 step 626: training loss: 29092.091829184043\n",
      "Epoch 2 step 627: training accuarcy: 0.7807000000000001\n",
      "Epoch 2 step 627: training loss: 28748.515282539407\n",
      "Epoch 2 step 628: training accuarcy: 0.7834\n",
      "Epoch 2 step 628: training loss: 29284.23322661876\n",
      "Epoch 2 step 629: training accuarcy: 0.7778\n",
      "Epoch 2 step 629: training loss: 29229.571548893702\n",
      "Epoch 2 step 630: training accuarcy: 0.7862\n",
      "Epoch 2 step 630: training loss: 28663.48667079176\n",
      "Epoch 2 step 631: training accuarcy: 0.7908000000000001\n",
      "Epoch 2 step 631: training loss: 29009.16800230555\n",
      "Epoch 2 step 632: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 632: training loss: 29691.22086094047\n",
      "Epoch 2 step 633: training accuarcy: 0.7852\n",
      "Epoch 2 step 633: training loss: 28848.308027448755\n",
      "Epoch 2 step 634: training accuarcy: 0.7825000000000001\n",
      "Epoch 2 step 634: training loss: 28463.432356462563\n",
      "Epoch 2 step 635: training accuarcy: 0.7854\n",
      "Epoch 2 step 635: training loss: 29442.638063916904\n",
      "Epoch 2 step 636: training accuarcy: 0.78\n",
      "Epoch 2 step 636: training loss: 28938.662051492196\n",
      "Epoch 2 step 637: training accuarcy: 0.7854\n",
      "Epoch 2 step 637: training loss: 29134.0690355818\n",
      "Epoch 2 step 638: training accuarcy: 0.7763\n",
      "Epoch 2 step 638: training loss: 29751.67044448101\n",
      "Epoch 2 step 639: training accuarcy: 0.7686000000000001\n",
      "Epoch 2 step 639: training loss: 29325.951542394425\n",
      "Epoch 2 step 640: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 640: training loss: 28682.444494799878\n",
      "Epoch 2 step 641: training accuarcy: 0.7869\n",
      "Epoch 2 step 641: training loss: 29340.043439750203\n",
      "Epoch 2 step 642: training accuarcy: 0.7828\n",
      "Epoch 2 step 642: training loss: 28850.207028881585\n",
      "Epoch 2 step 643: training accuarcy: 0.7891\n",
      "Epoch 2 step 643: training loss: 29330.425320176044\n",
      "Epoch 2 step 644: training accuarcy: 0.7803\n",
      "Epoch 2 step 644: training loss: 28403.428877573322\n",
      "Epoch 2 step 645: training accuarcy: 0.7976000000000001\n",
      "Epoch 2 step 645: training loss: 30027.22205800142\n",
      "Epoch 2 step 646: training accuarcy: 0.7689\n",
      "Epoch 2 step 646: training loss: 28844.761903004503\n",
      "Epoch 2 step 647: training accuarcy: 0.7883\n",
      "Epoch 2 step 647: training loss: 29344.495084766102\n",
      "Epoch 2 step 648: training accuarcy: 0.7725000000000001\n",
      "Epoch 2 step 648: training loss: 29232.597810307798\n",
      "Epoch 2 step 649: training accuarcy: 0.7821\n",
      "Epoch 2 step 649: training loss: 29208.170726144996\n",
      "Epoch 2 step 650: training accuarcy: 0.7776000000000001\n",
      "Epoch 2 step 650: training loss: 29788.36120915086\n",
      "Epoch 2 step 651: training accuarcy: 0.7675000000000001\n",
      "Epoch 2 step 651: training loss: 29079.15210815913\n",
      "Epoch 2 step 652: training accuarcy: 0.7794000000000001\n",
      "Epoch 2 step 652: training loss: 29069.55006978442\n",
      "Epoch 2 step 653: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 653: training loss: 29004.526924429847\n",
      "Epoch 2 step 654: training accuarcy: 0.7892\n",
      "Epoch 2 step 654: training loss: 28712.149169071923\n",
      "Epoch 2 step 655: training accuarcy: 0.7892\n",
      "Epoch 2 step 655: training loss: 29203.28988894337\n",
      "Epoch 2 step 656: training accuarcy: 0.7833\n",
      "Epoch 2 step 656: training loss: 29209.852577134363\n",
      "Epoch 2 step 657: training accuarcy: 0.7817000000000001\n",
      "Epoch 2 step 657: training loss: 28842.762337529726\n",
      "Epoch 2 step 658: training accuarcy: 0.7908000000000001\n",
      "Epoch 2 step 658: training loss: 29129.45809284199\n",
      "Epoch 2 step 659: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 659: training loss: 29032.441870628896\n",
      "Epoch 2 step 660: training accuarcy: 0.7866000000000001\n",
      "Epoch 2 step 660: training loss: 29496.666599277392\n",
      "Epoch 2 step 661: training accuarcy: 0.7792\n",
      "Epoch 2 step 661: training loss: 28494.627662823983\n",
      "Epoch 2 step 662: training accuarcy: 0.7959\n",
      "Epoch 2 step 662: training loss: 28626.610918150454\n",
      "Epoch 2 step 663: training accuarcy: 0.7911\n",
      "Epoch 2 step 663: training loss: 28937.099985190333\n",
      "Epoch 2 step 664: training accuarcy: 0.7895000000000001\n",
      "Epoch 2 step 664: training loss: 29310.639193375955\n",
      "Epoch 2 step 665: training accuarcy: 0.7807000000000001\n",
      "Epoch 2 step 665: training loss: 29022.926880787934\n",
      "Epoch 2 step 666: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 666: training loss: 29329.770623871314\n",
      "Epoch 2 step 667: training accuarcy: 0.776\n",
      "Epoch 2 step 667: training loss: 28974.332612925107\n",
      "Epoch 2 step 668: training accuarcy: 0.7859\n",
      "Epoch 2 step 668: training loss: 28558.45295577674\n",
      "Epoch 2 step 669: training accuarcy: 0.7758\n",
      "Epoch 2 step 669: training loss: 29154.645394302726\n",
      "Epoch 2 step 670: training accuarcy: 0.7892\n",
      "Epoch 2 step 670: training loss: 28769.11758166853\n",
      "Epoch 2 step 671: training accuarcy: 0.7877000000000001\n",
      "Epoch 2 step 671: training loss: 28715.042710195063\n",
      "Epoch 2 step 672: training accuarcy: 0.791\n",
      "Epoch 2 step 672: training loss: 29260.44041861438\n",
      "Epoch 2 step 673: training accuarcy: 0.7728\n",
      "Epoch 2 step 673: training loss: 28900.987905804897\n",
      "Epoch 2 step 674: training accuarcy: 0.7795000000000001\n",
      "Epoch 2 step 674: training loss: 29005.514637805547\n",
      "Epoch 2 step 675: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 675: training loss: 28928.442803494974\n",
      "Epoch 2 step 676: training accuarcy: 0.7821\n",
      "Epoch 2 step 676: training loss: 28692.709350711124\n",
      "Epoch 2 step 677: training accuarcy: 0.794\n",
      "Epoch 2 step 677: training loss: 29036.13198568846\n",
      "Epoch 2 step 678: training accuarcy: 0.7827000000000001\n",
      "Epoch 2 step 678: training loss: 28493.956733851424\n",
      "Epoch 2 step 679: training accuarcy: 0.7933\n",
      "Epoch 2 step 679: training loss: 28229.806701503447\n",
      "Epoch 2 step 680: training accuarcy: 0.7929\n",
      "Epoch 2 step 680: training loss: 28983.894536384047\n",
      "Epoch 2 step 681: training accuarcy: 0.7814\n",
      "Epoch 2 step 681: training loss: 29189.20759762916\n",
      "Epoch 2 step 682: training accuarcy: 0.7821\n",
      "Epoch 2 step 682: training loss: 28474.08652218888\n",
      "Epoch 2 step 683: training accuarcy: 0.7894\n",
      "Epoch 2 step 683: training loss: 28969.017622066218\n",
      "Epoch 2 step 684: training accuarcy: 0.7804\n",
      "Epoch 2 step 684: training loss: 29374.18733473781\n",
      "Epoch 2 step 685: training accuarcy: 0.7772\n",
      "Epoch 2 step 685: training loss: 28816.229974363963\n",
      "Epoch 2 step 686: training accuarcy: 0.7825000000000001\n",
      "Epoch 2 step 686: training loss: 28439.806784752764\n",
      "Epoch 2 step 687: training accuarcy: 0.7928000000000001\n",
      "Epoch 2 step 687: training loss: 29098.181310649656\n",
      "Epoch 2 step 688: training accuarcy: 0.7842\n",
      "Epoch 2 step 688: training loss: 28766.481536053314\n",
      "Epoch 2 step 689: training accuarcy: 0.7781\n",
      "Epoch 2 step 689: training loss: 28384.118761261147\n",
      "Epoch 2 step 690: training accuarcy: 0.7918000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 690: training loss: 28597.40305173381\n",
      "Epoch 2 step 691: training accuarcy: 0.7861\n",
      "Epoch 2 step 691: training loss: 28287.801654713963\n",
      "Epoch 2 step 692: training accuarcy: 0.7956000000000001\n",
      "Epoch 2 step 692: training loss: 29244.290852748567\n",
      "Epoch 2 step 693: training accuarcy: 0.7778\n",
      "Epoch 2 step 693: training loss: 28655.785406814524\n",
      "Epoch 2 step 694: training accuarcy: 0.7754000000000001\n",
      "Epoch 2 step 694: training loss: 28746.35303665627\n",
      "Epoch 2 step 695: training accuarcy: 0.7852\n",
      "Epoch 2 step 695: training loss: 28817.037135293744\n",
      "Epoch 2 step 696: training accuarcy: 0.7873\n",
      "Epoch 2 step 696: training loss: 28885.19211856198\n",
      "Epoch 2 step 697: training accuarcy: 0.7854\n",
      "Epoch 2 step 697: training loss: 28467.659738975814\n",
      "Epoch 2 step 698: training accuarcy: 0.7874\n",
      "Epoch 2 step 698: training loss: 29266.5728954465\n",
      "Epoch 2 step 699: training accuarcy: 0.7695000000000001\n",
      "Epoch 2 step 699: training loss: 28353.769339563405\n",
      "Epoch 2 step 700: training accuarcy: 0.7932\n",
      "Epoch 2 step 700: training loss: 28290.250054959255\n",
      "Epoch 2 step 701: training accuarcy: 0.7952\n",
      "Epoch 2 step 701: training loss: 28773.895843726415\n",
      "Epoch 2 step 702: training accuarcy: 0.7957000000000001\n",
      "Epoch 2 step 702: training loss: 28743.637393879966\n",
      "Epoch 2 step 703: training accuarcy: 0.7853\n",
      "Epoch 2 step 703: training loss: 28248.28531792525\n",
      "Epoch 2 step 704: training accuarcy: 0.7869\n",
      "Epoch 2 step 704: training loss: 28282.18390829066\n",
      "Epoch 2 step 705: training accuarcy: 0.7894\n",
      "Epoch 2 step 705: training loss: 29198.213430329844\n",
      "Epoch 2 step 706: training accuarcy: 0.7824\n",
      "Epoch 2 step 706: training loss: 29271.697129179614\n",
      "Epoch 2 step 707: training accuarcy: 0.7797000000000001\n",
      "Epoch 2 step 707: training loss: 28644.396721033227\n",
      "Epoch 2 step 708: training accuarcy: 0.7844\n",
      "Epoch 2 step 708: training loss: 28776.19965283303\n",
      "Epoch 2 step 709: training accuarcy: 0.7821\n",
      "Epoch 2 step 709: training loss: 29300.43565913195\n",
      "Epoch 2 step 710: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 710: training loss: 28348.475130434603\n",
      "Epoch 2 step 711: training accuarcy: 0.7905000000000001\n",
      "Epoch 2 step 711: training loss: 29251.06650847407\n",
      "Epoch 2 step 712: training accuarcy: 0.7735000000000001\n",
      "Epoch 2 step 712: training loss: 28785.50547472757\n",
      "Epoch 2 step 713: training accuarcy: 0.7827000000000001\n",
      "Epoch 2 step 713: training loss: 28636.51055709015\n",
      "Epoch 2 step 714: training accuarcy: 0.7884\n",
      "Epoch 2 step 714: training loss: 28595.775964620665\n",
      "Epoch 2 step 715: training accuarcy: 0.7898000000000001\n",
      "Epoch 2 step 715: training loss: 29063.323060266885\n",
      "Epoch 2 step 716: training accuarcy: 0.7862\n",
      "Epoch 2 step 716: training loss: 28585.504333468634\n",
      "Epoch 2 step 717: training accuarcy: 0.7873\n",
      "Epoch 2 step 717: training loss: 28981.736375586104\n",
      "Epoch 2 step 718: training accuarcy: 0.7792\n",
      "Epoch 2 step 718: training loss: 29302.309080758674\n",
      "Epoch 2 step 719: training accuarcy: 0.7823\n",
      "Epoch 2 step 719: training loss: 28245.892666200365\n",
      "Epoch 2 step 720: training accuarcy: 0.7874\n",
      "Epoch 2 step 720: training loss: 28688.046366039278\n",
      "Epoch 2 step 721: training accuarcy: 0.7906000000000001\n",
      "Epoch 2 step 721: training loss: 28663.8036260292\n",
      "Epoch 2 step 722: training accuarcy: 0.7934\n",
      "Epoch 2 step 722: training loss: 28575.148407328594\n",
      "Epoch 2 step 723: training accuarcy: 0.7837000000000001\n",
      "Epoch 2 step 723: training loss: 29106.143967722925\n",
      "Epoch 2 step 724: training accuarcy: 0.7876000000000001\n",
      "Epoch 2 step 724: training loss: 28256.03608964547\n",
      "Epoch 2 step 725: training accuarcy: 0.7919\n",
      "Epoch 2 step 725: training loss: 29193.592025958176\n",
      "Epoch 2 step 726: training accuarcy: 0.7807000000000001\n",
      "Epoch 2 step 726: training loss: 29336.185142134174\n",
      "Epoch 2 step 727: training accuarcy: 0.7722\n",
      "Epoch 2 step 727: training loss: 28962.898446169096\n",
      "Epoch 2 step 728: training accuarcy: 0.7863\n",
      "Epoch 2 step 728: training loss: 28117.231096757332\n",
      "Epoch 2 step 729: training accuarcy: 0.7918000000000001\n",
      "Epoch 2 step 729: training loss: 28839.910518495446\n",
      "Epoch 2 step 730: training accuarcy: 0.7927000000000001\n",
      "Epoch 2 step 730: training loss: 29192.371305927754\n",
      "Epoch 2 step 731: training accuarcy: 0.7725000000000001\n",
      "Epoch 2 step 731: training loss: 28286.986854705072\n",
      "Epoch 2 step 732: training accuarcy: 0.7969\n",
      "Epoch 2 step 732: training loss: 28738.418873785613\n",
      "Epoch 2 step 733: training accuarcy: 0.7874\n",
      "Epoch 2 step 733: training loss: 28636.943144297893\n",
      "Epoch 2 step 734: training accuarcy: 0.79\n",
      "Epoch 2 step 734: training loss: 28984.31754437962\n",
      "Epoch 2 step 735: training accuarcy: 0.7773\n",
      "Epoch 2 step 735: training loss: 28432.015120395125\n",
      "Epoch 2 step 736: training accuarcy: 0.7911\n",
      "Epoch 2 step 736: training loss: 29122.971294540046\n",
      "Epoch 2 step 737: training accuarcy: 0.7775000000000001\n",
      "Epoch 2 step 737: training loss: 28919.885674738336\n",
      "Epoch 2 step 738: training accuarcy: 0.7883\n",
      "Epoch 2 step 738: training loss: 28567.877430484154\n",
      "Epoch 2 step 739: training accuarcy: 0.7906000000000001\n",
      "Epoch 2 step 739: training loss: 28567.63124940709\n",
      "Epoch 2 step 740: training accuarcy: 0.7873\n",
      "Epoch 2 step 740: training loss: 28907.772476631122\n",
      "Epoch 2 step 741: training accuarcy: 0.7792\n",
      "Epoch 2 step 741: training loss: 27887.41274420786\n",
      "Epoch 2 step 742: training accuarcy: 0.8027000000000001\n",
      "Epoch 2 step 742: training loss: 29028.324327086815\n",
      "Epoch 2 step 743: training accuarcy: 0.78\n",
      "Epoch 2 step 743: training loss: 28830.06989388102\n",
      "Epoch 2 step 744: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 744: training loss: 28425.582902862763\n",
      "Epoch 2 step 745: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 745: training loss: 28775.990500963253\n",
      "Epoch 2 step 746: training accuarcy: 0.7888000000000001\n",
      "Epoch 2 step 746: training loss: 29061.4637093625\n",
      "Epoch 2 step 747: training accuarcy: 0.7751\n",
      "Epoch 2 step 747: training loss: 28828.50106405306\n",
      "Epoch 2 step 748: training accuarcy: 0.7835000000000001\n",
      "Epoch 2 step 748: training loss: 28972.024653152843\n",
      "Epoch 2 step 749: training accuarcy: 0.7808\n",
      "Epoch 2 step 749: training loss: 28843.62691110761\n",
      "Epoch 2 step 750: training accuarcy: 0.7822\n",
      "Epoch 2 step 750: training loss: 28762.181621039705\n",
      "Epoch 2 step 751: training accuarcy: 0.7791\n",
      "Epoch 2 step 751: training loss: 29646.20372179889\n",
      "Epoch 2 step 752: training accuarcy: 0.7702\n",
      "Epoch 2 step 752: training loss: 29002.307199279934\n",
      "Epoch 2 step 753: training accuarcy: 0.7852\n",
      "Epoch 2 step 753: training loss: 28957.899330592652\n",
      "Epoch 2 step 754: training accuarcy: 0.7803\n",
      "Epoch 2 step 754: training loss: 28459.815206606152\n",
      "Epoch 2 step 755: training accuarcy: 0.7828\n",
      "Epoch 2 step 755: training loss: 29003.09437638173\n",
      "Epoch 2 step 756: training accuarcy: 0.7819\n",
      "Epoch 2 step 756: training loss: 28700.181682970277\n",
      "Epoch 2 step 757: training accuarcy: 0.791\n",
      "Epoch 2 step 757: training loss: 28418.64620531527\n",
      "Epoch 2 step 758: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 758: training loss: 28832.746849772426\n",
      "Epoch 2 step 759: training accuarcy: 0.7872\n",
      "Epoch 2 step 759: training loss: 29189.99429249127\n",
      "Epoch 2 step 760: training accuarcy: 0.7791\n",
      "Epoch 2 step 760: training loss: 28673.37523548882\n",
      "Epoch 2 step 761: training accuarcy: 0.7896000000000001\n",
      "Epoch 2 step 761: training loss: 28362.29151992638\n",
      "Epoch 2 step 762: training accuarcy: 0.7853\n",
      "Epoch 2 step 762: training loss: 28196.19600945464\n",
      "Epoch 2 step 763: training accuarcy: 0.7864\n",
      "Epoch 2 step 763: training loss: 29165.83731743296\n",
      "Epoch 2 step 764: training accuarcy: 0.7801\n",
      "Epoch 2 step 764: training loss: 28541.616338218148\n",
      "Epoch 2 step 765: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 765: training loss: 28811.15698666816\n",
      "Epoch 2 step 766: training accuarcy: 0.7766000000000001\n",
      "Epoch 2 step 766: training loss: 28740.1923601297\n",
      "Epoch 2 step 767: training accuarcy: 0.7782\n",
      "Epoch 2 step 767: training loss: 29320.73238198382\n",
      "Epoch 2 step 768: training accuarcy: 0.7742\n",
      "Epoch 2 step 768: training loss: 28260.84672725874\n",
      "Epoch 2 step 769: training accuarcy: 0.7919\n",
      "Epoch 2 step 769: training loss: 28957.526672058677\n",
      "Epoch 2 step 770: training accuarcy: 0.7762\n",
      "Epoch 2 step 770: training loss: 28562.103378275435\n",
      "Epoch 2 step 771: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 771: training loss: 28475.05956443003\n",
      "Epoch 2 step 772: training accuarcy: 0.7803\n",
      "Epoch 2 step 772: training loss: 29019.41296146404\n",
      "Epoch 2 step 773: training accuarcy: 0.7732\n",
      "Epoch 2 step 773: training loss: 28652.311985166358\n",
      "Epoch 2 step 774: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 774: training loss: 28928.243440792026\n",
      "Epoch 2 step 775: training accuarcy: 0.7745000000000001\n",
      "Epoch 2 step 775: training loss: 28051.80545566762\n",
      "Epoch 2 step 776: training accuarcy: 0.7965\n",
      "Epoch 2 step 776: training loss: 28278.83419560922\n",
      "Epoch 2 step 777: training accuarcy: 0.7818\n",
      "Epoch 2 step 777: training loss: 28967.560284302526\n",
      "Epoch 2 step 778: training accuarcy: 0.7825000000000001\n",
      "Epoch 2 step 778: training loss: 28562.803906150028\n",
      "Epoch 2 step 779: training accuarcy: 0.7824\n",
      "Epoch 2 step 779: training loss: 28608.27201409693\n",
      "Epoch 2 step 780: training accuarcy: 0.7795000000000001\n",
      "Epoch 2 step 780: training loss: 28173.920706854355\n",
      "Epoch 2 step 781: training accuarcy: 0.791\n",
      "Epoch 2 step 781: training loss: 29469.059511362255\n",
      "Epoch 2 step 782: training accuarcy: 0.769\n",
      "Epoch 2 step 782: training loss: 28080.376779116737\n",
      "Epoch 2 step 783: training accuarcy: 0.7934\n",
      "Epoch 2 step 783: training loss: 28181.44120468637\n",
      "Epoch 2 step 784: training accuarcy: 0.7884\n",
      "Epoch 2 step 784: training loss: 27811.88896753308\n",
      "Epoch 2 step 785: training accuarcy: 0.7913\n",
      "Epoch 2 step 785: training loss: 28492.916805675373\n",
      "Epoch 2 step 786: training accuarcy: 0.782\n",
      "Epoch 2 step 786: training loss: 29131.799150168416\n",
      "Epoch 2 step 787: training accuarcy: 0.779\n",
      "Epoch 2 step 787: training loss: 27981.950554054547\n",
      "Epoch 2 step 788: training accuarcy: 0.7897000000000001\n",
      "Epoch 2 step 788: training loss: 12910.41790157804\n",
      "Epoch 2 step 789: training accuarcy: 0.796923076923077\n",
      "Epoch 2: train loss 28942.57826501978, train accuarcy 0.7592906951904297\n",
      "Epoch 2: valid loss 27549.076667420246, valid accuarcy 0.793188214302063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [15:15<00:00, 304.55s/it]\n"
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=3,\n",
    "                log_dir=get_log_dir('weight_topcoder', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:42:12.432109Z",
     "start_time": "2019-10-17T07:42:12.420094Z"
    }
   },
   "outputs": [],
   "source": [
    "del hrm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PRME FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:42:36.347089Z",
     "start_time": "2019-10-17T07:42:36.093091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchPrmeFM()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_model = TorchPrmeFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "prme_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:42:37.543123Z",
     "start_time": "2019-10-17T07:42:37.540102Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(prme_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:42:38.999092Z",
     "start_time": "2019-10-17T07:42:38.956089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x2516d857608>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_learner = FMLearner(prme_model, adam_opt, schedular, db)\n",
    "prme_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:42:49.432090Z",
     "start_time": "2019-10-17T07:42:49.428087Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='base',\n",
    "                     valid_col='base',\n",
    "                     test_col='base',\n",
    "                     loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:42:28.132692Z",
     "start_time": "2019-10-09T12:42:28.129692Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='seq',\n",
    "                     valid_col='seq',\n",
    "                     test_col='seq',\n",
    "                     loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:59:52.359203Z",
     "start_time": "2019-10-09T12:59:52.355203Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='seq',\n",
    "                     valid_col='seq',\n",
    "                     test_col='seq',\n",
    "                     loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:48:38.084104Z",
     "start_time": "2019-10-17T07:43:04.486090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 45930.1427652565\n",
      "Epoch 0 step 1: training accuarcy: 0.529\n",
      "Epoch 0 step 1: training loss: 44926.31891158488\n",
      "Epoch 0 step 2: training accuarcy: 0.529\n",
      "Epoch 0 step 2: training loss: 43107.71907088262\n",
      "Epoch 0 step 3: training accuarcy: 0.546\n",
      "Epoch 0 step 3: training loss: 42306.41889813116\n",
      "Epoch 0 step 4: training accuarcy: 0.5315\n",
      "Epoch 0 step 4: training loss: 41712.817676772436\n",
      "Epoch 0 step 5: training accuarcy: 0.5095000000000001\n",
      "Epoch 0 step 5: training loss: 40558.056182405955\n",
      "Epoch 0 step 6: training accuarcy: 0.53\n",
      "Epoch 0 step 6: training loss: 38942.33300396817\n",
      "Epoch 0 step 7: training accuarcy: 0.539\n",
      "Epoch 0 step 7: training loss: 38302.398712509435\n",
      "Epoch 0 step 8: training accuarcy: 0.5415\n",
      "Epoch 0 step 8: training loss: 37441.500794490676\n",
      "Epoch 0 step 9: training accuarcy: 0.543\n",
      "Epoch 0 step 9: training loss: 36332.164361024115\n",
      "Epoch 0 step 10: training accuarcy: 0.54\n",
      "Epoch 0 step 10: training loss: 34859.21795184726\n",
      "Epoch 0 step 11: training accuarcy: 0.532\n",
      "Epoch 0 step 11: training loss: 34124.12614879984\n",
      "Epoch 0 step 12: training accuarcy: 0.5365\n",
      "Epoch 0 step 12: training loss: 32942.0978726607\n",
      "Epoch 0 step 13: training accuarcy: 0.5575\n",
      "Epoch 0 step 13: training loss: 31851.754821138347\n",
      "Epoch 0 step 14: training accuarcy: 0.553\n",
      "Epoch 0 step 14: training loss: 31653.513115744907\n",
      "Epoch 0 step 15: training accuarcy: 0.525\n",
      "Epoch 0 step 15: training loss: 30365.99710519109\n",
      "Epoch 0 step 16: training accuarcy: 0.5465\n",
      "Epoch 0 step 16: training loss: 29955.05252529642\n",
      "Epoch 0 step 17: training accuarcy: 0.5335\n",
      "Epoch 0 step 17: training loss: 28728.169551080886\n",
      "Epoch 0 step 18: training accuarcy: 0.542\n",
      "Epoch 0 step 18: training loss: 28255.172461772036\n",
      "Epoch 0 step 19: training accuarcy: 0.5415\n",
      "Epoch 0 step 19: training loss: 27127.524250288956\n",
      "Epoch 0 step 20: training accuarcy: 0.5425\n",
      "Epoch 0 step 20: training loss: 26592.49635049076\n",
      "Epoch 0 step 21: training accuarcy: 0.523\n",
      "Epoch 0 step 21: training loss: 26182.53335516222\n",
      "Epoch 0 step 22: training accuarcy: 0.522\n",
      "Epoch 0 step 22: training loss: 25146.11034372073\n",
      "Epoch 0 step 23: training accuarcy: 0.544\n",
      "Epoch 0 step 23: training loss: 24389.914445695696\n",
      "Epoch 0 step 24: training accuarcy: 0.532\n",
      "Epoch 0 step 24: training loss: 23769.585668936503\n",
      "Epoch 0 step 25: training accuarcy: 0.5395\n",
      "Epoch 0 step 25: training loss: 22619.825362378288\n",
      "Epoch 0 step 26: training accuarcy: 0.5355\n",
      "Epoch 0 step 26: training loss: 21870.13439456194\n",
      "Epoch 0 step 27: training accuarcy: 0.5665\n",
      "Epoch 0 step 27: training loss: 21408.670294411033\n",
      "Epoch 0 step 28: training accuarcy: 0.5425\n",
      "Epoch 0 step 28: training loss: 20773.655972819317\n",
      "Epoch 0 step 29: training accuarcy: 0.5445\n",
      "Epoch 0 step 29: training loss: 20203.37808076678\n",
      "Epoch 0 step 30: training accuarcy: 0.5465\n",
      "Epoch 0 step 30: training loss: 19312.058086771183\n",
      "Epoch 0 step 31: training accuarcy: 0.5585\n",
      "Epoch 0 step 31: training loss: 18941.936216637947\n",
      "Epoch 0 step 32: training accuarcy: 0.5535\n",
      "Epoch 0 step 32: training loss: 18364.19689175181\n",
      "Epoch 0 step 33: training accuarcy: 0.5465\n",
      "Epoch 0 step 33: training loss: 17792.066979277763\n",
      "Epoch 0 step 34: training accuarcy: 0.5635\n",
      "Epoch 0 step 34: training loss: 17525.693195475018\n",
      "Epoch 0 step 35: training accuarcy: 0.541\n",
      "Epoch 0 step 35: training loss: 16472.55319522366\n",
      "Epoch 0 step 36: training accuarcy: 0.559\n",
      "Epoch 0 step 36: training loss: 16411.156353563536\n",
      "Epoch 0 step 37: training accuarcy: 0.5375\n",
      "Epoch 0 step 37: training loss: 16077.506739846003\n",
      "Epoch 0 step 38: training accuarcy: 0.528\n",
      "Epoch 0 step 38: training loss: 15226.118856194653\n",
      "Epoch 0 step 39: training accuarcy: 0.552\n",
      "Epoch 0 step 39: training loss: 14801.827477896235\n",
      "Epoch 0 step 40: training accuarcy: 0.5595\n",
      "Epoch 0 step 40: training loss: 14093.643252804402\n",
      "Epoch 0 step 41: training accuarcy: 0.551\n",
      "Epoch 0 step 41: training loss: 13968.152362154606\n",
      "Epoch 0 step 42: training accuarcy: 0.5395\n",
      "Epoch 0 step 42: training loss: 13479.168622190227\n",
      "Epoch 0 step 43: training accuarcy: 0.545\n",
      "Epoch 0 step 43: training loss: 12910.821272977955\n",
      "Epoch 0 step 44: training accuarcy: 0.561\n",
      "Epoch 0 step 44: training loss: 12650.340591556367\n",
      "Epoch 0 step 45: training accuarcy: 0.5575\n",
      "Epoch 0 step 45: training loss: 12333.061885044124\n",
      "Epoch 0 step 46: training accuarcy: 0.5375\n",
      "Epoch 0 step 46: training loss: 11906.200348727994\n",
      "Epoch 0 step 47: training accuarcy: 0.5585\n",
      "Epoch 0 step 47: training loss: 11576.767645873286\n",
      "Epoch 0 step 48: training accuarcy: 0.5375\n",
      "Epoch 0 step 48: training loss: 11238.544034393992\n",
      "Epoch 0 step 49: training accuarcy: 0.5435\n",
      "Epoch 0 step 49: training loss: 10822.344304564465\n",
      "Epoch 0 step 50: training accuarcy: 0.5375\n",
      "Epoch 0 step 50: training loss: 10312.053334021368\n",
      "Epoch 0 step 51: training accuarcy: 0.5715\n",
      "Epoch 0 step 51: training loss: 10105.80132836231\n",
      "Epoch 0 step 52: training accuarcy: 0.5445\n",
      "Epoch 0 step 52: training loss: 9921.490131313934\n",
      "Epoch 0 step 53: training accuarcy: 0.5355\n",
      "Epoch 0 step 53: training loss: 9611.19188809443\n",
      "Epoch 0 step 54: training accuarcy: 0.538\n",
      "Epoch 0 step 54: training loss: 9354.556674779724\n",
      "Epoch 0 step 55: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 55: training loss: 9033.071174146382\n",
      "Epoch 0 step 56: training accuarcy: 0.5495\n",
      "Epoch 0 step 56: training loss: 8680.38358243792\n",
      "Epoch 0 step 57: training accuarcy: 0.5505\n",
      "Epoch 0 step 57: training loss: 8489.989773141986\n",
      "Epoch 0 step 58: training accuarcy: 0.5545\n",
      "Epoch 0 step 58: training loss: 8223.741816004002\n",
      "Epoch 0 step 59: training accuarcy: 0.5495\n",
      "Epoch 0 step 59: training loss: 8190.1897858579705\n",
      "Epoch 0 step 60: training accuarcy: 0.531\n",
      "Epoch 0 step 60: training loss: 7813.231099560863\n",
      "Epoch 0 step 61: training accuarcy: 0.555\n",
      "Epoch 0 step 61: training loss: 7538.868630876367\n",
      "Epoch 0 step 62: training accuarcy: 0.5545\n",
      "Epoch 0 step 62: training loss: 7356.408177492825\n",
      "Epoch 0 step 63: training accuarcy: 0.5485\n",
      "Epoch 0 step 63: training loss: 7019.594098645123\n",
      "Epoch 0 step 64: training accuarcy: 0.5845\n",
      "Epoch 0 step 64: training loss: 6873.357792406288\n",
      "Epoch 0 step 65: training accuarcy: 0.578\n",
      "Epoch 0 step 65: training loss: 6791.831594484856\n",
      "Epoch 0 step 66: training accuarcy: 0.548\n",
      "Epoch 0 step 66: training loss: 6568.80846557002\n",
      "Epoch 0 step 67: training accuarcy: 0.5555\n",
      "Epoch 0 step 67: training loss: 6293.577467944432\n",
      "Epoch 0 step 68: training accuarcy: 0.584\n",
      "Epoch 0 step 68: training loss: 6185.176326784495\n",
      "Epoch 0 step 69: training accuarcy: 0.5630000000000001\n",
      "Epoch 0 step 69: training loss: 6033.005038882848\n",
      "Epoch 0 step 70: training accuarcy: 0.5615\n",
      "Epoch 0 step 70: training loss: 5873.810502208431\n",
      "Epoch 0 step 71: training accuarcy: 0.5715\n",
      "Epoch 0 step 71: training loss: 5651.184080065046\n",
      "Epoch 0 step 72: training accuarcy: 0.6\n",
      "Epoch 0 step 72: training loss: 5560.0821197418245\n",
      "Epoch 0 step 73: training accuarcy: 0.5905\n",
      "Epoch 0 step 73: training loss: 5479.015194628681\n",
      "Epoch 0 step 74: training accuarcy: 0.579\n",
      "Epoch 0 step 74: training loss: 5359.484036757731\n",
      "Epoch 0 step 75: training accuarcy: 0.5515\n",
      "Epoch 0 step 75: training loss: 5235.569060632359\n",
      "Epoch 0 step 76: training accuarcy: 0.54\n",
      "Epoch 0 step 76: training loss: 5078.11710948666\n",
      "Epoch 0 step 77: training accuarcy: 0.5660000000000001\n",
      "Epoch 0 step 77: training loss: 4937.301113773241\n",
      "Epoch 0 step 78: training accuarcy: 0.5760000000000001\n",
      "Epoch 0 step 78: training loss: 4830.647698713168\n",
      "Epoch 0 step 79: training accuarcy: 0.5595\n",
      "Epoch 0 step 79: training loss: 4723.14807429711\n",
      "Epoch 0 step 80: training accuarcy: 0.5730000000000001\n",
      "Epoch 0 step 80: training loss: 4613.192365886005\n",
      "Epoch 0 step 81: training accuarcy: 0.5605\n",
      "Epoch 0 step 81: training loss: 4469.77970413303\n",
      "Epoch 0 step 82: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 82: training loss: 4381.603576805945\n",
      "Epoch 0 step 83: training accuarcy: 0.5855\n",
      "Epoch 0 step 83: training loss: 4314.4397913110715\n",
      "Epoch 0 step 84: training accuarcy: 0.5585\n",
      "Epoch 0 step 84: training loss: 4175.207642939772\n",
      "Epoch 0 step 85: training accuarcy: 0.587\n",
      "Epoch 0 step 85: training loss: 4096.181618589878\n",
      "Epoch 0 step 86: training accuarcy: 0.5835\n",
      "Epoch 0 step 86: training loss: 4029.9570058460467\n",
      "Epoch 0 step 87: training accuarcy: 0.562\n",
      "Epoch 0 step 87: training loss: 3893.73950284467\n",
      "Epoch 0 step 88: training accuarcy: 0.585\n",
      "Epoch 0 step 88: training loss: 3845.99445174703\n",
      "Epoch 0 step 89: training accuarcy: 0.5915\n",
      "Epoch 0 step 89: training loss: 3777.8503624111486\n",
      "Epoch 0 step 90: training accuarcy: 0.5700000000000001\n",
      "Epoch 0 step 90: training loss: 3685.807722957133\n",
      "Epoch 0 step 91: training accuarcy: 0.582\n",
      "Epoch 0 step 91: training loss: 3600.148451705141\n",
      "Epoch 0 step 92: training accuarcy: 0.5955\n",
      "Epoch 0 step 92: training loss: 3534.4823769188733\n",
      "Epoch 0 step 93: training accuarcy: 0.5945\n",
      "Epoch 0 step 93: training loss: 3432.3882105333832\n",
      "Epoch 0 step 94: training accuarcy: 0.601\n",
      "Epoch 0 step 94: training loss: 3379.215771683267\n",
      "Epoch 0 step 95: training accuarcy: 0.5935\n",
      "Epoch 0 step 95: training loss: 3330.2608045120624\n",
      "Epoch 0 step 96: training accuarcy: 0.5975\n",
      "Epoch 0 step 96: training loss: 3288.3194924556865\n",
      "Epoch 0 step 97: training accuarcy: 0.585\n",
      "Epoch 0 step 97: training loss: 3185.6425544026083\n",
      "Epoch 0 step 98: training accuarcy: 0.626\n",
      "Epoch 0 step 98: training loss: 3129.171032418778\n",
      "Epoch 0 step 99: training accuarcy: 0.6205\n",
      "Epoch 0 step 99: training loss: 3119.972423083471\n",
      "Epoch 0 step 100: training accuarcy: 0.581\n",
      "Epoch 0 step 100: training loss: 3038.0278102821535\n",
      "Epoch 0 step 101: training accuarcy: 0.6115\n",
      "Epoch 0 step 101: training loss: 2978.4098258357726\n",
      "Epoch 0 step 102: training accuarcy: 0.5985\n",
      "Epoch 0 step 102: training loss: 2926.4206905527335\n",
      "Epoch 0 step 103: training accuarcy: 0.602\n",
      "Epoch 0 step 103: training loss: 2878.933290852589\n",
      "Epoch 0 step 104: training accuarcy: 0.618\n",
      "Epoch 0 step 104: training loss: 2852.488655049543\n",
      "Epoch 0 step 105: training accuarcy: 0.6125\n",
      "Epoch 0 step 105: training loss: 2778.771913068638\n",
      "Epoch 0 step 106: training accuarcy: 0.625\n",
      "Epoch 0 step 106: training loss: 2721.1645431718007\n",
      "Epoch 0 step 107: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 107: training loss: 2705.0335553670416\n",
      "Epoch 0 step 108: training accuarcy: 0.618\n",
      "Epoch 0 step 108: training loss: 2653.435767136546\n",
      "Epoch 0 step 109: training accuarcy: 0.624\n",
      "Epoch 0 step 109: training loss: 2640.759766679692\n",
      "Epoch 0 step 110: training accuarcy: 0.616\n",
      "Epoch 0 step 110: training loss: 2552.657280157492\n",
      "Epoch 0 step 111: training accuarcy: 0.6255000000000001\n",
      "Epoch 0 step 111: training loss: 2542.422277197615\n",
      "Epoch 0 step 112: training accuarcy: 0.629\n",
      "Epoch 0 step 112: training loss: 2487.31799782533\n",
      "Epoch 0 step 113: training accuarcy: 0.6385000000000001\n",
      "Epoch 0 step 113: training loss: 2468.344293430392\n",
      "Epoch 0 step 114: training accuarcy: 0.6315000000000001\n",
      "Epoch 0 step 114: training loss: 2453.4700893144754\n",
      "Epoch 0 step 115: training accuarcy: 0.6095\n",
      "Epoch 0 step 115: training loss: 2413.8244173996427\n",
      "Epoch 0 step 116: training accuarcy: 0.64\n",
      "Epoch 0 step 116: training loss: 2360.463075801501\n",
      "Epoch 0 step 117: training accuarcy: 0.6525\n",
      "Epoch 0 step 117: training loss: 2311.3906957504664\n",
      "Epoch 0 step 118: training accuarcy: 0.6485\n",
      "Epoch 0 step 118: training loss: 2267.510275538225\n",
      "Epoch 0 step 119: training accuarcy: 0.6495\n",
      "Epoch 0 step 119: training loss: 2252.3144331167923\n",
      "Epoch 0 step 120: training accuarcy: 0.647\n",
      "Epoch 0 step 120: training loss: 2218.8155119091357\n",
      "Epoch 0 step 121: training accuarcy: 0.6625\n",
      "Epoch 0 step 121: training loss: 2210.608138536604\n",
      "Epoch 0 step 122: training accuarcy: 0.6545\n",
      "Epoch 0 step 122: training loss: 2162.750443370177\n",
      "Epoch 0 step 123: training accuarcy: 0.6655\n",
      "Epoch 0 step 123: training loss: 2135.19610789617\n",
      "Epoch 0 step 124: training accuarcy: 0.66\n",
      "Epoch 0 step 124: training loss: 2102.7582672383196\n",
      "Epoch 0 step 125: training accuarcy: 0.6875\n",
      "Epoch 0 step 125: training loss: 2085.8090900523202\n",
      "Epoch 0 step 126: training accuarcy: 0.644\n",
      "Epoch 0 step 126: training loss: 2046.1955526144786\n",
      "Epoch 0 step 127: training accuarcy: 0.6755\n",
      "Epoch 0 step 127: training loss: 2085.5711065634814\n",
      "Epoch 0 step 128: training accuarcy: 0.652\n",
      "Epoch 0 step 128: training loss: 2024.6020196160127\n",
      "Epoch 0 step 129: training accuarcy: 0.6575\n",
      "Epoch 0 step 129: training loss: 2022.9808123279608\n",
      "Epoch 0 step 130: training accuarcy: 0.646\n",
      "Epoch 0 step 130: training loss: 1970.6130070108898\n",
      "Epoch 0 step 131: training accuarcy: 0.6855\n",
      "Epoch 0 step 131: training loss: 1948.2529080843356\n",
      "Epoch 0 step 132: training accuarcy: 0.6625\n",
      "Epoch 0 step 132: training loss: 1936.2219709564806\n",
      "Epoch 0 step 133: training accuarcy: 0.681\n",
      "Epoch 0 step 133: training loss: 1906.6821502785829\n",
      "Epoch 0 step 134: training accuarcy: 0.6775\n",
      "Epoch 0 step 134: training loss: 1903.1867027198698\n",
      "Epoch 0 step 135: training accuarcy: 0.6625\n",
      "Epoch 0 step 135: training loss: 1853.214038520092\n",
      "Epoch 0 step 136: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 136: training loss: 1837.722920886912\n",
      "Epoch 0 step 137: training accuarcy: 0.682\n",
      "Epoch 0 step 137: training loss: 1846.3053374492126\n",
      "Epoch 0 step 138: training accuarcy: 0.6685\n",
      "Epoch 0 step 138: training loss: 1812.8077484118398\n",
      "Epoch 0 step 139: training accuarcy: 0.6915\n",
      "Epoch 0 step 139: training loss: 1825.6461407253353\n",
      "Epoch 0 step 140: training accuarcy: 0.659\n",
      "Epoch 0 step 140: training loss: 1752.8185687473024\n",
      "Epoch 0 step 141: training accuarcy: 0.7015\n",
      "Epoch 0 step 141: training loss: 1758.640359635383\n",
      "Epoch 0 step 142: training accuarcy: 0.675\n",
      "Epoch 0 step 142: training loss: 1774.4229882069471\n",
      "Epoch 0 step 143: training accuarcy: 0.6765\n",
      "Epoch 0 step 143: training loss: 1716.7522047897564\n",
      "Epoch 0 step 144: training accuarcy: 0.6995\n",
      "Epoch 0 step 144: training loss: 1702.495944672929\n",
      "Epoch 0 step 145: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 145: training loss: 1662.1840137581185\n",
      "Epoch 0 step 146: training accuarcy: 0.722\n",
      "Epoch 0 step 146: training loss: 1652.5156628368659\n",
      "Epoch 0 step 147: training accuarcy: 0.6975\n",
      "Epoch 0 step 147: training loss: 1683.5486768656406\n",
      "Epoch 0 step 148: training accuarcy: 0.6825\n",
      "Epoch 0 step 148: training loss: 1681.6430634827957\n",
      "Epoch 0 step 149: training accuarcy: 0.71\n",
      "Epoch 0 step 149: training loss: 1640.7933814778717\n",
      "Epoch 0 step 150: training accuarcy: 0.717\n",
      "Epoch 0 step 150: training loss: 1649.9599589278278\n",
      "Epoch 0 step 151: training accuarcy: 0.6990000000000001\n",
      "Epoch 0 step 151: training loss: 1645.281220828952\n",
      "Epoch 0 step 152: training accuarcy: 0.6995\n",
      "Epoch 0 step 152: training loss: 1629.3377358825146\n",
      "Epoch 0 step 153: training accuarcy: 0.6950000000000001\n",
      "Epoch 0 step 153: training loss: 1599.2662291156844\n",
      "Epoch 0 step 154: training accuarcy: 0.7045\n",
      "Epoch 0 step 154: training loss: 1605.5755224353495\n",
      "Epoch 0 step 155: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 155: training loss: 1586.690337187012\n",
      "Epoch 0 step 156: training accuarcy: 0.6985\n",
      "Epoch 0 step 156: training loss: 1525.2514808432957\n",
      "Epoch 0 step 157: training accuarcy: 0.7235\n",
      "Epoch 0 step 157: training loss: 1564.5121875785003\n",
      "Epoch 0 step 158: training accuarcy: 0.7065\n",
      "Epoch 0 step 158: training loss: 1551.270505002953\n",
      "Epoch 0 step 159: training accuarcy: 0.7055\n",
      "Epoch 0 step 159: training loss: 1568.485615787727\n",
      "Epoch 0 step 160: training accuarcy: 0.6900000000000001\n",
      "Epoch 0 step 160: training loss: 1515.212845338494\n",
      "Epoch 0 step 161: training accuarcy: 0.7030000000000001\n",
      "Epoch 0 step 161: training loss: 1510.1642813994526\n",
      "Epoch 0 step 162: training accuarcy: 0.7275\n",
      "Epoch 0 step 162: training loss: 1483.33370512764\n",
      "Epoch 0 step 163: training accuarcy: 0.736\n",
      "Epoch 0 step 163: training loss: 1478.993205837537\n",
      "Epoch 0 step 164: training accuarcy: 0.732\n",
      "Epoch 0 step 164: training loss: 1465.1013791712644\n",
      "Epoch 0 step 165: training accuarcy: 0.7215\n",
      "Epoch 0 step 165: training loss: 1491.05557293359\n",
      "Epoch 0 step 166: training accuarcy: 0.7295\n",
      "Epoch 0 step 166: training loss: 1465.1404142089707\n",
      "Epoch 0 step 167: training accuarcy: 0.7205\n",
      "Epoch 0 step 167: training loss: 1458.6017559586999\n",
      "Epoch 0 step 168: training accuarcy: 0.7285\n",
      "Epoch 0 step 168: training loss: 1469.3813486864892\n",
      "Epoch 0 step 169: training accuarcy: 0.7075\n",
      "Epoch 0 step 169: training loss: 1452.8608656820738\n",
      "Epoch 0 step 170: training accuarcy: 0.72\n",
      "Epoch 0 step 170: training loss: 1415.7841144157114\n",
      "Epoch 0 step 171: training accuarcy: 0.741\n",
      "Epoch 0 step 171: training loss: 1433.413785614869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 172: training accuarcy: 0.72\n",
      "Epoch 0 step 172: training loss: 1442.1492595142863\n",
      "Epoch 0 step 173: training accuarcy: 0.722\n",
      "Epoch 0 step 173: training loss: 1384.6518471886536\n",
      "Epoch 0 step 174: training accuarcy: 0.755\n",
      "Epoch 0 step 174: training loss: 1422.8648673078271\n",
      "Epoch 0 step 175: training accuarcy: 0.733\n",
      "Epoch 0 step 175: training loss: 1400.6681082110008\n",
      "Epoch 0 step 176: training accuarcy: 0.7335\n",
      "Epoch 0 step 176: training loss: 1423.3400651094141\n",
      "Epoch 0 step 177: training accuarcy: 0.7245\n",
      "Epoch 0 step 177: training loss: 1384.8729873377806\n",
      "Epoch 0 step 178: training accuarcy: 0.7275\n",
      "Epoch 0 step 178: training loss: 1398.7661499754358\n",
      "Epoch 0 step 179: training accuarcy: 0.717\n",
      "Epoch 0 step 179: training loss: 1391.0536320455753\n",
      "Epoch 0 step 180: training accuarcy: 0.7105\n",
      "Epoch 0 step 180: training loss: 1385.547574740684\n",
      "Epoch 0 step 181: training accuarcy: 0.7355\n",
      "Epoch 0 step 181: training loss: 1382.0024146235376\n",
      "Epoch 0 step 182: training accuarcy: 0.7185\n",
      "Epoch 0 step 182: training loss: 1367.9857327394186\n",
      "Epoch 0 step 183: training accuarcy: 0.725\n",
      "Epoch 0 step 183: training loss: 1359.9886506989144\n",
      "Epoch 0 step 184: training accuarcy: 0.748\n",
      "Epoch 0 step 184: training loss: 1368.3570486241247\n",
      "Epoch 0 step 185: training accuarcy: 0.727\n",
      "Epoch 0 step 185: training loss: 1388.272872366084\n",
      "Epoch 0 step 186: training accuarcy: 0.749\n",
      "Epoch 0 step 186: training loss: 1326.7891627361657\n",
      "Epoch 0 step 187: training accuarcy: 0.7455\n",
      "Epoch 0 step 187: training loss: 1346.3440456722722\n",
      "Epoch 0 step 188: training accuarcy: 0.7195\n",
      "Epoch 0 step 188: training loss: 1333.146649556966\n",
      "Epoch 0 step 189: training accuarcy: 0.7345\n",
      "Epoch 0 step 189: training loss: 1294.1511301297448\n",
      "Epoch 0 step 190: training accuarcy: 0.7495\n",
      "Epoch 0 step 190: training loss: 1320.5534384577347\n",
      "Epoch 0 step 191: training accuarcy: 0.7365\n",
      "Epoch 0 step 191: training loss: 1328.769243636779\n",
      "Epoch 0 step 192: training accuarcy: 0.7415\n",
      "Epoch 0 step 192: training loss: 1321.978782225199\n",
      "Epoch 0 step 193: training accuarcy: 0.7405\n",
      "Epoch 0 step 193: training loss: 1305.3018623728108\n",
      "Epoch 0 step 194: training accuarcy: 0.752\n",
      "Epoch 0 step 194: training loss: 1300.7756669283413\n",
      "Epoch 0 step 195: training accuarcy: 0.7415\n",
      "Epoch 0 step 195: training loss: 1311.0428495822077\n",
      "Epoch 0 step 196: training accuarcy: 0.7455\n",
      "Epoch 0 step 196: training loss: 1297.8302858020818\n",
      "Epoch 0 step 197: training accuarcy: 0.7515000000000001\n",
      "Epoch 0 step 197: training loss: 1293.8738535232599\n",
      "Epoch 0 step 198: training accuarcy: 0.7445\n",
      "Epoch 0 step 198: training loss: 1309.9839864781404\n",
      "Epoch 0 step 199: training accuarcy: 0.737\n",
      "Epoch 0 step 199: training loss: 1292.421913423305\n",
      "Epoch 0 step 200: training accuarcy: 0.751\n",
      "Epoch 0 step 200: training loss: 1301.9777472122582\n",
      "Epoch 0 step 201: training accuarcy: 0.737\n",
      "Epoch 0 step 201: training loss: 1314.3365680714494\n",
      "Epoch 0 step 202: training accuarcy: 0.7305\n",
      "Epoch 0 step 202: training loss: 1289.9810583104572\n",
      "Epoch 0 step 203: training accuarcy: 0.7275\n",
      "Epoch 0 step 203: training loss: 1267.130465812801\n",
      "Epoch 0 step 204: training accuarcy: 0.757\n",
      "Epoch 0 step 204: training loss: 1280.6583487456385\n",
      "Epoch 0 step 205: training accuarcy: 0.741\n",
      "Epoch 0 step 205: training loss: 1269.2316042093607\n",
      "Epoch 0 step 206: training accuarcy: 0.74\n",
      "Epoch 0 step 206: training loss: 1305.9941742516316\n",
      "Epoch 0 step 207: training accuarcy: 0.733\n",
      "Epoch 0 step 207: training loss: 1251.7692602745828\n",
      "Epoch 0 step 208: training accuarcy: 0.7565000000000001\n",
      "Epoch 0 step 208: training loss: 1244.9787825419542\n",
      "Epoch 0 step 209: training accuarcy: 0.7435\n",
      "Epoch 0 step 209: training loss: 1269.424344711013\n",
      "Epoch 0 step 210: training accuarcy: 0.7505000000000001\n",
      "Epoch 0 step 210: training loss: 1240.2904273114796\n",
      "Epoch 0 step 211: training accuarcy: 0.758\n",
      "Epoch 0 step 211: training loss: 1266.76727484051\n",
      "Epoch 0 step 212: training accuarcy: 0.7615000000000001\n",
      "Epoch 0 step 212: training loss: 1223.028388430024\n",
      "Epoch 0 step 213: training accuarcy: 0.758\n",
      "Epoch 0 step 213: training loss: 1242.924716575533\n",
      "Epoch 0 step 214: training accuarcy: 0.756\n",
      "Epoch 0 step 214: training loss: 1261.0072289654\n",
      "Epoch 0 step 215: training accuarcy: 0.759\n",
      "Epoch 0 step 215: training loss: 1244.259047084801\n",
      "Epoch 0 step 216: training accuarcy: 0.744\n",
      "Epoch 0 step 216: training loss: 1243.355309099968\n",
      "Epoch 0 step 217: training accuarcy: 0.7395\n",
      "Epoch 0 step 217: training loss: 1268.677417855091\n",
      "Epoch 0 step 218: training accuarcy: 0.7465\n",
      "Epoch 0 step 218: training loss: 1229.861817365198\n",
      "Epoch 0 step 219: training accuarcy: 0.7655000000000001\n",
      "Epoch 0 step 219: training loss: 1203.7865883046366\n",
      "Epoch 0 step 220: training accuarcy: 0.77\n",
      "Epoch 0 step 220: training loss: 1210.6534196198008\n",
      "Epoch 0 step 221: training accuarcy: 0.7565000000000001\n",
      "Epoch 0 step 221: training loss: 1230.6823238881832\n",
      "Epoch 0 step 222: training accuarcy: 0.771\n",
      "Epoch 0 step 222: training loss: 1253.5062320632273\n",
      "Epoch 0 step 223: training accuarcy: 0.7475\n",
      "Epoch 0 step 223: training loss: 1247.8326160549648\n",
      "Epoch 0 step 224: training accuarcy: 0.742\n",
      "Epoch 0 step 224: training loss: 1203.8451773123375\n",
      "Epoch 0 step 225: training accuarcy: 0.762\n",
      "Epoch 0 step 225: training loss: 1211.415656232749\n",
      "Epoch 0 step 226: training accuarcy: 0.757\n",
      "Epoch 0 step 226: training loss: 1216.6015399383402\n",
      "Epoch 0 step 227: training accuarcy: 0.744\n",
      "Epoch 0 step 227: training loss: 1229.6115861254748\n",
      "Epoch 0 step 228: training accuarcy: 0.7605000000000001\n",
      "Epoch 0 step 228: training loss: 1190.2041226217216\n",
      "Epoch 0 step 229: training accuarcy: 0.775\n",
      "Epoch 0 step 229: training loss: 1212.9622079593767\n",
      "Epoch 0 step 230: training accuarcy: 0.7505000000000001\n",
      "Epoch 0 step 230: training loss: 1194.455761742025\n",
      "Epoch 0 step 231: training accuarcy: 0.759\n",
      "Epoch 0 step 231: training loss: 1207.4279986391355\n",
      "Epoch 0 step 232: training accuarcy: 0.764\n",
      "Epoch 0 step 232: training loss: 1215.966989511947\n",
      "Epoch 0 step 233: training accuarcy: 0.747\n",
      "Epoch 0 step 233: training loss: 1225.2526471199246\n",
      "Epoch 0 step 234: training accuarcy: 0.7575000000000001\n",
      "Epoch 0 step 234: training loss: 1233.1429295845758\n",
      "Epoch 0 step 235: training accuarcy: 0.7325\n",
      "Epoch 0 step 235: training loss: 1205.6530943141545\n",
      "Epoch 0 step 236: training accuarcy: 0.7635000000000001\n",
      "Epoch 0 step 236: training loss: 1204.60663692423\n",
      "Epoch 0 step 237: training accuarcy: 0.7645000000000001\n",
      "Epoch 0 step 237: training loss: 1191.409744524526\n",
      "Epoch 0 step 238: training accuarcy: 0.7495\n",
      "Epoch 0 step 238: training loss: 1192.5152133249128\n",
      "Epoch 0 step 239: training accuarcy: 0.7615000000000001\n",
      "Epoch 0 step 239: training loss: 1206.1445434801144\n",
      "Epoch 0 step 240: training accuarcy: 0.749\n",
      "Epoch 0 step 240: training loss: 1206.6665055575922\n",
      "Epoch 0 step 241: training accuarcy: 0.7615000000000001\n",
      "Epoch 0 step 241: training loss: 1188.2501661468993\n",
      "Epoch 0 step 242: training accuarcy: 0.756\n",
      "Epoch 0 step 242: training loss: 1183.0804987972026\n",
      "Epoch 0 step 243: training accuarcy: 0.7795\n",
      "Epoch 0 step 243: training loss: 1165.7124897913786\n",
      "Epoch 0 step 244: training accuarcy: 0.7795\n",
      "Epoch 0 step 244: training loss: 1175.83687726523\n",
      "Epoch 0 step 245: training accuarcy: 0.768\n",
      "Epoch 0 step 245: training loss: 1231.2932820034546\n",
      "Epoch 0 step 246: training accuarcy: 0.7425\n",
      "Epoch 0 step 246: training loss: 1193.936980002959\n",
      "Epoch 0 step 247: training accuarcy: 0.758\n",
      "Epoch 0 step 247: training loss: 1164.1886519660886\n",
      "Epoch 0 step 248: training accuarcy: 0.7695\n",
      "Epoch 0 step 248: training loss: 1204.9233097621018\n",
      "Epoch 0 step 249: training accuarcy: 0.7545000000000001\n",
      "Epoch 0 step 249: training loss: 1165.1050919050801\n",
      "Epoch 0 step 250: training accuarcy: 0.7805\n",
      "Epoch 0 step 250: training loss: 1163.0457088369262\n",
      "Epoch 0 step 251: training accuarcy: 0.7805\n",
      "Epoch 0 step 251: training loss: 1149.1409502827\n",
      "Epoch 0 step 252: training accuarcy: 0.786\n",
      "Epoch 0 step 252: training loss: 1178.3368127892381\n",
      "Epoch 0 step 253: training accuarcy: 0.757\n",
      "Epoch 0 step 253: training loss: 1180.2129737066548\n",
      "Epoch 0 step 254: training accuarcy: 0.7695\n",
      "Epoch 0 step 254: training loss: 1164.492824638293\n",
      "Epoch 0 step 255: training accuarcy: 0.763\n",
      "Epoch 0 step 255: training loss: 1174.0845971296649\n",
      "Epoch 0 step 256: training accuarcy: 0.7485\n",
      "Epoch 0 step 256: training loss: 1156.976535468935\n",
      "Epoch 0 step 257: training accuarcy: 0.762\n",
      "Epoch 0 step 257: training loss: 1173.7151004093314\n",
      "Epoch 0 step 258: training accuarcy: 0.7665000000000001\n",
      "Epoch 0 step 258: training loss: 1159.9302352842378\n",
      "Epoch 0 step 259: training accuarcy: 0.782\n",
      "Epoch 0 step 259: training loss: 1159.0240436305096\n",
      "Epoch 0 step 260: training accuarcy: 0.77\n",
      "Epoch 0 step 260: training loss: 1156.4426011461028\n",
      "Epoch 0 step 261: training accuarcy: 0.762\n",
      "Epoch 0 step 261: training loss: 1157.7552367827989\n",
      "Epoch 0 step 262: training accuarcy: 0.7605000000000001\n",
      "Epoch 0 step 262: training loss: 511.75537940830503\n",
      "Epoch 0 step 263: training accuarcy: 0.7384615384615385\n",
      "Epoch 0: train loss 6818.446054383349, train accuarcy 0.637304961681366\n",
      "Epoch 0: valid loss 1145.579089724311, valid accuarcy 0.7562158703804016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|█████████████████████████████████████████████████████████▎                                                                                                                  | 1/3 [01:50<03:41, 110.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1167.1539860411112\n",
      "Epoch 1 step 264: training accuarcy: 0.76\n",
      "Epoch 1 step 264: training loss: 1181.8496994763996\n",
      "Epoch 1 step 265: training accuarcy: 0.776\n",
      "Epoch 1 step 265: training loss: 1159.0912795804231\n",
      "Epoch 1 step 266: training accuarcy: 0.7625000000000001\n",
      "Epoch 1 step 266: training loss: 1169.263601847875\n",
      "Epoch 1 step 267: training accuarcy: 0.776\n",
      "Epoch 1 step 267: training loss: 1138.7867774106874\n",
      "Epoch 1 step 268: training accuarcy: 0.77\n",
      "Epoch 1 step 268: training loss: 1159.3624794780665\n",
      "Epoch 1 step 269: training accuarcy: 0.7725\n",
      "Epoch 1 step 269: training loss: 1220.2681861367\n",
      "Epoch 1 step 270: training accuarcy: 0.752\n",
      "Epoch 1 step 270: training loss: 1158.408705790937\n",
      "Epoch 1 step 271: training accuarcy: 0.801\n",
      "Epoch 1 step 271: training loss: 1165.451715004065\n",
      "Epoch 1 step 272: training accuarcy: 0.766\n",
      "Epoch 1 step 272: training loss: 1160.3018765540446\n",
      "Epoch 1 step 273: training accuarcy: 0.765\n",
      "Epoch 1 step 273: training loss: 1164.7636622825814\n",
      "Epoch 1 step 274: training accuarcy: 0.7705\n",
      "Epoch 1 step 274: training loss: 1168.89782469509\n",
      "Epoch 1 step 275: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 275: training loss: 1158.837546234785\n",
      "Epoch 1 step 276: training accuarcy: 0.763\n",
      "Epoch 1 step 276: training loss: 1127.3509487895328\n",
      "Epoch 1 step 277: training accuarcy: 0.7695\n",
      "Epoch 1 step 277: training loss: 1135.4948688779243\n",
      "Epoch 1 step 278: training accuarcy: 0.7655000000000001\n",
      "Epoch 1 step 278: training loss: 1168.2559326064252\n",
      "Epoch 1 step 279: training accuarcy: 0.7655000000000001\n",
      "Epoch 1 step 279: training loss: 1112.1381525884246\n",
      "Epoch 1 step 280: training accuarcy: 0.788\n",
      "Epoch 1 step 280: training loss: 1151.975026601623\n",
      "Epoch 1 step 281: training accuarcy: 0.774\n",
      "Epoch 1 step 281: training loss: 1165.1709226637875\n",
      "Epoch 1 step 282: training accuarcy: 0.785\n",
      "Epoch 1 step 282: training loss: 1134.6085819708146\n",
      "Epoch 1 step 283: training accuarcy: 0.7755\n",
      "Epoch 1 step 283: training loss: 1125.8825873630271\n",
      "Epoch 1 step 284: training accuarcy: 0.782\n",
      "Epoch 1 step 284: training loss: 1134.6047488602742\n",
      "Epoch 1 step 285: training accuarcy: 0.783\n",
      "Epoch 1 step 285: training loss: 1150.0950600422266\n",
      "Epoch 1 step 286: training accuarcy: 0.771\n",
      "Epoch 1 step 286: training loss: 1182.9228337929837\n",
      "Epoch 1 step 287: training accuarcy: 0.768\n",
      "Epoch 1 step 287: training loss: 1138.1998973520526\n",
      "Epoch 1 step 288: training accuarcy: 0.771\n",
      "Epoch 1 step 288: training loss: 1160.415156363784\n",
      "Epoch 1 step 289: training accuarcy: 0.763\n",
      "Epoch 1 step 289: training loss: 1124.1898338456049\n",
      "Epoch 1 step 290: training accuarcy: 0.776\n",
      "Epoch 1 step 290: training loss: 1147.783004684896\n",
      "Epoch 1 step 291: training accuarcy: 0.776\n",
      "Epoch 1 step 291: training loss: 1162.4036702861663\n",
      "Epoch 1 step 292: training accuarcy: 0.7695\n",
      "Epoch 1 step 292: training loss: 1107.7017595368297\n",
      "Epoch 1 step 293: training accuarcy: 0.775\n",
      "Epoch 1 step 293: training loss: 1117.4904439417066\n",
      "Epoch 1 step 294: training accuarcy: 0.775\n",
      "Epoch 1 step 294: training loss: 1123.0528636265453\n",
      "Epoch 1 step 295: training accuarcy: 0.777\n",
      "Epoch 1 step 295: training loss: 1120.15396548402\n",
      "Epoch 1 step 296: training accuarcy: 0.7845\n",
      "Epoch 1 step 296: training loss: 1128.4856374136873\n",
      "Epoch 1 step 297: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 297: training loss: 1127.270601189114\n",
      "Epoch 1 step 298: training accuarcy: 0.767\n",
      "Epoch 1 step 298: training loss: 1103.2748810606495\n",
      "Epoch 1 step 299: training accuarcy: 0.783\n",
      "Epoch 1 step 299: training loss: 1100.910787233035\n",
      "Epoch 1 step 300: training accuarcy: 0.7745\n",
      "Epoch 1 step 300: training loss: 1150.5283611301957\n",
      "Epoch 1 step 301: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 301: training loss: 1129.1177159166082\n",
      "Epoch 1 step 302: training accuarcy: 0.7685\n",
      "Epoch 1 step 302: training loss: 1116.726677856935\n",
      "Epoch 1 step 303: training accuarcy: 0.7735\n",
      "Epoch 1 step 303: training loss: 1132.0488801208635\n",
      "Epoch 1 step 304: training accuarcy: 0.7715\n",
      "Epoch 1 step 304: training loss: 1138.2751619885075\n",
      "Epoch 1 step 305: training accuarcy: 0.77\n",
      "Epoch 1 step 305: training loss: 1121.3475535356338\n",
      "Epoch 1 step 306: training accuarcy: 0.777\n",
      "Epoch 1 step 306: training loss: 1116.048457151686\n",
      "Epoch 1 step 307: training accuarcy: 0.7795\n",
      "Epoch 1 step 307: training loss: 1123.2653554129745\n",
      "Epoch 1 step 308: training accuarcy: 0.7715\n",
      "Epoch 1 step 308: training loss: 1129.9217662296437\n",
      "Epoch 1 step 309: training accuarcy: 0.7625000000000001\n",
      "Epoch 1 step 309: training loss: 1140.8606579911377\n",
      "Epoch 1 step 310: training accuarcy: 0.7575000000000001\n",
      "Epoch 1 step 310: training loss: 1124.3282792429486\n",
      "Epoch 1 step 311: training accuarcy: 0.777\n",
      "Epoch 1 step 311: training loss: 1124.7741338421924\n",
      "Epoch 1 step 312: training accuarcy: 0.775\n",
      "Epoch 1 step 312: training loss: 1113.7434524083815\n",
      "Epoch 1 step 313: training accuarcy: 0.7905\n",
      "Epoch 1 step 313: training loss: 1146.5488495421127\n",
      "Epoch 1 step 314: training accuarcy: 0.763\n",
      "Epoch 1 step 314: training loss: 1168.59894008736\n",
      "Epoch 1 step 315: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 315: training loss: 1115.8389143434345\n",
      "Epoch 1 step 316: training accuarcy: 0.7855\n",
      "Epoch 1 step 316: training loss: 1078.0551150546637\n",
      "Epoch 1 step 317: training accuarcy: 0.7845\n",
      "Epoch 1 step 317: training loss: 1140.797197701904\n",
      "Epoch 1 step 318: training accuarcy: 0.774\n",
      "Epoch 1 step 318: training loss: 1126.8697379179603\n",
      "Epoch 1 step 319: training accuarcy: 0.77\n",
      "Epoch 1 step 319: training loss: 1150.4505304628528\n",
      "Epoch 1 step 320: training accuarcy: 0.754\n",
      "Epoch 1 step 320: training loss: 1129.7684820322047\n",
      "Epoch 1 step 321: training accuarcy: 0.782\n",
      "Epoch 1 step 321: training loss: 1122.337154371582\n",
      "Epoch 1 step 322: training accuarcy: 0.779\n",
      "Epoch 1 step 322: training loss: 1116.5932026786775\n",
      "Epoch 1 step 323: training accuarcy: 0.78\n",
      "Epoch 1 step 323: training loss: 1135.3096673703644\n",
      "Epoch 1 step 324: training accuarcy: 0.785\n",
      "Epoch 1 step 324: training loss: 1116.184677990802\n",
      "Epoch 1 step 325: training accuarcy: 0.778\n",
      "Epoch 1 step 325: training loss: 1149.9109350929496\n",
      "Epoch 1 step 326: training accuarcy: 0.771\n",
      "Epoch 1 step 326: training loss: 1111.4467730275069\n",
      "Epoch 1 step 327: training accuarcy: 0.7915\n",
      "Epoch 1 step 327: training loss: 1117.6000532751582\n",
      "Epoch 1 step 328: training accuarcy: 0.7835\n",
      "Epoch 1 step 328: training loss: 1125.7510107837224\n",
      "Epoch 1 step 329: training accuarcy: 0.778\n",
      "Epoch 1 step 329: training loss: 1121.572439903347\n",
      "Epoch 1 step 330: training accuarcy: 0.7755\n",
      "Epoch 1 step 330: training loss: 1104.7763432888103\n",
      "Epoch 1 step 331: training accuarcy: 0.783\n",
      "Epoch 1 step 331: training loss: 1094.29926664977\n",
      "Epoch 1 step 332: training accuarcy: 0.7925\n",
      "Epoch 1 step 332: training loss: 1105.3385779002306\n",
      "Epoch 1 step 333: training accuarcy: 0.797\n",
      "Epoch 1 step 333: training loss: 1161.0564436227173\n",
      "Epoch 1 step 334: training accuarcy: 0.7565000000000001\n",
      "Epoch 1 step 334: training loss: 1098.2649890865655\n",
      "Epoch 1 step 335: training accuarcy: 0.786\n",
      "Epoch 1 step 335: training loss: 1128.8221635363263\n",
      "Epoch 1 step 336: training accuarcy: 0.7885\n",
      "Epoch 1 step 336: training loss: 1119.7615721166783\n",
      "Epoch 1 step 337: training accuarcy: 0.774\n",
      "Epoch 1 step 337: training loss: 1094.5968592332306\n",
      "Epoch 1 step 338: training accuarcy: 0.7725\n",
      "Epoch 1 step 338: training loss: 1102.7258986457466\n",
      "Epoch 1 step 339: training accuarcy: 0.772\n",
      "Epoch 1 step 339: training loss: 1101.9391000575497\n",
      "Epoch 1 step 340: training accuarcy: 0.7685\n",
      "Epoch 1 step 340: training loss: 1092.5150312955018\n",
      "Epoch 1 step 341: training accuarcy: 0.788\n",
      "Epoch 1 step 341: training loss: 1117.2012149479785\n",
      "Epoch 1 step 342: training accuarcy: 0.771\n",
      "Epoch 1 step 342: training loss: 1102.3701935074469\n",
      "Epoch 1 step 343: training accuarcy: 0.7915\n",
      "Epoch 1 step 343: training loss: 1096.2417054378675\n",
      "Epoch 1 step 344: training accuarcy: 0.7885\n",
      "Epoch 1 step 344: training loss: 1121.3709916841794\n",
      "Epoch 1 step 345: training accuarcy: 0.7745\n",
      "Epoch 1 step 345: training loss: 1156.6103351967183\n",
      "Epoch 1 step 346: training accuarcy: 0.7795\n",
      "Epoch 1 step 346: training loss: 1127.9722897618196\n",
      "Epoch 1 step 347: training accuarcy: 0.777\n",
      "Epoch 1 step 347: training loss: 1113.9547847168578\n",
      "Epoch 1 step 348: training accuarcy: 0.764\n",
      "Epoch 1 step 348: training loss: 1115.2374820329107\n",
      "Epoch 1 step 349: training accuarcy: 0.7785\n",
      "Epoch 1 step 349: training loss: 1113.8541024173003\n",
      "Epoch 1 step 350: training accuarcy: 0.7805\n",
      "Epoch 1 step 350: training loss: 1137.9030288910521\n",
      "Epoch 1 step 351: training accuarcy: 0.7865\n",
      "Epoch 1 step 351: training loss: 1111.1342514787766\n",
      "Epoch 1 step 352: training accuarcy: 0.7735\n",
      "Epoch 1 step 352: training loss: 1101.3559673318293\n",
      "Epoch 1 step 353: training accuarcy: 0.793\n",
      "Epoch 1 step 353: training loss: 1130.5980674631667\n",
      "Epoch 1 step 354: training accuarcy: 0.784\n",
      "Epoch 1 step 354: training loss: 1102.8930207997716\n",
      "Epoch 1 step 355: training accuarcy: 0.7755\n",
      "Epoch 1 step 355: training loss: 1096.5910239802865\n",
      "Epoch 1 step 356: training accuarcy: 0.7735\n",
      "Epoch 1 step 356: training loss: 1129.374968057376\n",
      "Epoch 1 step 357: training accuarcy: 0.768\n",
      "Epoch 1 step 357: training loss: 1121.2258917343147\n",
      "Epoch 1 step 358: training accuarcy: 0.776\n",
      "Epoch 1 step 358: training loss: 1091.55510592214\n",
      "Epoch 1 step 359: training accuarcy: 0.7895\n",
      "Epoch 1 step 359: training loss: 1102.715498610692\n",
      "Epoch 1 step 360: training accuarcy: 0.7735\n",
      "Epoch 1 step 360: training loss: 1119.1383837571548\n",
      "Epoch 1 step 361: training accuarcy: 0.785\n",
      "Epoch 1 step 361: training loss: 1080.9225896674461\n",
      "Epoch 1 step 362: training accuarcy: 0.797\n",
      "Epoch 1 step 362: training loss: 1102.610297204055\n",
      "Epoch 1 step 363: training accuarcy: 0.7845\n",
      "Epoch 1 step 363: training loss: 1114.47166239014\n",
      "Epoch 1 step 364: training accuarcy: 0.782\n",
      "Epoch 1 step 364: training loss: 1104.3995153606643\n",
      "Epoch 1 step 365: training accuarcy: 0.7685\n",
      "Epoch 1 step 365: training loss: 1101.5153676252191\n",
      "Epoch 1 step 366: training accuarcy: 0.794\n",
      "Epoch 1 step 366: training loss: 1080.0142459708475\n",
      "Epoch 1 step 367: training accuarcy: 0.7985\n",
      "Epoch 1 step 367: training loss: 1118.1217333157167\n",
      "Epoch 1 step 368: training accuarcy: 0.7865\n",
      "Epoch 1 step 368: training loss: 1077.3120316259296\n",
      "Epoch 1 step 369: training accuarcy: 0.794\n",
      "Epoch 1 step 369: training loss: 1131.859778789372\n",
      "Epoch 1 step 370: training accuarcy: 0.7925\n",
      "Epoch 1 step 370: training loss: 1138.6916959821929\n",
      "Epoch 1 step 371: training accuarcy: 0.778\n",
      "Epoch 1 step 371: training loss: 1119.4817590522941\n",
      "Epoch 1 step 372: training accuarcy: 0.7785\n",
      "Epoch 1 step 372: training loss: 1129.2112554925377\n",
      "Epoch 1 step 373: training accuarcy: 0.7745\n",
      "Epoch 1 step 373: training loss: 1104.8751012479972\n",
      "Epoch 1 step 374: training accuarcy: 0.766\n",
      "Epoch 1 step 374: training loss: 1091.9225170465772\n",
      "Epoch 1 step 375: training accuarcy: 0.7785\n",
      "Epoch 1 step 375: training loss: 1119.663513893991\n",
      "Epoch 1 step 376: training accuarcy: 0.779\n",
      "Epoch 1 step 376: training loss: 1112.8784820485093\n",
      "Epoch 1 step 377: training accuarcy: 0.773\n",
      "Epoch 1 step 377: training loss: 1116.6817789769348\n",
      "Epoch 1 step 378: training accuarcy: 0.775\n",
      "Epoch 1 step 378: training loss: 1132.1557610255013\n",
      "Epoch 1 step 379: training accuarcy: 0.7765\n",
      "Epoch 1 step 379: training loss: 1143.1387304323025\n",
      "Epoch 1 step 380: training accuarcy: 0.7655000000000001\n",
      "Epoch 1 step 380: training loss: 1118.0124324639942\n",
      "Epoch 1 step 381: training accuarcy: 0.7725\n",
      "Epoch 1 step 381: training loss: 1108.6408079332223\n",
      "Epoch 1 step 382: training accuarcy: 0.783\n",
      "Epoch 1 step 382: training loss: 1145.6643899427654\n",
      "Epoch 1 step 383: training accuarcy: 0.769\n",
      "Epoch 1 step 383: training loss: 1100.6366129858463\n",
      "Epoch 1 step 384: training accuarcy: 0.7895\n",
      "Epoch 1 step 384: training loss: 1099.8320047620937\n",
      "Epoch 1 step 385: training accuarcy: 0.7755\n",
      "Epoch 1 step 385: training loss: 1063.387091546465\n",
      "Epoch 1 step 386: training accuarcy: 0.794\n",
      "Epoch 1 step 386: training loss: 1093.7953280120548\n",
      "Epoch 1 step 387: training accuarcy: 0.7955\n",
      "Epoch 1 step 387: training loss: 1120.4279259042523\n",
      "Epoch 1 step 388: training accuarcy: 0.776\n",
      "Epoch 1 step 388: training loss: 1102.7013925798194\n",
      "Epoch 1 step 389: training accuarcy: 0.794\n",
      "Epoch 1 step 389: training loss: 1067.5156049979244\n",
      "Epoch 1 step 390: training accuarcy: 0.8035\n",
      "Epoch 1 step 390: training loss: 1105.110979310234\n",
      "Epoch 1 step 391: training accuarcy: 0.7945\n",
      "Epoch 1 step 391: training loss: 1074.756414260086\n",
      "Epoch 1 step 392: training accuarcy: 0.809\n",
      "Epoch 1 step 392: training loss: 1124.5587996855322\n",
      "Epoch 1 step 393: training accuarcy: 0.7795\n",
      "Epoch 1 step 393: training loss: 1094.6720846563805\n",
      "Epoch 1 step 394: training accuarcy: 0.785\n",
      "Epoch 1 step 394: training loss: 1122.195198108193\n",
      "Epoch 1 step 395: training accuarcy: 0.7715\n",
      "Epoch 1 step 395: training loss: 1101.1310768767312\n",
      "Epoch 1 step 396: training accuarcy: 0.7915\n",
      "Epoch 1 step 396: training loss: 1091.8046095140776\n",
      "Epoch 1 step 397: training accuarcy: 0.7975\n",
      "Epoch 1 step 397: training loss: 1118.3353870280382\n",
      "Epoch 1 step 398: training accuarcy: 0.787\n",
      "Epoch 1 step 398: training loss: 1123.252840128123\n",
      "Epoch 1 step 399: training accuarcy: 0.8085\n",
      "Epoch 1 step 399: training loss: 1068.9276921495718\n",
      "Epoch 1 step 400: training accuarcy: 0.7985\n",
      "Epoch 1 step 400: training loss: 1114.8592406311695\n",
      "Epoch 1 step 401: training accuarcy: 0.775\n",
      "Epoch 1 step 401: training loss: 1096.325739520649\n",
      "Epoch 1 step 402: training accuarcy: 0.794\n",
      "Epoch 1 step 402: training loss: 1093.159496098094\n",
      "Epoch 1 step 403: training accuarcy: 0.775\n",
      "Epoch 1 step 403: training loss: 1072.6399689437096\n",
      "Epoch 1 step 404: training accuarcy: 0.787\n",
      "Epoch 1 step 404: training loss: 1080.6731935139\n",
      "Epoch 1 step 405: training accuarcy: 0.7875\n",
      "Epoch 1 step 405: training loss: 1083.248153249557\n",
      "Epoch 1 step 406: training accuarcy: 0.799\n",
      "Epoch 1 step 406: training loss: 1072.6875873252989\n",
      "Epoch 1 step 407: training accuarcy: 0.7935\n",
      "Epoch 1 step 407: training loss: 1094.7700675297021\n",
      "Epoch 1 step 408: training accuarcy: 0.7855\n",
      "Epoch 1 step 408: training loss: 1098.092944774148\n",
      "Epoch 1 step 409: training accuarcy: 0.786\n",
      "Epoch 1 step 409: training loss: 1108.9472890274346\n",
      "Epoch 1 step 410: training accuarcy: 0.7705\n",
      "Epoch 1 step 410: training loss: 1107.0868068544848\n",
      "Epoch 1 step 411: training accuarcy: 0.7875\n",
      "Epoch 1 step 411: training loss: 1119.3005956377954\n",
      "Epoch 1 step 412: training accuarcy: 0.765\n",
      "Epoch 1 step 412: training loss: 1107.8709618445741\n",
      "Epoch 1 step 413: training accuarcy: 0.782\n",
      "Epoch 1 step 413: training loss: 1072.5446856379438\n",
      "Epoch 1 step 414: training accuarcy: 0.785\n",
      "Epoch 1 step 414: training loss: 1116.7907773462016\n",
      "Epoch 1 step 415: training accuarcy: 0.767\n",
      "Epoch 1 step 415: training loss: 1092.2561565420378\n",
      "Epoch 1 step 416: training accuarcy: 0.784\n",
      "Epoch 1 step 416: training loss: 1114.4140863502012\n",
      "Epoch 1 step 417: training accuarcy: 0.787\n",
      "Epoch 1 step 417: training loss: 1103.2707446479849\n",
      "Epoch 1 step 418: training accuarcy: 0.7815\n",
      "Epoch 1 step 418: training loss: 1071.8326277894864\n",
      "Epoch 1 step 419: training accuarcy: 0.808\n",
      "Epoch 1 step 419: training loss: 1081.879930414736\n",
      "Epoch 1 step 420: training accuarcy: 0.7905\n",
      "Epoch 1 step 420: training loss: 1112.5324866664637\n",
      "Epoch 1 step 421: training accuarcy: 0.787\n",
      "Epoch 1 step 421: training loss: 1132.1751061270227\n",
      "Epoch 1 step 422: training accuarcy: 0.7805\n",
      "Epoch 1 step 422: training loss: 1134.730203842716\n",
      "Epoch 1 step 423: training accuarcy: 0.7905\n",
      "Epoch 1 step 423: training loss: 1098.2495654239835\n",
      "Epoch 1 step 424: training accuarcy: 0.774\n",
      "Epoch 1 step 424: training loss: 1106.544963135682\n",
      "Epoch 1 step 425: training accuarcy: 0.784\n",
      "Epoch 1 step 425: training loss: 1100.7078920594681\n",
      "Epoch 1 step 426: training accuarcy: 0.7885\n",
      "Epoch 1 step 426: training loss: 1084.4030173021508\n",
      "Epoch 1 step 427: training accuarcy: 0.8035\n",
      "Epoch 1 step 427: training loss: 1074.9287806226728\n",
      "Epoch 1 step 428: training accuarcy: 0.8175\n",
      "Epoch 1 step 428: training loss: 1082.9857500467806\n",
      "Epoch 1 step 429: training accuarcy: 0.7995\n",
      "Epoch 1 step 429: training loss: 1090.910713552624\n",
      "Epoch 1 step 430: training accuarcy: 0.791\n",
      "Epoch 1 step 430: training loss: 1090.4459845264726\n",
      "Epoch 1 step 431: training accuarcy: 0.7995\n",
      "Epoch 1 step 431: training loss: 1096.6329337175805\n",
      "Epoch 1 step 432: training accuarcy: 0.8115\n",
      "Epoch 1 step 432: training loss: 1114.9577673418123\n",
      "Epoch 1 step 433: training accuarcy: 0.795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 433: training loss: 1116.6090469699702\n",
      "Epoch 1 step 434: training accuarcy: 0.7685\n",
      "Epoch 1 step 434: training loss: 1109.9365692266792\n",
      "Epoch 1 step 435: training accuarcy: 0.7815\n",
      "Epoch 1 step 435: training loss: 1091.6676773468048\n",
      "Epoch 1 step 436: training accuarcy: 0.8035\n",
      "Epoch 1 step 436: training loss: 1094.0061469619645\n",
      "Epoch 1 step 437: training accuarcy: 0.783\n",
      "Epoch 1 step 437: training loss: 1090.9326863794192\n",
      "Epoch 1 step 438: training accuarcy: 0.804\n",
      "Epoch 1 step 438: training loss: 1097.8986508429678\n",
      "Epoch 1 step 439: training accuarcy: 0.7845\n",
      "Epoch 1 step 439: training loss: 1082.479106029879\n",
      "Epoch 1 step 440: training accuarcy: 0.789\n",
      "Epoch 1 step 440: training loss: 1077.5749124581898\n",
      "Epoch 1 step 441: training accuarcy: 0.805\n",
      "Epoch 1 step 441: training loss: 1117.718832860423\n",
      "Epoch 1 step 442: training accuarcy: 0.7855\n",
      "Epoch 1 step 442: training loss: 1072.5240612646796\n",
      "Epoch 1 step 443: training accuarcy: 0.7965\n",
      "Epoch 1 step 443: training loss: 1104.0791448174082\n",
      "Epoch 1 step 444: training accuarcy: 0.7925\n",
      "Epoch 1 step 444: training loss: 1069.8123974501734\n",
      "Epoch 1 step 445: training accuarcy: 0.7955\n",
      "Epoch 1 step 445: training loss: 1106.443155767385\n",
      "Epoch 1 step 446: training accuarcy: 0.786\n",
      "Epoch 1 step 446: training loss: 1109.0155916887206\n",
      "Epoch 1 step 447: training accuarcy: 0.7985\n",
      "Epoch 1 step 447: training loss: 1090.3884050351733\n",
      "Epoch 1 step 448: training accuarcy: 0.7955\n",
      "Epoch 1 step 448: training loss: 1063.6108748081078\n",
      "Epoch 1 step 449: training accuarcy: 0.7935\n",
      "Epoch 1 step 449: training loss: 1067.2627964134065\n",
      "Epoch 1 step 450: training accuarcy: 0.8015\n",
      "Epoch 1 step 450: training loss: 1094.5582407972718\n",
      "Epoch 1 step 451: training accuarcy: 0.799\n",
      "Epoch 1 step 451: training loss: 1086.1927654325739\n",
      "Epoch 1 step 452: training accuarcy: 0.7855\n",
      "Epoch 1 step 452: training loss: 1115.7067704182934\n",
      "Epoch 1 step 453: training accuarcy: 0.782\n",
      "Epoch 1 step 453: training loss: 1123.7396738306575\n",
      "Epoch 1 step 454: training accuarcy: 0.7785\n",
      "Epoch 1 step 454: training loss: 1136.1903413521486\n",
      "Epoch 1 step 455: training accuarcy: 0.782\n",
      "Epoch 1 step 455: training loss: 1086.0194420920673\n",
      "Epoch 1 step 456: training accuarcy: 0.801\n",
      "Epoch 1 step 456: training loss: 1105.4443136449056\n",
      "Epoch 1 step 457: training accuarcy: 0.7915\n",
      "Epoch 1 step 457: training loss: 1089.1417333736078\n",
      "Epoch 1 step 458: training accuarcy: 0.7925\n",
      "Epoch 1 step 458: training loss: 1083.7239512260562\n",
      "Epoch 1 step 459: training accuarcy: 0.7855\n",
      "Epoch 1 step 459: training loss: 1137.9069085406904\n",
      "Epoch 1 step 460: training accuarcy: 0.781\n",
      "Epoch 1 step 460: training loss: 1090.3853242315556\n",
      "Epoch 1 step 461: training accuarcy: 0.7975\n",
      "Epoch 1 step 461: training loss: 1073.6238627265288\n",
      "Epoch 1 step 462: training accuarcy: 0.8015\n",
      "Epoch 1 step 462: training loss: 1066.3329441033088\n",
      "Epoch 1 step 463: training accuarcy: 0.8245\n",
      "Epoch 1 step 463: training loss: 1091.5349557764255\n",
      "Epoch 1 step 464: training accuarcy: 0.7875\n",
      "Epoch 1 step 464: training loss: 1104.4149540252345\n",
      "Epoch 1 step 465: training accuarcy: 0.8025\n",
      "Epoch 1 step 465: training loss: 1103.225632670681\n",
      "Epoch 1 step 466: training accuarcy: 0.7975\n",
      "Epoch 1 step 466: training loss: 1090.0313699619933\n",
      "Epoch 1 step 467: training accuarcy: 0.812\n",
      "Epoch 1 step 467: training loss: 1087.1581281440833\n",
      "Epoch 1 step 468: training accuarcy: 0.8\n",
      "Epoch 1 step 468: training loss: 1097.7248390195953\n",
      "Epoch 1 step 469: training accuarcy: 0.7745\n",
      "Epoch 1 step 469: training loss: 1121.7212437831681\n",
      "Epoch 1 step 470: training accuarcy: 0.799\n",
      "Epoch 1 step 470: training loss: 1073.1269175780105\n",
      "Epoch 1 step 471: training accuarcy: 0.803\n",
      "Epoch 1 step 471: training loss: 1073.6495035313912\n",
      "Epoch 1 step 472: training accuarcy: 0.7965\n",
      "Epoch 1 step 472: training loss: 1093.2056350991593\n",
      "Epoch 1 step 473: training accuarcy: 0.792\n",
      "Epoch 1 step 473: training loss: 1092.3586920959265\n",
      "Epoch 1 step 474: training accuarcy: 0.7895\n",
      "Epoch 1 step 474: training loss: 1113.9800218862972\n",
      "Epoch 1 step 475: training accuarcy: 0.778\n",
      "Epoch 1 step 475: training loss: 1084.4022637341143\n",
      "Epoch 1 step 476: training accuarcy: 0.7855\n",
      "Epoch 1 step 476: training loss: 1068.9740129129327\n",
      "Epoch 1 step 477: training accuarcy: 0.794\n",
      "Epoch 1 step 477: training loss: 1088.049402554235\n",
      "Epoch 1 step 478: training accuarcy: 0.8\n",
      "Epoch 1 step 478: training loss: 1068.949529862233\n",
      "Epoch 1 step 479: training accuarcy: 0.808\n",
      "Epoch 1 step 479: training loss: 1070.746869916447\n",
      "Epoch 1 step 480: training accuarcy: 0.787\n",
      "Epoch 1 step 480: training loss: 1062.737075341114\n",
      "Epoch 1 step 481: training accuarcy: 0.7875\n",
      "Epoch 1 step 481: training loss: 1075.1531770932263\n",
      "Epoch 1 step 482: training accuarcy: 0.805\n",
      "Epoch 1 step 482: training loss: 1138.0199765164618\n",
      "Epoch 1 step 483: training accuarcy: 0.769\n",
      "Epoch 1 step 483: training loss: 1105.3512220708433\n",
      "Epoch 1 step 484: training accuarcy: 0.7985\n",
      "Epoch 1 step 484: training loss: 1087.7755248632582\n",
      "Epoch 1 step 485: training accuarcy: 0.7865\n",
      "Epoch 1 step 485: training loss: 1061.2686319280565\n",
      "Epoch 1 step 486: training accuarcy: 0.8\n",
      "Epoch 1 step 486: training loss: 1053.7963774973693\n",
      "Epoch 1 step 487: training accuarcy: 0.8095\n",
      "Epoch 1 step 487: training loss: 1063.0745597508308\n",
      "Epoch 1 step 488: training accuarcy: 0.8075\n",
      "Epoch 1 step 488: training loss: 1095.5933003724263\n",
      "Epoch 1 step 489: training accuarcy: 0.788\n",
      "Epoch 1 step 489: training loss: 1087.5445644882197\n",
      "Epoch 1 step 490: training accuarcy: 0.81\n",
      "Epoch 1 step 490: training loss: 1073.8445209764704\n",
      "Epoch 1 step 491: training accuarcy: 0.7985\n",
      "Epoch 1 step 491: training loss: 1079.0626961226897\n",
      "Epoch 1 step 492: training accuarcy: 0.808\n",
      "Epoch 1 step 492: training loss: 1062.3399515232354\n",
      "Epoch 1 step 493: training accuarcy: 0.8105\n",
      "Epoch 1 step 493: training loss: 1069.8020424139413\n",
      "Epoch 1 step 494: training accuarcy: 0.7995\n",
      "Epoch 1 step 494: training loss: 1091.4149067722888\n",
      "Epoch 1 step 495: training accuarcy: 0.791\n",
      "Epoch 1 step 495: training loss: 1064.9109781950071\n",
      "Epoch 1 step 496: training accuarcy: 0.8045\n",
      "Epoch 1 step 496: training loss: 1073.269591519625\n",
      "Epoch 1 step 497: training accuarcy: 0.8005\n",
      "Epoch 1 step 497: training loss: 1087.5266062568799\n",
      "Epoch 1 step 498: training accuarcy: 0.7955\n",
      "Epoch 1 step 498: training loss: 1095.8441819179302\n",
      "Epoch 1 step 499: training accuarcy: 0.7905\n",
      "Epoch 1 step 499: training loss: 1087.09168180606\n",
      "Epoch 1 step 500: training accuarcy: 0.8005\n",
      "Epoch 1 step 500: training loss: 1047.8091334936312\n",
      "Epoch 1 step 501: training accuarcy: 0.81\n",
      "Epoch 1 step 501: training loss: 1100.6077997418693\n",
      "Epoch 1 step 502: training accuarcy: 0.792\n",
      "Epoch 1 step 502: training loss: 1091.5090796282686\n",
      "Epoch 1 step 503: training accuarcy: 0.7855\n",
      "Epoch 1 step 503: training loss: 1067.2100790271911\n",
      "Epoch 1 step 504: training accuarcy: 0.7955\n",
      "Epoch 1 step 504: training loss: 1083.8533165819777\n",
      "Epoch 1 step 505: training accuarcy: 0.793\n",
      "Epoch 1 step 505: training loss: 1070.336900936696\n",
      "Epoch 1 step 506: training accuarcy: 0.7875\n",
      "Epoch 1 step 506: training loss: 1023.6916881159776\n",
      "Epoch 1 step 507: training accuarcy: 0.8175\n",
      "Epoch 1 step 507: training loss: 1062.3225840182315\n",
      "Epoch 1 step 508: training accuarcy: 0.808\n",
      "Epoch 1 step 508: training loss: 1087.0949787322984\n",
      "Epoch 1 step 509: training accuarcy: 0.803\n",
      "Epoch 1 step 509: training loss: 1094.2440722863935\n",
      "Epoch 1 step 510: training accuarcy: 0.788\n",
      "Epoch 1 step 510: training loss: 1085.069242460387\n",
      "Epoch 1 step 511: training accuarcy: 0.8025\n",
      "Epoch 1 step 511: training loss: 1066.1499618380753\n",
      "Epoch 1 step 512: training accuarcy: 0.796\n",
      "Epoch 1 step 512: training loss: 1068.3271467815428\n",
      "Epoch 1 step 513: training accuarcy: 0.7945\n",
      "Epoch 1 step 513: training loss: 1063.1484763981573\n",
      "Epoch 1 step 514: training accuarcy: 0.792\n",
      "Epoch 1 step 514: training loss: 1094.1550166872105\n",
      "Epoch 1 step 515: training accuarcy: 0.787\n",
      "Epoch 1 step 515: training loss: 1110.341447940341\n",
      "Epoch 1 step 516: training accuarcy: 0.7865\n",
      "Epoch 1 step 516: training loss: 1094.3623112362213\n",
      "Epoch 1 step 517: training accuarcy: 0.784\n",
      "Epoch 1 step 517: training loss: 1080.3067302147738\n",
      "Epoch 1 step 518: training accuarcy: 0.7845\n",
      "Epoch 1 step 518: training loss: 1062.3939661231107\n",
      "Epoch 1 step 519: training accuarcy: 0.8015\n",
      "Epoch 1 step 519: training loss: 1082.7511064047949\n",
      "Epoch 1 step 520: training accuarcy: 0.796\n",
      "Epoch 1 step 520: training loss: 1050.0231073630225\n",
      "Epoch 1 step 521: training accuarcy: 0.8015\n",
      "Epoch 1 step 521: training loss: 1125.3357785688786\n",
      "Epoch 1 step 522: training accuarcy: 0.7805\n",
      "Epoch 1 step 522: training loss: 1101.2198222779134\n",
      "Epoch 1 step 523: training accuarcy: 0.8\n",
      "Epoch 1 step 523: training loss: 1101.8857838171618\n",
      "Epoch 1 step 524: training accuarcy: 0.7965\n",
      "Epoch 1 step 524: training loss: 1084.460041585769\n",
      "Epoch 1 step 525: training accuarcy: 0.7895\n",
      "Epoch 1 step 525: training loss: 446.8883271417407\n",
      "Epoch 1 step 526: training accuarcy: 0.7987179487179488\n",
      "Epoch 1: train loss 1104.9183523568197, train accuarcy 0.7327117919921875\n",
      "Epoch 1: valid loss 1067.9201666851422, valid accuarcy 0.8007883429527283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 2/3 [03:43<01:51, 111.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 1079.2565521023134\n",
      "Epoch 2 step 527: training accuarcy: 0.8180000000000001\n",
      "Epoch 2 step 527: training loss: 1054.6608990216007\n",
      "Epoch 2 step 528: training accuarcy: 0.81\n",
      "Epoch 2 step 528: training loss: 1081.114045001487\n",
      "Epoch 2 step 529: training accuarcy: 0.8140000000000001\n",
      "Epoch 2 step 529: training loss: 1129.2531142538235\n",
      "Epoch 2 step 530: training accuarcy: 0.785\n",
      "Epoch 2 step 530: training loss: 1075.2504198332804\n",
      "Epoch 2 step 531: training accuarcy: 0.803\n",
      "Epoch 2 step 531: training loss: 1097.8783071993746\n",
      "Epoch 2 step 532: training accuarcy: 0.8\n",
      "Epoch 2 step 532: training loss: 1124.0145303906029\n",
      "Epoch 2 step 533: training accuarcy: 0.8005\n",
      "Epoch 2 step 533: training loss: 1085.8969670247088\n",
      "Epoch 2 step 534: training accuarcy: 0.809\n",
      "Epoch 2 step 534: training loss: 1069.1565002380053\n",
      "Epoch 2 step 535: training accuarcy: 0.798\n",
      "Epoch 2 step 535: training loss: 1105.3322161568171\n",
      "Epoch 2 step 536: training accuarcy: 0.7855\n",
      "Epoch 2 step 536: training loss: 1072.3222723932083\n",
      "Epoch 2 step 537: training accuarcy: 0.81\n",
      "Epoch 2 step 537: training loss: 1060.1440537683347\n",
      "Epoch 2 step 538: training accuarcy: 0.8075\n",
      "Epoch 2 step 538: training loss: 1087.9155904769073\n",
      "Epoch 2 step 539: training accuarcy: 0.802\n",
      "Epoch 2 step 539: training loss: 1069.6869851507724\n",
      "Epoch 2 step 540: training accuarcy: 0.806\n",
      "Epoch 2 step 540: training loss: 1058.006563927169\n",
      "Epoch 2 step 541: training accuarcy: 0.8155\n",
      "Epoch 2 step 541: training loss: 1052.0169363197308\n",
      "Epoch 2 step 542: training accuarcy: 0.8135\n",
      "Epoch 2 step 542: training loss: 1073.0933858631956\n",
      "Epoch 2 step 543: training accuarcy: 0.802\n",
      "Epoch 2 step 543: training loss: 1105.9438803991434\n",
      "Epoch 2 step 544: training accuarcy: 0.807\n",
      "Epoch 2 step 544: training loss: 1090.0582159752528\n",
      "Epoch 2 step 545: training accuarcy: 0.7935\n",
      "Epoch 2 step 545: training loss: 1077.4601945750117\n",
      "Epoch 2 step 546: training accuarcy: 0.805\n",
      "Epoch 2 step 546: training loss: 1064.7695443385173\n",
      "Epoch 2 step 547: training accuarcy: 0.811\n",
      "Epoch 2 step 547: training loss: 1058.1940554069777\n",
      "Epoch 2 step 548: training accuarcy: 0.805\n",
      "Epoch 2 step 548: training loss: 1122.5982410941306\n",
      "Epoch 2 step 549: training accuarcy: 0.7955\n",
      "Epoch 2 step 549: training loss: 1069.0387226224946\n",
      "Epoch 2 step 550: training accuarcy: 0.8045\n",
      "Epoch 2 step 550: training loss: 1088.5352277317559\n",
      "Epoch 2 step 551: training accuarcy: 0.7965\n",
      "Epoch 2 step 551: training loss: 1085.6569618103092\n",
      "Epoch 2 step 552: training accuarcy: 0.8130000000000001\n",
      "Epoch 2 step 552: training loss: 1085.0146924389514\n",
      "Epoch 2 step 553: training accuarcy: 0.8025\n",
      "Epoch 2 step 553: training loss: 1088.9075452383038\n",
      "Epoch 2 step 554: training accuarcy: 0.799\n",
      "Epoch 2 step 554: training loss: 1061.010386441179\n",
      "Epoch 2 step 555: training accuarcy: 0.796\n",
      "Epoch 2 step 555: training loss: 1080.5566316552806\n",
      "Epoch 2 step 556: training accuarcy: 0.8005\n",
      "Epoch 2 step 556: training loss: 1087.3492564149533\n",
      "Epoch 2 step 557: training accuarcy: 0.794\n",
      "Epoch 2 step 557: training loss: 1131.9808805107577\n",
      "Epoch 2 step 558: training accuarcy: 0.8\n",
      "Epoch 2 step 558: training loss: 1097.1190120553615\n",
      "Epoch 2 step 559: training accuarcy: 0.7965\n",
      "Epoch 2 step 559: training loss: 1069.3932732181377\n",
      "Epoch 2 step 560: training accuarcy: 0.7975\n",
      "Epoch 2 step 560: training loss: 1088.5703114214748\n",
      "Epoch 2 step 561: training accuarcy: 0.7975\n",
      "Epoch 2 step 561: training loss: 1061.5019613005081\n",
      "Epoch 2 step 562: training accuarcy: 0.8135\n",
      "Epoch 2 step 562: training loss: 1082.799566840734\n",
      "Epoch 2 step 563: training accuarcy: 0.8025\n",
      "Epoch 2 step 563: training loss: 1056.4440270345965\n",
      "Epoch 2 step 564: training accuarcy: 0.801\n",
      "Epoch 2 step 564: training loss: 1078.108849470749\n",
      "Epoch 2 step 565: training accuarcy: 0.7965\n",
      "Epoch 2 step 565: training loss: 1050.2244992626852\n",
      "Epoch 2 step 566: training accuarcy: 0.8055\n",
      "Epoch 2 step 566: training loss: 1075.831475129151\n",
      "Epoch 2 step 567: training accuarcy: 0.801\n",
      "Epoch 2 step 567: training loss: 1074.6075550721991\n",
      "Epoch 2 step 568: training accuarcy: 0.8055\n",
      "Epoch 2 step 568: training loss: 1062.1419982637801\n",
      "Epoch 2 step 569: training accuarcy: 0.8105\n",
      "Epoch 2 step 569: training loss: 1094.5917160677725\n",
      "Epoch 2 step 570: training accuarcy: 0.8\n",
      "Epoch 2 step 570: training loss: 1062.1908439854064\n",
      "Epoch 2 step 571: training accuarcy: 0.8085\n",
      "Epoch 2 step 571: training loss: 1027.0219648695816\n",
      "Epoch 2 step 572: training accuarcy: 0.8305\n",
      "Epoch 2 step 572: training loss: 1063.5112710831245\n",
      "Epoch 2 step 573: training accuarcy: 0.807\n",
      "Epoch 2 step 573: training loss: 1071.5732430470193\n",
      "Epoch 2 step 574: training accuarcy: 0.806\n",
      "Epoch 2 step 574: training loss: 1083.8724834150216\n",
      "Epoch 2 step 575: training accuarcy: 0.807\n",
      "Epoch 2 step 575: training loss: 1070.6301753553453\n",
      "Epoch 2 step 576: training accuarcy: 0.8\n",
      "Epoch 2 step 576: training loss: 1055.8394731430628\n",
      "Epoch 2 step 577: training accuarcy: 0.8065\n",
      "Epoch 2 step 577: training loss: 1071.3571878351456\n",
      "Epoch 2 step 578: training accuarcy: 0.792\n",
      "Epoch 2 step 578: training loss: 1050.292310845142\n",
      "Epoch 2 step 579: training accuarcy: 0.8130000000000001\n",
      "Epoch 2 step 579: training loss: 1088.7807363508077\n",
      "Epoch 2 step 580: training accuarcy: 0.7895\n",
      "Epoch 2 step 580: training loss: 1099.4539492694246\n",
      "Epoch 2 step 581: training accuarcy: 0.803\n",
      "Epoch 2 step 581: training loss: 1084.4263875596064\n",
      "Epoch 2 step 582: training accuarcy: 0.8135\n",
      "Epoch 2 step 582: training loss: 1072.790093767172\n",
      "Epoch 2 step 583: training accuarcy: 0.7975\n",
      "Epoch 2 step 583: training loss: 1074.794835623088\n",
      "Epoch 2 step 584: training accuarcy: 0.7885\n",
      "Epoch 2 step 584: training loss: 1086.46930083828\n",
      "Epoch 2 step 585: training accuarcy: 0.8\n",
      "Epoch 2 step 585: training loss: 1050.2473405235435\n",
      "Epoch 2 step 586: training accuarcy: 0.8075\n",
      "Epoch 2 step 586: training loss: 1088.216990879052\n",
      "Epoch 2 step 587: training accuarcy: 0.8005\n",
      "Epoch 2 step 587: training loss: 1084.1640985264803\n",
      "Epoch 2 step 588: training accuarcy: 0.7925\n",
      "Epoch 2 step 588: training loss: 1070.4215644173587\n",
      "Epoch 2 step 589: training accuarcy: 0.808\n",
      "Epoch 2 step 589: training loss: 1077.2304545959616\n",
      "Epoch 2 step 590: training accuarcy: 0.7955\n",
      "Epoch 2 step 590: training loss: 1063.8478675982763\n",
      "Epoch 2 step 591: training accuarcy: 0.7995\n",
      "Epoch 2 step 591: training loss: 1055.0668711597286\n",
      "Epoch 2 step 592: training accuarcy: 0.8025\n",
      "Epoch 2 step 592: training loss: 1064.9608188486993\n",
      "Epoch 2 step 593: training accuarcy: 0.8005\n",
      "Epoch 2 step 593: training loss: 1061.7084491793016\n",
      "Epoch 2 step 594: training accuarcy: 0.8015\n",
      "Epoch 2 step 594: training loss: 1071.9616338142737\n",
      "Epoch 2 step 595: training accuarcy: 0.801\n",
      "Epoch 2 step 595: training loss: 1080.9407309255853\n",
      "Epoch 2 step 596: training accuarcy: 0.804\n",
      "Epoch 2 step 596: training loss: 1094.5732681538614\n",
      "Epoch 2 step 597: training accuarcy: 0.808\n",
      "Epoch 2 step 597: training loss: 1071.489668636489\n",
      "Epoch 2 step 598: training accuarcy: 0.807\n",
      "Epoch 2 step 598: training loss: 1071.7923265364363\n",
      "Epoch 2 step 599: training accuarcy: 0.7965\n",
      "Epoch 2 step 599: training loss: 1065.6320890957545\n",
      "Epoch 2 step 600: training accuarcy: 0.806\n",
      "Epoch 2 step 600: training loss: 1061.7187107870013\n",
      "Epoch 2 step 601: training accuarcy: 0.7995\n",
      "Epoch 2 step 601: training loss: 1095.5175880324014\n",
      "Epoch 2 step 602: training accuarcy: 0.791\n",
      "Epoch 2 step 602: training loss: 1076.9551525178572\n",
      "Epoch 2 step 603: training accuarcy: 0.8035\n",
      "Epoch 2 step 603: training loss: 1088.0666147202503\n",
      "Epoch 2 step 604: training accuarcy: 0.7975\n",
      "Epoch 2 step 604: training loss: 1043.5283768917316\n",
      "Epoch 2 step 605: training accuarcy: 0.8055\n",
      "Epoch 2 step 605: training loss: 1081.0314980623389\n",
      "Epoch 2 step 606: training accuarcy: 0.8095\n",
      "Epoch 2 step 606: training loss: 1068.2368206729145\n",
      "Epoch 2 step 607: training accuarcy: 0.8055\n",
      "Epoch 2 step 607: training loss: 1061.787679404591\n",
      "Epoch 2 step 608: training accuarcy: 0.806\n",
      "Epoch 2 step 608: training loss: 1034.4068013256083\n",
      "Epoch 2 step 609: training accuarcy: 0.802\n",
      "Epoch 2 step 609: training loss: 1085.0752582268487\n",
      "Epoch 2 step 610: training accuarcy: 0.792\n",
      "Epoch 2 step 610: training loss: 1066.623173854479\n",
      "Epoch 2 step 611: training accuarcy: 0.807\n",
      "Epoch 2 step 611: training loss: 1076.5790746552577\n",
      "Epoch 2 step 612: training accuarcy: 0.8005\n",
      "Epoch 2 step 612: training loss: 1050.9890212025489\n",
      "Epoch 2 step 613: training accuarcy: 0.8105\n",
      "Epoch 2 step 613: training loss: 1052.4640014408644\n",
      "Epoch 2 step 614: training accuarcy: 0.8\n",
      "Epoch 2 step 614: training loss: 1050.6391504716453\n",
      "Epoch 2 step 615: training accuarcy: 0.8155\n",
      "Epoch 2 step 615: training loss: 1070.129212244449\n",
      "Epoch 2 step 616: training accuarcy: 0.8175\n",
      "Epoch 2 step 616: training loss: 1100.141165376795\n",
      "Epoch 2 step 617: training accuarcy: 0.8065\n",
      "Epoch 2 step 617: training loss: 1066.7748095423808\n",
      "Epoch 2 step 618: training accuarcy: 0.7985\n",
      "Epoch 2 step 618: training loss: 1057.390622524325\n",
      "Epoch 2 step 619: training accuarcy: 0.8125\n",
      "Epoch 2 step 619: training loss: 1091.8761353088112\n",
      "Epoch 2 step 620: training accuarcy: 0.803\n",
      "Epoch 2 step 620: training loss: 1084.9759892074442\n",
      "Epoch 2 step 621: training accuarcy: 0.789\n",
      "Epoch 2 step 621: training loss: 1024.383940172808\n",
      "Epoch 2 step 622: training accuarcy: 0.8270000000000001\n",
      "Epoch 2 step 622: training loss: 1069.2555829527944\n",
      "Epoch 2 step 623: training accuarcy: 0.802\n",
      "Epoch 2 step 623: training loss: 1055.839566988166\n",
      "Epoch 2 step 624: training accuarcy: 0.799\n",
      "Epoch 2 step 624: training loss: 1082.1089897788688\n",
      "Epoch 2 step 625: training accuarcy: 0.7985\n",
      "Epoch 2 step 625: training loss: 1085.9227614877063\n",
      "Epoch 2 step 626: training accuarcy: 0.792\n",
      "Epoch 2 step 626: training loss: 1102.2732087456986\n",
      "Epoch 2 step 627: training accuarcy: 0.7965\n",
      "Epoch 2 step 627: training loss: 1065.577214989516\n",
      "Epoch 2 step 628: training accuarcy: 0.808\n",
      "Epoch 2 step 628: training loss: 1058.3555935562865\n",
      "Epoch 2 step 629: training accuarcy: 0.8005\n",
      "Epoch 2 step 629: training loss: 1061.6169432357085\n",
      "Epoch 2 step 630: training accuarcy: 0.8035\n",
      "Epoch 2 step 630: training loss: 1045.1254539088118\n",
      "Epoch 2 step 631: training accuarcy: 0.8130000000000001\n",
      "Epoch 2 step 631: training loss: 1042.5087485727677\n",
      "Epoch 2 step 632: training accuarcy: 0.8165\n",
      "Epoch 2 step 632: training loss: 1060.0996726130466\n",
      "Epoch 2 step 633: training accuarcy: 0.8150000000000001\n",
      "Epoch 2 step 633: training loss: 1084.845275947207\n",
      "Epoch 2 step 634: training accuarcy: 0.805\n",
      "Epoch 2 step 634: training loss: 1060.4650265657792\n",
      "Epoch 2 step 635: training accuarcy: 0.8075\n",
      "Epoch 2 step 635: training loss: 1077.2015567041262\n",
      "Epoch 2 step 636: training accuarcy: 0.7995\n",
      "Epoch 2 step 636: training loss: 1072.2511759135016\n",
      "Epoch 2 step 637: training accuarcy: 0.791\n",
      "Epoch 2 step 637: training loss: 1055.8232154542598\n",
      "Epoch 2 step 638: training accuarcy: 0.8095\n",
      "Epoch 2 step 638: training loss: 1073.962553166529\n",
      "Epoch 2 step 639: training accuarcy: 0.791\n",
      "Epoch 2 step 639: training loss: 1080.3819444756753\n",
      "Epoch 2 step 640: training accuarcy: 0.801\n",
      "Epoch 2 step 640: training loss: 1045.4623943129943\n",
      "Epoch 2 step 641: training accuarcy: 0.8130000000000001\n",
      "Epoch 2 step 641: training loss: 1082.007105130205\n",
      "Epoch 2 step 642: training accuarcy: 0.8025\n",
      "Epoch 2 step 642: training loss: 1096.3013433102187\n",
      "Epoch 2 step 643: training accuarcy: 0.8\n",
      "Epoch 2 step 643: training loss: 1069.5065100798313\n",
      "Epoch 2 step 644: training accuarcy: 0.8130000000000001\n",
      "Epoch 2 step 644: training loss: 1048.5571657994808\n",
      "Epoch 2 step 645: training accuarcy: 0.7995\n",
      "Epoch 2 step 645: training loss: 1059.657324918462\n",
      "Epoch 2 step 646: training accuarcy: 0.799\n",
      "Epoch 2 step 646: training loss: 1055.696021398623\n",
      "Epoch 2 step 647: training accuarcy: 0.8055\n",
      "Epoch 2 step 647: training loss: 1061.4817093026866\n",
      "Epoch 2 step 648: training accuarcy: 0.8075\n",
      "Epoch 2 step 648: training loss: 1048.6939575565077\n",
      "Epoch 2 step 649: training accuarcy: 0.8140000000000001\n",
      "Epoch 2 step 649: training loss: 1061.305809301594\n",
      "Epoch 2 step 650: training accuarcy: 0.8175\n",
      "Epoch 2 step 650: training loss: 1084.8007954989164\n",
      "Epoch 2 step 651: training accuarcy: 0.8130000000000001\n",
      "Epoch 2 step 651: training loss: 1092.4172592790158\n",
      "Epoch 2 step 652: training accuarcy: 0.7995\n",
      "Epoch 2 step 652: training loss: 1077.5359175930405\n",
      "Epoch 2 step 653: training accuarcy: 0.8055\n",
      "Epoch 2 step 653: training loss: 1041.4447947146423\n",
      "Epoch 2 step 654: training accuarcy: 0.8065\n",
      "Epoch 2 step 654: training loss: 1077.101553327622\n",
      "Epoch 2 step 655: training accuarcy: 0.7985\n",
      "Epoch 2 step 655: training loss: 1065.8260409663856\n",
      "Epoch 2 step 656: training accuarcy: 0.8115\n",
      "Epoch 2 step 656: training loss: 1044.9969913469106\n",
      "Epoch 2 step 657: training accuarcy: 0.8175\n",
      "Epoch 2 step 657: training loss: 1092.4724373956792\n",
      "Epoch 2 step 658: training accuarcy: 0.8055\n",
      "Epoch 2 step 658: training loss: 1086.4377259081873\n",
      "Epoch 2 step 659: training accuarcy: 0.8085\n",
      "Epoch 2 step 659: training loss: 1032.1726754699807\n",
      "Epoch 2 step 660: training accuarcy: 0.8200000000000001\n",
      "Epoch 2 step 660: training loss: 1054.2383191773772\n",
      "Epoch 2 step 661: training accuarcy: 0.8025\n",
      "Epoch 2 step 661: training loss: 1075.454088285656\n",
      "Epoch 2 step 662: training accuarcy: 0.8035\n",
      "Epoch 2 step 662: training loss: 1054.758228661457\n",
      "Epoch 2 step 663: training accuarcy: 0.8045\n",
      "Epoch 2 step 663: training loss: 1084.6676042824872\n",
      "Epoch 2 step 664: training accuarcy: 0.797\n",
      "Epoch 2 step 664: training loss: 1095.159613137774\n",
      "Epoch 2 step 665: training accuarcy: 0.788\n",
      "Epoch 2 step 665: training loss: 1066.6114105692936\n",
      "Epoch 2 step 666: training accuarcy: 0.807\n",
      "Epoch 2 step 666: training loss: 1080.6528752320023\n",
      "Epoch 2 step 667: training accuarcy: 0.798\n",
      "Epoch 2 step 667: training loss: 1080.8946041617721\n",
      "Epoch 2 step 668: training accuarcy: 0.8105\n",
      "Epoch 2 step 668: training loss: 1083.106941375965\n",
      "Epoch 2 step 669: training accuarcy: 0.786\n",
      "Epoch 2 step 669: training loss: 1050.2953146881746\n",
      "Epoch 2 step 670: training accuarcy: 0.8145\n",
      "Epoch 2 step 670: training loss: 1053.061506638424\n",
      "Epoch 2 step 671: training accuarcy: 0.8065\n",
      "Epoch 2 step 671: training loss: 1077.50918426625\n",
      "Epoch 2 step 672: training accuarcy: 0.797\n",
      "Epoch 2 step 672: training loss: 1117.7047406509532\n",
      "Epoch 2 step 673: training accuarcy: 0.786\n",
      "Epoch 2 step 673: training loss: 1102.9930104800924\n",
      "Epoch 2 step 674: training accuarcy: 0.7895\n",
      "Epoch 2 step 674: training loss: 1080.1177812142896\n",
      "Epoch 2 step 675: training accuarcy: 0.788\n",
      "Epoch 2 step 675: training loss: 1058.6375209761156\n",
      "Epoch 2 step 676: training accuarcy: 0.8075\n",
      "Epoch 2 step 676: training loss: 1070.54705666355\n",
      "Epoch 2 step 677: training accuarcy: 0.804\n",
      "Epoch 2 step 677: training loss: 1086.0308658341926\n",
      "Epoch 2 step 678: training accuarcy: 0.8085\n",
      "Epoch 2 step 678: training loss: 1038.4720583090045\n",
      "Epoch 2 step 679: training accuarcy: 0.8055\n",
      "Epoch 2 step 679: training loss: 1091.7746159079659\n",
      "Epoch 2 step 680: training accuarcy: 0.804\n",
      "Epoch 2 step 680: training loss: 1040.8534294634671\n",
      "Epoch 2 step 681: training accuarcy: 0.8245\n",
      "Epoch 2 step 681: training loss: 1072.7699376232458\n",
      "Epoch 2 step 682: training accuarcy: 0.8180000000000001\n",
      "Epoch 2 step 682: training loss: 1044.727317594518\n",
      "Epoch 2 step 683: training accuarcy: 0.81\n",
      "Epoch 2 step 683: training loss: 1043.2505041584984\n",
      "Epoch 2 step 684: training accuarcy: 0.8160000000000001\n",
      "Epoch 2 step 684: training loss: 1056.1656051841128\n",
      "Epoch 2 step 685: training accuarcy: 0.799\n",
      "Epoch 2 step 685: training loss: 1041.4468372321012\n",
      "Epoch 2 step 686: training accuarcy: 0.809\n",
      "Epoch 2 step 686: training loss: 1046.1637320209102\n",
      "Epoch 2 step 687: training accuarcy: 0.8135\n",
      "Epoch 2 step 687: training loss: 1079.0938504659273\n",
      "Epoch 2 step 688: training accuarcy: 0.7995\n",
      "Epoch 2 step 688: training loss: 1065.8010547157157\n",
      "Epoch 2 step 689: training accuarcy: 0.8045\n",
      "Epoch 2 step 689: training loss: 1040.0672258350676\n",
      "Epoch 2 step 690: training accuarcy: 0.8025\n",
      "Epoch 2 step 690: training loss: 1089.1177511409117\n",
      "Epoch 2 step 691: training accuarcy: 0.7895\n",
      "Epoch 2 step 691: training loss: 1082.2207932015106\n",
      "Epoch 2 step 692: training accuarcy: 0.799\n",
      "Epoch 2 step 692: training loss: 1051.7553331712447\n",
      "Epoch 2 step 693: training accuarcy: 0.809\n",
      "Epoch 2 step 693: training loss: 1083.2957562190752\n",
      "Epoch 2 step 694: training accuarcy: 0.808\n",
      "Epoch 2 step 694: training loss: 1063.0707261988546\n",
      "Epoch 2 step 695: training accuarcy: 0.8015\n",
      "Epoch 2 step 695: training loss: 1066.0429420895096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 696: training accuarcy: 0.805\n",
      "Epoch 2 step 696: training loss: 1088.4502864821359\n",
      "Epoch 2 step 697: training accuarcy: 0.81\n",
      "Epoch 2 step 697: training loss: 1047.7908373618059\n",
      "Epoch 2 step 698: training accuarcy: 0.808\n",
      "Epoch 2 step 698: training loss: 1097.6841731657648\n",
      "Epoch 2 step 699: training accuarcy: 0.804\n",
      "Epoch 2 step 699: training loss: 1032.5952040628438\n",
      "Epoch 2 step 700: training accuarcy: 0.8145\n",
      "Epoch 2 step 700: training loss: 1020.6399961645035\n",
      "Epoch 2 step 701: training accuarcy: 0.8210000000000001\n",
      "Epoch 2 step 701: training loss: 1094.6456881295787\n",
      "Epoch 2 step 702: training accuarcy: 0.791\n",
      "Epoch 2 step 702: training loss: 1050.0602117666988\n",
      "Epoch 2 step 703: training accuarcy: 0.8115\n",
      "Epoch 2 step 703: training loss: 1031.924193885374\n",
      "Epoch 2 step 704: training accuarcy: 0.811\n",
      "Epoch 2 step 704: training loss: 1041.7253006272836\n",
      "Epoch 2 step 705: training accuarcy: 0.8165\n",
      "Epoch 2 step 705: training loss: 1049.305623754387\n",
      "Epoch 2 step 706: training accuarcy: 0.81\n",
      "Epoch 2 step 706: training loss: 1071.7777679737824\n",
      "Epoch 2 step 707: training accuarcy: 0.8085\n",
      "Epoch 2 step 707: training loss: 1087.1735183114686\n",
      "Epoch 2 step 708: training accuarcy: 0.7895\n",
      "Epoch 2 step 708: training loss: 1022.7910436974009\n",
      "Epoch 2 step 709: training accuarcy: 0.812\n",
      "Epoch 2 step 709: training loss: 1050.9113774405348\n",
      "Epoch 2 step 710: training accuarcy: 0.8045\n",
      "Epoch 2 step 710: training loss: 1057.207379043116\n",
      "Epoch 2 step 711: training accuarcy: 0.7945\n",
      "Epoch 2 step 711: training loss: 1050.6391473533008\n",
      "Epoch 2 step 712: training accuarcy: 0.8095\n",
      "Epoch 2 step 712: training loss: 1060.0360507667485\n",
      "Epoch 2 step 713: training accuarcy: 0.803\n",
      "Epoch 2 step 713: training loss: 1071.0440220685034\n",
      "Epoch 2 step 714: training accuarcy: 0.8045\n",
      "Epoch 2 step 714: training loss: 1042.478795717411\n",
      "Epoch 2 step 715: training accuarcy: 0.8130000000000001\n",
      "Epoch 2 step 715: training loss: 1054.5218118529717\n",
      "Epoch 2 step 716: training accuarcy: 0.8015\n",
      "Epoch 2 step 716: training loss: 1083.208049895855\n",
      "Epoch 2 step 717: training accuarcy: 0.8085\n",
      "Epoch 2 step 717: training loss: 1073.0376263374592\n",
      "Epoch 2 step 718: training accuarcy: 0.804\n",
      "Epoch 2 step 718: training loss: 1062.6041112695739\n",
      "Epoch 2 step 719: training accuarcy: 0.8025\n",
      "Epoch 2 step 719: training loss: 1051.2692312547558\n",
      "Epoch 2 step 720: training accuarcy: 0.8005\n",
      "Epoch 2 step 720: training loss: 1062.4468153171456\n",
      "Epoch 2 step 721: training accuarcy: 0.8035\n",
      "Epoch 2 step 721: training loss: 1044.2386065455871\n",
      "Epoch 2 step 722: training accuarcy: 0.8145\n",
      "Epoch 2 step 722: training loss: 1058.735205837911\n",
      "Epoch 2 step 723: training accuarcy: 0.8195\n",
      "Epoch 2 step 723: training loss: 1035.3383864925122\n",
      "Epoch 2 step 724: training accuarcy: 0.8045\n",
      "Epoch 2 step 724: training loss: 1031.4542656037784\n",
      "Epoch 2 step 725: training accuarcy: 0.8115\n",
      "Epoch 2 step 725: training loss: 1025.1356148618036\n",
      "Epoch 2 step 726: training accuarcy: 0.8245\n",
      "Epoch 2 step 726: training loss: 1108.687766646913\n",
      "Epoch 2 step 727: training accuarcy: 0.7955\n",
      "Epoch 2 step 727: training loss: 1053.091334886288\n",
      "Epoch 2 step 728: training accuarcy: 0.8095\n",
      "Epoch 2 step 728: training loss: 1095.4014831780758\n",
      "Epoch 2 step 729: training accuarcy: 0.8045\n",
      "Epoch 2 step 729: training loss: 1057.268783980358\n",
      "Epoch 2 step 730: training accuarcy: 0.7965\n",
      "Epoch 2 step 730: training loss: 1048.2012171131375\n",
      "Epoch 2 step 731: training accuarcy: 0.8125\n",
      "Epoch 2 step 731: training loss: 1083.744860899686\n",
      "Epoch 2 step 732: training accuarcy: 0.8\n",
      "Epoch 2 step 732: training loss: 1071.960195305639\n",
      "Epoch 2 step 733: training accuarcy: 0.7935\n",
      "Epoch 2 step 733: training loss: 1067.0964095862616\n",
      "Epoch 2 step 734: training accuarcy: 0.8035\n",
      "Epoch 2 step 734: training loss: 1064.679777133085\n",
      "Epoch 2 step 735: training accuarcy: 0.7875\n",
      "Epoch 2 step 735: training loss: 1048.2900397027365\n",
      "Epoch 2 step 736: training accuarcy: 0.8140000000000001\n",
      "Epoch 2 step 736: training loss: 1079.5095664613516\n",
      "Epoch 2 step 737: training accuarcy: 0.8025\n",
      "Epoch 2 step 737: training loss: 1055.5221791311149\n",
      "Epoch 2 step 738: training accuarcy: 0.7945\n",
      "Epoch 2 step 738: training loss: 1040.492361781042\n",
      "Epoch 2 step 739: training accuarcy: 0.8140000000000001\n",
      "Epoch 2 step 739: training loss: 1060.1803418960747\n",
      "Epoch 2 step 740: training accuarcy: 0.808\n",
      "Epoch 2 step 740: training loss: 1093.2176554483458\n",
      "Epoch 2 step 741: training accuarcy: 0.8150000000000001\n",
      "Epoch 2 step 741: training loss: 1060.1278401325226\n",
      "Epoch 2 step 742: training accuarcy: 0.8075\n",
      "Epoch 2 step 742: training loss: 1060.974557939637\n",
      "Epoch 2 step 743: training accuarcy: 0.805\n",
      "Epoch 2 step 743: training loss: 1073.2715466966147\n",
      "Epoch 2 step 744: training accuarcy: 0.799\n",
      "Epoch 2 step 744: training loss: 1028.6570002184067\n",
      "Epoch 2 step 745: training accuarcy: 0.8105\n",
      "Epoch 2 step 745: training loss: 1076.89135764196\n",
      "Epoch 2 step 746: training accuarcy: 0.8145\n",
      "Epoch 2 step 746: training loss: 1071.3821028207785\n",
      "Epoch 2 step 747: training accuarcy: 0.8145\n",
      "Epoch 2 step 747: training loss: 1024.0838144787099\n",
      "Epoch 2 step 748: training accuarcy: 0.8170000000000001\n",
      "Epoch 2 step 748: training loss: 1053.6985281196955\n",
      "Epoch 2 step 749: training accuarcy: 0.8015\n",
      "Epoch 2 step 749: training loss: 1081.6221468880226\n",
      "Epoch 2 step 750: training accuarcy: 0.805\n",
      "Epoch 2 step 750: training loss: 1088.0350915752401\n",
      "Epoch 2 step 751: training accuarcy: 0.799\n",
      "Epoch 2 step 751: training loss: 1039.7740736946996\n",
      "Epoch 2 step 752: training accuarcy: 0.8065\n",
      "Epoch 2 step 752: training loss: 1045.1281210343284\n",
      "Epoch 2 step 753: training accuarcy: 0.8105\n",
      "Epoch 2 step 753: training loss: 1068.8866603159481\n",
      "Epoch 2 step 754: training accuarcy: 0.791\n",
      "Epoch 2 step 754: training loss: 1112.2324353696922\n",
      "Epoch 2 step 755: training accuarcy: 0.796\n",
      "Epoch 2 step 755: training loss: 1037.0871226056108\n",
      "Epoch 2 step 756: training accuarcy: 0.8210000000000001\n",
      "Epoch 2 step 756: training loss: 1051.0035309457323\n",
      "Epoch 2 step 757: training accuarcy: 0.802\n",
      "Epoch 2 step 757: training loss: 1079.36523171617\n",
      "Epoch 2 step 758: training accuarcy: 0.7925\n",
      "Epoch 2 step 758: training loss: 1042.7468087216432\n",
      "Epoch 2 step 759: training accuarcy: 0.8145\n",
      "Epoch 2 step 759: training loss: 1044.8008294958681\n",
      "Epoch 2 step 760: training accuarcy: 0.7975\n",
      "Epoch 2 step 760: training loss: 1021.2393940827089\n",
      "Epoch 2 step 761: training accuarcy: 0.8165\n",
      "Epoch 2 step 761: training loss: 1076.9860941163663\n",
      "Epoch 2 step 762: training accuarcy: 0.8035\n",
      "Epoch 2 step 762: training loss: 1028.6808960784726\n",
      "Epoch 2 step 763: training accuarcy: 0.809\n",
      "Epoch 2 step 763: training loss: 1042.4781013538918\n",
      "Epoch 2 step 764: training accuarcy: 0.806\n",
      "Epoch 2 step 764: training loss: 1063.87247921932\n",
      "Epoch 2 step 765: training accuarcy: 0.8025\n",
      "Epoch 2 step 765: training loss: 1053.0504162759016\n",
      "Epoch 2 step 766: training accuarcy: 0.8130000000000001\n",
      "Epoch 2 step 766: training loss: 1053.7811157705778\n",
      "Epoch 2 step 767: training accuarcy: 0.8175\n",
      "Epoch 2 step 767: training loss: 1103.7751967345307\n",
      "Epoch 2 step 768: training accuarcy: 0.7915\n",
      "Epoch 2 step 768: training loss: 1094.6713345306928\n",
      "Epoch 2 step 769: training accuarcy: 0.7955\n",
      "Epoch 2 step 769: training loss: 1075.8005553101643\n",
      "Epoch 2 step 770: training accuarcy: 0.7865\n",
      "Epoch 2 step 770: training loss: 1068.2452480914935\n",
      "Epoch 2 step 771: training accuarcy: 0.8045\n",
      "Epoch 2 step 771: training loss: 1098.664334416382\n",
      "Epoch 2 step 772: training accuarcy: 0.7955\n",
      "Epoch 2 step 772: training loss: 1075.0374348001224\n",
      "Epoch 2 step 773: training accuarcy: 0.8055\n",
      "Epoch 2 step 773: training loss: 1054.4209814542237\n",
      "Epoch 2 step 774: training accuarcy: 0.806\n",
      "Epoch 2 step 774: training loss: 1058.1981963242742\n",
      "Epoch 2 step 775: training accuarcy: 0.808\n",
      "Epoch 2 step 775: training loss: 1045.468384319447\n",
      "Epoch 2 step 776: training accuarcy: 0.805\n",
      "Epoch 2 step 776: training loss: 1100.3896252355016\n",
      "Epoch 2 step 777: training accuarcy: 0.792\n",
      "Epoch 2 step 777: training loss: 1031.3662133565242\n",
      "Epoch 2 step 778: training accuarcy: 0.8230000000000001\n",
      "Epoch 2 step 778: training loss: 1087.6860546564762\n",
      "Epoch 2 step 779: training accuarcy: 0.787\n",
      "Epoch 2 step 779: training loss: 1061.0466756651886\n",
      "Epoch 2 step 780: training accuarcy: 0.8240000000000001\n",
      "Epoch 2 step 780: training loss: 1032.753761630078\n",
      "Epoch 2 step 781: training accuarcy: 0.8165\n",
      "Epoch 2 step 781: training loss: 1049.294868620848\n",
      "Epoch 2 step 782: training accuarcy: 0.8155\n",
      "Epoch 2 step 782: training loss: 1059.9187464584695\n",
      "Epoch 2 step 783: training accuarcy: 0.7995\n",
      "Epoch 2 step 783: training loss: 1068.8923176707817\n",
      "Epoch 2 step 784: training accuarcy: 0.797\n",
      "Epoch 2 step 784: training loss: 1058.710209143463\n",
      "Epoch 2 step 785: training accuarcy: 0.8150000000000001\n",
      "Epoch 2 step 785: training loss: 1057.7193000299949\n",
      "Epoch 2 step 786: training accuarcy: 0.8055\n",
      "Epoch 2 step 786: training loss: 1053.3203005666858\n",
      "Epoch 2 step 787: training accuarcy: 0.7965\n",
      "Epoch 2 step 787: training loss: 1047.0254231547246\n",
      "Epoch 2 step 788: training accuarcy: 0.812\n",
      "Epoch 2 step 788: training loss: 448.576230321547\n",
      "Epoch 2 step 789: training accuarcy: 0.8051282051282052\n",
      "Epoch 2: train loss 1065.4107620072998, train accuarcy 0.7491326332092285\n",
      "Epoch 2: valid loss 1047.7315815691857, valid accuarcy 0.8052355051040649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [05:33<00:00, 111.19s/it]\n"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=3,\n",
    "                 log_dir=get_log_dir('topcoder', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:14:55.886401Z",
     "start_time": "2019-10-09T13:00:22.609968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 294606.13853030634\n",
      "Epoch 0 step 1: training accuarcy: 0.5142\n",
      "Epoch 0 step 1: training loss: 286108.444642681\n",
      "Epoch 0 step 2: training accuarcy: 0.5282\n",
      "Epoch 0 step 2: training loss: 293987.4134806083\n",
      "Epoch 0 step 3: training accuarcy: 0.5083\n",
      "Epoch 0 step 3: training loss: 284043.4721217331\n",
      "Epoch 0 step 4: training accuarcy: 0.5192\n",
      "Epoch 0 step 4: training loss: 278149.06834666454\n",
      "Epoch 0 step 5: training accuarcy: 0.5146000000000001\n",
      "Epoch 0 step 5: training loss: 279162.83377816185\n",
      "Epoch 0 step 6: training accuarcy: 0.5137\n",
      "Epoch 0 step 6: training loss: 263925.41081593785\n",
      "Epoch 0 step 7: training accuarcy: 0.533\n",
      "Epoch 0 step 7: training loss: 272980.8215130734\n",
      "Epoch 0 step 8: training accuarcy: 0.5086\n",
      "Epoch 0 step 8: training loss: 261422.55033331588\n",
      "Epoch 0 step 9: training accuarcy: 0.5252\n",
      "Epoch 0 step 9: training loss: 246522.51367903984\n",
      "Epoch 0 step 10: training accuarcy: 0.5244\n",
      "Epoch 0 step 10: training loss: 247796.70586116522\n",
      "Epoch 0 step 11: training accuarcy: 0.5341\n",
      "Epoch 0 step 11: training loss: 238984.72670435722\n",
      "Epoch 0 step 12: training accuarcy: 0.5286000000000001\n",
      "Epoch 0 step 12: training loss: 232933.34247990797\n",
      "Epoch 0 step 13: training accuarcy: 0.5319\n",
      "Epoch 0 step 13: training loss: 230210.2971837398\n",
      "Epoch 0 step 14: training accuarcy: 0.5256000000000001\n",
      "Epoch 0 step 14: training loss: 233485.07635888772\n",
      "Epoch 0 step 15: training accuarcy: 0.5235000000000001\n",
      "Epoch 0 step 15: training loss: 223323.81134403386\n",
      "Epoch 0 step 16: training accuarcy: 0.5248\n",
      "Epoch 0 step 16: training loss: 213327.34659066066\n",
      "Epoch 0 step 17: training accuarcy: 0.533\n",
      "Epoch 0 step 17: training loss: 209879.5444714632\n",
      "Epoch 0 step 18: training accuarcy: 0.5348\n",
      "Epoch 0 step 18: training loss: 205113.50736993892\n",
      "Epoch 0 step 19: training accuarcy: 0.5286000000000001\n",
      "Epoch 0 step 19: training loss: 199281.58614203692\n",
      "Epoch 0 step 20: training accuarcy: 0.5296000000000001\n",
      "Epoch 0 step 20: training loss: 192111.9664741734\n",
      "Epoch 0 step 21: training accuarcy: 0.5415\n",
      "Epoch 0 step 21: training loss: 189311.72476804312\n",
      "Epoch 0 step 22: training accuarcy: 0.5344\n",
      "Epoch 0 step 22: training loss: 180091.76698244683\n",
      "Epoch 0 step 23: training accuarcy: 0.5414\n",
      "Epoch 0 step 23: training loss: 177140.9345186835\n",
      "Epoch 0 step 24: training accuarcy: 0.5324\n",
      "Epoch 0 step 24: training loss: 174560.93247408385\n",
      "Epoch 0 step 25: training accuarcy: 0.544\n",
      "Epoch 0 step 25: training loss: 174409.33184522943\n",
      "Epoch 0 step 26: training accuarcy: 0.5296000000000001\n",
      "Epoch 0 step 26: training loss: 163819.90785822915\n",
      "Epoch 0 step 27: training accuarcy: 0.5372\n",
      "Epoch 0 step 27: training loss: 160254.54592900007\n",
      "Epoch 0 step 28: training accuarcy: 0.5302\n",
      "Epoch 0 step 28: training loss: 149365.67896409388\n",
      "Epoch 0 step 29: training accuarcy: 0.5451\n",
      "Epoch 0 step 29: training loss: 157405.21053059236\n",
      "Epoch 0 step 30: training accuarcy: 0.5251\n",
      "Epoch 0 step 30: training loss: 149808.0587127385\n",
      "Epoch 0 step 31: training accuarcy: 0.5335\n",
      "Epoch 0 step 31: training loss: 140425.23113409049\n",
      "Epoch 0 step 32: training accuarcy: 0.5416000000000001\n",
      "Epoch 0 step 32: training loss: 142977.7634677522\n",
      "Epoch 0 step 33: training accuarcy: 0.5191\n",
      "Epoch 0 step 33: training loss: 133403.7320041012\n",
      "Epoch 0 step 34: training accuarcy: 0.5362\n",
      "Epoch 0 step 34: training loss: 133048.10475581329\n",
      "Epoch 0 step 35: training accuarcy: 0.5268\n",
      "Epoch 0 step 35: training loss: 130997.7794195195\n",
      "Epoch 0 step 36: training accuarcy: 0.5298\n",
      "Epoch 0 step 36: training loss: 117542.52060988337\n",
      "Epoch 0 step 37: training accuarcy: 0.5575\n",
      "Epoch 0 step 37: training loss: 120609.30332886823\n",
      "Epoch 0 step 38: training accuarcy: 0.5401\n",
      "Epoch 0 step 38: training loss: 115569.06764903638\n",
      "Epoch 0 step 39: training accuarcy: 0.5464\n",
      "Epoch 0 step 39: training loss: 112037.18054295078\n",
      "Epoch 0 step 40: training accuarcy: 0.549\n",
      "Epoch 0 step 40: training loss: 110365.01318704188\n",
      "Epoch 0 step 41: training accuarcy: 0.533\n",
      "Epoch 0 step 41: training loss: 106127.02435295642\n",
      "Epoch 0 step 42: training accuarcy: 0.544\n",
      "Epoch 0 step 42: training loss: 104320.11406568302\n",
      "Epoch 0 step 43: training accuarcy: 0.5456\n",
      "Epoch 0 step 43: training loss: 98910.70261878318\n",
      "Epoch 0 step 44: training accuarcy: 0.5483\n",
      "Epoch 0 step 44: training loss: 93891.3381596104\n",
      "Epoch 0 step 45: training accuarcy: 0.5588000000000001\n",
      "Epoch 0 step 45: training loss: 92323.9410254358\n",
      "Epoch 0 step 46: training accuarcy: 0.5606\n",
      "Epoch 0 step 46: training loss: 90345.71008458006\n",
      "Epoch 0 step 47: training accuarcy: 0.5539000000000001\n",
      "Epoch 0 step 47: training loss: 89075.05734434206\n",
      "Epoch 0 step 48: training accuarcy: 0.5515\n",
      "Epoch 0 step 48: training loss: 85570.67977686659\n",
      "Epoch 0 step 49: training accuarcy: 0.5483\n",
      "Epoch 0 step 49: training loss: 83580.82839934735\n",
      "Epoch 0 step 50: training accuarcy: 0.5473\n",
      "Epoch 0 step 50: training loss: 83958.36806107688\n",
      "Epoch 0 step 51: training accuarcy: 0.541\n",
      "Epoch 0 step 51: training loss: 78010.6080669183\n",
      "Epoch 0 step 52: training accuarcy: 0.5535\n",
      "Epoch 0 step 52: training loss: 78606.9700591866\n",
      "Epoch 0 step 53: training accuarcy: 0.5413\n",
      "Epoch 0 step 53: training loss: 75289.90786955843\n",
      "Epoch 0 step 54: training accuarcy: 0.5594\n",
      "Epoch 0 step 54: training loss: 74269.66343819878\n",
      "Epoch 0 step 55: training accuarcy: 0.5406000000000001\n",
      "Epoch 0 step 55: training loss: 71777.96568676515\n",
      "Epoch 0 step 56: training accuarcy: 0.5585\n",
      "Epoch 0 step 56: training loss: 68616.96793213318\n",
      "Epoch 0 step 57: training accuarcy: 0.5773\n",
      "Epoch 0 step 57: training loss: 68390.37479003408\n",
      "Epoch 0 step 58: training accuarcy: 0.5592\n",
      "Epoch 0 step 58: training loss: 68699.03107022232\n",
      "Epoch 0 step 59: training accuarcy: 0.5482\n",
      "Epoch 0 step 59: training loss: 66820.50152926438\n",
      "Epoch 0 step 60: training accuarcy: 0.5677\n",
      "Epoch 0 step 60: training loss: 65912.27819156047\n",
      "Epoch 0 step 61: training accuarcy: 0.5522\n",
      "Epoch 0 step 61: training loss: 65288.7963928904\n",
      "Epoch 0 step 62: training accuarcy: 0.5630000000000001\n",
      "Epoch 0 step 62: training loss: 64387.5546108618\n",
      "Epoch 0 step 63: training accuarcy: 0.5553\n",
      "Epoch 0 step 63: training loss: 61795.30018373211\n",
      "Epoch 0 step 64: training accuarcy: 0.5653\n",
      "Epoch 0 step 64: training loss: 60548.35658973036\n",
      "Epoch 0 step 65: training accuarcy: 0.5652\n",
      "Epoch 0 step 65: training loss: 59251.686173066795\n",
      "Epoch 0 step 66: training accuarcy: 0.5776\n",
      "Epoch 0 step 66: training loss: 58983.84799881805\n",
      "Epoch 0 step 67: training accuarcy: 0.5613\n",
      "Epoch 0 step 67: training loss: 58176.41571022961\n",
      "Epoch 0 step 68: training accuarcy: 0.5632\n",
      "Epoch 0 step 68: training loss: 59662.420715975764\n",
      "Epoch 0 step 69: training accuarcy: 0.5519000000000001\n",
      "Epoch 0 step 69: training loss: 55926.721778058185\n",
      "Epoch 0 step 70: training accuarcy: 0.5847\n",
      "Epoch 0 step 70: training loss: 55870.49273634621\n",
      "Epoch 0 step 71: training accuarcy: 0.5772\n",
      "Epoch 0 step 71: training loss: 54822.54724703757\n",
      "Epoch 0 step 72: training accuarcy: 0.5748\n",
      "Epoch 0 step 72: training loss: 55376.4544312036\n",
      "Epoch 0 step 73: training accuarcy: 0.5683\n",
      "Epoch 0 step 73: training loss: 55932.68138140744\n",
      "Epoch 0 step 74: training accuarcy: 0.5717\n",
      "Epoch 0 step 74: training loss: 53070.58369867261\n",
      "Epoch 0 step 75: training accuarcy: 0.5835\n",
      "Epoch 0 step 75: training loss: 52623.302606139085\n",
      "Epoch 0 step 76: training accuarcy: 0.5847\n",
      "Epoch 0 step 76: training loss: 53877.06177768153\n",
      "Epoch 0 step 77: training accuarcy: 0.5744\n",
      "Epoch 0 step 77: training loss: 52386.36637776054\n",
      "Epoch 0 step 78: training accuarcy: 0.5942000000000001\n",
      "Epoch 0 step 78: training loss: 53577.345621185996\n",
      "Epoch 0 step 79: training accuarcy: 0.5705\n",
      "Epoch 0 step 79: training loss: 52631.920330626584\n",
      "Epoch 0 step 80: training accuarcy: 0.5742\n",
      "Epoch 0 step 80: training loss: 51559.69921226196\n",
      "Epoch 0 step 81: training accuarcy: 0.5803\n",
      "Epoch 0 step 81: training loss: 51009.090783616586\n",
      "Epoch 0 step 82: training accuarcy: 0.5842\n",
      "Epoch 0 step 82: training loss: 50999.70593507299\n",
      "Epoch 0 step 83: training accuarcy: 0.5726\n",
      "Epoch 0 step 83: training loss: 50631.768650179554\n",
      "Epoch 0 step 84: training accuarcy: 0.5851000000000001\n",
      "Epoch 0 step 84: training loss: 50266.478389920805\n",
      "Epoch 0 step 85: training accuarcy: 0.5847\n",
      "Epoch 0 step 85: training loss: 50051.87387286336\n",
      "Epoch 0 step 86: training accuarcy: 0.5888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 86: training loss: 49351.753423952265\n",
      "Epoch 0 step 87: training accuarcy: 0.5918\n",
      "Epoch 0 step 87: training loss: 50024.61976459105\n",
      "Epoch 0 step 88: training accuarcy: 0.5839\n",
      "Epoch 0 step 88: training loss: 49202.58714893614\n",
      "Epoch 0 step 89: training accuarcy: 0.5897\n",
      "Epoch 0 step 89: training loss: 48909.41989555757\n",
      "Epoch 0 step 90: training accuarcy: 0.5923\n",
      "Epoch 0 step 90: training loss: 48638.808378756796\n",
      "Epoch 0 step 91: training accuarcy: 0.5889\n",
      "Epoch 0 step 91: training loss: 49006.547890239744\n",
      "Epoch 0 step 92: training accuarcy: 0.5831000000000001\n",
      "Epoch 0 step 92: training loss: 47676.21878923138\n",
      "Epoch 0 step 93: training accuarcy: 0.6065\n",
      "Epoch 0 step 93: training loss: 48449.83790047556\n",
      "Epoch 0 step 94: training accuarcy: 0.5983\n",
      "Epoch 0 step 94: training loss: 47852.71035297997\n",
      "Epoch 0 step 95: training accuarcy: 0.6043000000000001\n",
      "Epoch 0 step 95: training loss: 47793.16903701683\n",
      "Epoch 0 step 96: training accuarcy: 0.6057\n",
      "Epoch 0 step 96: training loss: 47773.42434647083\n",
      "Epoch 0 step 97: training accuarcy: 0.606\n",
      "Epoch 0 step 97: training loss: 47182.65632621608\n",
      "Epoch 0 step 98: training accuarcy: 0.6027\n",
      "Epoch 0 step 98: training loss: 46329.32514615025\n",
      "Epoch 0 step 99: training accuarcy: 0.6107\n",
      "Epoch 0 step 99: training loss: 46981.77931949707\n",
      "Epoch 0 step 100: training accuarcy: 0.6065\n",
      "Epoch 0 step 100: training loss: 46543.16237072859\n",
      "Epoch 0 step 101: training accuarcy: 0.6156\n",
      "Epoch 0 step 101: training loss: 46435.400443131526\n",
      "Epoch 0 step 102: training accuarcy: 0.6032000000000001\n",
      "Epoch 0 step 102: training loss: 47512.59828230449\n",
      "Epoch 0 step 103: training accuarcy: 0.5887\n",
      "Epoch 0 step 103: training loss: 46067.822292044475\n",
      "Epoch 0 step 104: training accuarcy: 0.6125\n",
      "Epoch 0 step 104: training loss: 46796.80096719917\n",
      "Epoch 0 step 105: training accuarcy: 0.5991000000000001\n",
      "Epoch 0 step 105: training loss: 46316.188668303344\n",
      "Epoch 0 step 106: training accuarcy: 0.6089\n",
      "Epoch 0 step 106: training loss: 45687.46383163568\n",
      "Epoch 0 step 107: training accuarcy: 0.6191\n",
      "Epoch 0 step 107: training loss: 46449.984935144515\n",
      "Epoch 0 step 108: training accuarcy: 0.607\n",
      "Epoch 0 step 108: training loss: 45663.26056754888\n",
      "Epoch 0 step 109: training accuarcy: 0.6047\n",
      "Epoch 0 step 109: training loss: 45111.89735275197\n",
      "Epoch 0 step 110: training accuarcy: 0.6171\n",
      "Epoch 0 step 110: training loss: 46276.78862554478\n",
      "Epoch 0 step 111: training accuarcy: 0.6038\n",
      "Epoch 0 step 111: training loss: 45835.18766594704\n",
      "Epoch 0 step 112: training accuarcy: 0.6144000000000001\n",
      "Epoch 0 step 112: training loss: 45142.622092836355\n",
      "Epoch 0 step 113: training accuarcy: 0.608\n",
      "Epoch 0 step 113: training loss: 43777.51586382462\n",
      "Epoch 0 step 114: training accuarcy: 0.6401\n",
      "Epoch 0 step 114: training loss: 44507.51385826258\n",
      "Epoch 0 step 115: training accuarcy: 0.6134000000000001\n",
      "Epoch 0 step 115: training loss: 44226.673914934225\n",
      "Epoch 0 step 116: training accuarcy: 0.6205\n",
      "Epoch 0 step 116: training loss: 44520.87527122697\n",
      "Epoch 0 step 117: training accuarcy: 0.615\n",
      "Epoch 0 step 117: training loss: 45095.91779371997\n",
      "Epoch 0 step 118: training accuarcy: 0.6107\n",
      "Epoch 0 step 118: training loss: 44245.30065843064\n",
      "Epoch 0 step 119: training accuarcy: 0.626\n",
      "Epoch 0 step 119: training loss: 43605.0472264889\n",
      "Epoch 0 step 120: training accuarcy: 0.6369\n",
      "Epoch 0 step 120: training loss: 43771.77642261997\n",
      "Epoch 0 step 121: training accuarcy: 0.6251\n",
      "Epoch 0 step 121: training loss: 44068.88023852835\n",
      "Epoch 0 step 122: training accuarcy: 0.6237\n",
      "Epoch 0 step 122: training loss: 43056.729197456945\n",
      "Epoch 0 step 123: training accuarcy: 0.6439\n",
      "Epoch 0 step 123: training loss: 44600.51004993345\n",
      "Epoch 0 step 124: training accuarcy: 0.6124\n",
      "Epoch 0 step 124: training loss: 42684.00262842473\n",
      "Epoch 0 step 125: training accuarcy: 0.6394000000000001\n",
      "Epoch 0 step 125: training loss: 43390.331974001805\n",
      "Epoch 0 step 126: training accuarcy: 0.6358\n",
      "Epoch 0 step 126: training loss: 43397.91575089304\n",
      "Epoch 0 step 127: training accuarcy: 0.6369\n",
      "Epoch 0 step 127: training loss: 43529.58814174399\n",
      "Epoch 0 step 128: training accuarcy: 0.6361\n",
      "Epoch 0 step 128: training loss: 43313.329297820244\n",
      "Epoch 0 step 129: training accuarcy: 0.6282\n",
      "Epoch 0 step 129: training loss: 42699.85129977658\n",
      "Epoch 0 step 130: training accuarcy: 0.6351\n",
      "Epoch 0 step 130: training loss: 42684.47279645517\n",
      "Epoch 0 step 131: training accuarcy: 0.6497\n",
      "Epoch 0 step 131: training loss: 42916.56752804259\n",
      "Epoch 0 step 132: training accuarcy: 0.6407\n",
      "Epoch 0 step 132: training loss: 42368.3230918084\n",
      "Epoch 0 step 133: training accuarcy: 0.6431\n",
      "Epoch 0 step 133: training loss: 42887.16401329611\n",
      "Epoch 0 step 134: training accuarcy: 0.641\n",
      "Epoch 0 step 134: training loss: 42231.54951044463\n",
      "Epoch 0 step 135: training accuarcy: 0.6446000000000001\n",
      "Epoch 0 step 135: training loss: 42595.259927037776\n",
      "Epoch 0 step 136: training accuarcy: 0.6472\n",
      "Epoch 0 step 136: training loss: 42013.6985443755\n",
      "Epoch 0 step 137: training accuarcy: 0.6356\n",
      "Epoch 0 step 137: training loss: 42868.15281768461\n",
      "Epoch 0 step 138: training accuarcy: 0.643\n",
      "Epoch 0 step 138: training loss: 42220.30084343733\n",
      "Epoch 0 step 139: training accuarcy: 0.6471\n",
      "Epoch 0 step 139: training loss: 41952.01573456642\n",
      "Epoch 0 step 140: training accuarcy: 0.6458\n",
      "Epoch 0 step 140: training loss: 42007.574040781066\n",
      "Epoch 0 step 141: training accuarcy: 0.6458\n",
      "Epoch 0 step 141: training loss: 41297.929040276875\n",
      "Epoch 0 step 142: training accuarcy: 0.6527000000000001\n",
      "Epoch 0 step 142: training loss: 42353.42488434606\n",
      "Epoch 0 step 143: training accuarcy: 0.6402\n",
      "Epoch 0 step 143: training loss: 42402.404460157835\n",
      "Epoch 0 step 144: training accuarcy: 0.6468\n",
      "Epoch 0 step 144: training loss: 41679.55103347928\n",
      "Epoch 0 step 145: training accuarcy: 0.6496000000000001\n",
      "Epoch 0 step 145: training loss: 41379.19932344828\n",
      "Epoch 0 step 146: training accuarcy: 0.6585000000000001\n",
      "Epoch 0 step 146: training loss: 41709.56146845432\n",
      "Epoch 0 step 147: training accuarcy: 0.6478\n",
      "Epoch 0 step 147: training loss: 41974.13301484132\n",
      "Epoch 0 step 148: training accuarcy: 0.6538\n",
      "Epoch 0 step 148: training loss: 41876.355383770075\n",
      "Epoch 0 step 149: training accuarcy: 0.645\n",
      "Epoch 0 step 149: training loss: 41492.87403060056\n",
      "Epoch 0 step 150: training accuarcy: 0.6583\n",
      "Epoch 0 step 150: training loss: 40792.73472122817\n",
      "Epoch 0 step 151: training accuarcy: 0.6574\n",
      "Epoch 0 step 151: training loss: 41834.10270010105\n",
      "Epoch 0 step 152: training accuarcy: 0.6499\n",
      "Epoch 0 step 152: training loss: 39638.43272081352\n",
      "Epoch 0 step 153: training accuarcy: 0.6698000000000001\n",
      "Epoch 0 step 153: training loss: 41242.16898559815\n",
      "Epoch 0 step 154: training accuarcy: 0.6498\n",
      "Epoch 0 step 154: training loss: 40919.7628744457\n",
      "Epoch 0 step 155: training accuarcy: 0.6574\n",
      "Epoch 0 step 155: training loss: 41202.12115341227\n",
      "Epoch 0 step 156: training accuarcy: 0.6545000000000001\n",
      "Epoch 0 step 156: training loss: 40532.22515137412\n",
      "Epoch 0 step 157: training accuarcy: 0.6715\n",
      "Epoch 0 step 157: training loss: 40654.27276752574\n",
      "Epoch 0 step 158: training accuarcy: 0.6631\n",
      "Epoch 0 step 158: training loss: 40428.70883380744\n",
      "Epoch 0 step 159: training accuarcy: 0.6648000000000001\n",
      "Epoch 0 step 159: training loss: 40430.16551252845\n",
      "Epoch 0 step 160: training accuarcy: 0.6625\n",
      "Epoch 0 step 160: training loss: 40652.237257111454\n",
      "Epoch 0 step 161: training accuarcy: 0.6658000000000001\n",
      "Epoch 0 step 161: training loss: 40442.03522371963\n",
      "Epoch 0 step 162: training accuarcy: 0.6639\n",
      "Epoch 0 step 162: training loss: 40645.42320885142\n",
      "Epoch 0 step 163: training accuarcy: 0.66\n",
      "Epoch 0 step 163: training loss: 39996.68266652413\n",
      "Epoch 0 step 164: training accuarcy: 0.6768000000000001\n",
      "Epoch 0 step 164: training loss: 39451.0297184849\n",
      "Epoch 0 step 165: training accuarcy: 0.6804\n",
      "Epoch 0 step 165: training loss: 39950.06631709235\n",
      "Epoch 0 step 166: training accuarcy: 0.6674\n",
      "Epoch 0 step 166: training loss: 39731.829896121955\n",
      "Epoch 0 step 167: training accuarcy: 0.6697000000000001\n",
      "Epoch 0 step 167: training loss: 39967.67965756997\n",
      "Epoch 0 step 168: training accuarcy: 0.6711\n",
      "Epoch 0 step 168: training loss: 40143.3291097706\n",
      "Epoch 0 step 169: training accuarcy: 0.6776\n",
      "Epoch 0 step 169: training loss: 38946.2327888802\n",
      "Epoch 0 step 170: training accuarcy: 0.6893\n",
      "Epoch 0 step 170: training loss: 39202.0034520271\n",
      "Epoch 0 step 171: training accuarcy: 0.6786\n",
      "Epoch 0 step 171: training loss: 39170.801937912816\n",
      "Epoch 0 step 172: training accuarcy: 0.6756\n",
      "Epoch 0 step 172: training loss: 39557.2955702388\n",
      "Epoch 0 step 173: training accuarcy: 0.669\n",
      "Epoch 0 step 173: training loss: 39067.8986854975\n",
      "Epoch 0 step 174: training accuarcy: 0.6762\n",
      "Epoch 0 step 174: training loss: 38976.90380625868\n",
      "Epoch 0 step 175: training accuarcy: 0.6856\n",
      "Epoch 0 step 175: training loss: 39097.7987199493\n",
      "Epoch 0 step 176: training accuarcy: 0.6751\n",
      "Epoch 0 step 176: training loss: 39096.42293060083\n",
      "Epoch 0 step 177: training accuarcy: 0.6756\n",
      "Epoch 0 step 177: training loss: 38485.49366859559\n",
      "Epoch 0 step 178: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 178: training loss: 39472.11341639771\n",
      "Epoch 0 step 179: training accuarcy: 0.6763\n",
      "Epoch 0 step 179: training loss: 39325.646085751294\n",
      "Epoch 0 step 180: training accuarcy: 0.6801\n",
      "Epoch 0 step 180: training loss: 39364.10734793135\n",
      "Epoch 0 step 181: training accuarcy: 0.6736000000000001\n",
      "Epoch 0 step 181: training loss: 38849.94146060744\n",
      "Epoch 0 step 182: training accuarcy: 0.6786\n",
      "Epoch 0 step 182: training loss: 39173.9603958497\n",
      "Epoch 0 step 183: training accuarcy: 0.677\n",
      "Epoch 0 step 183: training loss: 39635.106958330536\n",
      "Epoch 0 step 184: training accuarcy: 0.6771\n",
      "Epoch 0 step 184: training loss: 39208.81132969295\n",
      "Epoch 0 step 185: training accuarcy: 0.6734\n",
      "Epoch 0 step 185: training loss: 38789.465841460915\n",
      "Epoch 0 step 186: training accuarcy: 0.6871\n",
      "Epoch 0 step 186: training loss: 39736.747229759225\n",
      "Epoch 0 step 187: training accuarcy: 0.6766\n",
      "Epoch 0 step 187: training loss: 39323.29212486597\n",
      "Epoch 0 step 188: training accuarcy: 0.6768000000000001\n",
      "Epoch 0 step 188: training loss: 39048.59629402049\n",
      "Epoch 0 step 189: training accuarcy: 0.6846\n",
      "Epoch 0 step 189: training loss: 37374.00909550568\n",
      "Epoch 0 step 190: training accuarcy: 0.7057\n",
      "Epoch 0 step 190: training loss: 39056.9780573401\n",
      "Epoch 0 step 191: training accuarcy: 0.6754\n",
      "Epoch 0 step 191: training loss: 38486.23274049359\n",
      "Epoch 0 step 192: training accuarcy: 0.6865\n",
      "Epoch 0 step 192: training loss: 38398.69184772979\n",
      "Epoch 0 step 193: training accuarcy: 0.6885\n",
      "Epoch 0 step 193: training loss: 38413.40951290011\n",
      "Epoch 0 step 194: training accuarcy: 0.6892\n",
      "Epoch 0 step 194: training loss: 37501.47006082535\n",
      "Epoch 0 step 195: training accuarcy: 0.6972\n",
      "Epoch 0 step 195: training loss: 37677.232122782094\n",
      "Epoch 0 step 196: training accuarcy: 0.6943\n",
      "Epoch 0 step 196: training loss: 38406.32554695066\n",
      "Epoch 0 step 197: training accuarcy: 0.6913\n",
      "Epoch 0 step 197: training loss: 38433.87161495251\n",
      "Epoch 0 step 198: training accuarcy: 0.6915\n",
      "Epoch 0 step 198: training loss: 37971.87203167089\n",
      "Epoch 0 step 199: training accuarcy: 0.6994\n",
      "Epoch 0 step 199: training loss: 38504.17769634327\n",
      "Epoch 0 step 200: training accuarcy: 0.6929000000000001\n",
      "Epoch 0 step 200: training loss: 37708.26631785342\n",
      "Epoch 0 step 201: training accuarcy: 0.6937\n",
      "Epoch 0 step 201: training loss: 38587.582549317885\n",
      "Epoch 0 step 202: training accuarcy: 0.6879000000000001\n",
      "Epoch 0 step 202: training loss: 38174.53621833149\n",
      "Epoch 0 step 203: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 203: training loss: 38244.40277356554\n",
      "Epoch 0 step 204: training accuarcy: 0.6902\n",
      "Epoch 0 step 204: training loss: 38081.89409782494\n",
      "Epoch 0 step 205: training accuarcy: 0.6994\n",
      "Epoch 0 step 205: training loss: 36718.55301575451\n",
      "Epoch 0 step 206: training accuarcy: 0.7115\n",
      "Epoch 0 step 206: training loss: 37567.999639955895\n",
      "Epoch 0 step 207: training accuarcy: 0.6929000000000001\n",
      "Epoch 0 step 207: training loss: 37371.9546372238\n",
      "Epoch 0 step 208: training accuarcy: 0.6997\n",
      "Epoch 0 step 208: training loss: 37974.332624995164\n",
      "Epoch 0 step 209: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 209: training loss: 37521.38619182989\n",
      "Epoch 0 step 210: training accuarcy: 0.6998000000000001\n",
      "Epoch 0 step 210: training loss: 37179.59751053986\n",
      "Epoch 0 step 211: training accuarcy: 0.7019000000000001\n",
      "Epoch 0 step 211: training loss: 36988.69344382011\n",
      "Epoch 0 step 212: training accuarcy: 0.7055\n",
      "Epoch 0 step 212: training loss: 36589.97509181199\n",
      "Epoch 0 step 213: training accuarcy: 0.7147\n",
      "Epoch 0 step 213: training loss: 36941.319995972925\n",
      "Epoch 0 step 214: training accuarcy: 0.7092\n",
      "Epoch 0 step 214: training loss: 37679.369496022955\n",
      "Epoch 0 step 215: training accuarcy: 0.6985\n",
      "Epoch 0 step 215: training loss: 37111.10253890523\n",
      "Epoch 0 step 216: training accuarcy: 0.7133\n",
      "Epoch 0 step 216: training loss: 36797.690407800495\n",
      "Epoch 0 step 217: training accuarcy: 0.7106\n",
      "Epoch 0 step 217: training loss: 37396.748765061464\n",
      "Epoch 0 step 218: training accuarcy: 0.7021000000000001\n",
      "Epoch 0 step 218: training loss: 37298.756098203376\n",
      "Epoch 0 step 219: training accuarcy: 0.6949000000000001\n",
      "Epoch 0 step 219: training loss: 37059.86564541167\n",
      "Epoch 0 step 220: training accuarcy: 0.7037\n",
      "Epoch 0 step 220: training loss: 37526.324279176064\n",
      "Epoch 0 step 221: training accuarcy: 0.7043\n",
      "Epoch 0 step 221: training loss: 37386.98382985782\n",
      "Epoch 0 step 222: training accuarcy: 0.6981\n",
      "Epoch 0 step 222: training loss: 37279.72338488457\n",
      "Epoch 0 step 223: training accuarcy: 0.7073\n",
      "Epoch 0 step 223: training loss: 37251.92267229447\n",
      "Epoch 0 step 224: training accuarcy: 0.7037\n",
      "Epoch 0 step 224: training loss: 36297.81249349617\n",
      "Epoch 0 step 225: training accuarcy: 0.7177\n",
      "Epoch 0 step 225: training loss: 36157.84497395159\n",
      "Epoch 0 step 226: training accuarcy: 0.7209\n",
      "Epoch 0 step 226: training loss: 37621.21377942727\n",
      "Epoch 0 step 227: training accuarcy: 0.6937\n",
      "Epoch 0 step 227: training loss: 37135.0136806387\n",
      "Epoch 0 step 228: training accuarcy: 0.7022\n",
      "Epoch 0 step 228: training loss: 37183.45011177058\n",
      "Epoch 0 step 229: training accuarcy: 0.7019000000000001\n",
      "Epoch 0 step 229: training loss: 36426.575025588594\n",
      "Epoch 0 step 230: training accuarcy: 0.7085\n",
      "Epoch 0 step 230: training loss: 36664.59114357017\n",
      "Epoch 0 step 231: training accuarcy: 0.7139\n",
      "Epoch 0 step 231: training loss: 36839.11500575305\n",
      "Epoch 0 step 232: training accuarcy: 0.7130000000000001\n",
      "Epoch 0 step 232: training loss: 36309.244827085444\n",
      "Epoch 0 step 233: training accuarcy: 0.7162000000000001\n",
      "Epoch 0 step 233: training loss: 35975.57297486562\n",
      "Epoch 0 step 234: training accuarcy: 0.7215\n",
      "Epoch 0 step 234: training loss: 36087.88928335821\n",
      "Epoch 0 step 235: training accuarcy: 0.7166\n",
      "Epoch 0 step 235: training loss: 36701.06899910561\n",
      "Epoch 0 step 236: training accuarcy: 0.7017\n",
      "Epoch 0 step 236: training loss: 36718.86285378187\n",
      "Epoch 0 step 237: training accuarcy: 0.7119000000000001\n",
      "Epoch 0 step 237: training loss: 36629.56285714996\n",
      "Epoch 0 step 238: training accuarcy: 0.7059000000000001\n",
      "Epoch 0 step 238: training loss: 35987.84967568511\n",
      "Epoch 0 step 239: training accuarcy: 0.7112\n",
      "Epoch 0 step 239: training loss: 35997.69075592249\n",
      "Epoch 0 step 240: training accuarcy: 0.7141000000000001\n",
      "Epoch 0 step 240: training loss: 36590.57572169325\n",
      "Epoch 0 step 241: training accuarcy: 0.7110000000000001\n",
      "Epoch 0 step 241: training loss: 36250.50318568181\n",
      "Epoch 0 step 242: training accuarcy: 0.7111000000000001\n",
      "Epoch 0 step 242: training loss: 36174.23376206614\n",
      "Epoch 0 step 243: training accuarcy: 0.7136\n",
      "Epoch 0 step 243: training loss: 36219.77574512607\n",
      "Epoch 0 step 244: training accuarcy: 0.7059000000000001\n",
      "Epoch 0 step 244: training loss: 35556.55232705056\n",
      "Epoch 0 step 245: training accuarcy: 0.7276\n",
      "Epoch 0 step 245: training loss: 35552.941122404154\n",
      "Epoch 0 step 246: training accuarcy: 0.7244\n",
      "Epoch 0 step 246: training loss: 36245.50773222487\n",
      "Epoch 0 step 247: training accuarcy: 0.7073\n",
      "Epoch 0 step 247: training loss: 36335.14419351912\n",
      "Epoch 0 step 248: training accuarcy: 0.7117\n",
      "Epoch 0 step 248: training loss: 35749.66628758664\n",
      "Epoch 0 step 249: training accuarcy: 0.7223\n",
      "Epoch 0 step 249: training loss: 36109.76084318312\n",
      "Epoch 0 step 250: training accuarcy: 0.7144\n",
      "Epoch 0 step 250: training loss: 36130.09847991765\n",
      "Epoch 0 step 251: training accuarcy: 0.7194\n",
      "Epoch 0 step 251: training loss: 35708.386421698015\n",
      "Epoch 0 step 252: training accuarcy: 0.7160000000000001\n",
      "Epoch 0 step 252: training loss: 35171.33923651849\n",
      "Epoch 0 step 253: training accuarcy: 0.7263000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 253: training loss: 35283.5836472917\n",
      "Epoch 0 step 254: training accuarcy: 0.7287\n",
      "Epoch 0 step 254: training loss: 36261.26602595637\n",
      "Epoch 0 step 255: training accuarcy: 0.7130000000000001\n",
      "Epoch 0 step 255: training loss: 35349.418825336514\n",
      "Epoch 0 step 256: training accuarcy: 0.7243\n",
      "Epoch 0 step 256: training loss: 35795.27394716085\n",
      "Epoch 0 step 257: training accuarcy: 0.7185\n",
      "Epoch 0 step 257: training loss: 35761.73537550089\n",
      "Epoch 0 step 258: training accuarcy: 0.7175\n",
      "Epoch 0 step 258: training loss: 35705.99420170844\n",
      "Epoch 0 step 259: training accuarcy: 0.7271000000000001\n",
      "Epoch 0 step 259: training loss: 35823.21189438501\n",
      "Epoch 0 step 260: training accuarcy: 0.7209\n",
      "Epoch 0 step 260: training loss: 36391.402454865245\n",
      "Epoch 0 step 261: training accuarcy: 0.7184\n",
      "Epoch 0 step 261: training loss: 35469.310334724505\n",
      "Epoch 0 step 262: training accuarcy: 0.7233\n",
      "Epoch 0 step 262: training loss: 19016.696263744274\n",
      "Epoch 0 step 263: training accuarcy: 0.7282051282051282\n",
      "Epoch 0: train loss 69507.04873029521, train accuarcy 0.6197700500488281\n",
      "Epoch 0: valid loss 34176.60520719804, valid accuarcy 0.7367500066757202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [04:44<09:28, 284.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 33897.73526608891\n",
      "Epoch 1 step 264: training accuarcy: 0.7512000000000001\n",
      "Epoch 1 step 264: training loss: 33627.92073109104\n",
      "Epoch 1 step 265: training accuarcy: 0.7629\n",
      "Epoch 1 step 265: training loss: 33859.43875398199\n",
      "Epoch 1 step 266: training accuarcy: 0.755\n",
      "Epoch 1 step 266: training loss: 34419.494406719954\n",
      "Epoch 1 step 267: training accuarcy: 0.7502000000000001\n",
      "Epoch 1 step 267: training loss: 33958.08873282592\n",
      "Epoch 1 step 268: training accuarcy: 0.7549\n",
      "Epoch 1 step 268: training loss: 34579.60479484293\n",
      "Epoch 1 step 269: training accuarcy: 0.744\n",
      "Epoch 1 step 269: training loss: 33975.558784306486\n",
      "Epoch 1 step 270: training accuarcy: 0.75\n",
      "Epoch 1 step 270: training loss: 33680.218308659765\n",
      "Epoch 1 step 271: training accuarcy: 0.7542\n",
      "Epoch 1 step 271: training loss: 34217.83717496517\n",
      "Epoch 1 step 272: training accuarcy: 0.7432000000000001\n",
      "Epoch 1 step 272: training loss: 33931.95661477577\n",
      "Epoch 1 step 273: training accuarcy: 0.7496\n",
      "Epoch 1 step 273: training loss: 33898.45510332137\n",
      "Epoch 1 step 274: training accuarcy: 0.7546\n",
      "Epoch 1 step 274: training loss: 34387.85024004977\n",
      "Epoch 1 step 275: training accuarcy: 0.7467\n",
      "Epoch 1 step 275: training loss: 33618.189885150656\n",
      "Epoch 1 step 276: training accuarcy: 0.7493000000000001\n",
      "Epoch 1 step 276: training loss: 33447.34793927985\n",
      "Epoch 1 step 277: training accuarcy: 0.755\n",
      "Epoch 1 step 277: training loss: 34338.75263358942\n",
      "Epoch 1 step 278: training accuarcy: 0.7507\n",
      "Epoch 1 step 278: training loss: 33778.05629960699\n",
      "Epoch 1 step 279: training accuarcy: 0.751\n",
      "Epoch 1 step 279: training loss: 33160.649189603835\n",
      "Epoch 1 step 280: training accuarcy: 0.7623000000000001\n",
      "Epoch 1 step 280: training loss: 33962.248747366466\n",
      "Epoch 1 step 281: training accuarcy: 0.7488\n",
      "Epoch 1 step 281: training loss: 33441.95899129592\n",
      "Epoch 1 step 282: training accuarcy: 0.7608\n",
      "Epoch 1 step 282: training loss: 33327.85852112321\n",
      "Epoch 1 step 283: training accuarcy: 0.7549\n",
      "Epoch 1 step 283: training loss: 33923.82041164738\n",
      "Epoch 1 step 284: training accuarcy: 0.7516\n",
      "Epoch 1 step 284: training loss: 33179.29775718394\n",
      "Epoch 1 step 285: training accuarcy: 0.7589\n",
      "Epoch 1 step 285: training loss: 33426.693047328634\n",
      "Epoch 1 step 286: training accuarcy: 0.7611\n",
      "Epoch 1 step 286: training loss: 33864.99457203272\n",
      "Epoch 1 step 287: training accuarcy: 0.7496\n",
      "Epoch 1 step 287: training loss: 33575.075097061606\n",
      "Epoch 1 step 288: training accuarcy: 0.7569\n",
      "Epoch 1 step 288: training loss: 33550.44780277942\n",
      "Epoch 1 step 289: training accuarcy: 0.7527\n",
      "Epoch 1 step 289: training loss: 33598.1135096469\n",
      "Epoch 1 step 290: training accuarcy: 0.7524000000000001\n",
      "Epoch 1 step 290: training loss: 33484.45293324196\n",
      "Epoch 1 step 291: training accuarcy: 0.7505000000000001\n",
      "Epoch 1 step 291: training loss: 33893.31591089372\n",
      "Epoch 1 step 292: training accuarcy: 0.7468\n",
      "Epoch 1 step 292: training loss: 32821.87965828134\n",
      "Epoch 1 step 293: training accuarcy: 0.7568\n",
      "Epoch 1 step 293: training loss: 33950.121212744685\n",
      "Epoch 1 step 294: training accuarcy: 0.7464000000000001\n",
      "Epoch 1 step 294: training loss: 33342.88358841283\n",
      "Epoch 1 step 295: training accuarcy: 0.7498\n",
      "Epoch 1 step 295: training loss: 32637.083185044903\n",
      "Epoch 1 step 296: training accuarcy: 0.7684000000000001\n",
      "Epoch 1 step 296: training loss: 33690.67167560696\n",
      "Epoch 1 step 297: training accuarcy: 0.7521\n",
      "Epoch 1 step 297: training loss: 33643.373256766135\n",
      "Epoch 1 step 298: training accuarcy: 0.7513000000000001\n",
      "Epoch 1 step 298: training loss: 33330.11631209559\n",
      "Epoch 1 step 299: training accuarcy: 0.7551\n",
      "Epoch 1 step 299: training loss: 33810.25767421289\n",
      "Epoch 1 step 300: training accuarcy: 0.7427\n",
      "Epoch 1 step 300: training loss: 32916.467535879645\n",
      "Epoch 1 step 301: training accuarcy: 0.7632\n",
      "Epoch 1 step 301: training loss: 33502.34684640385\n",
      "Epoch 1 step 302: training accuarcy: 0.7495\n",
      "Epoch 1 step 302: training loss: 33457.573543494545\n",
      "Epoch 1 step 303: training accuarcy: 0.7556\n",
      "Epoch 1 step 303: training loss: 33195.32564912731\n",
      "Epoch 1 step 304: training accuarcy: 0.7522000000000001\n",
      "Epoch 1 step 304: training loss: 33068.92145992437\n",
      "Epoch 1 step 305: training accuarcy: 0.7526\n",
      "Epoch 1 step 305: training loss: 33946.44745189696\n",
      "Epoch 1 step 306: training accuarcy: 0.7539\n",
      "Epoch 1 step 306: training loss: 33162.79082708075\n",
      "Epoch 1 step 307: training accuarcy: 0.7568\n",
      "Epoch 1 step 307: training loss: 33483.48400808708\n",
      "Epoch 1 step 308: training accuarcy: 0.7475\n",
      "Epoch 1 step 308: training loss: 33038.44627216138\n",
      "Epoch 1 step 309: training accuarcy: 0.7573000000000001\n",
      "Epoch 1 step 309: training loss: 33363.73430353974\n",
      "Epoch 1 step 310: training accuarcy: 0.7611\n",
      "Epoch 1 step 310: training loss: 32713.876858404597\n",
      "Epoch 1 step 311: training accuarcy: 0.7601\n",
      "Epoch 1 step 311: training loss: 32752.870026137392\n",
      "Epoch 1 step 312: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 312: training loss: 33337.90962280267\n",
      "Epoch 1 step 313: training accuarcy: 0.755\n",
      "Epoch 1 step 313: training loss: 33440.00165638448\n",
      "Epoch 1 step 314: training accuarcy: 0.7553000000000001\n",
      "Epoch 1 step 314: training loss: 33080.83486002656\n",
      "Epoch 1 step 315: training accuarcy: 0.7565000000000001\n",
      "Epoch 1 step 315: training loss: 33557.914359070375\n",
      "Epoch 1 step 316: training accuarcy: 0.7553000000000001\n",
      "Epoch 1 step 316: training loss: 33290.84712263517\n",
      "Epoch 1 step 317: training accuarcy: 0.7517\n",
      "Epoch 1 step 317: training loss: 32794.41550657168\n",
      "Epoch 1 step 318: training accuarcy: 0.7596\n",
      "Epoch 1 step 318: training loss: 33309.86865743852\n",
      "Epoch 1 step 319: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 319: training loss: 33463.351941748355\n",
      "Epoch 1 step 320: training accuarcy: 0.7502000000000001\n",
      "Epoch 1 step 320: training loss: 32963.2043193485\n",
      "Epoch 1 step 321: training accuarcy: 0.7574000000000001\n",
      "Epoch 1 step 321: training loss: 33265.96348891458\n",
      "Epoch 1 step 322: training accuarcy: 0.744\n",
      "Epoch 1 step 322: training loss: 32878.95551609124\n",
      "Epoch 1 step 323: training accuarcy: 0.7539\n",
      "Epoch 1 step 323: training loss: 32430.965472522377\n",
      "Epoch 1 step 324: training accuarcy: 0.762\n",
      "Epoch 1 step 324: training loss: 33063.33835467138\n",
      "Epoch 1 step 325: training accuarcy: 0.7575000000000001\n",
      "Epoch 1 step 325: training loss: 33046.692496496\n",
      "Epoch 1 step 326: training accuarcy: 0.7546\n",
      "Epoch 1 step 326: training loss: 33003.77914897269\n",
      "Epoch 1 step 327: training accuarcy: 0.7554000000000001\n",
      "Epoch 1 step 327: training loss: 33494.02876265966\n",
      "Epoch 1 step 328: training accuarcy: 0.7496\n",
      "Epoch 1 step 328: training loss: 33344.277414221106\n",
      "Epoch 1 step 329: training accuarcy: 0.7515000000000001\n",
      "Epoch 1 step 329: training loss: 33040.93526571947\n",
      "Epoch 1 step 330: training accuarcy: 0.7552\n",
      "Epoch 1 step 330: training loss: 32671.145095879747\n",
      "Epoch 1 step 331: training accuarcy: 0.7584000000000001\n",
      "Epoch 1 step 331: training loss: 32936.312434262334\n",
      "Epoch 1 step 332: training accuarcy: 0.758\n",
      "Epoch 1 step 332: training loss: 32669.99215151014\n",
      "Epoch 1 step 333: training accuarcy: 0.7587\n",
      "Epoch 1 step 333: training loss: 32941.216285688046\n",
      "Epoch 1 step 334: training accuarcy: 0.7567\n",
      "Epoch 1 step 334: training loss: 33074.02828427464\n",
      "Epoch 1 step 335: training accuarcy: 0.7512000000000001\n",
      "Epoch 1 step 335: training loss: 32871.65197144772\n",
      "Epoch 1 step 336: training accuarcy: 0.7544000000000001\n",
      "Epoch 1 step 336: training loss: 33798.76394094384\n",
      "Epoch 1 step 337: training accuarcy: 0.7418\n",
      "Epoch 1 step 337: training loss: 32565.333036479475\n",
      "Epoch 1 step 338: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 338: training loss: 32668.522789018873\n",
      "Epoch 1 step 339: training accuarcy: 0.7609\n",
      "Epoch 1 step 339: training loss: 33041.611571360125\n",
      "Epoch 1 step 340: training accuarcy: 0.7544000000000001\n",
      "Epoch 1 step 340: training loss: 32847.492937207164\n",
      "Epoch 1 step 341: training accuarcy: 0.7533000000000001\n",
      "Epoch 1 step 341: training loss: 32841.13285299356\n",
      "Epoch 1 step 342: training accuarcy: 0.7604000000000001\n",
      "Epoch 1 step 342: training loss: 33178.733410511915\n",
      "Epoch 1 step 343: training accuarcy: 0.7447\n",
      "Epoch 1 step 343: training loss: 33413.976098994826\n",
      "Epoch 1 step 344: training accuarcy: 0.7519\n",
      "Epoch 1 step 344: training loss: 32709.625318869756\n",
      "Epoch 1 step 345: training accuarcy: 0.7549\n",
      "Epoch 1 step 345: training loss: 32833.12797665099\n",
      "Epoch 1 step 346: training accuarcy: 0.7472000000000001\n",
      "Epoch 1 step 346: training loss: 32301.024615882332\n",
      "Epoch 1 step 347: training accuarcy: 0.7606\n",
      "Epoch 1 step 347: training loss: 32612.646994211987\n",
      "Epoch 1 step 348: training accuarcy: 0.7573000000000001\n",
      "Epoch 1 step 348: training loss: 32789.26414775667\n",
      "Epoch 1 step 349: training accuarcy: 0.7627\n",
      "Epoch 1 step 349: training loss: 32875.51866975961\n",
      "Epoch 1 step 350: training accuarcy: 0.755\n",
      "Epoch 1 step 350: training loss: 32283.088627578123\n",
      "Epoch 1 step 351: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 351: training loss: 32758.290694526935\n",
      "Epoch 1 step 352: training accuarcy: 0.7544000000000001\n",
      "Epoch 1 step 352: training loss: 32729.832013708365\n",
      "Epoch 1 step 353: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 353: training loss: 32552.40321312695\n",
      "Epoch 1 step 354: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 354: training loss: 33065.70785128449\n",
      "Epoch 1 step 355: training accuarcy: 0.7602\n",
      "Epoch 1 step 355: training loss: 32746.836344509866\n",
      "Epoch 1 step 356: training accuarcy: 0.7551\n",
      "Epoch 1 step 356: training loss: 32792.174047386645\n",
      "Epoch 1 step 357: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 357: training loss: 31907.405215076466\n",
      "Epoch 1 step 358: training accuarcy: 0.7672\n",
      "Epoch 1 step 358: training loss: 31889.518848441057\n",
      "Epoch 1 step 359: training accuarcy: 0.7585000000000001\n",
      "Epoch 1 step 359: training loss: 32531.070171478135\n",
      "Epoch 1 step 360: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 360: training loss: 32817.18331652684\n",
      "Epoch 1 step 361: training accuarcy: 0.7556\n",
      "Epoch 1 step 361: training loss: 32352.905730441646\n",
      "Epoch 1 step 362: training accuarcy: 0.7566\n",
      "Epoch 1 step 362: training loss: 32599.608607521204\n",
      "Epoch 1 step 363: training accuarcy: 0.7594000000000001\n",
      "Epoch 1 step 363: training loss: 32196.501833628714\n",
      "Epoch 1 step 364: training accuarcy: 0.762\n",
      "Epoch 1 step 364: training loss: 32321.346374023153\n",
      "Epoch 1 step 365: training accuarcy: 0.7592\n",
      "Epoch 1 step 365: training loss: 32669.111020846936\n",
      "Epoch 1 step 366: training accuarcy: 0.7662\n",
      "Epoch 1 step 366: training loss: 32840.23340395061\n",
      "Epoch 1 step 367: training accuarcy: 0.7496\n",
      "Epoch 1 step 367: training loss: 32704.535348515645\n",
      "Epoch 1 step 368: training accuarcy: 0.7505000000000001\n",
      "Epoch 1 step 368: training loss: 31966.6246183866\n",
      "Epoch 1 step 369: training accuarcy: 0.7598\n",
      "Epoch 1 step 369: training loss: 32029.66763187691\n",
      "Epoch 1 step 370: training accuarcy: 0.764\n",
      "Epoch 1 step 370: training loss: 32168.448043466473\n",
      "Epoch 1 step 371: training accuarcy: 0.7587\n",
      "Epoch 1 step 371: training loss: 33164.511349409724\n",
      "Epoch 1 step 372: training accuarcy: 0.7483000000000001\n",
      "Epoch 1 step 372: training loss: 32207.86436295799\n",
      "Epoch 1 step 373: training accuarcy: 0.7633000000000001\n",
      "Epoch 1 step 373: training loss: 32318.409828974625\n",
      "Epoch 1 step 374: training accuarcy: 0.7607\n",
      "Epoch 1 step 374: training loss: 32534.864342578825\n",
      "Epoch 1 step 375: training accuarcy: 0.7601\n",
      "Epoch 1 step 375: training loss: 31711.09836718583\n",
      "Epoch 1 step 376: training accuarcy: 0.7687\n",
      "Epoch 1 step 376: training loss: 31745.63953451581\n",
      "Epoch 1 step 377: training accuarcy: 0.7677\n",
      "Epoch 1 step 377: training loss: 32807.243137457124\n",
      "Epoch 1 step 378: training accuarcy: 0.7497\n",
      "Epoch 1 step 378: training loss: 32417.872862001605\n",
      "Epoch 1 step 379: training accuarcy: 0.7496\n",
      "Epoch 1 step 379: training loss: 32335.290759972147\n",
      "Epoch 1 step 380: training accuarcy: 0.7639\n",
      "Epoch 1 step 380: training loss: 32199.06204037211\n",
      "Epoch 1 step 381: training accuarcy: 0.7586\n",
      "Epoch 1 step 381: training loss: 32340.294214253678\n",
      "Epoch 1 step 382: training accuarcy: 0.7572\n",
      "Epoch 1 step 382: training loss: 32006.661291125332\n",
      "Epoch 1 step 383: training accuarcy: 0.7614000000000001\n",
      "Epoch 1 step 383: training loss: 32535.134146780878\n",
      "Epoch 1 step 384: training accuarcy: 0.7566\n",
      "Epoch 1 step 384: training loss: 31724.313978381484\n",
      "Epoch 1 step 385: training accuarcy: 0.7628\n",
      "Epoch 1 step 385: training loss: 32749.06921809152\n",
      "Epoch 1 step 386: training accuarcy: 0.7523000000000001\n",
      "Epoch 1 step 386: training loss: 31601.202044580958\n",
      "Epoch 1 step 387: training accuarcy: 0.7692\n",
      "Epoch 1 step 387: training loss: 31960.277607696822\n",
      "Epoch 1 step 388: training accuarcy: 0.7511\n",
      "Epoch 1 step 388: training loss: 32803.5282594162\n",
      "Epoch 1 step 389: training accuarcy: 0.7528\n",
      "Epoch 1 step 389: training loss: 31893.80161555691\n",
      "Epoch 1 step 390: training accuarcy: 0.7572\n",
      "Epoch 1 step 390: training loss: 32377.686256778063\n",
      "Epoch 1 step 391: training accuarcy: 0.7553000000000001\n",
      "Epoch 1 step 391: training loss: 32239.707307534947\n",
      "Epoch 1 step 392: training accuarcy: 0.7572\n",
      "Epoch 1 step 392: training loss: 32682.71947744873\n",
      "Epoch 1 step 393: training accuarcy: 0.7557\n",
      "Epoch 1 step 393: training loss: 32210.92010356725\n",
      "Epoch 1 step 394: training accuarcy: 0.7604000000000001\n",
      "Epoch 1 step 394: training loss: 31968.581868517063\n",
      "Epoch 1 step 395: training accuarcy: 0.7575000000000001\n",
      "Epoch 1 step 395: training loss: 32165.300826280483\n",
      "Epoch 1 step 396: training accuarcy: 0.7609\n",
      "Epoch 1 step 396: training loss: 31790.990701162493\n",
      "Epoch 1 step 397: training accuarcy: 0.7675000000000001\n",
      "Epoch 1 step 397: training loss: 32255.09904862519\n",
      "Epoch 1 step 398: training accuarcy: 0.7546\n",
      "Epoch 1 step 398: training loss: 32186.83098786801\n",
      "Epoch 1 step 399: training accuarcy: 0.7534000000000001\n",
      "Epoch 1 step 399: training loss: 32414.69396507468\n",
      "Epoch 1 step 400: training accuarcy: 0.7541\n",
      "Epoch 1 step 400: training loss: 31813.506357457867\n",
      "Epoch 1 step 401: training accuarcy: 0.7708\n",
      "Epoch 1 step 401: training loss: 31927.954225857153\n",
      "Epoch 1 step 402: training accuarcy: 0.769\n",
      "Epoch 1 step 402: training loss: 32057.412532388593\n",
      "Epoch 1 step 403: training accuarcy: 0.765\n",
      "Epoch 1 step 403: training loss: 31809.398318097803\n",
      "Epoch 1 step 404: training accuarcy: 0.7659\n",
      "Epoch 1 step 404: training loss: 32243.087835203733\n",
      "Epoch 1 step 405: training accuarcy: 0.7564000000000001\n",
      "Epoch 1 step 405: training loss: 31521.77031785419\n",
      "Epoch 1 step 406: training accuarcy: 0.7721\n",
      "Epoch 1 step 406: training loss: 31640.17240192563\n",
      "Epoch 1 step 407: training accuarcy: 0.7636000000000001\n",
      "Epoch 1 step 407: training loss: 31297.734369875365\n",
      "Epoch 1 step 408: training accuarcy: 0.7694000000000001\n",
      "Epoch 1 step 408: training loss: 31967.934905719936\n",
      "Epoch 1 step 409: training accuarcy: 0.759\n",
      "Epoch 1 step 409: training loss: 31191.621507700176\n",
      "Epoch 1 step 410: training accuarcy: 0.769\n",
      "Epoch 1 step 410: training loss: 32410.047695145528\n",
      "Epoch 1 step 411: training accuarcy: 0.758\n",
      "Epoch 1 step 411: training loss: 32072.95696705023\n",
      "Epoch 1 step 412: training accuarcy: 0.7617\n",
      "Epoch 1 step 412: training loss: 31815.97274926536\n",
      "Epoch 1 step 413: training accuarcy: 0.7542\n",
      "Epoch 1 step 413: training loss: 31100.34296249274\n",
      "Epoch 1 step 414: training accuarcy: 0.7677\n",
      "Epoch 1 step 414: training loss: 31807.693189530313\n",
      "Epoch 1 step 415: training accuarcy: 0.763\n",
      "Epoch 1 step 415: training loss: 31683.69475984109\n",
      "Epoch 1 step 416: training accuarcy: 0.7664000000000001\n",
      "Epoch 1 step 416: training loss: 31492.21572833546\n",
      "Epoch 1 step 417: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 417: training loss: 32030.29921588873\n",
      "Epoch 1 step 418: training accuarcy: 0.7539\n",
      "Epoch 1 step 418: training loss: 31742.47585022003\n",
      "Epoch 1 step 419: training accuarcy: 0.768\n",
      "Epoch 1 step 419: training loss: 31850.55601001879\n",
      "Epoch 1 step 420: training accuarcy: 0.757\n",
      "Epoch 1 step 420: training loss: 31596.01608605757\n",
      "Epoch 1 step 421: training accuarcy: 0.7702\n",
      "Epoch 1 step 421: training loss: 31446.833316659286\n",
      "Epoch 1 step 422: training accuarcy: 0.765\n",
      "Epoch 1 step 422: training loss: 31418.75077276092\n",
      "Epoch 1 step 423: training accuarcy: 0.7662\n",
      "Epoch 1 step 423: training loss: 32438.971586927968\n",
      "Epoch 1 step 424: training accuarcy: 0.7549\n",
      "Epoch 1 step 424: training loss: 31720.199972442824\n",
      "Epoch 1 step 425: training accuarcy: 0.7598\n",
      "Epoch 1 step 425: training loss: 31931.866868319114\n",
      "Epoch 1 step 426: training accuarcy: 0.7585000000000001\n",
      "Epoch 1 step 426: training loss: 31794.391303338823\n",
      "Epoch 1 step 427: training accuarcy: 0.7599\n",
      "Epoch 1 step 427: training loss: 31916.702145435374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 428: training accuarcy: 0.7621\n",
      "Epoch 1 step 428: training loss: 31340.68653146692\n",
      "Epoch 1 step 429: training accuarcy: 0.7647\n",
      "Epoch 1 step 429: training loss: 31231.03471677229\n",
      "Epoch 1 step 430: training accuarcy: 0.7698\n",
      "Epoch 1 step 430: training loss: 31528.196175496978\n",
      "Epoch 1 step 431: training accuarcy: 0.7651\n",
      "Epoch 1 step 431: training loss: 31313.9731503945\n",
      "Epoch 1 step 432: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 432: training loss: 32011.114723363236\n",
      "Epoch 1 step 433: training accuarcy: 0.7627\n",
      "Epoch 1 step 433: training loss: 31685.05147831228\n",
      "Epoch 1 step 434: training accuarcy: 0.7606\n",
      "Epoch 1 step 434: training loss: 31442.05208022455\n",
      "Epoch 1 step 435: training accuarcy: 0.7678\n",
      "Epoch 1 step 435: training loss: 30933.32182664942\n",
      "Epoch 1 step 436: training accuarcy: 0.7799\n",
      "Epoch 1 step 436: training loss: 31461.20992526184\n",
      "Epoch 1 step 437: training accuarcy: 0.769\n",
      "Epoch 1 step 437: training loss: 31561.013876726684\n",
      "Epoch 1 step 438: training accuarcy: 0.7659\n",
      "Epoch 1 step 438: training loss: 31364.77981067232\n",
      "Epoch 1 step 439: training accuarcy: 0.7624000000000001\n",
      "Epoch 1 step 439: training loss: 31239.816823571964\n",
      "Epoch 1 step 440: training accuarcy: 0.7616\n",
      "Epoch 1 step 440: training loss: 31933.709632518756\n",
      "Epoch 1 step 441: training accuarcy: 0.7577\n",
      "Epoch 1 step 441: training loss: 31711.891328222006\n",
      "Epoch 1 step 442: training accuarcy: 0.7576\n",
      "Epoch 1 step 442: training loss: 31189.2733942036\n",
      "Epoch 1 step 443: training accuarcy: 0.7704000000000001\n",
      "Epoch 1 step 443: training loss: 31198.186381080544\n",
      "Epoch 1 step 444: training accuarcy: 0.7643000000000001\n",
      "Epoch 1 step 444: training loss: 31530.42542307868\n",
      "Epoch 1 step 445: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 445: training loss: 31254.359650476166\n",
      "Epoch 1 step 446: training accuarcy: 0.7661\n",
      "Epoch 1 step 446: training loss: 31344.21264066226\n",
      "Epoch 1 step 447: training accuarcy: 0.7633000000000001\n",
      "Epoch 1 step 447: training loss: 31398.549995984442\n",
      "Epoch 1 step 448: training accuarcy: 0.7621\n",
      "Epoch 1 step 448: training loss: 31208.688716033197\n",
      "Epoch 1 step 449: training accuarcy: 0.7673\n",
      "Epoch 1 step 449: training loss: 30811.59059958956\n",
      "Epoch 1 step 450: training accuarcy: 0.7705000000000001\n",
      "Epoch 1 step 450: training loss: 31516.754948560676\n",
      "Epoch 1 step 451: training accuarcy: 0.7743\n",
      "Epoch 1 step 451: training loss: 31761.84011342301\n",
      "Epoch 1 step 452: training accuarcy: 0.7618\n",
      "Epoch 1 step 452: training loss: 31041.296678358605\n",
      "Epoch 1 step 453: training accuarcy: 0.7699\n",
      "Epoch 1 step 453: training loss: 31639.513659157183\n",
      "Epoch 1 step 454: training accuarcy: 0.7613000000000001\n",
      "Epoch 1 step 454: training loss: 31127.263651678582\n",
      "Epoch 1 step 455: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 455: training loss: 30789.276657678794\n",
      "Epoch 1 step 456: training accuarcy: 0.7689\n",
      "Epoch 1 step 456: training loss: 31322.65418290154\n",
      "Epoch 1 step 457: training accuarcy: 0.7649\n",
      "Epoch 1 step 457: training loss: 30397.547531135362\n",
      "Epoch 1 step 458: training accuarcy: 0.777\n",
      "Epoch 1 step 458: training loss: 31011.154911169462\n",
      "Epoch 1 step 459: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 459: training loss: 31243.590652410912\n",
      "Epoch 1 step 460: training accuarcy: 0.7623000000000001\n",
      "Epoch 1 step 460: training loss: 30780.38506041343\n",
      "Epoch 1 step 461: training accuarcy: 0.7698\n",
      "Epoch 1 step 461: training loss: 31329.320897118687\n",
      "Epoch 1 step 462: training accuarcy: 0.7625000000000001\n",
      "Epoch 1 step 462: training loss: 31231.386822193595\n",
      "Epoch 1 step 463: training accuarcy: 0.7679\n",
      "Epoch 1 step 463: training loss: 30532.79111943255\n",
      "Epoch 1 step 464: training accuarcy: 0.7722\n",
      "Epoch 1 step 464: training loss: 30177.550774406896\n",
      "Epoch 1 step 465: training accuarcy: 0.7777000000000001\n",
      "Epoch 1 step 465: training loss: 31177.168457109827\n",
      "Epoch 1 step 466: training accuarcy: 0.7703\n",
      "Epoch 1 step 466: training loss: 30743.949649857106\n",
      "Epoch 1 step 467: training accuarcy: 0.7731\n",
      "Epoch 1 step 467: training loss: 31971.961389356933\n",
      "Epoch 1 step 468: training accuarcy: 0.7559\n",
      "Epoch 1 step 468: training loss: 31534.106986922947\n",
      "Epoch 1 step 469: training accuarcy: 0.7634000000000001\n",
      "Epoch 1 step 469: training loss: 30927.3646369544\n",
      "Epoch 1 step 470: training accuarcy: 0.7637\n",
      "Epoch 1 step 470: training loss: 31196.57612729846\n",
      "Epoch 1 step 471: training accuarcy: 0.7523000000000001\n",
      "Epoch 1 step 471: training loss: 30690.6552643336\n",
      "Epoch 1 step 472: training accuarcy: 0.7682\n",
      "Epoch 1 step 472: training loss: 31515.4903580242\n",
      "Epoch 1 step 473: training accuarcy: 0.7693\n",
      "Epoch 1 step 473: training loss: 30558.818359656518\n",
      "Epoch 1 step 474: training accuarcy: 0.7727\n",
      "Epoch 1 step 474: training loss: 31655.666046955404\n",
      "Epoch 1 step 475: training accuarcy: 0.7585000000000001\n",
      "Epoch 1 step 475: training loss: 31191.623881723255\n",
      "Epoch 1 step 476: training accuarcy: 0.7675000000000001\n",
      "Epoch 1 step 476: training loss: 31234.00270141621\n",
      "Epoch 1 step 477: training accuarcy: 0.7746000000000001\n",
      "Epoch 1 step 477: training loss: 30509.689755217674\n",
      "Epoch 1 step 478: training accuarcy: 0.7754000000000001\n",
      "Epoch 1 step 478: training loss: 30428.226165934226\n",
      "Epoch 1 step 479: training accuarcy: 0.7788\n",
      "Epoch 1 step 479: training loss: 30213.59417472701\n",
      "Epoch 1 step 480: training accuarcy: 0.7753\n",
      "Epoch 1 step 480: training loss: 31317.067532237128\n",
      "Epoch 1 step 481: training accuarcy: 0.7721\n",
      "Epoch 1 step 481: training loss: 31047.274294729763\n",
      "Epoch 1 step 482: training accuarcy: 0.7676000000000001\n",
      "Epoch 1 step 482: training loss: 30449.31369445541\n",
      "Epoch 1 step 483: training accuarcy: 0.7767000000000001\n",
      "Epoch 1 step 483: training loss: 30874.05929711971\n",
      "Epoch 1 step 484: training accuarcy: 0.7788\n",
      "Epoch 1 step 484: training loss: 30874.263249790347\n",
      "Epoch 1 step 485: training accuarcy: 0.7621\n",
      "Epoch 1 step 485: training loss: 30786.90260452978\n",
      "Epoch 1 step 486: training accuarcy: 0.7736000000000001\n",
      "Epoch 1 step 486: training loss: 30786.908273640103\n",
      "Epoch 1 step 487: training accuarcy: 0.7685000000000001\n",
      "Epoch 1 step 487: training loss: 30817.730149620955\n",
      "Epoch 1 step 488: training accuarcy: 0.7648\n",
      "Epoch 1 step 488: training loss: 31200.305699358658\n",
      "Epoch 1 step 489: training accuarcy: 0.7613000000000001\n",
      "Epoch 1 step 489: training loss: 30336.690212818536\n",
      "Epoch 1 step 490: training accuarcy: 0.7804\n",
      "Epoch 1 step 490: training loss: 31006.561547345373\n",
      "Epoch 1 step 491: training accuarcy: 0.7673\n",
      "Epoch 1 step 491: training loss: 30709.58730930012\n",
      "Epoch 1 step 492: training accuarcy: 0.7688\n",
      "Epoch 1 step 492: training loss: 30986.071755612833\n",
      "Epoch 1 step 493: training accuarcy: 0.7639\n",
      "Epoch 1 step 493: training loss: 30672.419556552413\n",
      "Epoch 1 step 494: training accuarcy: 0.7715000000000001\n",
      "Epoch 1 step 494: training loss: 30845.331882341314\n",
      "Epoch 1 step 495: training accuarcy: 0.7632\n",
      "Epoch 1 step 495: training loss: 30785.577561886672\n",
      "Epoch 1 step 496: training accuarcy: 0.766\n",
      "Epoch 1 step 496: training loss: 30879.468770475927\n",
      "Epoch 1 step 497: training accuarcy: 0.7658\n",
      "Epoch 1 step 497: training loss: 30860.1428067725\n",
      "Epoch 1 step 498: training accuarcy: 0.7682\n",
      "Epoch 1 step 498: training loss: 30316.78826274063\n",
      "Epoch 1 step 499: training accuarcy: 0.7712\n",
      "Epoch 1 step 499: training loss: 30878.565826657614\n",
      "Epoch 1 step 500: training accuarcy: 0.7683\n",
      "Epoch 1 step 500: training loss: 30742.70694046261\n",
      "Epoch 1 step 501: training accuarcy: 0.7727\n",
      "Epoch 1 step 501: training loss: 29924.750974033872\n",
      "Epoch 1 step 502: training accuarcy: 0.7851\n",
      "Epoch 1 step 502: training loss: 30890.079474844802\n",
      "Epoch 1 step 503: training accuarcy: 0.7692\n",
      "Epoch 1 step 503: training loss: 30413.945626521447\n",
      "Epoch 1 step 504: training accuarcy: 0.7676000000000001\n",
      "Epoch 1 step 504: training loss: 30681.637316146698\n",
      "Epoch 1 step 505: training accuarcy: 0.7751\n",
      "Epoch 1 step 505: training loss: 31194.729674088536\n",
      "Epoch 1 step 506: training accuarcy: 0.7659\n",
      "Epoch 1 step 506: training loss: 30984.620404682406\n",
      "Epoch 1 step 507: training accuarcy: 0.7677\n",
      "Epoch 1 step 507: training loss: 30847.67498040521\n",
      "Epoch 1 step 508: training accuarcy: 0.7723\n",
      "Epoch 1 step 508: training loss: 30661.78931097259\n",
      "Epoch 1 step 509: training accuarcy: 0.7729\n",
      "Epoch 1 step 509: training loss: 30210.360307688556\n",
      "Epoch 1 step 510: training accuarcy: 0.7787000000000001\n",
      "Epoch 1 step 510: training loss: 31088.248385703675\n",
      "Epoch 1 step 511: training accuarcy: 0.7649\n",
      "Epoch 1 step 511: training loss: 30091.698038304497\n",
      "Epoch 1 step 512: training accuarcy: 0.7865000000000001\n",
      "Epoch 1 step 512: training loss: 31080.41246341725\n",
      "Epoch 1 step 513: training accuarcy: 0.765\n",
      "Epoch 1 step 513: training loss: 30956.608226797467\n",
      "Epoch 1 step 514: training accuarcy: 0.766\n",
      "Epoch 1 step 514: training loss: 30494.49962101173\n",
      "Epoch 1 step 515: training accuarcy: 0.7668\n",
      "Epoch 1 step 515: training loss: 30308.58264942144\n",
      "Epoch 1 step 516: training accuarcy: 0.7669\n",
      "Epoch 1 step 516: training loss: 30016.736601742497\n",
      "Epoch 1 step 517: training accuarcy: 0.7771\n",
      "Epoch 1 step 517: training loss: 30402.490754482762\n",
      "Epoch 1 step 518: training accuarcy: 0.7728\n",
      "Epoch 1 step 518: training loss: 30600.781787366817\n",
      "Epoch 1 step 519: training accuarcy: 0.7682\n",
      "Epoch 1 step 519: training loss: 30155.471997878347\n",
      "Epoch 1 step 520: training accuarcy: 0.773\n",
      "Epoch 1 step 520: training loss: 29612.74201782978\n",
      "Epoch 1 step 521: training accuarcy: 0.7826000000000001\n",
      "Epoch 1 step 521: training loss: 30837.533832018366\n",
      "Epoch 1 step 522: training accuarcy: 0.7701\n",
      "Epoch 1 step 522: training loss: 30179.623673998038\n",
      "Epoch 1 step 523: training accuarcy: 0.7759\n",
      "Epoch 1 step 523: training loss: 30232.397151933594\n",
      "Epoch 1 step 524: training accuarcy: 0.7821\n",
      "Epoch 1 step 524: training loss: 30275.16690638527\n",
      "Epoch 1 step 525: training accuarcy: 0.7775000000000001\n",
      "Epoch 1 step 525: training loss: 15045.977374179745\n",
      "Epoch 1 step 526: training accuarcy: 0.7612820512820513\n",
      "Epoch 1: train loss 32029.661246177333, train accuarcy 0.7435723543167114\n",
      "Epoch 1: valid loss 29596.49971615582, valid accuarcy 0.7761274576187134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [09:40<04:47, 287.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 29592.70226770191\n",
      "Epoch 2 step 527: training accuarcy: 0.7885000000000001\n",
      "Epoch 2 step 527: training loss: 29542.998646134674\n",
      "Epoch 2 step 528: training accuarcy: 0.7934\n",
      "Epoch 2 step 528: training loss: 29294.765387400574\n",
      "Epoch 2 step 529: training accuarcy: 0.7911\n",
      "Epoch 2 step 529: training loss: 29082.70863222242\n",
      "Epoch 2 step 530: training accuarcy: 0.7921\n",
      "Epoch 2 step 530: training loss: 29819.621505648596\n",
      "Epoch 2 step 531: training accuarcy: 0.7924\n",
      "Epoch 2 step 531: training loss: 30067.853013730157\n",
      "Epoch 2 step 532: training accuarcy: 0.781\n",
      "Epoch 2 step 532: training loss: 29661.87719487754\n",
      "Epoch 2 step 533: training accuarcy: 0.7889\n",
      "Epoch 2 step 533: training loss: 29798.27668340026\n",
      "Epoch 2 step 534: training accuarcy: 0.79\n",
      "Epoch 2 step 534: training loss: 29361.29082672926\n",
      "Epoch 2 step 535: training accuarcy: 0.7918000000000001\n",
      "Epoch 2 step 535: training loss: 29210.004528644535\n",
      "Epoch 2 step 536: training accuarcy: 0.8016000000000001\n",
      "Epoch 2 step 536: training loss: 29313.07267684312\n",
      "Epoch 2 step 537: training accuarcy: 0.7994\n",
      "Epoch 2 step 537: training loss: 29782.11569499864\n",
      "Epoch 2 step 538: training accuarcy: 0.7873\n",
      "Epoch 2 step 538: training loss: 30129.044534906418\n",
      "Epoch 2 step 539: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 539: training loss: 29454.069567793107\n",
      "Epoch 2 step 540: training accuarcy: 0.7921\n",
      "Epoch 2 step 540: training loss: 28574.59616904313\n",
      "Epoch 2 step 541: training accuarcy: 0.8063\n",
      "Epoch 2 step 541: training loss: 29838.54690035057\n",
      "Epoch 2 step 542: training accuarcy: 0.7823\n",
      "Epoch 2 step 542: training loss: 28906.652694586268\n",
      "Epoch 2 step 543: training accuarcy: 0.797\n",
      "Epoch 2 step 543: training loss: 29031.074843555907\n",
      "Epoch 2 step 544: training accuarcy: 0.7935\n",
      "Epoch 2 step 544: training loss: 28578.668650391926\n",
      "Epoch 2 step 545: training accuarcy: 0.8006000000000001\n",
      "Epoch 2 step 545: training loss: 29307.085513161117\n",
      "Epoch 2 step 546: training accuarcy: 0.7903\n",
      "Epoch 2 step 546: training loss: 29699.77302650849\n",
      "Epoch 2 step 547: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 547: training loss: 29383.518824447412\n",
      "Epoch 2 step 548: training accuarcy: 0.7923\n",
      "Epoch 2 step 548: training loss: 29654.199444825746\n",
      "Epoch 2 step 549: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 549: training loss: 29351.94995168376\n",
      "Epoch 2 step 550: training accuarcy: 0.7916000000000001\n",
      "Epoch 2 step 550: training loss: 29044.994336089054\n",
      "Epoch 2 step 551: training accuarcy: 0.7979\n",
      "Epoch 2 step 551: training loss: 29108.202428564284\n",
      "Epoch 2 step 552: training accuarcy: 0.7932\n",
      "Epoch 2 step 552: training loss: 29735.644941333987\n",
      "Epoch 2 step 553: training accuarcy: 0.7799\n",
      "Epoch 2 step 553: training loss: 29967.53177383885\n",
      "Epoch 2 step 554: training accuarcy: 0.7772\n",
      "Epoch 2 step 554: training loss: 29242.24277931359\n",
      "Epoch 2 step 555: training accuarcy: 0.7862\n",
      "Epoch 2 step 555: training loss: 29596.190817581566\n",
      "Epoch 2 step 556: training accuarcy: 0.7881\n",
      "Epoch 2 step 556: training loss: 28957.44585856754\n",
      "Epoch 2 step 557: training accuarcy: 0.7971\n",
      "Epoch 2 step 557: training loss: 29505.843722498947\n",
      "Epoch 2 step 558: training accuarcy: 0.79\n",
      "Epoch 2 step 558: training loss: 29782.493275117697\n",
      "Epoch 2 step 559: training accuarcy: 0.788\n",
      "Epoch 2 step 559: training loss: 29803.36887174484\n",
      "Epoch 2 step 560: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 560: training loss: 29520.29926603414\n",
      "Epoch 2 step 561: training accuarcy: 0.7942\n",
      "Epoch 2 step 561: training loss: 28919.897783858905\n",
      "Epoch 2 step 562: training accuarcy: 0.8006000000000001\n",
      "Epoch 2 step 562: training loss: 29246.71949639271\n",
      "Epoch 2 step 563: training accuarcy: 0.7828\n",
      "Epoch 2 step 563: training loss: 29319.908516175776\n",
      "Epoch 2 step 564: training accuarcy: 0.7918000000000001\n",
      "Epoch 2 step 564: training loss: 29241.69408771834\n",
      "Epoch 2 step 565: training accuarcy: 0.793\n",
      "Epoch 2 step 565: training loss: 29362.758050844946\n",
      "Epoch 2 step 566: training accuarcy: 0.789\n",
      "Epoch 2 step 566: training loss: 28588.216366276414\n",
      "Epoch 2 step 567: training accuarcy: 0.8017000000000001\n",
      "Epoch 2 step 567: training loss: 29930.059238298054\n",
      "Epoch 2 step 568: training accuarcy: 0.777\n",
      "Epoch 2 step 568: training loss: 29353.942767636516\n",
      "Epoch 2 step 569: training accuarcy: 0.7917000000000001\n",
      "Epoch 2 step 569: training loss: 29630.713938903453\n",
      "Epoch 2 step 570: training accuarcy: 0.7907000000000001\n",
      "Epoch 2 step 570: training loss: 30025.39618620897\n",
      "Epoch 2 step 571: training accuarcy: 0.7799\n",
      "Epoch 2 step 571: training loss: 29581.479106219376\n",
      "Epoch 2 step 572: training accuarcy: 0.7871\n",
      "Epoch 2 step 572: training loss: 29331.058177272724\n",
      "Epoch 2 step 573: training accuarcy: 0.7836000000000001\n",
      "Epoch 2 step 573: training loss: 28952.682584222654\n",
      "Epoch 2 step 574: training accuarcy: 0.794\n",
      "Epoch 2 step 574: training loss: 29869.23987908857\n",
      "Epoch 2 step 575: training accuarcy: 0.7812\n",
      "Epoch 2 step 575: training loss: 28782.740972623185\n",
      "Epoch 2 step 576: training accuarcy: 0.7984\n",
      "Epoch 2 step 576: training loss: 29372.46665865988\n",
      "Epoch 2 step 577: training accuarcy: 0.7844\n",
      "Epoch 2 step 577: training loss: 29655.38608143528\n",
      "Epoch 2 step 578: training accuarcy: 0.7782\n",
      "Epoch 2 step 578: training loss: 29136.346873116403\n",
      "Epoch 2 step 579: training accuarcy: 0.7929\n",
      "Epoch 2 step 579: training loss: 29373.066083078123\n",
      "Epoch 2 step 580: training accuarcy: 0.79\n",
      "Epoch 2 step 580: training loss: 29450.0366908692\n",
      "Epoch 2 step 581: training accuarcy: 0.7872\n",
      "Epoch 2 step 581: training loss: 28796.731242783415\n",
      "Epoch 2 step 582: training accuarcy: 0.799\n",
      "Epoch 2 step 582: training loss: 28867.897364721153\n",
      "Epoch 2 step 583: training accuarcy: 0.7946000000000001\n",
      "Epoch 2 step 583: training loss: 29164.56162547939\n",
      "Epoch 2 step 584: training accuarcy: 0.796\n",
      "Epoch 2 step 584: training loss: 29076.93183656131\n",
      "Epoch 2 step 585: training accuarcy: 0.7924\n",
      "Epoch 2 step 585: training loss: 29686.923821919157\n",
      "Epoch 2 step 586: training accuarcy: 0.784\n",
      "Epoch 2 step 586: training loss: 29082.754937043042\n",
      "Epoch 2 step 587: training accuarcy: 0.7888000000000001\n",
      "Epoch 2 step 587: training loss: 29340.841613078155\n",
      "Epoch 2 step 588: training accuarcy: 0.7935\n",
      "Epoch 2 step 588: training loss: 29229.146435051036\n",
      "Epoch 2 step 589: training accuarcy: 0.7862\n",
      "Epoch 2 step 589: training loss: 29498.51622159986\n",
      "Epoch 2 step 590: training accuarcy: 0.785\n",
      "Epoch 2 step 590: training loss: 29319.28209467603\n",
      "Epoch 2 step 591: training accuarcy: 0.7858\n",
      "Epoch 2 step 591: training loss: 29446.521588428015\n",
      "Epoch 2 step 592: training accuarcy: 0.786\n",
      "Epoch 2 step 592: training loss: 29129.682313523223\n",
      "Epoch 2 step 593: training accuarcy: 0.7922\n",
      "Epoch 2 step 593: training loss: 28997.6840747182\n",
      "Epoch 2 step 594: training accuarcy: 0.7939\n",
      "Epoch 2 step 594: training loss: 29279.64938810943\n",
      "Epoch 2 step 595: training accuarcy: 0.7902\n",
      "Epoch 2 step 595: training loss: 29173.638207021082\n",
      "Epoch 2 step 596: training accuarcy: 0.793\n",
      "Epoch 2 step 596: training loss: 29118.857006716564\n",
      "Epoch 2 step 597: training accuarcy: 0.7852\n",
      "Epoch 2 step 597: training loss: 29015.774729309313\n",
      "Epoch 2 step 598: training accuarcy: 0.7934\n",
      "Epoch 2 step 598: training loss: 29567.645075780198\n",
      "Epoch 2 step 599: training accuarcy: 0.7773\n",
      "Epoch 2 step 599: training loss: 29232.299272910543\n",
      "Epoch 2 step 600: training accuarcy: 0.7975\n",
      "Epoch 2 step 600: training loss: 29003.424400830925\n",
      "Epoch 2 step 601: training accuarcy: 0.7904\n",
      "Epoch 2 step 601: training loss: 29416.127496305693\n",
      "Epoch 2 step 602: training accuarcy: 0.7815000000000001\n",
      "Epoch 2 step 602: training loss: 30198.573117435993\n",
      "Epoch 2 step 603: training accuarcy: 0.7757000000000001\n",
      "Epoch 2 step 603: training loss: 28910.245387487594\n",
      "Epoch 2 step 604: training accuarcy: 0.7933\n",
      "Epoch 2 step 604: training loss: 29365.40855316794\n",
      "Epoch 2 step 605: training accuarcy: 0.794\n",
      "Epoch 2 step 605: training loss: 29227.741946519465\n",
      "Epoch 2 step 606: training accuarcy: 0.7886000000000001\n",
      "Epoch 2 step 606: training loss: 29379.316952877296\n",
      "Epoch 2 step 607: training accuarcy: 0.7857000000000001\n",
      "Epoch 2 step 607: training loss: 29160.96117390586\n",
      "Epoch 2 step 608: training accuarcy: 0.7928000000000001\n",
      "Epoch 2 step 608: training loss: 28947.337594862744\n",
      "Epoch 2 step 609: training accuarcy: 0.7956000000000001\n",
      "Epoch 2 step 609: training loss: 29664.6323343107\n",
      "Epoch 2 step 610: training accuarcy: 0.7792\n",
      "Epoch 2 step 610: training loss: 28915.086062798895\n",
      "Epoch 2 step 611: training accuarcy: 0.8003\n",
      "Epoch 2 step 611: training loss: 29135.636845866713\n",
      "Epoch 2 step 612: training accuarcy: 0.7852\n",
      "Epoch 2 step 612: training loss: 29208.651239100323\n",
      "Epoch 2 step 613: training accuarcy: 0.7909\n",
      "Epoch 2 step 613: training loss: 29833.65878955085\n",
      "Epoch 2 step 614: training accuarcy: 0.7789\n",
      "Epoch 2 step 614: training loss: 29102.917230686304\n",
      "Epoch 2 step 615: training accuarcy: 0.7908000000000001\n",
      "Epoch 2 step 615: training loss: 28425.888775653133\n",
      "Epoch 2 step 616: training accuarcy: 0.7987000000000001\n",
      "Epoch 2 step 616: training loss: 28327.93327843025\n",
      "Epoch 2 step 617: training accuarcy: 0.7948000000000001\n",
      "Epoch 2 step 617: training loss: 28958.10608098931\n",
      "Epoch 2 step 618: training accuarcy: 0.7822\n",
      "Epoch 2 step 618: training loss: 29738.384967976854\n",
      "Epoch 2 step 619: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 619: training loss: 28722.463021611256\n",
      "Epoch 2 step 620: training accuarcy: 0.7967000000000001\n",
      "Epoch 2 step 620: training loss: 29221.95531176193\n",
      "Epoch 2 step 621: training accuarcy: 0.7821\n",
      "Epoch 2 step 621: training loss: 29476.394192977397\n",
      "Epoch 2 step 622: training accuarcy: 0.7877000000000001\n",
      "Epoch 2 step 622: training loss: 29016.490063008503\n",
      "Epoch 2 step 623: training accuarcy: 0.7885000000000001\n",
      "Epoch 2 step 623: training loss: 28781.34436586748\n",
      "Epoch 2 step 624: training accuarcy: 0.7884\n",
      "Epoch 2 step 624: training loss: 28916.0131754624\n",
      "Epoch 2 step 625: training accuarcy: 0.794\n",
      "Epoch 2 step 625: training loss: 29160.314973256744\n",
      "Epoch 2 step 626: training accuarcy: 0.7975\n",
      "Epoch 2 step 626: training loss: 28818.147962691575\n",
      "Epoch 2 step 627: training accuarcy: 0.7884\n",
      "Epoch 2 step 627: training loss: 28716.376924122076\n",
      "Epoch 2 step 628: training accuarcy: 0.7961\n",
      "Epoch 2 step 628: training loss: 28766.22747917661\n",
      "Epoch 2 step 629: training accuarcy: 0.7945\n",
      "Epoch 2 step 629: training loss: 29213.087853511377\n",
      "Epoch 2 step 630: training accuarcy: 0.7873\n",
      "Epoch 2 step 630: training loss: 28907.081675391648\n",
      "Epoch 2 step 631: training accuarcy: 0.791\n",
      "Epoch 2 step 631: training loss: 29127.501227305245\n",
      "Epoch 2 step 632: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 632: training loss: 28842.257654887195\n",
      "Epoch 2 step 633: training accuarcy: 0.7936000000000001\n",
      "Epoch 2 step 633: training loss: 29342.00721080652\n",
      "Epoch 2 step 634: training accuarcy: 0.787\n",
      "Epoch 2 step 634: training loss: 29400.292255408476\n",
      "Epoch 2 step 635: training accuarcy: 0.7791\n",
      "Epoch 2 step 635: training loss: 29324.192094900398\n",
      "Epoch 2 step 636: training accuarcy: 0.7821\n",
      "Epoch 2 step 636: training loss: 28425.317610855\n",
      "Epoch 2 step 637: training accuarcy: 0.7937000000000001\n",
      "Epoch 2 step 637: training loss: 28485.661808412973\n",
      "Epoch 2 step 638: training accuarcy: 0.7989\n",
      "Epoch 2 step 638: training loss: 29709.993184483472\n",
      "Epoch 2 step 639: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 639: training loss: 29292.926699562213\n",
      "Epoch 2 step 640: training accuarcy: 0.7804\n",
      "Epoch 2 step 640: training loss: 28592.702662598254\n",
      "Epoch 2 step 641: training accuarcy: 0.7874\n",
      "Epoch 2 step 641: training loss: 28770.376533405375\n",
      "Epoch 2 step 642: training accuarcy: 0.7971\n",
      "Epoch 2 step 642: training loss: 29233.19353683782\n",
      "Epoch 2 step 643: training accuarcy: 0.7878000000000001\n",
      "Epoch 2 step 643: training loss: 29015.81259796406\n",
      "Epoch 2 step 644: training accuarcy: 0.7822\n",
      "Epoch 2 step 644: training loss: 28869.606103327056\n",
      "Epoch 2 step 645: training accuarcy: 0.7912\n",
      "Epoch 2 step 645: training loss: 29010.76686238147\n",
      "Epoch 2 step 646: training accuarcy: 0.7911\n",
      "Epoch 2 step 646: training loss: 28696.670492112557\n",
      "Epoch 2 step 647: training accuarcy: 0.7886000000000001\n",
      "Epoch 2 step 647: training loss: 29258.24042856775\n",
      "Epoch 2 step 648: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 648: training loss: 28825.299702337685\n",
      "Epoch 2 step 649: training accuarcy: 0.79\n",
      "Epoch 2 step 649: training loss: 28822.346318854674\n",
      "Epoch 2 step 650: training accuarcy: 0.7904\n",
      "Epoch 2 step 650: training loss: 28726.977876480447\n",
      "Epoch 2 step 651: training accuarcy: 0.786\n",
      "Epoch 2 step 651: training loss: 28857.97982775903\n",
      "Epoch 2 step 652: training accuarcy: 0.7881\n",
      "Epoch 2 step 652: training loss: 29180.588657922748\n",
      "Epoch 2 step 653: training accuarcy: 0.787\n",
      "Epoch 2 step 653: training loss: 29026.098005211366\n",
      "Epoch 2 step 654: training accuarcy: 0.7874\n",
      "Epoch 2 step 654: training loss: 29290.165117189834\n",
      "Epoch 2 step 655: training accuarcy: 0.7814\n",
      "Epoch 2 step 655: training loss: 29026.87538371847\n",
      "Epoch 2 step 656: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 656: training loss: 28910.585357826618\n",
      "Epoch 2 step 657: training accuarcy: 0.7915000000000001\n",
      "Epoch 2 step 657: training loss: 29172.86361311088\n",
      "Epoch 2 step 658: training accuarcy: 0.783\n",
      "Epoch 2 step 658: training loss: 28764.715714855403\n",
      "Epoch 2 step 659: training accuarcy: 0.7878000000000001\n",
      "Epoch 2 step 659: training loss: 29330.23882392396\n",
      "Epoch 2 step 660: training accuarcy: 0.7821\n",
      "Epoch 2 step 660: training loss: 29749.607941799062\n",
      "Epoch 2 step 661: training accuarcy: 0.7815000000000001\n",
      "Epoch 2 step 661: training loss: 29519.217741520624\n",
      "Epoch 2 step 662: training accuarcy: 0.7825000000000001\n",
      "Epoch 2 step 662: training loss: 28904.82170351783\n",
      "Epoch 2 step 663: training accuarcy: 0.7905000000000001\n",
      "Epoch 2 step 663: training loss: 28561.942706874055\n",
      "Epoch 2 step 664: training accuarcy: 0.7938000000000001\n",
      "Epoch 2 step 664: training loss: 28765.656791978003\n",
      "Epoch 2 step 665: training accuarcy: 0.788\n",
      "Epoch 2 step 665: training loss: 28010.82272213498\n",
      "Epoch 2 step 666: training accuarcy: 0.8053\n",
      "Epoch 2 step 666: training loss: 28910.7291353462\n",
      "Epoch 2 step 667: training accuarcy: 0.7811\n",
      "Epoch 2 step 667: training loss: 28944.978118520907\n",
      "Epoch 2 step 668: training accuarcy: 0.7862\n",
      "Epoch 2 step 668: training loss: 29118.043348197978\n",
      "Epoch 2 step 669: training accuarcy: 0.7857000000000001\n",
      "Epoch 2 step 669: training loss: 29298.551530994388\n",
      "Epoch 2 step 670: training accuarcy: 0.7869\n",
      "Epoch 2 step 670: training loss: 28907.40814821224\n",
      "Epoch 2 step 671: training accuarcy: 0.789\n",
      "Epoch 2 step 671: training loss: 28941.760313708\n",
      "Epoch 2 step 672: training accuarcy: 0.7862\n",
      "Epoch 2 step 672: training loss: 28716.923894131316\n",
      "Epoch 2 step 673: training accuarcy: 0.7851\n",
      "Epoch 2 step 673: training loss: 28695.927300960866\n",
      "Epoch 2 step 674: training accuarcy: 0.7884\n",
      "Epoch 2 step 674: training loss: 28985.700317505532\n",
      "Epoch 2 step 675: training accuarcy: 0.7873\n",
      "Epoch 2 step 675: training loss: 29074.262318602574\n",
      "Epoch 2 step 676: training accuarcy: 0.7816000000000001\n",
      "Epoch 2 step 676: training loss: 29408.116276504803\n",
      "Epoch 2 step 677: training accuarcy: 0.7883\n",
      "Epoch 2 step 677: training loss: 28443.531798151642\n",
      "Epoch 2 step 678: training accuarcy: 0.7957000000000001\n",
      "Epoch 2 step 678: training loss: 29208.307597065756\n",
      "Epoch 2 step 679: training accuarcy: 0.783\n",
      "Epoch 2 step 679: training loss: 28756.604712270433\n",
      "Epoch 2 step 680: training accuarcy: 0.7914\n",
      "Epoch 2 step 680: training loss: 28173.538045535068\n",
      "Epoch 2 step 681: training accuarcy: 0.7958000000000001\n",
      "Epoch 2 step 681: training loss: 28901.94001889949\n",
      "Epoch 2 step 682: training accuarcy: 0.7857000000000001\n",
      "Epoch 2 step 682: training loss: 28609.604604502914\n",
      "Epoch 2 step 683: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 683: training loss: 28740.492513194906\n",
      "Epoch 2 step 684: training accuarcy: 0.7924\n",
      "Epoch 2 step 684: training loss: 28776.13775182021\n",
      "Epoch 2 step 685: training accuarcy: 0.7897000000000001\n",
      "Epoch 2 step 685: training loss: 28806.024417505436\n",
      "Epoch 2 step 686: training accuarcy: 0.782\n",
      "Epoch 2 step 686: training loss: 28778.70471628703\n",
      "Epoch 2 step 687: training accuarcy: 0.786\n",
      "Epoch 2 step 687: training loss: 28706.768993616097\n",
      "Epoch 2 step 688: training accuarcy: 0.7885000000000001\n",
      "Epoch 2 step 688: training loss: 28978.35182889537\n",
      "Epoch 2 step 689: training accuarcy: 0.785\n",
      "Epoch 2 step 689: training loss: 29174.042497994455\n",
      "Epoch 2 step 690: training accuarcy: 0.7857000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 690: training loss: 28686.99754215468\n",
      "Epoch 2 step 691: training accuarcy: 0.7906000000000001\n",
      "Epoch 2 step 691: training loss: 28763.448810794554\n",
      "Epoch 2 step 692: training accuarcy: 0.7953\n",
      "Epoch 2 step 692: training loss: 28734.754986791093\n",
      "Epoch 2 step 693: training accuarcy: 0.7903\n",
      "Epoch 2 step 693: training loss: 28832.191333561503\n",
      "Epoch 2 step 694: training accuarcy: 0.7874\n",
      "Epoch 2 step 694: training loss: 28889.438832097236\n",
      "Epoch 2 step 695: training accuarcy: 0.7891\n",
      "Epoch 2 step 695: training loss: 29524.77223648897\n",
      "Epoch 2 step 696: training accuarcy: 0.7811\n",
      "Epoch 2 step 696: training loss: 28111.733619486877\n",
      "Epoch 2 step 697: training accuarcy: 0.793\n",
      "Epoch 2 step 697: training loss: 28398.059954069533\n",
      "Epoch 2 step 698: training accuarcy: 0.7891\n",
      "Epoch 2 step 698: training loss: 28536.42865893506\n",
      "Epoch 2 step 699: training accuarcy: 0.7935\n",
      "Epoch 2 step 699: training loss: 28458.421778173768\n",
      "Epoch 2 step 700: training accuarcy: 0.793\n",
      "Epoch 2 step 700: training loss: 28537.60089373502\n",
      "Epoch 2 step 701: training accuarcy: 0.7937000000000001\n",
      "Epoch 2 step 701: training loss: 29406.39799079691\n",
      "Epoch 2 step 702: training accuarcy: 0.7796000000000001\n",
      "Epoch 2 step 702: training loss: 28244.85140005411\n",
      "Epoch 2 step 703: training accuarcy: 0.7872\n",
      "Epoch 2 step 703: training loss: 28613.224860770002\n",
      "Epoch 2 step 704: training accuarcy: 0.7896000000000001\n",
      "Epoch 2 step 704: training loss: 28494.7043076548\n",
      "Epoch 2 step 705: training accuarcy: 0.7904\n",
      "Epoch 2 step 705: training loss: 28753.29395191621\n",
      "Epoch 2 step 706: training accuarcy: 0.7877000000000001\n",
      "Epoch 2 step 706: training loss: 28747.68011843908\n",
      "Epoch 2 step 707: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 707: training loss: 28416.392606205714\n",
      "Epoch 2 step 708: training accuarcy: 0.7985\n",
      "Epoch 2 step 708: training loss: 28689.872856565573\n",
      "Epoch 2 step 709: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 709: training loss: 29065.019023617264\n",
      "Epoch 2 step 710: training accuarcy: 0.7837000000000001\n",
      "Epoch 2 step 710: training loss: 28538.237912526663\n",
      "Epoch 2 step 711: training accuarcy: 0.7881\n",
      "Epoch 2 step 711: training loss: 28703.710739735958\n",
      "Epoch 2 step 712: training accuarcy: 0.793\n",
      "Epoch 2 step 712: training loss: 28305.865764865404\n",
      "Epoch 2 step 713: training accuarcy: 0.7959\n",
      "Epoch 2 step 713: training loss: 28239.877661572136\n",
      "Epoch 2 step 714: training accuarcy: 0.7971\n",
      "Epoch 2 step 714: training loss: 28722.112797924332\n",
      "Epoch 2 step 715: training accuarcy: 0.7888000000000001\n",
      "Epoch 2 step 715: training loss: 28229.66508466546\n",
      "Epoch 2 step 716: training accuarcy: 0.7946000000000001\n",
      "Epoch 2 step 716: training loss: 29084.309944818655\n",
      "Epoch 2 step 717: training accuarcy: 0.7919\n",
      "Epoch 2 step 717: training loss: 28653.318551916393\n",
      "Epoch 2 step 718: training accuarcy: 0.7889\n",
      "Epoch 2 step 718: training loss: 28670.12452869297\n",
      "Epoch 2 step 719: training accuarcy: 0.7876000000000001\n",
      "Epoch 2 step 719: training loss: 28704.85693992291\n",
      "Epoch 2 step 720: training accuarcy: 0.793\n",
      "Epoch 2 step 720: training loss: 28082.4735964578\n",
      "Epoch 2 step 721: training accuarcy: 0.7967000000000001\n",
      "Epoch 2 step 721: training loss: 29080.224752089696\n",
      "Epoch 2 step 722: training accuarcy: 0.7835000000000001\n",
      "Epoch 2 step 722: training loss: 29236.913340412353\n",
      "Epoch 2 step 723: training accuarcy: 0.7767000000000001\n",
      "Epoch 2 step 723: training loss: 28016.83503324126\n",
      "Epoch 2 step 724: training accuarcy: 0.8009000000000001\n",
      "Epoch 2 step 724: training loss: 27833.58825382112\n",
      "Epoch 2 step 725: training accuarcy: 0.8081\n",
      "Epoch 2 step 725: training loss: 29522.566540036954\n",
      "Epoch 2 step 726: training accuarcy: 0.781\n",
      "Epoch 2 step 726: training loss: 28020.036665895546\n",
      "Epoch 2 step 727: training accuarcy: 0.8006000000000001\n",
      "Epoch 2 step 727: training loss: 28112.14702061427\n",
      "Epoch 2 step 728: training accuarcy: 0.796\n",
      "Epoch 2 step 728: training loss: 28400.35462296768\n",
      "Epoch 2 step 729: training accuarcy: 0.7912\n",
      "Epoch 2 step 729: training loss: 28289.49292145045\n",
      "Epoch 2 step 730: training accuarcy: 0.7943\n",
      "Epoch 2 step 730: training loss: 28903.264657696676\n",
      "Epoch 2 step 731: training accuarcy: 0.7885000000000001\n",
      "Epoch 2 step 731: training loss: 28570.00273802383\n",
      "Epoch 2 step 732: training accuarcy: 0.7933\n",
      "Epoch 2 step 732: training loss: 28237.546384192756\n",
      "Epoch 2 step 733: training accuarcy: 0.7986000000000001\n",
      "Epoch 2 step 733: training loss: 28653.764064992592\n",
      "Epoch 2 step 734: training accuarcy: 0.7878000000000001\n",
      "Epoch 2 step 734: training loss: 29208.142173871365\n",
      "Epoch 2 step 735: training accuarcy: 0.7802\n",
      "Epoch 2 step 735: training loss: 28462.205706926055\n",
      "Epoch 2 step 736: training accuarcy: 0.7903\n",
      "Epoch 2 step 736: training loss: 28717.26787009107\n",
      "Epoch 2 step 737: training accuarcy: 0.7905000000000001\n",
      "Epoch 2 step 737: training loss: 28250.855104038892\n",
      "Epoch 2 step 738: training accuarcy: 0.7921\n",
      "Epoch 2 step 738: training loss: 28625.17293644898\n",
      "Epoch 2 step 739: training accuarcy: 0.789\n",
      "Epoch 2 step 739: training loss: 27822.09297728845\n",
      "Epoch 2 step 740: training accuarcy: 0.8\n",
      "Epoch 2 step 740: training loss: 28448.44924334123\n",
      "Epoch 2 step 741: training accuarcy: 0.7946000000000001\n",
      "Epoch 2 step 741: training loss: 28633.025583994306\n",
      "Epoch 2 step 742: training accuarcy: 0.7842\n",
      "Epoch 2 step 742: training loss: 28820.487354207704\n",
      "Epoch 2 step 743: training accuarcy: 0.7799\n",
      "Epoch 2 step 743: training loss: 28331.030772055554\n",
      "Epoch 2 step 744: training accuarcy: 0.7921\n",
      "Epoch 2 step 744: training loss: 28267.22536791706\n",
      "Epoch 2 step 745: training accuarcy: 0.7938000000000001\n",
      "Epoch 2 step 745: training loss: 28916.863185374714\n",
      "Epoch 2 step 746: training accuarcy: 0.7847000000000001\n",
      "Epoch 2 step 746: training loss: 28752.091790295246\n",
      "Epoch 2 step 747: training accuarcy: 0.7941\n",
      "Epoch 2 step 747: training loss: 28380.568629773235\n",
      "Epoch 2 step 748: training accuarcy: 0.7889\n",
      "Epoch 2 step 748: training loss: 28576.895844268587\n",
      "Epoch 2 step 749: training accuarcy: 0.7958000000000001\n",
      "Epoch 2 step 749: training loss: 28533.14334117301\n",
      "Epoch 2 step 750: training accuarcy: 0.7914\n",
      "Epoch 2 step 750: training loss: 28079.275060813074\n",
      "Epoch 2 step 751: training accuarcy: 0.7918000000000001\n",
      "Epoch 2 step 751: training loss: 28404.99686032047\n",
      "Epoch 2 step 752: training accuarcy: 0.7947000000000001\n",
      "Epoch 2 step 752: training loss: 29465.537269286975\n",
      "Epoch 2 step 753: training accuarcy: 0.7768\n",
      "Epoch 2 step 753: training loss: 29248.57575842818\n",
      "Epoch 2 step 754: training accuarcy: 0.7791\n",
      "Epoch 2 step 754: training loss: 28629.646279651075\n",
      "Epoch 2 step 755: training accuarcy: 0.7784000000000001\n",
      "Epoch 2 step 755: training loss: 27933.55489072378\n",
      "Epoch 2 step 756: training accuarcy: 0.7972\n",
      "Epoch 2 step 756: training loss: 28145.209405793546\n",
      "Epoch 2 step 757: training accuarcy: 0.7956000000000001\n",
      "Epoch 2 step 757: training loss: 28315.441583048127\n",
      "Epoch 2 step 758: training accuarcy: 0.795\n",
      "Epoch 2 step 758: training loss: 29106.698766068363\n",
      "Epoch 2 step 759: training accuarcy: 0.7811\n",
      "Epoch 2 step 759: training loss: 28111.746772246057\n",
      "Epoch 2 step 760: training accuarcy: 0.8002\n",
      "Epoch 2 step 760: training loss: 28154.785715353675\n",
      "Epoch 2 step 761: training accuarcy: 0.7968000000000001\n",
      "Epoch 2 step 761: training loss: 28754.37701903974\n",
      "Epoch 2 step 762: training accuarcy: 0.7908000000000001\n",
      "Epoch 2 step 762: training loss: 28802.932938167338\n",
      "Epoch 2 step 763: training accuarcy: 0.7864\n",
      "Epoch 2 step 763: training loss: 27910.566708281884\n",
      "Epoch 2 step 764: training accuarcy: 0.8035\n",
      "Epoch 2 step 764: training loss: 28999.19594803131\n",
      "Epoch 2 step 765: training accuarcy: 0.7853\n",
      "Epoch 2 step 765: training loss: 28559.22608996363\n",
      "Epoch 2 step 766: training accuarcy: 0.7857000000000001\n",
      "Epoch 2 step 766: training loss: 28920.636255479953\n",
      "Epoch 2 step 767: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 767: training loss: 28306.182557822194\n",
      "Epoch 2 step 768: training accuarcy: 0.7889\n",
      "Epoch 2 step 768: training loss: 28360.463291460946\n",
      "Epoch 2 step 769: training accuarcy: 0.7963\n",
      "Epoch 2 step 769: training loss: 28206.315582130024\n",
      "Epoch 2 step 770: training accuarcy: 0.7903\n",
      "Epoch 2 step 770: training loss: 28485.172164374606\n",
      "Epoch 2 step 771: training accuarcy: 0.7917000000000001\n",
      "Epoch 2 step 771: training loss: 28941.42721716859\n",
      "Epoch 2 step 772: training accuarcy: 0.7816000000000001\n",
      "Epoch 2 step 772: training loss: 28596.644890944728\n",
      "Epoch 2 step 773: training accuarcy: 0.7939\n",
      "Epoch 2 step 773: training loss: 28627.829615390678\n",
      "Epoch 2 step 774: training accuarcy: 0.783\n",
      "Epoch 2 step 774: training loss: 28547.175131956585\n",
      "Epoch 2 step 775: training accuarcy: 0.7868\n",
      "Epoch 2 step 775: training loss: 28239.733361198018\n",
      "Epoch 2 step 776: training accuarcy: 0.7979\n",
      "Epoch 2 step 776: training loss: 28080.279394928348\n",
      "Epoch 2 step 777: training accuarcy: 0.798\n",
      "Epoch 2 step 777: training loss: 28487.64659201348\n",
      "Epoch 2 step 778: training accuarcy: 0.7872\n",
      "Epoch 2 step 778: training loss: 27957.995743259737\n",
      "Epoch 2 step 779: training accuarcy: 0.7943\n",
      "Epoch 2 step 779: training loss: 28439.689447924073\n",
      "Epoch 2 step 780: training accuarcy: 0.7892\n",
      "Epoch 2 step 780: training loss: 28469.032599272432\n",
      "Epoch 2 step 781: training accuarcy: 0.7901\n",
      "Epoch 2 step 781: training loss: 28173.868287630146\n",
      "Epoch 2 step 782: training accuarcy: 0.7991\n",
      "Epoch 2 step 782: training loss: 28868.702169842538\n",
      "Epoch 2 step 783: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 783: training loss: 28382.502841091624\n",
      "Epoch 2 step 784: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 784: training loss: 28879.712151214775\n",
      "Epoch 2 step 785: training accuarcy: 0.7859\n",
      "Epoch 2 step 785: training loss: 28192.507274841017\n",
      "Epoch 2 step 786: training accuarcy: 0.7903\n",
      "Epoch 2 step 786: training loss: 28242.706637813033\n",
      "Epoch 2 step 787: training accuarcy: 0.7947000000000001\n",
      "Epoch 2 step 787: training loss: 27689.67048530811\n",
      "Epoch 2 step 788: training accuarcy: 0.7976000000000001\n",
      "Epoch 2 step 788: training loss: 13957.441669108577\n",
      "Epoch 2 step 789: training accuarcy: 0.781025641025641\n",
      "Epoch 2: train loss 28876.51624572148, train accuarcy 0.7660428285598755\n",
      "Epoch 2: valid loss 27744.73928404458, valid accuarcy 0.7934712171554565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [14:33<00:00, 289.31s/it]\n"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=3,\n",
    "                 log_dir=get_log_dir('weight_topcoder', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:49:37.401620Z",
     "start_time": "2019-10-17T07:49:37.388626Z"
    }
   },
   "outputs": [],
   "source": [
    "del prme_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Trans FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:49:38.882618Z",
     "start_time": "2019-10-17T07:49:38.404616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTransFM()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model = TorchTransFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "trans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:49:44.289623Z",
     "start_time": "2019-10-17T07:49:44.284618Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(trans_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:49:44.444622Z",
     "start_time": "2019-10-17T07:49:44.371617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x2516d5ff648>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_learner = FMLearner(trans_model, adam_opt, schedular, db)\n",
    "trans_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:49:53.246666Z",
     "start_time": "2019-10-17T07:49:53.243617Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='base',\n",
    "                      valid_col='base',\n",
    "                      test_col='base',\n",
    "                      loss_callback=trans_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:15:44.463502Z",
     "start_time": "2019-10-09T13:15:44.458502Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='seq',\n",
    "                      valid_col='seq',\n",
    "                      test_col='seq',\n",
    "                      loss_callback=trans_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:33:54.030261Z",
     "start_time": "2019-10-09T13:33:54.027294Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='seq',\n",
    "                      valid_col='seq',\n",
    "                      test_col='seq',\n",
    "                      loss_callback=trans_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:56:22.016531Z",
     "start_time": "2019-10-17T07:50:17.540618Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 46964.64759320144\n",
      "Epoch 0 step 1: training accuarcy: 0.5305\n",
      "Epoch 0 step 1: training loss: 46303.860487355196\n",
      "Epoch 0 step 2: training accuarcy: 0.52\n",
      "Epoch 0 step 2: training loss: 44911.01438392426\n",
      "Epoch 0 step 3: training accuarcy: 0.5285\n",
      "Epoch 0 step 3: training loss: 43899.87006394936\n",
      "Epoch 0 step 4: training accuarcy: 0.5215\n",
      "Epoch 0 step 4: training loss: 42597.08275518355\n",
      "Epoch 0 step 5: training accuarcy: 0.532\n",
      "Epoch 0 step 5: training loss: 41179.186404459506\n",
      "Epoch 0 step 6: training accuarcy: 0.5465\n",
      "Epoch 0 step 6: training loss: 40095.95313489541\n",
      "Epoch 0 step 7: training accuarcy: 0.5305\n",
      "Epoch 0 step 7: training loss: 39139.03911682185\n",
      "Epoch 0 step 8: training accuarcy: 0.5325\n",
      "Epoch 0 step 8: training loss: 38596.50010456356\n",
      "Epoch 0 step 9: training accuarcy: 0.5225\n",
      "Epoch 0 step 9: training loss: 36910.07042390128\n",
      "Epoch 0 step 10: training accuarcy: 0.5495\n",
      "Epoch 0 step 10: training loss: 36128.559668267524\n",
      "Epoch 0 step 11: training accuarcy: 0.5375\n",
      "Epoch 0 step 11: training loss: 34970.390062573555\n",
      "Epoch 0 step 12: training accuarcy: 0.5405\n",
      "Epoch 0 step 12: training loss: 34108.141109295495\n",
      "Epoch 0 step 13: training accuarcy: 0.533\n",
      "Epoch 0 step 13: training loss: 33329.74934047979\n",
      "Epoch 0 step 14: training accuarcy: 0.535\n",
      "Epoch 0 step 14: training loss: 31959.172864146396\n",
      "Epoch 0 step 15: training accuarcy: 0.5395\n",
      "Epoch 0 step 15: training loss: 31116.424016202345\n",
      "Epoch 0 step 16: training accuarcy: 0.5355\n",
      "Epoch 0 step 16: training loss: 30408.21959282276\n",
      "Epoch 0 step 17: training accuarcy: 0.5165\n",
      "Epoch 0 step 17: training loss: 28869.941609683927\n",
      "Epoch 0 step 18: training accuarcy: 0.5595\n",
      "Epoch 0 step 18: training loss: 28543.178167498478\n",
      "Epoch 0 step 19: training accuarcy: 0.549\n",
      "Epoch 0 step 19: training loss: 27284.757139102418\n",
      "Epoch 0 step 20: training accuarcy: 0.5575\n",
      "Epoch 0 step 20: training loss: 26472.13910235803\n",
      "Epoch 0 step 21: training accuarcy: 0.5455\n",
      "Epoch 0 step 21: training loss: 25584.01569009057\n",
      "Epoch 0 step 22: training accuarcy: 0.547\n",
      "Epoch 0 step 22: training loss: 25487.12999555759\n",
      "Epoch 0 step 23: training accuarcy: 0.5315\n",
      "Epoch 0 step 23: training loss: 24238.622403733156\n",
      "Epoch 0 step 24: training accuarcy: 0.5535\n",
      "Epoch 0 step 24: training loss: 23171.554247940272\n",
      "Epoch 0 step 25: training accuarcy: 0.5595\n",
      "Epoch 0 step 25: training loss: 22508.894058636444\n",
      "Epoch 0 step 26: training accuarcy: 0.543\n",
      "Epoch 0 step 26: training loss: 22071.776939670464\n",
      "Epoch 0 step 27: training accuarcy: 0.542\n",
      "Epoch 0 step 27: training loss: 21102.931338832066\n",
      "Epoch 0 step 28: training accuarcy: 0.535\n",
      "Epoch 0 step 28: training loss: 20433.34745266992\n",
      "Epoch 0 step 29: training accuarcy: 0.546\n",
      "Epoch 0 step 29: training loss: 19930.39623623586\n",
      "Epoch 0 step 30: training accuarcy: 0.555\n",
      "Epoch 0 step 30: training loss: 19145.901653536366\n",
      "Epoch 0 step 31: training accuarcy: 0.5660000000000001\n",
      "Epoch 0 step 31: training loss: 18651.452828617716\n",
      "Epoch 0 step 32: training accuarcy: 0.556\n",
      "Epoch 0 step 32: training loss: 17790.225205355433\n",
      "Epoch 0 step 33: training accuarcy: 0.5645\n",
      "Epoch 0 step 33: training loss: 17467.63920198714\n",
      "Epoch 0 step 34: training accuarcy: 0.54\n",
      "Epoch 0 step 34: training loss: 16846.19658277395\n",
      "Epoch 0 step 35: training accuarcy: 0.5505\n",
      "Epoch 0 step 35: training loss: 16262.196428970225\n",
      "Epoch 0 step 36: training accuarcy: 0.561\n",
      "Epoch 0 step 36: training loss: 15997.69762683437\n",
      "Epoch 0 step 37: training accuarcy: 0.536\n",
      "Epoch 0 step 37: training loss: 15271.537910734947\n",
      "Epoch 0 step 38: training accuarcy: 0.543\n",
      "Epoch 0 step 38: training loss: 14479.987036849392\n",
      "Epoch 0 step 39: training accuarcy: 0.5635\n",
      "Epoch 0 step 39: training loss: 14095.612540420185\n",
      "Epoch 0 step 40: training accuarcy: 0.5495\n",
      "Epoch 0 step 40: training loss: 13520.133907018044\n",
      "Epoch 0 step 41: training accuarcy: 0.5565\n",
      "Epoch 0 step 41: training loss: 13248.755274536412\n",
      "Epoch 0 step 42: training accuarcy: 0.544\n",
      "Epoch 0 step 42: training loss: 12688.401167870787\n",
      "Epoch 0 step 43: training accuarcy: 0.5710000000000001\n",
      "Epoch 0 step 43: training loss: 12286.913578966658\n",
      "Epoch 0 step 44: training accuarcy: 0.5735\n",
      "Epoch 0 step 44: training loss: 11856.23713624732\n",
      "Epoch 0 step 45: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 45: training loss: 11501.004821621125\n",
      "Epoch 0 step 46: training accuarcy: 0.542\n",
      "Epoch 0 step 46: training loss: 11023.097156773772\n",
      "Epoch 0 step 47: training accuarcy: 0.5775\n",
      "Epoch 0 step 47: training loss: 10607.971893478069\n",
      "Epoch 0 step 48: training accuarcy: 0.5695\n",
      "Epoch 0 step 48: training loss: 10247.07219540918\n",
      "Epoch 0 step 49: training accuarcy: 0.588\n",
      "Epoch 0 step 49: training loss: 9930.985899125779\n",
      "Epoch 0 step 50: training accuarcy: 0.584\n",
      "Epoch 0 step 50: training loss: 9647.186928685303\n",
      "Epoch 0 step 51: training accuarcy: 0.5740000000000001\n",
      "Epoch 0 step 51: training loss: 9344.599668400046\n",
      "Epoch 0 step 52: training accuarcy: 0.5700000000000001\n",
      "Epoch 0 step 52: training loss: 8854.225848069967\n",
      "Epoch 0 step 53: training accuarcy: 0.593\n",
      "Epoch 0 step 53: training loss: 8748.05645029362\n",
      "Epoch 0 step 54: training accuarcy: 0.5755\n",
      "Epoch 0 step 54: training loss: 8391.83029166028\n",
      "Epoch 0 step 55: training accuarcy: 0.5875\n",
      "Epoch 0 step 55: training loss: 8022.483259219181\n",
      "Epoch 0 step 56: training accuarcy: 0.601\n",
      "Epoch 0 step 56: training loss: 7877.0152380448835\n",
      "Epoch 0 step 57: training accuarcy: 0.585\n",
      "Epoch 0 step 57: training loss: 7565.553387119942\n",
      "Epoch 0 step 58: training accuarcy: 0.6045\n",
      "Epoch 0 step 58: training loss: 7407.599259019824\n",
      "Epoch 0 step 59: training accuarcy: 0.5975\n",
      "Epoch 0 step 59: training loss: 7169.663599012199\n",
      "Epoch 0 step 60: training accuarcy: 0.587\n",
      "Epoch 0 step 60: training loss: 6926.2943227667565\n",
      "Epoch 0 step 61: training accuarcy: 0.6035\n",
      "Epoch 0 step 61: training loss: 6663.150100084757\n",
      "Epoch 0 step 62: training accuarcy: 0.6125\n",
      "Epoch 0 step 62: training loss: 6539.086605371591\n",
      "Epoch 0 step 63: training accuarcy: 0.61\n",
      "Epoch 0 step 63: training loss: 6246.178862531502\n",
      "Epoch 0 step 64: training accuarcy: 0.6405\n",
      "Epoch 0 step 64: training loss: 6076.414024486457\n",
      "Epoch 0 step 65: training accuarcy: 0.629\n",
      "Epoch 0 step 65: training loss: 5877.848225185953\n",
      "Epoch 0 step 66: training accuarcy: 0.637\n",
      "Epoch 0 step 66: training loss: 5710.450125557911\n",
      "Epoch 0 step 67: training accuarcy: 0.6435\n",
      "Epoch 0 step 67: training loss: 5551.029995670427\n",
      "Epoch 0 step 68: training accuarcy: 0.6475\n",
      "Epoch 0 step 68: training loss: 5406.642256294923\n",
      "Epoch 0 step 69: training accuarcy: 0.6395000000000001\n",
      "Epoch 0 step 69: training loss: 5299.017273974434\n",
      "Epoch 0 step 70: training accuarcy: 0.6395000000000001\n",
      "Epoch 0 step 70: training loss: 5093.1014478449215\n",
      "Epoch 0 step 71: training accuarcy: 0.6525\n",
      "Epoch 0 step 71: training loss: 4895.735297965395\n",
      "Epoch 0 step 72: training accuarcy: 0.674\n",
      "Epoch 0 step 72: training loss: 4772.532967312354\n",
      "Epoch 0 step 73: training accuarcy: 0.6645\n",
      "Epoch 0 step 73: training loss: 4679.1869574546245\n",
      "Epoch 0 step 74: training accuarcy: 0.657\n",
      "Epoch 0 step 74: training loss: 4592.462527431325\n",
      "Epoch 0 step 75: training accuarcy: 0.648\n",
      "Epoch 0 step 75: training loss: 4438.146602084074\n",
      "Epoch 0 step 76: training accuarcy: 0.668\n",
      "Epoch 0 step 76: training loss: 4338.364483161578\n",
      "Epoch 0 step 77: training accuarcy: 0.6755\n",
      "Epoch 0 step 77: training loss: 4180.074568412616\n",
      "Epoch 0 step 78: training accuarcy: 0.675\n",
      "Epoch 0 step 78: training loss: 4064.424681523403\n",
      "Epoch 0 step 79: training accuarcy: 0.68\n",
      "Epoch 0 step 79: training loss: 3912.5770838000817\n",
      "Epoch 0 step 80: training accuarcy: 0.7035\n",
      "Epoch 0 step 80: training loss: 3797.0908265441626\n",
      "Epoch 0 step 81: training accuarcy: 0.7075\n",
      "Epoch 0 step 81: training loss: 3723.0744652366648\n",
      "Epoch 0 step 82: training accuarcy: 0.6925\n",
      "Epoch 0 step 82: training loss: 3596.9846306047543\n",
      "Epoch 0 step 83: training accuarcy: 0.6930000000000001\n",
      "Epoch 0 step 83: training loss: 3518.2969524107593\n",
      "Epoch 0 step 84: training accuarcy: 0.7030000000000001\n",
      "Epoch 0 step 84: training loss: 3369.4553982127263\n",
      "Epoch 0 step 85: training accuarcy: 0.7305\n",
      "Epoch 0 step 85: training loss: 3385.632418066367\n",
      "Epoch 0 step 86: training accuarcy: 0.7085\n",
      "Epoch 0 step 86: training loss: 3213.7664203577597\n",
      "Epoch 0 step 87: training accuarcy: 0.726\n",
      "Epoch 0 step 87: training loss: 3160.4229470591554\n",
      "Epoch 0 step 88: training accuarcy: 0.717\n",
      "Epoch 0 step 88: training loss: 3117.264732196528\n",
      "Epoch 0 step 89: training accuarcy: 0.709\n",
      "Epoch 0 step 89: training loss: 2998.410529491168\n",
      "Epoch 0 step 90: training accuarcy: 0.7245\n",
      "Epoch 0 step 90: training loss: 3004.6409157456264\n",
      "Epoch 0 step 91: training accuarcy: 0.718\n",
      "Epoch 0 step 91: training loss: 2875.456913770951\n",
      "Epoch 0 step 92: training accuarcy: 0.736\n",
      "Epoch 0 step 92: training loss: 2841.757050065252\n",
      "Epoch 0 step 93: training accuarcy: 0.7245\n",
      "Epoch 0 step 93: training loss: 2779.731240459945\n",
      "Epoch 0 step 94: training accuarcy: 0.7245\n",
      "Epoch 0 step 94: training loss: 2732.866948846052\n",
      "Epoch 0 step 95: training accuarcy: 0.712\n",
      "Epoch 0 step 95: training loss: 2682.235547828306\n",
      "Epoch 0 step 96: training accuarcy: 0.7235\n",
      "Epoch 0 step 96: training loss: 2603.8249550463333\n",
      "Epoch 0 step 97: training accuarcy: 0.722\n",
      "Epoch 0 step 97: training loss: 2526.945241143732\n",
      "Epoch 0 step 98: training accuarcy: 0.736\n",
      "Epoch 0 step 98: training loss: 2509.7213898220693\n",
      "Epoch 0 step 99: training accuarcy: 0.7265\n",
      "Epoch 0 step 99: training loss: 2426.685330474971\n",
      "Epoch 0 step 100: training accuarcy: 0.735\n",
      "Epoch 0 step 100: training loss: 2321.322208536013\n",
      "Epoch 0 step 101: training accuarcy: 0.7545000000000001\n",
      "Epoch 0 step 101: training loss: 2304.1805771492664\n",
      "Epoch 0 step 102: training accuarcy: 0.736\n",
      "Epoch 0 step 102: training loss: 2280.314045025322\n",
      "Epoch 0 step 103: training accuarcy: 0.7475\n",
      "Epoch 0 step 103: training loss: 2217.2781419662597\n",
      "Epoch 0 step 104: training accuarcy: 0.741\n",
      "Epoch 0 step 104: training loss: 2162.4412520781652\n",
      "Epoch 0 step 105: training accuarcy: 0.752\n",
      "Epoch 0 step 105: training loss: 2128.06101543453\n",
      "Epoch 0 step 106: training accuarcy: 0.7455\n",
      "Epoch 0 step 106: training loss: 2098.300250885421\n",
      "Epoch 0 step 107: training accuarcy: 0.74\n",
      "Epoch 0 step 107: training loss: 2033.5821721299067\n",
      "Epoch 0 step 108: training accuarcy: 0.759\n",
      "Epoch 0 step 108: training loss: 1962.2148158655496\n",
      "Epoch 0 step 109: training accuarcy: 0.7695\n",
      "Epoch 0 step 109: training loss: 1975.2139501941242\n",
      "Epoch 0 step 110: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 110: training loss: 1912.0032317771713\n",
      "Epoch 0 step 111: training accuarcy: 0.7585000000000001\n",
      "Epoch 0 step 111: training loss: 1889.181466103907\n",
      "Epoch 0 step 112: training accuarcy: 0.7695\n",
      "Epoch 0 step 112: training loss: 1875.4089847609505\n",
      "Epoch 0 step 113: training accuarcy: 0.765\n",
      "Epoch 0 step 113: training loss: 1836.744764753596\n",
      "Epoch 0 step 114: training accuarcy: 0.758\n",
      "Epoch 0 step 114: training loss: 1811.0552822721984\n",
      "Epoch 0 step 115: training accuarcy: 0.768\n",
      "Epoch 0 step 115: training loss: 1730.397698105538\n",
      "Epoch 0 step 116: training accuarcy: 0.78\n",
      "Epoch 0 step 116: training loss: 1747.5518120174384\n",
      "Epoch 0 step 117: training accuarcy: 0.768\n",
      "Epoch 0 step 117: training loss: 1691.926852668374\n",
      "Epoch 0 step 118: training accuarcy: 0.771\n",
      "Epoch 0 step 118: training loss: 1660.5303762516687\n",
      "Epoch 0 step 119: training accuarcy: 0.774\n",
      "Epoch 0 step 119: training loss: 1652.3693087532274\n",
      "Epoch 0 step 120: training accuarcy: 0.773\n",
      "Epoch 0 step 120: training loss: 1600.958050589955\n",
      "Epoch 0 step 121: training accuarcy: 0.7925\n",
      "Epoch 0 step 121: training loss: 1614.436349933014\n",
      "Epoch 0 step 122: training accuarcy: 0.7745\n",
      "Epoch 0 step 122: training loss: 1590.1626070401944\n",
      "Epoch 0 step 123: training accuarcy: 0.7725\n",
      "Epoch 0 step 123: training loss: 1550.678050869467\n",
      "Epoch 0 step 124: training accuarcy: 0.776\n",
      "Epoch 0 step 124: training loss: 1540.2067226886234\n",
      "Epoch 0 step 125: training accuarcy: 0.784\n",
      "Epoch 0 step 125: training loss: 1482.8752907203316\n",
      "Epoch 0 step 126: training accuarcy: 0.7915\n",
      "Epoch 0 step 126: training loss: 1489.9195441306993\n",
      "Epoch 0 step 127: training accuarcy: 0.7845\n",
      "Epoch 0 step 127: training loss: 1446.3755154825035\n",
      "Epoch 0 step 128: training accuarcy: 0.788\n",
      "Epoch 0 step 128: training loss: 1480.7225681398938\n",
      "Epoch 0 step 129: training accuarcy: 0.7785\n",
      "Epoch 0 step 129: training loss: 1442.2909185936446\n",
      "Epoch 0 step 130: training accuarcy: 0.78\n",
      "Epoch 0 step 130: training loss: 1386.730748242576\n",
      "Epoch 0 step 131: training accuarcy: 0.797\n",
      "Epoch 0 step 131: training loss: 1372.629535628852\n",
      "Epoch 0 step 132: training accuarcy: 0.8035\n",
      "Epoch 0 step 132: training loss: 1360.8622693322395\n",
      "Epoch 0 step 133: training accuarcy: 0.7955\n",
      "Epoch 0 step 133: training loss: 1351.4373946812766\n",
      "Epoch 0 step 134: training accuarcy: 0.795\n",
      "Epoch 0 step 134: training loss: 1339.6944425126712\n",
      "Epoch 0 step 135: training accuarcy: 0.7935\n",
      "Epoch 0 step 135: training loss: 1358.9217952422987\n",
      "Epoch 0 step 136: training accuarcy: 0.786\n",
      "Epoch 0 step 136: training loss: 1291.2425097914665\n",
      "Epoch 0 step 137: training accuarcy: 0.8035\n",
      "Epoch 0 step 137: training loss: 1311.632376543666\n",
      "Epoch 0 step 138: training accuarcy: 0.7995\n",
      "Epoch 0 step 138: training loss: 1247.5464470677812\n",
      "Epoch 0 step 139: training accuarcy: 0.8095\n",
      "Epoch 0 step 139: training loss: 1276.1491933430425\n",
      "Epoch 0 step 140: training accuarcy: 0.7965\n",
      "Epoch 0 step 140: training loss: 1294.0344591091184\n",
      "Epoch 0 step 141: training accuarcy: 0.7905\n",
      "Epoch 0 step 141: training loss: 1238.6392743017345\n",
      "Epoch 0 step 142: training accuarcy: 0.8005\n",
      "Epoch 0 step 142: training loss: 1249.3648068862383\n",
      "Epoch 0 step 143: training accuarcy: 0.799\n",
      "Epoch 0 step 143: training loss: 1205.7409367647467\n",
      "Epoch 0 step 144: training accuarcy: 0.8055\n",
      "Epoch 0 step 144: training loss: 1196.7215297642101\n",
      "Epoch 0 step 145: training accuarcy: 0.8130000000000001\n",
      "Epoch 0 step 145: training loss: 1219.5567977090484\n",
      "Epoch 0 step 146: training accuarcy: 0.8\n",
      "Epoch 0 step 146: training loss: 1180.4766018989403\n",
      "Epoch 0 step 147: training accuarcy: 0.7945\n",
      "Epoch 0 step 147: training loss: 1147.1743325162577\n",
      "Epoch 0 step 148: training accuarcy: 0.8180000000000001\n",
      "Epoch 0 step 148: training loss: 1202.648944873486\n",
      "Epoch 0 step 149: training accuarcy: 0.7905\n",
      "Epoch 0 step 149: training loss: 1147.036674461252\n",
      "Epoch 0 step 150: training accuarcy: 0.8145\n",
      "Epoch 0 step 150: training loss: 1103.016319266696\n",
      "Epoch 0 step 151: training accuarcy: 0.8270000000000001\n",
      "Epoch 0 step 151: training loss: 1140.2330346563228\n",
      "Epoch 0 step 152: training accuarcy: 0.8105\n",
      "Epoch 0 step 152: training loss: 1126.738097691433\n",
      "Epoch 0 step 153: training accuarcy: 0.8155\n",
      "Epoch 0 step 153: training loss: 1125.234364554692\n",
      "Epoch 0 step 154: training accuarcy: 0.8195\n",
      "Epoch 0 step 154: training loss: 1115.282593401732\n",
      "Epoch 0 step 155: training accuarcy: 0.8115\n",
      "Epoch 0 step 155: training loss: 1130.895834980534\n",
      "Epoch 0 step 156: training accuarcy: 0.806\n",
      "Epoch 0 step 156: training loss: 1042.6660891445463\n",
      "Epoch 0 step 157: training accuarcy: 0.8395\n",
      "Epoch 0 step 157: training loss: 1096.415708966487\n",
      "Epoch 0 step 158: training accuarcy: 0.8255\n",
      "Epoch 0 step 158: training loss: 1061.4048179524939\n",
      "Epoch 0 step 159: training accuarcy: 0.8270000000000001\n",
      "Epoch 0 step 159: training loss: 1075.4334683313848\n",
      "Epoch 0 step 160: training accuarcy: 0.8175\n",
      "Epoch 0 step 160: training loss: 1072.6568955691957\n",
      "Epoch 0 step 161: training accuarcy: 0.8145\n",
      "Epoch 0 step 161: training loss: 1031.3572887458681\n",
      "Epoch 0 step 162: training accuarcy: 0.8220000000000001\n",
      "Epoch 0 step 162: training loss: 1027.2279093895695\n",
      "Epoch 0 step 163: training accuarcy: 0.8260000000000001\n",
      "Epoch 0 step 163: training loss: 1024.693992507748\n",
      "Epoch 0 step 164: training accuarcy: 0.8285\n",
      "Epoch 0 step 164: training loss: 1031.2657763929833\n",
      "Epoch 0 step 165: training accuarcy: 0.8250000000000001\n",
      "Epoch 0 step 165: training loss: 1008.4440502608841\n",
      "Epoch 0 step 166: training accuarcy: 0.8280000000000001\n",
      "Epoch 0 step 166: training loss: 1038.8120047283853\n",
      "Epoch 0 step 167: training accuarcy: 0.8255\n",
      "Epoch 0 step 167: training loss: 1005.2768485203865\n",
      "Epoch 0 step 168: training accuarcy: 0.8355\n",
      "Epoch 0 step 168: training loss: 1000.941417850417\n",
      "Epoch 0 step 169: training accuarcy: 0.8300000000000001\n",
      "Epoch 0 step 169: training loss: 1004.2679794225114\n",
      "Epoch 0 step 170: training accuarcy: 0.8300000000000001\n",
      "Epoch 0 step 170: training loss: 988.9688383991789\n",
      "Epoch 0 step 171: training accuarcy: 0.8345\n",
      "Epoch 0 step 171: training loss: 993.3625645971783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 172: training accuarcy: 0.8305\n",
      "Epoch 0 step 172: training loss: 992.0807289740117\n",
      "Epoch 0 step 173: training accuarcy: 0.8315\n",
      "Epoch 0 step 173: training loss: 968.7280818279737\n",
      "Epoch 0 step 174: training accuarcy: 0.8365\n",
      "Epoch 0 step 174: training loss: 961.1830040256607\n",
      "Epoch 0 step 175: training accuarcy: 0.8425\n",
      "Epoch 0 step 175: training loss: 951.0140695935105\n",
      "Epoch 0 step 176: training accuarcy: 0.8445\n",
      "Epoch 0 step 176: training loss: 982.992963441875\n",
      "Epoch 0 step 177: training accuarcy: 0.833\n",
      "Epoch 0 step 177: training loss: 954.1620598194033\n",
      "Epoch 0 step 178: training accuarcy: 0.841\n",
      "Epoch 0 step 178: training loss: 921.3220892434172\n",
      "Epoch 0 step 179: training accuarcy: 0.8415\n",
      "Epoch 0 step 179: training loss: 940.4602866947916\n",
      "Epoch 0 step 180: training accuarcy: 0.841\n",
      "Epoch 0 step 180: training loss: 936.1212626623777\n",
      "Epoch 0 step 181: training accuarcy: 0.846\n",
      "Epoch 0 step 181: training loss: 905.4779193482661\n",
      "Epoch 0 step 182: training accuarcy: 0.8415\n",
      "Epoch 0 step 182: training loss: 906.407268979292\n",
      "Epoch 0 step 183: training accuarcy: 0.849\n",
      "Epoch 0 step 183: training loss: 955.7819582680985\n",
      "Epoch 0 step 184: training accuarcy: 0.84\n",
      "Epoch 0 step 184: training loss: 905.89139869477\n",
      "Epoch 0 step 185: training accuarcy: 0.8435\n",
      "Epoch 0 step 185: training loss: 860.6100474958092\n",
      "Epoch 0 step 186: training accuarcy: 0.851\n",
      "Epoch 0 step 186: training loss: 902.627523832891\n",
      "Epoch 0 step 187: training accuarcy: 0.8465\n",
      "Epoch 0 step 187: training loss: 905.808736678817\n",
      "Epoch 0 step 188: training accuarcy: 0.8535\n",
      "Epoch 0 step 188: training loss: 900.0073994387732\n",
      "Epoch 0 step 189: training accuarcy: 0.8515\n",
      "Epoch 0 step 189: training loss: 882.0701212676441\n",
      "Epoch 0 step 190: training accuarcy: 0.8365\n",
      "Epoch 0 step 190: training loss: 910.0416518695674\n",
      "Epoch 0 step 191: training accuarcy: 0.8465\n",
      "Epoch 0 step 191: training loss: 917.6420117201469\n",
      "Epoch 0 step 192: training accuarcy: 0.838\n",
      "Epoch 0 step 192: training loss: 868.3754916943781\n",
      "Epoch 0 step 193: training accuarcy: 0.855\n",
      "Epoch 0 step 193: training loss: 900.7110844510308\n",
      "Epoch 0 step 194: training accuarcy: 0.842\n",
      "Epoch 0 step 194: training loss: 872.8584850500235\n",
      "Epoch 0 step 195: training accuarcy: 0.844\n",
      "Epoch 0 step 195: training loss: 885.2892730923395\n",
      "Epoch 0 step 196: training accuarcy: 0.8475\n",
      "Epoch 0 step 196: training loss: 895.7163274372151\n",
      "Epoch 0 step 197: training accuarcy: 0.8565\n",
      "Epoch 0 step 197: training loss: 868.4294931259574\n",
      "Epoch 0 step 198: training accuarcy: 0.858\n",
      "Epoch 0 step 198: training loss: 895.9072593365021\n",
      "Epoch 0 step 199: training accuarcy: 0.8395\n",
      "Epoch 0 step 199: training loss: 877.5629458571357\n",
      "Epoch 0 step 200: training accuarcy: 0.8505\n",
      "Epoch 0 step 200: training loss: 892.311696660701\n",
      "Epoch 0 step 201: training accuarcy: 0.8415\n",
      "Epoch 0 step 201: training loss: 837.2404059875612\n",
      "Epoch 0 step 202: training accuarcy: 0.8645\n",
      "Epoch 0 step 202: training loss: 838.5631114948557\n",
      "Epoch 0 step 203: training accuarcy: 0.8535\n",
      "Epoch 0 step 203: training loss: 867.2953296351835\n",
      "Epoch 0 step 204: training accuarcy: 0.8485\n",
      "Epoch 0 step 204: training loss: 902.0036484200707\n",
      "Epoch 0 step 205: training accuarcy: 0.8385\n",
      "Epoch 0 step 205: training loss: 850.9941111161618\n",
      "Epoch 0 step 206: training accuarcy: 0.8535\n",
      "Epoch 0 step 206: training loss: 894.8350334543303\n",
      "Epoch 0 step 207: training accuarcy: 0.8465\n",
      "Epoch 0 step 207: training loss: 863.0155219103688\n",
      "Epoch 0 step 208: training accuarcy: 0.8515\n",
      "Epoch 0 step 208: training loss: 826.4444759190783\n",
      "Epoch 0 step 209: training accuarcy: 0.8555\n",
      "Epoch 0 step 209: training loss: 804.5614646786689\n",
      "Epoch 0 step 210: training accuarcy: 0.8615\n",
      "Epoch 0 step 210: training loss: 858.8430895957083\n",
      "Epoch 0 step 211: training accuarcy: 0.857\n",
      "Epoch 0 step 211: training loss: 850.7584539006766\n",
      "Epoch 0 step 212: training accuarcy: 0.853\n",
      "Epoch 0 step 212: training loss: 841.2161660176108\n",
      "Epoch 0 step 213: training accuarcy: 0.8575\n",
      "Epoch 0 step 213: training loss: 878.3438140006322\n",
      "Epoch 0 step 214: training accuarcy: 0.8445\n",
      "Epoch 0 step 214: training loss: 822.4275172518057\n",
      "Epoch 0 step 215: training accuarcy: 0.867\n",
      "Epoch 0 step 215: training loss: 831.6489178859634\n",
      "Epoch 0 step 216: training accuarcy: 0.858\n",
      "Epoch 0 step 216: training loss: 807.5515074548107\n",
      "Epoch 0 step 217: training accuarcy: 0.869\n",
      "Epoch 0 step 217: training loss: 834.9167507190491\n",
      "Epoch 0 step 218: training accuarcy: 0.8525\n",
      "Epoch 0 step 218: training loss: 843.3862095820286\n",
      "Epoch 0 step 219: training accuarcy: 0.8515\n",
      "Epoch 0 step 219: training loss: 817.4400743838653\n",
      "Epoch 0 step 220: training accuarcy: 0.857\n",
      "Epoch 0 step 220: training loss: 864.8106506822921\n",
      "Epoch 0 step 221: training accuarcy: 0.844\n",
      "Epoch 0 step 221: training loss: 821.5581655669671\n",
      "Epoch 0 step 222: training accuarcy: 0.857\n",
      "Epoch 0 step 222: training loss: 825.212604711618\n",
      "Epoch 0 step 223: training accuarcy: 0.8635\n",
      "Epoch 0 step 223: training loss: 803.3220540252071\n",
      "Epoch 0 step 224: training accuarcy: 0.8645\n",
      "Epoch 0 step 224: training loss: 831.1494237824794\n",
      "Epoch 0 step 225: training accuarcy: 0.8665\n",
      "Epoch 0 step 225: training loss: 796.5333377307991\n",
      "Epoch 0 step 226: training accuarcy: 0.8625\n",
      "Epoch 0 step 226: training loss: 783.9487311816854\n",
      "Epoch 0 step 227: training accuarcy: 0.866\n",
      "Epoch 0 step 227: training loss: 827.3750606997327\n",
      "Epoch 0 step 228: training accuarcy: 0.849\n",
      "Epoch 0 step 228: training loss: 818.576042142455\n",
      "Epoch 0 step 229: training accuarcy: 0.8655\n",
      "Epoch 0 step 229: training loss: 802.6370727294301\n",
      "Epoch 0 step 230: training accuarcy: 0.8595\n",
      "Epoch 0 step 230: training loss: 767.7989048072808\n",
      "Epoch 0 step 231: training accuarcy: 0.884\n",
      "Epoch 0 step 231: training loss: 770.7215744195412\n",
      "Epoch 0 step 232: training accuarcy: 0.871\n",
      "Epoch 0 step 232: training loss: 798.0484495655367\n",
      "Epoch 0 step 233: training accuarcy: 0.866\n",
      "Epoch 0 step 233: training loss: 765.8790761556604\n",
      "Epoch 0 step 234: training accuarcy: 0.8775000000000001\n",
      "Epoch 0 step 234: training loss: 779.8933744108633\n",
      "Epoch 0 step 235: training accuarcy: 0.8745\n",
      "Epoch 0 step 235: training loss: 755.9549210769909\n",
      "Epoch 0 step 236: training accuarcy: 0.877\n",
      "Epoch 0 step 236: training loss: 822.9752105617854\n",
      "Epoch 0 step 237: training accuarcy: 0.856\n",
      "Epoch 0 step 237: training loss: 797.1083915996572\n",
      "Epoch 0 step 238: training accuarcy: 0.8695\n",
      "Epoch 0 step 238: training loss: 787.0864854345642\n",
      "Epoch 0 step 239: training accuarcy: 0.8725\n",
      "Epoch 0 step 239: training loss: 783.3148959225105\n",
      "Epoch 0 step 240: training accuarcy: 0.8735\n",
      "Epoch 0 step 240: training loss: 758.1192427768365\n",
      "Epoch 0 step 241: training accuarcy: 0.8745\n",
      "Epoch 0 step 241: training loss: 785.3770520063692\n",
      "Epoch 0 step 242: training accuarcy: 0.8645\n",
      "Epoch 0 step 242: training loss: 793.2068987761324\n",
      "Epoch 0 step 243: training accuarcy: 0.862\n",
      "Epoch 0 step 243: training loss: 731.4936281681116\n",
      "Epoch 0 step 244: training accuarcy: 0.8805000000000001\n",
      "Epoch 0 step 244: training loss: 777.5851469410109\n",
      "Epoch 0 step 245: training accuarcy: 0.8775000000000001\n",
      "Epoch 0 step 245: training loss: 788.090113858415\n",
      "Epoch 0 step 246: training accuarcy: 0.862\n",
      "Epoch 0 step 246: training loss: 779.351219283605\n",
      "Epoch 0 step 247: training accuarcy: 0.8705\n",
      "Epoch 0 step 247: training loss: 755.9516249998742\n",
      "Epoch 0 step 248: training accuarcy: 0.876\n",
      "Epoch 0 step 248: training loss: 753.1134165164132\n",
      "Epoch 0 step 249: training accuarcy: 0.8755000000000001\n",
      "Epoch 0 step 249: training loss: 743.1784892108676\n",
      "Epoch 0 step 250: training accuarcy: 0.882\n",
      "Epoch 0 step 250: training loss: 753.1486175764502\n",
      "Epoch 0 step 251: training accuarcy: 0.8605\n",
      "Epoch 0 step 251: training loss: 788.9443799018911\n",
      "Epoch 0 step 252: training accuarcy: 0.8635\n",
      "Epoch 0 step 252: training loss: 773.2441537579549\n",
      "Epoch 0 step 253: training accuarcy: 0.867\n",
      "Epoch 0 step 253: training loss: 749.33022461556\n",
      "Epoch 0 step 254: training accuarcy: 0.8735\n",
      "Epoch 0 step 254: training loss: 756.8368402255558\n",
      "Epoch 0 step 255: training accuarcy: 0.8835000000000001\n",
      "Epoch 0 step 255: training loss: 731.40644252085\n",
      "Epoch 0 step 256: training accuarcy: 0.8835000000000001\n",
      "Epoch 0 step 256: training loss: 770.4142826914789\n",
      "Epoch 0 step 257: training accuarcy: 0.8685\n",
      "Epoch 0 step 257: training loss: 730.2812990708837\n",
      "Epoch 0 step 258: training accuarcy: 0.8715\n",
      "Epoch 0 step 258: training loss: 725.3122186946414\n",
      "Epoch 0 step 259: training accuarcy: 0.8785000000000001\n",
      "Epoch 0 step 259: training loss: 749.8219563150205\n",
      "Epoch 0 step 260: training accuarcy: 0.871\n",
      "Epoch 0 step 260: training loss: 764.2882947287416\n",
      "Epoch 0 step 261: training accuarcy: 0.8665\n",
      "Epoch 0 step 261: training loss: 747.728311277113\n",
      "Epoch 0 step 262: training accuarcy: 0.8725\n",
      "Epoch 0 step 262: training loss: 414.46920569230065\n",
      "Epoch 0 step 263: training accuarcy: 0.867948717948718\n",
      "Epoch 0: train loss 6385.003799485651, train accuarcy 0.7013980746269226\n",
      "Epoch 0: valid loss 844.9277639266414, valid accuarcy 0.8456640243530273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|█████████████████████████████████████████████████████████▎                                                                                                                  | 1/3 [02:00<04:00, 120.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 701.8108700745743\n",
      "Epoch 1 step 264: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 264: training loss: 677.0688283020372\n",
      "Epoch 1 step 265: training accuarcy: 0.899\n",
      "Epoch 1 step 265: training loss: 697.5448142336828\n",
      "Epoch 1 step 266: training accuarcy: 0.8875000000000001\n",
      "Epoch 1 step 266: training loss: 736.2438204047985\n",
      "Epoch 1 step 267: training accuarcy: 0.8795000000000001\n",
      "Epoch 1 step 267: training loss: 728.9153548235302\n",
      "Epoch 1 step 268: training accuarcy: 0.8785000000000001\n",
      "Epoch 1 step 268: training loss: 679.923000152653\n",
      "Epoch 1 step 269: training accuarcy: 0.901\n",
      "Epoch 1 step 269: training loss: 680.8993721859106\n",
      "Epoch 1 step 270: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 270: training loss: 716.9525343916167\n",
      "Epoch 1 step 271: training accuarcy: 0.8855000000000001\n",
      "Epoch 1 step 271: training loss: 692.0368629753517\n",
      "Epoch 1 step 272: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 272: training loss: 703.0588028288532\n",
      "Epoch 1 step 273: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 273: training loss: 703.0050661811816\n",
      "Epoch 1 step 274: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 274: training loss: 691.0081267556209\n",
      "Epoch 1 step 275: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 275: training loss: 688.2471734660523\n",
      "Epoch 1 step 276: training accuarcy: 0.889\n",
      "Epoch 1 step 276: training loss: 704.579161367867\n",
      "Epoch 1 step 277: training accuarcy: 0.896\n",
      "Epoch 1 step 277: training loss: 658.6483527078182\n",
      "Epoch 1 step 278: training accuarcy: 0.9015\n",
      "Epoch 1 step 278: training loss: 719.733290824319\n",
      "Epoch 1 step 279: training accuarcy: 0.88\n",
      "Epoch 1 step 279: training loss: 686.6399787545915\n",
      "Epoch 1 step 280: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 280: training loss: 661.7226661751107\n",
      "Epoch 1 step 281: training accuarcy: 0.9015\n",
      "Epoch 1 step 281: training loss: 701.0662406298396\n",
      "Epoch 1 step 282: training accuarcy: 0.886\n",
      "Epoch 1 step 282: training loss: 678.7760848568558\n",
      "Epoch 1 step 283: training accuarcy: 0.899\n",
      "Epoch 1 step 283: training loss: 701.0278098028523\n",
      "Epoch 1 step 284: training accuarcy: 0.89\n",
      "Epoch 1 step 284: training loss: 667.2157395360005\n",
      "Epoch 1 step 285: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 285: training loss: 714.5585208215932\n",
      "Epoch 1 step 286: training accuarcy: 0.8865000000000001\n",
      "Epoch 1 step 286: training loss: 706.9855853700474\n",
      "Epoch 1 step 287: training accuarcy: 0.885\n",
      "Epoch 1 step 287: training loss: 669.6098697710762\n",
      "Epoch 1 step 288: training accuarcy: 0.897\n",
      "Epoch 1 step 288: training loss: 708.619531055818\n",
      "Epoch 1 step 289: training accuarcy: 0.888\n",
      "Epoch 1 step 289: training loss: 671.429581815979\n",
      "Epoch 1 step 290: training accuarcy: 0.899\n",
      "Epoch 1 step 290: training loss: 676.3229745134945\n",
      "Epoch 1 step 291: training accuarcy: 0.886\n",
      "Epoch 1 step 291: training loss: 662.9945574064637\n",
      "Epoch 1 step 292: training accuarcy: 0.899\n",
      "Epoch 1 step 292: training loss: 714.3527023606597\n",
      "Epoch 1 step 293: training accuarcy: 0.879\n",
      "Epoch 1 step 293: training loss: 696.2032785094685\n",
      "Epoch 1 step 294: training accuarcy: 0.89\n",
      "Epoch 1 step 294: training loss: 631.9519611947918\n",
      "Epoch 1 step 295: training accuarcy: 0.915\n",
      "Epoch 1 step 295: training loss: 707.7280370971894\n",
      "Epoch 1 step 296: training accuarcy: 0.886\n",
      "Epoch 1 step 296: training loss: 692.5071973449257\n",
      "Epoch 1 step 297: training accuarcy: 0.886\n",
      "Epoch 1 step 297: training loss: 692.9919166792147\n",
      "Epoch 1 step 298: training accuarcy: 0.8825000000000001\n",
      "Epoch 1 step 298: training loss: 660.5916716474045\n",
      "Epoch 1 step 299: training accuarcy: 0.9\n",
      "Epoch 1 step 299: training loss: 694.9719995093827\n",
      "Epoch 1 step 300: training accuarcy: 0.887\n",
      "Epoch 1 step 300: training loss: 674.5842205565694\n",
      "Epoch 1 step 301: training accuarcy: 0.9035\n",
      "Epoch 1 step 301: training loss: 661.7575891344081\n",
      "Epoch 1 step 302: training accuarcy: 0.9015\n",
      "Epoch 1 step 302: training loss: 657.5632859241457\n",
      "Epoch 1 step 303: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 303: training loss: 689.6828448150093\n",
      "Epoch 1 step 304: training accuarcy: 0.887\n",
      "Epoch 1 step 304: training loss: 659.6393102206923\n",
      "Epoch 1 step 305: training accuarcy: 0.8965\n",
      "Epoch 1 step 305: training loss: 696.7506566376019\n",
      "Epoch 1 step 306: training accuarcy: 0.892\n",
      "Epoch 1 step 306: training loss: 666.2446437053535\n",
      "Epoch 1 step 307: training accuarcy: 0.895\n",
      "Epoch 1 step 307: training loss: 653.9650189003197\n",
      "Epoch 1 step 308: training accuarcy: 0.902\n",
      "Epoch 1 step 308: training loss: 661.6268266871659\n",
      "Epoch 1 step 309: training accuarcy: 0.8965\n",
      "Epoch 1 step 309: training loss: 670.4707186554889\n",
      "Epoch 1 step 310: training accuarcy: 0.8975\n",
      "Epoch 1 step 310: training loss: 658.0210282719249\n",
      "Epoch 1 step 311: training accuarcy: 0.9\n",
      "Epoch 1 step 311: training loss: 666.76982420033\n",
      "Epoch 1 step 312: training accuarcy: 0.899\n",
      "Epoch 1 step 312: training loss: 680.9246158463427\n",
      "Epoch 1 step 313: training accuarcy: 0.8955000000000001\n",
      "Epoch 1 step 313: training loss: 691.9536805705413\n",
      "Epoch 1 step 314: training accuarcy: 0.885\n",
      "Epoch 1 step 314: training loss: 629.8972041033093\n",
      "Epoch 1 step 315: training accuarcy: 0.908\n",
      "Epoch 1 step 315: training loss: 638.1026469919158\n",
      "Epoch 1 step 316: training accuarcy: 0.9095\n",
      "Epoch 1 step 316: training loss: 668.1395353932437\n",
      "Epoch 1 step 317: training accuarcy: 0.895\n",
      "Epoch 1 step 317: training loss: 684.1285524140665\n",
      "Epoch 1 step 318: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 318: training loss: 662.0109188640126\n",
      "Epoch 1 step 319: training accuarcy: 0.9\n",
      "Epoch 1 step 319: training loss: 653.6554586120435\n",
      "Epoch 1 step 320: training accuarcy: 0.893\n",
      "Epoch 1 step 320: training loss: 643.9195357449134\n",
      "Epoch 1 step 321: training accuarcy: 0.896\n",
      "Epoch 1 step 321: training loss: 631.1775040797569\n",
      "Epoch 1 step 322: training accuarcy: 0.9045\n",
      "Epoch 1 step 322: training loss: 665.8251103990796\n",
      "Epoch 1 step 323: training accuarcy: 0.896\n",
      "Epoch 1 step 323: training loss: 653.7985427883345\n",
      "Epoch 1 step 324: training accuarcy: 0.9015\n",
      "Epoch 1 step 324: training loss: 667.9702899464287\n",
      "Epoch 1 step 325: training accuarcy: 0.895\n",
      "Epoch 1 step 325: training loss: 626.944215055843\n",
      "Epoch 1 step 326: training accuarcy: 0.9135\n",
      "Epoch 1 step 326: training loss: 666.8138607982721\n",
      "Epoch 1 step 327: training accuarcy: 0.89\n",
      "Epoch 1 step 327: training loss: 671.4011492246646\n",
      "Epoch 1 step 328: training accuarcy: 0.901\n",
      "Epoch 1 step 328: training loss: 631.9593245708483\n",
      "Epoch 1 step 329: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 329: training loss: 637.208962885909\n",
      "Epoch 1 step 330: training accuarcy: 0.9085\n",
      "Epoch 1 step 330: training loss: 636.2913939678108\n",
      "Epoch 1 step 331: training accuarcy: 0.9025\n",
      "Epoch 1 step 331: training loss: 682.5155735676096\n",
      "Epoch 1 step 332: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 332: training loss: 632.0446713495289\n",
      "Epoch 1 step 333: training accuarcy: 0.906\n",
      "Epoch 1 step 333: training loss: 657.579011883348\n",
      "Epoch 1 step 334: training accuarcy: 0.8975\n",
      "Epoch 1 step 334: training loss: 693.747946362039\n",
      "Epoch 1 step 335: training accuarcy: 0.891\n",
      "Epoch 1 step 335: training loss: 631.3646611460994\n",
      "Epoch 1 step 336: training accuarcy: 0.902\n",
      "Epoch 1 step 336: training loss: 664.3822326629326\n",
      "Epoch 1 step 337: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 337: training loss: 670.2240361697807\n",
      "Epoch 1 step 338: training accuarcy: 0.883\n",
      "Epoch 1 step 338: training loss: 651.2802272061389\n",
      "Epoch 1 step 339: training accuarcy: 0.9015\n",
      "Epoch 1 step 339: training loss: 688.927416880709\n",
      "Epoch 1 step 340: training accuarcy: 0.8785000000000001\n",
      "Epoch 1 step 340: training loss: 674.0399498306701\n",
      "Epoch 1 step 341: training accuarcy: 0.895\n",
      "Epoch 1 step 341: training loss: 634.0513206004265\n",
      "Epoch 1 step 342: training accuarcy: 0.896\n",
      "Epoch 1 step 342: training loss: 625.1162264545987\n",
      "Epoch 1 step 343: training accuarcy: 0.9065\n",
      "Epoch 1 step 343: training loss: 664.5862285451085\n",
      "Epoch 1 step 344: training accuarcy: 0.9005\n",
      "Epoch 1 step 344: training loss: 671.732406496484\n",
      "Epoch 1 step 345: training accuarcy: 0.894\n",
      "Epoch 1 step 345: training loss: 634.6800029190221\n",
      "Epoch 1 step 346: training accuarcy: 0.9015\n",
      "Epoch 1 step 346: training loss: 622.4664649970935\n",
      "Epoch 1 step 347: training accuarcy: 0.906\n",
      "Epoch 1 step 347: training loss: 635.960654863663\n",
      "Epoch 1 step 348: training accuarcy: 0.902\n",
      "Epoch 1 step 348: training loss: 642.2816449277832\n",
      "Epoch 1 step 349: training accuarcy: 0.89\n",
      "Epoch 1 step 349: training loss: 606.9390681178435\n",
      "Epoch 1 step 350: training accuarcy: 0.9135\n",
      "Epoch 1 step 350: training loss: 661.1954789774134\n",
      "Epoch 1 step 351: training accuarcy: 0.898\n",
      "Epoch 1 step 351: training loss: 642.6164125291227\n",
      "Epoch 1 step 352: training accuarcy: 0.908\n",
      "Epoch 1 step 352: training loss: 650.2739011684691\n",
      "Epoch 1 step 353: training accuarcy: 0.8985\n",
      "Epoch 1 step 353: training loss: 641.9822198601483\n",
      "Epoch 1 step 354: training accuarcy: 0.8885000000000001\n",
      "Epoch 1 step 354: training loss: 641.7241173886073\n",
      "Epoch 1 step 355: training accuarcy: 0.9025\n",
      "Epoch 1 step 355: training loss: 654.3283819336135\n",
      "Epoch 1 step 356: training accuarcy: 0.899\n",
      "Epoch 1 step 356: training loss: 643.5790292464708\n",
      "Epoch 1 step 357: training accuarcy: 0.902\n",
      "Epoch 1 step 357: training loss: 641.2968546412421\n",
      "Epoch 1 step 358: training accuarcy: 0.898\n",
      "Epoch 1 step 358: training loss: 640.4484578772085\n",
      "Epoch 1 step 359: training accuarcy: 0.905\n",
      "Epoch 1 step 359: training loss: 695.2271191648157\n",
      "Epoch 1 step 360: training accuarcy: 0.896\n",
      "Epoch 1 step 360: training loss: 639.2044133521672\n",
      "Epoch 1 step 361: training accuarcy: 0.903\n",
      "Epoch 1 step 361: training loss: 629.0158456380984\n",
      "Epoch 1 step 362: training accuarcy: 0.9065\n",
      "Epoch 1 step 362: training loss: 624.405422573136\n",
      "Epoch 1 step 363: training accuarcy: 0.9085\n",
      "Epoch 1 step 363: training loss: 660.9477086111062\n",
      "Epoch 1 step 364: training accuarcy: 0.9015\n",
      "Epoch 1 step 364: training loss: 619.7404090831545\n",
      "Epoch 1 step 365: training accuarcy: 0.9045\n",
      "Epoch 1 step 365: training loss: 645.8257844608777\n",
      "Epoch 1 step 366: training accuarcy: 0.9055\n",
      "Epoch 1 step 366: training loss: 661.9838172295679\n",
      "Epoch 1 step 367: training accuarcy: 0.901\n",
      "Epoch 1 step 367: training loss: 627.8647267791034\n",
      "Epoch 1 step 368: training accuarcy: 0.9145\n",
      "Epoch 1 step 368: training loss: 638.2910626697692\n",
      "Epoch 1 step 369: training accuarcy: 0.8995\n",
      "Epoch 1 step 369: training loss: 604.7915439864151\n",
      "Epoch 1 step 370: training accuarcy: 0.917\n",
      "Epoch 1 step 370: training loss: 646.037392131673\n",
      "Epoch 1 step 371: training accuarcy: 0.901\n",
      "Epoch 1 step 371: training loss: 652.9680771163435\n",
      "Epoch 1 step 372: training accuarcy: 0.8985\n",
      "Epoch 1 step 372: training loss: 649.2372849006077\n",
      "Epoch 1 step 373: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 373: training loss: 620.6855391383151\n",
      "Epoch 1 step 374: training accuarcy: 0.9055\n",
      "Epoch 1 step 374: training loss: 616.5331471692514\n",
      "Epoch 1 step 375: training accuarcy: 0.908\n",
      "Epoch 1 step 375: training loss: 634.7305930422208\n",
      "Epoch 1 step 376: training accuarcy: 0.906\n",
      "Epoch 1 step 376: training loss: 629.22852087419\n",
      "Epoch 1 step 377: training accuarcy: 0.909\n",
      "Epoch 1 step 377: training loss: 612.5915081986986\n",
      "Epoch 1 step 378: training accuarcy: 0.9105\n",
      "Epoch 1 step 378: training loss: 634.6042644016115\n",
      "Epoch 1 step 379: training accuarcy: 0.907\n",
      "Epoch 1 step 379: training loss: 653.9008266061056\n",
      "Epoch 1 step 380: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 380: training loss: 638.2646880410233\n",
      "Epoch 1 step 381: training accuarcy: 0.897\n",
      "Epoch 1 step 381: training loss: 626.1847890527719\n",
      "Epoch 1 step 382: training accuarcy: 0.8985\n",
      "Epoch 1 step 382: training loss: 639.9201916958256\n",
      "Epoch 1 step 383: training accuarcy: 0.901\n",
      "Epoch 1 step 383: training loss: 613.6758170174098\n",
      "Epoch 1 step 384: training accuarcy: 0.9075\n",
      "Epoch 1 step 384: training loss: 617.7300254426168\n",
      "Epoch 1 step 385: training accuarcy: 0.911\n",
      "Epoch 1 step 385: training loss: 643.3595526769166\n",
      "Epoch 1 step 386: training accuarcy: 0.9035\n",
      "Epoch 1 step 386: training loss: 631.2426054765031\n",
      "Epoch 1 step 387: training accuarcy: 0.9105\n",
      "Epoch 1 step 387: training loss: 602.6267924651402\n",
      "Epoch 1 step 388: training accuarcy: 0.9165\n",
      "Epoch 1 step 388: training loss: 628.8189504890623\n",
      "Epoch 1 step 389: training accuarcy: 0.8995\n",
      "Epoch 1 step 389: training loss: 639.0405389200046\n",
      "Epoch 1 step 390: training accuarcy: 0.8975\n",
      "Epoch 1 step 390: training loss: 627.7929777423055\n",
      "Epoch 1 step 391: training accuarcy: 0.898\n",
      "Epoch 1 step 391: training loss: 621.8886456769861\n",
      "Epoch 1 step 392: training accuarcy: 0.904\n",
      "Epoch 1 step 392: training loss: 617.5816361403852\n",
      "Epoch 1 step 393: training accuarcy: 0.9055\n",
      "Epoch 1 step 393: training loss: 645.6767934750512\n",
      "Epoch 1 step 394: training accuarcy: 0.895\n",
      "Epoch 1 step 394: training loss: 610.6864102256492\n",
      "Epoch 1 step 395: training accuarcy: 0.9125\n",
      "Epoch 1 step 395: training loss: 626.417499203714\n",
      "Epoch 1 step 396: training accuarcy: 0.898\n",
      "Epoch 1 step 396: training loss: 652.1175119313453\n",
      "Epoch 1 step 397: training accuarcy: 0.903\n",
      "Epoch 1 step 397: training loss: 633.9104151278588\n",
      "Epoch 1 step 398: training accuarcy: 0.9\n",
      "Epoch 1 step 398: training loss: 605.1488667660459\n",
      "Epoch 1 step 399: training accuarcy: 0.91\n",
      "Epoch 1 step 399: training loss: 613.2643596473852\n",
      "Epoch 1 step 400: training accuarcy: 0.91\n",
      "Epoch 1 step 400: training loss: 589.3518190402654\n",
      "Epoch 1 step 401: training accuarcy: 0.915\n",
      "Epoch 1 step 401: training loss: 609.6178976397824\n",
      "Epoch 1 step 402: training accuarcy: 0.913\n",
      "Epoch 1 step 402: training loss: 589.2951487163153\n",
      "Epoch 1 step 403: training accuarcy: 0.917\n",
      "Epoch 1 step 403: training loss: 630.4520584848613\n",
      "Epoch 1 step 404: training accuarcy: 0.8945000000000001\n",
      "Epoch 1 step 404: training loss: 622.3015346276277\n",
      "Epoch 1 step 405: training accuarcy: 0.9025\n",
      "Epoch 1 step 405: training loss: 632.3496710029519\n",
      "Epoch 1 step 406: training accuarcy: 0.908\n",
      "Epoch 1 step 406: training loss: 608.5966395769358\n",
      "Epoch 1 step 407: training accuarcy: 0.9115\n",
      "Epoch 1 step 407: training loss: 608.2926822467596\n",
      "Epoch 1 step 408: training accuarcy: 0.912\n",
      "Epoch 1 step 408: training loss: 665.3705551690709\n",
      "Epoch 1 step 409: training accuarcy: 0.8995\n",
      "Epoch 1 step 409: training loss: 655.6219429679112\n",
      "Epoch 1 step 410: training accuarcy: 0.897\n",
      "Epoch 1 step 410: training loss: 623.59344344442\n",
      "Epoch 1 step 411: training accuarcy: 0.909\n",
      "Epoch 1 step 411: training loss: 631.8639302270437\n",
      "Epoch 1 step 412: training accuarcy: 0.897\n",
      "Epoch 1 step 412: training loss: 638.2008971009178\n",
      "Epoch 1 step 413: training accuarcy: 0.902\n",
      "Epoch 1 step 413: training loss: 626.8760429807297\n",
      "Epoch 1 step 414: training accuarcy: 0.9\n",
      "Epoch 1 step 414: training loss: 605.3853959035303\n",
      "Epoch 1 step 415: training accuarcy: 0.9065\n",
      "Epoch 1 step 415: training loss: 604.3374090515749\n",
      "Epoch 1 step 416: training accuarcy: 0.9055\n",
      "Epoch 1 step 416: training loss: 621.6303230752914\n",
      "Epoch 1 step 417: training accuarcy: 0.906\n",
      "Epoch 1 step 417: training loss: 607.4930187153219\n",
      "Epoch 1 step 418: training accuarcy: 0.9115\n",
      "Epoch 1 step 418: training loss: 604.6550750197337\n",
      "Epoch 1 step 419: training accuarcy: 0.9075\n",
      "Epoch 1 step 419: training loss: 616.0522281828412\n",
      "Epoch 1 step 420: training accuarcy: 0.902\n",
      "Epoch 1 step 420: training loss: 602.6521091144698\n",
      "Epoch 1 step 421: training accuarcy: 0.9065\n",
      "Epoch 1 step 421: training loss: 644.465749089022\n",
      "Epoch 1 step 422: training accuarcy: 0.9035\n",
      "Epoch 1 step 422: training loss: 620.0863171456555\n",
      "Epoch 1 step 423: training accuarcy: 0.908\n",
      "Epoch 1 step 423: training loss: 605.5725971205336\n",
      "Epoch 1 step 424: training accuarcy: 0.9165\n",
      "Epoch 1 step 424: training loss: 595.2127885361975\n",
      "Epoch 1 step 425: training accuarcy: 0.9145\n",
      "Epoch 1 step 425: training loss: 617.4134637327129\n",
      "Epoch 1 step 426: training accuarcy: 0.909\n",
      "Epoch 1 step 426: training loss: 651.2708001830188\n",
      "Epoch 1 step 427: training accuarcy: 0.9025\n",
      "Epoch 1 step 427: training loss: 603.5305980030249\n",
      "Epoch 1 step 428: training accuarcy: 0.909\n",
      "Epoch 1 step 428: training loss: 604.759717005442\n",
      "Epoch 1 step 429: training accuarcy: 0.9065\n",
      "Epoch 1 step 429: training loss: 570.8477932228944\n",
      "Epoch 1 step 430: training accuarcy: 0.914\n",
      "Epoch 1 step 430: training loss: 640.3174563168977\n",
      "Epoch 1 step 431: training accuarcy: 0.9\n",
      "Epoch 1 step 431: training loss: 592.476136163204\n",
      "Epoch 1 step 432: training accuarcy: 0.915\n",
      "Epoch 1 step 432: training loss: 598.5164828517966\n",
      "Epoch 1 step 433: training accuarcy: 0.9105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 433: training loss: 618.0906297669151\n",
      "Epoch 1 step 434: training accuarcy: 0.906\n",
      "Epoch 1 step 434: training loss: 629.1722150466322\n",
      "Epoch 1 step 435: training accuarcy: 0.9035\n",
      "Epoch 1 step 435: training loss: 593.9876006586827\n",
      "Epoch 1 step 436: training accuarcy: 0.916\n",
      "Epoch 1 step 436: training loss: 651.4552892530296\n",
      "Epoch 1 step 437: training accuarcy: 0.905\n",
      "Epoch 1 step 437: training loss: 580.8252509945722\n",
      "Epoch 1 step 438: training accuarcy: 0.916\n",
      "Epoch 1 step 438: training loss: 607.671725267024\n",
      "Epoch 1 step 439: training accuarcy: 0.907\n",
      "Epoch 1 step 439: training loss: 619.405723904622\n",
      "Epoch 1 step 440: training accuarcy: 0.9055\n",
      "Epoch 1 step 440: training loss: 620.0468855746135\n",
      "Epoch 1 step 441: training accuarcy: 0.9065\n",
      "Epoch 1 step 441: training loss: 620.5297027854897\n",
      "Epoch 1 step 442: training accuarcy: 0.9015\n",
      "Epoch 1 step 442: training loss: 613.918209907714\n",
      "Epoch 1 step 443: training accuarcy: 0.9055\n",
      "Epoch 1 step 443: training loss: 654.6236817969592\n",
      "Epoch 1 step 444: training accuarcy: 0.8965\n",
      "Epoch 1 step 444: training loss: 596.8218634443961\n",
      "Epoch 1 step 445: training accuarcy: 0.913\n",
      "Epoch 1 step 445: training loss: 602.5902905828739\n",
      "Epoch 1 step 446: training accuarcy: 0.8995\n",
      "Epoch 1 step 446: training loss: 619.1812386151155\n",
      "Epoch 1 step 447: training accuarcy: 0.911\n",
      "Epoch 1 step 447: training loss: 634.8360370122183\n",
      "Epoch 1 step 448: training accuarcy: 0.8965\n",
      "Epoch 1 step 448: training loss: 615.1481288116197\n",
      "Epoch 1 step 449: training accuarcy: 0.899\n",
      "Epoch 1 step 449: training loss: 614.7418330057683\n",
      "Epoch 1 step 450: training accuarcy: 0.907\n",
      "Epoch 1 step 450: training loss: 591.8042739562436\n",
      "Epoch 1 step 451: training accuarcy: 0.914\n",
      "Epoch 1 step 451: training loss: 599.188367781198\n",
      "Epoch 1 step 452: training accuarcy: 0.9045\n",
      "Epoch 1 step 452: training loss: 596.686946593542\n",
      "Epoch 1 step 453: training accuarcy: 0.917\n",
      "Epoch 1 step 453: training loss: 590.3228200494943\n",
      "Epoch 1 step 454: training accuarcy: 0.906\n",
      "Epoch 1 step 454: training loss: 650.7057305267392\n",
      "Epoch 1 step 455: training accuarcy: 0.901\n",
      "Epoch 1 step 455: training loss: 570.6148876397895\n",
      "Epoch 1 step 456: training accuarcy: 0.922\n",
      "Epoch 1 step 456: training loss: 600.6234730793917\n",
      "Epoch 1 step 457: training accuarcy: 0.9175\n",
      "Epoch 1 step 457: training loss: 602.2002110193188\n",
      "Epoch 1 step 458: training accuarcy: 0.9105\n",
      "Epoch 1 step 458: training loss: 573.148739501198\n",
      "Epoch 1 step 459: training accuarcy: 0.9185\n",
      "Epoch 1 step 459: training loss: 619.9373384147725\n",
      "Epoch 1 step 460: training accuarcy: 0.9095\n",
      "Epoch 1 step 460: training loss: 596.2826796408235\n",
      "Epoch 1 step 461: training accuarcy: 0.9095\n",
      "Epoch 1 step 461: training loss: 602.9885678511874\n",
      "Epoch 1 step 462: training accuarcy: 0.9125\n",
      "Epoch 1 step 462: training loss: 598.8015951161025\n",
      "Epoch 1 step 463: training accuarcy: 0.9095\n",
      "Epoch 1 step 463: training loss: 588.2997041932953\n",
      "Epoch 1 step 464: training accuarcy: 0.916\n",
      "Epoch 1 step 464: training loss: 626.4240954021127\n",
      "Epoch 1 step 465: training accuarcy: 0.906\n",
      "Epoch 1 step 465: training loss: 588.505057254868\n",
      "Epoch 1 step 466: training accuarcy: 0.9115\n",
      "Epoch 1 step 466: training loss: 588.259853616669\n",
      "Epoch 1 step 467: training accuarcy: 0.9215\n",
      "Epoch 1 step 467: training loss: 604.3141165612518\n",
      "Epoch 1 step 468: training accuarcy: 0.9095\n",
      "Epoch 1 step 468: training loss: 583.4495959481611\n",
      "Epoch 1 step 469: training accuarcy: 0.919\n",
      "Epoch 1 step 469: training loss: 571.4102625007845\n",
      "Epoch 1 step 470: training accuarcy: 0.9215\n",
      "Epoch 1 step 470: training loss: 596.5819224418535\n",
      "Epoch 1 step 471: training accuarcy: 0.913\n",
      "Epoch 1 step 471: training loss: 565.1641349554145\n",
      "Epoch 1 step 472: training accuarcy: 0.9225\n",
      "Epoch 1 step 472: training loss: 630.1040453288259\n",
      "Epoch 1 step 473: training accuarcy: 0.897\n",
      "Epoch 1 step 473: training loss: 602.3840504297893\n",
      "Epoch 1 step 474: training accuarcy: 0.9115\n",
      "Epoch 1 step 474: training loss: 602.2361822697019\n",
      "Epoch 1 step 475: training accuarcy: 0.9105\n",
      "Epoch 1 step 475: training loss: 606.5790009078182\n",
      "Epoch 1 step 476: training accuarcy: 0.9115\n",
      "Epoch 1 step 476: training loss: 593.7900695322489\n",
      "Epoch 1 step 477: training accuarcy: 0.919\n",
      "Epoch 1 step 477: training loss: 569.921048271498\n",
      "Epoch 1 step 478: training accuarcy: 0.9095\n",
      "Epoch 1 step 478: training loss: 597.7875669108023\n",
      "Epoch 1 step 479: training accuarcy: 0.905\n",
      "Epoch 1 step 479: training loss: 589.1103719324515\n",
      "Epoch 1 step 480: training accuarcy: 0.9145\n",
      "Epoch 1 step 480: training loss: 599.3372054022622\n",
      "Epoch 1 step 481: training accuarcy: 0.9065\n",
      "Epoch 1 step 481: training loss: 644.3004152982511\n",
      "Epoch 1 step 482: training accuarcy: 0.8975\n",
      "Epoch 1 step 482: training loss: 576.8706833421859\n",
      "Epoch 1 step 483: training accuarcy: 0.9155\n",
      "Epoch 1 step 483: training loss: 604.6355181200547\n",
      "Epoch 1 step 484: training accuarcy: 0.9055\n",
      "Epoch 1 step 484: training loss: 593.9607584603618\n",
      "Epoch 1 step 485: training accuarcy: 0.9105\n",
      "Epoch 1 step 485: training loss: 569.2262905197274\n",
      "Epoch 1 step 486: training accuarcy: 0.912\n",
      "Epoch 1 step 486: training loss: 616.3808595697344\n",
      "Epoch 1 step 487: training accuarcy: 0.9035\n",
      "Epoch 1 step 487: training loss: 619.3781332480859\n",
      "Epoch 1 step 488: training accuarcy: 0.9075\n",
      "Epoch 1 step 488: training loss: 617.1978043675522\n",
      "Epoch 1 step 489: training accuarcy: 0.9055\n",
      "Epoch 1 step 489: training loss: 607.2398229702576\n",
      "Epoch 1 step 490: training accuarcy: 0.908\n",
      "Epoch 1 step 490: training loss: 611.6657358740651\n",
      "Epoch 1 step 491: training accuarcy: 0.9065\n",
      "Epoch 1 step 491: training loss: 603.194619407256\n",
      "Epoch 1 step 492: training accuarcy: 0.899\n",
      "Epoch 1 step 492: training loss: 590.4165678194885\n",
      "Epoch 1 step 493: training accuarcy: 0.912\n",
      "Epoch 1 step 493: training loss: 601.8013953952191\n",
      "Epoch 1 step 494: training accuarcy: 0.913\n",
      "Epoch 1 step 494: training loss: 587.1999958237453\n",
      "Epoch 1 step 495: training accuarcy: 0.9145\n",
      "Epoch 1 step 495: training loss: 613.4870002705527\n",
      "Epoch 1 step 496: training accuarcy: 0.9015\n",
      "Epoch 1 step 496: training loss: 589.995239657526\n",
      "Epoch 1 step 497: training accuarcy: 0.9125\n",
      "Epoch 1 step 497: training loss: 594.6254109253981\n",
      "Epoch 1 step 498: training accuarcy: 0.907\n",
      "Epoch 1 step 498: training loss: 545.9654018029366\n",
      "Epoch 1 step 499: training accuarcy: 0.9245\n",
      "Epoch 1 step 499: training loss: 601.27270664198\n",
      "Epoch 1 step 500: training accuarcy: 0.907\n",
      "Epoch 1 step 500: training loss: 650.5992414357306\n",
      "Epoch 1 step 501: training accuarcy: 0.8985\n",
      "Epoch 1 step 501: training loss: 618.9798494243572\n",
      "Epoch 1 step 502: training accuarcy: 0.9015\n",
      "Epoch 1 step 502: training loss: 619.077598985613\n",
      "Epoch 1 step 503: training accuarcy: 0.9125\n",
      "Epoch 1 step 503: training loss: 627.7066226773904\n",
      "Epoch 1 step 504: training accuarcy: 0.905\n",
      "Epoch 1 step 504: training loss: 583.3709172510124\n",
      "Epoch 1 step 505: training accuarcy: 0.913\n",
      "Epoch 1 step 505: training loss: 594.1499765559595\n",
      "Epoch 1 step 506: training accuarcy: 0.9125\n",
      "Epoch 1 step 506: training loss: 607.1316374592359\n",
      "Epoch 1 step 507: training accuarcy: 0.896\n",
      "Epoch 1 step 507: training loss: 614.796938945407\n",
      "Epoch 1 step 508: training accuarcy: 0.905\n",
      "Epoch 1 step 508: training loss: 622.3259617525368\n",
      "Epoch 1 step 509: training accuarcy: 0.902\n",
      "Epoch 1 step 509: training loss: 600.5560187247214\n",
      "Epoch 1 step 510: training accuarcy: 0.906\n",
      "Epoch 1 step 510: training loss: 576.9768092018006\n",
      "Epoch 1 step 511: training accuarcy: 0.9155\n",
      "Epoch 1 step 511: training loss: 581.8941722592899\n",
      "Epoch 1 step 512: training accuarcy: 0.91\n",
      "Epoch 1 step 512: training loss: 605.1995007436485\n",
      "Epoch 1 step 513: training accuarcy: 0.9135\n",
      "Epoch 1 step 513: training loss: 643.4003531687093\n",
      "Epoch 1 step 514: training accuarcy: 0.9025\n",
      "Epoch 1 step 514: training loss: 589.6053254650011\n",
      "Epoch 1 step 515: training accuarcy: 0.915\n",
      "Epoch 1 step 515: training loss: 578.1443940848324\n",
      "Epoch 1 step 516: training accuarcy: 0.923\n",
      "Epoch 1 step 516: training loss: 593.1270323642386\n",
      "Epoch 1 step 517: training accuarcy: 0.906\n",
      "Epoch 1 step 517: training loss: 585.9244031941959\n",
      "Epoch 1 step 518: training accuarcy: 0.922\n",
      "Epoch 1 step 518: training loss: 603.1007946776048\n",
      "Epoch 1 step 519: training accuarcy: 0.9085\n",
      "Epoch 1 step 519: training loss: 603.4168520727981\n",
      "Epoch 1 step 520: training accuarcy: 0.9105\n",
      "Epoch 1 step 520: training loss: 599.0746386355745\n",
      "Epoch 1 step 521: training accuarcy: 0.9205\n",
      "Epoch 1 step 521: training loss: 578.2064602310861\n",
      "Epoch 1 step 522: training accuarcy: 0.9125\n",
      "Epoch 1 step 522: training loss: 597.3812332422397\n",
      "Epoch 1 step 523: training accuarcy: 0.9195\n",
      "Epoch 1 step 523: training loss: 577.9168831508983\n",
      "Epoch 1 step 524: training accuarcy: 0.919\n",
      "Epoch 1 step 524: training loss: 630.4972879763275\n",
      "Epoch 1 step 525: training accuarcy: 0.9065\n",
      "Epoch 1 step 525: training loss: 334.6699751406985\n",
      "Epoch 1 step 526: training accuarcy: 0.9012820512820513\n",
      "Epoch 1: train loss 632.342911258142, train accuarcy 0.8824292421340942\n",
      "Epoch 1: valid loss 693.5633778219023, valid accuarcy 0.8839700818061829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 2/3 [03:59<01:59, 119.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 552.0526878855613\n",
      "Epoch 2 step 527: training accuarcy: 0.9185\n",
      "Epoch 2 step 527: training loss: 534.5683291432863\n",
      "Epoch 2 step 528: training accuarcy: 0.924\n",
      "Epoch 2 step 528: training loss: 548.1074612817788\n",
      "Epoch 2 step 529: training accuarcy: 0.927\n",
      "Epoch 2 step 529: training loss: 559.8465337082757\n",
      "Epoch 2 step 530: training accuarcy: 0.9235\n",
      "Epoch 2 step 530: training loss: 531.1384140414998\n",
      "Epoch 2 step 531: training accuarcy: 0.9305\n",
      "Epoch 2 step 531: training loss: 558.5680281442114\n",
      "Epoch 2 step 532: training accuarcy: 0.916\n",
      "Epoch 2 step 532: training loss: 542.7798058085571\n",
      "Epoch 2 step 533: training accuarcy: 0.9265\n",
      "Epoch 2 step 533: training loss: 548.968407527061\n",
      "Epoch 2 step 534: training accuarcy: 0.9285\n",
      "Epoch 2 step 534: training loss: 522.6574790028385\n",
      "Epoch 2 step 535: training accuarcy: 0.925\n",
      "Epoch 2 step 535: training loss: 552.5348446817952\n",
      "Epoch 2 step 536: training accuarcy: 0.9295\n",
      "Epoch 2 step 536: training loss: 549.1651849959298\n",
      "Epoch 2 step 537: training accuarcy: 0.9245\n",
      "Epoch 2 step 537: training loss: 551.6794350117452\n",
      "Epoch 2 step 538: training accuarcy: 0.922\n",
      "Epoch 2 step 538: training loss: 561.4258121699492\n",
      "Epoch 2 step 539: training accuarcy: 0.9155\n",
      "Epoch 2 step 539: training loss: 548.1622935012347\n",
      "Epoch 2 step 540: training accuarcy: 0.925\n",
      "Epoch 2 step 540: training loss: 556.4880599937812\n",
      "Epoch 2 step 541: training accuarcy: 0.9265\n",
      "Epoch 2 step 541: training loss: 546.5902151044659\n",
      "Epoch 2 step 542: training accuarcy: 0.928\n",
      "Epoch 2 step 542: training loss: 547.2751244231578\n",
      "Epoch 2 step 543: training accuarcy: 0.9205\n",
      "Epoch 2 step 543: training loss: 570.9673437362048\n",
      "Epoch 2 step 544: training accuarcy: 0.915\n",
      "Epoch 2 step 544: training loss: 561.2411428096539\n",
      "Epoch 2 step 545: training accuarcy: 0.922\n",
      "Epoch 2 step 545: training loss: 547.6254154067908\n",
      "Epoch 2 step 546: training accuarcy: 0.924\n",
      "Epoch 2 step 546: training loss: 564.6183106762811\n",
      "Epoch 2 step 547: training accuarcy: 0.913\n",
      "Epoch 2 step 547: training loss: 556.3461630682808\n",
      "Epoch 2 step 548: training accuarcy: 0.9185\n",
      "Epoch 2 step 548: training loss: 528.6799283088578\n",
      "Epoch 2 step 549: training accuarcy: 0.931\n",
      "Epoch 2 step 549: training loss: 558.7638365432376\n",
      "Epoch 2 step 550: training accuarcy: 0.924\n",
      "Epoch 2 step 550: training loss: 556.1009753672826\n",
      "Epoch 2 step 551: training accuarcy: 0.9175\n",
      "Epoch 2 step 551: training loss: 535.8480096721572\n",
      "Epoch 2 step 552: training accuarcy: 0.928\n",
      "Epoch 2 step 552: training loss: 539.2684385404398\n",
      "Epoch 2 step 553: training accuarcy: 0.929\n",
      "Epoch 2 step 553: training loss: 559.2136376591757\n",
      "Epoch 2 step 554: training accuarcy: 0.912\n",
      "Epoch 2 step 554: training loss: 565.7700824497623\n",
      "Epoch 2 step 555: training accuarcy: 0.918\n",
      "Epoch 2 step 555: training loss: 564.0130355727506\n",
      "Epoch 2 step 556: training accuarcy: 0.914\n",
      "Epoch 2 step 556: training loss: 522.1213331467225\n",
      "Epoch 2 step 557: training accuarcy: 0.928\n",
      "Epoch 2 step 557: training loss: 528.9248473225823\n",
      "Epoch 2 step 558: training accuarcy: 0.9315\n",
      "Epoch 2 step 558: training loss: 563.307138375014\n",
      "Epoch 2 step 559: training accuarcy: 0.9095\n",
      "Epoch 2 step 559: training loss: 552.6653162185255\n",
      "Epoch 2 step 560: training accuarcy: 0.916\n",
      "Epoch 2 step 560: training loss: 592.5360071094685\n",
      "Epoch 2 step 561: training accuarcy: 0.908\n",
      "Epoch 2 step 561: training loss: 532.1884360272632\n",
      "Epoch 2 step 562: training accuarcy: 0.931\n",
      "Epoch 2 step 562: training loss: 567.8185912679244\n",
      "Epoch 2 step 563: training accuarcy: 0.9215\n",
      "Epoch 2 step 563: training loss: 551.4308364827215\n",
      "Epoch 2 step 564: training accuarcy: 0.923\n",
      "Epoch 2 step 564: training loss: 553.7735337802778\n",
      "Epoch 2 step 565: training accuarcy: 0.9205\n",
      "Epoch 2 step 565: training loss: 544.0293180029541\n",
      "Epoch 2 step 566: training accuarcy: 0.9255\n",
      "Epoch 2 step 566: training loss: 507.8976078364155\n",
      "Epoch 2 step 567: training accuarcy: 0.926\n",
      "Epoch 2 step 567: training loss: 546.7905216123811\n",
      "Epoch 2 step 568: training accuarcy: 0.9245\n",
      "Epoch 2 step 568: training loss: 556.1023483725327\n",
      "Epoch 2 step 569: training accuarcy: 0.925\n",
      "Epoch 2 step 569: training loss: 557.6440510112229\n",
      "Epoch 2 step 570: training accuarcy: 0.9225\n",
      "Epoch 2 step 570: training loss: 543.6523200685119\n",
      "Epoch 2 step 571: training accuarcy: 0.9255\n",
      "Epoch 2 step 571: training loss: 550.5368168931378\n",
      "Epoch 2 step 572: training accuarcy: 0.9255\n",
      "Epoch 2 step 572: training loss: 528.5042851176801\n",
      "Epoch 2 step 573: training accuarcy: 0.925\n",
      "Epoch 2 step 573: training loss: 532.5330448203208\n",
      "Epoch 2 step 574: training accuarcy: 0.9235\n",
      "Epoch 2 step 574: training loss: 551.9390542954844\n",
      "Epoch 2 step 575: training accuarcy: 0.927\n",
      "Epoch 2 step 575: training loss: 594.4433517554169\n",
      "Epoch 2 step 576: training accuarcy: 0.908\n",
      "Epoch 2 step 576: training loss: 522.2562233193015\n",
      "Epoch 2 step 577: training accuarcy: 0.9265\n",
      "Epoch 2 step 577: training loss: 529.754617894367\n",
      "Epoch 2 step 578: training accuarcy: 0.9345\n",
      "Epoch 2 step 578: training loss: 568.1619964035001\n",
      "Epoch 2 step 579: training accuarcy: 0.9095\n",
      "Epoch 2 step 579: training loss: 583.7226360434836\n",
      "Epoch 2 step 580: training accuarcy: 0.9095\n",
      "Epoch 2 step 580: training loss: 550.6708741663329\n",
      "Epoch 2 step 581: training accuarcy: 0.9205\n",
      "Epoch 2 step 581: training loss: 554.8649937174798\n",
      "Epoch 2 step 582: training accuarcy: 0.919\n",
      "Epoch 2 step 582: training loss: 562.6722937147791\n",
      "Epoch 2 step 583: training accuarcy: 0.918\n",
      "Epoch 2 step 583: training loss: 575.657479564421\n",
      "Epoch 2 step 584: training accuarcy: 0.9105\n",
      "Epoch 2 step 584: training loss: 550.589095655068\n",
      "Epoch 2 step 585: training accuarcy: 0.9255\n",
      "Epoch 2 step 585: training loss: 539.9789801623278\n",
      "Epoch 2 step 586: training accuarcy: 0.9235\n",
      "Epoch 2 step 586: training loss: 566.4849599599415\n",
      "Epoch 2 step 587: training accuarcy: 0.918\n",
      "Epoch 2 step 587: training loss: 531.7814121826013\n",
      "Epoch 2 step 588: training accuarcy: 0.9255\n",
      "Epoch 2 step 588: training loss: 544.0261654487243\n",
      "Epoch 2 step 589: training accuarcy: 0.916\n",
      "Epoch 2 step 589: training loss: 548.486191606625\n",
      "Epoch 2 step 590: training accuarcy: 0.9225\n",
      "Epoch 2 step 590: training loss: 539.2362211057871\n",
      "Epoch 2 step 591: training accuarcy: 0.9255\n",
      "Epoch 2 step 591: training loss: 564.0584435849676\n",
      "Epoch 2 step 592: training accuarcy: 0.917\n",
      "Epoch 2 step 592: training loss: 547.5478837752944\n",
      "Epoch 2 step 593: training accuarcy: 0.9165\n",
      "Epoch 2 step 593: training loss: 547.906509687511\n",
      "Epoch 2 step 594: training accuarcy: 0.928\n",
      "Epoch 2 step 594: training loss: 596.2178489431427\n",
      "Epoch 2 step 595: training accuarcy: 0.916\n",
      "Epoch 2 step 595: training loss: 560.6539736466269\n",
      "Epoch 2 step 596: training accuarcy: 0.921\n",
      "Epoch 2 step 596: training loss: 552.5143289168407\n",
      "Epoch 2 step 597: training accuarcy: 0.9195\n",
      "Epoch 2 step 597: training loss: 525.9711911710637\n",
      "Epoch 2 step 598: training accuarcy: 0.9245\n",
      "Epoch 2 step 598: training loss: 550.8524484340712\n",
      "Epoch 2 step 599: training accuarcy: 0.9175\n",
      "Epoch 2 step 599: training loss: 560.0079083908743\n",
      "Epoch 2 step 600: training accuarcy: 0.9145\n",
      "Epoch 2 step 600: training loss: 508.25914709675385\n",
      "Epoch 2 step 601: training accuarcy: 0.933\n",
      "Epoch 2 step 601: training loss: 576.233135226663\n",
      "Epoch 2 step 602: training accuarcy: 0.916\n",
      "Epoch 2 step 602: training loss: 576.3730095541973\n",
      "Epoch 2 step 603: training accuarcy: 0.92\n",
      "Epoch 2 step 603: training loss: 522.5909793789872\n",
      "Epoch 2 step 604: training accuarcy: 0.9305\n",
      "Epoch 2 step 604: training loss: 478.73795546066674\n",
      "Epoch 2 step 605: training accuarcy: 0.9405\n",
      "Epoch 2 step 605: training loss: 555.0989590113787\n",
      "Epoch 2 step 606: training accuarcy: 0.925\n",
      "Epoch 2 step 606: training loss: 545.9893637416321\n",
      "Epoch 2 step 607: training accuarcy: 0.919\n",
      "Epoch 2 step 607: training loss: 550.2904673840336\n",
      "Epoch 2 step 608: training accuarcy: 0.9205\n",
      "Epoch 2 step 608: training loss: 556.0986583331598\n",
      "Epoch 2 step 609: training accuarcy: 0.919\n",
      "Epoch 2 step 609: training loss: 568.8073036513965\n",
      "Epoch 2 step 610: training accuarcy: 0.91\n",
      "Epoch 2 step 610: training loss: 521.496917705917\n",
      "Epoch 2 step 611: training accuarcy: 0.929\n",
      "Epoch 2 step 611: training loss: 533.4915002252587\n",
      "Epoch 2 step 612: training accuarcy: 0.9305\n",
      "Epoch 2 step 612: training loss: 551.764828750532\n",
      "Epoch 2 step 613: training accuarcy: 0.9155\n",
      "Epoch 2 step 613: training loss: 579.2561408339634\n",
      "Epoch 2 step 614: training accuarcy: 0.9195\n",
      "Epoch 2 step 614: training loss: 544.3748307015348\n",
      "Epoch 2 step 615: training accuarcy: 0.924\n",
      "Epoch 2 step 615: training loss: 558.7309971986779\n",
      "Epoch 2 step 616: training accuarcy: 0.919\n",
      "Epoch 2 step 616: training loss: 554.1358223744784\n",
      "Epoch 2 step 617: training accuarcy: 0.9165\n",
      "Epoch 2 step 617: training loss: 535.7007656990013\n",
      "Epoch 2 step 618: training accuarcy: 0.9245\n",
      "Epoch 2 step 618: training loss: 579.2454215074238\n",
      "Epoch 2 step 619: training accuarcy: 0.91\n",
      "Epoch 2 step 619: training loss: 526.315401210711\n",
      "Epoch 2 step 620: training accuarcy: 0.93\n",
      "Epoch 2 step 620: training loss: 580.4253487087597\n",
      "Epoch 2 step 621: training accuarcy: 0.9135\n",
      "Epoch 2 step 621: training loss: 518.3637119428789\n",
      "Epoch 2 step 622: training accuarcy: 0.9315\n",
      "Epoch 2 step 622: training loss: 597.7057524485797\n",
      "Epoch 2 step 623: training accuarcy: 0.917\n",
      "Epoch 2 step 623: training loss: 556.472290303627\n",
      "Epoch 2 step 624: training accuarcy: 0.9185\n",
      "Epoch 2 step 624: training loss: 527.9922578437536\n",
      "Epoch 2 step 625: training accuarcy: 0.932\n",
      "Epoch 2 step 625: training loss: 571.571323159963\n",
      "Epoch 2 step 626: training accuarcy: 0.918\n",
      "Epoch 2 step 626: training loss: 529.5731315361902\n",
      "Epoch 2 step 627: training accuarcy: 0.922\n",
      "Epoch 2 step 627: training loss: 539.7788083636132\n",
      "Epoch 2 step 628: training accuarcy: 0.9155\n",
      "Epoch 2 step 628: training loss: 544.9086590530759\n",
      "Epoch 2 step 629: training accuarcy: 0.9195\n",
      "Epoch 2 step 629: training loss: 566.8138042754888\n",
      "Epoch 2 step 630: training accuarcy: 0.912\n",
      "Epoch 2 step 630: training loss: 562.3784602758315\n",
      "Epoch 2 step 631: training accuarcy: 0.9195\n",
      "Epoch 2 step 631: training loss: 554.5966042911067\n",
      "Epoch 2 step 632: training accuarcy: 0.9265\n",
      "Epoch 2 step 632: training loss: 552.9489638105629\n",
      "Epoch 2 step 633: training accuarcy: 0.9205\n",
      "Epoch 2 step 633: training loss: 558.1687693432568\n",
      "Epoch 2 step 634: training accuarcy: 0.9255\n",
      "Epoch 2 step 634: training loss: 555.0679380116896\n",
      "Epoch 2 step 635: training accuarcy: 0.926\n",
      "Epoch 2 step 635: training loss: 543.7274917337317\n",
      "Epoch 2 step 636: training accuarcy: 0.922\n",
      "Epoch 2 step 636: training loss: 525.9953170532315\n",
      "Epoch 2 step 637: training accuarcy: 0.928\n",
      "Epoch 2 step 637: training loss: 530.2973860414402\n",
      "Epoch 2 step 638: training accuarcy: 0.9255\n",
      "Epoch 2 step 638: training loss: 543.5325582156836\n",
      "Epoch 2 step 639: training accuarcy: 0.923\n",
      "Epoch 2 step 639: training loss: 544.6814890176131\n",
      "Epoch 2 step 640: training accuarcy: 0.925\n",
      "Epoch 2 step 640: training loss: 558.6597497951277\n",
      "Epoch 2 step 641: training accuarcy: 0.9215\n",
      "Epoch 2 step 641: training loss: 536.857145180175\n",
      "Epoch 2 step 642: training accuarcy: 0.919\n",
      "Epoch 2 step 642: training loss: 562.2117229390972\n",
      "Epoch 2 step 643: training accuarcy: 0.9175\n",
      "Epoch 2 step 643: training loss: 511.4389337337857\n",
      "Epoch 2 step 644: training accuarcy: 0.9295\n",
      "Epoch 2 step 644: training loss: 533.613961937062\n",
      "Epoch 2 step 645: training accuarcy: 0.927\n",
      "Epoch 2 step 645: training loss: 569.3164721076737\n",
      "Epoch 2 step 646: training accuarcy: 0.914\n",
      "Epoch 2 step 646: training loss: 554.2778286125151\n",
      "Epoch 2 step 647: training accuarcy: 0.923\n",
      "Epoch 2 step 647: training loss: 525.4012253224784\n",
      "Epoch 2 step 648: training accuarcy: 0.923\n",
      "Epoch 2 step 648: training loss: 591.1834235417712\n",
      "Epoch 2 step 649: training accuarcy: 0.914\n",
      "Epoch 2 step 649: training loss: 559.1172589461535\n",
      "Epoch 2 step 650: training accuarcy: 0.9245\n",
      "Epoch 2 step 650: training loss: 552.5077809443322\n",
      "Epoch 2 step 651: training accuarcy: 0.917\n",
      "Epoch 2 step 651: training loss: 567.9717092731769\n",
      "Epoch 2 step 652: training accuarcy: 0.9185\n",
      "Epoch 2 step 652: training loss: 535.9826497881199\n",
      "Epoch 2 step 653: training accuarcy: 0.924\n",
      "Epoch 2 step 653: training loss: 570.4423070227531\n",
      "Epoch 2 step 654: training accuarcy: 0.9195\n",
      "Epoch 2 step 654: training loss: 527.1498758519164\n",
      "Epoch 2 step 655: training accuarcy: 0.927\n",
      "Epoch 2 step 655: training loss: 538.8976916123514\n",
      "Epoch 2 step 656: training accuarcy: 0.9245\n",
      "Epoch 2 step 656: training loss: 527.7072753479688\n",
      "Epoch 2 step 657: training accuarcy: 0.927\n",
      "Epoch 2 step 657: training loss: 529.9999976012781\n",
      "Epoch 2 step 658: training accuarcy: 0.928\n",
      "Epoch 2 step 658: training loss: 561.1483501919892\n",
      "Epoch 2 step 659: training accuarcy: 0.918\n",
      "Epoch 2 step 659: training loss: 582.7077852480037\n",
      "Epoch 2 step 660: training accuarcy: 0.918\n",
      "Epoch 2 step 660: training loss: 542.7780842284233\n",
      "Epoch 2 step 661: training accuarcy: 0.923\n",
      "Epoch 2 step 661: training loss: 540.511296425625\n",
      "Epoch 2 step 662: training accuarcy: 0.922\n",
      "Epoch 2 step 662: training loss: 547.447992719709\n",
      "Epoch 2 step 663: training accuarcy: 0.9215\n",
      "Epoch 2 step 663: training loss: 556.2199632820738\n",
      "Epoch 2 step 664: training accuarcy: 0.9235\n",
      "Epoch 2 step 664: training loss: 531.1493117802386\n",
      "Epoch 2 step 665: training accuarcy: 0.9325\n",
      "Epoch 2 step 665: training loss: 536.5520421506748\n",
      "Epoch 2 step 666: training accuarcy: 0.9215\n",
      "Epoch 2 step 666: training loss: 551.6279949243007\n",
      "Epoch 2 step 667: training accuarcy: 0.9235\n",
      "Epoch 2 step 667: training loss: 554.7323909601469\n",
      "Epoch 2 step 668: training accuarcy: 0.9235\n",
      "Epoch 2 step 668: training loss: 541.9926067137646\n",
      "Epoch 2 step 669: training accuarcy: 0.923\n",
      "Epoch 2 step 669: training loss: 547.5138894286443\n",
      "Epoch 2 step 670: training accuarcy: 0.922\n",
      "Epoch 2 step 670: training loss: 536.8594106807618\n",
      "Epoch 2 step 671: training accuarcy: 0.9205\n",
      "Epoch 2 step 671: training loss: 512.1836620766254\n",
      "Epoch 2 step 672: training accuarcy: 0.9325\n",
      "Epoch 2 step 672: training loss: 545.6423907941722\n",
      "Epoch 2 step 673: training accuarcy: 0.92\n",
      "Epoch 2 step 673: training loss: 524.9067007953655\n",
      "Epoch 2 step 674: training accuarcy: 0.932\n",
      "Epoch 2 step 674: training loss: 534.5039825274099\n",
      "Epoch 2 step 675: training accuarcy: 0.923\n",
      "Epoch 2 step 675: training loss: 552.7446616025609\n",
      "Epoch 2 step 676: training accuarcy: 0.9155\n",
      "Epoch 2 step 676: training loss: 552.7254350686418\n",
      "Epoch 2 step 677: training accuarcy: 0.9255\n",
      "Epoch 2 step 677: training loss: 543.9202763414179\n",
      "Epoch 2 step 678: training accuarcy: 0.9275\n",
      "Epoch 2 step 678: training loss: 550.011089629697\n",
      "Epoch 2 step 679: training accuarcy: 0.9195\n",
      "Epoch 2 step 679: training loss: 560.2845838161873\n",
      "Epoch 2 step 680: training accuarcy: 0.9185\n",
      "Epoch 2 step 680: training loss: 544.3501219748916\n",
      "Epoch 2 step 681: training accuarcy: 0.923\n",
      "Epoch 2 step 681: training loss: 525.3933101131762\n",
      "Epoch 2 step 682: training accuarcy: 0.924\n",
      "Epoch 2 step 682: training loss: 529.3667269229093\n",
      "Epoch 2 step 683: training accuarcy: 0.9235\n",
      "Epoch 2 step 683: training loss: 567.7343933786257\n",
      "Epoch 2 step 684: training accuarcy: 0.9205\n",
      "Epoch 2 step 684: training loss: 536.0890032673074\n",
      "Epoch 2 step 685: training accuarcy: 0.9285\n",
      "Epoch 2 step 685: training loss: 573.064539337277\n",
      "Epoch 2 step 686: training accuarcy: 0.9055\n",
      "Epoch 2 step 686: training loss: 529.5789952089849\n",
      "Epoch 2 step 687: training accuarcy: 0.9285\n",
      "Epoch 2 step 687: training loss: 566.4747086898509\n",
      "Epoch 2 step 688: training accuarcy: 0.916\n",
      "Epoch 2 step 688: training loss: 530.6567087871653\n",
      "Epoch 2 step 689: training accuarcy: 0.924\n",
      "Epoch 2 step 689: training loss: 531.2079936768736\n",
      "Epoch 2 step 690: training accuarcy: 0.926\n",
      "Epoch 2 step 690: training loss: 536.5356753116614\n",
      "Epoch 2 step 691: training accuarcy: 0.925\n",
      "Epoch 2 step 691: training loss: 545.5264015481692\n",
      "Epoch 2 step 692: training accuarcy: 0.9165\n",
      "Epoch 2 step 692: training loss: 536.4393773680379\n",
      "Epoch 2 step 693: training accuarcy: 0.929\n",
      "Epoch 2 step 693: training loss: 543.3878565411047\n",
      "Epoch 2 step 694: training accuarcy: 0.9245\n",
      "Epoch 2 step 694: training loss: 565.5221814843001\n",
      "Epoch 2 step 695: training accuarcy: 0.92\n",
      "Epoch 2 step 695: training loss: 552.0454786772095\n",
      "Epoch 2 step 696: training accuarcy: 0.9245\n",
      "Epoch 2 step 696: training loss: 563.5602726202322\n",
      "Epoch 2 step 697: training accuarcy: 0.9195\n",
      "Epoch 2 step 697: training loss: 547.1963851328014\n",
      "Epoch 2 step 698: training accuarcy: 0.9235\n",
      "Epoch 2 step 698: training loss: 531.3837264534611\n",
      "Epoch 2 step 699: training accuarcy: 0.9295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 699: training loss: 546.2942293102979\n",
      "Epoch 2 step 700: training accuarcy: 0.9215\n",
      "Epoch 2 step 700: training loss: 557.2768137697982\n",
      "Epoch 2 step 701: training accuarcy: 0.919\n",
      "Epoch 2 step 701: training loss: 542.0525564088518\n",
      "Epoch 2 step 702: training accuarcy: 0.925\n",
      "Epoch 2 step 702: training loss: 582.9115506085553\n",
      "Epoch 2 step 703: training accuarcy: 0.9185\n",
      "Epoch 2 step 703: training loss: 518.5597576352862\n",
      "Epoch 2 step 704: training accuarcy: 0.9205\n",
      "Epoch 2 step 704: training loss: 539.4045196849677\n",
      "Epoch 2 step 705: training accuarcy: 0.916\n",
      "Epoch 2 step 705: training loss: 515.2406695714028\n",
      "Epoch 2 step 706: training accuarcy: 0.934\n",
      "Epoch 2 step 706: training loss: 533.4209577365324\n",
      "Epoch 2 step 707: training accuarcy: 0.932\n",
      "Epoch 2 step 707: training loss: 544.621908566643\n",
      "Epoch 2 step 708: training accuarcy: 0.9215\n",
      "Epoch 2 step 708: training loss: 507.03076866304343\n",
      "Epoch 2 step 709: training accuarcy: 0.9385\n",
      "Epoch 2 step 709: training loss: 499.1550378772791\n",
      "Epoch 2 step 710: training accuarcy: 0.932\n",
      "Epoch 2 step 710: training loss: 560.8145013171255\n",
      "Epoch 2 step 711: training accuarcy: 0.921\n",
      "Epoch 2 step 711: training loss: 533.3661485698524\n",
      "Epoch 2 step 712: training accuarcy: 0.9265\n",
      "Epoch 2 step 712: training loss: 506.35911308627584\n",
      "Epoch 2 step 713: training accuarcy: 0.935\n",
      "Epoch 2 step 713: training loss: 552.536078039427\n",
      "Epoch 2 step 714: training accuarcy: 0.9235\n",
      "Epoch 2 step 714: training loss: 516.6730579935038\n",
      "Epoch 2 step 715: training accuarcy: 0.931\n",
      "Epoch 2 step 715: training loss: 515.3001042756448\n",
      "Epoch 2 step 716: training accuarcy: 0.9375\n",
      "Epoch 2 step 716: training loss: 564.721460473176\n",
      "Epoch 2 step 717: training accuarcy: 0.921\n",
      "Epoch 2 step 717: training loss: 525.8104638306897\n",
      "Epoch 2 step 718: training accuarcy: 0.9225\n",
      "Epoch 2 step 718: training loss: 527.3570345550357\n",
      "Epoch 2 step 719: training accuarcy: 0.93\n",
      "Epoch 2 step 719: training loss: 526.3800658537965\n",
      "Epoch 2 step 720: training accuarcy: 0.924\n",
      "Epoch 2 step 720: training loss: 539.6421617545609\n",
      "Epoch 2 step 721: training accuarcy: 0.92\n",
      "Epoch 2 step 721: training loss: 543.9836679602965\n",
      "Epoch 2 step 722: training accuarcy: 0.922\n",
      "Epoch 2 step 722: training loss: 533.145493134141\n",
      "Epoch 2 step 723: training accuarcy: 0.924\n",
      "Epoch 2 step 723: training loss: 545.6501146222125\n",
      "Epoch 2 step 724: training accuarcy: 0.918\n",
      "Epoch 2 step 724: training loss: 537.4909528043688\n",
      "Epoch 2 step 725: training accuarcy: 0.9205\n",
      "Epoch 2 step 725: training loss: 563.6727907567831\n",
      "Epoch 2 step 726: training accuarcy: 0.9145\n",
      "Epoch 2 step 726: training loss: 514.7182658841457\n",
      "Epoch 2 step 727: training accuarcy: 0.9305\n",
      "Epoch 2 step 727: training loss: 544.479892294484\n",
      "Epoch 2 step 728: training accuarcy: 0.92\n",
      "Epoch 2 step 728: training loss: 575.98853932324\n",
      "Epoch 2 step 729: training accuarcy: 0.9155\n",
      "Epoch 2 step 729: training loss: 525.4474104369269\n",
      "Epoch 2 step 730: training accuarcy: 0.9275\n",
      "Epoch 2 step 730: training loss: 530.5532146999715\n",
      "Epoch 2 step 731: training accuarcy: 0.9225\n",
      "Epoch 2 step 731: training loss: 536.444768112309\n",
      "Epoch 2 step 732: training accuarcy: 0.9225\n",
      "Epoch 2 step 732: training loss: 550.3001401776228\n",
      "Epoch 2 step 733: training accuarcy: 0.917\n",
      "Epoch 2 step 733: training loss: 523.9619650014572\n",
      "Epoch 2 step 734: training accuarcy: 0.926\n",
      "Epoch 2 step 734: training loss: 524.2402501907729\n",
      "Epoch 2 step 735: training accuarcy: 0.9275\n",
      "Epoch 2 step 735: training loss: 529.8351056050848\n",
      "Epoch 2 step 736: training accuarcy: 0.929\n",
      "Epoch 2 step 736: training loss: 521.4299430409\n",
      "Epoch 2 step 737: training accuarcy: 0.931\n",
      "Epoch 2 step 737: training loss: 524.2542481488356\n",
      "Epoch 2 step 738: training accuarcy: 0.931\n",
      "Epoch 2 step 738: training loss: 530.4511274664065\n",
      "Epoch 2 step 739: training accuarcy: 0.927\n",
      "Epoch 2 step 739: training loss: 513.8273457981801\n",
      "Epoch 2 step 740: training accuarcy: 0.924\n",
      "Epoch 2 step 740: training loss: 536.3283610721658\n",
      "Epoch 2 step 741: training accuarcy: 0.924\n",
      "Epoch 2 step 741: training loss: 529.5393722109729\n",
      "Epoch 2 step 742: training accuarcy: 0.9275\n",
      "Epoch 2 step 742: training loss: 553.4914335911296\n",
      "Epoch 2 step 743: training accuarcy: 0.9205\n",
      "Epoch 2 step 743: training loss: 523.9944070437504\n",
      "Epoch 2 step 744: training accuarcy: 0.9255\n",
      "Epoch 2 step 744: training loss: 556.5505159655434\n",
      "Epoch 2 step 745: training accuarcy: 0.923\n",
      "Epoch 2 step 745: training loss: 531.3289274611878\n",
      "Epoch 2 step 746: training accuarcy: 0.923\n",
      "Epoch 2 step 746: training loss: 520.8837148613845\n",
      "Epoch 2 step 747: training accuarcy: 0.9225\n",
      "Epoch 2 step 747: training loss: 527.6035900358755\n",
      "Epoch 2 step 748: training accuarcy: 0.93\n",
      "Epoch 2 step 748: training loss: 538.3182189051175\n",
      "Epoch 2 step 749: training accuarcy: 0.9175\n",
      "Epoch 2 step 749: training loss: 543.9670825059557\n",
      "Epoch 2 step 750: training accuarcy: 0.9205\n",
      "Epoch 2 step 750: training loss: 545.8432687195216\n",
      "Epoch 2 step 751: training accuarcy: 0.928\n",
      "Epoch 2 step 751: training loss: 521.8717394132696\n",
      "Epoch 2 step 752: training accuarcy: 0.9255\n",
      "Epoch 2 step 752: training loss: 579.4882131691588\n",
      "Epoch 2 step 753: training accuarcy: 0.9195\n",
      "Epoch 2 step 753: training loss: 526.8730737025381\n",
      "Epoch 2 step 754: training accuarcy: 0.9275\n",
      "Epoch 2 step 754: training loss: 504.0856688740739\n",
      "Epoch 2 step 755: training accuarcy: 0.9275\n",
      "Epoch 2 step 755: training loss: 527.6637365209372\n",
      "Epoch 2 step 756: training accuarcy: 0.926\n",
      "Epoch 2 step 756: training loss: 547.7824419250597\n",
      "Epoch 2 step 757: training accuarcy: 0.924\n",
      "Epoch 2 step 757: training loss: 552.7737855018219\n",
      "Epoch 2 step 758: training accuarcy: 0.919\n",
      "Epoch 2 step 758: training loss: 541.3670118330901\n",
      "Epoch 2 step 759: training accuarcy: 0.9165\n",
      "Epoch 2 step 759: training loss: 533.2515336991071\n",
      "Epoch 2 step 760: training accuarcy: 0.925\n",
      "Epoch 2 step 760: training loss: 548.4953880372731\n",
      "Epoch 2 step 761: training accuarcy: 0.9205\n",
      "Epoch 2 step 761: training loss: 540.8396328767009\n",
      "Epoch 2 step 762: training accuarcy: 0.924\n",
      "Epoch 2 step 762: training loss: 535.8022001526456\n",
      "Epoch 2 step 763: training accuarcy: 0.9255\n",
      "Epoch 2 step 763: training loss: 547.1594169574449\n",
      "Epoch 2 step 764: training accuarcy: 0.9155\n",
      "Epoch 2 step 764: training loss: 533.5321566312434\n",
      "Epoch 2 step 765: training accuarcy: 0.926\n",
      "Epoch 2 step 765: training loss: 497.5790386012957\n",
      "Epoch 2 step 766: training accuarcy: 0.927\n",
      "Epoch 2 step 766: training loss: 522.9554344344258\n",
      "Epoch 2 step 767: training accuarcy: 0.9305\n",
      "Epoch 2 step 767: training loss: 525.9933480555876\n",
      "Epoch 2 step 768: training accuarcy: 0.9285\n",
      "Epoch 2 step 768: training loss: 525.6959592228415\n",
      "Epoch 2 step 769: training accuarcy: 0.929\n",
      "Epoch 2 step 769: training loss: 551.4467498347748\n",
      "Epoch 2 step 770: training accuarcy: 0.925\n",
      "Epoch 2 step 770: training loss: 589.7300299669728\n",
      "Epoch 2 step 771: training accuarcy: 0.918\n",
      "Epoch 2 step 771: training loss: 515.0455547968011\n",
      "Epoch 2 step 772: training accuarcy: 0.925\n",
      "Epoch 2 step 772: training loss: 533.6421807960488\n",
      "Epoch 2 step 773: training accuarcy: 0.9205\n",
      "Epoch 2 step 773: training loss: 535.8609828579333\n",
      "Epoch 2 step 774: training accuarcy: 0.9265\n",
      "Epoch 2 step 774: training loss: 565.3072886405423\n",
      "Epoch 2 step 775: training accuarcy: 0.9195\n",
      "Epoch 2 step 775: training loss: 507.95735207249055\n",
      "Epoch 2 step 776: training accuarcy: 0.9345\n",
      "Epoch 2 step 776: training loss: 535.5617590820946\n",
      "Epoch 2 step 777: training accuarcy: 0.917\n",
      "Epoch 2 step 777: training loss: 535.8128923606824\n",
      "Epoch 2 step 778: training accuarcy: 0.922\n",
      "Epoch 2 step 778: training loss: 519.3461530620928\n",
      "Epoch 2 step 779: training accuarcy: 0.9275\n",
      "Epoch 2 step 779: training loss: 523.0119957115813\n",
      "Epoch 2 step 780: training accuarcy: 0.93\n",
      "Epoch 2 step 780: training loss: 532.1588792894438\n",
      "Epoch 2 step 781: training accuarcy: 0.9165\n",
      "Epoch 2 step 781: training loss: 520.1112800494654\n",
      "Epoch 2 step 782: training accuarcy: 0.924\n",
      "Epoch 2 step 782: training loss: 517.4205658244705\n",
      "Epoch 2 step 783: training accuarcy: 0.933\n",
      "Epoch 2 step 783: training loss: 506.21382994031796\n",
      "Epoch 2 step 784: training accuarcy: 0.9355\n",
      "Epoch 2 step 784: training loss: 511.1183799007774\n",
      "Epoch 2 step 785: training accuarcy: 0.925\n",
      "Epoch 2 step 785: training loss: 513.0406880538405\n",
      "Epoch 2 step 786: training accuarcy: 0.928\n",
      "Epoch 2 step 786: training loss: 563.6471872797531\n",
      "Epoch 2 step 787: training accuarcy: 0.914\n",
      "Epoch 2 step 787: training loss: 519.7168812000252\n",
      "Epoch 2 step 788: training accuarcy: 0.9265\n",
      "Epoch 2 step 788: training loss: 289.45460990756084\n",
      "Epoch 2 step 789: training accuarcy: 0.9320512820512821\n",
      "Epoch 2: train loss 542.7728228852129, train accuarcy 0.9106795787811279\n",
      "Epoch 2: valid loss 634.6902237137496, valid accuarcy 0.8951889872550964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [06:04<00:00, 121.49s/it]\n"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=3,\n",
    "                  log_dir=get_log_dir('topcoder', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:49:20.906892Z",
     "start_time": "2019-10-09T13:33:56.043932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 328693.0835166554\n",
      "Epoch 0 step 1: training accuarcy: 0.4978\n",
      "Epoch 0 step 1: training loss: 314058.7249141159\n",
      "Epoch 0 step 2: training accuarcy: 0.5168\n",
      "Epoch 0 step 2: training loss: 300176.1108415055\n",
      "Epoch 0 step 3: training accuarcy: 0.5159\n",
      "Epoch 0 step 3: training loss: 307014.09675405966\n",
      "Epoch 0 step 4: training accuarcy: 0.5114000000000001\n",
      "Epoch 0 step 4: training loss: 297557.54419490095\n",
      "Epoch 0 step 5: training accuarcy: 0.5051\n",
      "Epoch 0 step 5: training loss: 291071.6806946936\n",
      "Epoch 0 step 6: training accuarcy: 0.5202\n",
      "Epoch 0 step 6: training loss: 281828.43845424784\n",
      "Epoch 0 step 7: training accuarcy: 0.5189\n",
      "Epoch 0 step 7: training loss: 268520.46315127116\n",
      "Epoch 0 step 8: training accuarcy: 0.5279\n",
      "Epoch 0 step 8: training loss: 272973.084331861\n",
      "Epoch 0 step 9: training accuarcy: 0.5239\n",
      "Epoch 0 step 9: training loss: 262233.4747713839\n",
      "Epoch 0 step 10: training accuarcy: 0.5155000000000001\n",
      "Epoch 0 step 10: training loss: 253659.75581515126\n",
      "Epoch 0 step 11: training accuarcy: 0.5236000000000001\n",
      "Epoch 0 step 11: training loss: 245521.83561244112\n",
      "Epoch 0 step 12: training accuarcy: 0.5334\n",
      "Epoch 0 step 12: training loss: 238246.9712663459\n",
      "Epoch 0 step 13: training accuarcy: 0.5327000000000001\n",
      "Epoch 0 step 13: training loss: 227894.65987047972\n",
      "Epoch 0 step 14: training accuarcy: 0.5297000000000001\n",
      "Epoch 0 step 14: training loss: 224434.26415784127\n",
      "Epoch 0 step 15: training accuarcy: 0.5265000000000001\n",
      "Epoch 0 step 15: training loss: 227920.68982185336\n",
      "Epoch 0 step 16: training accuarcy: 0.5185000000000001\n",
      "Epoch 0 step 16: training loss: 213573.65249683443\n",
      "Epoch 0 step 17: training accuarcy: 0.5199\n",
      "Epoch 0 step 17: training loss: 205760.82216515916\n",
      "Epoch 0 step 18: training accuarcy: 0.5297000000000001\n",
      "Epoch 0 step 18: training loss: 195993.21968429457\n",
      "Epoch 0 step 19: training accuarcy: 0.5378000000000001\n",
      "Epoch 0 step 19: training loss: 190672.09664731997\n",
      "Epoch 0 step 20: training accuarcy: 0.5333\n",
      "Epoch 0 step 20: training loss: 187184.00941107835\n",
      "Epoch 0 step 21: training accuarcy: 0.5385\n",
      "Epoch 0 step 21: training loss: 172272.62197615643\n",
      "Epoch 0 step 22: training accuarcy: 0.5393\n",
      "Epoch 0 step 22: training loss: 171427.09876749298\n",
      "Epoch 0 step 23: training accuarcy: 0.5358\n",
      "Epoch 0 step 23: training loss: 158777.58968002483\n",
      "Epoch 0 step 24: training accuarcy: 0.5383\n",
      "Epoch 0 step 24: training loss: 159199.56134566866\n",
      "Epoch 0 step 25: training accuarcy: 0.5339\n",
      "Epoch 0 step 25: training loss: 149505.90415560076\n",
      "Epoch 0 step 26: training accuarcy: 0.5483\n",
      "Epoch 0 step 26: training loss: 145433.9539670169\n",
      "Epoch 0 step 27: training accuarcy: 0.5367000000000001\n",
      "Epoch 0 step 27: training loss: 145692.31919362565\n",
      "Epoch 0 step 28: training accuarcy: 0.5258\n",
      "Epoch 0 step 28: training loss: 134964.44540080492\n",
      "Epoch 0 step 29: training accuarcy: 0.5492\n",
      "Epoch 0 step 29: training loss: 129311.61446401285\n",
      "Epoch 0 step 30: training accuarcy: 0.5385\n",
      "Epoch 0 step 30: training loss: 124854.12180417485\n",
      "Epoch 0 step 31: training accuarcy: 0.5378000000000001\n",
      "Epoch 0 step 31: training loss: 120776.99816914114\n",
      "Epoch 0 step 32: training accuarcy: 0.5351\n",
      "Epoch 0 step 32: training loss: 113383.27987170972\n",
      "Epoch 0 step 33: training accuarcy: 0.5349\n",
      "Epoch 0 step 33: training loss: 104671.77793726545\n",
      "Epoch 0 step 34: training accuarcy: 0.5535\n",
      "Epoch 0 step 34: training loss: 102026.50746937367\n",
      "Epoch 0 step 35: training accuarcy: 0.5508000000000001\n",
      "Epoch 0 step 35: training loss: 96502.41438937956\n",
      "Epoch 0 step 36: training accuarcy: 0.5513\n",
      "Epoch 0 step 36: training loss: 93163.29043214142\n",
      "Epoch 0 step 37: training accuarcy: 0.5543\n",
      "Epoch 0 step 37: training loss: 91378.73037651011\n",
      "Epoch 0 step 38: training accuarcy: 0.5443\n",
      "Epoch 0 step 38: training loss: 84508.43171610298\n",
      "Epoch 0 step 39: training accuarcy: 0.5549000000000001\n",
      "Epoch 0 step 39: training loss: 80861.76837788319\n",
      "Epoch 0 step 40: training accuarcy: 0.5604\n",
      "Epoch 0 step 40: training loss: 76270.91579029309\n",
      "Epoch 0 step 41: training accuarcy: 0.5587\n",
      "Epoch 0 step 41: training loss: 74229.96361268911\n",
      "Epoch 0 step 42: training accuarcy: 0.5503\n",
      "Epoch 0 step 42: training loss: 71542.3568061638\n",
      "Epoch 0 step 43: training accuarcy: 0.5643\n",
      "Epoch 0 step 43: training loss: 67580.12355452834\n",
      "Epoch 0 step 44: training accuarcy: 0.5711\n",
      "Epoch 0 step 44: training loss: 64672.12101786064\n",
      "Epoch 0 step 45: training accuarcy: 0.5921000000000001\n",
      "Epoch 0 step 45: training loss: 62371.28063935337\n",
      "Epoch 0 step 46: training accuarcy: 0.5935\n",
      "Epoch 0 step 46: training loss: 60545.06577266203\n",
      "Epoch 0 step 47: training accuarcy: 0.6004\n",
      "Epoch 0 step 47: training loss: 59075.751157979736\n",
      "Epoch 0 step 48: training accuarcy: 0.6023000000000001\n",
      "Epoch 0 step 48: training loss: 58136.17421056572\n",
      "Epoch 0 step 49: training accuarcy: 0.6121\n",
      "Epoch 0 step 49: training loss: 57138.17813150489\n",
      "Epoch 0 step 50: training accuarcy: 0.6117\n",
      "Epoch 0 step 50: training loss: 54837.24530362412\n",
      "Epoch 0 step 51: training accuarcy: 0.6217\n",
      "Epoch 0 step 51: training loss: 54091.30360110415\n",
      "Epoch 0 step 52: training accuarcy: 0.6301\n",
      "Epoch 0 step 52: training loss: 51809.87140204931\n",
      "Epoch 0 step 53: training accuarcy: 0.6376000000000001\n",
      "Epoch 0 step 53: training loss: 50624.77967079912\n",
      "Epoch 0 step 54: training accuarcy: 0.6577000000000001\n",
      "Epoch 0 step 54: training loss: 50048.496274538964\n",
      "Epoch 0 step 55: training accuarcy: 0.661\n",
      "Epoch 0 step 55: training loss: 49833.90638095401\n",
      "Epoch 0 step 56: training accuarcy: 0.6625\n",
      "Epoch 0 step 56: training loss: 49266.03298078814\n",
      "Epoch 0 step 57: training accuarcy: 0.6669\n",
      "Epoch 0 step 57: training loss: 47676.435443680435\n",
      "Epoch 0 step 58: training accuarcy: 0.6751\n",
      "Epoch 0 step 58: training loss: 46773.99221770922\n",
      "Epoch 0 step 59: training accuarcy: 0.6849000000000001\n",
      "Epoch 0 step 59: training loss: 46109.936139697755\n",
      "Epoch 0 step 60: training accuarcy: 0.6942\n",
      "Epoch 0 step 60: training loss: 45703.45067339204\n",
      "Epoch 0 step 61: training accuarcy: 0.6922\n",
      "Epoch 0 step 61: training loss: 45085.69718524005\n",
      "Epoch 0 step 62: training accuarcy: 0.6985\n",
      "Epoch 0 step 62: training loss: 44516.40051346527\n",
      "Epoch 0 step 63: training accuarcy: 0.7000000000000001\n",
      "Epoch 0 step 63: training loss: 44075.387228416774\n",
      "Epoch 0 step 64: training accuarcy: 0.7015\n",
      "Epoch 0 step 64: training loss: 43474.38330984606\n",
      "Epoch 0 step 65: training accuarcy: 0.7144\n",
      "Epoch 0 step 65: training loss: 42640.62684665913\n",
      "Epoch 0 step 66: training accuarcy: 0.7192000000000001\n",
      "Epoch 0 step 66: training loss: 42894.610858579734\n",
      "Epoch 0 step 67: training accuarcy: 0.7146\n",
      "Epoch 0 step 67: training loss: 42640.2568469846\n",
      "Epoch 0 step 68: training accuarcy: 0.7170000000000001\n",
      "Epoch 0 step 68: training loss: 42327.94354538612\n",
      "Epoch 0 step 69: training accuarcy: 0.7318\n",
      "Epoch 0 step 69: training loss: 41480.36396119179\n",
      "Epoch 0 step 70: training accuarcy: 0.7292000000000001\n",
      "Epoch 0 step 70: training loss: 41934.09061717037\n",
      "Epoch 0 step 71: training accuarcy: 0.7256\n",
      "Epoch 0 step 71: training loss: 41292.09739420544\n",
      "Epoch 0 step 72: training accuarcy: 0.7315\n",
      "Epoch 0 step 72: training loss: 40870.481203908814\n",
      "Epoch 0 step 73: training accuarcy: 0.7305\n",
      "Epoch 0 step 73: training loss: 41161.012993446246\n",
      "Epoch 0 step 74: training accuarcy: 0.7293000000000001\n",
      "Epoch 0 step 74: training loss: 39507.157358965494\n",
      "Epoch 0 step 75: training accuarcy: 0.7423000000000001\n",
      "Epoch 0 step 75: training loss: 40427.393459666106\n",
      "Epoch 0 step 76: training accuarcy: 0.7366\n",
      "Epoch 0 step 76: training loss: 39375.65041998311\n",
      "Epoch 0 step 77: training accuarcy: 0.7422000000000001\n",
      "Epoch 0 step 77: training loss: 39678.26832467686\n",
      "Epoch 0 step 78: training accuarcy: 0.7461\n",
      "Epoch 0 step 78: training loss: 39589.59979600123\n",
      "Epoch 0 step 79: training accuarcy: 0.7381000000000001\n",
      "Epoch 0 step 79: training loss: 37650.88392499117\n",
      "Epoch 0 step 80: training accuarcy: 0.7604000000000001\n",
      "Epoch 0 step 80: training loss: 37870.03841772117\n",
      "Epoch 0 step 81: training accuarcy: 0.7557\n",
      "Epoch 0 step 81: training loss: 37779.345607288196\n",
      "Epoch 0 step 82: training accuarcy: 0.7622\n",
      "Epoch 0 step 82: training loss: 38863.920313287315\n",
      "Epoch 0 step 83: training accuarcy: 0.75\n",
      "Epoch 0 step 83: training loss: 36965.35208052884\n",
      "Epoch 0 step 84: training accuarcy: 0.7696000000000001\n",
      "Epoch 0 step 84: training loss: 37021.59833738739\n",
      "Epoch 0 step 85: training accuarcy: 0.7614000000000001\n",
      "Epoch 0 step 85: training loss: 37115.563093654804\n",
      "Epoch 0 step 86: training accuarcy: 0.7664000000000001\n",
      "Epoch 0 step 86: training loss: 36621.16092949527\n",
      "Epoch 0 step 87: training accuarcy: 0.7632\n",
      "Epoch 0 step 87: training loss: 36754.84782784991\n",
      "Epoch 0 step 88: training accuarcy: 0.7665000000000001\n",
      "Epoch 0 step 88: training loss: 37244.81074448023\n",
      "Epoch 0 step 89: training accuarcy: 0.7556\n",
      "Epoch 0 step 89: training loss: 36105.546669099494\n",
      "Epoch 0 step 90: training accuarcy: 0.7735000000000001\n",
      "Epoch 0 step 90: training loss: 36084.91944435393\n",
      "Epoch 0 step 91: training accuarcy: 0.7739\n",
      "Epoch 0 step 91: training loss: 35836.71114717996\n",
      "Epoch 0 step 92: training accuarcy: 0.7769\n",
      "Epoch 0 step 92: training loss: 35005.81801922584\n",
      "Epoch 0 step 93: training accuarcy: 0.7817000000000001\n",
      "Epoch 0 step 93: training loss: 35921.09514532122\n",
      "Epoch 0 step 94: training accuarcy: 0.7675000000000001\n",
      "Epoch 0 step 94: training loss: 34892.377483794815\n",
      "Epoch 0 step 95: training accuarcy: 0.7837000000000001\n",
      "Epoch 0 step 95: training loss: 35190.348962633565\n",
      "Epoch 0 step 96: training accuarcy: 0.7763\n",
      "Epoch 0 step 96: training loss: 34664.61664207566\n",
      "Epoch 0 step 97: training accuarcy: 0.7818\n",
      "Epoch 0 step 97: training loss: 34298.18086458065\n",
      "Epoch 0 step 98: training accuarcy: 0.7838\n",
      "Epoch 0 step 98: training loss: 34591.7176541692\n",
      "Epoch 0 step 99: training accuarcy: 0.7801\n",
      "Epoch 0 step 99: training loss: 34534.19563403107\n",
      "Epoch 0 step 100: training accuarcy: 0.7839\n",
      "Epoch 0 step 100: training loss: 33151.52792662458\n",
      "Epoch 0 step 101: training accuarcy: 0.8033\n",
      "Epoch 0 step 101: training loss: 33388.38377388371\n",
      "Epoch 0 step 102: training accuarcy: 0.7957000000000001\n",
      "Epoch 0 step 102: training loss: 33797.416059046525\n",
      "Epoch 0 step 103: training accuarcy: 0.7909\n",
      "Epoch 0 step 103: training loss: 33249.53598475779\n",
      "Epoch 0 step 104: training accuarcy: 0.7961\n",
      "Epoch 0 step 104: training loss: 33337.51388175108\n",
      "Epoch 0 step 105: training accuarcy: 0.7925000000000001\n",
      "Epoch 0 step 105: training loss: 32922.2997865511\n",
      "Epoch 0 step 106: training accuarcy: 0.7943\n",
      "Epoch 0 step 106: training loss: 32127.37501537402\n",
      "Epoch 0 step 107: training accuarcy: 0.8067000000000001\n",
      "Epoch 0 step 107: training loss: 32324.64933183186\n",
      "Epoch 0 step 108: training accuarcy: 0.801\n",
      "Epoch 0 step 108: training loss: 32514.08550298521\n",
      "Epoch 0 step 109: training accuarcy: 0.7992\n",
      "Epoch 0 step 109: training loss: 31827.207208727275\n",
      "Epoch 0 step 110: training accuarcy: 0.8102\n",
      "Epoch 0 step 110: training loss: 31710.45452926231\n",
      "Epoch 0 step 111: training accuarcy: 0.8119000000000001\n",
      "Epoch 0 step 111: training loss: 31265.795152087623\n",
      "Epoch 0 step 112: training accuarcy: 0.8099000000000001\n",
      "Epoch 0 step 112: training loss: 32409.245731691808\n",
      "Epoch 0 step 113: training accuarcy: 0.795\n",
      "Epoch 0 step 113: training loss: 31399.44816112424\n",
      "Epoch 0 step 114: training accuarcy: 0.8162\n",
      "Epoch 0 step 114: training loss: 31354.407797106018\n",
      "Epoch 0 step 115: training accuarcy: 0.8161\n",
      "Epoch 0 step 115: training loss: 31112.195974522532\n",
      "Epoch 0 step 116: training accuarcy: 0.8115\n",
      "Epoch 0 step 116: training loss: 31272.424486263175\n",
      "Epoch 0 step 117: training accuarcy: 0.8088000000000001\n",
      "Epoch 0 step 117: training loss: 31055.09677289822\n",
      "Epoch 0 step 118: training accuarcy: 0.8072\n",
      "Epoch 0 step 118: training loss: 31428.048987459348\n",
      "Epoch 0 step 119: training accuarcy: 0.8088000000000001\n",
      "Epoch 0 step 119: training loss: 29901.96742354043\n",
      "Epoch 0 step 120: training accuarcy: 0.8194\n",
      "Epoch 0 step 120: training loss: 30637.25364083889\n",
      "Epoch 0 step 121: training accuarcy: 0.8243\n",
      "Epoch 0 step 121: training loss: 31337.58632480281\n",
      "Epoch 0 step 122: training accuarcy: 0.809\n",
      "Epoch 0 step 122: training loss: 30511.181612044464\n",
      "Epoch 0 step 123: training accuarcy: 0.8118000000000001\n",
      "Epoch 0 step 123: training loss: 30073.62021305813\n",
      "Epoch 0 step 124: training accuarcy: 0.8191\n",
      "Epoch 0 step 124: training loss: 30474.023257955487\n",
      "Epoch 0 step 125: training accuarcy: 0.8174\n",
      "Epoch 0 step 125: training loss: 30253.34197182329\n",
      "Epoch 0 step 126: training accuarcy: 0.8139000000000001\n",
      "Epoch 0 step 126: training loss: 30453.10745342641\n",
      "Epoch 0 step 127: training accuarcy: 0.8126\n",
      "Epoch 0 step 127: training loss: 29670.682457874944\n",
      "Epoch 0 step 128: training accuarcy: 0.8262\n",
      "Epoch 0 step 128: training loss: 30058.955638652882\n",
      "Epoch 0 step 129: training accuarcy: 0.8199000000000001\n",
      "Epoch 0 step 129: training loss: 29530.763274516754\n",
      "Epoch 0 step 130: training accuarcy: 0.8242\n",
      "Epoch 0 step 130: training loss: 29846.56711774723\n",
      "Epoch 0 step 131: training accuarcy: 0.8224\n",
      "Epoch 0 step 131: training loss: 29862.0137429422\n",
      "Epoch 0 step 132: training accuarcy: 0.8201\n",
      "Epoch 0 step 132: training loss: 28700.00492254748\n",
      "Epoch 0 step 133: training accuarcy: 0.8299000000000001\n",
      "Epoch 0 step 133: training loss: 29459.01618064181\n",
      "Epoch 0 step 134: training accuarcy: 0.8244\n",
      "Epoch 0 step 134: training loss: 29658.303617347534\n",
      "Epoch 0 step 135: training accuarcy: 0.8188000000000001\n",
      "Epoch 0 step 135: training loss: 28455.247530921413\n",
      "Epoch 0 step 136: training accuarcy: 0.8335\n",
      "Epoch 0 step 136: training loss: 28522.491304026018\n",
      "Epoch 0 step 137: training accuarcy: 0.8313\n",
      "Epoch 0 step 137: training loss: 28708.497263467645\n",
      "Epoch 0 step 138: training accuarcy: 0.8286\n",
      "Epoch 0 step 138: training loss: 28097.860681181926\n",
      "Epoch 0 step 139: training accuarcy: 0.8389000000000001\n",
      "Epoch 0 step 139: training loss: 28396.847991233568\n",
      "Epoch 0 step 140: training accuarcy: 0.8336\n",
      "Epoch 0 step 140: training loss: 28751.91912437087\n",
      "Epoch 0 step 141: training accuarcy: 0.8268000000000001\n",
      "Epoch 0 step 141: training loss: 27912.031003635508\n",
      "Epoch 0 step 142: training accuarcy: 0.8307\n",
      "Epoch 0 step 142: training loss: 28178.212753350235\n",
      "Epoch 0 step 143: training accuarcy: 0.8322\n",
      "Epoch 0 step 143: training loss: 28750.955910971898\n",
      "Epoch 0 step 144: training accuarcy: 0.8257\n",
      "Epoch 0 step 144: training loss: 28163.9598742466\n",
      "Epoch 0 step 145: training accuarcy: 0.8299000000000001\n",
      "Epoch 0 step 145: training loss: 27588.8401365808\n",
      "Epoch 0 step 146: training accuarcy: 0.8378\n",
      "Epoch 0 step 146: training loss: 28147.975560998355\n",
      "Epoch 0 step 147: training accuarcy: 0.8343\n",
      "Epoch 0 step 147: training loss: 27353.197831402475\n",
      "Epoch 0 step 148: training accuarcy: 0.8379000000000001\n",
      "Epoch 0 step 148: training loss: 28005.565622493872\n",
      "Epoch 0 step 149: training accuarcy: 0.8276\n",
      "Epoch 0 step 149: training loss: 27938.729398723735\n",
      "Epoch 0 step 150: training accuarcy: 0.8321000000000001\n",
      "Epoch 0 step 150: training loss: 27896.478546006547\n",
      "Epoch 0 step 151: training accuarcy: 0.8348\n",
      "Epoch 0 step 151: training loss: 27073.890614802913\n",
      "Epoch 0 step 152: training accuarcy: 0.8397\n",
      "Epoch 0 step 152: training loss: 27520.91852783539\n",
      "Epoch 0 step 153: training accuarcy: 0.8382000000000001\n",
      "Epoch 0 step 153: training loss: 26215.292882835467\n",
      "Epoch 0 step 154: training accuarcy: 0.8484\n",
      "Epoch 0 step 154: training loss: 26959.230122253437\n",
      "Epoch 0 step 155: training accuarcy: 0.8417\n",
      "Epoch 0 step 155: training loss: 26907.342279097604\n",
      "Epoch 0 step 156: training accuarcy: 0.8424\n",
      "Epoch 0 step 156: training loss: 27381.96165011591\n",
      "Epoch 0 step 157: training accuarcy: 0.8365\n",
      "Epoch 0 step 157: training loss: 26837.13106412734\n",
      "Epoch 0 step 158: training accuarcy: 0.8413\n",
      "Epoch 0 step 158: training loss: 26963.201004288458\n",
      "Epoch 0 step 159: training accuarcy: 0.8398\n",
      "Epoch 0 step 159: training loss: 26539.806750381413\n",
      "Epoch 0 step 160: training accuarcy: 0.8504\n",
      "Epoch 0 step 160: training loss: 27185.16556744732\n",
      "Epoch 0 step 161: training accuarcy: 0.8370000000000001\n",
      "Epoch 0 step 161: training loss: 26409.383840583996\n",
      "Epoch 0 step 162: training accuarcy: 0.8481000000000001\n",
      "Epoch 0 step 162: training loss: 26109.963129431428\n",
      "Epoch 0 step 163: training accuarcy: 0.8503000000000001\n",
      "Epoch 0 step 163: training loss: 26397.481592604876\n",
      "Epoch 0 step 164: training accuarcy: 0.8462000000000001\n",
      "Epoch 0 step 164: training loss: 26132.0445493035\n",
      "Epoch 0 step 165: training accuarcy: 0.8439000000000001\n",
      "Epoch 0 step 165: training loss: 26329.936642214634\n",
      "Epoch 0 step 166: training accuarcy: 0.8415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 166: training loss: 26601.093323936373\n",
      "Epoch 0 step 167: training accuarcy: 0.8469\n",
      "Epoch 0 step 167: training loss: 26099.55135480284\n",
      "Epoch 0 step 168: training accuarcy: 0.8402000000000001\n",
      "Epoch 0 step 168: training loss: 25822.06086071969\n",
      "Epoch 0 step 169: training accuarcy: 0.8493\n",
      "Epoch 0 step 169: training loss: 26762.53642552209\n",
      "Epoch 0 step 170: training accuarcy: 0.8391000000000001\n",
      "Epoch 0 step 170: training loss: 25803.678595341225\n",
      "Epoch 0 step 171: training accuarcy: 0.8464\n",
      "Epoch 0 step 171: training loss: 25582.729381492667\n",
      "Epoch 0 step 172: training accuarcy: 0.8496\n",
      "Epoch 0 step 172: training loss: 26239.21400536701\n",
      "Epoch 0 step 173: training accuarcy: 0.8427\n",
      "Epoch 0 step 173: training loss: 25087.77036797405\n",
      "Epoch 0 step 174: training accuarcy: 0.8572000000000001\n",
      "Epoch 0 step 174: training loss: 25512.108946995184\n",
      "Epoch 0 step 175: training accuarcy: 0.8507\n",
      "Epoch 0 step 175: training loss: 25481.058037329185\n",
      "Epoch 0 step 176: training accuarcy: 0.8503000000000001\n",
      "Epoch 0 step 176: training loss: 25035.6313160098\n",
      "Epoch 0 step 177: training accuarcy: 0.8539\n",
      "Epoch 0 step 177: training loss: 24389.98885709177\n",
      "Epoch 0 step 178: training accuarcy: 0.8582000000000001\n",
      "Epoch 0 step 178: training loss: 25113.025152775175\n",
      "Epoch 0 step 179: training accuarcy: 0.8503000000000001\n",
      "Epoch 0 step 179: training loss: 24727.849229329193\n",
      "Epoch 0 step 180: training accuarcy: 0.8572000000000001\n",
      "Epoch 0 step 180: training loss: 25077.575902157798\n",
      "Epoch 0 step 181: training accuarcy: 0.8509\n",
      "Epoch 0 step 181: training loss: 24725.919028914686\n",
      "Epoch 0 step 182: training accuarcy: 0.8529\n",
      "Epoch 0 step 182: training loss: 24931.83979289495\n",
      "Epoch 0 step 183: training accuarcy: 0.8504\n",
      "Epoch 0 step 183: training loss: 24934.429690475044\n",
      "Epoch 0 step 184: training accuarcy: 0.8574\n",
      "Epoch 0 step 184: training loss: 25167.139641304144\n",
      "Epoch 0 step 185: training accuarcy: 0.8513000000000001\n",
      "Epoch 0 step 185: training loss: 25037.454286393782\n",
      "Epoch 0 step 186: training accuarcy: 0.8473\n",
      "Epoch 0 step 186: training loss: 24369.23054396917\n",
      "Epoch 0 step 187: training accuarcy: 0.8594\n",
      "Epoch 0 step 187: training loss: 23970.81297475435\n",
      "Epoch 0 step 188: training accuarcy: 0.8654000000000001\n",
      "Epoch 0 step 188: training loss: 24691.177080681682\n",
      "Epoch 0 step 189: training accuarcy: 0.8505\n",
      "Epoch 0 step 189: training loss: 24527.129552071405\n",
      "Epoch 0 step 190: training accuarcy: 0.8528\n",
      "Epoch 0 step 190: training loss: 23814.340829846213\n",
      "Epoch 0 step 191: training accuarcy: 0.8632000000000001\n",
      "Epoch 0 step 191: training loss: 24340.968901255917\n",
      "Epoch 0 step 192: training accuarcy: 0.8543000000000001\n",
      "Epoch 0 step 192: training loss: 23986.72111691603\n",
      "Epoch 0 step 193: training accuarcy: 0.8598\n",
      "Epoch 0 step 193: training loss: 24440.873944381852\n",
      "Epoch 0 step 194: training accuarcy: 0.8557\n",
      "Epoch 0 step 194: training loss: 24089.20448901858\n",
      "Epoch 0 step 195: training accuarcy: 0.8566\n",
      "Epoch 0 step 195: training loss: 24001.374953513652\n",
      "Epoch 0 step 196: training accuarcy: 0.8584\n",
      "Epoch 0 step 196: training loss: 24226.372424903995\n",
      "Epoch 0 step 197: training accuarcy: 0.8588\n",
      "Epoch 0 step 197: training loss: 23382.13720202564\n",
      "Epoch 0 step 198: training accuarcy: 0.8662000000000001\n",
      "Epoch 0 step 198: training loss: 24021.78489632673\n",
      "Epoch 0 step 199: training accuarcy: 0.8501000000000001\n",
      "Epoch 0 step 199: training loss: 23942.690122445365\n",
      "Epoch 0 step 200: training accuarcy: 0.8582000000000001\n",
      "Epoch 0 step 200: training loss: 23781.370632143677\n",
      "Epoch 0 step 201: training accuarcy: 0.8592000000000001\n",
      "Epoch 0 step 201: training loss: 24336.552916998953\n",
      "Epoch 0 step 202: training accuarcy: 0.8518\n",
      "Epoch 0 step 202: training loss: 23749.726943766236\n",
      "Epoch 0 step 203: training accuarcy: 0.8560000000000001\n",
      "Epoch 0 step 203: training loss: 22917.162674961408\n",
      "Epoch 0 step 204: training accuarcy: 0.8688\n",
      "Epoch 0 step 204: training loss: 23534.263480628277\n",
      "Epoch 0 step 205: training accuarcy: 0.8648\n",
      "Epoch 0 step 205: training loss: 23540.825699690056\n",
      "Epoch 0 step 206: training accuarcy: 0.8652000000000001\n",
      "Epoch 0 step 206: training loss: 23506.41150224444\n",
      "Epoch 0 step 207: training accuarcy: 0.8608\n",
      "Epoch 0 step 207: training loss: 23640.60893281858\n",
      "Epoch 0 step 208: training accuarcy: 0.863\n",
      "Epoch 0 step 208: training loss: 23603.960537486266\n",
      "Epoch 0 step 209: training accuarcy: 0.8663000000000001\n",
      "Epoch 0 step 209: training loss: 22809.899445858588\n",
      "Epoch 0 step 210: training accuarcy: 0.87\n",
      "Epoch 0 step 210: training loss: 23027.86702219095\n",
      "Epoch 0 step 211: training accuarcy: 0.8665\n",
      "Epoch 0 step 211: training loss: 22971.681975517364\n",
      "Epoch 0 step 212: training accuarcy: 0.8662000000000001\n",
      "Epoch 0 step 212: training loss: 23078.624569365402\n",
      "Epoch 0 step 213: training accuarcy: 0.8639\n",
      "Epoch 0 step 213: training loss: 23023.127176983264\n",
      "Epoch 0 step 214: training accuarcy: 0.8605\n",
      "Epoch 0 step 214: training loss: 22763.474052748537\n",
      "Epoch 0 step 215: training accuarcy: 0.8607\n",
      "Epoch 0 step 215: training loss: 23371.348405250923\n",
      "Epoch 0 step 216: training accuarcy: 0.8583000000000001\n",
      "Epoch 0 step 216: training loss: 23011.5653594249\n",
      "Epoch 0 step 217: training accuarcy: 0.8651000000000001\n",
      "Epoch 0 step 217: training loss: 22875.152046288997\n",
      "Epoch 0 step 218: training accuarcy: 0.8605\n",
      "Epoch 0 step 218: training loss: 22224.847322567955\n",
      "Epoch 0 step 219: training accuarcy: 0.8723000000000001\n",
      "Epoch 0 step 219: training loss: 22809.32773700078\n",
      "Epoch 0 step 220: training accuarcy: 0.8689\n",
      "Epoch 0 step 220: training loss: 22487.3502713198\n",
      "Epoch 0 step 221: training accuarcy: 0.865\n",
      "Epoch 0 step 221: training loss: 22062.03598915726\n",
      "Epoch 0 step 222: training accuarcy: 0.8724000000000001\n",
      "Epoch 0 step 222: training loss: 22020.420569299386\n",
      "Epoch 0 step 223: training accuarcy: 0.8706\n",
      "Epoch 0 step 223: training loss: 22186.00841821736\n",
      "Epoch 0 step 224: training accuarcy: 0.8714000000000001\n",
      "Epoch 0 step 224: training loss: 22459.86075033503\n",
      "Epoch 0 step 225: training accuarcy: 0.8716\n",
      "Epoch 0 step 225: training loss: 21734.630857384545\n",
      "Epoch 0 step 226: training accuarcy: 0.8712000000000001\n",
      "Epoch 0 step 226: training loss: 21928.93961678992\n",
      "Epoch 0 step 227: training accuarcy: 0.8733000000000001\n",
      "Epoch 0 step 227: training loss: 21937.894764508575\n",
      "Epoch 0 step 228: training accuarcy: 0.8758\n",
      "Epoch 0 step 228: training loss: 21305.5507471787\n",
      "Epoch 0 step 229: training accuarcy: 0.8775000000000001\n",
      "Epoch 0 step 229: training loss: 22432.49028313433\n",
      "Epoch 0 step 230: training accuarcy: 0.8674000000000001\n",
      "Epoch 0 step 230: training loss: 21781.22997723048\n",
      "Epoch 0 step 231: training accuarcy: 0.8709\n",
      "Epoch 0 step 231: training loss: 22689.606523235692\n",
      "Epoch 0 step 232: training accuarcy: 0.8641000000000001\n",
      "Epoch 0 step 232: training loss: 21412.4736308015\n",
      "Epoch 0 step 233: training accuarcy: 0.8753000000000001\n",
      "Epoch 0 step 233: training loss: 21855.86009807379\n",
      "Epoch 0 step 234: training accuarcy: 0.8727\n",
      "Epoch 0 step 234: training loss: 21341.81851962594\n",
      "Epoch 0 step 235: training accuarcy: 0.8769\n",
      "Epoch 0 step 235: training loss: 21328.439680003994\n",
      "Epoch 0 step 236: training accuarcy: 0.8742000000000001\n",
      "Epoch 0 step 236: training loss: 21212.01191353881\n",
      "Epoch 0 step 237: training accuarcy: 0.8779\n",
      "Epoch 0 step 237: training loss: 21681.953347736846\n",
      "Epoch 0 step 238: training accuarcy: 0.8664000000000001\n",
      "Epoch 0 step 238: training loss: 21195.0275524499\n",
      "Epoch 0 step 239: training accuarcy: 0.8778\n",
      "Epoch 0 step 239: training loss: 21505.498013554097\n",
      "Epoch 0 step 240: training accuarcy: 0.8718\n",
      "Epoch 0 step 240: training loss: 21454.793613797327\n",
      "Epoch 0 step 241: training accuarcy: 0.8744000000000001\n",
      "Epoch 0 step 241: training loss: 20871.59162369756\n",
      "Epoch 0 step 242: training accuarcy: 0.8751\n",
      "Epoch 0 step 242: training loss: 20770.768836649393\n",
      "Epoch 0 step 243: training accuarcy: 0.8806\n",
      "Epoch 0 step 243: training loss: 21891.29704530297\n",
      "Epoch 0 step 244: training accuarcy: 0.8692000000000001\n",
      "Epoch 0 step 244: training loss: 20835.111244900236\n",
      "Epoch 0 step 245: training accuarcy: 0.8845000000000001\n",
      "Epoch 0 step 245: training loss: 21238.50851981523\n",
      "Epoch 0 step 246: training accuarcy: 0.8712000000000001\n",
      "Epoch 0 step 246: training loss: 21264.08652601624\n",
      "Epoch 0 step 247: training accuarcy: 0.8746\n",
      "Epoch 0 step 247: training loss: 21181.59605143062\n",
      "Epoch 0 step 248: training accuarcy: 0.8744000000000001\n",
      "Epoch 0 step 248: training loss: 20717.414362144315\n",
      "Epoch 0 step 249: training accuarcy: 0.8818\n",
      "Epoch 0 step 249: training loss: 21020.817223240316\n",
      "Epoch 0 step 250: training accuarcy: 0.8737\n",
      "Epoch 0 step 250: training loss: 20807.829010201334\n",
      "Epoch 0 step 251: training accuarcy: 0.8784000000000001\n",
      "Epoch 0 step 251: training loss: 20442.383027732518\n",
      "Epoch 0 step 252: training accuarcy: 0.8804000000000001\n",
      "Epoch 0 step 252: training loss: 20480.054775489694\n",
      "Epoch 0 step 253: training accuarcy: 0.8854000000000001\n",
      "Epoch 0 step 253: training loss: 20596.99193253302\n",
      "Epoch 0 step 254: training accuarcy: 0.8804000000000001\n",
      "Epoch 0 step 254: training loss: 20183.7893707749\n",
      "Epoch 0 step 255: training accuarcy: 0.8834000000000001\n",
      "Epoch 0 step 255: training loss: 20341.919353814716\n",
      "Epoch 0 step 256: training accuarcy: 0.8811\n",
      "Epoch 0 step 256: training loss: 20605.72333395158\n",
      "Epoch 0 step 257: training accuarcy: 0.8785000000000001\n",
      "Epoch 0 step 257: training loss: 20213.784426142403\n",
      "Epoch 0 step 258: training accuarcy: 0.8784000000000001\n",
      "Epoch 0 step 258: training loss: 21077.46703227536\n",
      "Epoch 0 step 259: training accuarcy: 0.8711000000000001\n",
      "Epoch 0 step 259: training loss: 20593.85218557363\n",
      "Epoch 0 step 260: training accuarcy: 0.8785000000000001\n",
      "Epoch 0 step 260: training loss: 20224.313027959328\n",
      "Epoch 0 step 261: training accuarcy: 0.8791\n",
      "Epoch 0 step 261: training loss: 20271.92135110731\n",
      "Epoch 0 step 262: training accuarcy: 0.8807\n",
      "Epoch 0 step 262: training loss: 11475.218761201035\n",
      "Epoch 0 step 263: training accuarcy: 0.8812820512820513\n",
      "Epoch 0: train loss 55458.42443780722, train accuarcy 0.727400004863739\n",
      "Epoch 0: valid loss 22225.606414519825, valid accuarcy 0.8519107103347778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [05:16<10:32, 316.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 18654.763376803552\n",
      "Epoch 1 step 264: training accuarcy: 0.904\n",
      "Epoch 1 step 264: training loss: 19169.8509397557\n",
      "Epoch 1 step 265: training accuarcy: 0.8995000000000001\n",
      "Epoch 1 step 265: training loss: 18744.404241145487\n",
      "Epoch 1 step 266: training accuarcy: 0.9029\n",
      "Epoch 1 step 266: training loss: 19100.47757906397\n",
      "Epoch 1 step 267: training accuarcy: 0.8979\n",
      "Epoch 1 step 267: training loss: 19022.88913226355\n",
      "Epoch 1 step 268: training accuarcy: 0.8978\n",
      "Epoch 1 step 268: training loss: 18556.884732716415\n",
      "Epoch 1 step 269: training accuarcy: 0.9047000000000001\n",
      "Epoch 1 step 269: training loss: 18892.98198253762\n",
      "Epoch 1 step 270: training accuarcy: 0.9004000000000001\n",
      "Epoch 1 step 270: training loss: 18943.752710191136\n",
      "Epoch 1 step 271: training accuarcy: 0.8983000000000001\n",
      "Epoch 1 step 271: training loss: 18532.35758176867\n",
      "Epoch 1 step 272: training accuarcy: 0.8993\n",
      "Epoch 1 step 272: training loss: 18644.70328773265\n",
      "Epoch 1 step 273: training accuarcy: 0.9009\n",
      "Epoch 1 step 273: training loss: 18677.632361417796\n",
      "Epoch 1 step 274: training accuarcy: 0.8979\n",
      "Epoch 1 step 274: training loss: 18574.59249575129\n",
      "Epoch 1 step 275: training accuarcy: 0.8997\n",
      "Epoch 1 step 275: training loss: 18423.709282840788\n",
      "Epoch 1 step 276: training accuarcy: 0.9009\n",
      "Epoch 1 step 276: training loss: 18242.937535876736\n",
      "Epoch 1 step 277: training accuarcy: 0.9023\n",
      "Epoch 1 step 277: training loss: 18307.70177116529\n",
      "Epoch 1 step 278: training accuarcy: 0.9018\n",
      "Epoch 1 step 278: training loss: 18638.60871057914\n",
      "Epoch 1 step 279: training accuarcy: 0.8988\n",
      "Epoch 1 step 279: training loss: 18646.505172373872\n",
      "Epoch 1 step 280: training accuarcy: 0.8987\n",
      "Epoch 1 step 280: training loss: 17913.37958253085\n",
      "Epoch 1 step 281: training accuarcy: 0.9073\n",
      "Epoch 1 step 281: training loss: 18900.287417506366\n",
      "Epoch 1 step 282: training accuarcy: 0.8911\n",
      "Epoch 1 step 282: training loss: 17865.202987801415\n",
      "Epoch 1 step 283: training accuarcy: 0.9058\n",
      "Epoch 1 step 283: training loss: 18485.584848478586\n",
      "Epoch 1 step 284: training accuarcy: 0.9004000000000001\n",
      "Epoch 1 step 284: training loss: 18144.60235281142\n",
      "Epoch 1 step 285: training accuarcy: 0.8997\n",
      "Epoch 1 step 285: training loss: 18090.609739649786\n",
      "Epoch 1 step 286: training accuarcy: 0.9029\n",
      "Epoch 1 step 286: training loss: 18291.29389960708\n",
      "Epoch 1 step 287: training accuarcy: 0.9004000000000001\n",
      "Epoch 1 step 287: training loss: 18398.17613822346\n",
      "Epoch 1 step 288: training accuarcy: 0.8966000000000001\n",
      "Epoch 1 step 288: training loss: 17956.66713683578\n",
      "Epoch 1 step 289: training accuarcy: 0.9013\n",
      "Epoch 1 step 289: training loss: 18238.686505694277\n",
      "Epoch 1 step 290: training accuarcy: 0.9027000000000001\n",
      "Epoch 1 step 290: training loss: 18635.253282116188\n",
      "Epoch 1 step 291: training accuarcy: 0.898\n",
      "Epoch 1 step 291: training loss: 18776.24876540261\n",
      "Epoch 1 step 292: training accuarcy: 0.8926000000000001\n",
      "Epoch 1 step 292: training loss: 18265.80934297526\n",
      "Epoch 1 step 293: training accuarcy: 0.8956000000000001\n",
      "Epoch 1 step 293: training loss: 18301.508553336942\n",
      "Epoch 1 step 294: training accuarcy: 0.8991\n",
      "Epoch 1 step 294: training loss: 17856.824603350175\n",
      "Epoch 1 step 295: training accuarcy: 0.9023\n",
      "Epoch 1 step 295: training loss: 17836.442881884635\n",
      "Epoch 1 step 296: training accuarcy: 0.8997\n",
      "Epoch 1 step 296: training loss: 17914.887705530964\n",
      "Epoch 1 step 297: training accuarcy: 0.9026000000000001\n",
      "Epoch 1 step 297: training loss: 18117.32244103771\n",
      "Epoch 1 step 298: training accuarcy: 0.8987\n",
      "Epoch 1 step 298: training loss: 17909.585636634332\n",
      "Epoch 1 step 299: training accuarcy: 0.9029\n",
      "Epoch 1 step 299: training loss: 17655.63218251622\n",
      "Epoch 1 step 300: training accuarcy: 0.9026000000000001\n",
      "Epoch 1 step 300: training loss: 17507.144420761902\n",
      "Epoch 1 step 301: training accuarcy: 0.9066000000000001\n",
      "Epoch 1 step 301: training loss: 17836.40267473429\n",
      "Epoch 1 step 302: training accuarcy: 0.902\n",
      "Epoch 1 step 302: training loss: 17598.208927028205\n",
      "Epoch 1 step 303: training accuarcy: 0.9033\n",
      "Epoch 1 step 303: training loss: 18532.887774053153\n",
      "Epoch 1 step 304: training accuarcy: 0.8959\n",
      "Epoch 1 step 304: training loss: 17842.93223962416\n",
      "Epoch 1 step 305: training accuarcy: 0.9026000000000001\n",
      "Epoch 1 step 305: training loss: 17649.426156860285\n",
      "Epoch 1 step 306: training accuarcy: 0.9043\n",
      "Epoch 1 step 306: training loss: 17630.297496664876\n",
      "Epoch 1 step 307: training accuarcy: 0.903\n",
      "Epoch 1 step 307: training loss: 17844.997178894115\n",
      "Epoch 1 step 308: training accuarcy: 0.9005000000000001\n",
      "Epoch 1 step 308: training loss: 17498.114594921342\n",
      "Epoch 1 step 309: training accuarcy: 0.9002\n",
      "Epoch 1 step 309: training loss: 18092.614477225492\n",
      "Epoch 1 step 310: training accuarcy: 0.8988\n",
      "Epoch 1 step 310: training loss: 17712.074182985598\n",
      "Epoch 1 step 311: training accuarcy: 0.9019\n",
      "Epoch 1 step 311: training loss: 17642.56195066231\n",
      "Epoch 1 step 312: training accuarcy: 0.9024000000000001\n",
      "Epoch 1 step 312: training loss: 17588.767272965168\n",
      "Epoch 1 step 313: training accuarcy: 0.8958\n",
      "Epoch 1 step 313: training loss: 17817.414644271666\n",
      "Epoch 1 step 314: training accuarcy: 0.8949\n",
      "Epoch 1 step 314: training loss: 17222.014457001074\n",
      "Epoch 1 step 315: training accuarcy: 0.9063\n",
      "Epoch 1 step 315: training loss: 17519.625446477174\n",
      "Epoch 1 step 316: training accuarcy: 0.899\n",
      "Epoch 1 step 316: training loss: 18261.337125606726\n",
      "Epoch 1 step 317: training accuarcy: 0.8954000000000001\n",
      "Epoch 1 step 317: training loss: 17133.61156032388\n",
      "Epoch 1 step 318: training accuarcy: 0.9054000000000001\n",
      "Epoch 1 step 318: training loss: 17620.134193159724\n",
      "Epoch 1 step 319: training accuarcy: 0.8986000000000001\n",
      "Epoch 1 step 319: training loss: 17737.395861257733\n",
      "Epoch 1 step 320: training accuarcy: 0.896\n",
      "Epoch 1 step 320: training loss: 17544.931653181622\n",
      "Epoch 1 step 321: training accuarcy: 0.8979\n",
      "Epoch 1 step 321: training loss: 17106.36859356808\n",
      "Epoch 1 step 322: training accuarcy: 0.9075000000000001\n",
      "Epoch 1 step 322: training loss: 17557.774586511983\n",
      "Epoch 1 step 323: training accuarcy: 0.8987\n",
      "Epoch 1 step 323: training loss: 17684.767447660877\n",
      "Epoch 1 step 324: training accuarcy: 0.8949\n",
      "Epoch 1 step 324: training loss: 17237.337199773407\n",
      "Epoch 1 step 325: training accuarcy: 0.9005000000000001\n",
      "Epoch 1 step 325: training loss: 17476.794641825858\n",
      "Epoch 1 step 326: training accuarcy: 0.8973000000000001\n",
      "Epoch 1 step 326: training loss: 18027.03987244766\n",
      "Epoch 1 step 327: training accuarcy: 0.894\n",
      "Epoch 1 step 327: training loss: 17125.64318146466\n",
      "Epoch 1 step 328: training accuarcy: 0.8966000000000001\n",
      "Epoch 1 step 328: training loss: 17284.561200421194\n",
      "Epoch 1 step 329: training accuarcy: 0.9014000000000001\n",
      "Epoch 1 step 329: training loss: 17849.8235879133\n",
      "Epoch 1 step 330: training accuarcy: 0.8981\n",
      "Epoch 1 step 330: training loss: 17181.364746523745\n",
      "Epoch 1 step 331: training accuarcy: 0.8978\n",
      "Epoch 1 step 331: training loss: 17317.331215900253\n",
      "Epoch 1 step 332: training accuarcy: 0.9009\n",
      "Epoch 1 step 332: training loss: 16843.812938978557\n",
      "Epoch 1 step 333: training accuarcy: 0.9048\n",
      "Epoch 1 step 333: training loss: 16546.886112102686\n",
      "Epoch 1 step 334: training accuarcy: 0.9049\n",
      "Epoch 1 step 334: training loss: 16994.947053573684\n",
      "Epoch 1 step 335: training accuarcy: 0.9025000000000001\n",
      "Epoch 1 step 335: training loss: 17291.913884888956\n",
      "Epoch 1 step 336: training accuarcy: 0.8982\n",
      "Epoch 1 step 336: training loss: 16897.25667152463\n",
      "Epoch 1 step 337: training accuarcy: 0.9055000000000001\n",
      "Epoch 1 step 337: training loss: 17180.19497621621\n",
      "Epoch 1 step 338: training accuarcy: 0.8987\n",
      "Epoch 1 step 338: training loss: 16914.788713489706\n",
      "Epoch 1 step 339: training accuarcy: 0.9056000000000001\n",
      "Epoch 1 step 339: training loss: 17278.779427104382\n",
      "Epoch 1 step 340: training accuarcy: 0.8993\n",
      "Epoch 1 step 340: training loss: 17109.246325724223\n",
      "Epoch 1 step 341: training accuarcy: 0.8999\n",
      "Epoch 1 step 341: training loss: 17046.148803613793\n",
      "Epoch 1 step 342: training accuarcy: 0.9003\n",
      "Epoch 1 step 342: training loss: 16613.243705781773\n",
      "Epoch 1 step 343: training accuarcy: 0.9046000000000001\n",
      "Epoch 1 step 343: training loss: 16470.71060697616\n",
      "Epoch 1 step 344: training accuarcy: 0.9095000000000001\n",
      "Epoch 1 step 344: training loss: 16465.470732414935\n",
      "Epoch 1 step 345: training accuarcy: 0.9084000000000001\n",
      "Epoch 1 step 345: training loss: 16800.59675577887\n",
      "Epoch 1 step 346: training accuarcy: 0.9046000000000001\n",
      "Epoch 1 step 346: training loss: 16712.19062631345\n",
      "Epoch 1 step 347: training accuarcy: 0.9003\n",
      "Epoch 1 step 347: training loss: 16906.92779257843\n",
      "Epoch 1 step 348: training accuarcy: 0.9009\n",
      "Epoch 1 step 348: training loss: 16783.713519891895\n",
      "Epoch 1 step 349: training accuarcy: 0.9033\n",
      "Epoch 1 step 349: training loss: 16668.13921562823\n",
      "Epoch 1 step 350: training accuarcy: 0.9041\n",
      "Epoch 1 step 350: training loss: 16668.845680572966\n",
      "Epoch 1 step 351: training accuarcy: 0.9056000000000001\n",
      "Epoch 1 step 351: training loss: 16586.101331534668\n",
      "Epoch 1 step 352: training accuarcy: 0.9042\n",
      "Epoch 1 step 352: training loss: 16773.133050739045\n",
      "Epoch 1 step 353: training accuarcy: 0.9041\n",
      "Epoch 1 step 353: training loss: 16410.085223775175\n",
      "Epoch 1 step 354: training accuarcy: 0.9058\n",
      "Epoch 1 step 354: training loss: 16750.537457018298\n",
      "Epoch 1 step 355: training accuarcy: 0.9044000000000001\n",
      "Epoch 1 step 355: training loss: 16699.982681191043\n",
      "Epoch 1 step 356: training accuarcy: 0.9048\n",
      "Epoch 1 step 356: training loss: 16637.430941136226\n",
      "Epoch 1 step 357: training accuarcy: 0.9008\n",
      "Epoch 1 step 357: training loss: 16145.33835157563\n",
      "Epoch 1 step 358: training accuarcy: 0.9113\n",
      "Epoch 1 step 358: training loss: 16785.283942611353\n",
      "Epoch 1 step 359: training accuarcy: 0.9039\n",
      "Epoch 1 step 359: training loss: 16506.40892266547\n",
      "Epoch 1 step 360: training accuarcy: 0.9021\n",
      "Epoch 1 step 360: training loss: 16621.45373822891\n",
      "Epoch 1 step 361: training accuarcy: 0.9058\n",
      "Epoch 1 step 361: training loss: 16455.017505565535\n",
      "Epoch 1 step 362: training accuarcy: 0.9034000000000001\n",
      "Epoch 1 step 362: training loss: 16738.89367400583\n",
      "Epoch 1 step 363: training accuarcy: 0.8987\n",
      "Epoch 1 step 363: training loss: 16675.38564481992\n",
      "Epoch 1 step 364: training accuarcy: 0.9056000000000001\n",
      "Epoch 1 step 364: training loss: 16547.45779436327\n",
      "Epoch 1 step 365: training accuarcy: 0.9044000000000001\n",
      "Epoch 1 step 365: training loss: 16815.851374368714\n",
      "Epoch 1 step 366: training accuarcy: 0.9007000000000001\n",
      "Epoch 1 step 366: training loss: 16133.85277873015\n",
      "Epoch 1 step 367: training accuarcy: 0.9091\n",
      "Epoch 1 step 367: training loss: 16779.82426089553\n",
      "Epoch 1 step 368: training accuarcy: 0.8979\n",
      "Epoch 1 step 368: training loss: 15996.885559467697\n",
      "Epoch 1 step 369: training accuarcy: 0.9084000000000001\n",
      "Epoch 1 step 369: training loss: 15831.19122596573\n",
      "Epoch 1 step 370: training accuarcy: 0.9129\n",
      "Epoch 1 step 370: training loss: 16325.48645118215\n",
      "Epoch 1 step 371: training accuarcy: 0.9041\n",
      "Epoch 1 step 371: training loss: 16092.80752148877\n",
      "Epoch 1 step 372: training accuarcy: 0.9072\n",
      "Epoch 1 step 372: training loss: 16426.303546476705\n",
      "Epoch 1 step 373: training accuarcy: 0.9028\n",
      "Epoch 1 step 373: training loss: 15853.424372840764\n",
      "Epoch 1 step 374: training accuarcy: 0.9081\n",
      "Epoch 1 step 374: training loss: 16812.41666858533\n",
      "Epoch 1 step 375: training accuarcy: 0.9063\n",
      "Epoch 1 step 375: training loss: 15960.496206972097\n",
      "Epoch 1 step 376: training accuarcy: 0.907\n",
      "Epoch 1 step 376: training loss: 16213.462648075136\n",
      "Epoch 1 step 377: training accuarcy: 0.9044000000000001\n",
      "Epoch 1 step 377: training loss: 16228.209514329781\n",
      "Epoch 1 step 378: training accuarcy: 0.9075000000000001\n",
      "Epoch 1 step 378: training loss: 15907.46898994027\n",
      "Epoch 1 step 379: training accuarcy: 0.9074000000000001\n",
      "Epoch 1 step 379: training loss: 16381.5476734046\n",
      "Epoch 1 step 380: training accuarcy: 0.902\n",
      "Epoch 1 step 380: training loss: 16021.801350124988\n",
      "Epoch 1 step 381: training accuarcy: 0.906\n",
      "Epoch 1 step 381: training loss: 15862.241852203912\n",
      "Epoch 1 step 382: training accuarcy: 0.9079\n",
      "Epoch 1 step 382: training loss: 16136.99647150402\n",
      "Epoch 1 step 383: training accuarcy: 0.9063\n",
      "Epoch 1 step 383: training loss: 16296.838616782756\n",
      "Epoch 1 step 384: training accuarcy: 0.9022\n",
      "Epoch 1 step 384: training loss: 16039.820999621548\n",
      "Epoch 1 step 385: training accuarcy: 0.9047000000000001\n",
      "Epoch 1 step 385: training loss: 16061.367758574344\n",
      "Epoch 1 step 386: training accuarcy: 0.9045000000000001\n",
      "Epoch 1 step 386: training loss: 15806.434387168294\n",
      "Epoch 1 step 387: training accuarcy: 0.9085000000000001\n",
      "Epoch 1 step 387: training loss: 15797.66176702938\n",
      "Epoch 1 step 388: training accuarcy: 0.9071\n",
      "Epoch 1 step 388: training loss: 16290.045615951247\n",
      "Epoch 1 step 389: training accuarcy: 0.9017000000000001\n",
      "Epoch 1 step 389: training loss: 15646.91942696261\n",
      "Epoch 1 step 390: training accuarcy: 0.9092\n",
      "Epoch 1 step 390: training loss: 15887.659747618061\n",
      "Epoch 1 step 391: training accuarcy: 0.9063\n",
      "Epoch 1 step 391: training loss: 15907.66041694404\n",
      "Epoch 1 step 392: training accuarcy: 0.9088\n",
      "Epoch 1 step 392: training loss: 15919.383859215315\n",
      "Epoch 1 step 393: training accuarcy: 0.9046000000000001\n",
      "Epoch 1 step 393: training loss: 15868.249477165173\n",
      "Epoch 1 step 394: training accuarcy: 0.9051\n",
      "Epoch 1 step 394: training loss: 15583.261589249767\n",
      "Epoch 1 step 395: training accuarcy: 0.9119\n",
      "Epoch 1 step 395: training loss: 15437.735586540377\n",
      "Epoch 1 step 396: training accuarcy: 0.9099\n",
      "Epoch 1 step 396: training loss: 15726.088237256201\n",
      "Epoch 1 step 397: training accuarcy: 0.9096000000000001\n",
      "Epoch 1 step 397: training loss: 16215.48926017727\n",
      "Epoch 1 step 398: training accuarcy: 0.9018\n",
      "Epoch 1 step 398: training loss: 16650.842731848494\n",
      "Epoch 1 step 399: training accuarcy: 0.8942\n",
      "Epoch 1 step 399: training loss: 15596.15595897229\n",
      "Epoch 1 step 400: training accuarcy: 0.9059\n",
      "Epoch 1 step 400: training loss: 16513.06269891131\n",
      "Epoch 1 step 401: training accuarcy: 0.897\n",
      "Epoch 1 step 401: training loss: 15887.460564081512\n",
      "Epoch 1 step 402: training accuarcy: 0.9025000000000001\n",
      "Epoch 1 step 402: training loss: 15506.716448139156\n",
      "Epoch 1 step 403: training accuarcy: 0.9113\n",
      "Epoch 1 step 403: training loss: 15851.488676433088\n",
      "Epoch 1 step 404: training accuarcy: 0.9068\n",
      "Epoch 1 step 404: training loss: 15464.243239585283\n",
      "Epoch 1 step 405: training accuarcy: 0.907\n",
      "Epoch 1 step 405: training loss: 16105.794621637348\n",
      "Epoch 1 step 406: training accuarcy: 0.8976000000000001\n",
      "Epoch 1 step 406: training loss: 15511.452259603926\n",
      "Epoch 1 step 407: training accuarcy: 0.9076000000000001\n",
      "Epoch 1 step 407: training loss: 15691.01479837894\n",
      "Epoch 1 step 408: training accuarcy: 0.9068\n",
      "Epoch 1 step 408: training loss: 15629.259527781098\n",
      "Epoch 1 step 409: training accuarcy: 0.9052\n",
      "Epoch 1 step 409: training loss: 15505.074774415009\n",
      "Epoch 1 step 410: training accuarcy: 0.9072\n",
      "Epoch 1 step 410: training loss: 15566.933416191263\n",
      "Epoch 1 step 411: training accuarcy: 0.905\n",
      "Epoch 1 step 411: training loss: 15474.152423549353\n",
      "Epoch 1 step 412: training accuarcy: 0.9085000000000001\n",
      "Epoch 1 step 412: training loss: 15306.438920049824\n",
      "Epoch 1 step 413: training accuarcy: 0.9078\n",
      "Epoch 1 step 413: training loss: 15894.6940698964\n",
      "Epoch 1 step 414: training accuarcy: 0.904\n",
      "Epoch 1 step 414: training loss: 15328.016042264695\n",
      "Epoch 1 step 415: training accuarcy: 0.9099\n",
      "Epoch 1 step 415: training loss: 15186.631004897541\n",
      "Epoch 1 step 416: training accuarcy: 0.9098\n",
      "Epoch 1 step 416: training loss: 15370.280742625722\n",
      "Epoch 1 step 417: training accuarcy: 0.906\n",
      "Epoch 1 step 417: training loss: 15325.56922677136\n",
      "Epoch 1 step 418: training accuarcy: 0.9078\n",
      "Epoch 1 step 418: training loss: 15043.46218620695\n",
      "Epoch 1 step 419: training accuarcy: 0.909\n",
      "Epoch 1 step 419: training loss: 15036.338790384998\n",
      "Epoch 1 step 420: training accuarcy: 0.9091\n",
      "Epoch 1 step 420: training loss: 15801.148862494454\n",
      "Epoch 1 step 421: training accuarcy: 0.9036000000000001\n",
      "Epoch 1 step 421: training loss: 14960.187686612771\n",
      "Epoch 1 step 422: training accuarcy: 0.9134\n",
      "Epoch 1 step 422: training loss: 15194.999939582058\n",
      "Epoch 1 step 423: training accuarcy: 0.9124\n",
      "Epoch 1 step 423: training loss: 14761.99734435216\n",
      "Epoch 1 step 424: training accuarcy: 0.915\n",
      "Epoch 1 step 424: training loss: 15043.732982189023\n",
      "Epoch 1 step 425: training accuarcy: 0.9112\n",
      "Epoch 1 step 425: training loss: 15701.803154234709\n",
      "Epoch 1 step 426: training accuarcy: 0.9005000000000001\n",
      "Epoch 1 step 426: training loss: 15930.605339774967\n",
      "Epoch 1 step 427: training accuarcy: 0.8999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 427: training loss: 15603.772535963177\n",
      "Epoch 1 step 428: training accuarcy: 0.9072\n",
      "Epoch 1 step 428: training loss: 15564.097606553598\n",
      "Epoch 1 step 429: training accuarcy: 0.902\n",
      "Epoch 1 step 429: training loss: 15377.576241857663\n",
      "Epoch 1 step 430: training accuarcy: 0.9052\n",
      "Epoch 1 step 430: training loss: 15162.334503598253\n",
      "Epoch 1 step 431: training accuarcy: 0.9084000000000001\n",
      "Epoch 1 step 431: training loss: 15015.394849332051\n",
      "Epoch 1 step 432: training accuarcy: 0.9085000000000001\n",
      "Epoch 1 step 432: training loss: 15004.000818654247\n",
      "Epoch 1 step 433: training accuarcy: 0.9114000000000001\n",
      "Epoch 1 step 433: training loss: 15325.914791305458\n",
      "Epoch 1 step 434: training accuarcy: 0.9122\n",
      "Epoch 1 step 434: training loss: 14814.798455274773\n",
      "Epoch 1 step 435: training accuarcy: 0.9128000000000001\n",
      "Epoch 1 step 435: training loss: 15250.127153931393\n",
      "Epoch 1 step 436: training accuarcy: 0.9052\n",
      "Epoch 1 step 436: training loss: 14903.909304156427\n",
      "Epoch 1 step 437: training accuarcy: 0.9128000000000001\n",
      "Epoch 1 step 437: training loss: 14933.766370645433\n",
      "Epoch 1 step 438: training accuarcy: 0.907\n",
      "Epoch 1 step 438: training loss: 15298.662995896577\n",
      "Epoch 1 step 439: training accuarcy: 0.9063\n",
      "Epoch 1 step 439: training loss: 15321.813671878273\n",
      "Epoch 1 step 440: training accuarcy: 0.9065000000000001\n",
      "Epoch 1 step 440: training loss: 15191.807997212098\n",
      "Epoch 1 step 441: training accuarcy: 0.9022\n",
      "Epoch 1 step 441: training loss: 14969.261098491894\n",
      "Epoch 1 step 442: training accuarcy: 0.9095000000000001\n",
      "Epoch 1 step 442: training loss: 15571.92399511274\n",
      "Epoch 1 step 443: training accuarcy: 0.9052\n",
      "Epoch 1 step 443: training loss: 14763.906962947036\n",
      "Epoch 1 step 444: training accuarcy: 0.9143\n",
      "Epoch 1 step 444: training loss: 14573.012632122282\n",
      "Epoch 1 step 445: training accuarcy: 0.9142\n",
      "Epoch 1 step 445: training loss: 14672.695941595332\n",
      "Epoch 1 step 446: training accuarcy: 0.912\n",
      "Epoch 1 step 446: training loss: 14696.348030936157\n",
      "Epoch 1 step 447: training accuarcy: 0.9128000000000001\n",
      "Epoch 1 step 447: training loss: 14870.797881579794\n",
      "Epoch 1 step 448: training accuarcy: 0.9113\n",
      "Epoch 1 step 448: training loss: 14653.14855149926\n",
      "Epoch 1 step 449: training accuarcy: 0.9135000000000001\n",
      "Epoch 1 step 449: training loss: 14763.811187784013\n",
      "Epoch 1 step 450: training accuarcy: 0.9119\n",
      "Epoch 1 step 450: training loss: 15123.978590452622\n",
      "Epoch 1 step 451: training accuarcy: 0.9079\n",
      "Epoch 1 step 451: training loss: 14624.991473814332\n",
      "Epoch 1 step 452: training accuarcy: 0.911\n",
      "Epoch 1 step 452: training loss: 14901.976762089133\n",
      "Epoch 1 step 453: training accuarcy: 0.9086000000000001\n",
      "Epoch 1 step 453: training loss: 14738.91328380922\n",
      "Epoch 1 step 454: training accuarcy: 0.9115000000000001\n",
      "Epoch 1 step 454: training loss: 14739.76532654268\n",
      "Epoch 1 step 455: training accuarcy: 0.9106000000000001\n",
      "Epoch 1 step 455: training loss: 14606.887189051151\n",
      "Epoch 1 step 456: training accuarcy: 0.9119\n",
      "Epoch 1 step 456: training loss: 15001.275331173983\n",
      "Epoch 1 step 457: training accuarcy: 0.9046000000000001\n",
      "Epoch 1 step 457: training loss: 14833.296003054505\n",
      "Epoch 1 step 458: training accuarcy: 0.909\n",
      "Epoch 1 step 458: training loss: 14603.044949317677\n",
      "Epoch 1 step 459: training accuarcy: 0.9142\n",
      "Epoch 1 step 459: training loss: 15120.902167616687\n",
      "Epoch 1 step 460: training accuarcy: 0.9078\n",
      "Epoch 1 step 460: training loss: 14862.775451200694\n",
      "Epoch 1 step 461: training accuarcy: 0.9126000000000001\n",
      "Epoch 1 step 461: training loss: 14549.020901749773\n",
      "Epoch 1 step 462: training accuarcy: 0.9094000000000001\n",
      "Epoch 1 step 462: training loss: 14563.11485461105\n",
      "Epoch 1 step 463: training accuarcy: 0.9131\n",
      "Epoch 1 step 463: training loss: 14897.660582075661\n",
      "Epoch 1 step 464: training accuarcy: 0.9064000000000001\n",
      "Epoch 1 step 464: training loss: 14761.41014106105\n",
      "Epoch 1 step 465: training accuarcy: 0.9082\n",
      "Epoch 1 step 465: training loss: 14424.692742259058\n",
      "Epoch 1 step 466: training accuarcy: 0.9165000000000001\n",
      "Epoch 1 step 466: training loss: 14392.811180874643\n",
      "Epoch 1 step 467: training accuarcy: 0.9137000000000001\n",
      "Epoch 1 step 467: training loss: 14864.779471971488\n",
      "Epoch 1 step 468: training accuarcy: 0.9131\n",
      "Epoch 1 step 468: training loss: 14537.269231774213\n",
      "Epoch 1 step 469: training accuarcy: 0.9109\n",
      "Epoch 1 step 469: training loss: 14291.04119216332\n",
      "Epoch 1 step 470: training accuarcy: 0.9152\n",
      "Epoch 1 step 470: training loss: 14448.579808129747\n",
      "Epoch 1 step 471: training accuarcy: 0.9107000000000001\n",
      "Epoch 1 step 471: training loss: 14198.611326596942\n",
      "Epoch 1 step 472: training accuarcy: 0.9153\n",
      "Epoch 1 step 472: training loss: 14665.44269734179\n",
      "Epoch 1 step 473: training accuarcy: 0.9154\n",
      "Epoch 1 step 473: training loss: 14499.251891244146\n",
      "Epoch 1 step 474: training accuarcy: 0.9112\n",
      "Epoch 1 step 474: training loss: 14385.718746315026\n",
      "Epoch 1 step 475: training accuarcy: 0.9151\n",
      "Epoch 1 step 475: training loss: 14127.57828953709\n",
      "Epoch 1 step 476: training accuarcy: 0.9141\n",
      "Epoch 1 step 476: training loss: 13885.20859098227\n",
      "Epoch 1 step 477: training accuarcy: 0.9198000000000001\n",
      "Epoch 1 step 477: training loss: 14232.93722044163\n",
      "Epoch 1 step 478: training accuarcy: 0.9138000000000001\n",
      "Epoch 1 step 478: training loss: 14432.21421566842\n",
      "Epoch 1 step 479: training accuarcy: 0.9156000000000001\n",
      "Epoch 1 step 479: training loss: 13999.458189249859\n",
      "Epoch 1 step 480: training accuarcy: 0.9155000000000001\n",
      "Epoch 1 step 480: training loss: 14200.689659952686\n",
      "Epoch 1 step 481: training accuarcy: 0.9138000000000001\n",
      "Epoch 1 step 481: training loss: 14427.907430891137\n",
      "Epoch 1 step 482: training accuarcy: 0.9109\n",
      "Epoch 1 step 482: training loss: 14339.92702084485\n",
      "Epoch 1 step 483: training accuarcy: 0.9134\n",
      "Epoch 1 step 483: training loss: 14102.379297700678\n",
      "Epoch 1 step 484: training accuarcy: 0.9112\n",
      "Epoch 1 step 484: training loss: 14391.675487095748\n",
      "Epoch 1 step 485: training accuarcy: 0.9152\n",
      "Epoch 1 step 485: training loss: 14372.10150034111\n",
      "Epoch 1 step 486: training accuarcy: 0.9115000000000001\n",
      "Epoch 1 step 486: training loss: 14584.27540789716\n",
      "Epoch 1 step 487: training accuarcy: 0.9075000000000001\n",
      "Epoch 1 step 487: training loss: 14580.8960062066\n",
      "Epoch 1 step 488: training accuarcy: 0.9076000000000001\n",
      "Epoch 1 step 488: training loss: 14016.516429291933\n",
      "Epoch 1 step 489: training accuarcy: 0.9171\n",
      "Epoch 1 step 489: training loss: 13957.20931939118\n",
      "Epoch 1 step 490: training accuarcy: 0.9151\n",
      "Epoch 1 step 490: training loss: 14633.562803851903\n",
      "Epoch 1 step 491: training accuarcy: 0.9049\n",
      "Epoch 1 step 491: training loss: 13827.47171661998\n",
      "Epoch 1 step 492: training accuarcy: 0.9184\n",
      "Epoch 1 step 492: training loss: 14195.342183254872\n",
      "Epoch 1 step 493: training accuarcy: 0.9103\n",
      "Epoch 1 step 493: training loss: 14119.39137463236\n",
      "Epoch 1 step 494: training accuarcy: 0.914\n",
      "Epoch 1 step 494: training loss: 14133.38042817952\n",
      "Epoch 1 step 495: training accuarcy: 0.9197000000000001\n",
      "Epoch 1 step 495: training loss: 14380.259391878557\n",
      "Epoch 1 step 496: training accuarcy: 0.9117000000000001\n",
      "Epoch 1 step 496: training loss: 14169.378285577062\n",
      "Epoch 1 step 497: training accuarcy: 0.9115000000000001\n",
      "Epoch 1 step 497: training loss: 14157.316382864528\n",
      "Epoch 1 step 498: training accuarcy: 0.9154\n",
      "Epoch 1 step 498: training loss: 14341.51829462576\n",
      "Epoch 1 step 499: training accuarcy: 0.9075000000000001\n",
      "Epoch 1 step 499: training loss: 14270.414075849203\n",
      "Epoch 1 step 500: training accuarcy: 0.9138000000000001\n",
      "Epoch 1 step 500: training loss: 14668.741122586747\n",
      "Epoch 1 step 501: training accuarcy: 0.9022\n",
      "Epoch 1 step 501: training loss: 14065.655356665098\n",
      "Epoch 1 step 502: training accuarcy: 0.9114000000000001\n",
      "Epoch 1 step 502: training loss: 14047.244849692603\n",
      "Epoch 1 step 503: training accuarcy: 0.915\n",
      "Epoch 1 step 503: training loss: 14288.97750547294\n",
      "Epoch 1 step 504: training accuarcy: 0.9109\n",
      "Epoch 1 step 504: training loss: 13792.686824784027\n",
      "Epoch 1 step 505: training accuarcy: 0.9146000000000001\n",
      "Epoch 1 step 505: training loss: 14039.317728547883\n",
      "Epoch 1 step 506: training accuarcy: 0.9157000000000001\n",
      "Epoch 1 step 506: training loss: 14052.364304776725\n",
      "Epoch 1 step 507: training accuarcy: 0.9146000000000001\n",
      "Epoch 1 step 507: training loss: 13645.023833004367\n",
      "Epoch 1 step 508: training accuarcy: 0.9167000000000001\n",
      "Epoch 1 step 508: training loss: 13844.445084853465\n",
      "Epoch 1 step 509: training accuarcy: 0.917\n",
      "Epoch 1 step 509: training loss: 13775.352405666505\n",
      "Epoch 1 step 510: training accuarcy: 0.9159\n",
      "Epoch 1 step 510: training loss: 13937.938024954907\n",
      "Epoch 1 step 511: training accuarcy: 0.9141\n",
      "Epoch 1 step 511: training loss: 13485.92898365947\n",
      "Epoch 1 step 512: training accuarcy: 0.9215000000000001\n",
      "Epoch 1 step 512: training loss: 13773.379973430361\n",
      "Epoch 1 step 513: training accuarcy: 0.9141\n",
      "Epoch 1 step 513: training loss: 14302.648783475783\n",
      "Epoch 1 step 514: training accuarcy: 0.9108\n",
      "Epoch 1 step 514: training loss: 13754.366754954295\n",
      "Epoch 1 step 515: training accuarcy: 0.9154\n",
      "Epoch 1 step 515: training loss: 14055.161592770386\n",
      "Epoch 1 step 516: training accuarcy: 0.9127000000000001\n",
      "Epoch 1 step 516: training loss: 13590.540730265346\n",
      "Epoch 1 step 517: training accuarcy: 0.9182\n",
      "Epoch 1 step 517: training loss: 14293.18508265386\n",
      "Epoch 1 step 518: training accuarcy: 0.9125000000000001\n",
      "Epoch 1 step 518: training loss: 13659.83231767518\n",
      "Epoch 1 step 519: training accuarcy: 0.9165000000000001\n",
      "Epoch 1 step 519: training loss: 14021.44986186821\n",
      "Epoch 1 step 520: training accuarcy: 0.9106000000000001\n",
      "Epoch 1 step 520: training loss: 13677.29264200555\n",
      "Epoch 1 step 521: training accuarcy: 0.9184\n",
      "Epoch 1 step 521: training loss: 13872.296698874661\n",
      "Epoch 1 step 522: training accuarcy: 0.9146000000000001\n",
      "Epoch 1 step 522: training loss: 13903.782969430522\n",
      "Epoch 1 step 523: training accuarcy: 0.9129\n",
      "Epoch 1 step 523: training loss: 13306.622126120332\n",
      "Epoch 1 step 524: training accuarcy: 0.9186000000000001\n",
      "Epoch 1 step 524: training loss: 13301.91599346366\n",
      "Epoch 1 step 525: training accuarcy: 0.9186000000000001\n",
      "Epoch 1 step 525: training loss: 7123.241825909099\n",
      "Epoch 1 step 526: training accuarcy: 0.9158974358974359\n",
      "Epoch 1: train loss 15976.196669819967, train accuarcy 0.8896301984786987\n",
      "Epoch 1: valid loss 15801.941754075971, valid accuarcy 0.8880537152290344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [10:19<05:12, 312.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 12579.079971786003\n",
      "Epoch 2 step 527: training accuarcy: 0.932\n",
      "Epoch 2 step 527: training loss: 12467.619066556137\n",
      "Epoch 2 step 528: training accuarcy: 0.9273\n",
      "Epoch 2 step 528: training loss: 12956.344311152803\n",
      "Epoch 2 step 529: training accuarcy: 0.9261\n",
      "Epoch 2 step 529: training loss: 12257.875309358122\n",
      "Epoch 2 step 530: training accuarcy: 0.9339000000000001\n",
      "Epoch 2 step 530: training loss: 12327.81546683406\n",
      "Epoch 2 step 531: training accuarcy: 0.934\n",
      "Epoch 2 step 531: training loss: 12897.246040983051\n",
      "Epoch 2 step 532: training accuarcy: 0.9271\n",
      "Epoch 2 step 532: training loss: 12555.067693549696\n",
      "Epoch 2 step 533: training accuarcy: 0.9292\n",
      "Epoch 2 step 533: training loss: 12355.227634627197\n",
      "Epoch 2 step 534: training accuarcy: 0.9334\n",
      "Epoch 2 step 534: training loss: 12954.758991505158\n",
      "Epoch 2 step 535: training accuarcy: 0.9284\n",
      "Epoch 2 step 535: training loss: 12456.148345488487\n",
      "Epoch 2 step 536: training accuarcy: 0.9321\n",
      "Epoch 2 step 536: training loss: 12267.289753859326\n",
      "Epoch 2 step 537: training accuarcy: 0.9317000000000001\n",
      "Epoch 2 step 537: training loss: 12321.236526448349\n",
      "Epoch 2 step 538: training accuarcy: 0.9363\n",
      "Epoch 2 step 538: training loss: 12480.912403988636\n",
      "Epoch 2 step 539: training accuarcy: 0.9295\n",
      "Epoch 2 step 539: training loss: 12275.839111569077\n",
      "Epoch 2 step 540: training accuarcy: 0.9344\n",
      "Epoch 2 step 540: training loss: 12325.60669809585\n",
      "Epoch 2 step 541: training accuarcy: 0.9313\n",
      "Epoch 2 step 541: training loss: 12433.910904260623\n",
      "Epoch 2 step 542: training accuarcy: 0.9352\n",
      "Epoch 2 step 542: training loss: 12793.060817825726\n",
      "Epoch 2 step 543: training accuarcy: 0.9264\n",
      "Epoch 2 step 543: training loss: 12062.090456456732\n",
      "Epoch 2 step 544: training accuarcy: 0.9361\n",
      "Epoch 2 step 544: training loss: 12936.53844600427\n",
      "Epoch 2 step 545: training accuarcy: 0.927\n",
      "Epoch 2 step 545: training loss: 12772.251551927748\n",
      "Epoch 2 step 546: training accuarcy: 0.9281\n",
      "Epoch 2 step 546: training loss: 12028.122080641679\n",
      "Epoch 2 step 547: training accuarcy: 0.9315\n",
      "Epoch 2 step 547: training loss: 12441.671685080539\n",
      "Epoch 2 step 548: training accuarcy: 0.9343\n",
      "Epoch 2 step 548: training loss: 12230.798831367876\n",
      "Epoch 2 step 549: training accuarcy: 0.9348000000000001\n",
      "Epoch 2 step 549: training loss: 12397.000668349003\n",
      "Epoch 2 step 550: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 550: training loss: 12863.37087194978\n",
      "Epoch 2 step 551: training accuarcy: 0.924\n",
      "Epoch 2 step 551: training loss: 12602.293749940793\n",
      "Epoch 2 step 552: training accuarcy: 0.9292\n",
      "Epoch 2 step 552: training loss: 12748.903720412643\n",
      "Epoch 2 step 553: training accuarcy: 0.9228000000000001\n",
      "Epoch 2 step 553: training loss: 12636.907738385235\n",
      "Epoch 2 step 554: training accuarcy: 0.9296000000000001\n",
      "Epoch 2 step 554: training loss: 12399.67824987497\n",
      "Epoch 2 step 555: training accuarcy: 0.9278000000000001\n",
      "Epoch 2 step 555: training loss: 12514.089073828194\n",
      "Epoch 2 step 556: training accuarcy: 0.9312\n",
      "Epoch 2 step 556: training loss: 12660.01129281354\n",
      "Epoch 2 step 557: training accuarcy: 0.9266000000000001\n",
      "Epoch 2 step 557: training loss: 12373.075298495438\n",
      "Epoch 2 step 558: training accuarcy: 0.9316000000000001\n",
      "Epoch 2 step 558: training loss: 12376.913663473304\n",
      "Epoch 2 step 559: training accuarcy: 0.9312\n",
      "Epoch 2 step 559: training loss: 12861.679180628575\n",
      "Epoch 2 step 560: training accuarcy: 0.9242\n",
      "Epoch 2 step 560: training loss: 12183.146210513505\n",
      "Epoch 2 step 561: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 561: training loss: 12583.414860436009\n",
      "Epoch 2 step 562: training accuarcy: 0.9275\n",
      "Epoch 2 step 562: training loss: 12427.1461319801\n",
      "Epoch 2 step 563: training accuarcy: 0.9283\n",
      "Epoch 2 step 563: training loss: 12268.11259945213\n",
      "Epoch 2 step 564: training accuarcy: 0.93\n",
      "Epoch 2 step 564: training loss: 12409.58536401944\n",
      "Epoch 2 step 565: training accuarcy: 0.9284\n",
      "Epoch 2 step 565: training loss: 12982.030893855026\n",
      "Epoch 2 step 566: training accuarcy: 0.9207000000000001\n",
      "Epoch 2 step 566: training loss: 12285.509123749434\n",
      "Epoch 2 step 567: training accuarcy: 0.932\n",
      "Epoch 2 step 567: training loss: 12242.737663675933\n",
      "Epoch 2 step 568: training accuarcy: 0.93\n",
      "Epoch 2 step 568: training loss: 12285.381543985246\n",
      "Epoch 2 step 569: training accuarcy: 0.9274\n",
      "Epoch 2 step 569: training loss: 12708.454150655578\n",
      "Epoch 2 step 570: training accuarcy: 0.9266000000000001\n",
      "Epoch 2 step 570: training loss: 12218.739147589946\n",
      "Epoch 2 step 571: training accuarcy: 0.934\n",
      "Epoch 2 step 571: training loss: 12767.192302218384\n",
      "Epoch 2 step 572: training accuarcy: 0.922\n",
      "Epoch 2 step 572: training loss: 12791.831562561076\n",
      "Epoch 2 step 573: training accuarcy: 0.9273\n",
      "Epoch 2 step 573: training loss: 12448.080772564048\n",
      "Epoch 2 step 574: training accuarcy: 0.9314\n",
      "Epoch 2 step 574: training loss: 12337.672080294686\n",
      "Epoch 2 step 575: training accuarcy: 0.9274\n",
      "Epoch 2 step 575: training loss: 12390.376221320745\n",
      "Epoch 2 step 576: training accuarcy: 0.9301\n",
      "Epoch 2 step 576: training loss: 12253.579117310843\n",
      "Epoch 2 step 577: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 577: training loss: 12382.911057018155\n",
      "Epoch 2 step 578: training accuarcy: 0.9292\n",
      "Epoch 2 step 578: training loss: 12388.351390932487\n",
      "Epoch 2 step 579: training accuarcy: 0.9273\n",
      "Epoch 2 step 579: training loss: 12263.943314813376\n",
      "Epoch 2 step 580: training accuarcy: 0.9287000000000001\n",
      "Epoch 2 step 580: training loss: 11901.056028873652\n",
      "Epoch 2 step 581: training accuarcy: 0.9352\n",
      "Epoch 2 step 581: training loss: 12227.610360428862\n",
      "Epoch 2 step 582: training accuarcy: 0.9302\n",
      "Epoch 2 step 582: training loss: 12605.064552590959\n",
      "Epoch 2 step 583: training accuarcy: 0.9263\n",
      "Epoch 2 step 583: training loss: 12520.343108298095\n",
      "Epoch 2 step 584: training accuarcy: 0.9239\n",
      "Epoch 2 step 584: training loss: 12306.355394846658\n",
      "Epoch 2 step 585: training accuarcy: 0.9294\n",
      "Epoch 2 step 585: training loss: 12250.56602672824\n",
      "Epoch 2 step 586: training accuarcy: 0.9313\n",
      "Epoch 2 step 586: training loss: 12237.749990679438\n",
      "Epoch 2 step 587: training accuarcy: 0.9297000000000001\n",
      "Epoch 2 step 587: training loss: 12365.592013140775\n",
      "Epoch 2 step 588: training accuarcy: 0.9264\n",
      "Epoch 2 step 588: training loss: 12215.976714437902\n",
      "Epoch 2 step 589: training accuarcy: 0.9325\n",
      "Epoch 2 step 589: training loss: 12260.566548271208\n",
      "Epoch 2 step 590: training accuarcy: 0.931\n",
      "Epoch 2 step 590: training loss: 12373.277105863843\n",
      "Epoch 2 step 591: training accuarcy: 0.9254\n",
      "Epoch 2 step 591: training loss: 12622.811790882075\n",
      "Epoch 2 step 592: training accuarcy: 0.9251\n",
      "Epoch 2 step 592: training loss: 12651.808625681488\n",
      "Epoch 2 step 593: training accuarcy: 0.9249\n",
      "Epoch 2 step 593: training loss: 12458.26979199331\n",
      "Epoch 2 step 594: training accuarcy: 0.928\n",
      "Epoch 2 step 594: training loss: 12260.584604343945\n",
      "Epoch 2 step 595: training accuarcy: 0.9294\n",
      "Epoch 2 step 595: training loss: 12773.673025151815\n",
      "Epoch 2 step 596: training accuarcy: 0.9207000000000001\n",
      "Epoch 2 step 596: training loss: 12152.688711555998\n",
      "Epoch 2 step 597: training accuarcy: 0.9297000000000001\n",
      "Epoch 2 step 597: training loss: 12271.024068360988\n",
      "Epoch 2 step 598: training accuarcy: 0.9272\n",
      "Epoch 2 step 598: training loss: 12629.987784551502\n",
      "Epoch 2 step 599: training accuarcy: 0.9235000000000001\n",
      "Epoch 2 step 599: training loss: 12425.398545875063\n",
      "Epoch 2 step 600: training accuarcy: 0.9273\n",
      "Epoch 2 step 600: training loss: 12406.188696614525\n",
      "Epoch 2 step 601: training accuarcy: 0.9267000000000001\n",
      "Epoch 2 step 601: training loss: 12104.373441173559\n",
      "Epoch 2 step 602: training accuarcy: 0.9278000000000001\n",
      "Epoch 2 step 602: training loss: 12137.495938523476\n",
      "Epoch 2 step 603: training accuarcy: 0.93\n",
      "Epoch 2 step 603: training loss: 12532.696013054541\n",
      "Epoch 2 step 604: training accuarcy: 0.9239\n",
      "Epoch 2 step 604: training loss: 12767.699686667727\n",
      "Epoch 2 step 605: training accuarcy: 0.9244\n",
      "Epoch 2 step 605: training loss: 11961.526731370053\n",
      "Epoch 2 step 606: training accuarcy: 0.9362\n",
      "Epoch 2 step 606: training loss: 12476.536602976854\n",
      "Epoch 2 step 607: training accuarcy: 0.9264\n",
      "Epoch 2 step 607: training loss: 12328.7423127744\n",
      "Epoch 2 step 608: training accuarcy: 0.9304\n",
      "Epoch 2 step 608: training loss: 12713.297096140386\n",
      "Epoch 2 step 609: training accuarcy: 0.923\n",
      "Epoch 2 step 609: training loss: 12326.172528546733\n",
      "Epoch 2 step 610: training accuarcy: 0.9286000000000001\n",
      "Epoch 2 step 610: training loss: 11747.185809195607\n",
      "Epoch 2 step 611: training accuarcy: 0.9336000000000001\n",
      "Epoch 2 step 611: training loss: 12297.782410734995\n",
      "Epoch 2 step 612: training accuarcy: 0.9331\n",
      "Epoch 2 step 612: training loss: 11963.905872445495\n",
      "Epoch 2 step 613: training accuarcy: 0.9299000000000001\n",
      "Epoch 2 step 613: training loss: 12053.08268501908\n",
      "Epoch 2 step 614: training accuarcy: 0.9276000000000001\n",
      "Epoch 2 step 614: training loss: 12080.945894977689\n",
      "Epoch 2 step 615: training accuarcy: 0.9289000000000001\n",
      "Epoch 2 step 615: training loss: 12388.82632281529\n",
      "Epoch 2 step 616: training accuarcy: 0.9233\n",
      "Epoch 2 step 616: training loss: 12094.645916003059\n",
      "Epoch 2 step 617: training accuarcy: 0.9303\n",
      "Epoch 2 step 617: training loss: 12037.060737366077\n",
      "Epoch 2 step 618: training accuarcy: 0.9322\n",
      "Epoch 2 step 618: training loss: 12039.309792153434\n",
      "Epoch 2 step 619: training accuarcy: 0.9323\n",
      "Epoch 2 step 619: training loss: 12242.255778695195\n",
      "Epoch 2 step 620: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 620: training loss: 12176.994180936581\n",
      "Epoch 2 step 621: training accuarcy: 0.9245000000000001\n",
      "Epoch 2 step 621: training loss: 11871.20518650512\n",
      "Epoch 2 step 622: training accuarcy: 0.9318000000000001\n",
      "Epoch 2 step 622: training loss: 12006.077342521538\n",
      "Epoch 2 step 623: training accuarcy: 0.9308000000000001\n",
      "Epoch 2 step 623: training loss: 11830.511827467495\n",
      "Epoch 2 step 624: training accuarcy: 0.9281\n",
      "Epoch 2 step 624: training loss: 12344.527444796244\n",
      "Epoch 2 step 625: training accuarcy: 0.9221\n",
      "Epoch 2 step 625: training loss: 11739.970098564092\n",
      "Epoch 2 step 626: training accuarcy: 0.9339000000000001\n",
      "Epoch 2 step 626: training loss: 11775.155182370916\n",
      "Epoch 2 step 627: training accuarcy: 0.9316000000000001\n",
      "Epoch 2 step 627: training loss: 11563.315685055966\n",
      "Epoch 2 step 628: training accuarcy: 0.9331\n",
      "Epoch 2 step 628: training loss: 12234.425675778048\n",
      "Epoch 2 step 629: training accuarcy: 0.9217000000000001\n",
      "Epoch 2 step 629: training loss: 11974.389033437608\n",
      "Epoch 2 step 630: training accuarcy: 0.9277000000000001\n",
      "Epoch 2 step 630: training loss: 11824.418717300552\n",
      "Epoch 2 step 631: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 631: training loss: 12326.469804946453\n",
      "Epoch 2 step 632: training accuarcy: 0.9264\n",
      "Epoch 2 step 632: training loss: 11949.508066088767\n",
      "Epoch 2 step 633: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 633: training loss: 12232.007424914038\n",
      "Epoch 2 step 634: training accuarcy: 0.9247000000000001\n",
      "Epoch 2 step 634: training loss: 11360.20607202164\n",
      "Epoch 2 step 635: training accuarcy: 0.9348000000000001\n",
      "Epoch 2 step 635: training loss: 11790.926020676769\n",
      "Epoch 2 step 636: training accuarcy: 0.933\n",
      "Epoch 2 step 636: training loss: 11897.147193098084\n",
      "Epoch 2 step 637: training accuarcy: 0.9303\n",
      "Epoch 2 step 637: training loss: 12120.041600671819\n",
      "Epoch 2 step 638: training accuarcy: 0.9282\n",
      "Epoch 2 step 638: training loss: 11850.81633377847\n",
      "Epoch 2 step 639: training accuarcy: 0.9293\n",
      "Epoch 2 step 639: training loss: 12029.935052960196\n",
      "Epoch 2 step 640: training accuarcy: 0.9281\n",
      "Epoch 2 step 640: training loss: 12148.477223926177\n",
      "Epoch 2 step 641: training accuarcy: 0.9264\n",
      "Epoch 2 step 641: training loss: 11716.750310218073\n",
      "Epoch 2 step 642: training accuarcy: 0.9316000000000001\n",
      "Epoch 2 step 642: training loss: 12292.283687132169\n",
      "Epoch 2 step 643: training accuarcy: 0.9242\n",
      "Epoch 2 step 643: training loss: 11537.960109953965\n",
      "Epoch 2 step 644: training accuarcy: 0.9329000000000001\n",
      "Epoch 2 step 644: training loss: 11547.947763069149\n",
      "Epoch 2 step 645: training accuarcy: 0.9342\n",
      "Epoch 2 step 645: training loss: 12015.672488907336\n",
      "Epoch 2 step 646: training accuarcy: 0.9291\n",
      "Epoch 2 step 646: training loss: 11863.355888829388\n",
      "Epoch 2 step 647: training accuarcy: 0.931\n",
      "Epoch 2 step 647: training loss: 11938.244527859963\n",
      "Epoch 2 step 648: training accuarcy: 0.9303\n",
      "Epoch 2 step 648: training loss: 12061.833477667893\n",
      "Epoch 2 step 649: training accuarcy: 0.9272\n",
      "Epoch 2 step 649: training loss: 11398.914598959633\n",
      "Epoch 2 step 650: training accuarcy: 0.9329000000000001\n",
      "Epoch 2 step 650: training loss: 11959.159986304305\n",
      "Epoch 2 step 651: training accuarcy: 0.9238000000000001\n",
      "Epoch 2 step 651: training loss: 11944.912445953752\n",
      "Epoch 2 step 652: training accuarcy: 0.9258000000000001\n",
      "Epoch 2 step 652: training loss: 12054.596292005905\n",
      "Epoch 2 step 653: training accuarcy: 0.9251\n",
      "Epoch 2 step 653: training loss: 11991.484527828687\n",
      "Epoch 2 step 654: training accuarcy: 0.9319000000000001\n",
      "Epoch 2 step 654: training loss: 12075.905525327562\n",
      "Epoch 2 step 655: training accuarcy: 0.9235000000000001\n",
      "Epoch 2 step 655: training loss: 11695.023704545143\n",
      "Epoch 2 step 656: training accuarcy: 0.9291\n",
      "Epoch 2 step 656: training loss: 11598.578139420813\n",
      "Epoch 2 step 657: training accuarcy: 0.9313\n",
      "Epoch 2 step 657: training loss: 11623.194624225274\n",
      "Epoch 2 step 658: training accuarcy: 0.9331\n",
      "Epoch 2 step 658: training loss: 11880.263086875213\n",
      "Epoch 2 step 659: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 659: training loss: 11880.525127408937\n",
      "Epoch 2 step 660: training accuarcy: 0.929\n",
      "Epoch 2 step 660: training loss: 11927.432812966625\n",
      "Epoch 2 step 661: training accuarcy: 0.9292\n",
      "Epoch 2 step 661: training loss: 11770.488546483704\n",
      "Epoch 2 step 662: training accuarcy: 0.9304\n",
      "Epoch 2 step 662: training loss: 12075.94976677821\n",
      "Epoch 2 step 663: training accuarcy: 0.926\n",
      "Epoch 2 step 663: training loss: 11837.442798963642\n",
      "Epoch 2 step 664: training accuarcy: 0.9283\n",
      "Epoch 2 step 664: training loss: 11925.576644359993\n",
      "Epoch 2 step 665: training accuarcy: 0.9266000000000001\n",
      "Epoch 2 step 665: training loss: 11982.571157794284\n",
      "Epoch 2 step 666: training accuarcy: 0.9279000000000001\n",
      "Epoch 2 step 666: training loss: 12014.668253196023\n",
      "Epoch 2 step 667: training accuarcy: 0.9269000000000001\n",
      "Epoch 2 step 667: training loss: 11739.496964330801\n",
      "Epoch 2 step 668: training accuarcy: 0.9291\n",
      "Epoch 2 step 668: training loss: 11553.305384302399\n",
      "Epoch 2 step 669: training accuarcy: 0.9311\n",
      "Epoch 2 step 669: training loss: 11816.155255761949\n",
      "Epoch 2 step 670: training accuarcy: 0.9313\n",
      "Epoch 2 step 670: training loss: 11671.141259563972\n",
      "Epoch 2 step 671: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 671: training loss: 11945.618120281824\n",
      "Epoch 2 step 672: training accuarcy: 0.9277000000000001\n",
      "Epoch 2 step 672: training loss: 11741.716352200101\n",
      "Epoch 2 step 673: training accuarcy: 0.9265\n",
      "Epoch 2 step 673: training loss: 11633.884193832697\n",
      "Epoch 2 step 674: training accuarcy: 0.9275\n",
      "Epoch 2 step 674: training loss: 11938.509849367205\n",
      "Epoch 2 step 675: training accuarcy: 0.9239\n",
      "Epoch 2 step 675: training loss: 11361.287915536996\n",
      "Epoch 2 step 676: training accuarcy: 0.9318000000000001\n",
      "Epoch 2 step 676: training loss: 11609.832039564606\n",
      "Epoch 2 step 677: training accuarcy: 0.9299000000000001\n",
      "Epoch 2 step 677: training loss: 11299.545016285796\n",
      "Epoch 2 step 678: training accuarcy: 0.9323\n",
      "Epoch 2 step 678: training loss: 11774.108265410916\n",
      "Epoch 2 step 679: training accuarcy: 0.93\n",
      "Epoch 2 step 679: training loss: 11863.307208188164\n",
      "Epoch 2 step 680: training accuarcy: 0.9284\n",
      "Epoch 2 step 680: training loss: 11461.880473751084\n",
      "Epoch 2 step 681: training accuarcy: 0.9356000000000001\n",
      "Epoch 2 step 681: training loss: 11321.040068691495\n",
      "Epoch 2 step 682: training accuarcy: 0.9328000000000001\n",
      "Epoch 2 step 682: training loss: 11810.496575139205\n",
      "Epoch 2 step 683: training accuarcy: 0.9284\n",
      "Epoch 2 step 683: training loss: 11900.054049584809\n",
      "Epoch 2 step 684: training accuarcy: 0.925\n",
      "Epoch 2 step 684: training loss: 11730.712781024413\n",
      "Epoch 2 step 685: training accuarcy: 0.9276000000000001\n",
      "Epoch 2 step 685: training loss: 11517.577470591576\n",
      "Epoch 2 step 686: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 686: training loss: 11516.29370667542\n",
      "Epoch 2 step 687: training accuarcy: 0.9314\n",
      "Epoch 2 step 687: training loss: 12089.153903208591\n",
      "Epoch 2 step 688: training accuarcy: 0.9233\n",
      "Epoch 2 step 688: training loss: 11785.648075404135\n",
      "Epoch 2 step 689: training accuarcy: 0.9288000000000001\n",
      "Epoch 2 step 689: training loss: 11445.513514406359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 690: training accuarcy: 0.9318000000000001\n",
      "Epoch 2 step 690: training loss: 11941.249305893562\n",
      "Epoch 2 step 691: training accuarcy: 0.9281\n",
      "Epoch 2 step 691: training loss: 11631.430190287456\n",
      "Epoch 2 step 692: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 692: training loss: 11589.049984246412\n",
      "Epoch 2 step 693: training accuarcy: 0.9275\n",
      "Epoch 2 step 693: training loss: 11455.207522423885\n",
      "Epoch 2 step 694: training accuarcy: 0.9323\n",
      "Epoch 2 step 694: training loss: 11613.57102550763\n",
      "Epoch 2 step 695: training accuarcy: 0.9311\n",
      "Epoch 2 step 695: training loss: 11602.339899947161\n",
      "Epoch 2 step 696: training accuarcy: 0.9281\n",
      "Epoch 2 step 696: training loss: 11640.08287377311\n",
      "Epoch 2 step 697: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 697: training loss: 11676.68072996902\n",
      "Epoch 2 step 698: training accuarcy: 0.9277000000000001\n",
      "Epoch 2 step 698: training loss: 11423.882087064068\n",
      "Epoch 2 step 699: training accuarcy: 0.9314\n",
      "Epoch 2 step 699: training loss: 11673.403059396373\n",
      "Epoch 2 step 700: training accuarcy: 0.929\n",
      "Epoch 2 step 700: training loss: 11654.927845489674\n",
      "Epoch 2 step 701: training accuarcy: 0.9302\n",
      "Epoch 2 step 701: training loss: 11335.314574051945\n",
      "Epoch 2 step 702: training accuarcy: 0.9294\n",
      "Epoch 2 step 702: training loss: 11749.012445113787\n",
      "Epoch 2 step 703: training accuarcy: 0.9293\n",
      "Epoch 2 step 703: training loss: 11746.239944835159\n",
      "Epoch 2 step 704: training accuarcy: 0.9286000000000001\n",
      "Epoch 2 step 704: training loss: 11669.32979143395\n",
      "Epoch 2 step 705: training accuarcy: 0.9264\n",
      "Epoch 2 step 705: training loss: 11235.825676135762\n",
      "Epoch 2 step 706: training accuarcy: 0.9311\n",
      "Epoch 2 step 706: training loss: 11244.10752013971\n",
      "Epoch 2 step 707: training accuarcy: 0.9353\n",
      "Epoch 2 step 707: training loss: 11258.405568813514\n",
      "Epoch 2 step 708: training accuarcy: 0.9312\n",
      "Epoch 2 step 708: training loss: 11554.727337526487\n",
      "Epoch 2 step 709: training accuarcy: 0.9261\n",
      "Epoch 2 step 709: training loss: 11822.193484453937\n",
      "Epoch 2 step 710: training accuarcy: 0.9244\n",
      "Epoch 2 step 710: training loss: 11533.552823292752\n",
      "Epoch 2 step 711: training accuarcy: 0.928\n",
      "Epoch 2 step 711: training loss: 11558.920695873365\n",
      "Epoch 2 step 712: training accuarcy: 0.9294\n",
      "Epoch 2 step 712: training loss: 11301.704194675614\n",
      "Epoch 2 step 713: training accuarcy: 0.9324\n",
      "Epoch 2 step 713: training loss: 11362.876793843847\n",
      "Epoch 2 step 714: training accuarcy: 0.9299000000000001\n",
      "Epoch 2 step 714: training loss: 11566.324347335967\n",
      "Epoch 2 step 715: training accuarcy: 0.9317000000000001\n",
      "Epoch 2 step 715: training loss: 11322.11976715055\n",
      "Epoch 2 step 716: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 716: training loss: 11079.916791719595\n",
      "Epoch 2 step 717: training accuarcy: 0.9344\n",
      "Epoch 2 step 717: training loss: 11236.155903927098\n",
      "Epoch 2 step 718: training accuarcy: 0.9335\n",
      "Epoch 2 step 718: training loss: 11430.769524363392\n",
      "Epoch 2 step 719: training accuarcy: 0.9292\n",
      "Epoch 2 step 719: training loss: 11259.69812858109\n",
      "Epoch 2 step 720: training accuarcy: 0.9328000000000001\n",
      "Epoch 2 step 720: training loss: 11181.643792351033\n",
      "Epoch 2 step 721: training accuarcy: 0.9304\n",
      "Epoch 2 step 721: training loss: 12080.763737177786\n",
      "Epoch 2 step 722: training accuarcy: 0.9221\n",
      "Epoch 2 step 722: training loss: 11261.942456386781\n",
      "Epoch 2 step 723: training accuarcy: 0.9343\n",
      "Epoch 2 step 723: training loss: 11743.348700366716\n",
      "Epoch 2 step 724: training accuarcy: 0.9312\n",
      "Epoch 2 step 724: training loss: 11230.675970086095\n",
      "Epoch 2 step 725: training accuarcy: 0.9341\n",
      "Epoch 2 step 725: training loss: 11761.100599499374\n",
      "Epoch 2 step 726: training accuarcy: 0.9274\n",
      "Epoch 2 step 726: training loss: 11512.069917121942\n",
      "Epoch 2 step 727: training accuarcy: 0.928\n",
      "Epoch 2 step 727: training loss: 11216.426510554038\n",
      "Epoch 2 step 728: training accuarcy: 0.9325\n",
      "Epoch 2 step 728: training loss: 11364.969198328894\n",
      "Epoch 2 step 729: training accuarcy: 0.933\n",
      "Epoch 2 step 729: training loss: 11345.601454560221\n",
      "Epoch 2 step 730: training accuarcy: 0.9305\n",
      "Epoch 2 step 730: training loss: 10917.5477785084\n",
      "Epoch 2 step 731: training accuarcy: 0.9376000000000001\n",
      "Epoch 2 step 731: training loss: 11473.402800799719\n",
      "Epoch 2 step 732: training accuarcy: 0.93\n",
      "Epoch 2 step 732: training loss: 10911.405607991439\n",
      "Epoch 2 step 733: training accuarcy: 0.9346000000000001\n",
      "Epoch 2 step 733: training loss: 11386.213645049698\n",
      "Epoch 2 step 734: training accuarcy: 0.9288000000000001\n",
      "Epoch 2 step 734: training loss: 11365.228674850801\n",
      "Epoch 2 step 735: training accuarcy: 0.9323\n",
      "Epoch 2 step 735: training loss: 11232.517662412198\n",
      "Epoch 2 step 736: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 736: training loss: 11406.309748691048\n",
      "Epoch 2 step 737: training accuarcy: 0.9289000000000001\n",
      "Epoch 2 step 737: training loss: 11264.116501986398\n",
      "Epoch 2 step 738: training accuarcy: 0.9307000000000001\n",
      "Epoch 2 step 738: training loss: 10963.25297623809\n",
      "Epoch 2 step 739: training accuarcy: 0.9369000000000001\n",
      "Epoch 2 step 739: training loss: 10857.75786876929\n",
      "Epoch 2 step 740: training accuarcy: 0.9404\n",
      "Epoch 2 step 740: training loss: 11364.663105219512\n",
      "Epoch 2 step 741: training accuarcy: 0.9295\n",
      "Epoch 2 step 741: training loss: 11083.605329936283\n",
      "Epoch 2 step 742: training accuarcy: 0.9335\n",
      "Epoch 2 step 742: training loss: 11571.319400463159\n",
      "Epoch 2 step 743: training accuarcy: 0.925\n",
      "Epoch 2 step 743: training loss: 11367.030407168368\n",
      "Epoch 2 step 744: training accuarcy: 0.9314\n",
      "Epoch 2 step 744: training loss: 11051.482619387469\n",
      "Epoch 2 step 745: training accuarcy: 0.9341\n",
      "Epoch 2 step 745: training loss: 11233.40428102017\n",
      "Epoch 2 step 746: training accuarcy: 0.9331\n",
      "Epoch 2 step 746: training loss: 11073.762324073115\n",
      "Epoch 2 step 747: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 747: training loss: 11015.483837999802\n",
      "Epoch 2 step 748: training accuarcy: 0.9318000000000001\n",
      "Epoch 2 step 748: training loss: 11214.232217287301\n",
      "Epoch 2 step 749: training accuarcy: 0.9326000000000001\n",
      "Epoch 2 step 749: training loss: 10838.66032523348\n",
      "Epoch 2 step 750: training accuarcy: 0.937\n",
      "Epoch 2 step 750: training loss: 11296.564457328277\n",
      "Epoch 2 step 751: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 751: training loss: 11010.318927517254\n",
      "Epoch 2 step 752: training accuarcy: 0.9353\n",
      "Epoch 2 step 752: training loss: 11249.850744270589\n",
      "Epoch 2 step 753: training accuarcy: 0.9322\n",
      "Epoch 2 step 753: training loss: 11345.000462756348\n",
      "Epoch 2 step 754: training accuarcy: 0.9301\n",
      "Epoch 2 step 754: training loss: 11224.687621864385\n",
      "Epoch 2 step 755: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 755: training loss: 10918.875526438864\n",
      "Epoch 2 step 756: training accuarcy: 0.936\n",
      "Epoch 2 step 756: training loss: 11135.161675976165\n",
      "Epoch 2 step 757: training accuarcy: 0.9332\n",
      "Epoch 2 step 757: training loss: 11436.535951979586\n",
      "Epoch 2 step 758: training accuarcy: 0.929\n",
      "Epoch 2 step 758: training loss: 11498.694095742605\n",
      "Epoch 2 step 759: training accuarcy: 0.9295\n",
      "Epoch 2 step 759: training loss: 11269.999209233678\n",
      "Epoch 2 step 760: training accuarcy: 0.9279000000000001\n",
      "Epoch 2 step 760: training loss: 11221.088370034553\n",
      "Epoch 2 step 761: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 761: training loss: 11032.972368587192\n",
      "Epoch 2 step 762: training accuarcy: 0.9321\n",
      "Epoch 2 step 762: training loss: 11247.469509888288\n",
      "Epoch 2 step 763: training accuarcy: 0.9281\n",
      "Epoch 2 step 763: training loss: 11164.102859707538\n",
      "Epoch 2 step 764: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 764: training loss: 11775.2354771164\n",
      "Epoch 2 step 765: training accuarcy: 0.9247000000000001\n",
      "Epoch 2 step 765: training loss: 11094.676157766307\n",
      "Epoch 2 step 766: training accuarcy: 0.9331\n",
      "Epoch 2 step 766: training loss: 10699.564524911411\n",
      "Epoch 2 step 767: training accuarcy: 0.9396\n",
      "Epoch 2 step 767: training loss: 11265.424328296795\n",
      "Epoch 2 step 768: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 768: training loss: 11421.199630228046\n",
      "Epoch 2 step 769: training accuarcy: 0.9266000000000001\n",
      "Epoch 2 step 769: training loss: 11175.040994174198\n",
      "Epoch 2 step 770: training accuarcy: 0.9284\n",
      "Epoch 2 step 770: training loss: 11059.527509646898\n",
      "Epoch 2 step 771: training accuarcy: 0.9321\n",
      "Epoch 2 step 771: training loss: 10982.552014572946\n",
      "Epoch 2 step 772: training accuarcy: 0.9314\n",
      "Epoch 2 step 772: training loss: 11156.548918593204\n",
      "Epoch 2 step 773: training accuarcy: 0.9317000000000001\n",
      "Epoch 2 step 773: training loss: 10960.9980655112\n",
      "Epoch 2 step 774: training accuarcy: 0.9326000000000001\n",
      "Epoch 2 step 774: training loss: 10597.352543006278\n",
      "Epoch 2 step 775: training accuarcy: 0.936\n",
      "Epoch 2 step 775: training loss: 10752.717864063485\n",
      "Epoch 2 step 776: training accuarcy: 0.9375\n",
      "Epoch 2 step 776: training loss: 10911.885200742496\n",
      "Epoch 2 step 777: training accuarcy: 0.9334\n",
      "Epoch 2 step 777: training loss: 11166.66684664055\n",
      "Epoch 2 step 778: training accuarcy: 0.9295\n",
      "Epoch 2 step 778: training loss: 10970.034636902245\n",
      "Epoch 2 step 779: training accuarcy: 0.9328000000000001\n",
      "Epoch 2 step 779: training loss: 10721.367067623518\n",
      "Epoch 2 step 780: training accuarcy: 0.9366000000000001\n",
      "Epoch 2 step 780: training loss: 11074.548780542173\n",
      "Epoch 2 step 781: training accuarcy: 0.9321\n",
      "Epoch 2 step 781: training loss: 11194.387461613402\n",
      "Epoch 2 step 782: training accuarcy: 0.933\n",
      "Epoch 2 step 782: training loss: 10774.664290708652\n",
      "Epoch 2 step 783: training accuarcy: 0.9352\n",
      "Epoch 2 step 783: training loss: 11283.825982433795\n",
      "Epoch 2 step 784: training accuarcy: 0.9315\n",
      "Epoch 2 step 784: training loss: 10618.858911559093\n",
      "Epoch 2 step 785: training accuarcy: 0.9379000000000001\n",
      "Epoch 2 step 785: training loss: 10968.066095281467\n",
      "Epoch 2 step 786: training accuarcy: 0.9317000000000001\n",
      "Epoch 2 step 786: training loss: 11000.806166123677\n",
      "Epoch 2 step 787: training accuarcy: 0.9328000000000001\n",
      "Epoch 2 step 787: training loss: 11054.02644501294\n",
      "Epoch 2 step 788: training accuarcy: 0.9327000000000001\n",
      "Epoch 2 step 788: training loss: 5691.466533856963\n",
      "Epoch 2 step 789: training accuarcy: 0.9251282051282051\n",
      "Epoch 2: train loss 11808.000112529522, train accuarcy 0.9213521480560303\n",
      "Epoch 2: valid loss 13300.429108328372, valid accuarcy 0.9048920273780823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [15:24<00:00, 310.22s/it]\n"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=3,\n",
    "                  log_dir=get_log_dir('weight_topcoder', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T07:56:57.779505Z",
     "start_time": "2019-10-17T07:56:57.765509Z"
    }
   },
   "outputs": [],
   "source": [
    "del trans_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Trans Probability Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:30.389093Z",
     "start_time": "2019-10-17T13:40:29.895092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransProbFM()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_model = TransProbFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN, init_scale=INIT_SCALE)\n",
    "prob_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:31.259094Z",
     "start_time": "2019-10-17T13:40:31.255093Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(prob_model.parameters(), lr=0.01)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=100,\n",
    "                                      gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:46.041099Z",
     "start_time": "2019-10-17T13:40:44.031093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1dd1384ccc8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_learner = FMLearner(prob_model, adam_opt, schedular, db)\n",
    "prob_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:40:53.291093Z",
     "start_time": "2019-10-17T13:40:53.287092Z"
    }
   },
   "outputs": [],
   "source": [
    "prob_learner.compile(train_col='base',\n",
    "                     valid_col='base',\n",
    "                     test_col='base',\n",
    "                     loss_callback=trans_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-17T13:40:54.597Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                           | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 46914.52543605391\n",
      "Epoch 0 step 1: training accuarcy: 0.5265\n",
      "Epoch 0 step 1: training loss: 35919.0044689248\n",
      "Epoch 0 step 2: training accuarcy: 0.5395\n",
      "Epoch 0 step 2: training loss: 27242.175542358495\n",
      "Epoch 0 step 3: training accuarcy: 0.523\n",
      "Epoch 0 step 3: training loss: 19556.374216549906\n",
      "Epoch 0 step 4: training accuarcy: 0.5345\n",
      "Epoch 0 step 4: training loss: 14213.189092008139\n",
      "Epoch 0 step 5: training accuarcy: 0.5625\n",
      "Epoch 0 step 5: training loss: 10640.469354668963\n",
      "Epoch 0 step 6: training accuarcy: 0.555\n",
      "Epoch 0 step 6: training loss: 8382.043762964946\n",
      "Epoch 0 step 7: training accuarcy: 0.557\n",
      "Epoch 0 step 7: training loss: 7225.966130737879\n",
      "Epoch 0 step 8: training accuarcy: 0.5535\n",
      "Epoch 0 step 8: training loss: 6239.35087284336\n",
      "Epoch 0 step 9: training accuarcy: 0.5795\n",
      "Epoch 0 step 9: training loss: 5779.441470311422\n",
      "Epoch 0 step 10: training accuarcy: 0.578\n",
      "Epoch 0 step 10: training loss: 5959.56671379762\n",
      "Epoch 0 step 11: training accuarcy: 0.5765\n",
      "Epoch 0 step 11: training loss: 6285.822997075798\n",
      "Epoch 0 step 12: training accuarcy: 0.5755\n",
      "Epoch 0 step 12: training loss: 6325.42662063828\n",
      "Epoch 0 step 13: training accuarcy: 0.5805\n",
      "Epoch 0 step 13: training loss: 6645.182758010729\n",
      "Epoch 0 step 14: training accuarcy: 0.6\n",
      "Epoch 0 step 14: training loss: 6575.206128316846\n",
      "Epoch 0 step 15: training accuarcy: 0.6065\n",
      "Epoch 0 step 15: training loss: 6526.305446056031\n",
      "Epoch 0 step 16: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 16: training loss: 6532.753524134928\n",
      "Epoch 0 step 17: training accuarcy: 0.628\n",
      "Epoch 0 step 17: training loss: 6216.486483363968\n",
      "Epoch 0 step 18: training accuarcy: 0.643\n",
      "Epoch 0 step 18: training loss: 6310.419867231412\n",
      "Epoch 0 step 19: training accuarcy: 0.649\n",
      "Epoch 0 step 19: training loss: 5956.636554747243\n",
      "Epoch 0 step 20: training accuarcy: 0.6645\n",
      "Epoch 0 step 20: training loss: 5645.826512058615\n",
      "Epoch 0 step 21: training accuarcy: 0.6715\n",
      "Epoch 0 step 21: training loss: 5356.047472698328\n",
      "Epoch 0 step 22: training accuarcy: 0.6665\n",
      "Epoch 0 step 22: training loss: 5122.769763793873\n",
      "Epoch 0 step 23: training accuarcy: 0.667\n",
      "Epoch 0 step 23: training loss: 4751.655717256475\n",
      "Epoch 0 step 24: training accuarcy: 0.683\n",
      "Epoch 0 step 24: training loss: 4589.951817989892\n",
      "Epoch 0 step 25: training accuarcy: 0.7015\n",
      "Epoch 0 step 25: training loss: 4401.73600645996\n",
      "Epoch 0 step 26: training accuarcy: 0.6965\n",
      "Epoch 0 step 26: training loss: 4340.597507524786\n",
      "Epoch 0 step 27: training accuarcy: 0.7000000000000001\n",
      "Epoch 0 step 27: training loss: 4069.302506060536\n",
      "Epoch 0 step 28: training accuarcy: 0.7225\n",
      "Epoch 0 step 28: training loss: 4099.444578099327\n",
      "Epoch 0 step 29: training accuarcy: 0.7125\n",
      "Epoch 0 step 29: training loss: 4148.319078292395\n",
      "Epoch 0 step 30: training accuarcy: 0.7205\n",
      "Epoch 0 step 30: training loss: 4121.876297690175\n",
      "Epoch 0 step 31: training accuarcy: 0.7295\n",
      "Epoch 0 step 31: training loss: 3972.2310256283668\n",
      "Epoch 0 step 32: training accuarcy: 0.748\n",
      "Epoch 0 step 32: training loss: 3889.4377930812343\n",
      "Epoch 0 step 33: training accuarcy: 0.7435\n",
      "Epoch 0 step 33: training loss: 3852.8395028402365\n",
      "Epoch 0 step 34: training accuarcy: 0.7545000000000001\n",
      "Epoch 0 step 34: training loss: 3760.796101864061\n",
      "Epoch 0 step 35: training accuarcy: 0.7705\n",
      "Epoch 0 step 35: training loss: 3723.709890810387\n",
      "Epoch 0 step 36: training accuarcy: 0.7695\n",
      "Epoch 0 step 36: training loss: 3678.97176310537\n",
      "Epoch 0 step 37: training accuarcy: 0.7735\n",
      "Epoch 0 step 37: training loss: 3785.5391771662057\n",
      "Epoch 0 step 38: training accuarcy: 0.769\n",
      "Epoch 0 step 38: training loss: 3717.013706893452\n",
      "Epoch 0 step 39: training accuarcy: 0.771\n",
      "Epoch 0 step 39: training loss: 3611.8790715008618\n",
      "Epoch 0 step 40: training accuarcy: 0.781\n",
      "Epoch 0 step 40: training loss: 3731.6722303083347\n",
      "Epoch 0 step 41: training accuarcy: 0.771\n",
      "Epoch 0 step 41: training loss: 3656.854910923158\n",
      "Epoch 0 step 42: training accuarcy: 0.762\n",
      "Epoch 0 step 42: training loss: 3534.2244492738196\n",
      "Epoch 0 step 43: training accuarcy: 0.773\n",
      "Epoch 0 step 43: training loss: 3634.9976889477625\n",
      "Epoch 0 step 44: training accuarcy: 0.7845\n",
      "Epoch 0 step 44: training loss: 3401.7110980546395\n",
      "Epoch 0 step 45: training accuarcy: 0.789\n",
      "Epoch 0 step 45: training loss: 3569.4071790790076\n",
      "Epoch 0 step 46: training accuarcy: 0.781\n",
      "Epoch 0 step 46: training loss: 3577.1226561230797\n",
      "Epoch 0 step 47: training accuarcy: 0.7885\n",
      "Epoch 0 step 47: training loss: 3658.464558008017\n",
      "Epoch 0 step 48: training accuarcy: 0.7885\n",
      "Epoch 0 step 48: training loss: 3476.41969794597\n",
      "Epoch 0 step 49: training accuarcy: 0.801\n",
      "Epoch 0 step 49: training loss: 3708.983283254648\n",
      "Epoch 0 step 50: training accuarcy: 0.786\n",
      "Epoch 0 step 50: training loss: 3541.633561129729\n",
      "Epoch 0 step 51: training accuarcy: 0.802\n",
      "Epoch 0 step 51: training loss: 3501.1664808252954\n",
      "Epoch 0 step 52: training accuarcy: 0.796\n",
      "Epoch 0 step 52: training loss: 3543.8128828582567\n",
      "Epoch 0 step 53: training accuarcy: 0.8105\n",
      "Epoch 0 step 53: training loss: 3457.913040119349\n",
      "Epoch 0 step 54: training accuarcy: 0.8055\n",
      "Epoch 0 step 54: training loss: 3573.4073414637255\n",
      "Epoch 0 step 55: training accuarcy: 0.8005\n",
      "Epoch 0 step 55: training loss: 3709.1148051192818\n",
      "Epoch 0 step 56: training accuarcy: 0.803\n",
      "Epoch 0 step 56: training loss: 3540.330787690308\n",
      "Epoch 0 step 57: training accuarcy: 0.8095\n",
      "Epoch 0 step 57: training loss: 3429.4015190073583\n",
      "Epoch 0 step 58: training accuarcy: 0.807\n",
      "Epoch 0 step 58: training loss: 3306.4293920575687\n",
      "Epoch 0 step 59: training accuarcy: 0.8225\n",
      "Epoch 0 step 59: training loss: 3772.773847905075\n",
      "Epoch 0 step 60: training accuarcy: 0.7965\n",
      "Epoch 0 step 60: training loss: 3683.021118082407\n",
      "Epoch 0 step 61: training accuarcy: 0.8035\n",
      "Epoch 0 step 61: training loss: 3683.3787040533834\n",
      "Epoch 0 step 62: training accuarcy: 0.8160000000000001\n",
      "Epoch 0 step 62: training loss: 3609.2783497001274\n",
      "Epoch 0 step 63: training accuarcy: 0.8115\n",
      "Epoch 0 step 63: training loss: 3885.321869024948\n",
      "Epoch 0 step 64: training accuarcy: 0.7985\n",
      "Epoch 0 step 64: training loss: 3584.24380373814\n",
      "Epoch 0 step 65: training accuarcy: 0.8095\n",
      "Epoch 0 step 65: training loss: 3892.1879934998706\n",
      "Epoch 0 step 66: training accuarcy: 0.798\n",
      "Epoch 0 step 66: training loss: 3603.7256766994556\n",
      "Epoch 0 step 67: training accuarcy: 0.8185\n",
      "Epoch 0 step 67: training loss: 3907.780287140954\n",
      "Epoch 0 step 68: training accuarcy: 0.8005\n",
      "Epoch 0 step 68: training loss: 3649.18771794219\n",
      "Epoch 0 step 69: training accuarcy: 0.8210000000000001\n",
      "Epoch 0 step 69: training loss: 3955.4880650972073\n",
      "Epoch 0 step 70: training accuarcy: 0.8045\n",
      "Epoch 0 step 70: training loss: 3751.7387223530286\n",
      "Epoch 0 step 71: training accuarcy: 0.8260000000000001\n",
      "Epoch 0 step 71: training loss: 3995.195657991856\n",
      "Epoch 0 step 72: training accuarcy: 0.807\n",
      "Epoch 0 step 72: training loss: 4234.460512098962\n",
      "Epoch 0 step 73: training accuarcy: 0.8140000000000001\n",
      "Epoch 0 step 73: training loss: 3802.412235871027\n",
      "Epoch 0 step 74: training accuarcy: 0.8185\n",
      "Epoch 0 step 74: training loss: 4007.5872695325647\n",
      "Epoch 0 step 75: training accuarcy: 0.8135\n",
      "Epoch 0 step 75: training loss: 3865.421008475206\n",
      "Epoch 0 step 76: training accuarcy: 0.8185\n",
      "Epoch 0 step 76: training loss: 4130.10776294998\n",
      "Epoch 0 step 77: training accuarcy: 0.8065\n",
      "Epoch 0 step 77: training loss: 4020.5651690600407\n",
      "Epoch 0 step 78: training accuarcy: 0.806\n",
      "Epoch 0 step 78: training loss: 3682.439149727168\n",
      "Epoch 0 step 79: training accuarcy: 0.8285\n",
      "Epoch 0 step 79: training loss: 4124.483658010391\n",
      "Epoch 0 step 80: training accuarcy: 0.8150000000000001\n",
      "Epoch 0 step 80: training loss: 3957.2025402170075\n",
      "Epoch 0 step 81: training accuarcy: 0.8190000000000001\n",
      "Epoch 0 step 81: training loss: 3912.702051964836\n",
      "Epoch 0 step 82: training accuarcy: 0.8175\n",
      "Epoch 0 step 82: training loss: 3987.5876149848445\n",
      "Epoch 0 step 83: training accuarcy: 0.8215\n",
      "Epoch 0 step 83: training loss: 4113.143127130803\n",
      "Epoch 0 step 84: training accuarcy: 0.8180000000000001\n",
      "Epoch 0 step 84: training loss: 3945.518167747484\n",
      "Epoch 0 step 85: training accuarcy: 0.8270000000000001\n",
      "Epoch 0 step 85: training loss: 4076.3568203390732\n",
      "Epoch 0 step 86: training accuarcy: 0.8190000000000001\n",
      "Epoch 0 step 86: training loss: 4211.267137561824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 87: training accuarcy: 0.8180000000000001\n",
      "Epoch 0 step 87: training loss: 4308.806507685686\n",
      "Epoch 0 step 88: training accuarcy: 0.8135\n",
      "Epoch 0 step 88: training loss: 4230.616008118185\n",
      "Epoch 0 step 89: training accuarcy: 0.8155\n",
      "Epoch 0 step 89: training loss: 4449.043227141733\n",
      "Epoch 0 step 90: training accuarcy: 0.8170000000000001\n",
      "Epoch 0 step 90: training loss: 4342.627977375192\n",
      "Epoch 0 step 91: training accuarcy: 0.8200000000000001\n",
      "Epoch 0 step 91: training loss: 4437.580208013199\n",
      "Epoch 0 step 92: training accuarcy: 0.8160000000000001\n",
      "Epoch 0 step 92: training loss: 4352.9991128314705\n",
      "Epoch 0 step 93: training accuarcy: 0.8255\n",
      "Epoch 0 step 93: training loss: 4303.712833765702\n",
      "Epoch 0 step 94: training accuarcy: 0.8210000000000001\n",
      "Epoch 0 step 94: training loss: 4373.448334461702\n",
      "Epoch 0 step 95: training accuarcy: 0.8200000000000001\n",
      "Epoch 0 step 95: training loss: 4736.407048960332\n",
      "Epoch 0 step 96: training accuarcy: 0.799\n",
      "Epoch 0 step 96: training loss: 4413.5213023403285\n",
      "Epoch 0 step 97: training accuarcy: 0.8235\n",
      "Epoch 0 step 97: training loss: 4703.842059754512\n",
      "Epoch 0 step 98: training accuarcy: 0.8210000000000001\n",
      "Epoch 0 step 98: training loss: 4670.786890110001\n",
      "Epoch 0 step 99: training accuarcy: 0.8165\n",
      "Epoch 0 step 99: training loss: 4421.435645543126\n",
      "Epoch 0 step 100: training accuarcy: 0.8185\n",
      "Epoch 0 step 100: training loss: 5244.297271398671\n",
      "Epoch 0 step 101: training accuarcy: 0.7915\n",
      "Epoch 0 step 101: training loss: 4383.517921672733\n",
      "Epoch 0 step 102: training accuarcy: 0.836\n",
      "Epoch 0 step 102: training loss: 4487.093528691742\n",
      "Epoch 0 step 103: training accuarcy: 0.8205\n",
      "Epoch 0 step 103: training loss: 4499.312004053343\n",
      "Epoch 0 step 104: training accuarcy: 0.8230000000000001\n",
      "Epoch 0 step 104: training loss: 4427.402480363206\n",
      "Epoch 0 step 105: training accuarcy: 0.8250000000000001\n",
      "Epoch 0 step 105: training loss: 4617.192272824579\n",
      "Epoch 0 step 106: training accuarcy: 0.8175\n",
      "Epoch 0 step 106: training loss: 4546.077584029292\n",
      "Epoch 0 step 107: training accuarcy: 0.8190000000000001\n",
      "Epoch 0 step 107: training loss: 4551.527892938191\n",
      "Epoch 0 step 108: training accuarcy: 0.8275\n",
      "Epoch 0 step 108: training loss: 4438.354553009186\n",
      "Epoch 0 step 109: training accuarcy: 0.8220000000000001\n",
      "Epoch 0 step 109: training loss: 4178.024030720867\n",
      "Epoch 0 step 110: training accuarcy: 0.8345\n",
      "Epoch 0 step 110: training loss: 4074.766816424764\n",
      "Epoch 0 step 111: training accuarcy: 0.8230000000000001\n",
      "Epoch 0 step 111: training loss: 4071.3072027164176\n",
      "Epoch 0 step 112: training accuarcy: 0.8415\n",
      "Epoch 0 step 112: training loss: 4050.8420258646456\n",
      "Epoch 0 step 113: training accuarcy: 0.8305\n",
      "Epoch 0 step 113: training loss: 3922.3587047016767\n",
      "Epoch 0 step 114: training accuarcy: 0.8475\n",
      "Epoch 0 step 114: training loss: 3997.5578913635\n",
      "Epoch 0 step 115: training accuarcy: 0.843\n",
      "Epoch 0 step 115: training loss: 3836.2571052652465\n",
      "Epoch 0 step 116: training accuarcy: 0.8475\n",
      "Epoch 0 step 116: training loss: 3977.3647655222653\n",
      "Epoch 0 step 117: training accuarcy: 0.8240000000000001\n",
      "Epoch 0 step 117: training loss: 3553.1959100209124\n",
      "Epoch 0 step 118: training accuarcy: 0.8555\n",
      "Epoch 0 step 118: training loss: 3981.9771474873755\n",
      "Epoch 0 step 119: training accuarcy: 0.838\n",
      "Epoch 0 step 119: training loss: 3665.035770571918\n",
      "Epoch 0 step 120: training accuarcy: 0.844\n",
      "Epoch 0 step 120: training loss: 3721.1369134946835\n",
      "Epoch 0 step 121: training accuarcy: 0.8365\n",
      "Epoch 0 step 121: training loss: 3752.441968629525\n",
      "Epoch 0 step 122: training accuarcy: 0.84\n",
      "Epoch 0 step 122: training loss: 3776.8357923376648\n",
      "Epoch 0 step 123: training accuarcy: 0.8425\n",
      "Epoch 0 step 123: training loss: 3483.722038141522\n",
      "Epoch 0 step 124: training accuarcy: 0.8465\n",
      "Epoch 0 step 124: training loss: 3754.4810282433564\n",
      "Epoch 0 step 125: training accuarcy: 0.8425\n",
      "Epoch 0 step 125: training loss: 3859.6276394953975\n",
      "Epoch 0 step 126: training accuarcy: 0.8285\n",
      "Epoch 0 step 126: training loss: 3875.8105963065696\n",
      "Epoch 0 step 127: training accuarcy: 0.838\n",
      "Epoch 0 step 127: training loss: 3840.345691452905\n",
      "Epoch 0 step 128: training accuarcy: 0.8345\n",
      "Epoch 0 step 128: training loss: 3501.8814723353044\n",
      "Epoch 0 step 129: training accuarcy: 0.8545\n",
      "Epoch 0 step 129: training loss: 3574.0854007038015\n",
      "Epoch 0 step 130: training accuarcy: 0.8615\n",
      "Epoch 0 step 130: training loss: 3546.528278414334\n",
      "Epoch 0 step 131: training accuarcy: 0.8495\n",
      "Epoch 0 step 131: training loss: 3671.3162529515953\n",
      "Epoch 0 step 132: training accuarcy: 0.8435\n",
      "Epoch 0 step 132: training loss: 3887.23626362056\n",
      "Epoch 0 step 133: training accuarcy: 0.8295\n",
      "Epoch 0 step 133: training loss: 3419.8922485747516\n",
      "Epoch 0 step 134: training accuarcy: 0.844\n",
      "Epoch 0 step 134: training loss: 3693.9807371487163\n",
      "Epoch 0 step 135: training accuarcy: 0.846\n",
      "Epoch 0 step 135: training loss: 3711.69192606177\n",
      "Epoch 0 step 136: training accuarcy: 0.8315\n",
      "Epoch 0 step 136: training loss: 3503.7705638066827\n",
      "Epoch 0 step 137: training accuarcy: 0.852\n",
      "Epoch 0 step 137: training loss: 3456.1548472145178\n",
      "Epoch 0 step 138: training accuarcy: 0.8495\n",
      "Epoch 0 step 138: training loss: 3558.23945585245\n",
      "Epoch 0 step 139: training accuarcy: 0.8465\n",
      "Epoch 0 step 139: training loss: 3578.4700387142484\n",
      "Epoch 0 step 140: training accuarcy: 0.844\n",
      "Epoch 0 step 140: training loss: 3594.0248119033877\n",
      "Epoch 0 step 141: training accuarcy: 0.853\n",
      "Epoch 0 step 141: training loss: 3047.295109021642\n",
      "Epoch 0 step 142: training accuarcy: 0.866\n",
      "Epoch 0 step 142: training loss: 3651.431371808403\n",
      "Epoch 0 step 143: training accuarcy: 0.842\n",
      "Epoch 0 step 143: training loss: 3375.341696497264\n",
      "Epoch 0 step 144: training accuarcy: 0.858\n",
      "Epoch 0 step 144: training loss: 3680.452743192006\n",
      "Epoch 0 step 145: training accuarcy: 0.8525\n",
      "Epoch 0 step 145: training loss: 3345.434210922952\n",
      "Epoch 0 step 146: training accuarcy: 0.855\n",
      "Epoch 0 step 146: training loss: 3627.201714672553\n",
      "Epoch 0 step 147: training accuarcy: 0.8495\n",
      "Epoch 0 step 147: training loss: 3753.763864732346\n",
      "Epoch 0 step 148: training accuarcy: 0.835\n",
      "Epoch 0 step 148: training loss: 3583.2892610662166\n",
      "Epoch 0 step 149: training accuarcy: 0.8535\n",
      "Epoch 0 step 149: training loss: 3361.3925101022633\n",
      "Epoch 0 step 150: training accuarcy: 0.844\n",
      "Epoch 0 step 150: training loss: 3492.995914896278\n",
      "Epoch 0 step 151: training accuarcy: 0.8475\n",
      "Epoch 0 step 151: training loss: 3483.9819830143715\n",
      "Epoch 0 step 152: training accuarcy: 0.844\n",
      "Epoch 0 step 152: training loss: 3331.5552093566203\n",
      "Epoch 0 step 153: training accuarcy: 0.847\n",
      "Epoch 0 step 153: training loss: 3328.0284742805998\n",
      "Epoch 0 step 154: training accuarcy: 0.851\n",
      "Epoch 0 step 154: training loss: 3497.2335011580626\n",
      "Epoch 0 step 155: training accuarcy: 0.8585\n",
      "Epoch 0 step 155: training loss: 3468.510300168591\n",
      "Epoch 0 step 156: training accuarcy: 0.855\n",
      "Epoch 0 step 156: training loss: 3347.7002864316446\n",
      "Epoch 0 step 157: training accuarcy: 0.8545\n",
      "Epoch 0 step 157: training loss: 3381.961853261695\n",
      "Epoch 0 step 158: training accuarcy: 0.852\n",
      "Epoch 0 step 158: training loss: 3360.651121855599\n",
      "Epoch 0 step 159: training accuarcy: 0.851\n",
      "Epoch 0 step 159: training loss: 3332.2121081817263\n",
      "Epoch 0 step 160: training accuarcy: 0.8565\n",
      "Epoch 0 step 160: training loss: 3468.6813232625673\n",
      "Epoch 0 step 161: training accuarcy: 0.8475\n",
      "Epoch 0 step 161: training loss: 3214.288044475994\n",
      "Epoch 0 step 162: training accuarcy: 0.8665\n",
      "Epoch 0 step 162: training loss: 3257.037598326373\n",
      "Epoch 0 step 163: training accuarcy: 0.863\n",
      "Epoch 0 step 163: training loss: 3292.0002919620083\n",
      "Epoch 0 step 164: training accuarcy: 0.8575\n",
      "Epoch 0 step 164: training loss: 3313.307762264234\n",
      "Epoch 0 step 165: training accuarcy: 0.8655\n",
      "Epoch 0 step 165: training loss: 3379.0615801514946\n",
      "Epoch 0 step 166: training accuarcy: 0.85\n",
      "Epoch 0 step 166: training loss: 3198.8018289862666\n",
      "Epoch 0 step 167: training accuarcy: 0.856\n",
      "Epoch 0 step 167: training loss: 3179.1694154918378\n",
      "Epoch 0 step 168: training accuarcy: 0.866\n",
      "Epoch 0 step 168: training loss: 3202.002762333147\n",
      "Epoch 0 step 169: training accuarcy: 0.8625\n",
      "Epoch 0 step 169: training loss: 3279.5570385952087\n",
      "Epoch 0 step 170: training accuarcy: 0.852\n",
      "Epoch 0 step 170: training loss: 3171.766367433577\n",
      "Epoch 0 step 171: training accuarcy: 0.865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 171: training loss: 3278.014820346404\n",
      "Epoch 0 step 172: training accuarcy: 0.857\n",
      "Epoch 0 step 172: training loss: 3382.71176975627\n",
      "Epoch 0 step 173: training accuarcy: 0.8565\n",
      "Epoch 0 step 173: training loss: 3203.9518986471157\n",
      "Epoch 0 step 174: training accuarcy: 0.8535\n",
      "Epoch 0 step 174: training loss: 3306.1572768369983\n",
      "Epoch 0 step 175: training accuarcy: 0.847\n",
      "Epoch 0 step 175: training loss: 3400.3387793358434\n",
      "Epoch 0 step 176: training accuarcy: 0.8525\n",
      "Epoch 0 step 176: training loss: 3313.611008988354\n",
      "Epoch 0 step 177: training accuarcy: 0.848\n",
      "Epoch 0 step 177: training loss: 3192.588621227758\n",
      "Epoch 0 step 178: training accuarcy: 0.8615\n",
      "Epoch 0 step 178: training loss: 3325.320352853744\n",
      "Epoch 0 step 179: training accuarcy: 0.856\n",
      "Epoch 0 step 179: training loss: 3308.919915313704\n",
      "Epoch 0 step 180: training accuarcy: 0.8515\n",
      "Epoch 0 step 180: training loss: 3420.516076546224\n",
      "Epoch 0 step 181: training accuarcy: 0.8575\n",
      "Epoch 0 step 181: training loss: 3447.308250341687\n",
      "Epoch 0 step 182: training accuarcy: 0.8545\n",
      "Epoch 0 step 182: training loss: 3372.756528740826\n",
      "Epoch 0 step 183: training accuarcy: 0.8405\n",
      "Epoch 0 step 183: training loss: 3271.4787062762007\n",
      "Epoch 0 step 184: training accuarcy: 0.857\n",
      "Epoch 0 step 184: training loss: 3064.043613081237\n",
      "Epoch 0 step 185: training accuarcy: 0.866\n",
      "Epoch 0 step 185: training loss: 3380.8825674408217\n",
      "Epoch 0 step 186: training accuarcy: 0.853\n",
      "Epoch 0 step 186: training loss: 3500.3846613434325\n",
      "Epoch 0 step 187: training accuarcy: 0.8585\n",
      "Epoch 0 step 187: training loss: 3251.1115738814265\n",
      "Epoch 0 step 188: training accuarcy: 0.8715\n",
      "Epoch 0 step 188: training loss: 3307.850662013525\n",
      "Epoch 0 step 189: training accuarcy: 0.8485\n",
      "Epoch 0 step 189: training loss: 3110.6556193871384\n",
      "Epoch 0 step 190: training accuarcy: 0.862\n",
      "Epoch 0 step 190: training loss: 3504.2330926038685\n",
      "Epoch 0 step 191: training accuarcy: 0.8595\n",
      "Epoch 0 step 191: training loss: 3480.5516186792324\n",
      "Epoch 0 step 192: training accuarcy: 0.8445\n",
      "Epoch 0 step 192: training loss: 3323.991857327731\n",
      "Epoch 0 step 193: training accuarcy: 0.864\n",
      "Epoch 0 step 193: training loss: 3317.9115062037745\n",
      "Epoch 0 step 194: training accuarcy: 0.869\n",
      "Epoch 0 step 194: training loss: 3428.118293520642\n",
      "Epoch 0 step 195: training accuarcy: 0.861\n",
      "Epoch 0 step 195: training loss: 3244.650262946717\n",
      "Epoch 0 step 196: training accuarcy: 0.863\n",
      "Epoch 0 step 196: training loss: 3191.7404853651387\n",
      "Epoch 0 step 197: training accuarcy: 0.855\n",
      "Epoch 0 step 197: training loss: 3473.472038266158\n",
      "Epoch 0 step 198: training accuarcy: 0.857\n",
      "Epoch 0 step 198: training loss: 3196.372415188054\n",
      "Epoch 0 step 199: training accuarcy: 0.8695\n",
      "Epoch 0 step 199: training loss: 3434.7086395985043\n",
      "Epoch 0 step 200: training accuarcy: 0.843\n",
      "Epoch 0 step 200: training loss: 3145.7647341949687\n",
      "Epoch 0 step 201: training accuarcy: 0.8645\n",
      "Epoch 0 step 201: training loss: 3351.2921070919183\n",
      "Epoch 0 step 202: training accuarcy: 0.856\n",
      "Epoch 0 step 202: training loss: 3082.2999832298237\n",
      "Epoch 0 step 203: training accuarcy: 0.8725\n",
      "Epoch 0 step 203: training loss: 3234.562835885028\n",
      "Epoch 0 step 204: training accuarcy: 0.8655\n",
      "Epoch 0 step 204: training loss: 3329.0775886223933\n",
      "Epoch 0 step 205: training accuarcy: 0.858\n",
      "Epoch 0 step 205: training loss: 3357.0993038492747\n",
      "Epoch 0 step 206: training accuarcy: 0.8625\n",
      "Epoch 0 step 206: training loss: 3404.1232885786667\n",
      "Epoch 0 step 207: training accuarcy: 0.8565\n",
      "Epoch 0 step 207: training loss: 3293.2738992251116\n",
      "Epoch 0 step 208: training accuarcy: 0.857\n",
      "Epoch 0 step 208: training loss: 3253.290234112719\n",
      "Epoch 0 step 209: training accuarcy: 0.858\n",
      "Epoch 0 step 209: training loss: 3290.060733125807\n",
      "Epoch 0 step 210: training accuarcy: 0.8565\n",
      "Epoch 0 step 210: training loss: 3212.3705490205557\n",
      "Epoch 0 step 211: training accuarcy: 0.863\n",
      "Epoch 0 step 211: training loss: 3164.3414149946457\n",
      "Epoch 0 step 212: training accuarcy: 0.85\n",
      "Epoch 0 step 212: training loss: 2977.0369813262005\n",
      "Epoch 0 step 213: training accuarcy: 0.8665\n",
      "Epoch 0 step 213: training loss: 3155.16438227626\n",
      "Epoch 0 step 214: training accuarcy: 0.854\n",
      "Epoch 0 step 214: training loss: 3090.0648531781585\n",
      "Epoch 0 step 215: training accuarcy: 0.862\n",
      "Epoch 0 step 215: training loss: 2804.1588616722374\n",
      "Epoch 0 step 216: training accuarcy: 0.857\n",
      "Epoch 0 step 216: training loss: 2811.9769775781824\n",
      "Epoch 0 step 217: training accuarcy: 0.8775000000000001\n",
      "Epoch 0 step 217: training loss: 3217.5914734189073\n",
      "Epoch 0 step 218: training accuarcy: 0.8515\n",
      "Epoch 0 step 218: training loss: 2916.4126458956016\n",
      "Epoch 0 step 219: training accuarcy: 0.858\n",
      "Epoch 0 step 219: training loss: 3062.0638945512187\n",
      "Epoch 0 step 220: training accuarcy: 0.8605\n",
      "Epoch 0 step 220: training loss: 2745.4479734946403\n",
      "Epoch 0 step 221: training accuarcy: 0.8605\n",
      "Epoch 0 step 221: training loss: 2830.202963891038\n",
      "Epoch 0 step 222: training accuarcy: 0.877\n",
      "Epoch 0 step 222: training loss: 2627.077917534112\n",
      "Epoch 0 step 223: training accuarcy: 0.8785000000000001\n",
      "Epoch 0 step 223: training loss: 2825.772250905785\n",
      "Epoch 0 step 224: training accuarcy: 0.8685\n",
      "Epoch 0 step 224: training loss: 3235.9289515968526\n",
      "Epoch 0 step 225: training accuarcy: 0.8445\n",
      "Epoch 0 step 225: training loss: 2958.5161103588744\n",
      "Epoch 0 step 226: training accuarcy: 0.861\n",
      "Epoch 0 step 226: training loss: 3158.4760468000645\n",
      "Epoch 0 step 227: training accuarcy: 0.864\n",
      "Epoch 0 step 227: training loss: 2819.6072502888182\n",
      "Epoch 0 step 228: training accuarcy: 0.86\n",
      "Epoch 0 step 228: training loss: 2531.0118594681812\n",
      "Epoch 0 step 229: training accuarcy: 0.881\n",
      "Epoch 0 step 229: training loss: 2690.2795877672834\n",
      "Epoch 0 step 230: training accuarcy: 0.874\n",
      "Epoch 0 step 230: training loss: 3003.492719775135\n",
      "Epoch 0 step 231: training accuarcy: 0.848\n",
      "Epoch 0 step 231: training loss: 2617.87005093682\n",
      "Epoch 0 step 232: training accuarcy: 0.8735\n",
      "Epoch 0 step 232: training loss: 2585.6460204485184\n",
      "Epoch 0 step 233: training accuarcy: 0.8835000000000001\n",
      "Epoch 0 step 233: training loss: 2698.8740908511827\n",
      "Epoch 0 step 234: training accuarcy: 0.871\n",
      "Epoch 0 step 234: training loss: 2662.9351765948695\n",
      "Epoch 0 step 235: training accuarcy: 0.8815000000000001\n",
      "Epoch 0 step 235: training loss: 2841.6590688161687\n",
      "Epoch 0 step 236: training accuarcy: 0.8565\n",
      "Epoch 0 step 236: training loss: 2701.546741491082\n",
      "Epoch 0 step 237: training accuarcy: 0.873\n",
      "Epoch 0 step 237: training loss: 2752.2013764205362\n",
      "Epoch 0 step 238: training accuarcy: 0.87\n",
      "Epoch 0 step 238: training loss: 2912.9252519047523\n",
      "Epoch 0 step 239: training accuarcy: 0.87\n",
      "Epoch 0 step 239: training loss: 2777.0283204398165\n",
      "Epoch 0 step 240: training accuarcy: 0.873\n",
      "Epoch 0 step 240: training loss: 2717.6378940876375\n",
      "Epoch 0 step 241: training accuarcy: 0.8775000000000001\n",
      "Epoch 0 step 241: training loss: 2643.1103080784715\n",
      "Epoch 0 step 242: training accuarcy: 0.876\n",
      "Epoch 0 step 242: training loss: 2536.058053642865\n",
      "Epoch 0 step 243: training accuarcy: 0.867\n",
      "Epoch 0 step 243: training loss: 2582.874551638873\n",
      "Epoch 0 step 244: training accuarcy: 0.8755000000000001\n",
      "Epoch 0 step 244: training loss: 2654.224178196753\n",
      "Epoch 0 step 245: training accuarcy: 0.8665\n",
      "Epoch 0 step 245: training loss: 2888.075323245381\n",
      "Epoch 0 step 246: training accuarcy: 0.858\n",
      "Epoch 0 step 246: training loss: 2384.0974921080547\n",
      "Epoch 0 step 247: training accuarcy: 0.8875000000000001\n",
      "Epoch 0 step 247: training loss: 2595.603937628036\n",
      "Epoch 0 step 248: training accuarcy: 0.876\n",
      "Epoch 0 step 248: training loss: 2824.709870832249\n",
      "Epoch 0 step 249: training accuarcy: 0.8735\n",
      "Epoch 0 step 249: training loss: 2388.246672417077\n",
      "Epoch 0 step 250: training accuarcy: 0.889\n",
      "Epoch 0 step 250: training loss: 2675.337754609698\n",
      "Epoch 0 step 251: training accuarcy: 0.8725\n",
      "Epoch 0 step 251: training loss: 2594.1923241408804\n",
      "Epoch 0 step 252: training accuarcy: 0.8605\n",
      "Epoch 0 step 252: training loss: 2676.208062716311\n",
      "Epoch 0 step 253: training accuarcy: 0.8735\n",
      "Epoch 0 step 253: training loss: 2598.2832204467086\n",
      "Epoch 0 step 254: training accuarcy: 0.868\n",
      "Epoch 0 step 254: training loss: 2339.491998017416\n",
      "Epoch 0 step 255: training accuarcy: 0.8725\n",
      "Epoch 0 step 255: training loss: 2648.2252899944697\n",
      "Epoch 0 step 256: training accuarcy: 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 256: training loss: 2378.30446814462\n",
      "Epoch 0 step 257: training accuarcy: 0.8865000000000001\n",
      "Epoch 0 step 257: training loss: 2689.290625966659\n",
      "Epoch 0 step 258: training accuarcy: 0.869\n",
      "Epoch 0 step 258: training loss: 2599.7357488807675\n",
      "Epoch 0 step 259: training accuarcy: 0.8715\n",
      "Epoch 0 step 259: training loss: 2486.467270905074\n",
      "Epoch 0 step 260: training accuarcy: 0.8765000000000001\n",
      "Epoch 0 step 260: training loss: 2337.265990648815\n",
      "Epoch 0 step 261: training accuarcy: 0.8805000000000001\n",
      "Epoch 0 step 261: training loss: 2687.31482556325\n",
      "Epoch 0 step 262: training accuarcy: 0.8685\n",
      "Epoch 0 step 262: training loss: 1674.5314784903028\n",
      "Epoch 0 step 263: training accuarcy: 0.867948717948718\n",
      "Epoch 0: train loss 4187.308647550641, train accuarcy 0.7998396158218384\n",
      "Epoch 0: valid loss 2786.230083246025, valid accuarcy 0.8596118688583374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████████████████████▌                                                                                                             | 1/4 [01:59<05:57, 119.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 2552.9152741528496\n",
      "Epoch 1 step 264: training accuarcy: 0.865\n",
      "Epoch 1 step 264: training loss: 2251.3550618867876\n",
      "Epoch 1 step 265: training accuarcy: 0.887\n",
      "Epoch 1 step 265: training loss: 2371.301806707946\n",
      "Epoch 1 step 266: training accuarcy: 0.886\n",
      "Epoch 1 step 266: training loss: 2475.425561611898\n",
      "Epoch 1 step 267: training accuarcy: 0.887\n",
      "Epoch 1 step 267: training loss: 2431.748425860962\n",
      "Epoch 1 step 268: training accuarcy: 0.88\n",
      "Epoch 1 step 268: training loss: 2415.903708288012\n",
      "Epoch 1 step 269: training accuarcy: 0.882\n",
      "Epoch 1 step 269: training loss: 2253.7453844560528\n",
      "Epoch 1 step 270: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 270: training loss: 2278.2321205494254\n",
      "Epoch 1 step 271: training accuarcy: 0.897\n",
      "Epoch 1 step 271: training loss: 2450.5300269267764\n",
      "Epoch 1 step 272: training accuarcy: 0.877\n",
      "Epoch 1 step 272: training loss: 2203.113285587355\n",
      "Epoch 1 step 273: training accuarcy: 0.881\n",
      "Epoch 1 step 273: training loss: 2372.944399564242\n",
      "Epoch 1 step 274: training accuarcy: 0.876\n",
      "Epoch 1 step 274: training loss: 2352.9089512548335\n",
      "Epoch 1 step 275: training accuarcy: 0.8875000000000001\n",
      "Epoch 1 step 275: training loss: 2258.984652808922\n",
      "Epoch 1 step 276: training accuarcy: 0.884\n",
      "Epoch 1 step 276: training loss: 2484.355384275426\n",
      "Epoch 1 step 277: training accuarcy: 0.8805000000000001\n",
      "Epoch 1 step 277: training loss: 2276.9117471371987\n",
      "Epoch 1 step 278: training accuarcy: 0.889\n",
      "Epoch 1 step 278: training loss: 2359.741883997233\n",
      "Epoch 1 step 279: training accuarcy: 0.882\n",
      "Epoch 1 step 279: training loss: 2197.0006086643198\n",
      "Epoch 1 step 280: training accuarcy: 0.885\n",
      "Epoch 1 step 280: training loss: 2260.9284511793526\n",
      "Epoch 1 step 281: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 281: training loss: 2291.3402255827623\n",
      "Epoch 1 step 282: training accuarcy: 0.8795000000000001\n",
      "Epoch 1 step 282: training loss: 2079.9909354080182\n",
      "Epoch 1 step 283: training accuarcy: 0.897\n",
      "Epoch 1 step 283: training loss: 2402.2829586177795\n",
      "Epoch 1 step 284: training accuarcy: 0.879\n",
      "Epoch 1 step 284: training loss: 2181.7689744802583\n",
      "Epoch 1 step 285: training accuarcy: 0.8945000000000001\n",
      "Epoch 1 step 285: training loss: 2332.589109289428\n",
      "Epoch 1 step 286: training accuarcy: 0.886\n",
      "Epoch 1 step 286: training loss: 2219.9427545122917\n",
      "Epoch 1 step 287: training accuarcy: 0.891\n",
      "Epoch 1 step 287: training loss: 2385.7558655257762\n",
      "Epoch 1 step 288: training accuarcy: 0.8775000000000001\n",
      "Epoch 1 step 288: training loss: 2335.741113009508\n",
      "Epoch 1 step 289: training accuarcy: 0.884\n",
      "Epoch 1 step 289: training loss: 2449.0346925980493\n",
      "Epoch 1 step 290: training accuarcy: 0.8785000000000001\n",
      "Epoch 1 step 290: training loss: 2426.6407455169265\n",
      "Epoch 1 step 291: training accuarcy: 0.8825000000000001\n",
      "Epoch 1 step 291: training loss: 2340.0493120621004\n",
      "Epoch 1 step 292: training accuarcy: 0.8825000000000001\n",
      "Epoch 1 step 292: training loss: 2361.5169771037818\n",
      "Epoch 1 step 293: training accuarcy: 0.883\n",
      "Epoch 1 step 293: training loss: 2195.4486918677107\n",
      "Epoch 1 step 294: training accuarcy: 0.8945000000000001\n",
      "Epoch 1 step 294: training loss: 2131.211929759775\n",
      "Epoch 1 step 295: training accuarcy: 0.887\n",
      "Epoch 1 step 295: training loss: 2316.385100255253\n",
      "Epoch 1 step 296: training accuarcy: 0.882\n",
      "Epoch 1 step 296: training loss: 2174.7522492775224\n",
      "Epoch 1 step 297: training accuarcy: 0.8975\n",
      "Epoch 1 step 297: training loss: 2185.7966138710262\n",
      "Epoch 1 step 298: training accuarcy: 0.893\n",
      "Epoch 1 step 298: training loss: 2311.3921148575073\n",
      "Epoch 1 step 299: training accuarcy: 0.89\n",
      "Epoch 1 step 299: training loss: 2358.327838604575\n",
      "Epoch 1 step 300: training accuarcy: 0.8815000000000001\n",
      "Epoch 1 step 300: training loss: 2178.0232844839957\n",
      "Epoch 1 step 301: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 301: training loss: 2194.0734077393913\n",
      "Epoch 1 step 302: training accuarcy: 0.874\n",
      "Epoch 1 step 302: training loss: 2318.202962899098\n",
      "Epoch 1 step 303: training accuarcy: 0.889\n",
      "Epoch 1 step 303: training loss: 2205.7745323106415\n",
      "Epoch 1 step 304: training accuarcy: 0.901\n",
      "Epoch 1 step 304: training loss: 2125.2389632912227\n",
      "Epoch 1 step 305: training accuarcy: 0.8905000000000001\n",
      "Epoch 1 step 305: training loss: 2195.3517073480825\n",
      "Epoch 1 step 306: training accuarcy: 0.8855000000000001\n",
      "Epoch 1 step 306: training loss: 2239.971549565802\n",
      "Epoch 1 step 307: training accuarcy: 0.891\n",
      "Epoch 1 step 307: training loss: 2178.8006048689845\n",
      "Epoch 1 step 308: training accuarcy: 0.889\n",
      "Epoch 1 step 308: training loss: 2113.301795752581\n",
      "Epoch 1 step 309: training accuarcy: 0.8855000000000001\n",
      "Epoch 1 step 309: training loss: 2189.2348169845773\n",
      "Epoch 1 step 310: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 310: training loss: 2109.617074480912\n",
      "Epoch 1 step 311: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 311: training loss: 2297.9334671719143\n",
      "Epoch 1 step 312: training accuarcy: 0.882\n",
      "Epoch 1 step 312: training loss: 2253.0658018635886\n",
      "Epoch 1 step 313: training accuarcy: 0.886\n",
      "Epoch 1 step 313: training loss: 2084.6379491077\n",
      "Epoch 1 step 314: training accuarcy: 0.8815000000000001\n",
      "Epoch 1 step 314: training loss: 2036.5110771503626\n",
      "Epoch 1 step 315: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 315: training loss: 2175.7614915716204\n",
      "Epoch 1 step 316: training accuarcy: 0.8885000000000001\n",
      "Epoch 1 step 316: training loss: 2215.489449308202\n",
      "Epoch 1 step 317: training accuarcy: 0.884\n",
      "Epoch 1 step 317: training loss: 2052.060391857547\n",
      "Epoch 1 step 318: training accuarcy: 0.899\n",
      "Epoch 1 step 318: training loss: 2342.9312564373063\n",
      "Epoch 1 step 319: training accuarcy: 0.872\n",
      "Epoch 1 step 319: training loss: 2049.226123309411\n",
      "Epoch 1 step 320: training accuarcy: 0.899\n",
      "Epoch 1 step 320: training loss: 2058.8457709818063\n",
      "Epoch 1 step 321: training accuarcy: 0.893\n",
      "Epoch 1 step 321: training loss: 2329.5356995240277\n",
      "Epoch 1 step 322: training accuarcy: 0.8695\n",
      "Epoch 1 step 322: training loss: 2053.106600206562\n",
      "Epoch 1 step 323: training accuarcy: 0.895\n",
      "Epoch 1 step 323: training loss: 2349.3446491051955\n",
      "Epoch 1 step 324: training accuarcy: 0.877\n",
      "Epoch 1 step 324: training loss: 2112.7053143882463\n",
      "Epoch 1 step 325: training accuarcy: 0.9\n",
      "Epoch 1 step 325: training loss: 2301.9231353359796\n",
      "Epoch 1 step 326: training accuarcy: 0.8825000000000001\n",
      "Epoch 1 step 326: training loss: 1942.1753268747643\n",
      "Epoch 1 step 327: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 327: training loss: 2167.4971542222306\n",
      "Epoch 1 step 328: training accuarcy: 0.8955000000000001\n",
      "Epoch 1 step 328: training loss: 2287.1326348481557\n",
      "Epoch 1 step 329: training accuarcy: 0.869\n",
      "Epoch 1 step 329: training loss: 2004.1334314901715\n",
      "Epoch 1 step 330: training accuarcy: 0.896\n",
      "Epoch 1 step 330: training loss: 2009.7305099424402\n",
      "Epoch 1 step 331: training accuarcy: 0.89\n",
      "Epoch 1 step 331: training loss: 1964.5491804130174\n",
      "Epoch 1 step 332: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 332: training loss: 2027.6249471290735\n",
      "Epoch 1 step 333: training accuarcy: 0.8865000000000001\n",
      "Epoch 1 step 333: training loss: 2076.789575042258\n",
      "Epoch 1 step 334: training accuarcy: 0.8945000000000001\n",
      "Epoch 1 step 334: training loss: 2067.789757230791\n",
      "Epoch 1 step 335: training accuarcy: 0.885\n",
      "Epoch 1 step 335: training loss: 1873.0443858250483\n",
      "Epoch 1 step 336: training accuarcy: 0.895\n",
      "Epoch 1 step 336: training loss: 2041.1636299670981\n",
      "Epoch 1 step 337: training accuarcy: 0.892\n",
      "Epoch 1 step 337: training loss: 2210.123897524431\n",
      "Epoch 1 step 338: training accuarcy: 0.881\n",
      "Epoch 1 step 338: training loss: 1929.0964033955465\n",
      "Epoch 1 step 339: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 339: training loss: 2008.2476272568103\n",
      "Epoch 1 step 340: training accuarcy: 0.8945000000000001\n",
      "Epoch 1 step 340: training loss: 2193.811636158243\n",
      "Epoch 1 step 341: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 341: training loss: 1967.9653805528264\n",
      "Epoch 1 step 342: training accuarcy: 0.9065\n",
      "Epoch 1 step 342: training loss: 2146.1359073836134\n",
      "Epoch 1 step 343: training accuarcy: 0.8805000000000001\n",
      "Epoch 1 step 343: training loss: 2070.5002331721134\n",
      "Epoch 1 step 344: training accuarcy: 0.888\n",
      "Epoch 1 step 344: training loss: 2142.7384157868437\n",
      "Epoch 1 step 345: training accuarcy: 0.882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 345: training loss: 2117.064449048222\n",
      "Epoch 1 step 346: training accuarcy: 0.888\n",
      "Epoch 1 step 346: training loss: 1798.3851108974573\n",
      "Epoch 1 step 347: training accuarcy: 0.9\n",
      "Epoch 1 step 347: training loss: 2152.0554015751527\n",
      "Epoch 1 step 348: training accuarcy: 0.885\n",
      "Epoch 1 step 348: training loss: 2184.7654357681245\n",
      "Epoch 1 step 349: training accuarcy: 0.8785000000000001\n",
      "Epoch 1 step 349: training loss: 2188.517942902666\n",
      "Epoch 1 step 350: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 350: training loss: 2064.108603757277\n",
      "Epoch 1 step 351: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 351: training loss: 1972.5252772456945\n",
      "Epoch 1 step 352: training accuarcy: 0.879\n",
      "Epoch 1 step 352: training loss: 1766.1980789719723\n",
      "Epoch 1 step 353: training accuarcy: 0.9045\n",
      "Epoch 1 step 353: training loss: 2006.6456322273011\n",
      "Epoch 1 step 354: training accuarcy: 0.893\n",
      "Epoch 1 step 354: training loss: 1969.8088316062365\n",
      "Epoch 1 step 355: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 355: training loss: 1877.090465766485\n",
      "Epoch 1 step 356: training accuarcy: 0.8975\n",
      "Epoch 1 step 356: training loss: 2065.5242456823426\n",
      "Epoch 1 step 357: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 357: training loss: 2003.7331797933898\n",
      "Epoch 1 step 358: training accuarcy: 0.884\n",
      "Epoch 1 step 358: training loss: 1960.85191212561\n",
      "Epoch 1 step 359: training accuarcy: 0.8805000000000001\n",
      "Epoch 1 step 359: training loss: 1818.8395798948482\n",
      "Epoch 1 step 360: training accuarcy: 0.894\n",
      "Epoch 1 step 360: training loss: 2028.3507534657538\n",
      "Epoch 1 step 361: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 361: training loss: 1918.778666448561\n",
      "Epoch 1 step 362: training accuarcy: 0.8965\n",
      "Epoch 1 step 362: training loss: 2061.0423314419936\n",
      "Epoch 1 step 363: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 363: training loss: 1959.344242167701\n",
      "Epoch 1 step 364: training accuarcy: 0.891\n",
      "Epoch 1 step 364: training loss: 1846.0973737601078\n",
      "Epoch 1 step 365: training accuarcy: 0.9035\n",
      "Epoch 1 step 365: training loss: 1885.343856717188\n",
      "Epoch 1 step 366: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 366: training loss: 2059.096689740268\n",
      "Epoch 1 step 367: training accuarcy: 0.8765000000000001\n",
      "Epoch 1 step 367: training loss: 2156.825301455015\n",
      "Epoch 1 step 368: training accuarcy: 0.881\n",
      "Epoch 1 step 368: training loss: 1998.2240742651481\n",
      "Epoch 1 step 369: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 369: training loss: 1911.9468862680533\n",
      "Epoch 1 step 370: training accuarcy: 0.8865000000000001\n",
      "Epoch 1 step 370: training loss: 2016.3829438082464\n",
      "Epoch 1 step 371: training accuarcy: 0.88\n",
      "Epoch 1 step 371: training loss: 1712.243855037584\n",
      "Epoch 1 step 372: training accuarcy: 0.901\n",
      "Epoch 1 step 372: training loss: 2029.3836661748865\n",
      "Epoch 1 step 373: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 373: training loss: 1867.3474647732658\n",
      "Epoch 1 step 374: training accuarcy: 0.886\n",
      "Epoch 1 step 374: training loss: 1867.5328608159982\n",
      "Epoch 1 step 375: training accuarcy: 0.898\n",
      "Epoch 1 step 375: training loss: 1901.4405545151233\n",
      "Epoch 1 step 376: training accuarcy: 0.8945000000000001\n",
      "Epoch 1 step 376: training loss: 1985.8541158370617\n",
      "Epoch 1 step 377: training accuarcy: 0.8875000000000001\n",
      "Epoch 1 step 377: training loss: 1979.1990182086615\n",
      "Epoch 1 step 378: training accuarcy: 0.898\n",
      "Epoch 1 step 378: training loss: 2006.3279444013594\n",
      "Epoch 1 step 379: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 379: training loss: 1946.5858839448927\n",
      "Epoch 1 step 380: training accuarcy: 0.888\n",
      "Epoch 1 step 380: training loss: 1799.5486916317159\n",
      "Epoch 1 step 381: training accuarcy: 0.8965\n",
      "Epoch 1 step 381: training loss: 1907.1474451152726\n",
      "Epoch 1 step 382: training accuarcy: 0.894\n",
      "Epoch 1 step 382: training loss: 2041.5599547651327\n",
      "Epoch 1 step 383: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 383: training loss: 1903.6291524679812\n",
      "Epoch 1 step 384: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 384: training loss: 1836.8508672587568\n",
      "Epoch 1 step 385: training accuarcy: 0.8855000000000001\n",
      "Epoch 1 step 385: training loss: 1852.8575925274877\n",
      "Epoch 1 step 386: training accuarcy: 0.8955000000000001\n",
      "Epoch 1 step 386: training loss: 1919.9726395148932\n",
      "Epoch 1 step 387: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 387: training loss: 1871.0864495160768\n",
      "Epoch 1 step 388: training accuarcy: 0.901\n",
      "Epoch 1 step 388: training loss: 2035.5527392358308\n",
      "Epoch 1 step 389: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 389: training loss: 1835.5375824178902\n",
      "Epoch 1 step 390: training accuarcy: 0.897\n",
      "Epoch 1 step 390: training loss: 1749.3378829251853\n",
      "Epoch 1 step 391: training accuarcy: 0.901\n",
      "Epoch 1 step 391: training loss: 2062.1300558817875\n",
      "Epoch 1 step 392: training accuarcy: 0.886\n",
      "Epoch 1 step 392: training loss: 1813.3391103144522\n",
      "Epoch 1 step 393: training accuarcy: 0.9025\n",
      "Epoch 1 step 393: training loss: 1823.0786196064469\n",
      "Epoch 1 step 394: training accuarcy: 0.898\n",
      "Epoch 1 step 394: training loss: 2152.5770892753135\n",
      "Epoch 1 step 395: training accuarcy: 0.8785000000000001\n",
      "Epoch 1 step 395: training loss: 1947.3991224795789\n",
      "Epoch 1 step 396: training accuarcy: 0.889\n",
      "Epoch 1 step 396: training loss: 2017.8247246609726\n",
      "Epoch 1 step 397: training accuarcy: 0.8775000000000001\n",
      "Epoch 1 step 397: training loss: 1914.0206666863075\n",
      "Epoch 1 step 398: training accuarcy: 0.887\n",
      "Epoch 1 step 398: training loss: 1822.3752451839841\n",
      "Epoch 1 step 399: training accuarcy: 0.8985\n",
      "Epoch 1 step 399: training loss: 1848.5407288135648\n",
      "Epoch 1 step 400: training accuarcy: 0.896\n",
      "Epoch 1 step 400: training loss: 1963.7094510654974\n",
      "Epoch 1 step 401: training accuarcy: 0.8865000000000001\n",
      "Epoch 1 step 401: training loss: 2022.1798374474033\n",
      "Epoch 1 step 402: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 402: training loss: 1925.5384633413837\n",
      "Epoch 1 step 403: training accuarcy: 0.8905000000000001\n",
      "Epoch 1 step 403: training loss: 1660.1749971152556\n",
      "Epoch 1 step 404: training accuarcy: 0.9105\n",
      "Epoch 1 step 404: training loss: 1834.578303851666\n",
      "Epoch 1 step 405: training accuarcy: 0.8945000000000001\n",
      "Epoch 1 step 405: training loss: 1742.770742840139\n",
      "Epoch 1 step 406: training accuarcy: 0.8955000000000001\n",
      "Epoch 1 step 406: training loss: 1817.9352776500737\n",
      "Epoch 1 step 407: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 407: training loss: 1973.1671999963091\n",
      "Epoch 1 step 408: training accuarcy: 0.882\n",
      "Epoch 1 step 408: training loss: 1868.5823320641855\n",
      "Epoch 1 step 409: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 409: training loss: 1822.8389257476792\n",
      "Epoch 1 step 410: training accuarcy: 0.896\n",
      "Epoch 1 step 410: training loss: 1952.1960076765076\n",
      "Epoch 1 step 411: training accuarcy: 0.8795000000000001\n",
      "Epoch 1 step 411: training loss: 1815.0540877739477\n",
      "Epoch 1 step 412: training accuarcy: 0.8995\n",
      "Epoch 1 step 412: training loss: 1824.7319569288718\n",
      "Epoch 1 step 413: training accuarcy: 0.8975\n",
      "Epoch 1 step 413: training loss: 1909.3513439186029\n",
      "Epoch 1 step 414: training accuarcy: 0.881\n",
      "Epoch 1 step 414: training loss: 1782.1190238518907\n",
      "Epoch 1 step 415: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 415: training loss: 2055.983001978084\n",
      "Epoch 1 step 416: training accuarcy: 0.8845000000000001\n",
      "Epoch 1 step 416: training loss: 1812.766737467886\n",
      "Epoch 1 step 417: training accuarcy: 0.8995\n",
      "Epoch 1 step 417: training loss: 1737.1731377232381\n",
      "Epoch 1 step 418: training accuarcy: 0.904\n",
      "Epoch 1 step 418: training loss: 1825.2567588871555\n",
      "Epoch 1 step 419: training accuarcy: 0.897\n",
      "Epoch 1 step 419: training loss: 1922.7602077266781\n",
      "Epoch 1 step 420: training accuarcy: 0.889\n",
      "Epoch 1 step 420: training loss: 1805.4780903919764\n",
      "Epoch 1 step 421: training accuarcy: 0.899\n",
      "Epoch 1 step 421: training loss: 1654.750339942861\n",
      "Epoch 1 step 422: training accuarcy: 0.9025\n",
      "Epoch 1 step 422: training loss: 1899.2393123747502\n",
      "Epoch 1 step 423: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 423: training loss: 1885.572414919951\n",
      "Epoch 1 step 424: training accuarcy: 0.883\n",
      "Epoch 1 step 424: training loss: 1723.4794039343428\n",
      "Epoch 1 step 425: training accuarcy: 0.902\n",
      "Epoch 1 step 425: training loss: 1838.5619892042446\n",
      "Epoch 1 step 426: training accuarcy: 0.8855000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 426: training loss: 1597.4124785215558\n",
      "Epoch 1 step 427: training accuarcy: 0.907\n",
      "Epoch 1 step 427: training loss: 1672.195608623873\n",
      "Epoch 1 step 428: training accuarcy: 0.8995\n",
      "Epoch 1 step 428: training loss: 1663.9937274358813\n",
      "Epoch 1 step 429: training accuarcy: 0.8995\n",
      "Epoch 1 step 429: training loss: 1842.9085585607784\n",
      "Epoch 1 step 430: training accuarcy: 0.8885000000000001\n",
      "Epoch 1 step 430: training loss: 1962.1751028924832\n",
      "Epoch 1 step 431: training accuarcy: 0.8855000000000001\n",
      "Epoch 1 step 431: training loss: 2014.998471977296\n",
      "Epoch 1 step 432: training accuarcy: 0.885\n",
      "Epoch 1 step 432: training loss: 1661.702527930444\n",
      "Epoch 1 step 433: training accuarcy: 0.904\n",
      "Epoch 1 step 433: training loss: 1743.6693833639215\n",
      "Epoch 1 step 434: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 434: training loss: 1745.129272152571\n",
      "Epoch 1 step 435: training accuarcy: 0.8975\n",
      "Epoch 1 step 435: training loss: 1783.0792649050736\n",
      "Epoch 1 step 436: training accuarcy: 0.893\n",
      "Epoch 1 step 436: training loss: 1613.12844999482\n",
      "Epoch 1 step 437: training accuarcy: 0.903\n",
      "Epoch 1 step 437: training loss: 1705.323864638407\n",
      "Epoch 1 step 438: training accuarcy: 0.897\n",
      "Epoch 1 step 438: training loss: 1772.1064715951838\n",
      "Epoch 1 step 439: training accuarcy: 0.898\n",
      "Epoch 1 step 439: training loss: 1556.030013912337\n",
      "Epoch 1 step 440: training accuarcy: 0.9095\n",
      "Epoch 1 step 440: training loss: 1737.0757015212007\n",
      "Epoch 1 step 441: training accuarcy: 0.9\n",
      "Epoch 1 step 441: training loss: 1826.4553478004163\n",
      "Epoch 1 step 442: training accuarcy: 0.892\n",
      "Epoch 1 step 442: training loss: 1854.2042554750553\n",
      "Epoch 1 step 443: training accuarcy: 0.891\n",
      "Epoch 1 step 443: training loss: 1704.7992747849307\n",
      "Epoch 1 step 444: training accuarcy: 0.897\n",
      "Epoch 1 step 444: training loss: 1766.9992168203848\n",
      "Epoch 1 step 445: training accuarcy: 0.9035\n",
      "Epoch 1 step 445: training loss: 2033.7102670262893\n",
      "Epoch 1 step 446: training accuarcy: 0.88\n",
      "Epoch 1 step 446: training loss: 1717.5366807053183\n",
      "Epoch 1 step 447: training accuarcy: 0.899\n",
      "Epoch 1 step 447: training loss: 1901.1261225704766\n",
      "Epoch 1 step 448: training accuarcy: 0.893\n",
      "Epoch 1 step 448: training loss: 1776.3921777945857\n",
      "Epoch 1 step 449: training accuarcy: 0.89\n",
      "Epoch 1 step 449: training loss: 1709.5871772198398\n",
      "Epoch 1 step 450: training accuarcy: 0.9025\n",
      "Epoch 1 step 450: training loss: 1734.9756769653823\n",
      "Epoch 1 step 451: training accuarcy: 0.9\n",
      "Epoch 1 step 451: training loss: 1635.8633151381932\n",
      "Epoch 1 step 452: training accuarcy: 0.8995\n",
      "Epoch 1 step 452: training loss: 1767.057655797471\n",
      "Epoch 1 step 453: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 453: training loss: 1707.8289872554792\n",
      "Epoch 1 step 454: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 454: training loss: 1945.601956843693\n",
      "Epoch 1 step 455: training accuarcy: 0.892\n",
      "Epoch 1 step 455: training loss: 1660.513050779206\n",
      "Epoch 1 step 456: training accuarcy: 0.8955000000000001\n",
      "Epoch 1 step 456: training loss: 1697.1337209465369\n",
      "Epoch 1 step 457: training accuarcy: 0.8985\n",
      "Epoch 1 step 457: training loss: 1716.1069380022761\n",
      "Epoch 1 step 458: training accuarcy: 0.8975\n",
      "Epoch 1 step 458: training loss: 1653.47647725536\n",
      "Epoch 1 step 459: training accuarcy: 0.9085\n",
      "Epoch 1 step 459: training loss: 1775.6955380887093\n",
      "Epoch 1 step 460: training accuarcy: 0.9\n",
      "Epoch 1 step 460: training loss: 1777.5362778122758\n",
      "Epoch 1 step 461: training accuarcy: 0.9\n",
      "Epoch 1 step 461: training loss: 1687.3722209440966\n",
      "Epoch 1 step 462: training accuarcy: 0.902\n",
      "Epoch 1 step 462: training loss: 1799.2270951653345\n",
      "Epoch 1 step 463: training accuarcy: 0.8985\n",
      "Epoch 1 step 463: training loss: 1650.5370058139501\n",
      "Epoch 1 step 464: training accuarcy: 0.9\n",
      "Epoch 1 step 464: training loss: 1651.804630601644\n",
      "Epoch 1 step 465: training accuarcy: 0.9\n",
      "Epoch 1 step 465: training loss: 1576.240981496341\n",
      "Epoch 1 step 466: training accuarcy: 0.906\n",
      "Epoch 1 step 466: training loss: 1664.6101880108454\n",
      "Epoch 1 step 467: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 467: training loss: 1869.1195603039814\n",
      "Epoch 1 step 468: training accuarcy: 0.886\n",
      "Epoch 1 step 468: training loss: 1620.1255012698657\n",
      "Epoch 1 step 469: training accuarcy: 0.903\n",
      "Epoch 1 step 469: training loss: 1578.0765488367724\n",
      "Epoch 1 step 470: training accuarcy: 0.8975\n",
      "Epoch 1 step 470: training loss: 1793.8254525139246\n",
      "Epoch 1 step 471: training accuarcy: 0.8995\n",
      "Epoch 1 step 471: training loss: 1693.7089359202537\n",
      "Epoch 1 step 472: training accuarcy: 0.9025\n",
      "Epoch 1 step 472: training loss: 1765.4688584437301\n",
      "Epoch 1 step 473: training accuarcy: 0.897\n",
      "Epoch 1 step 473: training loss: 1750.6093377304644\n",
      "Epoch 1 step 474: training accuarcy: 0.9045\n",
      "Epoch 1 step 474: training loss: 1606.014516553136\n",
      "Epoch 1 step 475: training accuarcy: 0.9145\n",
      "Epoch 1 step 475: training loss: 1645.4512324915481\n",
      "Epoch 1 step 476: training accuarcy: 0.9015\n",
      "Epoch 1 step 476: training loss: 1820.227027069628\n",
      "Epoch 1 step 477: training accuarcy: 0.8895000000000001\n",
      "Epoch 1 step 477: training loss: 1540.2035349782682\n",
      "Epoch 1 step 478: training accuarcy: 0.903\n",
      "Epoch 1 step 478: training loss: 1840.374128571708\n",
      "Epoch 1 step 479: training accuarcy: 0.8885000000000001\n",
      "Epoch 1 step 479: training loss: 1580.660126323888\n",
      "Epoch 1 step 480: training accuarcy: 0.8975\n",
      "Epoch 1 step 480: training loss: 1660.1750602973984\n",
      "Epoch 1 step 481: training accuarcy: 0.9005\n",
      "Epoch 1 step 481: training loss: 1826.926457755991\n",
      "Epoch 1 step 482: training accuarcy: 0.8995\n",
      "Epoch 1 step 482: training loss: 1635.0689233316361\n",
      "Epoch 1 step 483: training accuarcy: 0.9015\n",
      "Epoch 1 step 483: training loss: 1765.855649456923\n",
      "Epoch 1 step 484: training accuarcy: 0.888\n",
      "Epoch 1 step 484: training loss: 1597.5367628546655\n",
      "Epoch 1 step 485: training accuarcy: 0.9035\n",
      "Epoch 1 step 485: training loss: 1714.7876936066252\n",
      "Epoch 1 step 486: training accuarcy: 0.8975\n",
      "Epoch 1 step 486: training loss: 1614.5289239374388\n",
      "Epoch 1 step 487: training accuarcy: 0.897\n",
      "Epoch 1 step 487: training loss: 1696.2478057225921\n",
      "Epoch 1 step 488: training accuarcy: 0.8995\n",
      "Epoch 1 step 488: training loss: 1698.100596175498\n",
      "Epoch 1 step 489: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 489: training loss: 1716.1522200708314\n",
      "Epoch 1 step 490: training accuarcy: 0.8995\n",
      "Epoch 1 step 490: training loss: 1902.2403805345066\n",
      "Epoch 1 step 491: training accuarcy: 0.8805000000000001\n",
      "Epoch 1 step 491: training loss: 1859.0599905667177\n",
      "Epoch 1 step 492: training accuarcy: 0.889\n",
      "Epoch 1 step 492: training loss: 1709.1232854475707\n",
      "Epoch 1 step 493: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 493: training loss: 1744.4678964356276\n",
      "Epoch 1 step 494: training accuarcy: 0.8805000000000001\n",
      "Epoch 1 step 494: training loss: 1747.7926008442098\n",
      "Epoch 1 step 495: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 495: training loss: 1605.2193028100955\n",
      "Epoch 1 step 496: training accuarcy: 0.906\n",
      "Epoch 1 step 496: training loss: 1785.923359025476\n",
      "Epoch 1 step 497: training accuarcy: 0.888\n",
      "Epoch 1 step 497: training loss: 1707.8302483918321\n",
      "Epoch 1 step 498: training accuarcy: 0.89\n",
      "Epoch 1 step 498: training loss: 1933.4479231240607\n",
      "Epoch 1 step 499: training accuarcy: 0.8835000000000001\n",
      "Epoch 1 step 499: training loss: 1571.7284839243255\n",
      "Epoch 1 step 500: training accuarcy: 0.894\n",
      "Epoch 1 step 500: training loss: 1714.9166192458997\n",
      "Epoch 1 step 501: training accuarcy: 0.8945000000000001\n",
      "Epoch 1 step 501: training loss: 1818.3381940309705\n",
      "Epoch 1 step 502: training accuarcy: 0.887\n",
      "Epoch 1 step 502: training loss: 1903.4188580821906\n",
      "Epoch 1 step 503: training accuarcy: 0.886\n",
      "Epoch 1 step 503: training loss: 1511.2218660248072\n",
      "Epoch 1 step 504: training accuarcy: 0.9075\n",
      "Epoch 1 step 504: training loss: 1506.2910263967683\n",
      "Epoch 1 step 505: training accuarcy: 0.909\n",
      "Epoch 1 step 505: training loss: 1636.9349754539912\n",
      "Epoch 1 step 506: training accuarcy: 0.9005\n",
      "Epoch 1 step 506: training loss: 1674.325444239174\n",
      "Epoch 1 step 507: training accuarcy: 0.903\n",
      "Epoch 1 step 507: training loss: 1797.640288383341\n",
      "Epoch 1 step 508: training accuarcy: 0.888\n",
      "Epoch 1 step 508: training loss: 1702.4989587956456\n",
      "Epoch 1 step 509: training accuarcy: 0.8985\n",
      "Epoch 1 step 509: training loss: 1620.6416001221244\n",
      "Epoch 1 step 510: training accuarcy: 0.8905000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 510: training loss: 1747.6074247200322\n",
      "Epoch 1 step 511: training accuarcy: 0.9015\n",
      "Epoch 1 step 511: training loss: 1660.8542156667604\n",
      "Epoch 1 step 512: training accuarcy: 0.903\n",
      "Epoch 1 step 512: training loss: 1661.4022571047935\n",
      "Epoch 1 step 513: training accuarcy: 0.894\n",
      "Epoch 1 step 513: training loss: 1684.4944126169858\n",
      "Epoch 1 step 514: training accuarcy: 0.895\n",
      "Epoch 1 step 514: training loss: 1584.5413906725576\n",
      "Epoch 1 step 515: training accuarcy: 0.9\n",
      "Epoch 1 step 515: training loss: 1562.1089164863388\n",
      "Epoch 1 step 516: training accuarcy: 0.9055\n",
      "Epoch 1 step 516: training loss: 1635.7283621700608\n",
      "Epoch 1 step 517: training accuarcy: 0.902\n",
      "Epoch 1 step 517: training loss: 1617.3579243219324\n",
      "Epoch 1 step 518: training accuarcy: 0.9005\n",
      "Epoch 1 step 518: training loss: 1650.9778458697035\n",
      "Epoch 1 step 519: training accuarcy: 0.899\n",
      "Epoch 1 step 519: training loss: 1705.6762999891944\n",
      "Epoch 1 step 520: training accuarcy: 0.888\n",
      "Epoch 1 step 520: training loss: 1776.6185028500033\n",
      "Epoch 1 step 521: training accuarcy: 0.8955000000000001\n",
      "Epoch 1 step 521: training loss: 1529.9515838638736\n",
      "Epoch 1 step 522: training accuarcy: 0.91\n",
      "Epoch 1 step 522: training loss: 1585.740444194627\n",
      "Epoch 1 step 523: training accuarcy: 0.895\n",
      "Epoch 1 step 523: training loss: 1504.3182178643174\n",
      "Epoch 1 step 524: training accuarcy: 0.9045\n",
      "Epoch 1 step 524: training loss: 1496.4746686663884\n",
      "Epoch 1 step 525: training accuarcy: 0.9015\n",
      "Epoch 1 step 525: training loss: 1107.120013382062\n",
      "Epoch 1 step 526: training accuarcy: 0.8884615384615384\n",
      "Epoch 1: train loss 1929.2369275918418, train accuarcy 0.88047194480896\n",
      "Epoch 1: valid loss 2038.4559416846714, valid accuarcy 0.8715382814407349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████████████████████████████████████                                                                         | 2/4 [03:54<03:56, 118.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 1646.548359780817\n",
      "Epoch 2 step 527: training accuarcy: 0.897\n",
      "Epoch 2 step 527: training loss: 1757.5727729512162\n",
      "Epoch 2 step 528: training accuarcy: 0.902\n",
      "Epoch 2 step 528: training loss: 1592.6025893717006\n",
      "Epoch 2 step 529: training accuarcy: 0.9075\n",
      "Epoch 2 step 529: training loss: 1564.6298620474167\n",
      "Epoch 2 step 530: training accuarcy: 0.9085\n",
      "Epoch 2 step 530: training loss: 1445.8536056388948\n",
      "Epoch 2 step 531: training accuarcy: 0.9165\n",
      "Epoch 2 step 531: training loss: 1588.1101836552687\n",
      "Epoch 2 step 532: training accuarcy: 0.9015\n",
      "Epoch 2 step 532: training loss: 1532.0823423229117\n",
      "Epoch 2 step 533: training accuarcy: 0.911\n",
      "Epoch 2 step 533: training loss: 1592.49515958836\n",
      "Epoch 2 step 534: training accuarcy: 0.9005\n",
      "Epoch 2 step 534: training loss: 1550.0078764846326\n",
      "Epoch 2 step 535: training accuarcy: 0.901\n",
      "Epoch 2 step 535: training loss: 1622.256007048045\n",
      "Epoch 2 step 536: training accuarcy: 0.902\n",
      "Epoch 2 step 536: training loss: 1637.8426852375728\n",
      "Epoch 2 step 537: training accuarcy: 0.894\n",
      "Epoch 2 step 537: training loss: 1517.2828349155063\n",
      "Epoch 2 step 538: training accuarcy: 0.909\n",
      "Epoch 2 step 538: training loss: 1525.6637306190007\n",
      "Epoch 2 step 539: training accuarcy: 0.8955000000000001\n",
      "Epoch 2 step 539: training loss: 1704.9892780967525\n",
      "Epoch 2 step 540: training accuarcy: 0.889\n",
      "Epoch 2 step 540: training loss: 1685.255497755874\n",
      "Epoch 2 step 541: training accuarcy: 0.9005\n",
      "Epoch 2 step 541: training loss: 1531.7282375525383\n",
      "Epoch 2 step 542: training accuarcy: 0.8935000000000001\n",
      "Epoch 2 step 542: training loss: 1424.2975151524379\n",
      "Epoch 2 step 543: training accuarcy: 0.9095\n",
      "Epoch 2 step 543: training loss: 1640.2681762684815\n",
      "Epoch 2 step 544: training accuarcy: 0.8975\n",
      "Epoch 2 step 544: training loss: 1557.320216523082\n",
      "Epoch 2 step 545: training accuarcy: 0.8975\n",
      "Epoch 2 step 545: training loss: 1534.7455189868235\n",
      "Epoch 2 step 546: training accuarcy: 0.9035\n",
      "Epoch 2 step 546: training loss: 1691.1596574029145\n",
      "Epoch 2 step 547: training accuarcy: 0.896\n",
      "Epoch 2 step 547: training loss: 1669.9134413715287\n",
      "Epoch 2 step 548: training accuarcy: 0.8985\n",
      "Epoch 2 step 548: training loss: 1434.4360154599256\n",
      "Epoch 2 step 549: training accuarcy: 0.909\n",
      "Epoch 2 step 549: training loss: 1698.9136667202838\n",
      "Epoch 2 step 550: training accuarcy: 0.8945000000000001\n",
      "Epoch 2 step 550: training loss: 1622.2137918917156\n",
      "Epoch 2 step 551: training accuarcy: 0.8995\n",
      "Epoch 2 step 551: training loss: 1507.3831389067163\n",
      "Epoch 2 step 552: training accuarcy: 0.8985\n",
      "Epoch 2 step 552: training loss: 1608.2161687086048\n",
      "Epoch 2 step 553: training accuarcy: 0.9015\n",
      "Epoch 2 step 553: training loss: 1642.6123262479161\n",
      "Epoch 2 step 554: training accuarcy: 0.8985\n",
      "Epoch 2 step 554: training loss: 1795.695927594505\n",
      "Epoch 2 step 555: training accuarcy: 0.886\n",
      "Epoch 2 step 555: training loss: 1589.565505746306\n",
      "Epoch 2 step 556: training accuarcy: 0.897\n",
      "Epoch 2 step 556: training loss: 1603.8961778927005\n",
      "Epoch 2 step 557: training accuarcy: 0.9055\n",
      "Epoch 2 step 557: training loss: 1505.8733128449035\n",
      "Epoch 2 step 558: training accuarcy: 0.8965\n",
      "Epoch 2 step 558: training loss: 1521.3577889068665\n",
      "Epoch 2 step 559: training accuarcy: 0.9035\n",
      "Epoch 2 step 559: training loss: 1569.6072215691743\n",
      "Epoch 2 step 560: training accuarcy: 0.898\n",
      "Epoch 2 step 560: training loss: 1600.2811428054556\n",
      "Epoch 2 step 561: training accuarcy: 0.905\n",
      "Epoch 2 step 561: training loss: 1699.109173562847\n",
      "Epoch 2 step 562: training accuarcy: 0.8965\n",
      "Epoch 2 step 562: training loss: 1500.8968781764781\n",
      "Epoch 2 step 563: training accuarcy: 0.9\n",
      "Epoch 2 step 563: training loss: 1680.6191932254233\n",
      "Epoch 2 step 564: training accuarcy: 0.8995\n",
      "Epoch 2 step 564: training loss: 1496.3918535161908\n",
      "Epoch 2 step 565: training accuarcy: 0.9045\n",
      "Epoch 2 step 565: training loss: 1504.439358436493\n",
      "Epoch 2 step 566: training accuarcy: 0.91\n",
      "Epoch 2 step 566: training loss: 1491.5801231914943\n",
      "Epoch 2 step 567: training accuarcy: 0.9085\n",
      "Epoch 2 step 567: training loss: 1647.444714584516\n",
      "Epoch 2 step 568: training accuarcy: 0.8945000000000001\n",
      "Epoch 2 step 568: training loss: 1459.3448121222466\n",
      "Epoch 2 step 569: training accuarcy: 0.904\n",
      "Epoch 2 step 569: training loss: 1519.9751079040257\n",
      "Epoch 2 step 570: training accuarcy: 0.904\n",
      "Epoch 2 step 570: training loss: 1696.7244443941977\n",
      "Epoch 2 step 571: training accuarcy: 0.892\n",
      "Epoch 2 step 571: training loss: 1639.8648038304352\n",
      "Epoch 2 step 572: training accuarcy: 0.9025\n",
      "Epoch 2 step 572: training loss: 1551.557375699878\n",
      "Epoch 2 step 573: training accuarcy: 0.897\n",
      "Epoch 2 step 573: training loss: 1547.673310297452\n",
      "Epoch 2 step 574: training accuarcy: 0.9045\n",
      "Epoch 2 step 574: training loss: 1701.9044769653556\n",
      "Epoch 2 step 575: training accuarcy: 0.891\n",
      "Epoch 2 step 575: training loss: 1569.8475365126196\n",
      "Epoch 2 step 576: training accuarcy: 0.905\n",
      "Epoch 2 step 576: training loss: 1578.7830220754572\n",
      "Epoch 2 step 577: training accuarcy: 0.8955000000000001\n",
      "Epoch 2 step 577: training loss: 1582.840456241201\n",
      "Epoch 2 step 578: training accuarcy: 0.898\n",
      "Epoch 2 step 578: training loss: 1480.13107509262\n",
      "Epoch 2 step 579: training accuarcy: 0.904\n",
      "Epoch 2 step 579: training loss: 1535.7364779836917\n",
      "Epoch 2 step 580: training accuarcy: 0.9025\n",
      "Epoch 2 step 580: training loss: 1586.261910000649\n",
      "Epoch 2 step 581: training accuarcy: 0.895\n",
      "Epoch 2 step 581: training loss: 1604.8374710293485\n",
      "Epoch 2 step 582: training accuarcy: 0.8985\n",
      "Epoch 2 step 582: training loss: 1570.4263054813214\n",
      "Epoch 2 step 583: training accuarcy: 0.9035\n",
      "Epoch 2 step 583: training loss: 1650.6571524080905\n",
      "Epoch 2 step 584: training accuarcy: 0.892\n",
      "Epoch 2 step 584: training loss: 1510.49500883236\n",
      "Epoch 2 step 585: training accuarcy: 0.9105\n",
      "Epoch 2 step 585: training loss: 1612.672166176379\n",
      "Epoch 2 step 586: training accuarcy: 0.9\n",
      "Epoch 2 step 586: training loss: 1459.6678881925516\n",
      "Epoch 2 step 587: training accuarcy: 0.912\n",
      "Epoch 2 step 587: training loss: 1461.4148275012208\n",
      "Epoch 2 step 588: training accuarcy: 0.9045\n",
      "Epoch 2 step 588: training loss: 1350.779600412912\n",
      "Epoch 2 step 589: training accuarcy: 0.91\n",
      "Epoch 2 step 589: training loss: 1514.7067574016849\n",
      "Epoch 2 step 590: training accuarcy: 0.9\n",
      "Epoch 2 step 590: training loss: 1650.2893347091035\n",
      "Epoch 2 step 591: training accuarcy: 0.9025\n",
      "Epoch 2 step 591: training loss: 1523.7270261872366\n",
      "Epoch 2 step 592: training accuarcy: 0.9095\n",
      "Epoch 2 step 592: training loss: 1536.9126806854701\n",
      "Epoch 2 step 593: training accuarcy: 0.9065\n",
      "Epoch 2 step 593: training loss: 1452.1747401615075\n",
      "Epoch 2 step 594: training accuarcy: 0.913\n",
      "Epoch 2 step 594: training loss: 1561.8828435544065\n",
      "Epoch 2 step 595: training accuarcy: 0.906\n",
      "Epoch 2 step 595: training loss: 1533.5756964375669\n",
      "Epoch 2 step 596: training accuarcy: 0.9055\n",
      "Epoch 2 step 596: training loss: 1613.0785147821462\n",
      "Epoch 2 step 597: training accuarcy: 0.8945000000000001\n",
      "Epoch 2 step 597: training loss: 1715.5495667061568\n",
      "Epoch 2 step 598: training accuarcy: 0.893\n",
      "Epoch 2 step 598: training loss: 1441.9481691437422\n",
      "Epoch 2 step 599: training accuarcy: 0.9105\n",
      "Epoch 2 step 599: training loss: 1503.5085422691232\n",
      "Epoch 2 step 600: training accuarcy: 0.9045\n",
      "Epoch 2 step 600: training loss: 1533.5778501798832\n",
      "Epoch 2 step 601: training accuarcy: 0.8955000000000001\n",
      "Epoch 2 step 601: training loss: 1520.4359373397072\n",
      "Epoch 2 step 602: training accuarcy: 0.9035\n",
      "Epoch 2 step 602: training loss: 1455.335723769923\n",
      "Epoch 2 step 603: training accuarcy: 0.9035\n",
      "Epoch 2 step 603: training loss: 1411.9595659345255\n",
      "Epoch 2 step 604: training accuarcy: 0.906\n",
      "Epoch 2 step 604: training loss: 1553.1558108589377\n",
      "Epoch 2 step 605: training accuarcy: 0.909\n",
      "Epoch 2 step 605: training loss: 1847.8960102872848\n",
      "Epoch 2 step 606: training accuarcy: 0.893\n",
      "Epoch 2 step 606: training loss: 1484.1232333390449\n",
      "Epoch 2 step 607: training accuarcy: 0.916\n",
      "Epoch 2 step 607: training loss: 1470.987029527055\n",
      "Epoch 2 step 608: training accuarcy: 0.91\n",
      "Epoch 2 step 608: training loss: 1700.6107525716893\n",
      "Epoch 2 step 609: training accuarcy: 0.886\n",
      "Epoch 2 step 609: training loss: 1634.8915048968772\n",
      "Epoch 2 step 610: training accuarcy: 0.893\n",
      "Epoch 2 step 610: training loss: 1539.9282257719399\n",
      "Epoch 2 step 611: training accuarcy: 0.9065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 611: training loss: 1538.910563896608\n",
      "Epoch 2 step 612: training accuarcy: 0.8945000000000001\n",
      "Epoch 2 step 612: training loss: 1544.243201069412\n",
      "Epoch 2 step 613: training accuarcy: 0.9\n",
      "Epoch 2 step 613: training loss: 1416.5144179518422\n",
      "Epoch 2 step 614: training accuarcy: 0.9035\n",
      "Epoch 2 step 614: training loss: 1429.4535641328469\n",
      "Epoch 2 step 615: training accuarcy: 0.916\n",
      "Epoch 2 step 615: training loss: 1521.069822201428\n",
      "Epoch 2 step 616: training accuarcy: 0.9095\n",
      "Epoch 2 step 616: training loss: 1513.5068027623174\n",
      "Epoch 2 step 617: training accuarcy: 0.9005\n",
      "Epoch 2 step 617: training loss: 1524.7943883140304\n",
      "Epoch 2 step 618: training accuarcy: 0.905\n",
      "Epoch 2 step 618: training loss: 1469.7991410242728\n",
      "Epoch 2 step 619: training accuarcy: 0.912\n",
      "Epoch 2 step 619: training loss: 1528.3083228949506\n",
      "Epoch 2 step 620: training accuarcy: 0.9065\n",
      "Epoch 2 step 620: training loss: 1632.6855200553243\n",
      "Epoch 2 step 621: training accuarcy: 0.897\n",
      "Epoch 2 step 621: training loss: 1552.8867996904753\n",
      "Epoch 2 step 622: training accuarcy: 0.896\n",
      "Epoch 2 step 622: training loss: 1572.4530950070425\n",
      "Epoch 2 step 623: training accuarcy: 0.8985\n",
      "Epoch 2 step 623: training loss: 1519.2812620839659\n",
      "Epoch 2 step 624: training accuarcy: 0.9025\n",
      "Epoch 2 step 624: training loss: 1405.95159188659\n",
      "Epoch 2 step 625: training accuarcy: 0.911\n",
      "Epoch 2 step 625: training loss: 1499.3805789428384\n",
      "Epoch 2 step 626: training accuarcy: 0.9055\n",
      "Epoch 2 step 626: training loss: 1506.1514007447543\n",
      "Epoch 2 step 627: training accuarcy: 0.9085\n",
      "Epoch 2 step 627: training loss: 1431.7214621381288\n",
      "Epoch 2 step 628: training accuarcy: 0.907\n",
      "Epoch 2 step 628: training loss: 1523.0378985819448\n",
      "Epoch 2 step 629: training accuarcy: 0.9105\n",
      "Epoch 2 step 629: training loss: 1518.3723181962623\n",
      "Epoch 2 step 630: training accuarcy: 0.9035\n",
      "Epoch 2 step 630: training loss: 1711.0617200673325\n",
      "Epoch 2 step 631: training accuarcy: 0.887\n",
      "Epoch 2 step 631: training loss: 1433.1635682843923\n",
      "Epoch 2 step 632: training accuarcy: 0.912\n",
      "Epoch 2 step 632: training loss: 1500.2152891634278\n",
      "Epoch 2 step 633: training accuarcy: 0.903\n",
      "Epoch 2 step 633: training loss: 1616.047482508735\n",
      "Epoch 2 step 634: training accuarcy: 0.893\n",
      "Epoch 2 step 634: training loss: 1568.3468064991262\n",
      "Epoch 2 step 635: training accuarcy: 0.897\n",
      "Epoch 2 step 635: training loss: 1423.0147754070813\n",
      "Epoch 2 step 636: training accuarcy: 0.9095\n",
      "Epoch 2 step 636: training loss: 1462.6387396445034\n",
      "Epoch 2 step 637: training accuarcy: 0.9085\n",
      "Epoch 2 step 637: training loss: 1357.642726508017\n",
      "Epoch 2 step 638: training accuarcy: 0.905\n",
      "Epoch 2 step 638: training loss: 1533.0315894440246\n",
      "Epoch 2 step 639: training accuarcy: 0.909\n",
      "Epoch 2 step 639: training loss: 1544.2009411388638\n",
      "Epoch 2 step 640: training accuarcy: 0.9025\n",
      "Epoch 2 step 640: training loss: 1460.1474991220202\n",
      "Epoch 2 step 641: training accuarcy: 0.9105\n",
      "Epoch 2 step 641: training loss: 1393.3479657411879\n",
      "Epoch 2 step 642: training accuarcy: 0.912\n",
      "Epoch 2 step 642: training loss: 1590.072752661606\n",
      "Epoch 2 step 643: training accuarcy: 0.898\n",
      "Epoch 2 step 643: training loss: 1810.9564648087617\n",
      "Epoch 2 step 644: training accuarcy: 0.892\n",
      "Epoch 2 step 644: training loss: 1561.4294681129186\n",
      "Epoch 2 step 645: training accuarcy: 0.897\n",
      "Epoch 2 step 645: training loss: 1512.8423410425746\n",
      "Epoch 2 step 646: training accuarcy: 0.904\n",
      "Epoch 2 step 646: training loss: 1549.1932661586986\n",
      "Epoch 2 step 647: training accuarcy: 0.903\n",
      "Epoch 2 step 647: training loss: 1584.6086288050083\n",
      "Epoch 2 step 648: training accuarcy: 0.902\n",
      "Epoch 2 step 648: training loss: 1484.8825542167478\n",
      "Epoch 2 step 649: training accuarcy: 0.9025\n",
      "Epoch 2 step 649: training loss: 1505.1034475090532\n",
      "Epoch 2 step 650: training accuarcy: 0.904\n",
      "Epoch 2 step 650: training loss: 1435.5414668330009\n",
      "Epoch 2 step 651: training accuarcy: 0.8955000000000001\n",
      "Epoch 2 step 651: training loss: 1507.3683199990687\n",
      "Epoch 2 step 652: training accuarcy: 0.907\n",
      "Epoch 2 step 652: training loss: 1533.6427072415981\n",
      "Epoch 2 step 653: training accuarcy: 0.9015\n",
      "Epoch 2 step 653: training loss: 1573.3554682420277\n",
      "Epoch 2 step 654: training accuarcy: 0.901\n",
      "Epoch 2 step 654: training loss: 1478.436469279257\n",
      "Epoch 2 step 655: training accuarcy: 0.91\n",
      "Epoch 2 step 655: training loss: 1527.9846387000139\n",
      "Epoch 2 step 656: training accuarcy: 0.905\n",
      "Epoch 2 step 656: training loss: 1646.9765280611766\n",
      "Epoch 2 step 657: training accuarcy: 0.8985\n",
      "Epoch 2 step 657: training loss: 1674.518062706751\n",
      "Epoch 2 step 658: training accuarcy: 0.9005\n",
      "Epoch 2 step 658: training loss: 1476.9704844603177\n",
      "Epoch 2 step 659: training accuarcy: 0.908\n",
      "Epoch 2 step 659: training loss: 1441.6850627092847\n",
      "Epoch 2 step 660: training accuarcy: 0.9075\n",
      "Epoch 2 step 660: training loss: 1407.3759390053087\n",
      "Epoch 2 step 661: training accuarcy: 0.912\n",
      "Epoch 2 step 661: training loss: 1649.5736761820895\n",
      "Epoch 2 step 662: training accuarcy: 0.8975\n",
      "Epoch 2 step 662: training loss: 1509.281419232097\n",
      "Epoch 2 step 663: training accuarcy: 0.9085\n",
      "Epoch 2 step 663: training loss: 1507.9162138674124\n",
      "Epoch 2 step 664: training accuarcy: 0.9015\n",
      "Epoch 2 step 664: training loss: 1371.9492535953523\n",
      "Epoch 2 step 665: training accuarcy: 0.914\n",
      "Epoch 2 step 665: training loss: 1486.9625483015257\n",
      "Epoch 2 step 666: training accuarcy: 0.904\n",
      "Epoch 2 step 666: training loss: 1510.4717812287527\n",
      "Epoch 2 step 667: training accuarcy: 0.899\n",
      "Epoch 2 step 667: training loss: 1472.2693726986454\n",
      "Epoch 2 step 668: training accuarcy: 0.904\n",
      "Epoch 2 step 668: training loss: 1768.220797638995\n",
      "Epoch 2 step 669: training accuarcy: 0.894\n",
      "Epoch 2 step 669: training loss: 1447.5471121303708\n",
      "Epoch 2 step 670: training accuarcy: 0.907\n",
      "Epoch 2 step 670: training loss: 1499.5466799099104\n",
      "Epoch 2 step 671: training accuarcy: 0.9045\n",
      "Epoch 2 step 671: training loss: 1510.6893803835355\n",
      "Epoch 2 step 672: training accuarcy: 0.9075\n",
      "Epoch 2 step 672: training loss: 1593.2720177129422\n",
      "Epoch 2 step 673: training accuarcy: 0.9015\n",
      "Epoch 2 step 673: training loss: 1613.0826274790006\n",
      "Epoch 2 step 674: training accuarcy: 0.889\n",
      "Epoch 2 step 674: training loss: 1625.759231420435\n",
      "Epoch 2 step 675: training accuarcy: 0.9065\n",
      "Epoch 2 step 675: training loss: 1654.724984295623\n",
      "Epoch 2 step 676: training accuarcy: 0.8965\n",
      "Epoch 2 step 676: training loss: 1523.4530507415898\n",
      "Epoch 2 step 677: training accuarcy: 0.893\n",
      "Epoch 2 step 677: training loss: 1563.4564449950374\n",
      "Epoch 2 step 678: training accuarcy: 0.9005\n",
      "Epoch 2 step 678: training loss: 1383.8165811836566\n",
      "Epoch 2 step 679: training accuarcy: 0.909\n",
      "Epoch 2 step 679: training loss: 1460.9286973597123\n",
      "Epoch 2 step 680: training accuarcy: 0.899\n",
      "Epoch 2 step 680: training loss: 1422.0949103615505\n",
      "Epoch 2 step 681: training accuarcy: 0.914\n",
      "Epoch 2 step 681: training loss: 1390.344564815072\n",
      "Epoch 2 step 682: training accuarcy: 0.9115\n",
      "Epoch 2 step 682: training loss: 1540.6309541627904\n",
      "Epoch 2 step 683: training accuarcy: 0.8995\n",
      "Epoch 2 step 683: training loss: 1384.7178687261548\n",
      "Epoch 2 step 684: training accuarcy: 0.907\n",
      "Epoch 2 step 684: training loss: 1643.8327248150777\n",
      "Epoch 2 step 685: training accuarcy: 0.8965\n",
      "Epoch 2 step 685: training loss: 1456.3878878147689\n",
      "Epoch 2 step 686: training accuarcy: 0.902\n",
      "Epoch 2 step 686: training loss: 1511.3782166582241\n",
      "Epoch 2 step 687: training accuarcy: 0.9045\n",
      "Epoch 2 step 687: training loss: 1525.9815395158103\n",
      "Epoch 2 step 688: training accuarcy: 0.9025\n",
      "Epoch 2 step 688: training loss: 1542.8415453317511\n",
      "Epoch 2 step 689: training accuarcy: 0.897\n",
      "Epoch 2 step 689: training loss: 1597.8866010996826\n",
      "Epoch 2 step 690: training accuarcy: 0.902\n",
      "Epoch 2 step 690: training loss: 1621.149305371961\n",
      "Epoch 2 step 691: training accuarcy: 0.9015\n",
      "Epoch 2 step 691: training loss: 1395.1445715601294\n",
      "Epoch 2 step 692: training accuarcy: 0.908\n",
      "Epoch 2 step 692: training loss: 1430.014970235387\n",
      "Epoch 2 step 693: training accuarcy: 0.9065\n",
      "Epoch 2 step 693: training loss: 1440.7208642612522\n",
      "Epoch 2 step 694: training accuarcy: 0.9095\n",
      "Epoch 2 step 694: training loss: 1519.2692225763835\n",
      "Epoch 2 step 695: training accuarcy: 0.9025\n",
      "Epoch 2 step 695: training loss: 1580.3213318947155\n",
      "Epoch 2 step 696: training accuarcy: 0.902\n",
      "Epoch 2 step 696: training loss: 1604.0786010734782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 697: training accuarcy: 0.8945000000000001\n",
      "Epoch 2 step 697: training loss: 1518.3340768774415\n",
      "Epoch 2 step 698: training accuarcy: 0.899\n",
      "Epoch 2 step 698: training loss: 1492.2449764692315\n",
      "Epoch 2 step 699: training accuarcy: 0.905\n",
      "Epoch 2 step 699: training loss: 1460.1931585250043\n",
      "Epoch 2 step 700: training accuarcy: 0.9145\n",
      "Epoch 2 step 700: training loss: 1545.656473456194\n",
      "Epoch 2 step 701: training accuarcy: 0.9015\n",
      "Epoch 2 step 701: training loss: 1467.9086901247329\n",
      "Epoch 2 step 702: training accuarcy: 0.904\n",
      "Epoch 2 step 702: training loss: 1491.3159299742822\n",
      "Epoch 2 step 703: training accuarcy: 0.9095\n",
      "Epoch 2 step 703: training loss: 1484.5099879280974\n",
      "Epoch 2 step 704: training accuarcy: 0.9035\n",
      "Epoch 2 step 704: training loss: 1510.3849245850133\n",
      "Epoch 2 step 705: training accuarcy: 0.9055\n",
      "Epoch 2 step 705: training loss: 1445.695599552168\n",
      "Epoch 2 step 706: training accuarcy: 0.9035\n",
      "Epoch 2 step 706: training loss: 1264.273495613737\n",
      "Epoch 2 step 707: training accuarcy: 0.916\n",
      "Epoch 2 step 707: training loss: 1493.0311669561427\n",
      "Epoch 2 step 708: training accuarcy: 0.9055\n",
      "Epoch 2 step 708: training loss: 1515.4688719506494\n",
      "Epoch 2 step 709: training accuarcy: 0.8995\n",
      "Epoch 2 step 709: training loss: 1348.666908084755\n",
      "Epoch 2 step 710: training accuarcy: 0.9125\n",
      "Epoch 2 step 710: training loss: 1505.07311668572\n",
      "Epoch 2 step 711: training accuarcy: 0.909\n",
      "Epoch 2 step 711: training loss: 1518.73909345104\n",
      "Epoch 2 step 712: training accuarcy: 0.9035\n",
      "Epoch 2 step 712: training loss: 1487.4868613700564\n",
      "Epoch 2 step 713: training accuarcy: 0.8965\n",
      "Epoch 2 step 713: training loss: 1567.8874654793792\n",
      "Epoch 2 step 714: training accuarcy: 0.9015\n",
      "Epoch 2 step 714: training loss: 1366.2786367233284\n",
      "Epoch 2 step 715: training accuarcy: 0.919\n",
      "Epoch 2 step 715: training loss: 1541.7622962176192\n",
      "Epoch 2 step 716: training accuarcy: 0.8985\n",
      "Epoch 2 step 716: training loss: 1476.8388846039072\n",
      "Epoch 2 step 717: training accuarcy: 0.8945000000000001\n",
      "Epoch 2 step 717: training loss: 1486.8899772133377\n",
      "Epoch 2 step 718: training accuarcy: 0.8965\n",
      "Epoch 2 step 718: training loss: 1547.0315736297061\n",
      "Epoch 2 step 719: training accuarcy: 0.899\n",
      "Epoch 2 step 719: training loss: 1637.6021615253262\n",
      "Epoch 2 step 720: training accuarcy: 0.8865000000000001\n",
      "Epoch 2 step 720: training loss: 1509.3233570496814\n",
      "Epoch 2 step 721: training accuarcy: 0.8925000000000001\n",
      "Epoch 2 step 721: training loss: 1503.5438146330357\n",
      "Epoch 2 step 722: training accuarcy: 0.9075\n",
      "Epoch 2 step 722: training loss: 1447.9407689968878\n",
      "Epoch 2 step 723: training accuarcy: 0.916\n",
      "Epoch 2 step 723: training loss: 1439.583745605144\n",
      "Epoch 2 step 724: training accuarcy: 0.8975\n",
      "Epoch 2 step 724: training loss: 1481.8685095584447\n",
      "Epoch 2 step 725: training accuarcy: 0.9\n",
      "Epoch 2 step 725: training loss: 1327.2230218035984\n",
      "Epoch 2 step 726: training accuarcy: 0.918\n",
      "Epoch 2 step 726: training loss: 1525.337450669746\n",
      "Epoch 2 step 727: training accuarcy: 0.8995\n",
      "Epoch 2 step 727: training loss: 1535.6607312234541\n",
      "Epoch 2 step 728: training accuarcy: 0.902\n",
      "Epoch 2 step 728: training loss: 1589.6930234535462\n",
      "Epoch 2 step 729: training accuarcy: 0.902\n",
      "Epoch 2 step 729: training loss: 1533.9740876343985\n",
      "Epoch 2 step 730: training accuarcy: 0.91\n",
      "Epoch 2 step 730: training loss: 1485.8438956531968\n",
      "Epoch 2 step 731: training accuarcy: 0.905\n",
      "Epoch 2 step 731: training loss: 1508.085851223554\n",
      "Epoch 2 step 732: training accuarcy: 0.912\n",
      "Epoch 2 step 732: training loss: 1582.224385311164\n",
      "Epoch 2 step 733: training accuarcy: 0.8975\n",
      "Epoch 2 step 733: training loss: 1529.8039451028649\n",
      "Epoch 2 step 734: training accuarcy: 0.8965\n",
      "Epoch 2 step 734: training loss: 1467.2895366600787\n",
      "Epoch 2 step 735: training accuarcy: 0.909\n",
      "Epoch 2 step 735: training loss: 1503.085415453618\n",
      "Epoch 2 step 736: training accuarcy: 0.9\n",
      "Epoch 2 step 736: training loss: 1679.362154369106\n",
      "Epoch 2 step 737: training accuarcy: 0.8885000000000001\n",
      "Epoch 2 step 737: training loss: 1583.6227154958015\n",
      "Epoch 2 step 738: training accuarcy: 0.895\n",
      "Epoch 2 step 738: training loss: 1574.94958904854\n",
      "Epoch 2 step 739: training accuarcy: 0.894\n",
      "Epoch 2 step 739: training loss: 1550.4699649578263\n",
      "Epoch 2 step 740: training accuarcy: 0.902\n",
      "Epoch 2 step 740: training loss: 1550.693774029773\n",
      "Epoch 2 step 741: training accuarcy: 0.9045\n",
      "Epoch 2 step 741: training loss: 1535.9494481271252\n",
      "Epoch 2 step 742: training accuarcy: 0.896\n",
      "Epoch 2 step 742: training loss: 1557.410553112647\n",
      "Epoch 2 step 743: training accuarcy: 0.8965\n",
      "Epoch 2 step 743: training loss: 1376.4247713523205\n",
      "Epoch 2 step 744: training accuarcy: 0.9135\n",
      "Epoch 2 step 744: training loss: 1439.0442718664522\n",
      "Epoch 2 step 745: training accuarcy: 0.909\n",
      "Epoch 2 step 745: training loss: 1372.3757189306743\n",
      "Epoch 2 step 746: training accuarcy: 0.9155\n",
      "Epoch 2 step 746: training loss: 1532.3033924698425\n",
      "Epoch 2 step 747: training accuarcy: 0.896\n",
      "Epoch 2 step 747: training loss: 1428.0849440542283\n",
      "Epoch 2 step 748: training accuarcy: 0.91\n",
      "Epoch 2 step 748: training loss: 1535.952524862133\n",
      "Epoch 2 step 749: training accuarcy: 0.903\n",
      "Epoch 2 step 749: training loss: 1402.905781481229\n",
      "Epoch 2 step 750: training accuarcy: 0.9095\n",
      "Epoch 2 step 750: training loss: 1503.7964765126494\n",
      "Epoch 2 step 751: training accuarcy: 0.9075\n",
      "Epoch 2 step 751: training loss: 1609.5583842244866\n",
      "Epoch 2 step 752: training accuarcy: 0.896\n",
      "Epoch 2 step 752: training loss: 1673.6398152258644\n",
      "Epoch 2 step 753: training accuarcy: 0.895\n",
      "Epoch 2 step 753: training loss: 1411.9352360438604\n",
      "Epoch 2 step 754: training accuarcy: 0.9105\n",
      "Epoch 2 step 754: training loss: 1480.3586967495412\n",
      "Epoch 2 step 755: training accuarcy: 0.905\n",
      "Epoch 2 step 755: training loss: 1387.6631096311453\n",
      "Epoch 2 step 756: training accuarcy: 0.906\n",
      "Epoch 2 step 756: training loss: 1495.819512944045"
     ]
    }
   ],
   "source": [
    "prob_learner.fit(epoch=4, log_dir=get_log_dir('topcoder', 'prob'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:24:12.762780Z",
     "start_time": "2019-10-17T13:24:12.738782Z"
    }
   },
   "outputs": [],
   "source": [
    "del prob_model\n",
    "T.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}