{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:01:30.581717Z",
     "start_time": "2019-09-25T14:01:30.456659Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:57:10.470469Z",
     "start_time": "2019-10-09T06:57:10.462467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\python\\recommender\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:57:11.556879Z",
     "start_time": "2019-10-09T06:57:11.029794Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import get_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:58:27.393734Z",
     "start_time": "2019-10-09T06:58:27.390763Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import SeqTopcoder\n",
    "from models import FMLearner, TorchFM, TorchHrmFM, TorchPrmeFM, TorchTransFM\n",
    "from models.fm_learner import simple_loss, trans_loss, simple_weight_loss, trans_weight_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:57:14.183359Z",
     "start_time": "2019-10-09T06:57:13.974273Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = T.cuda.current_device()\n",
    "BATCH = 2000\n",
    "SHUFFLE = True\n",
    "WORKERS = 0\n",
    "NEG_SAMPLE = 5\n",
    "REGS_PATH = Path(\"./inputs/topcoder/regs.csv\")\n",
    "CHAG_PATH = Path(\"./inputs/topcoder/challenge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:57:16.703704Z",
     "start_time": "2019-10-09T06:57:14.693707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset in inputs\\topcoder\\regs.csv\n",
      "Original regs shape: (610025, 3)\n",
      "Original registants size: 60017\n",
      "Original challenges size: 39916\n",
      "Filter dataframe shape: (544568, 3)\n",
      "Index(['challengeId', 'period', 'date', 'prizes', 'technologies', 'platforms'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datasets.torch_topcoder.SeqTopcoder at 0x1b4ac6c5f28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = SeqTopcoder(regs_path=REGS_PATH, chag_path=CHAG_PATH)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:57:21.847270Z",
     "start_time": "2019-10-09T06:57:21.844271Z"
    }
   },
   "outputs": [],
   "source": [
    "db.config_db(batch_size=BATCH,\n",
    "             shuffle=SHUFFLE,\n",
    "             num_workers=WORKERS,\n",
    "             device=DEVICE,\n",
    "             neg_sample=NEG_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:57:22.200299Z",
     "start_time": "2019-10-09T06:57:22.197296Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:58:00.809969Z",
     "start_time": "2019-10-09T06:58:00.806969Z"
    }
   },
   "outputs": [],
   "source": [
    "# regst setting\n",
    "LINEAR_REG = 1\n",
    "EMB_REG = 1\n",
    "TRANS_REG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:58:01.074535Z",
     "start_time": "2019-10-09T06:58:01.069557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_loss at 0x000001B4AC69C730>, 1, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_loss_callback = partial(simple_loss, LINEAR_REG, EMB_REG)\n",
    "simple_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:58:02.442281Z",
     "start_time": "2019-10-09T06:58:02.438306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_loss at 0x000001B4AC6F3950>, 1, 1, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_loss_callback = partial(trans_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T06:59:54.890538Z",
     "start_time": "2019-10-09T06:59:54.886537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_weight_loss at 0x000001B4AC6F39D8>, 1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weight_loss_callback = partial(simple_weight_loss, LINEAR_REG, EMB_REG)\n",
    "simple_weight_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T07:00:20.683945Z",
     "start_time": "2019-10-09T07:00:20.678945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_weight_loss at 0x000001B4AC6F3A60>, 1, 1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_weight_loss_callback = partial(trans_weight_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_weight_loss_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "#### Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T07:00:26.576470Z",
     "start_time": "2019-10-09T07:00:26.573470Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T07:00:30.976819Z",
     "start_time": "2019-10-09T07:00:30.973820Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DECAY_FREQ = 1000\n",
    "DECAY_GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T07:00:31.443401Z",
     "start_time": "2019-10-09T07:00:31.206401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchFM()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model = TorchFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T07:00:32.001816Z",
     "start_time": "2019-10-09T07:00:31.997815Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(fm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T07:00:34.813477Z",
     "start_time": "2019-10-09T07:00:32.533759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1b4b2e552b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_learner = FMLearner(fm_model, adam_opt, schedular, db)\n",
    "fm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:25.856807Z",
     "start_time": "2019-10-07T13:48:25.853806Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T07:01:05.218870Z",
     "start_time": "2019-10-09T07:01:05.214836Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='seq',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='seq',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=trans_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T07:03:43.378952Z",
     "start_time": "2019-10-09T07:01:41.935887Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 94905.21758678491\n",
      "Epoch 0 step 1: training accuarcy: 0.3252\n",
      "Epoch 0 step 1: training loss: 86457.55434978122\n",
      "Epoch 0 step 2: training accuarcy: 0.3966\n",
      "Epoch 0 step 2: training loss: 78937.53425374633\n",
      "Epoch 0 step 3: training accuarcy: 0.45690000000000003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-afd34bde4953>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m fm_learner.fit(epoch=3,\n\u001b[1;32m----> 2\u001b[1;33m                log_dir=get_log_dir('simple_topcoder', 'fm'))\n\u001b[0m",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, epoch, log_dir)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mschedular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muser_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\datasets\\torch_topcoder.py\u001b[0m in \u001b[0;36m_seq_collate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m             chag_df[chag_df['period'] == per].sample(n=neg_sample,\n\u001b[0;32m    204\u001b[0m                                                      replace=True)\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mper\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mper_series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         ]\n\u001b[0;32m    207\u001b[0m         \u001b[0mneg_feat_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\datasets\\torch_topcoder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    203\u001b[0m             chag_df[chag_df['period'] == per].sample(n=neg_sample,\n\u001b[0;32m    204\u001b[0m                                                      replace=True)\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mper\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mper_series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         ]\n\u001b[0;32m    207\u001b[0m         \u001b[0mneg_feat_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other, axis)\u001b[0m\n\u001b[0;32m   1764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1766\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1767\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1768\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1625\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_datetimelike_v_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=3,\n",
    "               log_dir=get_log_dir('simple_topcoder', 'fm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:17:00.484179Z",
     "start_time": "2019-10-07T14:17:00.468206Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ed9dfae1365c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mfm_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fm_model' is not defined"
     ]
    }
   ],
   "source": [
    "del fm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HRM FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:14:25.264225Z",
     "start_time": "2019-10-08T01:14:25.037228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchHrmFM()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_model = TorchHrmFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "hrm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:14:25.753333Z",
     "start_time": "2019-10-08T01:14:25.748333Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(hrm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:14:32.783625Z",
     "start_time": "2019-10-08T01:14:29.645158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x29045e67198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_learner = FMLearner(hrm_model, adam_opt, schedular, db)\n",
    "hrm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:14:32.789624Z",
     "start_time": "2019-10-08T01:14:32.785628Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:38:50.922692Z",
     "start_time": "2019-10-08T01:14:33.939540Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 42431.971135041786\n",
      "Epoch 0 step 1: training accuarcy: 0.5425\n",
      "Epoch 0 step 1: training loss: 41611.87258174788\n",
      "Epoch 0 step 2: training accuarcy: 0.533\n",
      "Epoch 0 step 2: training loss: 40729.178744947305\n",
      "Epoch 0 step 3: training accuarcy: 0.534\n",
      "Epoch 0 step 3: training loss: 39077.242215819024\n",
      "Epoch 0 step 4: training accuarcy: 0.545\n",
      "Epoch 0 step 4: training loss: 37973.78130915589\n",
      "Epoch 0 step 5: training accuarcy: 0.55\n",
      "Epoch 0 step 5: training loss: 37284.86898758583\n",
      "Epoch 0 step 6: training accuarcy: 0.521\n",
      "Epoch 0 step 6: training loss: 36142.27887250477\n",
      "Epoch 0 step 7: training accuarcy: 0.5425\n",
      "Epoch 0 step 7: training loss: 35028.941934657676\n",
      "Epoch 0 step 8: training accuarcy: 0.5345\n",
      "Epoch 0 step 8: training loss: 33826.59320649134\n",
      "Epoch 0 step 9: training accuarcy: 0.5445\n",
      "Epoch 0 step 9: training loss: 33069.860875282175\n",
      "Epoch 0 step 10: training accuarcy: 0.5245\n",
      "Epoch 0 step 10: training loss: 31872.833027400273\n",
      "Epoch 0 step 11: training accuarcy: 0.542\n",
      "Epoch 0 step 11: training loss: 31032.925116178496\n",
      "Epoch 0 step 12: training accuarcy: 0.535\n",
      "Epoch 0 step 12: training loss: 30030.86200465079\n",
      "Epoch 0 step 13: training accuarcy: 0.548\n",
      "Epoch 0 step 13: training loss: 29402.785086181233\n",
      "Epoch 0 step 14: training accuarcy: 0.5245\n",
      "Epoch 0 step 14: training loss: 28304.040581071888\n",
      "Epoch 0 step 15: training accuarcy: 0.5535\n",
      "Epoch 0 step 15: training loss: 27295.34361441395\n",
      "Epoch 0 step 16: training accuarcy: 0.527\n",
      "Epoch 0 step 16: training loss: 26694.01957768256\n",
      "Epoch 0 step 17: training accuarcy: 0.525\n",
      "Epoch 0 step 17: training loss: 25641.975768548877\n",
      "Epoch 0 step 18: training accuarcy: 0.542\n",
      "Epoch 0 step 18: training loss: 25188.280477998516\n",
      "Epoch 0 step 19: training accuarcy: 0.55\n",
      "Epoch 0 step 19: training loss: 24616.87430627883\n",
      "Epoch 0 step 20: training accuarcy: 0.526\n",
      "Epoch 0 step 20: training loss: 23630.131966635585\n",
      "Epoch 0 step 21: training accuarcy: 0.53\n",
      "Epoch 0 step 21: training loss: 22925.878803435287\n",
      "Epoch 0 step 22: training accuarcy: 0.54\n",
      "Epoch 0 step 22: training loss: 21864.155335399024\n",
      "Epoch 0 step 23: training accuarcy: 0.5595\n",
      "Epoch 0 step 23: training loss: 21781.961126202772\n",
      "Epoch 0 step 24: training accuarcy: 0.531\n",
      "Epoch 0 step 24: training loss: 20777.89086544091\n",
      "Epoch 0 step 25: training accuarcy: 0.5355\n",
      "Epoch 0 step 25: training loss: 20205.670465463827\n",
      "Epoch 0 step 26: training accuarcy: 0.5285\n",
      "Epoch 0 step 26: training loss: 19386.063359962813\n",
      "Epoch 0 step 27: training accuarcy: 0.5605\n",
      "Epoch 0 step 27: training loss: 18870.139282983444\n",
      "Epoch 0 step 28: training accuarcy: 0.5465\n",
      "Epoch 0 step 28: training loss: 18442.730702185203\n",
      "Epoch 0 step 29: training accuarcy: 0.5145\n",
      "Epoch 0 step 29: training loss: 17729.758581774637\n",
      "Epoch 0 step 30: training accuarcy: 0.538\n",
      "Epoch 0 step 30: training loss: 17273.261994106324\n",
      "Epoch 0 step 31: training accuarcy: 0.5335\n",
      "Epoch 0 step 31: training loss: 16648.735403645904\n",
      "Epoch 0 step 32: training accuarcy: 0.533\n",
      "Epoch 0 step 32: training loss: 15990.392993658468\n",
      "Epoch 0 step 33: training accuarcy: 0.548\n",
      "Epoch 0 step 33: training loss: 15597.486874326287\n",
      "Epoch 0 step 34: training accuarcy: 0.5525\n",
      "Epoch 0 step 34: training loss: 15003.40112534933\n",
      "Epoch 0 step 35: training accuarcy: 0.544\n",
      "Epoch 0 step 35: training loss: 14432.407680126857\n",
      "Epoch 0 step 36: training accuarcy: 0.535\n",
      "Epoch 0 step 36: training loss: 14151.715728580466\n",
      "Epoch 0 step 37: training accuarcy: 0.5405\n",
      "Epoch 0 step 37: training loss: 13826.860517886716\n",
      "Epoch 0 step 38: training accuarcy: 0.515\n",
      "Epoch 0 step 38: training loss: 13286.317317168814\n",
      "Epoch 0 step 39: training accuarcy: 0.5415\n",
      "Epoch 0 step 39: training loss: 12744.25394107027\n",
      "Epoch 0 step 40: training accuarcy: 0.5545\n",
      "Epoch 0 step 40: training loss: 12514.226156161241\n",
      "Epoch 0 step 41: training accuarcy: 0.533\n",
      "Epoch 0 step 41: training loss: 11948.885558576976\n",
      "Epoch 0 step 42: training accuarcy: 0.542\n",
      "Epoch 0 step 42: training loss: 11580.113016397407\n",
      "Epoch 0 step 43: training accuarcy: 0.5625\n",
      "Epoch 0 step 43: training loss: 11213.073026473832\n",
      "Epoch 0 step 44: training accuarcy: 0.5415\n",
      "Epoch 0 step 44: training loss: 10965.715015559746\n",
      "Epoch 0 step 45: training accuarcy: 0.5435\n",
      "Epoch 0 step 45: training loss: 10547.388902060311\n",
      "Epoch 0 step 46: training accuarcy: 0.5405\n",
      "Epoch 0 step 46: training loss: 10173.578312027877\n",
      "Epoch 0 step 47: training accuarcy: 0.542\n",
      "Epoch 0 step 47: training loss: 9925.271677320956\n",
      "Epoch 0 step 48: training accuarcy: 0.55\n",
      "Epoch 0 step 48: training loss: 9505.445841208812\n",
      "Epoch 0 step 49: training accuarcy: 0.5475\n",
      "Epoch 0 step 49: training loss: 9273.115931310183\n",
      "Epoch 0 step 50: training accuarcy: 0.5725\n",
      "Epoch 0 step 50: training loss: 9049.956410892042\n",
      "Epoch 0 step 51: training accuarcy: 0.56\n",
      "Epoch 0 step 51: training loss: 8759.63268020007\n",
      "Epoch 0 step 52: training accuarcy: 0.5455\n",
      "Epoch 0 step 52: training loss: 8399.05733741754\n",
      "Epoch 0 step 53: training accuarcy: 0.5345\n",
      "Epoch 0 step 53: training loss: 8197.720479681095\n",
      "Epoch 0 step 54: training accuarcy: 0.546\n",
      "Epoch 0 step 54: training loss: 7883.970444781546\n",
      "Epoch 0 step 55: training accuarcy: 0.5595\n",
      "Epoch 0 step 55: training loss: 7738.04095522153\n",
      "Epoch 0 step 56: training accuarcy: 0.547\n",
      "Epoch 0 step 56: training loss: 7531.446666574475\n",
      "Epoch 0 step 57: training accuarcy: 0.537\n",
      "Epoch 0 step 57: training loss: 7305.656290932265\n",
      "Epoch 0 step 58: training accuarcy: 0.546\n",
      "Epoch 0 step 58: training loss: 7034.236863886239\n",
      "Epoch 0 step 59: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 59: training loss: 6905.99794910454\n",
      "Epoch 0 step 60: training accuarcy: 0.535\n",
      "Epoch 0 step 60: training loss: 6635.387716852902\n",
      "Epoch 0 step 61: training accuarcy: 0.577\n",
      "Epoch 0 step 61: training loss: 6432.551771825514\n",
      "Epoch 0 step 62: training accuarcy: 0.555\n",
      "Epoch 0 step 62: training loss: 6272.620465590471\n",
      "Epoch 0 step 63: training accuarcy: 0.5575\n",
      "Epoch 0 step 63: training loss: 6091.544931651397\n",
      "Epoch 0 step 64: training accuarcy: 0.5585\n",
      "Epoch 0 step 64: training loss: 5838.610282531145\n",
      "Epoch 0 step 65: training accuarcy: 0.558\n",
      "Epoch 0 step 65: training loss: 5716.659496690093\n",
      "Epoch 0 step 66: training accuarcy: 0.5525\n",
      "Epoch 0 step 66: training loss: 5586.777231787621\n",
      "Epoch 0 step 67: training accuarcy: 0.5365\n",
      "Epoch 0 step 67: training loss: 5321.534450986821\n",
      "Epoch 0 step 68: training accuarcy: 0.5740000000000001\n",
      "Epoch 0 step 68: training loss: 5173.646768037146\n",
      "Epoch 0 step 69: training accuarcy: 0.577\n",
      "Epoch 0 step 69: training loss: 5072.453637680395\n",
      "Epoch 0 step 70: training accuarcy: 0.5615\n",
      "Epoch 0 step 70: training loss: 4940.594533714138\n",
      "Epoch 0 step 71: training accuarcy: 0.548\n",
      "Epoch 0 step 71: training loss: 4849.597882521482\n",
      "Epoch 0 step 72: training accuarcy: 0.5565\n",
      "Epoch 0 step 72: training loss: 4704.946477825659\n",
      "Epoch 0 step 73: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 73: training loss: 4558.468395762754\n",
      "Epoch 0 step 74: training accuarcy: 0.553\n",
      "Epoch 0 step 74: training loss: 4422.265929321195\n",
      "Epoch 0 step 75: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 75: training loss: 4333.11292437291\n",
      "Epoch 0 step 76: training accuarcy: 0.5740000000000001\n",
      "Epoch 0 step 76: training loss: 4204.720923202045\n",
      "Epoch 0 step 77: training accuarcy: 0.5855\n",
      "Epoch 0 step 77: training loss: 4119.112976496217\n",
      "Epoch 0 step 78: training accuarcy: 0.5755\n",
      "Epoch 0 step 78: training loss: 4031.1587170334224\n",
      "Epoch 0 step 79: training accuarcy: 0.5720000000000001\n",
      "Epoch 0 step 79: training loss: 3921.1164788140154\n",
      "Epoch 0 step 80: training accuarcy: 0.561\n",
      "Epoch 0 step 80: training loss: 3804.349064007438\n",
      "Epoch 0 step 81: training accuarcy: 0.5655\n",
      "Epoch 0 step 81: training loss: 3706.2935259734427\n",
      "Epoch 0 step 82: training accuarcy: 0.5705\n",
      "Epoch 0 step 82: training loss: 3644.1174791055023\n",
      "Epoch 0 step 83: training accuarcy: 0.5710000000000001\n",
      "Epoch 0 step 83: training loss: 3552.025247705368\n",
      "Epoch 0 step 84: training accuarcy: 0.5660000000000001\n",
      "Epoch 0 step 84: training loss: 3464.885682879957\n",
      "Epoch 0 step 85: training accuarcy: 0.56\n",
      "Epoch 0 step 85: training loss: 3356.8257379125635\n",
      "Epoch 0 step 86: training accuarcy: 0.5835\n",
      "Epoch 0 step 86: training loss: 3272.6100716377864\n",
      "Epoch 0 step 87: training accuarcy: 0.5925\n",
      "Epoch 0 step 87: training loss: 3219.8627636738292\n",
      "Epoch 0 step 88: training accuarcy: 0.5775\n",
      "Epoch 0 step 88: training loss: 3156.2428024411747\n",
      "Epoch 0 step 89: training accuarcy: 0.5710000000000001\n",
      "Epoch 0 step 89: training loss: 3103.7480828606376\n",
      "Epoch 0 step 90: training accuarcy: 0.5815\n",
      "Epoch 0 step 90: training loss: 3013.3641462531123\n",
      "Epoch 0 step 91: training accuarcy: 0.5925\n",
      "Epoch 0 step 91: training loss: 2961.8766590776395\n",
      "Epoch 0 step 92: training accuarcy: 0.5945\n",
      "Epoch 0 step 92: training loss: 2909.9618242401293\n",
      "Epoch 0 step 93: training accuarcy: 0.5815\n",
      "Epoch 0 step 93: training loss: 2847.25039335004\n",
      "Epoch 0 step 94: training accuarcy: 0.581\n",
      "Epoch 0 step 94: training loss: 2789.589220814636\n",
      "Epoch 0 step 95: training accuarcy: 0.588\n",
      "Epoch 0 step 95: training loss: 2729.9601197767406\n",
      "Epoch 0 step 96: training accuarcy: 0.5945\n",
      "Epoch 0 step 96: training loss: 2695.142586885094\n",
      "Epoch 0 step 97: training accuarcy: 0.5725\n",
      "Epoch 0 step 97: training loss: 2641.602723586985\n",
      "Epoch 0 step 98: training accuarcy: 0.5825\n",
      "Epoch 0 step 98: training loss: 2561.620505634327\n",
      "Epoch 0 step 99: training accuarcy: 0.611\n",
      "Epoch 0 step 99: training loss: 2523.758552779082\n",
      "Epoch 0 step 100: training accuarcy: 0.588\n",
      "Epoch 0 step 100: training loss: 2469.9430847786043\n",
      "Epoch 0 step 101: training accuarcy: 0.605\n",
      "Epoch 0 step 101: training loss: 2437.2359068017613\n",
      "Epoch 0 step 102: training accuarcy: 0.587\n",
      "Epoch 0 step 102: training loss: 2396.814248292919\n",
      "Epoch 0 step 103: training accuarcy: 0.578\n",
      "Epoch 0 step 103: training loss: 2363.8279393116036\n",
      "Epoch 0 step 104: training accuarcy: 0.5915\n",
      "Epoch 0 step 104: training loss: 2328.3997630365775\n",
      "Epoch 0 step 105: training accuarcy: 0.5935\n",
      "Epoch 0 step 105: training loss: 2289.3329164335446\n",
      "Epoch 0 step 106: training accuarcy: 0.6065\n",
      "Epoch 0 step 106: training loss: 2249.6187914143306\n",
      "Epoch 0 step 107: training accuarcy: 0.5985\n",
      "Epoch 0 step 107: training loss: 2220.3682616669403\n",
      "Epoch 0 step 108: training accuarcy: 0.589\n",
      "Epoch 0 step 108: training loss: 2187.565508639832\n",
      "Epoch 0 step 109: training accuarcy: 0.596\n",
      "Epoch 0 step 109: training loss: 2159.057918662552\n",
      "Epoch 0 step 110: training accuarcy: 0.5925\n",
      "Epoch 0 step 110: training loss: 2109.7967063957694\n",
      "Epoch 0 step 111: training accuarcy: 0.601\n",
      "Epoch 0 step 111: training loss: 2084.4924541481614\n",
      "Epoch 0 step 112: training accuarcy: 0.588\n",
      "Epoch 0 step 112: training loss: 2067.8418925233195\n",
      "Epoch 0 step 113: training accuarcy: 0.594\n",
      "Epoch 0 step 113: training loss: 2057.245884771506\n",
      "Epoch 0 step 114: training accuarcy: 0.582\n",
      "Epoch 0 step 114: training loss: 2014.3862814307138\n",
      "Epoch 0 step 115: training accuarcy: 0.599\n",
      "Epoch 0 step 115: training loss: 1975.297486180069\n",
      "Epoch 0 step 116: training accuarcy: 0.6115\n",
      "Epoch 0 step 116: training loss: 1959.476925897001\n",
      "Epoch 0 step 117: training accuarcy: 0.596\n",
      "Epoch 0 step 117: training loss: 1949.904663184364\n",
      "Epoch 0 step 118: training accuarcy: 0.597\n",
      "Epoch 0 step 118: training loss: 1920.3368138443693\n",
      "Epoch 0 step 119: training accuarcy: 0.593\n",
      "Epoch 0 step 119: training loss: 1888.0683438162955\n",
      "Epoch 0 step 120: training accuarcy: 0.6015\n",
      "Epoch 0 step 120: training loss: 1871.836883217308\n",
      "Epoch 0 step 121: training accuarcy: 0.5865\n",
      "Epoch 0 step 121: training loss: 1853.1225067236953\n",
      "Epoch 0 step 122: training accuarcy: 0.599\n",
      "Epoch 0 step 122: training loss: 1822.5851264669557\n",
      "Epoch 0 step 123: training accuarcy: 0.624\n",
      "Epoch 0 step 123: training loss: 1815.126168558905\n",
      "Epoch 0 step 124: training accuarcy: 0.61\n",
      "Epoch 0 step 124: training loss: 1800.7023940909164\n",
      "Epoch 0 step 125: training accuarcy: 0.5985\n",
      "Epoch 0 step 125: training loss: 1779.5828271112687\n",
      "Epoch 0 step 126: training accuarcy: 0.6035\n",
      "Epoch 0 step 126: training loss: 1758.8135810624344\n",
      "Epoch 0 step 127: training accuarcy: 0.6055\n",
      "Epoch 0 step 127: training loss: 1756.9527000891692\n",
      "Epoch 0 step 128: training accuarcy: 0.592\n",
      "Epoch 0 step 128: training loss: 1742.1108974840731\n",
      "Epoch 0 step 129: training accuarcy: 0.6065\n",
      "Epoch 0 step 129: training loss: 1715.8700853244377\n",
      "Epoch 0 step 130: training accuarcy: 0.608\n",
      "Epoch 0 step 130: training loss: 1703.9021154363359\n",
      "Epoch 0 step 131: training accuarcy: 0.614\n",
      "Epoch 0 step 131: training loss: 1700.1416381717108\n",
      "Epoch 0 step 132: training accuarcy: 0.6045\n",
      "Epoch 0 step 132: training loss: 1672.186979299099\n",
      "Epoch 0 step 133: training accuarcy: 0.6035\n",
      "Epoch 0 step 133: training loss: 1682.6327158090157\n",
      "Epoch 0 step 134: training accuarcy: 0.605\n",
      "Epoch 0 step 134: training loss: 1664.6296503917488\n",
      "Epoch 0 step 135: training accuarcy: 0.6035\n",
      "Epoch 0 step 135: training loss: 1647.3118591094535\n",
      "Epoch 0 step 136: training accuarcy: 0.5845\n",
      "Epoch 0 step 136: training loss: 1637.3864508894294\n",
      "Epoch 0 step 137: training accuarcy: 0.6075\n",
      "Epoch 0 step 137: training loss: 1623.2637757558268\n",
      "Epoch 0 step 138: training accuarcy: 0.604\n",
      "Epoch 0 step 138: training loss: 1616.9771603148254\n",
      "Epoch 0 step 139: training accuarcy: 0.599\n",
      "Epoch 0 step 139: training loss: 1607.2010801653107\n",
      "Epoch 0 step 140: training accuarcy: 0.586\n",
      "Epoch 0 step 140: training loss: 1591.8668227367361\n",
      "Epoch 0 step 141: training accuarcy: 0.6125\n",
      "Epoch 0 step 141: training loss: 1588.9307679695978\n",
      "Epoch 0 step 142: training accuarcy: 0.609\n",
      "Epoch 0 step 142: training loss: 1578.2453828961397\n",
      "Epoch 0 step 143: training accuarcy: 0.606\n",
      "Epoch 0 step 143: training loss: 1567.1349819391244\n",
      "Epoch 0 step 144: training accuarcy: 0.607\n",
      "Epoch 0 step 144: training loss: 1560.077716509543\n",
      "Epoch 0 step 145: training accuarcy: 0.604\n",
      "Epoch 0 step 145: training loss: 1546.110074950389\n",
      "Epoch 0 step 146: training accuarcy: 0.633\n",
      "Epoch 0 step 146: training loss: 1536.2703092329423\n",
      "Epoch 0 step 147: training accuarcy: 0.617\n",
      "Epoch 0 step 147: training loss: 1533.030759292925\n",
      "Epoch 0 step 148: training accuarcy: 0.627\n",
      "Epoch 0 step 148: training loss: 1538.6635848672381\n",
      "Epoch 0 step 149: training accuarcy: 0.589\n",
      "Epoch 0 step 149: training loss: 1538.7728640952416\n",
      "Epoch 0 step 150: training accuarcy: 0.582\n",
      "Epoch 0 step 150: training loss: 1516.1052902830284\n",
      "Epoch 0 step 151: training accuarcy: 0.6105\n",
      "Epoch 0 step 151: training loss: 1507.82433484831\n",
      "Epoch 0 step 152: training accuarcy: 0.6215\n",
      "Epoch 0 step 152: training loss: 1511.246123636755\n",
      "Epoch 0 step 153: training accuarcy: 0.6\n",
      "Epoch 0 step 153: training loss: 1510.719485766878\n",
      "Epoch 0 step 154: training accuarcy: 0.5915\n",
      "Epoch 0 step 154: training loss: 1493.5088885630369\n",
      "Epoch 0 step 155: training accuarcy: 0.604\n",
      "Epoch 0 step 155: training loss: 1507.0154267589078\n",
      "Epoch 0 step 156: training accuarcy: 0.5825\n",
      "Epoch 0 step 156: training loss: 1499.3263091584101\n",
      "Epoch 0 step 157: training accuarcy: 0.5805\n",
      "Epoch 0 step 157: training loss: 1492.763482000154\n",
      "Epoch 0 step 158: training accuarcy: 0.605\n",
      "Epoch 0 step 158: training loss: 1498.9036083818178\n",
      "Epoch 0 step 159: training accuarcy: 0.6095\n",
      "Epoch 0 step 159: training loss: 1471.3997937658605\n",
      "Epoch 0 step 160: training accuarcy: 0.5955\n",
      "Epoch 0 step 160: training loss: 1481.2154736633202\n",
      "Epoch 0 step 161: training accuarcy: 0.595\n",
      "Epoch 0 step 161: training loss: 1466.0391580033133\n",
      "Epoch 0 step 162: training accuarcy: 0.6125\n",
      "Epoch 0 step 162: training loss: 1486.4280487297754\n",
      "Epoch 0 step 163: training accuarcy: 0.5955\n",
      "Epoch 0 step 163: training loss: 1461.4884641412646\n",
      "Epoch 0 step 164: training accuarcy: 0.6115\n",
      "Epoch 0 step 164: training loss: 1454.0582659579027\n",
      "Epoch 0 step 165: training accuarcy: 0.596\n",
      "Epoch 0 step 165: training loss: 1439.7024746372072\n",
      "Epoch 0 step 166: training accuarcy: 0.6065\n",
      "Epoch 0 step 166: training loss: 1441.492216715743\n",
      "Epoch 0 step 167: training accuarcy: 0.626\n",
      "Epoch 0 step 167: training loss: 1433.6078378975321\n",
      "Epoch 0 step 168: training accuarcy: 0.6135\n",
      "Epoch 0 step 168: training loss: 1436.4862728146368\n",
      "Epoch 0 step 169: training accuarcy: 0.6205\n",
      "Epoch 0 step 169: training loss: 1427.158547538956\n",
      "Epoch 0 step 170: training accuarcy: 0.6095\n",
      "Epoch 0 step 170: training loss: 1419.2675198093712\n",
      "Epoch 0 step 171: training accuarcy: 0.622\n",
      "Epoch 0 step 171: training loss: 1440.9262792063507\n",
      "Epoch 0 step 172: training accuarcy: 0.599\n",
      "Epoch 0 step 172: training loss: 1438.6797768219333\n",
      "Epoch 0 step 173: training accuarcy: 0.5875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 173: training loss: 1413.1483539222527\n",
      "Epoch 0 step 174: training accuarcy: 0.6135\n",
      "Epoch 0 step 174: training loss: 1413.161282819326\n",
      "Epoch 0 step 175: training accuarcy: 0.637\n",
      "Epoch 0 step 175: training loss: 1419.1655759174848\n",
      "Epoch 0 step 176: training accuarcy: 0.606\n",
      "Epoch 0 step 176: training loss: 1423.0336349849777\n",
      "Epoch 0 step 177: training accuarcy: 0.6035\n",
      "Epoch 0 step 177: training loss: 1430.4855983113669\n",
      "Epoch 0 step 178: training accuarcy: 0.6245\n",
      "Epoch 0 step 178: training loss: 1404.5872416932928\n",
      "Epoch 0 step 179: training accuarcy: 0.609\n",
      "Epoch 0 step 179: training loss: 1403.2749256895318\n",
      "Epoch 0 step 180: training accuarcy: 0.612\n",
      "Epoch 0 step 180: training loss: 1409.388347349875\n",
      "Epoch 0 step 181: training accuarcy: 0.605\n",
      "Epoch 0 step 181: training loss: 1398.170818423775\n",
      "Epoch 0 step 182: training accuarcy: 0.6045\n",
      "Epoch 0 step 182: training loss: 1399.647368426476\n",
      "Epoch 0 step 183: training accuarcy: 0.6105\n",
      "Epoch 0 step 183: training loss: 1397.15300451512\n",
      "Epoch 0 step 184: training accuarcy: 0.604\n",
      "Epoch 0 step 184: training loss: 1398.535817514797\n",
      "Epoch 0 step 185: training accuarcy: 0.61\n",
      "Epoch 0 step 185: training loss: 1404.4189444591182\n",
      "Epoch 0 step 186: training accuarcy: 0.6125\n",
      "Epoch 0 step 186: training loss: 1401.8185434368222\n",
      "Epoch 0 step 187: training accuarcy: 0.5995\n",
      "Epoch 0 step 187: training loss: 1387.578953769653\n",
      "Epoch 0 step 188: training accuarcy: 0.6135\n",
      "Epoch 0 step 188: training loss: 1413.2053732125783\n",
      "Epoch 0 step 189: training accuarcy: 0.605\n",
      "Epoch 0 step 189: training loss: 1390.8707753952376\n",
      "Epoch 0 step 190: training accuarcy: 0.6065\n",
      "Epoch 0 step 190: training loss: 1390.432900904097\n",
      "Epoch 0 step 191: training accuarcy: 0.6105\n",
      "Epoch 0 step 191: training loss: 1392.5481520436883\n",
      "Epoch 0 step 192: training accuarcy: 0.596\n",
      "Epoch 0 step 192: training loss: 1378.8554662465215\n",
      "Epoch 0 step 193: training accuarcy: 0.6205\n",
      "Epoch 0 step 193: training loss: 1390.3842016629558\n",
      "Epoch 0 step 194: training accuarcy: 0.605\n",
      "Epoch 0 step 194: training loss: 1400.2752750692086\n",
      "Epoch 0 step 195: training accuarcy: 0.6\n",
      "Epoch 0 step 195: training loss: 1373.8029958971272\n",
      "Epoch 0 step 196: training accuarcy: 0.609\n",
      "Epoch 0 step 196: training loss: 1388.0177531846052\n",
      "Epoch 0 step 197: training accuarcy: 0.602\n",
      "Epoch 0 step 197: training loss: 1390.5106784579737\n",
      "Epoch 0 step 198: training accuarcy: 0.598\n",
      "Epoch 0 step 198: training loss: 1374.7184654486832\n",
      "Epoch 0 step 199: training accuarcy: 0.6225\n",
      "Epoch 0 step 199: training loss: 1399.072381054634\n",
      "Epoch 0 step 200: training accuarcy: 0.599\n",
      "Epoch 0 step 200: training loss: 1392.7820001155021\n",
      "Epoch 0 step 201: training accuarcy: 0.6015\n",
      "Epoch 0 step 201: training loss: 1377.9697850938765\n",
      "Epoch 0 step 202: training accuarcy: 0.6005\n",
      "Epoch 0 step 202: training loss: 1380.5401430493173\n",
      "Epoch 0 step 203: training accuarcy: 0.61\n",
      "Epoch 0 step 203: training loss: 1363.02161449695\n",
      "Epoch 0 step 204: training accuarcy: 0.612\n",
      "Epoch 0 step 204: training loss: 1374.6640278293337\n",
      "Epoch 0 step 205: training accuarcy: 0.6205\n",
      "Epoch 0 step 205: training loss: 1368.065283023168\n",
      "Epoch 0 step 206: training accuarcy: 0.609\n",
      "Epoch 0 step 206: training loss: 1373.1397498777153\n",
      "Epoch 0 step 207: training accuarcy: 0.6115\n",
      "Epoch 0 step 207: training loss: 1401.5125171055392\n",
      "Epoch 0 step 208: training accuarcy: 0.606\n",
      "Epoch 0 step 208: training loss: 1365.4929132424336\n",
      "Epoch 0 step 209: training accuarcy: 0.621\n",
      "Epoch 0 step 209: training loss: 1386.5972605485656\n",
      "Epoch 0 step 210: training accuarcy: 0.6065\n",
      "Epoch 0 step 210: training loss: 1363.147404331423\n",
      "Epoch 0 step 211: training accuarcy: 0.629\n",
      "Epoch 0 step 211: training loss: 1379.2869805971015\n",
      "Epoch 0 step 212: training accuarcy: 0.608\n",
      "Epoch 0 step 212: training loss: 1365.5065386946249\n",
      "Epoch 0 step 213: training accuarcy: 0.6125\n",
      "Epoch 0 step 213: training loss: 1354.6711381113155\n",
      "Epoch 0 step 214: training accuarcy: 0.633\n",
      "Epoch 0 step 214: training loss: 1357.4632274963399\n",
      "Epoch 0 step 215: training accuarcy: 0.6275000000000001\n",
      "Epoch 0 step 215: training loss: 1363.1160028777736\n",
      "Epoch 0 step 216: training accuarcy: 0.614\n",
      "Epoch 0 step 216: training loss: 1365.1282862694718\n",
      "Epoch 0 step 217: training accuarcy: 0.5935\n",
      "Epoch 0 step 217: training loss: 1369.3166262359484\n",
      "Epoch 0 step 218: training accuarcy: 0.606\n",
      "Epoch 0 step 218: training loss: 1350.5080168051602\n",
      "Epoch 0 step 219: training accuarcy: 0.638\n",
      "Epoch 0 step 219: training loss: 1366.8571039485175\n",
      "Epoch 0 step 220: training accuarcy: 0.5935\n",
      "Epoch 0 step 220: training loss: 1358.014706467397\n",
      "Epoch 0 step 221: training accuarcy: 0.63\n",
      "Epoch 0 step 221: training loss: 1383.4181253828554\n",
      "Epoch 0 step 222: training accuarcy: 0.5745\n",
      "Epoch 0 step 222: training loss: 1362.8188068379172\n",
      "Epoch 0 step 223: training accuarcy: 0.5965\n",
      "Epoch 0 step 223: training loss: 1357.0981325868734\n",
      "Epoch 0 step 224: training accuarcy: 0.6245\n",
      "Epoch 0 step 224: training loss: 1360.7859874681812\n",
      "Epoch 0 step 225: training accuarcy: 0.611\n",
      "Epoch 0 step 225: training loss: 1352.9586515810968\n",
      "Epoch 0 step 226: training accuarcy: 0.6095\n",
      "Epoch 0 step 226: training loss: 1367.653202361014\n",
      "Epoch 0 step 227: training accuarcy: 0.6035\n",
      "Epoch 0 step 227: training loss: 1353.9412599261414\n",
      "Epoch 0 step 228: training accuarcy: 0.616\n",
      "Epoch 0 step 228: training loss: 1350.5139031340457\n",
      "Epoch 0 step 229: training accuarcy: 0.6195\n",
      "Epoch 0 step 229: training loss: 1347.8614163582085\n",
      "Epoch 0 step 230: training accuarcy: 0.614\n",
      "Epoch 0 step 230: training loss: 1359.5912143113417\n",
      "Epoch 0 step 231: training accuarcy: 0.6015\n",
      "Epoch 0 step 231: training loss: 1358.618725713701\n",
      "Epoch 0 step 232: training accuarcy: 0.619\n",
      "Epoch 0 step 232: training loss: 1347.9357261742773\n",
      "Epoch 0 step 233: training accuarcy: 0.623\n",
      "Epoch 0 step 233: training loss: 1344.4794214716333\n",
      "Epoch 0 step 234: training accuarcy: 0.6235\n",
      "Epoch 0 step 234: training loss: 1366.568623532887\n",
      "Epoch 0 step 235: training accuarcy: 0.592\n",
      "Epoch 0 step 235: training loss: 1357.9197730139076\n",
      "Epoch 0 step 236: training accuarcy: 0.6145\n",
      "Epoch 0 step 236: training loss: 1348.6780596005858\n",
      "Epoch 0 step 237: training accuarcy: 0.605\n",
      "Epoch 0 step 237: training loss: 1359.786974385985\n",
      "Epoch 0 step 238: training accuarcy: 0.619\n",
      "Epoch 0 step 238: training loss: 1348.1888862256424\n",
      "Epoch 0 step 239: training accuarcy: 0.6255000000000001\n",
      "Epoch 0 step 239: training loss: 1361.0493418066558\n",
      "Epoch 0 step 240: training accuarcy: 0.6075\n",
      "Epoch 0 step 240: training loss: 1347.5219124420232\n",
      "Epoch 0 step 241: training accuarcy: 0.6075\n",
      "Epoch 0 step 241: training loss: 1341.2684929437398\n",
      "Epoch 0 step 242: training accuarcy: 0.624\n",
      "Epoch 0 step 242: training loss: 1361.1538934212151\n",
      "Epoch 0 step 243: training accuarcy: 0.606\n",
      "Epoch 0 step 243: training loss: 1346.220234939734\n",
      "Epoch 0 step 244: training accuarcy: 0.63\n",
      "Epoch 0 step 244: training loss: 1369.683682024793\n",
      "Epoch 0 step 245: training accuarcy: 0.5955\n",
      "Epoch 0 step 245: training loss: 1360.7522868716258\n",
      "Epoch 0 step 246: training accuarcy: 0.603\n",
      "Epoch 0 step 246: training loss: 1340.941934897669\n",
      "Epoch 0 step 247: training accuarcy: 0.628\n",
      "Epoch 0 step 247: training loss: 1356.4638204349826\n",
      "Epoch 0 step 248: training accuarcy: 0.603\n",
      "Epoch 0 step 248: training loss: 1349.0340421086607\n",
      "Epoch 0 step 249: training accuarcy: 0.616\n",
      "Epoch 0 step 249: training loss: 1352.4660561277524\n",
      "Epoch 0 step 250: training accuarcy: 0.6235\n",
      "Epoch 0 step 250: training loss: 1360.615399675597\n",
      "Epoch 0 step 251: training accuarcy: 0.602\n",
      "Epoch 0 step 251: training loss: 1358.1264148111675\n",
      "Epoch 0 step 252: training accuarcy: 0.6025\n",
      "Epoch 0 step 252: training loss: 1357.9963583931185\n",
      "Epoch 0 step 253: training accuarcy: 0.6115\n",
      "Epoch 0 step 253: training loss: 1346.7426339713631\n",
      "Epoch 0 step 254: training accuarcy: 0.621\n",
      "Epoch 0 step 254: training loss: 1355.3446257728122\n",
      "Epoch 0 step 255: training accuarcy: 0.61\n",
      "Epoch 0 step 255: training loss: 1347.4052643308441\n",
      "Epoch 0 step 256: training accuarcy: 0.622\n",
      "Epoch 0 step 256: training loss: 1360.5481918624955\n",
      "Epoch 0 step 257: training accuarcy: 0.602\n",
      "Epoch 0 step 257: training loss: 1350.3094899107057\n",
      "Epoch 0 step 258: training accuarcy: 0.61\n",
      "Epoch 0 step 258: training loss: 1369.557049584107\n",
      "Epoch 0 step 259: training accuarcy: 0.605\n",
      "Epoch 0 step 259: training loss: 1349.2309532156148\n",
      "Epoch 0 step 260: training accuarcy: 0.6055\n",
      "Epoch 0 step 260: training loss: 1353.8397702365512\n",
      "Epoch 0 step 261: training accuarcy: 0.6015\n",
      "Epoch 0 step 261: training loss: 1352.1404913661697\n",
      "Epoch 0 step 262: training accuarcy: 0.618\n",
      "Epoch 0 step 262: training loss: 533.1650287888334\n",
      "Epoch 0 step 263: training accuarcy: 0.6358974358974359\n",
      "Epoch 0: train loss 6129.846065910456, train accuarcy 0.5873441696166992\n",
      "Epoch 0: valid loss 6691.339799172716, valid accuarcy 0.5847988724708557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▍                                                                                                                                         | 1/5 [04:49<19:19, 289.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1370.000425387888\n",
      "Epoch 1 step 264: training accuarcy: 0.6005\n",
      "Epoch 1 step 264: training loss: 1337.3001136712044\n",
      "Epoch 1 step 265: training accuarcy: 0.6325000000000001\n",
      "Epoch 1 step 265: training loss: 1371.8238745968526\n",
      "Epoch 1 step 266: training accuarcy: 0.584\n",
      "Epoch 1 step 266: training loss: 1343.0026475911757\n",
      "Epoch 1 step 267: training accuarcy: 0.6145\n",
      "Epoch 1 step 267: training loss: 1348.0691573820966\n",
      "Epoch 1 step 268: training accuarcy: 0.615\n",
      "Epoch 1 step 268: training loss: 1365.8553596597278\n",
      "Epoch 1 step 269: training accuarcy: 0.591\n",
      "Epoch 1 step 269: training loss: 1352.8658934356934\n",
      "Epoch 1 step 270: training accuarcy: 0.6115\n",
      "Epoch 1 step 270: training loss: 1348.3941088485494\n",
      "Epoch 1 step 271: training accuarcy: 0.625\n",
      "Epoch 1 step 271: training loss: 1361.3921383169684\n",
      "Epoch 1 step 272: training accuarcy: 0.5975\n",
      "Epoch 1 step 272: training loss: 1345.3344596428212\n",
      "Epoch 1 step 273: training accuarcy: 0.619\n",
      "Epoch 1 step 273: training loss: 1337.6164086699955\n",
      "Epoch 1 step 274: training accuarcy: 0.62\n",
      "Epoch 1 step 274: training loss: 1329.9681565300914\n",
      "Epoch 1 step 275: training accuarcy: 0.6285000000000001\n",
      "Epoch 1 step 275: training loss: 1343.9039893535346\n",
      "Epoch 1 step 276: training accuarcy: 0.617\n",
      "Epoch 1 step 276: training loss: 1351.5336804235744\n",
      "Epoch 1 step 277: training accuarcy: 0.612\n",
      "Epoch 1 step 277: training loss: 1340.6728168858294\n",
      "Epoch 1 step 278: training accuarcy: 0.6185\n",
      "Epoch 1 step 278: training loss: 1354.107918979184\n",
      "Epoch 1 step 279: training accuarcy: 0.6085\n",
      "Epoch 1 step 279: training loss: 1356.1656937453395\n",
      "Epoch 1 step 280: training accuarcy: 0.612\n",
      "Epoch 1 step 280: training loss: 1352.2995468392267\n",
      "Epoch 1 step 281: training accuarcy: 0.604\n",
      "Epoch 1 step 281: training loss: 1349.6636117747269\n",
      "Epoch 1 step 282: training accuarcy: 0.605\n",
      "Epoch 1 step 282: training loss: 1342.0700351026485\n",
      "Epoch 1 step 283: training accuarcy: 0.621\n",
      "Epoch 1 step 283: training loss: 1356.8443212330806\n",
      "Epoch 1 step 284: training accuarcy: 0.6045\n",
      "Epoch 1 step 284: training loss: 1340.6303119914708\n",
      "Epoch 1 step 285: training accuarcy: 0.632\n",
      "Epoch 1 step 285: training loss: 1341.6572891877665\n",
      "Epoch 1 step 286: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 286: training loss: 1343.7027142264542\n",
      "Epoch 1 step 287: training accuarcy: 0.6205\n",
      "Epoch 1 step 287: training loss: 1336.9660971353646\n",
      "Epoch 1 step 288: training accuarcy: 0.629\n",
      "Epoch 1 step 288: training loss: 1349.334275505802\n",
      "Epoch 1 step 289: training accuarcy: 0.6105\n",
      "Epoch 1 step 289: training loss: 1333.9817653641194\n",
      "Epoch 1 step 290: training accuarcy: 0.6325000000000001\n",
      "Epoch 1 step 290: training loss: 1355.986040648633\n",
      "Epoch 1 step 291: training accuarcy: 0.6035\n",
      "Epoch 1 step 291: training loss: 1344.7086879984101\n",
      "Epoch 1 step 292: training accuarcy: 0.614\n",
      "Epoch 1 step 292: training loss: 1349.652535686875\n",
      "Epoch 1 step 293: training accuarcy: 0.6065\n",
      "Epoch 1 step 293: training loss: 1368.3839858333638\n",
      "Epoch 1 step 294: training accuarcy: 0.589\n",
      "Epoch 1 step 294: training loss: 1339.4527380908455\n",
      "Epoch 1 step 295: training accuarcy: 0.607\n",
      "Epoch 1 step 295: training loss: 1337.787405339218\n",
      "Epoch 1 step 296: training accuarcy: 0.6255000000000001\n",
      "Epoch 1 step 296: training loss: 1338.1427510717788\n",
      "Epoch 1 step 297: training accuarcy: 0.6185\n",
      "Epoch 1 step 297: training loss: 1360.881452802789\n",
      "Epoch 1 step 298: training accuarcy: 0.602\n",
      "Epoch 1 step 298: training loss: 1362.3352902173065\n",
      "Epoch 1 step 299: training accuarcy: 0.608\n",
      "Epoch 1 step 299: training loss: 1357.898344662868\n",
      "Epoch 1 step 300: training accuarcy: 0.609\n",
      "Epoch 1 step 300: training loss: 1349.6850355804659\n",
      "Epoch 1 step 301: training accuarcy: 0.6065\n",
      "Epoch 1 step 301: training loss: 1367.1836598811362\n",
      "Epoch 1 step 302: training accuarcy: 0.597\n",
      "Epoch 1 step 302: training loss: 1357.6691408034612\n",
      "Epoch 1 step 303: training accuarcy: 0.618\n",
      "Epoch 1 step 303: training loss: 1334.9868396674208\n",
      "Epoch 1 step 304: training accuarcy: 0.629\n",
      "Epoch 1 step 304: training loss: 1341.2651814656638\n",
      "Epoch 1 step 305: training accuarcy: 0.62\n",
      "Epoch 1 step 305: training loss: 1342.6483390825827\n",
      "Epoch 1 step 306: training accuarcy: 0.615\n",
      "Epoch 1 step 306: training loss: 1339.918497416311\n",
      "Epoch 1 step 307: training accuarcy: 0.6105\n",
      "Epoch 1 step 307: training loss: 1326.1096111591653\n",
      "Epoch 1 step 308: training accuarcy: 0.621\n",
      "Epoch 1 step 308: training loss: 1349.0426057966404\n",
      "Epoch 1 step 309: training accuarcy: 0.6105\n",
      "Epoch 1 step 309: training loss: 1339.6148673447751\n",
      "Epoch 1 step 310: training accuarcy: 0.62\n",
      "Epoch 1 step 310: training loss: 1333.1629996615443\n",
      "Epoch 1 step 311: training accuarcy: 0.6135\n",
      "Epoch 1 step 311: training loss: 1349.6386588821088\n",
      "Epoch 1 step 312: training accuarcy: 0.6185\n",
      "Epoch 1 step 312: training loss: 1347.1517976264452\n",
      "Epoch 1 step 313: training accuarcy: 0.623\n",
      "Epoch 1 step 313: training loss: 1349.9392443471309\n",
      "Epoch 1 step 314: training accuarcy: 0.6125\n",
      "Epoch 1 step 314: training loss: 1324.252251200273\n",
      "Epoch 1 step 315: training accuarcy: 0.628\n",
      "Epoch 1 step 315: training loss: 1351.76145240796\n",
      "Epoch 1 step 316: training accuarcy: 0.612\n",
      "Epoch 1 step 316: training loss: 1348.387627717233\n",
      "Epoch 1 step 317: training accuarcy: 0.6105\n",
      "Epoch 1 step 317: training loss: 1337.04896404934\n",
      "Epoch 1 step 318: training accuarcy: 0.612\n",
      "Epoch 1 step 318: training loss: 1345.1853117040787\n",
      "Epoch 1 step 319: training accuarcy: 0.609\n",
      "Epoch 1 step 319: training loss: 1341.8191035083607\n",
      "Epoch 1 step 320: training accuarcy: 0.6125\n",
      "Epoch 1 step 320: training loss: 1335.3640290425988\n",
      "Epoch 1 step 321: training accuarcy: 0.611\n",
      "Epoch 1 step 321: training loss: 1345.8185219568315\n",
      "Epoch 1 step 322: training accuarcy: 0.6095\n",
      "Epoch 1 step 322: training loss: 1338.340762206349\n",
      "Epoch 1 step 323: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 323: training loss: 1369.1115127621683\n",
      "Epoch 1 step 324: training accuarcy: 0.607\n",
      "Epoch 1 step 324: training loss: 1348.5582824731912\n",
      "Epoch 1 step 325: training accuarcy: 0.602\n",
      "Epoch 1 step 325: training loss: 1350.256831541826\n",
      "Epoch 1 step 326: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 326: training loss: 1344.5025382718118\n",
      "Epoch 1 step 327: training accuarcy: 0.6165\n",
      "Epoch 1 step 327: training loss: 1347.614765416896\n",
      "Epoch 1 step 328: training accuarcy: 0.622\n",
      "Epoch 1 step 328: training loss: 1341.0676713869618\n",
      "Epoch 1 step 329: training accuarcy: 0.627\n",
      "Epoch 1 step 329: training loss: 1344.497041669331\n",
      "Epoch 1 step 330: training accuarcy: 0.6015\n",
      "Epoch 1 step 330: training loss: 1329.7157989393475\n",
      "Epoch 1 step 331: training accuarcy: 0.617\n",
      "Epoch 1 step 331: training loss: 1345.7608908091038\n",
      "Epoch 1 step 332: training accuarcy: 0.606\n",
      "Epoch 1 step 332: training loss: 1352.1579431131322\n",
      "Epoch 1 step 333: training accuarcy: 0.6075\n",
      "Epoch 1 step 333: training loss: 1341.6226839678118\n",
      "Epoch 1 step 334: training accuarcy: 0.61\n",
      "Epoch 1 step 334: training loss: 1341.0714679766743\n",
      "Epoch 1 step 335: training accuarcy: 0.612\n",
      "Epoch 1 step 335: training loss: 1333.7948744239427\n",
      "Epoch 1 step 336: training accuarcy: 0.634\n",
      "Epoch 1 step 336: training loss: 1348.637479649742\n",
      "Epoch 1 step 337: training accuarcy: 0.6135\n",
      "Epoch 1 step 337: training loss: 1325.0322871194896\n",
      "Epoch 1 step 338: training accuarcy: 0.6195\n",
      "Epoch 1 step 338: training loss: 1359.6549090522315\n",
      "Epoch 1 step 339: training accuarcy: 0.6\n",
      "Epoch 1 step 339: training loss: 1361.1354773980845\n",
      "Epoch 1 step 340: training accuarcy: 0.6135\n",
      "Epoch 1 step 340: training loss: 1341.5288677386416\n",
      "Epoch 1 step 341: training accuarcy: 0.6145\n",
      "Epoch 1 step 341: training loss: 1354.8983154230214\n",
      "Epoch 1 step 342: training accuarcy: 0.614\n",
      "Epoch 1 step 342: training loss: 1345.4013829554256\n",
      "Epoch 1 step 343: training accuarcy: 0.597\n",
      "Epoch 1 step 343: training loss: 1339.5337456505506\n",
      "Epoch 1 step 344: training accuarcy: 0.6125\n",
      "Epoch 1 step 344: training loss: 1340.48345343804\n",
      "Epoch 1 step 345: training accuarcy: 0.618\n",
      "Epoch 1 step 345: training loss: 1346.837460065091\n",
      "Epoch 1 step 346: training accuarcy: 0.6165\n",
      "Epoch 1 step 346: training loss: 1346.469035872581\n",
      "Epoch 1 step 347: training accuarcy: 0.613\n",
      "Epoch 1 step 347: training loss: 1334.9511627353015\n",
      "Epoch 1 step 348: training accuarcy: 0.6235\n",
      "Epoch 1 step 348: training loss: 1361.3272777229058\n",
      "Epoch 1 step 349: training accuarcy: 0.594\n",
      "Epoch 1 step 349: training loss: 1351.0807228005294\n",
      "Epoch 1 step 350: training accuarcy: 0.625\n",
      "Epoch 1 step 350: training loss: 1338.1086143866223\n",
      "Epoch 1 step 351: training accuarcy: 0.6075\n",
      "Epoch 1 step 351: training loss: 1353.086177926037\n",
      "Epoch 1 step 352: training accuarcy: 0.636\n",
      "Epoch 1 step 352: training loss: 1344.4558550669158\n",
      "Epoch 1 step 353: training accuarcy: 0.6175\n",
      "Epoch 1 step 353: training loss: 1332.9422288102232\n",
      "Epoch 1 step 354: training accuarcy: 0.628\n",
      "Epoch 1 step 354: training loss: 1336.9061911483964\n",
      "Epoch 1 step 355: training accuarcy: 0.612\n",
      "Epoch 1 step 355: training loss: 1336.650783290268\n",
      "Epoch 1 step 356: training accuarcy: 0.619\n",
      "Epoch 1 step 356: training loss: 1348.2867671609974\n",
      "Epoch 1 step 357: training accuarcy: 0.5985\n",
      "Epoch 1 step 357: training loss: 1351.3354786072637\n",
      "Epoch 1 step 358: training accuarcy: 0.599\n",
      "Epoch 1 step 358: training loss: 1329.2508464873501\n",
      "Epoch 1 step 359: training accuarcy: 0.6305000000000001\n",
      "Epoch 1 step 359: training loss: 1355.562576637802\n",
      "Epoch 1 step 360: training accuarcy: 0.607\n",
      "Epoch 1 step 360: training loss: 1343.529152997428\n",
      "Epoch 1 step 361: training accuarcy: 0.6075\n",
      "Epoch 1 step 361: training loss: 1334.3491695779828\n",
      "Epoch 1 step 362: training accuarcy: 0.6175\n",
      "Epoch 1 step 362: training loss: 1349.251077218467\n",
      "Epoch 1 step 363: training accuarcy: 0.615\n",
      "Epoch 1 step 363: training loss: 1338.1470827850962\n",
      "Epoch 1 step 364: training accuarcy: 0.612\n",
      "Epoch 1 step 364: training loss: 1349.616282502658\n",
      "Epoch 1 step 365: training accuarcy: 0.603\n",
      "Epoch 1 step 365: training loss: 1339.1964916326276\n",
      "Epoch 1 step 366: training accuarcy: 0.617\n",
      "Epoch 1 step 366: training loss: 1346.0754362718124\n",
      "Epoch 1 step 367: training accuarcy: 0.597\n",
      "Epoch 1 step 367: training loss: 1343.7505690482483\n",
      "Epoch 1 step 368: training accuarcy: 0.6215\n",
      "Epoch 1 step 368: training loss: 1338.164830336396\n",
      "Epoch 1 step 369: training accuarcy: 0.613\n",
      "Epoch 1 step 369: training loss: 1339.7490898379501\n",
      "Epoch 1 step 370: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 370: training loss: 1360.57525178007\n",
      "Epoch 1 step 371: training accuarcy: 0.585\n",
      "Epoch 1 step 371: training loss: 1347.088109273443\n",
      "Epoch 1 step 372: training accuarcy: 0.5965\n",
      "Epoch 1 step 372: training loss: 1332.1277845036543\n",
      "Epoch 1 step 373: training accuarcy: 0.625\n",
      "Epoch 1 step 373: training loss: 1336.5826627643437\n",
      "Epoch 1 step 374: training accuarcy: 0.61\n",
      "Epoch 1 step 374: training loss: 1346.475981096188\n",
      "Epoch 1 step 375: training accuarcy: 0.6135\n",
      "Epoch 1 step 375: training loss: 1339.3716466762307\n",
      "Epoch 1 step 376: training accuarcy: 0.6295000000000001\n",
      "Epoch 1 step 376: training loss: 1339.3381673339243\n",
      "Epoch 1 step 377: training accuarcy: 0.6135\n",
      "Epoch 1 step 377: training loss: 1340.7948760236322\n",
      "Epoch 1 step 378: training accuarcy: 0.598\n",
      "Epoch 1 step 378: training loss: 1339.3492163153492\n",
      "Epoch 1 step 379: training accuarcy: 0.6185\n",
      "Epoch 1 step 379: training loss: 1350.348635354978\n",
      "Epoch 1 step 380: training accuarcy: 0.607\n",
      "Epoch 1 step 380: training loss: 1355.2254326834097\n",
      "Epoch 1 step 381: training accuarcy: 0.6\n",
      "Epoch 1 step 381: training loss: 1346.910680351296\n",
      "Epoch 1 step 382: training accuarcy: 0.598\n",
      "Epoch 1 step 382: training loss: 1359.922655405819\n",
      "Epoch 1 step 383: training accuarcy: 0.609\n",
      "Epoch 1 step 383: training loss: 1350.5133703177312\n",
      "Epoch 1 step 384: training accuarcy: 0.6035\n",
      "Epoch 1 step 384: training loss: 1342.6738019771617\n",
      "Epoch 1 step 385: training accuarcy: 0.617\n",
      "Epoch 1 step 385: training loss: 1349.3664666084562\n",
      "Epoch 1 step 386: training accuarcy: 0.616\n",
      "Epoch 1 step 386: training loss: 1349.1820228385368\n",
      "Epoch 1 step 387: training accuarcy: 0.5985\n",
      "Epoch 1 step 387: training loss: 1326.7852038251783\n",
      "Epoch 1 step 388: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 388: training loss: 1350.5715431767724\n",
      "Epoch 1 step 389: training accuarcy: 0.6095\n",
      "Epoch 1 step 389: training loss: 1350.7937958135303\n",
      "Epoch 1 step 390: training accuarcy: 0.6145\n",
      "Epoch 1 step 390: training loss: 1340.5938467825106\n",
      "Epoch 1 step 391: training accuarcy: 0.6115\n",
      "Epoch 1 step 391: training loss: 1351.611423087018\n",
      "Epoch 1 step 392: training accuarcy: 0.614\n",
      "Epoch 1 step 392: training loss: 1348.4328242398203\n",
      "Epoch 1 step 393: training accuarcy: 0.6135\n",
      "Epoch 1 step 393: training loss: 1344.4975043293364\n",
      "Epoch 1 step 394: training accuarcy: 0.6165\n",
      "Epoch 1 step 394: training loss: 1344.9400824538425\n",
      "Epoch 1 step 395: training accuarcy: 0.6145\n",
      "Epoch 1 step 395: training loss: 1346.3619359874838\n",
      "Epoch 1 step 396: training accuarcy: 0.603\n",
      "Epoch 1 step 396: training loss: 1333.6189790071828\n",
      "Epoch 1 step 397: training accuarcy: 0.6245\n",
      "Epoch 1 step 397: training loss: 1335.5324471932734\n",
      "Epoch 1 step 398: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 398: training loss: 1349.8143352656514\n",
      "Epoch 1 step 399: training accuarcy: 0.6185\n",
      "Epoch 1 step 399: training loss: 1332.0074679739166\n",
      "Epoch 1 step 400: training accuarcy: 0.6155\n",
      "Epoch 1 step 400: training loss: 1340.9571587701257\n",
      "Epoch 1 step 401: training accuarcy: 0.6185\n",
      "Epoch 1 step 401: training loss: 1337.0102880990237\n",
      "Epoch 1 step 402: training accuarcy: 0.6045\n",
      "Epoch 1 step 402: training loss: 1335.2359144740294\n",
      "Epoch 1 step 403: training accuarcy: 0.618\n",
      "Epoch 1 step 403: training loss: 1340.1028753549322\n",
      "Epoch 1 step 404: training accuarcy: 0.612\n",
      "Epoch 1 step 404: training loss: 1354.1644035842596\n",
      "Epoch 1 step 405: training accuarcy: 0.602\n",
      "Epoch 1 step 405: training loss: 1353.3266310654894\n",
      "Epoch 1 step 406: training accuarcy: 0.6125\n",
      "Epoch 1 step 406: training loss: 1346.2241661121218\n",
      "Epoch 1 step 407: training accuarcy: 0.6115\n",
      "Epoch 1 step 407: training loss: 1329.3884684618233\n",
      "Epoch 1 step 408: training accuarcy: 0.635\n",
      "Epoch 1 step 408: training loss: 1346.3100514376067\n",
      "Epoch 1 step 409: training accuarcy: 0.6065\n",
      "Epoch 1 step 409: training loss: 1349.4211252627404\n",
      "Epoch 1 step 410: training accuarcy: 0.611\n",
      "Epoch 1 step 410: training loss: 1332.0605698886814\n",
      "Epoch 1 step 411: training accuarcy: 0.6085\n",
      "Epoch 1 step 411: training loss: 1338.801147621816\n",
      "Epoch 1 step 412: training accuarcy: 0.62\n",
      "Epoch 1 step 412: training loss: 1352.0909755639022\n",
      "Epoch 1 step 413: training accuarcy: 0.6135\n",
      "Epoch 1 step 413: training loss: 1330.6028875912593\n",
      "Epoch 1 step 414: training accuarcy: 0.62\n",
      "Epoch 1 step 414: training loss: 1353.4728774502462\n",
      "Epoch 1 step 415: training accuarcy: 0.601\n",
      "Epoch 1 step 415: training loss: 1334.385766798099\n",
      "Epoch 1 step 416: training accuarcy: 0.617\n",
      "Epoch 1 step 416: training loss: 1328.5841795185622\n",
      "Epoch 1 step 417: training accuarcy: 0.632\n",
      "Epoch 1 step 417: training loss: 1348.2103455980591\n",
      "Epoch 1 step 418: training accuarcy: 0.611\n",
      "Epoch 1 step 418: training loss: 1353.0023754734575\n",
      "Epoch 1 step 419: training accuarcy: 0.594\n",
      "Epoch 1 step 419: training loss: 1337.6211118757126\n",
      "Epoch 1 step 420: training accuarcy: 0.616\n",
      "Epoch 1 step 420: training loss: 1339.3919377542138\n",
      "Epoch 1 step 421: training accuarcy: 0.6195\n",
      "Epoch 1 step 421: training loss: 1352.9810261768569\n",
      "Epoch 1 step 422: training accuarcy: 0.6105\n",
      "Epoch 1 step 422: training loss: 1334.2225768210021\n",
      "Epoch 1 step 423: training accuarcy: 0.6075\n",
      "Epoch 1 step 423: training loss: 1334.2579478131215\n",
      "Epoch 1 step 424: training accuarcy: 0.607\n",
      "Epoch 1 step 424: training loss: 1355.143452922959\n",
      "Epoch 1 step 425: training accuarcy: 0.598\n",
      "Epoch 1 step 425: training loss: 1336.226924377744\n",
      "Epoch 1 step 426: training accuarcy: 0.6185\n",
      "Epoch 1 step 426: training loss: 1329.257118679843\n",
      "Epoch 1 step 427: training accuarcy: 0.64\n",
      "Epoch 1 step 427: training loss: 1357.679807064116\n",
      "Epoch 1 step 428: training accuarcy: 0.5995\n",
      "Epoch 1 step 428: training loss: 1350.630604958537\n",
      "Epoch 1 step 429: training accuarcy: 0.6085\n",
      "Epoch 1 step 429: training loss: 1341.9557492543158\n",
      "Epoch 1 step 430: training accuarcy: 0.6225\n",
      "Epoch 1 step 430: training loss: 1348.4629624433715\n",
      "Epoch 1 step 431: training accuarcy: 0.606\n",
      "Epoch 1 step 431: training loss: 1352.0123506974526\n",
      "Epoch 1 step 432: training accuarcy: 0.6155\n",
      "Epoch 1 step 432: training loss: 1335.2858561974194\n",
      "Epoch 1 step 433: training accuarcy: 0.606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 433: training loss: 1348.5947188580703\n",
      "Epoch 1 step 434: training accuarcy: 0.609\n",
      "Epoch 1 step 434: training loss: 1339.314142998349\n",
      "Epoch 1 step 435: training accuarcy: 0.6185\n",
      "Epoch 1 step 435: training loss: 1343.3462731506677\n",
      "Epoch 1 step 436: training accuarcy: 0.611\n",
      "Epoch 1 step 436: training loss: 1332.1232493210937\n",
      "Epoch 1 step 437: training accuarcy: 0.615\n",
      "Epoch 1 step 437: training loss: 1336.7870718817728\n",
      "Epoch 1 step 438: training accuarcy: 0.6245\n",
      "Epoch 1 step 438: training loss: 1355.2338915200487\n",
      "Epoch 1 step 439: training accuarcy: 0.6005\n",
      "Epoch 1 step 439: training loss: 1334.603009756785\n",
      "Epoch 1 step 440: training accuarcy: 0.618\n",
      "Epoch 1 step 440: training loss: 1337.9802707673225\n",
      "Epoch 1 step 441: training accuarcy: 0.6245\n",
      "Epoch 1 step 441: training loss: 1347.0879459400448\n",
      "Epoch 1 step 442: training accuarcy: 0.6035\n",
      "Epoch 1 step 442: training loss: 1345.5188790841307\n",
      "Epoch 1 step 443: training accuarcy: 0.6165\n",
      "Epoch 1 step 443: training loss: 1357.2296551630611\n",
      "Epoch 1 step 444: training accuarcy: 0.59\n",
      "Epoch 1 step 444: training loss: 1331.7082338381285\n",
      "Epoch 1 step 445: training accuarcy: 0.631\n",
      "Epoch 1 step 445: training loss: 1342.351068209188\n",
      "Epoch 1 step 446: training accuarcy: 0.6055\n",
      "Epoch 1 step 446: training loss: 1344.5329151583353\n",
      "Epoch 1 step 447: training accuarcy: 0.6045\n",
      "Epoch 1 step 447: training loss: 1348.5909030912032\n",
      "Epoch 1 step 448: training accuarcy: 0.61\n",
      "Epoch 1 step 448: training loss: 1338.0005237432476\n",
      "Epoch 1 step 449: training accuarcy: 0.6125\n",
      "Epoch 1 step 449: training loss: 1352.31152523542\n",
      "Epoch 1 step 450: training accuarcy: 0.602\n",
      "Epoch 1 step 450: training loss: 1329.6500430882631\n",
      "Epoch 1 step 451: training accuarcy: 0.6205\n",
      "Epoch 1 step 451: training loss: 1338.74171286519\n",
      "Epoch 1 step 452: training accuarcy: 0.619\n",
      "Epoch 1 step 452: training loss: 1344.5886227490737\n",
      "Epoch 1 step 453: training accuarcy: 0.612\n",
      "Epoch 1 step 453: training loss: 1332.4143612629582\n",
      "Epoch 1 step 454: training accuarcy: 0.627\n",
      "Epoch 1 step 454: training loss: 1344.3719597464847\n",
      "Epoch 1 step 455: training accuarcy: 0.612\n",
      "Epoch 1 step 455: training loss: 1326.1634782574424\n",
      "Epoch 1 step 456: training accuarcy: 0.633\n",
      "Epoch 1 step 456: training loss: 1329.1587603127964\n",
      "Epoch 1 step 457: training accuarcy: 0.6305000000000001\n",
      "Epoch 1 step 457: training loss: 1326.3200301735667\n",
      "Epoch 1 step 458: training accuarcy: 0.6085\n",
      "Epoch 1 step 458: training loss: 1330.8093061542652\n",
      "Epoch 1 step 459: training accuarcy: 0.622\n",
      "Epoch 1 step 459: training loss: 1339.6388901530522\n",
      "Epoch 1 step 460: training accuarcy: 0.608\n",
      "Epoch 1 step 460: training loss: 1331.6829619735565\n",
      "Epoch 1 step 461: training accuarcy: 0.614\n",
      "Epoch 1 step 461: training loss: 1337.625361152845\n",
      "Epoch 1 step 462: training accuarcy: 0.622\n",
      "Epoch 1 step 462: training loss: 1352.7332682403282\n",
      "Epoch 1 step 463: training accuarcy: 0.5895\n",
      "Epoch 1 step 463: training loss: 1341.851450312127\n",
      "Epoch 1 step 464: training accuarcy: 0.6125\n",
      "Epoch 1 step 464: training loss: 1338.0324236200381\n",
      "Epoch 1 step 465: training accuarcy: 0.598\n",
      "Epoch 1 step 465: training loss: 1360.8609499136628\n",
      "Epoch 1 step 466: training accuarcy: 0.588\n",
      "Epoch 1 step 466: training loss: 1338.2102090854653\n",
      "Epoch 1 step 467: training accuarcy: 0.615\n",
      "Epoch 1 step 467: training loss: 1344.4748035636635\n",
      "Epoch 1 step 468: training accuarcy: 0.6225\n",
      "Epoch 1 step 468: training loss: 1336.0082024516514\n",
      "Epoch 1 step 469: training accuarcy: 0.62\n",
      "Epoch 1 step 469: training loss: 1321.04150415487\n",
      "Epoch 1 step 470: training accuarcy: 0.6365000000000001\n",
      "Epoch 1 step 470: training loss: 1335.968000860704\n",
      "Epoch 1 step 471: training accuarcy: 0.6195\n",
      "Epoch 1 step 471: training loss: 1329.838911479692\n",
      "Epoch 1 step 472: training accuarcy: 0.619\n",
      "Epoch 1 step 472: training loss: 1320.116455798193\n",
      "Epoch 1 step 473: training accuarcy: 0.6405\n",
      "Epoch 1 step 473: training loss: 1335.7300831506955\n",
      "Epoch 1 step 474: training accuarcy: 0.6255000000000001\n",
      "Epoch 1 step 474: training loss: 1342.362171727179\n",
      "Epoch 1 step 475: training accuarcy: 0.623\n",
      "Epoch 1 step 475: training loss: 1349.9783834065754\n",
      "Epoch 1 step 476: training accuarcy: 0.61\n",
      "Epoch 1 step 476: training loss: 1339.822555702545\n",
      "Epoch 1 step 477: training accuarcy: 0.608\n",
      "Epoch 1 step 477: training loss: 1351.7541090370996\n",
      "Epoch 1 step 478: training accuarcy: 0.6105\n",
      "Epoch 1 step 478: training loss: 1341.6557380489985\n",
      "Epoch 1 step 479: training accuarcy: 0.6075\n",
      "Epoch 1 step 479: training loss: 1333.3304876117836\n",
      "Epoch 1 step 480: training accuarcy: 0.625\n",
      "Epoch 1 step 480: training loss: 1348.0792907489076\n",
      "Epoch 1 step 481: training accuarcy: 0.61\n",
      "Epoch 1 step 481: training loss: 1330.8878041965306\n",
      "Epoch 1 step 482: training accuarcy: 0.618\n",
      "Epoch 1 step 482: training loss: 1356.1897855880663\n",
      "Epoch 1 step 483: training accuarcy: 0.6055\n",
      "Epoch 1 step 483: training loss: 1342.1571579567326\n",
      "Epoch 1 step 484: training accuarcy: 0.608\n",
      "Epoch 1 step 484: training loss: 1337.2135359936508\n",
      "Epoch 1 step 485: training accuarcy: 0.631\n",
      "Epoch 1 step 485: training loss: 1358.6057578617356\n",
      "Epoch 1 step 486: training accuarcy: 0.598\n",
      "Epoch 1 step 486: training loss: 1343.186795355403\n",
      "Epoch 1 step 487: training accuarcy: 0.603\n",
      "Epoch 1 step 487: training loss: 1350.2031268652381\n",
      "Epoch 1 step 488: training accuarcy: 0.607\n",
      "Epoch 1 step 488: training loss: 1335.7177003132917\n",
      "Epoch 1 step 489: training accuarcy: 0.6145\n",
      "Epoch 1 step 489: training loss: 1340.592826702675\n",
      "Epoch 1 step 490: training accuarcy: 0.6095\n",
      "Epoch 1 step 490: training loss: 1343.4913469080427\n",
      "Epoch 1 step 491: training accuarcy: 0.605\n",
      "Epoch 1 step 491: training loss: 1353.7977440694428\n",
      "Epoch 1 step 492: training accuarcy: 0.593\n",
      "Epoch 1 step 492: training loss: 1334.5201956110461\n",
      "Epoch 1 step 493: training accuarcy: 0.6225\n",
      "Epoch 1 step 493: training loss: 1320.577581879006\n",
      "Epoch 1 step 494: training accuarcy: 0.644\n",
      "Epoch 1 step 494: training loss: 1362.7548579775407\n",
      "Epoch 1 step 495: training accuarcy: 0.598\n",
      "Epoch 1 step 495: training loss: 1341.7800433126056\n",
      "Epoch 1 step 496: training accuarcy: 0.6175\n",
      "Epoch 1 step 496: training loss: 1348.330626646348\n",
      "Epoch 1 step 497: training accuarcy: 0.6015\n",
      "Epoch 1 step 497: training loss: 1346.4886046401193\n",
      "Epoch 1 step 498: training accuarcy: 0.6105\n",
      "Epoch 1 step 498: training loss: 1335.4632355558276\n",
      "Epoch 1 step 499: training accuarcy: 0.603\n",
      "Epoch 1 step 499: training loss: 1340.905049976299\n",
      "Epoch 1 step 500: training accuarcy: 0.6235\n",
      "Epoch 1 step 500: training loss: 1323.1307421093875\n",
      "Epoch 1 step 501: training accuarcy: 0.6395000000000001\n",
      "Epoch 1 step 501: training loss: 1328.3817870467988\n",
      "Epoch 1 step 502: training accuarcy: 0.6155\n",
      "Epoch 1 step 502: training loss: 1335.8636608300396\n",
      "Epoch 1 step 503: training accuarcy: 0.6185\n",
      "Epoch 1 step 503: training loss: 1343.9634269558399\n",
      "Epoch 1 step 504: training accuarcy: 0.6165\n",
      "Epoch 1 step 504: training loss: 1341.0756875187458\n",
      "Epoch 1 step 505: training accuarcy: 0.6185\n",
      "Epoch 1 step 505: training loss: 1330.9755069123119\n",
      "Epoch 1 step 506: training accuarcy: 0.6055\n",
      "Epoch 1 step 506: training loss: 1340.2286832785228\n",
      "Epoch 1 step 507: training accuarcy: 0.6135\n",
      "Epoch 1 step 507: training loss: 1334.2710567511378\n",
      "Epoch 1 step 508: training accuarcy: 0.611\n",
      "Epoch 1 step 508: training loss: 1346.6022202549425\n",
      "Epoch 1 step 509: training accuarcy: 0.609\n",
      "Epoch 1 step 509: training loss: 1330.362178010831\n",
      "Epoch 1 step 510: training accuarcy: 0.613\n",
      "Epoch 1 step 510: training loss: 1341.2138621216238\n",
      "Epoch 1 step 511: training accuarcy: 0.603\n",
      "Epoch 1 step 511: training loss: 1347.0659058497656\n",
      "Epoch 1 step 512: training accuarcy: 0.61\n",
      "Epoch 1 step 512: training loss: 1334.7845503001417\n",
      "Epoch 1 step 513: training accuarcy: 0.6245\n",
      "Epoch 1 step 513: training loss: 1338.996686510193\n",
      "Epoch 1 step 514: training accuarcy: 0.6015\n",
      "Epoch 1 step 514: training loss: 1329.239277195683\n",
      "Epoch 1 step 515: training accuarcy: 0.619\n",
      "Epoch 1 step 515: training loss: 1324.6763330183232\n",
      "Epoch 1 step 516: training accuarcy: 0.6315000000000001\n",
      "Epoch 1 step 516: training loss: 1345.8990102832327\n",
      "Epoch 1 step 517: training accuarcy: 0.611\n",
      "Epoch 1 step 517: training loss: 1334.4774214194113\n",
      "Epoch 1 step 518: training accuarcy: 0.597\n",
      "Epoch 1 step 518: training loss: 1338.8966303647176\n",
      "Epoch 1 step 519: training accuarcy: 0.6\n",
      "Epoch 1 step 519: training loss: 1334.0189799808327\n",
      "Epoch 1 step 520: training accuarcy: 0.618\n",
      "Epoch 1 step 520: training loss: 1331.668480591171\n",
      "Epoch 1 step 521: training accuarcy: 0.615\n",
      "Epoch 1 step 521: training loss: 1346.77636258311\n",
      "Epoch 1 step 522: training accuarcy: 0.6075\n",
      "Epoch 1 step 522: training loss: 1340.913627891337\n",
      "Epoch 1 step 523: training accuarcy: 0.6105\n",
      "Epoch 1 step 523: training loss: 1325.6515204834081\n",
      "Epoch 1 step 524: training accuarcy: 0.6245\n",
      "Epoch 1 step 524: training loss: 1344.86168401746\n",
      "Epoch 1 step 525: training accuarcy: 0.62\n",
      "Epoch 1 step 525: training loss: 531.7600413006064\n",
      "Epoch 1 step 526: training accuarcy: 0.6051282051282051\n",
      "Epoch 1: train loss 1340.0697215952782, train accuarcy 0.6178756952285767\n",
      "Epoch 1: valid loss 6666.3279985804165, valid accuarcy 0.5893269777297974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████████████████████████████▊                                                                                                       | 2/5 [09:38<14:28, 289.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 1332.095426407643\n",
      "Epoch 2 step 527: training accuarcy: 0.6155\n",
      "Epoch 2 step 527: training loss: 1328.6302345933602\n",
      "Epoch 2 step 528: training accuarcy: 0.635\n",
      "Epoch 2 step 528: training loss: 1332.1218942754674\n",
      "Epoch 2 step 529: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 529: training loss: 1333.006303805942\n",
      "Epoch 2 step 530: training accuarcy: 0.6175\n",
      "Epoch 2 step 530: training loss: 1344.764891940574\n",
      "Epoch 2 step 531: training accuarcy: 0.5975\n",
      "Epoch 2 step 531: training loss: 1331.1625332342367\n",
      "Epoch 2 step 532: training accuarcy: 0.624\n",
      "Epoch 2 step 532: training loss: 1325.5540146222415\n",
      "Epoch 2 step 533: training accuarcy: 0.635\n",
      "Epoch 2 step 533: training loss: 1337.7637676940174\n",
      "Epoch 2 step 534: training accuarcy: 0.615\n",
      "Epoch 2 step 534: training loss: 1346.1481183222213\n",
      "Epoch 2 step 535: training accuarcy: 0.5935\n",
      "Epoch 2 step 535: training loss: 1342.70014249405\n",
      "Epoch 2 step 536: training accuarcy: 0.611\n",
      "Epoch 2 step 536: training loss: 1340.1810452755021\n",
      "Epoch 2 step 537: training accuarcy: 0.6035\n",
      "Epoch 2 step 537: training loss: 1335.9873758155416\n",
      "Epoch 2 step 538: training accuarcy: 0.6285000000000001\n",
      "Epoch 2 step 538: training loss: 1348.874116998068\n",
      "Epoch 2 step 539: training accuarcy: 0.6175\n",
      "Epoch 2 step 539: training loss: 1342.5031872963507\n",
      "Epoch 2 step 540: training accuarcy: 0.6195\n",
      "Epoch 2 step 540: training loss: 1332.2465759432027\n",
      "Epoch 2 step 541: training accuarcy: 0.6185\n",
      "Epoch 2 step 541: training loss: 1322.9703685491659\n",
      "Epoch 2 step 542: training accuarcy: 0.63\n",
      "Epoch 2 step 542: training loss: 1338.3077684234336\n",
      "Epoch 2 step 543: training accuarcy: 0.615\n",
      "Epoch 2 step 543: training loss: 1342.097290308075\n",
      "Epoch 2 step 544: training accuarcy: 0.6135\n",
      "Epoch 2 step 544: training loss: 1350.311941364497\n",
      "Epoch 2 step 545: training accuarcy: 0.598\n",
      "Epoch 2 step 545: training loss: 1341.8511468288157\n",
      "Epoch 2 step 546: training accuarcy: 0.5935\n",
      "Epoch 2 step 546: training loss: 1345.1850469841486\n",
      "Epoch 2 step 547: training accuarcy: 0.6115\n",
      "Epoch 2 step 547: training loss: 1329.562755162256\n",
      "Epoch 2 step 548: training accuarcy: 0.624\n",
      "Epoch 2 step 548: training loss: 1338.333824524249\n",
      "Epoch 2 step 549: training accuarcy: 0.625\n",
      "Epoch 2 step 549: training loss: 1346.3287467357823\n",
      "Epoch 2 step 550: training accuarcy: 0.614\n",
      "Epoch 2 step 550: training loss: 1335.8550694725432\n",
      "Epoch 2 step 551: training accuarcy: 0.6135\n",
      "Epoch 2 step 551: training loss: 1323.464880199973\n",
      "Epoch 2 step 552: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 552: training loss: 1345.793396770056\n",
      "Epoch 2 step 553: training accuarcy: 0.611\n",
      "Epoch 2 step 553: training loss: 1349.2108902786124\n",
      "Epoch 2 step 554: training accuarcy: 0.5995\n",
      "Epoch 2 step 554: training loss: 1329.8871829612685\n",
      "Epoch 2 step 555: training accuarcy: 0.624\n",
      "Epoch 2 step 555: training loss: 1339.9175672958042\n",
      "Epoch 2 step 556: training accuarcy: 0.6155\n",
      "Epoch 2 step 556: training loss: 1347.4194312602826\n",
      "Epoch 2 step 557: training accuarcy: 0.6035\n",
      "Epoch 2 step 557: training loss: 1345.9292680869758\n",
      "Epoch 2 step 558: training accuarcy: 0.61\n",
      "Epoch 2 step 558: training loss: 1332.108241646468\n",
      "Epoch 2 step 559: training accuarcy: 0.621\n",
      "Epoch 2 step 559: training loss: 1343.4325326156404\n",
      "Epoch 2 step 560: training accuarcy: 0.612\n",
      "Epoch 2 step 560: training loss: 1343.4663471381605\n",
      "Epoch 2 step 561: training accuarcy: 0.606\n",
      "Epoch 2 step 561: training loss: 1335.9687001316895\n",
      "Epoch 2 step 562: training accuarcy: 0.6095\n",
      "Epoch 2 step 562: training loss: 1334.023993943823\n",
      "Epoch 2 step 563: training accuarcy: 0.6105\n",
      "Epoch 2 step 563: training loss: 1328.362760662843\n",
      "Epoch 2 step 564: training accuarcy: 0.6195\n",
      "Epoch 2 step 564: training loss: 1344.3332617882309\n",
      "Epoch 2 step 565: training accuarcy: 0.6175\n",
      "Epoch 2 step 565: training loss: 1328.0288097455084\n",
      "Epoch 2 step 566: training accuarcy: 0.6325000000000001\n",
      "Epoch 2 step 566: training loss: 1339.5518072865527\n",
      "Epoch 2 step 567: training accuarcy: 0.6085\n",
      "Epoch 2 step 567: training loss: 1343.0170258447292\n",
      "Epoch 2 step 568: training accuarcy: 0.607\n",
      "Epoch 2 step 568: training loss: 1336.8439645088297\n",
      "Epoch 2 step 569: training accuarcy: 0.6075\n",
      "Epoch 2 step 569: training loss: 1346.4119539073197\n",
      "Epoch 2 step 570: training accuarcy: 0.602\n",
      "Epoch 2 step 570: training loss: 1325.8731266628606\n",
      "Epoch 2 step 571: training accuarcy: 0.632\n",
      "Epoch 2 step 571: training loss: 1345.4799687511265\n",
      "Epoch 2 step 572: training accuarcy: 0.6\n",
      "Epoch 2 step 572: training loss: 1337.3867035146422\n",
      "Epoch 2 step 573: training accuarcy: 0.6135\n",
      "Epoch 2 step 573: training loss: 1341.880632926564\n",
      "Epoch 2 step 574: training accuarcy: 0.6205\n",
      "Epoch 2 step 574: training loss: 1342.0547371067537\n",
      "Epoch 2 step 575: training accuarcy: 0.6195\n",
      "Epoch 2 step 575: training loss: 1343.209457386768\n",
      "Epoch 2 step 576: training accuarcy: 0.6005\n",
      "Epoch 2 step 576: training loss: 1340.1510871420917\n",
      "Epoch 2 step 577: training accuarcy: 0.612\n",
      "Epoch 2 step 577: training loss: 1353.0963414696912\n",
      "Epoch 2 step 578: training accuarcy: 0.5995\n",
      "Epoch 2 step 578: training loss: 1346.1688504733663\n",
      "Epoch 2 step 579: training accuarcy: 0.6045\n",
      "Epoch 2 step 579: training loss: 1345.7545323242293\n",
      "Epoch 2 step 580: training accuarcy: 0.5975\n",
      "Epoch 2 step 580: training loss: 1340.1358358397054\n",
      "Epoch 2 step 581: training accuarcy: 0.6145\n",
      "Epoch 2 step 581: training loss: 1337.731834789277\n",
      "Epoch 2 step 582: training accuarcy: 0.604\n",
      "Epoch 2 step 582: training loss: 1331.3106177627483\n",
      "Epoch 2 step 583: training accuarcy: 0.601\n",
      "Epoch 2 step 583: training loss: 1341.2035802320752\n",
      "Epoch 2 step 584: training accuarcy: 0.604\n",
      "Epoch 2 step 584: training loss: 1334.1636032720785\n",
      "Epoch 2 step 585: training accuarcy: 0.614\n",
      "Epoch 2 step 585: training loss: 1341.4420921947524\n",
      "Epoch 2 step 586: training accuarcy: 0.621\n",
      "Epoch 2 step 586: training loss: 1332.7664366284416\n",
      "Epoch 2 step 587: training accuarcy: 0.623\n",
      "Epoch 2 step 587: training loss: 1330.7387688195295\n",
      "Epoch 2 step 588: training accuarcy: 0.6145\n",
      "Epoch 2 step 588: training loss: 1338.5602952609313\n",
      "Epoch 2 step 589: training accuarcy: 0.611\n",
      "Epoch 2 step 589: training loss: 1329.3538204714687\n",
      "Epoch 2 step 590: training accuarcy: 0.632\n",
      "Epoch 2 step 590: training loss: 1354.6860190968682\n",
      "Epoch 2 step 591: training accuarcy: 0.5975\n",
      "Epoch 2 step 591: training loss: 1335.4372726911472\n",
      "Epoch 2 step 592: training accuarcy: 0.6155\n",
      "Epoch 2 step 592: training loss: 1332.0427215777777\n",
      "Epoch 2 step 593: training accuarcy: 0.6065\n",
      "Epoch 2 step 593: training loss: 1332.0740873864058\n",
      "Epoch 2 step 594: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 594: training loss: 1349.4404769108312\n",
      "Epoch 2 step 595: training accuarcy: 0.5925\n",
      "Epoch 2 step 595: training loss: 1341.7316460477325\n",
      "Epoch 2 step 596: training accuarcy: 0.6015\n",
      "Epoch 2 step 596: training loss: 1341.3465380459602\n",
      "Epoch 2 step 597: training accuarcy: 0.6025\n",
      "Epoch 2 step 597: training loss: 1336.9947580672167\n",
      "Epoch 2 step 598: training accuarcy: 0.6115\n",
      "Epoch 2 step 598: training loss: 1347.8185045028256\n",
      "Epoch 2 step 599: training accuarcy: 0.616\n",
      "Epoch 2 step 599: training loss: 1331.1610289016999\n",
      "Epoch 2 step 600: training accuarcy: 0.62\n",
      "Epoch 2 step 600: training loss: 1336.8262129920354\n",
      "Epoch 2 step 601: training accuarcy: 0.6315000000000001\n",
      "Epoch 2 step 601: training loss: 1338.0180145257557\n",
      "Epoch 2 step 602: training accuarcy: 0.6025\n",
      "Epoch 2 step 602: training loss: 1336.7231078787413\n",
      "Epoch 2 step 603: training accuarcy: 0.6215\n",
      "Epoch 2 step 603: training loss: 1340.6980804969392\n",
      "Epoch 2 step 604: training accuarcy: 0.61\n",
      "Epoch 2 step 604: training loss: 1329.893140848719\n",
      "Epoch 2 step 605: training accuarcy: 0.6165\n",
      "Epoch 2 step 605: training loss: 1344.5143141391193\n",
      "Epoch 2 step 606: training accuarcy: 0.6105\n",
      "Epoch 2 step 606: training loss: 1354.5333578959599\n",
      "Epoch 2 step 607: training accuarcy: 0.605\n",
      "Epoch 2 step 607: training loss: 1325.1904389899264\n",
      "Epoch 2 step 608: training accuarcy: 0.6195\n",
      "Epoch 2 step 608: training loss: 1339.7816152218854\n",
      "Epoch 2 step 609: training accuarcy: 0.612\n",
      "Epoch 2 step 609: training loss: 1323.2613936354232\n",
      "Epoch 2 step 610: training accuarcy: 0.6205\n",
      "Epoch 2 step 610: training loss: 1337.2495404556578\n",
      "Epoch 2 step 611: training accuarcy: 0.622\n",
      "Epoch 2 step 611: training loss: 1336.4801123965153\n",
      "Epoch 2 step 612: training accuarcy: 0.615\n",
      "Epoch 2 step 612: training loss: 1335.6633217814708\n",
      "Epoch 2 step 613: training accuarcy: 0.604\n",
      "Epoch 2 step 613: training loss: 1348.2472456761407\n",
      "Epoch 2 step 614: training accuarcy: 0.6275000000000001\n",
      "Epoch 2 step 614: training loss: 1342.3782536657698\n",
      "Epoch 2 step 615: training accuarcy: 0.595\n",
      "Epoch 2 step 615: training loss: 1325.1484468921767\n",
      "Epoch 2 step 616: training accuarcy: 0.6175\n",
      "Epoch 2 step 616: training loss: 1340.2992361241293\n",
      "Epoch 2 step 617: training accuarcy: 0.6185\n",
      "Epoch 2 step 617: training loss: 1339.355637187622\n",
      "Epoch 2 step 618: training accuarcy: 0.616\n",
      "Epoch 2 step 618: training loss: 1339.49680540959\n",
      "Epoch 2 step 619: training accuarcy: 0.5975\n",
      "Epoch 2 step 619: training loss: 1347.996830295261\n",
      "Epoch 2 step 620: training accuarcy: 0.606\n",
      "Epoch 2 step 620: training loss: 1335.9628527510877\n",
      "Epoch 2 step 621: training accuarcy: 0.62\n",
      "Epoch 2 step 621: training loss: 1331.8263878117486\n",
      "Epoch 2 step 622: training accuarcy: 0.6165\n",
      "Epoch 2 step 622: training loss: 1338.8276896034965\n",
      "Epoch 2 step 623: training accuarcy: 0.6035\n",
      "Epoch 2 step 623: training loss: 1344.2558305670993\n",
      "Epoch 2 step 624: training accuarcy: 0.611\n",
      "Epoch 2 step 624: training loss: 1319.3469785266934\n",
      "Epoch 2 step 625: training accuarcy: 0.6185\n",
      "Epoch 2 step 625: training loss: 1322.510477067895\n",
      "Epoch 2 step 626: training accuarcy: 0.63\n",
      "Epoch 2 step 626: training loss: 1339.4872531576573\n",
      "Epoch 2 step 627: training accuarcy: 0.5975\n",
      "Epoch 2 step 627: training loss: 1335.9427474043675\n",
      "Epoch 2 step 628: training accuarcy: 0.6145\n",
      "Epoch 2 step 628: training loss: 1334.9004840332204\n",
      "Epoch 2 step 629: training accuarcy: 0.608\n",
      "Epoch 2 step 629: training loss: 1341.497394328844\n",
      "Epoch 2 step 630: training accuarcy: 0.607\n",
      "Epoch 2 step 630: training loss: 1306.536141304574\n",
      "Epoch 2 step 631: training accuarcy: 0.6405\n",
      "Epoch 2 step 631: training loss: 1335.052147646957\n",
      "Epoch 2 step 632: training accuarcy: 0.612\n",
      "Epoch 2 step 632: training loss: 1331.3978108536946\n",
      "Epoch 2 step 633: training accuarcy: 0.621\n",
      "Epoch 2 step 633: training loss: 1327.3715565932139\n",
      "Epoch 2 step 634: training accuarcy: 0.619\n",
      "Epoch 2 step 634: training loss: 1329.344189156623\n",
      "Epoch 2 step 635: training accuarcy: 0.6195\n",
      "Epoch 2 step 635: training loss: 1354.0945721657251\n",
      "Epoch 2 step 636: training accuarcy: 0.607\n",
      "Epoch 2 step 636: training loss: 1336.1140613599782\n",
      "Epoch 2 step 637: training accuarcy: 0.6135\n",
      "Epoch 2 step 637: training loss: 1338.0696816798693\n",
      "Epoch 2 step 638: training accuarcy: 0.617\n",
      "Epoch 2 step 638: training loss: 1345.2608281147468\n",
      "Epoch 2 step 639: training accuarcy: 0.612\n",
      "Epoch 2 step 639: training loss: 1336.4780286882274\n",
      "Epoch 2 step 640: training accuarcy: 0.608\n",
      "Epoch 2 step 640: training loss: 1347.5531472667585\n",
      "Epoch 2 step 641: training accuarcy: 0.614\n",
      "Epoch 2 step 641: training loss: 1327.9146363460693\n",
      "Epoch 2 step 642: training accuarcy: 0.606\n",
      "Epoch 2 step 642: training loss: 1321.2977648608748\n",
      "Epoch 2 step 643: training accuarcy: 0.6285000000000001\n",
      "Epoch 2 step 643: training loss: 1326.4145700326667\n",
      "Epoch 2 step 644: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 644: training loss: 1329.3879714773532\n",
      "Epoch 2 step 645: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 645: training loss: 1355.9062871274823\n",
      "Epoch 2 step 646: training accuarcy: 0.6015\n",
      "Epoch 2 step 646: training loss: 1320.1920918835258\n",
      "Epoch 2 step 647: training accuarcy: 0.631\n",
      "Epoch 2 step 647: training loss: 1321.5772902850053\n",
      "Epoch 2 step 648: training accuarcy: 0.632\n",
      "Epoch 2 step 648: training loss: 1352.65847593107\n",
      "Epoch 2 step 649: training accuarcy: 0.612\n",
      "Epoch 2 step 649: training loss: 1318.0856306046503\n",
      "Epoch 2 step 650: training accuarcy: 0.6285000000000001\n",
      "Epoch 2 step 650: training loss: 1338.0494830562561\n",
      "Epoch 2 step 651: training accuarcy: 0.64\n",
      "Epoch 2 step 651: training loss: 1343.7304655552875\n",
      "Epoch 2 step 652: training accuarcy: 0.6185\n",
      "Epoch 2 step 652: training loss: 1324.0560415235818\n",
      "Epoch 2 step 653: training accuarcy: 0.627\n",
      "Epoch 2 step 653: training loss: 1335.4788665140425\n",
      "Epoch 2 step 654: training accuarcy: 0.6155\n",
      "Epoch 2 step 654: training loss: 1342.8098792696683\n",
      "Epoch 2 step 655: training accuarcy: 0.605\n",
      "Epoch 2 step 655: training loss: 1337.290745507336\n",
      "Epoch 2 step 656: training accuarcy: 0.619\n",
      "Epoch 2 step 656: training loss: 1331.670457050749\n",
      "Epoch 2 step 657: training accuarcy: 0.621\n",
      "Epoch 2 step 657: training loss: 1339.5874298437159\n",
      "Epoch 2 step 658: training accuarcy: 0.618\n",
      "Epoch 2 step 658: training loss: 1345.9355893633162\n",
      "Epoch 2 step 659: training accuarcy: 0.6025\n",
      "Epoch 2 step 659: training loss: 1339.1170766454784\n",
      "Epoch 2 step 660: training accuarcy: 0.613\n",
      "Epoch 2 step 660: training loss: 1341.8898649765429\n",
      "Epoch 2 step 661: training accuarcy: 0.608\n",
      "Epoch 2 step 661: training loss: 1327.8516105337899\n",
      "Epoch 2 step 662: training accuarcy: 0.6295000000000001\n",
      "Epoch 2 step 662: training loss: 1329.1348856355107\n",
      "Epoch 2 step 663: training accuarcy: 0.622\n",
      "Epoch 2 step 663: training loss: 1320.9856078344749\n",
      "Epoch 2 step 664: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 664: training loss: 1345.2885364111958\n",
      "Epoch 2 step 665: training accuarcy: 0.6025\n",
      "Epoch 2 step 665: training loss: 1328.9910438362651\n",
      "Epoch 2 step 666: training accuarcy: 0.6325000000000001\n",
      "Epoch 2 step 666: training loss: 1336.9642746056234\n",
      "Epoch 2 step 667: training accuarcy: 0.6175\n",
      "Epoch 2 step 667: training loss: 1338.9840338110816\n",
      "Epoch 2 step 668: training accuarcy: 0.611\n",
      "Epoch 2 step 668: training loss: 1327.5125931158118\n",
      "Epoch 2 step 669: training accuarcy: 0.6165\n",
      "Epoch 2 step 669: training loss: 1333.5226031996158\n",
      "Epoch 2 step 670: training accuarcy: 0.6295000000000001\n",
      "Epoch 2 step 670: training loss: 1335.9576755926616\n",
      "Epoch 2 step 671: training accuarcy: 0.619\n",
      "Epoch 2 step 671: training loss: 1331.8962287763707\n",
      "Epoch 2 step 672: training accuarcy: 0.621\n",
      "Epoch 2 step 672: training loss: 1332.7158774545333\n",
      "Epoch 2 step 673: training accuarcy: 0.622\n",
      "Epoch 2 step 673: training loss: 1337.9378262165592\n",
      "Epoch 2 step 674: training accuarcy: 0.6105\n",
      "Epoch 2 step 674: training loss: 1334.6969531083087\n",
      "Epoch 2 step 675: training accuarcy: 0.621\n",
      "Epoch 2 step 675: training loss: 1330.3515613335508\n",
      "Epoch 2 step 676: training accuarcy: 0.6115\n",
      "Epoch 2 step 676: training loss: 1330.4805795155496\n",
      "Epoch 2 step 677: training accuarcy: 0.6215\n",
      "Epoch 2 step 677: training loss: 1345.2454141964915\n",
      "Epoch 2 step 678: training accuarcy: 0.6075\n",
      "Epoch 2 step 678: training loss: 1337.1630508453923\n",
      "Epoch 2 step 679: training accuarcy: 0.61\n",
      "Epoch 2 step 679: training loss: 1345.4629558528297\n",
      "Epoch 2 step 680: training accuarcy: 0.594\n",
      "Epoch 2 step 680: training loss: 1358.5303358537658\n",
      "Epoch 2 step 681: training accuarcy: 0.5865\n",
      "Epoch 2 step 681: training loss: 1336.148988235608\n",
      "Epoch 2 step 682: training accuarcy: 0.618\n",
      "Epoch 2 step 682: training loss: 1335.0851094206912\n",
      "Epoch 2 step 683: training accuarcy: 0.609\n",
      "Epoch 2 step 683: training loss: 1329.5553337434142\n",
      "Epoch 2 step 684: training accuarcy: 0.6165\n",
      "Epoch 2 step 684: training loss: 1333.1109256594257\n",
      "Epoch 2 step 685: training accuarcy: 0.614\n",
      "Epoch 2 step 685: training loss: 1343.177477956973\n",
      "Epoch 2 step 686: training accuarcy: 0.6095\n",
      "Epoch 2 step 686: training loss: 1355.8243152327284\n",
      "Epoch 2 step 687: training accuarcy: 0.6085\n",
      "Epoch 2 step 687: training loss: 1328.3013459083327\n",
      "Epoch 2 step 688: training accuarcy: 0.617\n",
      "Epoch 2 step 688: training loss: 1340.9009671591039\n",
      "Epoch 2 step 689: training accuarcy: 0.6195\n",
      "Epoch 2 step 689: training loss: 1352.8852120555644\n",
      "Epoch 2 step 690: training accuarcy: 0.6025\n",
      "Epoch 2 step 690: training loss: 1335.9409363150685\n",
      "Epoch 2 step 691: training accuarcy: 0.5995\n",
      "Epoch 2 step 691: training loss: 1337.4733488025859\n",
      "Epoch 2 step 692: training accuarcy: 0.613\n",
      "Epoch 2 step 692: training loss: 1331.0309142821768\n",
      "Epoch 2 step 693: training accuarcy: 0.6175\n",
      "Epoch 2 step 693: training loss: 1339.3453358892928\n",
      "Epoch 2 step 694: training accuarcy: 0.62\n",
      "Epoch 2 step 694: training loss: 1347.6461486480348\n",
      "Epoch 2 step 695: training accuarcy: 0.6015\n",
      "Epoch 2 step 695: training loss: 1333.5049538700923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 696: training accuarcy: 0.6115\n",
      "Epoch 2 step 696: training loss: 1323.078792391633\n",
      "Epoch 2 step 697: training accuarcy: 0.6245\n",
      "Epoch 2 step 697: training loss: 1331.9109882547666\n",
      "Epoch 2 step 698: training accuarcy: 0.622\n",
      "Epoch 2 step 698: training loss: 1329.084672645247\n",
      "Epoch 2 step 699: training accuarcy: 0.6325000000000001\n",
      "Epoch 2 step 699: training loss: 1326.7793998508653\n",
      "Epoch 2 step 700: training accuarcy: 0.642\n",
      "Epoch 2 step 700: training loss: 1327.2867475444568\n",
      "Epoch 2 step 701: training accuarcy: 0.627\n",
      "Epoch 2 step 701: training loss: 1342.636157225832\n",
      "Epoch 2 step 702: training accuarcy: 0.6235\n",
      "Epoch 2 step 702: training loss: 1341.6928987352949\n",
      "Epoch 2 step 703: training accuarcy: 0.6175\n",
      "Epoch 2 step 703: training loss: 1341.762118414496\n",
      "Epoch 2 step 704: training accuarcy: 0.616\n",
      "Epoch 2 step 704: training loss: 1343.6280157349702\n",
      "Epoch 2 step 705: training accuarcy: 0.5985\n",
      "Epoch 2 step 705: training loss: 1346.9712317619692\n",
      "Epoch 2 step 706: training accuarcy: 0.6075\n",
      "Epoch 2 step 706: training loss: 1333.2169604078651\n",
      "Epoch 2 step 707: training accuarcy: 0.6195\n",
      "Epoch 2 step 707: training loss: 1334.431713746386\n",
      "Epoch 2 step 708: training accuarcy: 0.6145\n",
      "Epoch 2 step 708: training loss: 1343.9003252061525\n",
      "Epoch 2 step 709: training accuarcy: 0.624\n",
      "Epoch 2 step 709: training loss: 1328.5251271605514\n",
      "Epoch 2 step 710: training accuarcy: 0.62\n",
      "Epoch 2 step 710: training loss: 1327.3014577175566\n",
      "Epoch 2 step 711: training accuarcy: 0.6095\n",
      "Epoch 2 step 711: training loss: 1347.2701951573922\n",
      "Epoch 2 step 712: training accuarcy: 0.6045\n",
      "Epoch 2 step 712: training loss: 1341.4436135018802\n",
      "Epoch 2 step 713: training accuarcy: 0.6115\n",
      "Epoch 2 step 713: training loss: 1341.7495788393678\n",
      "Epoch 2 step 714: training accuarcy: 0.6135\n",
      "Epoch 2 step 714: training loss: 1347.3899546426246\n",
      "Epoch 2 step 715: training accuarcy: 0.608\n",
      "Epoch 2 step 715: training loss: 1318.414048737173\n",
      "Epoch 2 step 716: training accuarcy: 0.6345000000000001\n",
      "Epoch 2 step 716: training loss: 1333.2425250134884\n",
      "Epoch 2 step 717: training accuarcy: 0.6155\n",
      "Epoch 2 step 717: training loss: 1343.7365216986643\n",
      "Epoch 2 step 718: training accuarcy: 0.6005\n",
      "Epoch 2 step 718: training loss: 1340.6079064943106\n",
      "Epoch 2 step 719: training accuarcy: 0.603\n",
      "Epoch 2 step 719: training loss: 1325.633086341179\n",
      "Epoch 2 step 720: training accuarcy: 0.6335000000000001\n",
      "Epoch 2 step 720: training loss: 1329.6731648405475\n",
      "Epoch 2 step 721: training accuarcy: 0.609\n",
      "Epoch 2 step 721: training loss: 1329.8391277198066\n",
      "Epoch 2 step 722: training accuarcy: 0.618\n",
      "Epoch 2 step 722: training loss: 1343.2416891582884\n",
      "Epoch 2 step 723: training accuarcy: 0.6065\n",
      "Epoch 2 step 723: training loss: 1331.9777852346529\n",
      "Epoch 2 step 724: training accuarcy: 0.617\n",
      "Epoch 2 step 724: training loss: 1327.7342337009509\n",
      "Epoch 2 step 725: training accuarcy: 0.6235\n",
      "Epoch 2 step 725: training loss: 1356.245200164577\n",
      "Epoch 2 step 726: training accuarcy: 0.5885\n",
      "Epoch 2 step 726: training loss: 1332.303733642736\n",
      "Epoch 2 step 727: training accuarcy: 0.6275000000000001\n",
      "Epoch 2 step 727: training loss: 1354.5729184117552\n",
      "Epoch 2 step 728: training accuarcy: 0.584\n",
      "Epoch 2 step 728: training loss: 1339.0297631405574\n",
      "Epoch 2 step 729: training accuarcy: 0.6115\n",
      "Epoch 2 step 729: training loss: 1332.296869412846\n",
      "Epoch 2 step 730: training accuarcy: 0.6215\n",
      "Epoch 2 step 730: training loss: 1330.825217555124\n",
      "Epoch 2 step 731: training accuarcy: 0.6175\n",
      "Epoch 2 step 731: training loss: 1332.9427746646977\n",
      "Epoch 2 step 732: training accuarcy: 0.617\n",
      "Epoch 2 step 732: training loss: 1334.4291911414193\n",
      "Epoch 2 step 733: training accuarcy: 0.614\n",
      "Epoch 2 step 733: training loss: 1326.4412212654775\n",
      "Epoch 2 step 734: training accuarcy: 0.624\n",
      "Epoch 2 step 734: training loss: 1333.7371499265626\n",
      "Epoch 2 step 735: training accuarcy: 0.63\n",
      "Epoch 2 step 735: training loss: 1333.8767667608347\n",
      "Epoch 2 step 736: training accuarcy: 0.626\n",
      "Epoch 2 step 736: training loss: 1335.9917189078808\n",
      "Epoch 2 step 737: training accuarcy: 0.6145\n",
      "Epoch 2 step 737: training loss: 1329.0707274830363\n",
      "Epoch 2 step 738: training accuarcy: 0.6055\n",
      "Epoch 2 step 738: training loss: 1333.8513887856475\n",
      "Epoch 2 step 739: training accuarcy: 0.622\n",
      "Epoch 2 step 739: training loss: 1341.7407664212071\n",
      "Epoch 2 step 740: training accuarcy: 0.5965\n",
      "Epoch 2 step 740: training loss: 1342.3690012614322\n",
      "Epoch 2 step 741: training accuarcy: 0.624\n",
      "Epoch 2 step 741: training loss: 1343.8780253465927\n",
      "Epoch 2 step 742: training accuarcy: 0.606\n",
      "Epoch 2 step 742: training loss: 1334.5364481471565\n",
      "Epoch 2 step 743: training accuarcy: 0.621\n",
      "Epoch 2 step 743: training loss: 1323.6062308389319\n",
      "Epoch 2 step 744: training accuarcy: 0.6285000000000001\n",
      "Epoch 2 step 744: training loss: 1353.5137156686023\n",
      "Epoch 2 step 745: training accuarcy: 0.605\n",
      "Epoch 2 step 745: training loss: 1318.248781602697\n",
      "Epoch 2 step 746: training accuarcy: 0.6335000000000001\n",
      "Epoch 2 step 746: training loss: 1348.4853460840804\n",
      "Epoch 2 step 747: training accuarcy: 0.595\n",
      "Epoch 2 step 747: training loss: 1343.94134389736\n",
      "Epoch 2 step 748: training accuarcy: 0.59\n",
      "Epoch 2 step 748: training loss: 1321.000929496012\n",
      "Epoch 2 step 749: training accuarcy: 0.6295000000000001\n",
      "Epoch 2 step 749: training loss: 1333.3735728761633\n",
      "Epoch 2 step 750: training accuarcy: 0.6205\n",
      "Epoch 2 step 750: training loss: 1350.407303372588\n",
      "Epoch 2 step 751: training accuarcy: 0.601\n",
      "Epoch 2 step 751: training loss: 1331.9510814079958\n",
      "Epoch 2 step 752: training accuarcy: 0.62\n",
      "Epoch 2 step 752: training loss: 1341.491169095857\n",
      "Epoch 2 step 753: training accuarcy: 0.61\n",
      "Epoch 2 step 753: training loss: 1323.0293435494148\n",
      "Epoch 2 step 754: training accuarcy: 0.624\n",
      "Epoch 2 step 754: training loss: 1329.7275094629329\n",
      "Epoch 2 step 755: training accuarcy: 0.623\n",
      "Epoch 2 step 755: training loss: 1335.0524607965951\n",
      "Epoch 2 step 756: training accuarcy: 0.627\n",
      "Epoch 2 step 756: training loss: 1335.9503074828829\n",
      "Epoch 2 step 757: training accuarcy: 0.616\n",
      "Epoch 2 step 757: training loss: 1324.0959883837475\n",
      "Epoch 2 step 758: training accuarcy: 0.6285000000000001\n",
      "Epoch 2 step 758: training loss: 1307.6276821469667\n",
      "Epoch 2 step 759: training accuarcy: 0.649\n",
      "Epoch 2 step 759: training loss: 1345.1472583413758\n",
      "Epoch 2 step 760: training accuarcy: 0.5895\n",
      "Epoch 2 step 760: training loss: 1335.914978909734\n",
      "Epoch 2 step 761: training accuarcy: 0.625\n",
      "Epoch 2 step 761: training loss: 1351.6555988778796\n",
      "Epoch 2 step 762: training accuarcy: 0.602\n",
      "Epoch 2 step 762: training loss: 1338.0092012636926\n",
      "Epoch 2 step 763: training accuarcy: 0.631\n",
      "Epoch 2 step 763: training loss: 1327.6447007094555\n",
      "Epoch 2 step 764: training accuarcy: 0.6215\n",
      "Epoch 2 step 764: training loss: 1319.5738352461651\n",
      "Epoch 2 step 765: training accuarcy: 0.638\n",
      "Epoch 2 step 765: training loss: 1325.2142274974174\n",
      "Epoch 2 step 766: training accuarcy: 0.6335000000000001\n",
      "Epoch 2 step 766: training loss: 1323.7357478020122\n",
      "Epoch 2 step 767: training accuarcy: 0.6365000000000001\n",
      "Epoch 2 step 767: training loss: 1354.7860944857798\n",
      "Epoch 2 step 768: training accuarcy: 0.599\n",
      "Epoch 2 step 768: training loss: 1340.113973501612\n",
      "Epoch 2 step 769: training accuarcy: 0.606\n",
      "Epoch 2 step 769: training loss: 1350.626981771376\n",
      "Epoch 2 step 770: training accuarcy: 0.6005\n",
      "Epoch 2 step 770: training loss: 1336.1492835529596\n",
      "Epoch 2 step 771: training accuarcy: 0.6065\n",
      "Epoch 2 step 771: training loss: 1332.75047924569\n",
      "Epoch 2 step 772: training accuarcy: 0.6045\n",
      "Epoch 2 step 772: training loss: 1356.3861591507743\n",
      "Epoch 2 step 773: training accuarcy: 0.5905\n",
      "Epoch 2 step 773: training loss: 1344.8740587679436\n",
      "Epoch 2 step 774: training accuarcy: 0.6035\n",
      "Epoch 2 step 774: training loss: 1336.266587488743\n",
      "Epoch 2 step 775: training accuarcy: 0.618\n",
      "Epoch 2 step 775: training loss: 1342.5425940400835\n",
      "Epoch 2 step 776: training accuarcy: 0.6025\n",
      "Epoch 2 step 776: training loss: 1349.2169855856732\n",
      "Epoch 2 step 777: training accuarcy: 0.616\n",
      "Epoch 2 step 777: training loss: 1330.0095845364583\n",
      "Epoch 2 step 778: training accuarcy: 0.609\n",
      "Epoch 2 step 778: training loss: 1336.7386317348492\n",
      "Epoch 2 step 779: training accuarcy: 0.5945\n",
      "Epoch 2 step 779: training loss: 1327.053120882755\n",
      "Epoch 2 step 780: training accuarcy: 0.6115\n",
      "Epoch 2 step 780: training loss: 1329.4233732034527\n",
      "Epoch 2 step 781: training accuarcy: 0.618\n",
      "Epoch 2 step 781: training loss: 1330.978734655056\n",
      "Epoch 2 step 782: training accuarcy: 0.6205\n",
      "Epoch 2 step 782: training loss: 1346.3173887260757\n",
      "Epoch 2 step 783: training accuarcy: 0.5995\n",
      "Epoch 2 step 783: training loss: 1340.2407463509658\n",
      "Epoch 2 step 784: training accuarcy: 0.616\n",
      "Epoch 2 step 784: training loss: 1331.2652673225143\n",
      "Epoch 2 step 785: training accuarcy: 0.613\n",
      "Epoch 2 step 785: training loss: 1329.6887837680144\n",
      "Epoch 2 step 786: training accuarcy: 0.612\n",
      "Epoch 2 step 786: training loss: 1329.3963587924893\n",
      "Epoch 2 step 787: training accuarcy: 0.605\n",
      "Epoch 2 step 787: training loss: 1340.1525507827143\n",
      "Epoch 2 step 788: training accuarcy: 0.61\n",
      "Epoch 2 step 788: training loss: 519.907379523933\n",
      "Epoch 2 step 789: training accuarcy: 0.632051282051282\n",
      "Epoch 2: train loss 1333.668191170169, train accuarcy 0.6131253242492676\n",
      "Epoch 2: valid loss 6695.52100004064, valid accuarcy 0.58403080701828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 3/5 [14:35<09:43, 291.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 1342.7223250854433\n",
      "Epoch 3 step 790: training accuarcy: 0.6065\n",
      "Epoch 3 step 790: training loss: 1348.2753128382176\n",
      "Epoch 3 step 791: training accuarcy: 0.601\n",
      "Epoch 3 step 791: training loss: 1325.9128978801375\n",
      "Epoch 3 step 792: training accuarcy: 0.626\n",
      "Epoch 3 step 792: training loss: 1325.1100745412507\n",
      "Epoch 3 step 793: training accuarcy: 0.609\n",
      "Epoch 3 step 793: training loss: 1334.997346441092\n",
      "Epoch 3 step 794: training accuarcy: 0.6065\n",
      "Epoch 3 step 794: training loss: 1335.137870185958\n",
      "Epoch 3 step 795: training accuarcy: 0.618\n",
      "Epoch 3 step 795: training loss: 1345.502700573903\n",
      "Epoch 3 step 796: training accuarcy: 0.6155\n",
      "Epoch 3 step 796: training loss: 1331.888842575172\n",
      "Epoch 3 step 797: training accuarcy: 0.63\n",
      "Epoch 3 step 797: training loss: 1333.0495507936994\n",
      "Epoch 3 step 798: training accuarcy: 0.621\n",
      "Epoch 3 step 798: training loss: 1338.5397983572188\n",
      "Epoch 3 step 799: training accuarcy: 0.609\n",
      "Epoch 3 step 799: training loss: 1329.0621546381676\n",
      "Epoch 3 step 800: training accuarcy: 0.629\n",
      "Epoch 3 step 800: training loss: 1342.586791983704\n",
      "Epoch 3 step 801: training accuarcy: 0.613\n",
      "Epoch 3 step 801: training loss: 1333.2695099870186\n",
      "Epoch 3 step 802: training accuarcy: 0.623\n",
      "Epoch 3 step 802: training loss: 1320.436228320551\n",
      "Epoch 3 step 803: training accuarcy: 0.63\n",
      "Epoch 3 step 803: training loss: 1332.616427503324\n",
      "Epoch 3 step 804: training accuarcy: 0.614\n",
      "Epoch 3 step 804: training loss: 1328.1116619533486\n",
      "Epoch 3 step 805: training accuarcy: 0.6145\n",
      "Epoch 3 step 805: training loss: 1338.0642222331408\n",
      "Epoch 3 step 806: training accuarcy: 0.623\n",
      "Epoch 3 step 806: training loss: 1338.653546946003\n",
      "Epoch 3 step 807: training accuarcy: 0.6025\n",
      "Epoch 3 step 807: training loss: 1326.0768499963144\n",
      "Epoch 3 step 808: training accuarcy: 0.621\n",
      "Epoch 3 step 808: training loss: 1329.8308353844527\n",
      "Epoch 3 step 809: training accuarcy: 0.627\n",
      "Epoch 3 step 809: training loss: 1333.8677960763541\n",
      "Epoch 3 step 810: training accuarcy: 0.626\n",
      "Epoch 3 step 810: training loss: 1347.6301750399452\n",
      "Epoch 3 step 811: training accuarcy: 0.5995\n",
      "Epoch 3 step 811: training loss: 1319.4478471799264\n",
      "Epoch 3 step 812: training accuarcy: 0.641\n",
      "Epoch 3 step 812: training loss: 1329.147013810181\n",
      "Epoch 3 step 813: training accuarcy: 0.6135\n",
      "Epoch 3 step 813: training loss: 1328.5602116654152\n",
      "Epoch 3 step 814: training accuarcy: 0.6245\n",
      "Epoch 3 step 814: training loss: 1335.9284074203445\n",
      "Epoch 3 step 815: training accuarcy: 0.616\n",
      "Epoch 3 step 815: training loss: 1320.3809840936337\n",
      "Epoch 3 step 816: training accuarcy: 0.633\n",
      "Epoch 3 step 816: training loss: 1331.4845118658586\n",
      "Epoch 3 step 817: training accuarcy: 0.6135\n",
      "Epoch 3 step 817: training loss: 1330.6311023261244\n",
      "Epoch 3 step 818: training accuarcy: 0.611\n",
      "Epoch 3 step 818: training loss: 1335.9672260694633\n",
      "Epoch 3 step 819: training accuarcy: 0.608\n",
      "Epoch 3 step 819: training loss: 1342.8117407721293\n",
      "Epoch 3 step 820: training accuarcy: 0.6025\n",
      "Epoch 3 step 820: training loss: 1330.897978953006\n",
      "Epoch 3 step 821: training accuarcy: 0.6215\n",
      "Epoch 3 step 821: training loss: 1322.597208813226\n",
      "Epoch 3 step 822: training accuarcy: 0.6245\n",
      "Epoch 3 step 822: training loss: 1324.1448602846822\n",
      "Epoch 3 step 823: training accuarcy: 0.625\n",
      "Epoch 3 step 823: training loss: 1350.2211038912847\n",
      "Epoch 3 step 824: training accuarcy: 0.6065\n",
      "Epoch 3 step 824: training loss: 1315.6927768290345\n",
      "Epoch 3 step 825: training accuarcy: 0.6385000000000001\n",
      "Epoch 3 step 825: training loss: 1332.6417550312146\n",
      "Epoch 3 step 826: training accuarcy: 0.63\n",
      "Epoch 3 step 826: training loss: 1341.2464623106846\n",
      "Epoch 3 step 827: training accuarcy: 0.5965\n",
      "Epoch 3 step 827: training loss: 1346.4321456224227\n",
      "Epoch 3 step 828: training accuarcy: 0.616\n",
      "Epoch 3 step 828: training loss: 1349.0536230185944\n",
      "Epoch 3 step 829: training accuarcy: 0.609\n",
      "Epoch 3 step 829: training loss: 1321.8356185485775\n",
      "Epoch 3 step 830: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 830: training loss: 1339.5611658074224\n",
      "Epoch 3 step 831: training accuarcy: 0.6085\n",
      "Epoch 3 step 831: training loss: 1341.6443663262917\n",
      "Epoch 3 step 832: training accuarcy: 0.6265000000000001\n",
      "Epoch 3 step 832: training loss: 1328.7891178793886\n",
      "Epoch 3 step 833: training accuarcy: 0.616\n",
      "Epoch 3 step 833: training loss: 1329.7966332361555\n",
      "Epoch 3 step 834: training accuarcy: 0.606\n",
      "Epoch 3 step 834: training loss: 1334.4614833433511\n",
      "Epoch 3 step 835: training accuarcy: 0.627\n",
      "Epoch 3 step 835: training loss: 1329.9643066056065\n",
      "Epoch 3 step 836: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 836: training loss: 1334.6397702427278\n",
      "Epoch 3 step 837: training accuarcy: 0.6235\n",
      "Epoch 3 step 837: training loss: 1310.482046850451\n",
      "Epoch 3 step 838: training accuarcy: 0.6455\n",
      "Epoch 3 step 838: training loss: 1337.4480795836218\n",
      "Epoch 3 step 839: training accuarcy: 0.611\n",
      "Epoch 3 step 839: training loss: 1325.5266343218848\n",
      "Epoch 3 step 840: training accuarcy: 0.6145\n",
      "Epoch 3 step 840: training loss: 1336.2637705165598\n",
      "Epoch 3 step 841: training accuarcy: 0.626\n",
      "Epoch 3 step 841: training loss: 1333.8874268628708\n",
      "Epoch 3 step 842: training accuarcy: 0.6195\n",
      "Epoch 3 step 842: training loss: 1336.6387054001639\n",
      "Epoch 3 step 843: training accuarcy: 0.61\n",
      "Epoch 3 step 843: training loss: 1339.6313264034316\n",
      "Epoch 3 step 844: training accuarcy: 0.611\n",
      "Epoch 3 step 844: training loss: 1355.357095584175\n",
      "Epoch 3 step 845: training accuarcy: 0.6155\n",
      "Epoch 3 step 845: training loss: 1343.1725245814118\n",
      "Epoch 3 step 846: training accuarcy: 0.6065\n",
      "Epoch 3 step 846: training loss: 1325.0443952976173\n",
      "Epoch 3 step 847: training accuarcy: 0.6235\n",
      "Epoch 3 step 847: training loss: 1319.2305993133746\n",
      "Epoch 3 step 848: training accuarcy: 0.628\n",
      "Epoch 3 step 848: training loss: 1342.717597780566\n",
      "Epoch 3 step 849: training accuarcy: 0.6165\n",
      "Epoch 3 step 849: training loss: 1336.2430157806566\n",
      "Epoch 3 step 850: training accuarcy: 0.613\n",
      "Epoch 3 step 850: training loss: 1340.8238592054267\n",
      "Epoch 3 step 851: training accuarcy: 0.6055\n",
      "Epoch 3 step 851: training loss: 1338.2747986786449\n",
      "Epoch 3 step 852: training accuarcy: 0.6215\n",
      "Epoch 3 step 852: training loss: 1340.2192987102032\n",
      "Epoch 3 step 853: training accuarcy: 0.609\n",
      "Epoch 3 step 853: training loss: 1339.196737128438\n",
      "Epoch 3 step 854: training accuarcy: 0.601\n",
      "Epoch 3 step 854: training loss: 1345.1739395774778\n",
      "Epoch 3 step 855: training accuarcy: 0.5895\n",
      "Epoch 3 step 855: training loss: 1335.3053365453081\n",
      "Epoch 3 step 856: training accuarcy: 0.623\n",
      "Epoch 3 step 856: training loss: 1334.8287230006347\n",
      "Epoch 3 step 857: training accuarcy: 0.6165\n",
      "Epoch 3 step 857: training loss: 1338.8491795865282\n",
      "Epoch 3 step 858: training accuarcy: 0.615\n",
      "Epoch 3 step 858: training loss: 1306.2436909606085\n",
      "Epoch 3 step 859: training accuarcy: 0.6265000000000001\n",
      "Epoch 3 step 859: training loss: 1327.2305038588117\n",
      "Epoch 3 step 860: training accuarcy: 0.613\n",
      "Epoch 3 step 860: training loss: 1338.4449934180398\n",
      "Epoch 3 step 861: training accuarcy: 0.612\n",
      "Epoch 3 step 861: training loss: 1340.2482080300167\n",
      "Epoch 3 step 862: training accuarcy: 0.5915\n",
      "Epoch 3 step 862: training loss: 1331.4795315823594\n",
      "Epoch 3 step 863: training accuarcy: 0.608\n",
      "Epoch 3 step 863: training loss: 1339.6098980824008\n",
      "Epoch 3 step 864: training accuarcy: 0.606\n",
      "Epoch 3 step 864: training loss: 1327.2859170723993\n",
      "Epoch 3 step 865: training accuarcy: 0.6165\n",
      "Epoch 3 step 865: training loss: 1348.344444521254\n",
      "Epoch 3 step 866: training accuarcy: 0.598\n",
      "Epoch 3 step 866: training loss: 1330.7433739078112\n",
      "Epoch 3 step 867: training accuarcy: 0.622\n",
      "Epoch 3 step 867: training loss: 1335.758556762399\n",
      "Epoch 3 step 868: training accuarcy: 0.611\n",
      "Epoch 3 step 868: training loss: 1334.749317376725\n",
      "Epoch 3 step 869: training accuarcy: 0.6175\n",
      "Epoch 3 step 869: training loss: 1333.4589611998335\n",
      "Epoch 3 step 870: training accuarcy: 0.6105\n",
      "Epoch 3 step 870: training loss: 1321.1606298362121\n",
      "Epoch 3 step 871: training accuarcy: 0.6245\n",
      "Epoch 3 step 871: training loss: 1339.3995276546334\n",
      "Epoch 3 step 872: training accuarcy: 0.606\n",
      "Epoch 3 step 872: training loss: 1353.038845599178\n",
      "Epoch 3 step 873: training accuarcy: 0.595\n",
      "Epoch 3 step 873: training loss: 1333.3867901695137\n",
      "Epoch 3 step 874: training accuarcy: 0.614\n",
      "Epoch 3 step 874: training loss: 1332.5014614667377\n",
      "Epoch 3 step 875: training accuarcy: 0.624\n",
      "Epoch 3 step 875: training loss: 1331.8636844389882\n",
      "Epoch 3 step 876: training accuarcy: 0.6175\n",
      "Epoch 3 step 876: training loss: 1326.461346208732\n",
      "Epoch 3 step 877: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 877: training loss: 1344.4736444383257\n",
      "Epoch 3 step 878: training accuarcy: 0.613\n",
      "Epoch 3 step 878: training loss: 1345.5123907970565\n",
      "Epoch 3 step 879: training accuarcy: 0.598\n",
      "Epoch 3 step 879: training loss: 1336.1296535955717\n",
      "Epoch 3 step 880: training accuarcy: 0.621\n",
      "Epoch 3 step 880: training loss: 1338.4574980755247\n",
      "Epoch 3 step 881: training accuarcy: 0.609\n",
      "Epoch 3 step 881: training loss: 1341.8929275942528\n",
      "Epoch 3 step 882: training accuarcy: 0.6115\n",
      "Epoch 3 step 882: training loss: 1329.09612304164\n",
      "Epoch 3 step 883: training accuarcy: 0.616\n",
      "Epoch 3 step 883: training loss: 1326.9607673367182\n",
      "Epoch 3 step 884: training accuarcy: 0.6235\n",
      "Epoch 3 step 884: training loss: 1354.5732268050356\n",
      "Epoch 3 step 885: training accuarcy: 0.5985\n",
      "Epoch 3 step 885: training loss: 1330.78734849384\n",
      "Epoch 3 step 886: training accuarcy: 0.608\n",
      "Epoch 3 step 886: training loss: 1350.967985520336\n",
      "Epoch 3 step 887: training accuarcy: 0.595\n",
      "Epoch 3 step 887: training loss: 1328.6102759677997\n",
      "Epoch 3 step 888: training accuarcy: 0.618\n",
      "Epoch 3 step 888: training loss: 1329.7853885263287\n",
      "Epoch 3 step 889: training accuarcy: 0.6095\n",
      "Epoch 3 step 889: training loss: 1342.2882229540683\n",
      "Epoch 3 step 890: training accuarcy: 0.6155\n",
      "Epoch 3 step 890: training loss: 1328.7701974839244\n",
      "Epoch 3 step 891: training accuarcy: 0.624\n",
      "Epoch 3 step 891: training loss: 1330.1794495655683\n",
      "Epoch 3 step 892: training accuarcy: 0.622\n",
      "Epoch 3 step 892: training loss: 1334.5835784265594\n",
      "Epoch 3 step 893: training accuarcy: 0.6045\n",
      "Epoch 3 step 893: training loss: 1326.98691798436\n",
      "Epoch 3 step 894: training accuarcy: 0.616\n",
      "Epoch 3 step 894: training loss: 1337.031537424937\n",
      "Epoch 3 step 895: training accuarcy: 0.611\n",
      "Epoch 3 step 895: training loss: 1347.9188429784429\n",
      "Epoch 3 step 896: training accuarcy: 0.61\n",
      "Epoch 3 step 896: training loss: 1331.8281028857853\n",
      "Epoch 3 step 897: training accuarcy: 0.608\n",
      "Epoch 3 step 897: training loss: 1340.980056411304\n",
      "Epoch 3 step 898: training accuarcy: 0.607\n",
      "Epoch 3 step 898: training loss: 1328.866559609571\n",
      "Epoch 3 step 899: training accuarcy: 0.611\n",
      "Epoch 3 step 899: training loss: 1332.2127881396252\n",
      "Epoch 3 step 900: training accuarcy: 0.6125\n",
      "Epoch 3 step 900: training loss: 1329.5827485174002\n",
      "Epoch 3 step 901: training accuarcy: 0.614\n",
      "Epoch 3 step 901: training loss: 1332.9425726356678\n",
      "Epoch 3 step 902: training accuarcy: 0.626\n",
      "Epoch 3 step 902: training loss: 1345.7294141742016\n",
      "Epoch 3 step 903: training accuarcy: 0.6\n",
      "Epoch 3 step 903: training loss: 1332.500070300963\n",
      "Epoch 3 step 904: training accuarcy: 0.616\n",
      "Epoch 3 step 904: training loss: 1333.2458064995901\n",
      "Epoch 3 step 905: training accuarcy: 0.615\n",
      "Epoch 3 step 905: training loss: 1326.1030352109738\n",
      "Epoch 3 step 906: training accuarcy: 0.62\n",
      "Epoch 3 step 906: training loss: 1320.5878109984537\n",
      "Epoch 3 step 907: training accuarcy: 0.625\n",
      "Epoch 3 step 907: training loss: 1335.6765564536831\n",
      "Epoch 3 step 908: training accuarcy: 0.6215\n",
      "Epoch 3 step 908: training loss: 1349.8806972616655\n",
      "Epoch 3 step 909: training accuarcy: 0.6005\n",
      "Epoch 3 step 909: training loss: 1327.5344735638253\n",
      "Epoch 3 step 910: training accuarcy: 0.6185\n",
      "Epoch 3 step 910: training loss: 1332.3563256953064\n",
      "Epoch 3 step 911: training accuarcy: 0.62\n",
      "Epoch 3 step 911: training loss: 1342.2522837652605\n",
      "Epoch 3 step 912: training accuarcy: 0.6085\n",
      "Epoch 3 step 912: training loss: 1331.6213692269255\n",
      "Epoch 3 step 913: training accuarcy: 0.612\n",
      "Epoch 3 step 913: training loss: 1315.6799540117913\n",
      "Epoch 3 step 914: training accuarcy: 0.6235\n",
      "Epoch 3 step 914: training loss: 1338.4051224636871\n",
      "Epoch 3 step 915: training accuarcy: 0.612\n",
      "Epoch 3 step 915: training loss: 1337.885585165443\n",
      "Epoch 3 step 916: training accuarcy: 0.6075\n",
      "Epoch 3 step 916: training loss: 1331.8100533368324\n",
      "Epoch 3 step 917: training accuarcy: 0.6195\n",
      "Epoch 3 step 917: training loss: 1338.1078345793192\n",
      "Epoch 3 step 918: training accuarcy: 0.622\n",
      "Epoch 3 step 918: training loss: 1335.0003443807298\n",
      "Epoch 3 step 919: training accuarcy: 0.616\n",
      "Epoch 3 step 919: training loss: 1316.0623905242164\n",
      "Epoch 3 step 920: training accuarcy: 0.6245\n",
      "Epoch 3 step 920: training loss: 1335.8140469638663\n",
      "Epoch 3 step 921: training accuarcy: 0.6195\n",
      "Epoch 3 step 921: training loss: 1327.7931817216297\n",
      "Epoch 3 step 922: training accuarcy: 0.6325000000000001\n",
      "Epoch 3 step 922: training loss: 1323.0002022521442\n",
      "Epoch 3 step 923: training accuarcy: 0.6155\n",
      "Epoch 3 step 923: training loss: 1343.3286935251897\n",
      "Epoch 3 step 924: training accuarcy: 0.6145\n",
      "Epoch 3 step 924: training loss: 1334.0282902468173\n",
      "Epoch 3 step 925: training accuarcy: 0.614\n",
      "Epoch 3 step 925: training loss: 1338.9563097612765\n",
      "Epoch 3 step 926: training accuarcy: 0.6155\n",
      "Epoch 3 step 926: training loss: 1329.5879235435052\n",
      "Epoch 3 step 927: training accuarcy: 0.626\n",
      "Epoch 3 step 927: training loss: 1340.8584043313156\n",
      "Epoch 3 step 928: training accuarcy: 0.607\n",
      "Epoch 3 step 928: training loss: 1335.0532168853267\n",
      "Epoch 3 step 929: training accuarcy: 0.614\n",
      "Epoch 3 step 929: training loss: 1334.1574201108847\n",
      "Epoch 3 step 930: training accuarcy: 0.62\n",
      "Epoch 3 step 930: training loss: 1333.8529148939172\n",
      "Epoch 3 step 931: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 931: training loss: 1346.2299508257597\n",
      "Epoch 3 step 932: training accuarcy: 0.6035\n",
      "Epoch 3 step 932: training loss: 1337.7520972354207\n",
      "Epoch 3 step 933: training accuarcy: 0.616\n",
      "Epoch 3 step 933: training loss: 1332.912726978296\n",
      "Epoch 3 step 934: training accuarcy: 0.621\n",
      "Epoch 3 step 934: training loss: 1339.5066395871227\n",
      "Epoch 3 step 935: training accuarcy: 0.5935\n",
      "Epoch 3 step 935: training loss: 1331.8813918369995\n",
      "Epoch 3 step 936: training accuarcy: 0.6145\n",
      "Epoch 3 step 936: training loss: 1343.5627480294474\n",
      "Epoch 3 step 937: training accuarcy: 0.6005\n",
      "Epoch 3 step 937: training loss: 1328.511079903063\n",
      "Epoch 3 step 938: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 938: training loss: 1339.7003925152223\n",
      "Epoch 3 step 939: training accuarcy: 0.613\n",
      "Epoch 3 step 939: training loss: 1331.5134295762707\n",
      "Epoch 3 step 940: training accuarcy: 0.609\n",
      "Epoch 3 step 940: training loss: 1324.0678650136003\n",
      "Epoch 3 step 941: training accuarcy: 0.6175\n",
      "Epoch 3 step 941: training loss: 1330.42611975903\n",
      "Epoch 3 step 942: training accuarcy: 0.6205\n",
      "Epoch 3 step 942: training loss: 1335.7196507120364\n",
      "Epoch 3 step 943: training accuarcy: 0.606\n",
      "Epoch 3 step 943: training loss: 1329.910122948634\n",
      "Epoch 3 step 944: training accuarcy: 0.625\n",
      "Epoch 3 step 944: training loss: 1331.4963225303977\n",
      "Epoch 3 step 945: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 945: training loss: 1348.859719120271\n",
      "Epoch 3 step 946: training accuarcy: 0.597\n",
      "Epoch 3 step 946: training loss: 1339.6003380412903\n",
      "Epoch 3 step 947: training accuarcy: 0.6065\n",
      "Epoch 3 step 947: training loss: 1332.2722584488938\n",
      "Epoch 3 step 948: training accuarcy: 0.6025\n",
      "Epoch 3 step 948: training loss: 1330.840899002748\n",
      "Epoch 3 step 949: training accuarcy: 0.6225\n",
      "Epoch 3 step 949: training loss: 1326.6901809274236\n",
      "Epoch 3 step 950: training accuarcy: 0.603\n",
      "Epoch 3 step 950: training loss: 1340.3793794318678\n",
      "Epoch 3 step 951: training accuarcy: 0.612\n",
      "Epoch 3 step 951: training loss: 1330.2368822887518\n",
      "Epoch 3 step 952: training accuarcy: 0.603\n",
      "Epoch 3 step 952: training loss: 1336.988516998073\n",
      "Epoch 3 step 953: training accuarcy: 0.597\n",
      "Epoch 3 step 953: training loss: 1339.4761416199387\n",
      "Epoch 3 step 954: training accuarcy: 0.62\n",
      "Epoch 3 step 954: training loss: 1327.5340692231912\n",
      "Epoch 3 step 955: training accuarcy: 0.625\n",
      "Epoch 3 step 955: training loss: 1326.3651620493904\n",
      "Epoch 3 step 956: training accuarcy: 0.6285000000000001\n",
      "Epoch 3 step 956: training loss: 1327.2122441781466\n",
      "Epoch 3 step 957: training accuarcy: 0.616\n",
      "Epoch 3 step 957: training loss: 1348.6095116991755\n",
      "Epoch 3 step 958: training accuarcy: 0.5955\n",
      "Epoch 3 step 958: training loss: 1344.5695851896294\n",
      "Epoch 3 step 959: training accuarcy: 0.6105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 959: training loss: 1322.6312451263373\n",
      "Epoch 3 step 960: training accuarcy: 0.6365000000000001\n",
      "Epoch 3 step 960: training loss: 1331.308937026565\n",
      "Epoch 3 step 961: training accuarcy: 0.625\n",
      "Epoch 3 step 961: training loss: 1329.6602580650076\n",
      "Epoch 3 step 962: training accuarcy: 0.6075\n",
      "Epoch 3 step 962: training loss: 1337.9052071951016\n",
      "Epoch 3 step 963: training accuarcy: 0.609\n",
      "Epoch 3 step 963: training loss: 1345.7294448084106\n",
      "Epoch 3 step 964: training accuarcy: 0.603\n",
      "Epoch 3 step 964: training loss: 1333.9702016634976\n",
      "Epoch 3 step 965: training accuarcy: 0.6155\n",
      "Epoch 3 step 965: training loss: 1334.6999319372296\n",
      "Epoch 3 step 966: training accuarcy: 0.6125\n",
      "Epoch 3 step 966: training loss: 1337.7582848166253\n",
      "Epoch 3 step 967: training accuarcy: 0.619\n",
      "Epoch 3 step 967: training loss: 1348.795168443645\n",
      "Epoch 3 step 968: training accuarcy: 0.6025\n",
      "Epoch 3 step 968: training loss: 1324.883135915111\n",
      "Epoch 3 step 969: training accuarcy: 0.6205\n",
      "Epoch 3 step 969: training loss: 1321.01513847446\n",
      "Epoch 3 step 970: training accuarcy: 0.6245\n",
      "Epoch 3 step 970: training loss: 1322.1119410347242\n",
      "Epoch 3 step 971: training accuarcy: 0.6205\n",
      "Epoch 3 step 971: training loss: 1340.9250552395627\n",
      "Epoch 3 step 972: training accuarcy: 0.614\n",
      "Epoch 3 step 972: training loss: 1338.2289234525756\n",
      "Epoch 3 step 973: training accuarcy: 0.6105\n",
      "Epoch 3 step 973: training loss: 1328.7663331025058\n",
      "Epoch 3 step 974: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 974: training loss: 1347.6022867044303\n",
      "Epoch 3 step 975: training accuarcy: 0.6125\n",
      "Epoch 3 step 975: training loss: 1332.0443476443902\n",
      "Epoch 3 step 976: training accuarcy: 0.625\n",
      "Epoch 3 step 976: training loss: 1334.9393926850655\n",
      "Epoch 3 step 977: training accuarcy: 0.636\n",
      "Epoch 3 step 977: training loss: 1321.3403610194841\n",
      "Epoch 3 step 978: training accuarcy: 0.642\n",
      "Epoch 3 step 978: training loss: 1341.692094456906\n",
      "Epoch 3 step 979: training accuarcy: 0.6045\n",
      "Epoch 3 step 979: training loss: 1306.395476482991\n",
      "Epoch 3 step 980: training accuarcy: 0.632\n",
      "Epoch 3 step 980: training loss: 1317.870319750955\n",
      "Epoch 3 step 981: training accuarcy: 0.634\n",
      "Epoch 3 step 981: training loss: 1341.9026105508863\n",
      "Epoch 3 step 982: training accuarcy: 0.608\n",
      "Epoch 3 step 982: training loss: 1332.9591187058445\n",
      "Epoch 3 step 983: training accuarcy: 0.6175\n",
      "Epoch 3 step 983: training loss: 1323.506900549186\n",
      "Epoch 3 step 984: training accuarcy: 0.623\n",
      "Epoch 3 step 984: training loss: 1321.8215623455933\n",
      "Epoch 3 step 985: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 985: training loss: 1323.3236533057516\n",
      "Epoch 3 step 986: training accuarcy: 0.6295000000000001\n",
      "Epoch 3 step 986: training loss: 1315.4361920850893\n",
      "Epoch 3 step 987: training accuarcy: 0.6375000000000001\n",
      "Epoch 3 step 987: training loss: 1334.632883698833\n",
      "Epoch 3 step 988: training accuarcy: 0.6155\n",
      "Epoch 3 step 988: training loss: 1318.2697238815567\n",
      "Epoch 3 step 989: training accuarcy: 0.628\n",
      "Epoch 3 step 989: training loss: 1341.9311428748713\n",
      "Epoch 3 step 990: training accuarcy: 0.6055\n",
      "Epoch 3 step 990: training loss: 1333.016301467439\n",
      "Epoch 3 step 991: training accuarcy: 0.6235\n",
      "Epoch 3 step 991: training loss: 1324.419821704246\n",
      "Epoch 3 step 992: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 992: training loss: 1330.3150226595487\n",
      "Epoch 3 step 993: training accuarcy: 0.611\n",
      "Epoch 3 step 993: training loss: 1324.77873701223\n",
      "Epoch 3 step 994: training accuarcy: 0.618\n",
      "Epoch 3 step 994: training loss: 1333.0218181491666\n",
      "Epoch 3 step 995: training accuarcy: 0.607\n",
      "Epoch 3 step 995: training loss: 1349.5437297858157\n",
      "Epoch 3 step 996: training accuarcy: 0.585\n",
      "Epoch 3 step 996: training loss: 1342.9477803059108\n",
      "Epoch 3 step 997: training accuarcy: 0.619\n",
      "Epoch 3 step 997: training loss: 1328.7680345311685\n",
      "Epoch 3 step 998: training accuarcy: 0.623\n",
      "Epoch 3 step 998: training loss: 1323.8956052831938\n",
      "Epoch 3 step 999: training accuarcy: 0.62\n",
      "Epoch 3 step 999: training loss: 1343.8145895838118\n",
      "Epoch 3 step 1000: training accuarcy: 0.603\n",
      "Epoch 3 step 1000: training loss: 1346.4495079925298\n",
      "Epoch 3 step 1001: training accuarcy: 0.5975\n",
      "Epoch 3 step 1001: training loss: 1341.052215356606\n",
      "Epoch 3 step 1002: training accuarcy: 0.6165\n",
      "Epoch 3 step 1002: training loss: 1326.938130961759\n",
      "Epoch 3 step 1003: training accuarcy: 0.6215\n",
      "Epoch 3 step 1003: training loss: 1328.3115569659055\n",
      "Epoch 3 step 1004: training accuarcy: 0.621\n",
      "Epoch 3 step 1004: training loss: 1330.7637151615831\n",
      "Epoch 3 step 1005: training accuarcy: 0.611\n",
      "Epoch 3 step 1005: training loss: 1325.674360323981\n",
      "Epoch 3 step 1006: training accuarcy: 0.6205\n",
      "Epoch 3 step 1006: training loss: 1331.3360524023014\n",
      "Epoch 3 step 1007: training accuarcy: 0.631\n",
      "Epoch 3 step 1007: training loss: 1314.1166146518444\n",
      "Epoch 3 step 1008: training accuarcy: 0.6285000000000001\n",
      "Epoch 3 step 1008: training loss: 1340.181309304987\n",
      "Epoch 3 step 1009: training accuarcy: 0.614\n",
      "Epoch 3 step 1009: training loss: 1332.601069210394\n",
      "Epoch 3 step 1010: training accuarcy: 0.62\n",
      "Epoch 3 step 1010: training loss: 1338.7626716535976\n",
      "Epoch 3 step 1011: training accuarcy: 0.608\n",
      "Epoch 3 step 1011: training loss: 1338.2866207394227\n",
      "Epoch 3 step 1012: training accuarcy: 0.6195\n",
      "Epoch 3 step 1012: training loss: 1338.2584395739098\n",
      "Epoch 3 step 1013: training accuarcy: 0.613\n",
      "Epoch 3 step 1013: training loss: 1328.9507361777912\n",
      "Epoch 3 step 1014: training accuarcy: 0.627\n",
      "Epoch 3 step 1014: training loss: 1334.3428157179048\n",
      "Epoch 3 step 1015: training accuarcy: 0.618\n",
      "Epoch 3 step 1015: training loss: 1349.2559733956627\n",
      "Epoch 3 step 1016: training accuarcy: 0.604\n",
      "Epoch 3 step 1016: training loss: 1356.432021992906\n",
      "Epoch 3 step 1017: training accuarcy: 0.5985\n",
      "Epoch 3 step 1017: training loss: 1334.333423019699\n",
      "Epoch 3 step 1018: training accuarcy: 0.608\n",
      "Epoch 3 step 1018: training loss: 1336.081533875327\n",
      "Epoch 3 step 1019: training accuarcy: 0.6085\n",
      "Epoch 3 step 1019: training loss: 1339.0152801980016\n",
      "Epoch 3 step 1020: training accuarcy: 0.5995\n",
      "Epoch 3 step 1020: training loss: 1341.4868345925772\n",
      "Epoch 3 step 1021: training accuarcy: 0.6015\n",
      "Epoch 3 step 1021: training loss: 1335.5093361023828\n",
      "Epoch 3 step 1022: training accuarcy: 0.6095\n",
      "Epoch 3 step 1022: training loss: 1340.1924305452096\n",
      "Epoch 3 step 1023: training accuarcy: 0.62\n",
      "Epoch 3 step 1023: training loss: 1334.6068802001892\n",
      "Epoch 3 step 1024: training accuarcy: 0.6195\n",
      "Epoch 3 step 1024: training loss: 1327.1127087851376\n",
      "Epoch 3 step 1025: training accuarcy: 0.6235\n",
      "Epoch 3 step 1025: training loss: 1334.277668911972\n",
      "Epoch 3 step 1026: training accuarcy: 0.6115\n",
      "Epoch 3 step 1026: training loss: 1324.8203435854523\n",
      "Epoch 3 step 1027: training accuarcy: 0.6215\n",
      "Epoch 3 step 1027: training loss: 1330.1240165536237\n",
      "Epoch 3 step 1028: training accuarcy: 0.622\n",
      "Epoch 3 step 1028: training loss: 1325.5148164437537\n",
      "Epoch 3 step 1029: training accuarcy: 0.619\n",
      "Epoch 3 step 1029: training loss: 1326.3026116789088\n",
      "Epoch 3 step 1030: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 1030: training loss: 1352.7306529835269\n",
      "Epoch 3 step 1031: training accuarcy: 0.608\n",
      "Epoch 3 step 1031: training loss: 1336.9330332108316\n",
      "Epoch 3 step 1032: training accuarcy: 0.613\n",
      "Epoch 3 step 1032: training loss: 1344.1623861910045\n",
      "Epoch 3 step 1033: training accuarcy: 0.596\n",
      "Epoch 3 step 1033: training loss: 1340.1938748534972\n",
      "Epoch 3 step 1034: training accuarcy: 0.614\n",
      "Epoch 3 step 1034: training loss: 1321.681367687425\n",
      "Epoch 3 step 1035: training accuarcy: 0.6335000000000001\n",
      "Epoch 3 step 1035: training loss: 1339.700666075434\n",
      "Epoch 3 step 1036: training accuarcy: 0.6055\n",
      "Epoch 3 step 1036: training loss: 1334.3196845416699\n",
      "Epoch 3 step 1037: training accuarcy: 0.604\n",
      "Epoch 3 step 1037: training loss: 1316.9243962782716\n",
      "Epoch 3 step 1038: training accuarcy: 0.6425\n",
      "Epoch 3 step 1038: training loss: 1341.3997054471763\n",
      "Epoch 3 step 1039: training accuarcy: 0.614\n",
      "Epoch 3 step 1039: training loss: 1343.7630104090524\n",
      "Epoch 3 step 1040: training accuarcy: 0.599\n",
      "Epoch 3 step 1040: training loss: 1308.7706245834647\n",
      "Epoch 3 step 1041: training accuarcy: 0.6575\n",
      "Epoch 3 step 1041: training loss: 1329.8528603759553\n",
      "Epoch 3 step 1042: training accuarcy: 0.6115\n",
      "Epoch 3 step 1042: training loss: 1314.7360347086612\n",
      "Epoch 3 step 1043: training accuarcy: 0.6295000000000001\n",
      "Epoch 3 step 1043: training loss: 1326.4104103883956\n",
      "Epoch 3 step 1044: training accuarcy: 0.6175\n",
      "Epoch 3 step 1044: training loss: 1341.4721870537662\n",
      "Epoch 3 step 1045: training accuarcy: 0.6125\n",
      "Epoch 3 step 1045: training loss: 1327.2631461530248\n",
      "Epoch 3 step 1046: training accuarcy: 0.618\n",
      "Epoch 3 step 1046: training loss: 1324.6820168907932\n",
      "Epoch 3 step 1047: training accuarcy: 0.6195\n",
      "Epoch 3 step 1047: training loss: 1345.3526276369914\n",
      "Epoch 3 step 1048: training accuarcy: 0.6095\n",
      "Epoch 3 step 1048: training loss: 1349.237086507501\n",
      "Epoch 3 step 1049: training accuarcy: 0.5985\n",
      "Epoch 3 step 1049: training loss: 1333.3469152119658\n",
      "Epoch 3 step 1050: training accuarcy: 0.6165\n",
      "Epoch 3 step 1050: training loss: 1318.5900704258434\n",
      "Epoch 3 step 1051: training accuarcy: 0.632\n",
      "Epoch 3 step 1051: training loss: 516.0956425270698\n",
      "Epoch 3 step 1052: training accuarcy: 0.6346153846153846\n",
      "Epoch 3: train loss 1330.6975280859638, train accuarcy 0.6144849061965942\n",
      "Epoch 3: valid loss 6692.007574695458, valid accuarcy 0.5864363312721252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 4/5 [19:25<04:51, 291.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 1317.8338241354895\n",
      "Epoch 4 step 1053: training accuarcy: 0.632\n",
      "Epoch 4 step 1053: training loss: 1337.295673641742\n",
      "Epoch 4 step 1054: training accuarcy: 0.617\n",
      "Epoch 4 step 1054: training loss: 1339.4580334156717\n",
      "Epoch 4 step 1055: training accuarcy: 0.6085\n",
      "Epoch 4 step 1055: training loss: 1335.1523034373586\n",
      "Epoch 4 step 1056: training accuarcy: 0.6165\n",
      "Epoch 4 step 1056: training loss: 1330.850338818631\n",
      "Epoch 4 step 1057: training accuarcy: 0.6245\n",
      "Epoch 4 step 1057: training loss: 1342.3026604404238\n",
      "Epoch 4 step 1058: training accuarcy: 0.6085\n",
      "Epoch 4 step 1058: training loss: 1320.9174572165612\n",
      "Epoch 4 step 1059: training accuarcy: 0.634\n",
      "Epoch 4 step 1059: training loss: 1335.5893587405053\n",
      "Epoch 4 step 1060: training accuarcy: 0.609\n",
      "Epoch 4 step 1060: training loss: 1336.6762282303\n",
      "Epoch 4 step 1061: training accuarcy: 0.6095\n",
      "Epoch 4 step 1061: training loss: 1329.878388692607\n",
      "Epoch 4 step 1062: training accuarcy: 0.6245\n",
      "Epoch 4 step 1062: training loss: 1331.1665521278462\n",
      "Epoch 4 step 1063: training accuarcy: 0.616\n",
      "Epoch 4 step 1063: training loss: 1311.6385664032052\n",
      "Epoch 4 step 1064: training accuarcy: 0.634\n",
      "Epoch 4 step 1064: training loss: 1338.1156208731402\n",
      "Epoch 4 step 1065: training accuarcy: 0.6045\n",
      "Epoch 4 step 1065: training loss: 1324.083015552482\n",
      "Epoch 4 step 1066: training accuarcy: 0.627\n",
      "Epoch 4 step 1066: training loss: 1340.7834182119682\n",
      "Epoch 4 step 1067: training accuarcy: 0.6085\n",
      "Epoch 4 step 1067: training loss: 1341.829027360962\n",
      "Epoch 4 step 1068: training accuarcy: 0.626\n",
      "Epoch 4 step 1068: training loss: 1328.3920687019106\n",
      "Epoch 4 step 1069: training accuarcy: 0.61\n",
      "Epoch 4 step 1069: training loss: 1330.3686671377532\n",
      "Epoch 4 step 1070: training accuarcy: 0.627\n",
      "Epoch 4 step 1070: training loss: 1356.1532164131036\n",
      "Epoch 4 step 1071: training accuarcy: 0.609\n",
      "Epoch 4 step 1071: training loss: 1339.7476530116337\n",
      "Epoch 4 step 1072: training accuarcy: 0.615\n",
      "Epoch 4 step 1072: training loss: 1332.9814750990442\n",
      "Epoch 4 step 1073: training accuarcy: 0.609\n",
      "Epoch 4 step 1073: training loss: 1341.6991831106711\n",
      "Epoch 4 step 1074: training accuarcy: 0.617\n",
      "Epoch 4 step 1074: training loss: 1338.1409201950783\n",
      "Epoch 4 step 1075: training accuarcy: 0.627\n",
      "Epoch 4 step 1075: training loss: 1329.1054487069712\n",
      "Epoch 4 step 1076: training accuarcy: 0.621\n",
      "Epoch 4 step 1076: training loss: 1321.821586824643\n",
      "Epoch 4 step 1077: training accuarcy: 0.635\n",
      "Epoch 4 step 1077: training loss: 1343.9971453351957\n",
      "Epoch 4 step 1078: training accuarcy: 0.6085\n",
      "Epoch 4 step 1078: training loss: 1331.9549761814892\n",
      "Epoch 4 step 1079: training accuarcy: 0.621\n",
      "Epoch 4 step 1079: training loss: 1332.502513540372\n",
      "Epoch 4 step 1080: training accuarcy: 0.635\n",
      "Epoch 4 step 1080: training loss: 1335.4666638550043\n",
      "Epoch 4 step 1081: training accuarcy: 0.607\n",
      "Epoch 4 step 1081: training loss: 1319.3248400641614\n",
      "Epoch 4 step 1082: training accuarcy: 0.6265000000000001\n",
      "Epoch 4 step 1082: training loss: 1351.9293612856948\n",
      "Epoch 4 step 1083: training accuarcy: 0.5935\n",
      "Epoch 4 step 1083: training loss: 1328.0153167240649\n",
      "Epoch 4 step 1084: training accuarcy: 0.6215\n",
      "Epoch 4 step 1084: training loss: 1348.006027243698\n",
      "Epoch 4 step 1085: training accuarcy: 0.6015\n",
      "Epoch 4 step 1085: training loss: 1343.422789350303\n",
      "Epoch 4 step 1086: training accuarcy: 0.6055\n",
      "Epoch 4 step 1086: training loss: 1336.2189463679604\n",
      "Epoch 4 step 1087: training accuarcy: 0.6035\n",
      "Epoch 4 step 1087: training loss: 1330.5598876512504\n",
      "Epoch 4 step 1088: training accuarcy: 0.6155\n",
      "Epoch 4 step 1088: training loss: 1332.9458103980608\n",
      "Epoch 4 step 1089: training accuarcy: 0.622\n",
      "Epoch 4 step 1089: training loss: 1348.1105644912752\n",
      "Epoch 4 step 1090: training accuarcy: 0.5955\n",
      "Epoch 4 step 1090: training loss: 1332.1801639423895\n",
      "Epoch 4 step 1091: training accuarcy: 0.615\n",
      "Epoch 4 step 1091: training loss: 1340.9319746020828\n",
      "Epoch 4 step 1092: training accuarcy: 0.609\n",
      "Epoch 4 step 1092: training loss: 1326.5417602996786\n",
      "Epoch 4 step 1093: training accuarcy: 0.6185\n",
      "Epoch 4 step 1093: training loss: 1335.0626586929259\n",
      "Epoch 4 step 1094: training accuarcy: 0.618\n",
      "Epoch 4 step 1094: training loss: 1331.205825356942\n",
      "Epoch 4 step 1095: training accuarcy: 0.6055\n",
      "Epoch 4 step 1095: training loss: 1336.6423840990303\n",
      "Epoch 4 step 1096: training accuarcy: 0.614\n",
      "Epoch 4 step 1096: training loss: 1349.7301772329902\n",
      "Epoch 4 step 1097: training accuarcy: 0.603\n",
      "Epoch 4 step 1097: training loss: 1343.470350035368\n",
      "Epoch 4 step 1098: training accuarcy: 0.6065\n",
      "Epoch 4 step 1098: training loss: 1316.1348847622116\n",
      "Epoch 4 step 1099: training accuarcy: 0.62\n",
      "Epoch 4 step 1099: training loss: 1340.3508351402781\n",
      "Epoch 4 step 1100: training accuarcy: 0.6215\n",
      "Epoch 4 step 1100: training loss: 1332.9521017620907\n",
      "Epoch 4 step 1101: training accuarcy: 0.6135\n",
      "Epoch 4 step 1101: training loss: 1337.7302733191732\n",
      "Epoch 4 step 1102: training accuarcy: 0.626\n",
      "Epoch 4 step 1102: training loss: 1335.3127209270003\n",
      "Epoch 4 step 1103: training accuarcy: 0.614\n",
      "Epoch 4 step 1103: training loss: 1312.224549076583\n",
      "Epoch 4 step 1104: training accuarcy: 0.6345000000000001\n",
      "Epoch 4 step 1104: training loss: 1337.91931440563\n",
      "Epoch 4 step 1105: training accuarcy: 0.618\n",
      "Epoch 4 step 1105: training loss: 1337.2035826261235\n",
      "Epoch 4 step 1106: training accuarcy: 0.625\n",
      "Epoch 4 step 1106: training loss: 1329.9901693032607\n",
      "Epoch 4 step 1107: training accuarcy: 0.6205\n",
      "Epoch 4 step 1107: training loss: 1322.5882176475438\n",
      "Epoch 4 step 1108: training accuarcy: 0.632\n",
      "Epoch 4 step 1108: training loss: 1336.4230019470244\n",
      "Epoch 4 step 1109: training accuarcy: 0.5975\n",
      "Epoch 4 step 1109: training loss: 1344.2517024426636\n",
      "Epoch 4 step 1110: training accuarcy: 0.613\n",
      "Epoch 4 step 1110: training loss: 1330.7903184813024\n",
      "Epoch 4 step 1111: training accuarcy: 0.6155\n",
      "Epoch 4 step 1111: training loss: 1333.6840181005362\n",
      "Epoch 4 step 1112: training accuarcy: 0.615\n",
      "Epoch 4 step 1112: training loss: 1338.205255738531\n",
      "Epoch 4 step 1113: training accuarcy: 0.615\n",
      "Epoch 4 step 1113: training loss: 1314.0673262757828\n",
      "Epoch 4 step 1114: training accuarcy: 0.6275000000000001\n",
      "Epoch 4 step 1114: training loss: 1328.3664792179084\n",
      "Epoch 4 step 1115: training accuarcy: 0.626\n",
      "Epoch 4 step 1115: training loss: 1333.3027625378872\n",
      "Epoch 4 step 1116: training accuarcy: 0.605\n",
      "Epoch 4 step 1116: training loss: 1347.1038845340363\n",
      "Epoch 4 step 1117: training accuarcy: 0.5985\n",
      "Epoch 4 step 1117: training loss: 1344.0284602216304\n",
      "Epoch 4 step 1118: training accuarcy: 0.6095\n",
      "Epoch 4 step 1118: training loss: 1327.597294727876\n",
      "Epoch 4 step 1119: training accuarcy: 0.6345000000000001\n",
      "Epoch 4 step 1119: training loss: 1346.2772125741599\n",
      "Epoch 4 step 1120: training accuarcy: 0.6215\n",
      "Epoch 4 step 1120: training loss: 1361.8836375729006\n",
      "Epoch 4 step 1121: training accuarcy: 0.6115\n",
      "Epoch 4 step 1121: training loss: 1336.7157527322584\n",
      "Epoch 4 step 1122: training accuarcy: 0.6045\n",
      "Epoch 4 step 1122: training loss: 1356.9976042099397\n",
      "Epoch 4 step 1123: training accuarcy: 0.5945\n",
      "Epoch 4 step 1123: training loss: 1335.4037675978398\n",
      "Epoch 4 step 1124: training accuarcy: 0.608\n",
      "Epoch 4 step 1124: training loss: 1336.9420777593582\n",
      "Epoch 4 step 1125: training accuarcy: 0.5995\n",
      "Epoch 4 step 1125: training loss: 1332.7037850123888\n",
      "Epoch 4 step 1126: training accuarcy: 0.626\n",
      "Epoch 4 step 1126: training loss: 1344.4870687282767\n",
      "Epoch 4 step 1127: training accuarcy: 0.614\n",
      "Epoch 4 step 1127: training loss: 1344.196737976049\n",
      "Epoch 4 step 1128: training accuarcy: 0.6005\n",
      "Epoch 4 step 1128: training loss: 1323.5241095508559\n",
      "Epoch 4 step 1129: training accuarcy: 0.623\n",
      "Epoch 4 step 1129: training loss: 1334.9918595333006\n",
      "Epoch 4 step 1130: training accuarcy: 0.612\n",
      "Epoch 4 step 1130: training loss: 1327.461889342167\n",
      "Epoch 4 step 1131: training accuarcy: 0.6145\n",
      "Epoch 4 step 1131: training loss: 1331.8929458546695\n",
      "Epoch 4 step 1132: training accuarcy: 0.622\n",
      "Epoch 4 step 1132: training loss: 1319.960170829172\n",
      "Epoch 4 step 1133: training accuarcy: 0.64\n",
      "Epoch 4 step 1133: training loss: 1343.9757420946312\n",
      "Epoch 4 step 1134: training accuarcy: 0.6115\n",
      "Epoch 4 step 1134: training loss: 1321.66470636906\n",
      "Epoch 4 step 1135: training accuarcy: 0.627\n",
      "Epoch 4 step 1135: training loss: 1326.864663280837\n",
      "Epoch 4 step 1136: training accuarcy: 0.6175\n",
      "Epoch 4 step 1136: training loss: 1337.8593805973799\n",
      "Epoch 4 step 1137: training accuarcy: 0.6095\n",
      "Epoch 4 step 1137: training loss: 1335.2832458935807\n",
      "Epoch 4 step 1138: training accuarcy: 0.6145\n",
      "Epoch 4 step 1138: training loss: 1321.3420189147687\n",
      "Epoch 4 step 1139: training accuarcy: 0.6255000000000001\n",
      "Epoch 4 step 1139: training loss: 1329.3433664817132\n",
      "Epoch 4 step 1140: training accuarcy: 0.6105\n",
      "Epoch 4 step 1140: training loss: 1340.1053719239417\n",
      "Epoch 4 step 1141: training accuarcy: 0.6105\n",
      "Epoch 4 step 1141: training loss: 1339.4811910016003\n",
      "Epoch 4 step 1142: training accuarcy: 0.604\n",
      "Epoch 4 step 1142: training loss: 1331.8549364542594\n",
      "Epoch 4 step 1143: training accuarcy: 0.6225\n",
      "Epoch 4 step 1143: training loss: 1338.69143968783\n",
      "Epoch 4 step 1144: training accuarcy: 0.614\n",
      "Epoch 4 step 1144: training loss: 1323.9233352441263\n",
      "Epoch 4 step 1145: training accuarcy: 0.6225\n",
      "Epoch 4 step 1145: training loss: 1334.407798070034\n",
      "Epoch 4 step 1146: training accuarcy: 0.6245\n",
      "Epoch 4 step 1146: training loss: 1323.441759097132\n",
      "Epoch 4 step 1147: training accuarcy: 0.6275000000000001\n",
      "Epoch 4 step 1147: training loss: 1326.803982718359\n",
      "Epoch 4 step 1148: training accuarcy: 0.611\n",
      "Epoch 4 step 1148: training loss: 1355.1960002940918\n",
      "Epoch 4 step 1149: training accuarcy: 0.607\n",
      "Epoch 4 step 1149: training loss: 1339.1149379616863\n",
      "Epoch 4 step 1150: training accuarcy: 0.6155\n",
      "Epoch 4 step 1150: training loss: 1326.2055683227165\n",
      "Epoch 4 step 1151: training accuarcy: 0.632\n",
      "Epoch 4 step 1151: training loss: 1338.195305769953\n",
      "Epoch 4 step 1152: training accuarcy: 0.63\n",
      "Epoch 4 step 1152: training loss: 1326.4207626513041\n",
      "Epoch 4 step 1153: training accuarcy: 0.6285000000000001\n",
      "Epoch 4 step 1153: training loss: 1340.842712985395\n",
      "Epoch 4 step 1154: training accuarcy: 0.615\n",
      "Epoch 4 step 1154: training loss: 1329.6999417780694\n",
      "Epoch 4 step 1155: training accuarcy: 0.621\n",
      "Epoch 4 step 1155: training loss: 1333.9354702875983\n",
      "Epoch 4 step 1156: training accuarcy: 0.619\n",
      "Epoch 4 step 1156: training loss: 1350.5340227028664\n",
      "Epoch 4 step 1157: training accuarcy: 0.5995\n",
      "Epoch 4 step 1157: training loss: 1336.0093755964024\n",
      "Epoch 4 step 1158: training accuarcy: 0.597\n",
      "Epoch 4 step 1158: training loss: 1323.055850875717\n",
      "Epoch 4 step 1159: training accuarcy: 0.624\n",
      "Epoch 4 step 1159: training loss: 1331.5874711611284\n",
      "Epoch 4 step 1160: training accuarcy: 0.6015\n",
      "Epoch 4 step 1160: training loss: 1328.9464089210103\n",
      "Epoch 4 step 1161: training accuarcy: 0.619\n",
      "Epoch 4 step 1161: training loss: 1336.8647605809804\n",
      "Epoch 4 step 1162: training accuarcy: 0.6135\n",
      "Epoch 4 step 1162: training loss: 1330.4069632988712\n",
      "Epoch 4 step 1163: training accuarcy: 0.6195\n",
      "Epoch 4 step 1163: training loss: 1327.8102067838722\n",
      "Epoch 4 step 1164: training accuarcy: 0.617\n",
      "Epoch 4 step 1164: training loss: 1337.3608667964431\n",
      "Epoch 4 step 1165: training accuarcy: 0.623\n",
      "Epoch 4 step 1165: training loss: 1320.5419971992292\n",
      "Epoch 4 step 1166: training accuarcy: 0.622\n",
      "Epoch 4 step 1166: training loss: 1319.9840422638176\n",
      "Epoch 4 step 1167: training accuarcy: 0.6305000000000001\n",
      "Epoch 4 step 1167: training loss: 1347.3446747218288\n",
      "Epoch 4 step 1168: training accuarcy: 0.599\n",
      "Epoch 4 step 1168: training loss: 1348.8022737548833\n",
      "Epoch 4 step 1169: training accuarcy: 0.601\n",
      "Epoch 4 step 1169: training loss: 1321.172323855289\n",
      "Epoch 4 step 1170: training accuarcy: 0.623\n",
      "Epoch 4 step 1170: training loss: 1323.2991560033547\n",
      "Epoch 4 step 1171: training accuarcy: 0.6315000000000001\n",
      "Epoch 4 step 1171: training loss: 1327.8353260898232\n",
      "Epoch 4 step 1172: training accuarcy: 0.6235\n",
      "Epoch 4 step 1172: training loss: 1335.9932676239002\n",
      "Epoch 4 step 1173: training accuarcy: 0.6165\n",
      "Epoch 4 step 1173: training loss: 1331.571644800777\n",
      "Epoch 4 step 1174: training accuarcy: 0.6185\n",
      "Epoch 4 step 1174: training loss: 1337.1146058939557\n",
      "Epoch 4 step 1175: training accuarcy: 0.613\n",
      "Epoch 4 step 1175: training loss: 1327.2570780746066\n",
      "Epoch 4 step 1176: training accuarcy: 0.6225\n",
      "Epoch 4 step 1176: training loss: 1326.667198746633\n",
      "Epoch 4 step 1177: training accuarcy: 0.614\n",
      "Epoch 4 step 1177: training loss: 1329.6177836298918\n",
      "Epoch 4 step 1178: training accuarcy: 0.6335000000000001\n",
      "Epoch 4 step 1178: training loss: 1331.4142538426609\n",
      "Epoch 4 step 1179: training accuarcy: 0.6205\n",
      "Epoch 4 step 1179: training loss: 1336.0686224088543\n",
      "Epoch 4 step 1180: training accuarcy: 0.6025\n",
      "Epoch 4 step 1180: training loss: 1331.463074209533\n",
      "Epoch 4 step 1181: training accuarcy: 0.6135\n",
      "Epoch 4 step 1181: training loss: 1324.1147536936444\n",
      "Epoch 4 step 1182: training accuarcy: 0.6245\n",
      "Epoch 4 step 1182: training loss: 1310.2918701379529\n",
      "Epoch 4 step 1183: training accuarcy: 0.635\n",
      "Epoch 4 step 1183: training loss: 1329.562743312428\n",
      "Epoch 4 step 1184: training accuarcy: 0.637\n",
      "Epoch 4 step 1184: training loss: 1335.9559909057562\n",
      "Epoch 4 step 1185: training accuarcy: 0.606\n",
      "Epoch 4 step 1185: training loss: 1322.955326674583\n",
      "Epoch 4 step 1186: training accuarcy: 0.617\n",
      "Epoch 4 step 1186: training loss: 1331.7887982489085\n",
      "Epoch 4 step 1187: training accuarcy: 0.622\n",
      "Epoch 4 step 1187: training loss: 1354.053185368178\n",
      "Epoch 4 step 1188: training accuarcy: 0.5935\n",
      "Epoch 4 step 1188: training loss: 1339.39293188118\n",
      "Epoch 4 step 1189: training accuarcy: 0.606\n",
      "Epoch 4 step 1189: training loss: 1327.97304929187\n",
      "Epoch 4 step 1190: training accuarcy: 0.613\n",
      "Epoch 4 step 1190: training loss: 1345.9609738758957\n",
      "Epoch 4 step 1191: training accuarcy: 0.6015\n",
      "Epoch 4 step 1191: training loss: 1345.1123301958016\n",
      "Epoch 4 step 1192: training accuarcy: 0.6135\n",
      "Epoch 4 step 1192: training loss: 1330.7331808798672\n",
      "Epoch 4 step 1193: training accuarcy: 0.6185\n",
      "Epoch 4 step 1193: training loss: 1339.8646900843123\n",
      "Epoch 4 step 1194: training accuarcy: 0.608\n",
      "Epoch 4 step 1194: training loss: 1337.8278447500688\n",
      "Epoch 4 step 1195: training accuarcy: 0.6095\n",
      "Epoch 4 step 1195: training loss: 1326.547658734999\n",
      "Epoch 4 step 1196: training accuarcy: 0.6155\n",
      "Epoch 4 step 1196: training loss: 1336.6519016942725\n",
      "Epoch 4 step 1197: training accuarcy: 0.6115\n",
      "Epoch 4 step 1197: training loss: 1329.3133291660254\n",
      "Epoch 4 step 1198: training accuarcy: 0.618\n",
      "Epoch 4 step 1198: training loss: 1326.8627008686806\n",
      "Epoch 4 step 1199: training accuarcy: 0.6125\n",
      "Epoch 4 step 1199: training loss: 1346.2162395939079\n",
      "Epoch 4 step 1200: training accuarcy: 0.6\n",
      "Epoch 4 step 1200: training loss: 1333.7983737614018\n",
      "Epoch 4 step 1201: training accuarcy: 0.61\n",
      "Epoch 4 step 1201: training loss: 1332.6740586171613\n",
      "Epoch 4 step 1202: training accuarcy: 0.604\n",
      "Epoch 4 step 1202: training loss: 1331.1103808576906\n",
      "Epoch 4 step 1203: training accuarcy: 0.6165\n",
      "Epoch 4 step 1203: training loss: 1328.358684822884\n",
      "Epoch 4 step 1204: training accuarcy: 0.6205\n",
      "Epoch 4 step 1204: training loss: 1338.6033866983282\n",
      "Epoch 4 step 1205: training accuarcy: 0.6105\n",
      "Epoch 4 step 1205: training loss: 1325.0113659172669\n",
      "Epoch 4 step 1206: training accuarcy: 0.6185\n",
      "Epoch 4 step 1206: training loss: 1337.1096156533188\n",
      "Epoch 4 step 1207: training accuarcy: 0.5965\n",
      "Epoch 4 step 1207: training loss: 1345.0474587121826\n",
      "Epoch 4 step 1208: training accuarcy: 0.602\n",
      "Epoch 4 step 1208: training loss: 1344.9583893813974\n",
      "Epoch 4 step 1209: training accuarcy: 0.617\n",
      "Epoch 4 step 1209: training loss: 1338.6718543878396\n",
      "Epoch 4 step 1210: training accuarcy: 0.6005\n",
      "Epoch 4 step 1210: training loss: 1330.0670196058572\n",
      "Epoch 4 step 1211: training accuarcy: 0.615\n",
      "Epoch 4 step 1211: training loss: 1323.0200587855138\n",
      "Epoch 4 step 1212: training accuarcy: 0.611\n",
      "Epoch 4 step 1212: training loss: 1336.5299060187601\n",
      "Epoch 4 step 1213: training accuarcy: 0.5965\n",
      "Epoch 4 step 1213: training loss: 1331.1267981888286\n",
      "Epoch 4 step 1214: training accuarcy: 0.6255000000000001\n",
      "Epoch 4 step 1214: training loss: 1333.436776818916\n",
      "Epoch 4 step 1215: training accuarcy: 0.6235\n",
      "Epoch 4 step 1215: training loss: 1328.7982349273834\n",
      "Epoch 4 step 1216: training accuarcy: 0.6365000000000001\n",
      "Epoch 4 step 1216: training loss: 1330.9504649112457\n",
      "Epoch 4 step 1217: training accuarcy: 0.6035\n",
      "Epoch 4 step 1217: training loss: 1334.0430311748742\n",
      "Epoch 4 step 1218: training accuarcy: 0.609\n",
      "Epoch 4 step 1218: training loss: 1330.6852545985325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1219: training accuarcy: 0.602\n",
      "Epoch 4 step 1219: training loss: 1336.8399403828596\n",
      "Epoch 4 step 1220: training accuarcy: 0.6205\n",
      "Epoch 4 step 1220: training loss: 1338.2458758308348\n",
      "Epoch 4 step 1221: training accuarcy: 0.617\n",
      "Epoch 4 step 1221: training loss: 1339.3198867926476\n",
      "Epoch 4 step 1222: training accuarcy: 0.617\n",
      "Epoch 4 step 1222: training loss: 1323.054068040676\n",
      "Epoch 4 step 1223: training accuarcy: 0.623\n",
      "Epoch 4 step 1223: training loss: 1313.8534209165816\n",
      "Epoch 4 step 1224: training accuarcy: 0.637\n",
      "Epoch 4 step 1224: training loss: 1345.8422689262527\n",
      "Epoch 4 step 1225: training accuarcy: 0.6095\n",
      "Epoch 4 step 1225: training loss: 1338.5957478019843\n",
      "Epoch 4 step 1226: training accuarcy: 0.615\n",
      "Epoch 4 step 1226: training loss: 1341.1451612400979\n",
      "Epoch 4 step 1227: training accuarcy: 0.607\n",
      "Epoch 4 step 1227: training loss: 1328.603303964803\n",
      "Epoch 4 step 1228: training accuarcy: 0.6165\n",
      "Epoch 4 step 1228: training loss: 1347.4785473648208\n",
      "Epoch 4 step 1229: training accuarcy: 0.6\n",
      "Epoch 4 step 1229: training loss: 1332.177023605595\n",
      "Epoch 4 step 1230: training accuarcy: 0.62\n",
      "Epoch 4 step 1230: training loss: 1341.8925587167457\n",
      "Epoch 4 step 1231: training accuarcy: 0.615\n",
      "Epoch 4 step 1231: training loss: 1333.4662351512827\n",
      "Epoch 4 step 1232: training accuarcy: 0.6185\n",
      "Epoch 4 step 1232: training loss: 1339.0534007218541\n",
      "Epoch 4 step 1233: training accuarcy: 0.6055\n",
      "Epoch 4 step 1233: training loss: 1324.1674338642524\n",
      "Epoch 4 step 1234: training accuarcy: 0.6405\n",
      "Epoch 4 step 1234: training loss: 1343.9812175975997\n",
      "Epoch 4 step 1235: training accuarcy: 0.6125\n",
      "Epoch 4 step 1235: training loss: 1341.9157767006272\n",
      "Epoch 4 step 1236: training accuarcy: 0.6165\n",
      "Epoch 4 step 1236: training loss: 1326.7598341484443\n",
      "Epoch 4 step 1237: training accuarcy: 0.6145\n",
      "Epoch 4 step 1237: training loss: 1328.416546389084\n",
      "Epoch 4 step 1238: training accuarcy: 0.63\n",
      "Epoch 4 step 1238: training loss: 1334.4657981342775\n",
      "Epoch 4 step 1239: training accuarcy: 0.6175\n",
      "Epoch 4 step 1239: training loss: 1327.910432909412\n",
      "Epoch 4 step 1240: training accuarcy: 0.637\n",
      "Epoch 4 step 1240: training loss: 1333.9354257369378\n",
      "Epoch 4 step 1241: training accuarcy: 0.6145\n",
      "Epoch 4 step 1241: training loss: 1313.9259779201027\n",
      "Epoch 4 step 1242: training accuarcy: 0.634\n",
      "Epoch 4 step 1242: training loss: 1316.846571184011\n",
      "Epoch 4 step 1243: training accuarcy: 0.63\n",
      "Epoch 4 step 1243: training loss: 1347.908619750954\n",
      "Epoch 4 step 1244: training accuarcy: 0.606\n",
      "Epoch 4 step 1244: training loss: 1334.8396436901282\n",
      "Epoch 4 step 1245: training accuarcy: 0.617\n",
      "Epoch 4 step 1245: training loss: 1323.4849691228526\n",
      "Epoch 4 step 1246: training accuarcy: 0.632\n",
      "Epoch 4 step 1246: training loss: 1323.9123562230614\n",
      "Epoch 4 step 1247: training accuarcy: 0.6265000000000001\n",
      "Epoch 4 step 1247: training loss: 1324.9062928944218\n",
      "Epoch 4 step 1248: training accuarcy: 0.635\n",
      "Epoch 4 step 1248: training loss: 1336.4085442132057\n",
      "Epoch 4 step 1249: training accuarcy: 0.6235\n",
      "Epoch 4 step 1249: training loss: 1330.8041756761527\n",
      "Epoch 4 step 1250: training accuarcy: 0.614\n",
      "Epoch 4 step 1250: training loss: 1331.7664164234375\n",
      "Epoch 4 step 1251: training accuarcy: 0.6185\n",
      "Epoch 4 step 1251: training loss: 1323.5152882121083\n",
      "Epoch 4 step 1252: training accuarcy: 0.6185\n",
      "Epoch 4 step 1252: training loss: 1331.3101039173089\n",
      "Epoch 4 step 1253: training accuarcy: 0.6195\n",
      "Epoch 4 step 1253: training loss: 1325.987760957603\n",
      "Epoch 4 step 1254: training accuarcy: 0.6135\n",
      "Epoch 4 step 1254: training loss: 1341.345271664387\n",
      "Epoch 4 step 1255: training accuarcy: 0.6075\n",
      "Epoch 4 step 1255: training loss: 1340.2354192467624\n",
      "Epoch 4 step 1256: training accuarcy: 0.602\n",
      "Epoch 4 step 1256: training loss: 1346.0243091220011\n",
      "Epoch 4 step 1257: training accuarcy: 0.6005\n",
      "Epoch 4 step 1257: training loss: 1328.031495741987\n",
      "Epoch 4 step 1258: training accuarcy: 0.6065\n",
      "Epoch 4 step 1258: training loss: 1340.3515321242712\n",
      "Epoch 4 step 1259: training accuarcy: 0.594\n",
      "Epoch 4 step 1259: training loss: 1328.2102929066868\n",
      "Epoch 4 step 1260: training accuarcy: 0.6305000000000001\n",
      "Epoch 4 step 1260: training loss: 1338.6481040316044\n",
      "Epoch 4 step 1261: training accuarcy: 0.59\n",
      "Epoch 4 step 1261: training loss: 1317.3914676507527\n",
      "Epoch 4 step 1262: training accuarcy: 0.622\n",
      "Epoch 4 step 1262: training loss: 1330.858924377445\n",
      "Epoch 4 step 1263: training accuarcy: 0.604\n",
      "Epoch 4 step 1263: training loss: 1327.7215668344695\n",
      "Epoch 4 step 1264: training accuarcy: 0.6205\n",
      "Epoch 4 step 1264: training loss: 1331.3582372082703\n",
      "Epoch 4 step 1265: training accuarcy: 0.6245\n",
      "Epoch 4 step 1265: training loss: 1349.2647986920877\n",
      "Epoch 4 step 1266: training accuarcy: 0.594\n",
      "Epoch 4 step 1266: training loss: 1337.7832126971414\n",
      "Epoch 4 step 1267: training accuarcy: 0.5945\n",
      "Epoch 4 step 1267: training loss: 1339.1307944477546\n",
      "Epoch 4 step 1268: training accuarcy: 0.6035\n",
      "Epoch 4 step 1268: training loss: 1321.572324399782\n",
      "Epoch 4 step 1269: training accuarcy: 0.63\n",
      "Epoch 4 step 1269: training loss: 1333.1173539004158\n",
      "Epoch 4 step 1270: training accuarcy: 0.617\n",
      "Epoch 4 step 1270: training loss: 1332.0052844086595\n",
      "Epoch 4 step 1271: training accuarcy: 0.6155\n",
      "Epoch 4 step 1271: training loss: 1334.2230713316712\n",
      "Epoch 4 step 1272: training accuarcy: 0.6095\n",
      "Epoch 4 step 1272: training loss: 1327.4422518525325\n",
      "Epoch 4 step 1273: training accuarcy: 0.6155\n",
      "Epoch 4 step 1273: training loss: 1344.0472165418246\n",
      "Epoch 4 step 1274: training accuarcy: 0.6\n",
      "Epoch 4 step 1274: training loss: 1342.360783834311\n",
      "Epoch 4 step 1275: training accuarcy: 0.6065\n",
      "Epoch 4 step 1275: training loss: 1332.2370103891094\n",
      "Epoch 4 step 1276: training accuarcy: 0.6115\n",
      "Epoch 4 step 1276: training loss: 1355.5874305277605\n",
      "Epoch 4 step 1277: training accuarcy: 0.5975\n",
      "Epoch 4 step 1277: training loss: 1324.4139992816908\n",
      "Epoch 4 step 1278: training accuarcy: 0.63\n",
      "Epoch 4 step 1278: training loss: 1329.3930282230594\n",
      "Epoch 4 step 1279: training accuarcy: 0.613\n",
      "Epoch 4 step 1279: training loss: 1342.739363562993\n",
      "Epoch 4 step 1280: training accuarcy: 0.619\n",
      "Epoch 4 step 1280: training loss: 1335.7238178519522\n",
      "Epoch 4 step 1281: training accuarcy: 0.613\n",
      "Epoch 4 step 1281: training loss: 1327.8646647318274\n",
      "Epoch 4 step 1282: training accuarcy: 0.62\n",
      "Epoch 4 step 1282: training loss: 1330.2719501291126\n",
      "Epoch 4 step 1283: training accuarcy: 0.6195\n",
      "Epoch 4 step 1283: training loss: 1323.5849689232818\n",
      "Epoch 4 step 1284: training accuarcy: 0.6265000000000001\n",
      "Epoch 4 step 1284: training loss: 1325.62774786122\n",
      "Epoch 4 step 1285: training accuarcy: 0.6245\n",
      "Epoch 4 step 1285: training loss: 1349.820393854185\n",
      "Epoch 4 step 1286: training accuarcy: 0.608\n",
      "Epoch 4 step 1286: training loss: 1326.3700524118294\n",
      "Epoch 4 step 1287: training accuarcy: 0.618\n",
      "Epoch 4 step 1287: training loss: 1335.403351746616\n",
      "Epoch 4 step 1288: training accuarcy: 0.6325000000000001\n",
      "Epoch 4 step 1288: training loss: 1346.2123810754972\n",
      "Epoch 4 step 1289: training accuarcy: 0.598\n",
      "Epoch 4 step 1289: training loss: 1342.7568989483534\n",
      "Epoch 4 step 1290: training accuarcy: 0.6075\n",
      "Epoch 4 step 1290: training loss: 1324.5958073116487\n",
      "Epoch 4 step 1291: training accuarcy: 0.6125\n",
      "Epoch 4 step 1291: training loss: 1322.4355827132956\n",
      "Epoch 4 step 1292: training accuarcy: 0.615\n",
      "Epoch 4 step 1292: training loss: 1351.0998902012668\n",
      "Epoch 4 step 1293: training accuarcy: 0.608\n",
      "Epoch 4 step 1293: training loss: 1324.1488451760451\n",
      "Epoch 4 step 1294: training accuarcy: 0.614\n",
      "Epoch 4 step 1294: training loss: 1334.0015683331733\n",
      "Epoch 4 step 1295: training accuarcy: 0.6215\n",
      "Epoch 4 step 1295: training loss: 1334.28020141261\n",
      "Epoch 4 step 1296: training accuarcy: 0.633\n",
      "Epoch 4 step 1296: training loss: 1332.924645091227\n",
      "Epoch 4 step 1297: training accuarcy: 0.6215\n",
      "Epoch 4 step 1297: training loss: 1341.473608341351\n",
      "Epoch 4 step 1298: training accuarcy: 0.6085\n",
      "Epoch 4 step 1298: training loss: 1335.0571377243955\n",
      "Epoch 4 step 1299: training accuarcy: 0.601\n",
      "Epoch 4 step 1299: training loss: 1332.4589842765363\n",
      "Epoch 4 step 1300: training accuarcy: 0.628\n",
      "Epoch 4 step 1300: training loss: 1336.191370597291\n",
      "Epoch 4 step 1301: training accuarcy: 0.6245\n",
      "Epoch 4 step 1301: training loss: 1347.1071362099735\n",
      "Epoch 4 step 1302: training accuarcy: 0.5975\n",
      "Epoch 4 step 1302: training loss: 1336.9939623202442\n",
      "Epoch 4 step 1303: training accuarcy: 0.611\n",
      "Epoch 4 step 1303: training loss: 1319.5324606025706\n",
      "Epoch 4 step 1304: training accuarcy: 0.6265000000000001\n",
      "Epoch 4 step 1304: training loss: 1328.324068956839\n",
      "Epoch 4 step 1305: training accuarcy: 0.623\n",
      "Epoch 4 step 1305: training loss: 1344.9838833392075\n",
      "Epoch 4 step 1306: training accuarcy: 0.605\n",
      "Epoch 4 step 1306: training loss: 1350.826260046678\n",
      "Epoch 4 step 1307: training accuarcy: 0.6075\n",
      "Epoch 4 step 1307: training loss: 1352.668263788315\n",
      "Epoch 4 step 1308: training accuarcy: 0.5945\n",
      "Epoch 4 step 1308: training loss: 1330.4302150253218\n",
      "Epoch 4 step 1309: training accuarcy: 0.623\n",
      "Epoch 4 step 1309: training loss: 1313.4404820174009\n",
      "Epoch 4 step 1310: training accuarcy: 0.651\n",
      "Epoch 4 step 1310: training loss: 1344.290427795048\n",
      "Epoch 4 step 1311: training accuarcy: 0.617\n",
      "Epoch 4 step 1311: training loss: 1326.2041253809323\n",
      "Epoch 4 step 1312: training accuarcy: 0.6195\n",
      "Epoch 4 step 1312: training loss: 1333.3000664307483\n",
      "Epoch 4 step 1313: training accuarcy: 0.608\n",
      "Epoch 4 step 1313: training loss: 1330.8318123668162\n",
      "Epoch 4 step 1314: training accuarcy: 0.619\n",
      "Epoch 4 step 1314: training loss: 534.5828419227197\n",
      "Epoch 4 step 1315: training accuarcy: 0.6089743589743589\n",
      "Epoch 4: train loss 1330.8383085234611, train accuarcy 0.612054169178009\n",
      "Epoch 4: valid loss 6704.672626010775, valid accuarcy 0.5840712189674377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [24:16<00:00, 291.35s/it]\n"
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=5,\n",
    "                log_dir=get_log_dir('seq_topcoder', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:39:48.760476Z",
     "start_time": "2019-10-08T01:39:48.749439Z"
    }
   },
   "outputs": [],
   "source": [
    "del hrm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PRME FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:39:49.865875Z",
     "start_time": "2019-10-08T01:39:49.620886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchPrmeFM()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_model = TorchPrmeFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "prme_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:39:51.769734Z",
     "start_time": "2019-10-08T01:39:51.765733Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(prme_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:39:52.520152Z",
     "start_time": "2019-10-08T01:39:52.477153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x290451cf1d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_learner = FMLearner(prme_model, adam_opt, schedular, db)\n",
    "prme_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:39:56.824240Z",
     "start_time": "2019-10-08T01:39:56.821271Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:04:41.432972Z",
     "start_time": "2019-10-08T01:39:58.640128Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 46927.16156849063\n",
      "Epoch 0 step 1: training accuarcy: 0.528\n",
      "Epoch 0 step 1: training loss: 46267.131020927816\n",
      "Epoch 0 step 2: training accuarcy: 0.522\n",
      "Epoch 0 step 2: training loss: 44997.714309480885\n",
      "Epoch 0 step 3: training accuarcy: 0.508\n",
      "Epoch 0 step 3: training loss: 43504.5824063122\n",
      "Epoch 0 step 4: training accuarcy: 0.533\n",
      "Epoch 0 step 4: training loss: 41738.84964275482\n",
      "Epoch 0 step 5: training accuarcy: 0.5385\n",
      "Epoch 0 step 5: training loss: 41276.64514832693\n",
      "Epoch 0 step 6: training accuarcy: 0.519\n",
      "Epoch 0 step 6: training loss: 40171.31425944169\n",
      "Epoch 0 step 7: training accuarcy: 0.5265\n",
      "Epoch 0 step 7: training loss: 38565.05773775875\n",
      "Epoch 0 step 8: training accuarcy: 0.5425\n",
      "Epoch 0 step 8: training loss: 38105.47435526393\n",
      "Epoch 0 step 9: training accuarcy: 0.5205\n",
      "Epoch 0 step 9: training loss: 36857.612616393904\n",
      "Epoch 0 step 10: training accuarcy: 0.544\n",
      "Epoch 0 step 10: training loss: 35801.712217204855\n",
      "Epoch 0 step 11: training accuarcy: 0.5335\n",
      "Epoch 0 step 11: training loss: 34800.58966036716\n",
      "Epoch 0 step 12: training accuarcy: 0.5285\n",
      "Epoch 0 step 12: training loss: 33447.43001331162\n",
      "Epoch 0 step 13: training accuarcy: 0.548\n",
      "Epoch 0 step 13: training loss: 32871.64370967203\n",
      "Epoch 0 step 14: training accuarcy: 0.544\n",
      "Epoch 0 step 14: training loss: 32243.750326126203\n",
      "Epoch 0 step 15: training accuarcy: 0.5425\n",
      "Epoch 0 step 15: training loss: 31422.85065001123\n",
      "Epoch 0 step 16: training accuarcy: 0.5435\n",
      "Epoch 0 step 16: training loss: 30215.421835580302\n",
      "Epoch 0 step 17: training accuarcy: 0.54\n",
      "Epoch 0 step 17: training loss: 29544.990005749998\n",
      "Epoch 0 step 18: training accuarcy: 0.534\n",
      "Epoch 0 step 18: training loss: 28827.378789034254\n",
      "Epoch 0 step 19: training accuarcy: 0.5415\n",
      "Epoch 0 step 19: training loss: 27525.271005299743\n",
      "Epoch 0 step 20: training accuarcy: 0.5565\n",
      "Epoch 0 step 20: training loss: 26929.614179048298\n",
      "Epoch 0 step 21: training accuarcy: 0.5395\n",
      "Epoch 0 step 21: training loss: 26586.86221246812\n",
      "Epoch 0 step 22: training accuarcy: 0.5355\n",
      "Epoch 0 step 22: training loss: 25703.665675775912\n",
      "Epoch 0 step 23: training accuarcy: 0.5155\n",
      "Epoch 0 step 23: training loss: 24636.56928582914\n",
      "Epoch 0 step 24: training accuarcy: 0.5435\n",
      "Epoch 0 step 24: training loss: 24048.938525175363\n",
      "Epoch 0 step 25: training accuarcy: 0.5515\n",
      "Epoch 0 step 25: training loss: 23715.43837324416\n",
      "Epoch 0 step 26: training accuarcy: 0.5365\n",
      "Epoch 0 step 26: training loss: 22565.389630881342\n",
      "Epoch 0 step 27: training accuarcy: 0.5475\n",
      "Epoch 0 step 27: training loss: 21979.77554666354\n",
      "Epoch 0 step 28: training accuarcy: 0.531\n",
      "Epoch 0 step 28: training loss: 21138.022624430603\n",
      "Epoch 0 step 29: training accuarcy: 0.5595\n",
      "Epoch 0 step 29: training loss: 20775.199642432293\n",
      "Epoch 0 step 30: training accuarcy: 0.551\n",
      "Epoch 0 step 30: training loss: 20252.14793461324\n",
      "Epoch 0 step 31: training accuarcy: 0.544\n",
      "Epoch 0 step 31: training loss: 19536.47578224165\n",
      "Epoch 0 step 32: training accuarcy: 0.542\n",
      "Epoch 0 step 32: training loss: 18994.672864383727\n",
      "Epoch 0 step 33: training accuarcy: 0.543\n",
      "Epoch 0 step 33: training loss: 18197.381039435477\n",
      "Epoch 0 step 34: training accuarcy: 0.5445\n",
      "Epoch 0 step 34: training loss: 17756.7936145097\n",
      "Epoch 0 step 35: training accuarcy: 0.5385\n",
      "Epoch 0 step 35: training loss: 17317.50342535185\n",
      "Epoch 0 step 36: training accuarcy: 0.545\n",
      "Epoch 0 step 36: training loss: 16786.478263973637\n",
      "Epoch 0 step 37: training accuarcy: 0.5505\n",
      "Epoch 0 step 37: training loss: 15947.867351328721\n",
      "Epoch 0 step 38: training accuarcy: 0.5585\n",
      "Epoch 0 step 38: training loss: 15572.43214256528\n",
      "Epoch 0 step 39: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 39: training loss: 15028.011713422351\n",
      "Epoch 0 step 40: training accuarcy: 0.5635\n",
      "Epoch 0 step 40: training loss: 14771.455233685645\n",
      "Epoch 0 step 41: training accuarcy: 0.543\n",
      "Epoch 0 step 41: training loss: 14422.374348376323\n",
      "Epoch 0 step 42: training accuarcy: 0.55\n",
      "Epoch 0 step 42: training loss: 13894.542270638585\n",
      "Epoch 0 step 43: training accuarcy: 0.55\n",
      "Epoch 0 step 43: training loss: 13492.301657938506\n",
      "Epoch 0 step 44: training accuarcy: 0.5415\n",
      "Epoch 0 step 44: training loss: 13039.690740895297\n",
      "Epoch 0 step 45: training accuarcy: 0.5495\n",
      "Epoch 0 step 45: training loss: 12550.325621190404\n",
      "Epoch 0 step 46: training accuarcy: 0.5575\n",
      "Epoch 0 step 46: training loss: 12190.348315773052\n",
      "Epoch 0 step 47: training accuarcy: 0.5595\n",
      "Epoch 0 step 47: training loss: 11729.13329983232\n",
      "Epoch 0 step 48: training accuarcy: 0.5395\n",
      "Epoch 0 step 48: training loss: 11461.075948387854\n",
      "Epoch 0 step 49: training accuarcy: 0.5535\n",
      "Epoch 0 step 49: training loss: 10989.440623555252\n",
      "Epoch 0 step 50: training accuarcy: 0.5725\n",
      "Epoch 0 step 50: training loss: 10786.110122823437\n",
      "Epoch 0 step 51: training accuarcy: 0.5650000000000001\n",
      "Epoch 0 step 51: training loss: 10476.523477658611\n",
      "Epoch 0 step 52: training accuarcy: 0.55\n",
      "Epoch 0 step 52: training loss: 10183.647217375634\n",
      "Epoch 0 step 53: training accuarcy: 0.5375\n",
      "Epoch 0 step 53: training loss: 9894.029234387459\n",
      "Epoch 0 step 54: training accuarcy: 0.5565\n",
      "Epoch 0 step 54: training loss: 9471.975872636343\n",
      "Epoch 0 step 55: training accuarcy: 0.561\n",
      "Epoch 0 step 55: training loss: 9350.68326073622\n",
      "Epoch 0 step 56: training accuarcy: 0.562\n",
      "Epoch 0 step 56: training loss: 9015.11266236952\n",
      "Epoch 0 step 57: training accuarcy: 0.548\n",
      "Epoch 0 step 57: training loss: 8760.419503878573\n",
      "Epoch 0 step 58: training accuarcy: 0.5465\n",
      "Epoch 0 step 58: training loss: 8471.61104893636\n",
      "Epoch 0 step 59: training accuarcy: 0.557\n",
      "Epoch 0 step 59: training loss: 8305.28884067535\n",
      "Epoch 0 step 60: training accuarcy: 0.551\n",
      "Epoch 0 step 60: training loss: 8077.917201514849\n",
      "Epoch 0 step 61: training accuarcy: 0.5730000000000001\n",
      "Epoch 0 step 61: training loss: 7748.57195828423\n",
      "Epoch 0 step 62: training accuarcy: 0.5785\n",
      "Epoch 0 step 62: training loss: 7671.510489478742\n",
      "Epoch 0 step 63: training accuarcy: 0.5665\n",
      "Epoch 0 step 63: training loss: 7405.72767613759\n",
      "Epoch 0 step 64: training accuarcy: 0.5685\n",
      "Epoch 0 step 64: training loss: 7238.935027772531\n",
      "Epoch 0 step 65: training accuarcy: 0.5555\n",
      "Epoch 0 step 65: training loss: 6991.621463414847\n",
      "Epoch 0 step 66: training accuarcy: 0.5805\n",
      "Epoch 0 step 66: training loss: 6813.833230761604\n",
      "Epoch 0 step 67: training accuarcy: 0.5710000000000001\n",
      "Epoch 0 step 67: training loss: 6727.240826695287\n",
      "Epoch 0 step 68: training accuarcy: 0.582\n",
      "Epoch 0 step 68: training loss: 6485.750591785653\n",
      "Epoch 0 step 69: training accuarcy: 0.5725\n",
      "Epoch 0 step 69: training loss: 6307.2687118645\n",
      "Epoch 0 step 70: training accuarcy: 0.5700000000000001\n",
      "Epoch 0 step 70: training loss: 6206.080883784136\n",
      "Epoch 0 step 71: training accuarcy: 0.5615\n",
      "Epoch 0 step 71: training loss: 6008.391176260431\n",
      "Epoch 0 step 72: training accuarcy: 0.595\n",
      "Epoch 0 step 72: training loss: 5834.727505006756\n",
      "Epoch 0 step 73: training accuarcy: 0.5855\n",
      "Epoch 0 step 73: training loss: 5733.261477365834\n",
      "Epoch 0 step 74: training accuarcy: 0.598\n",
      "Epoch 0 step 74: training loss: 5589.568433675965\n",
      "Epoch 0 step 75: training accuarcy: 0.579\n",
      "Epoch 0 step 75: training loss: 5487.66461285956\n",
      "Epoch 0 step 76: training accuarcy: 0.5720000000000001\n",
      "Epoch 0 step 76: training loss: 5323.086617056603\n",
      "Epoch 0 step 77: training accuarcy: 0.5975\n",
      "Epoch 0 step 77: training loss: 5239.811388797632\n",
      "Epoch 0 step 78: training accuarcy: 0.5795\n",
      "Epoch 0 step 78: training loss: 5126.27495405724\n",
      "Epoch 0 step 79: training accuarcy: 0.5740000000000001\n",
      "Epoch 0 step 79: training loss: 4994.890250245469\n",
      "Epoch 0 step 80: training accuarcy: 0.5760000000000001\n",
      "Epoch 0 step 80: training loss: 4884.08715668009\n",
      "Epoch 0 step 81: training accuarcy: 0.5740000000000001\n",
      "Epoch 0 step 81: training loss: 4754.721930430654\n",
      "Epoch 0 step 82: training accuarcy: 0.5965\n",
      "Epoch 0 step 82: training loss: 4673.495836331537\n",
      "Epoch 0 step 83: training accuarcy: 0.5875\n",
      "Epoch 0 step 83: training loss: 4568.660717614919\n",
      "Epoch 0 step 84: training accuarcy: 0.591\n",
      "Epoch 0 step 84: training loss: 4444.593328447698\n",
      "Epoch 0 step 85: training accuarcy: 0.5915\n",
      "Epoch 0 step 85: training loss: 4399.564908451477\n",
      "Epoch 0 step 86: training accuarcy: 0.594\n",
      "Epoch 0 step 86: training loss: 4290.691150864306\n",
      "Epoch 0 step 87: training accuarcy: 0.602\n",
      "Epoch 0 step 87: training loss: 4199.003184630483\n",
      "Epoch 0 step 88: training accuarcy: 0.6115\n",
      "Epoch 0 step 88: training loss: 4094.2522177300616\n",
      "Epoch 0 step 89: training accuarcy: 0.609\n",
      "Epoch 0 step 89: training loss: 4020.4730633878894\n",
      "Epoch 0 step 90: training accuarcy: 0.612\n",
      "Epoch 0 step 90: training loss: 3943.019222730679\n",
      "Epoch 0 step 91: training accuarcy: 0.617\n",
      "Epoch 0 step 91: training loss: 3876.15521099281\n",
      "Epoch 0 step 92: training accuarcy: 0.607\n",
      "Epoch 0 step 92: training loss: 3797.0490255419177\n",
      "Epoch 0 step 93: training accuarcy: 0.616\n",
      "Epoch 0 step 93: training loss: 3730.7646854581662\n",
      "Epoch 0 step 94: training accuarcy: 0.601\n",
      "Epoch 0 step 94: training loss: 3676.7603365778637\n",
      "Epoch 0 step 95: training accuarcy: 0.606\n",
      "Epoch 0 step 95: training loss: 3586.1226988180024\n",
      "Epoch 0 step 96: training accuarcy: 0.61\n",
      "Epoch 0 step 96: training loss: 3555.859190743114\n",
      "Epoch 0 step 97: training accuarcy: 0.603\n",
      "Epoch 0 step 97: training loss: 3463.5148674111833\n",
      "Epoch 0 step 98: training accuarcy: 0.6105\n",
      "Epoch 0 step 98: training loss: 3436.0501813509964\n",
      "Epoch 0 step 99: training accuarcy: 0.6105\n",
      "Epoch 0 step 99: training loss: 3354.548151223262\n",
      "Epoch 0 step 100: training accuarcy: 0.636\n",
      "Epoch 0 step 100: training loss: 3309.1983870233175\n",
      "Epoch 0 step 101: training accuarcy: 0.61\n",
      "Epoch 0 step 101: training loss: 3239.8812544008124\n",
      "Epoch 0 step 102: training accuarcy: 0.612\n",
      "Epoch 0 step 102: training loss: 3177.019660960785\n",
      "Epoch 0 step 103: training accuarcy: 0.638\n",
      "Epoch 0 step 103: training loss: 3137.0805034797995\n",
      "Epoch 0 step 104: training accuarcy: 0.6265000000000001\n",
      "Epoch 0 step 104: training loss: 3091.410584532131\n",
      "Epoch 0 step 105: training accuarcy: 0.6225\n",
      "Epoch 0 step 105: training loss: 3035.2841987353704\n",
      "Epoch 0 step 106: training accuarcy: 0.6375000000000001\n",
      "Epoch 0 step 106: training loss: 2993.7960414426093\n",
      "Epoch 0 step 107: training accuarcy: 0.642\n",
      "Epoch 0 step 107: training loss: 2943.9685289540002\n",
      "Epoch 0 step 108: training accuarcy: 0.6235\n",
      "Epoch 0 step 108: training loss: 2906.513951568735\n",
      "Epoch 0 step 109: training accuarcy: 0.6255000000000001\n",
      "Epoch 0 step 109: training loss: 2850.607592454081\n",
      "Epoch 0 step 110: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 110: training loss: 2821.329512284751\n",
      "Epoch 0 step 111: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 111: training loss: 2780.139443070212\n",
      "Epoch 0 step 112: training accuarcy: 0.6195\n",
      "Epoch 0 step 112: training loss: 2765.5500974982133\n",
      "Epoch 0 step 113: training accuarcy: 0.6125\n",
      "Epoch 0 step 113: training loss: 2707.040109915558\n",
      "Epoch 0 step 114: training accuarcy: 0.633\n",
      "Epoch 0 step 114: training loss: 2663.4676454255837\n",
      "Epoch 0 step 115: training accuarcy: 0.6425\n",
      "Epoch 0 step 115: training loss: 2646.424238845284\n",
      "Epoch 0 step 116: training accuarcy: 0.6185\n",
      "Epoch 0 step 116: training loss: 2601.185279532063\n",
      "Epoch 0 step 117: training accuarcy: 0.613\n",
      "Epoch 0 step 117: training loss: 2549.269357353496\n",
      "Epoch 0 step 118: training accuarcy: 0.6375000000000001\n",
      "Epoch 0 step 118: training loss: 2532.7619303878473\n",
      "Epoch 0 step 119: training accuarcy: 0.6225\n",
      "Epoch 0 step 119: training loss: 2512.061219075562\n",
      "Epoch 0 step 120: training accuarcy: 0.6375000000000001\n",
      "Epoch 0 step 120: training loss: 2496.9043274794176\n",
      "Epoch 0 step 121: training accuarcy: 0.637\n",
      "Epoch 0 step 121: training loss: 2456.2089927914235\n",
      "Epoch 0 step 122: training accuarcy: 0.6145\n",
      "Epoch 0 step 122: training loss: 2419.7270731484537\n",
      "Epoch 0 step 123: training accuarcy: 0.6265000000000001\n",
      "Epoch 0 step 123: training loss: 2390.354988159058\n",
      "Epoch 0 step 124: training accuarcy: 0.637\n",
      "Epoch 0 step 124: training loss: 2362.891898968025\n",
      "Epoch 0 step 125: training accuarcy: 0.634\n",
      "Epoch 0 step 125: training loss: 2322.2900408813707\n",
      "Epoch 0 step 126: training accuarcy: 0.642\n",
      "Epoch 0 step 126: training loss: 2313.6206482262223\n",
      "Epoch 0 step 127: training accuarcy: 0.643\n",
      "Epoch 0 step 127: training loss: 2280.6790362551\n",
      "Epoch 0 step 128: training accuarcy: 0.636\n",
      "Epoch 0 step 128: training loss: 2251.735282743663\n",
      "Epoch 0 step 129: training accuarcy: 0.6455\n",
      "Epoch 0 step 129: training loss: 2240.2033712823177\n",
      "Epoch 0 step 130: training accuarcy: 0.6295000000000001\n",
      "Epoch 0 step 130: training loss: 2228.1420716444436\n",
      "Epoch 0 step 131: training accuarcy: 0.627\n",
      "Epoch 0 step 131: training loss: 2198.3345400872645\n",
      "Epoch 0 step 132: training accuarcy: 0.641\n",
      "Epoch 0 step 132: training loss: 2178.6304258210916\n",
      "Epoch 0 step 133: training accuarcy: 0.6525\n",
      "Epoch 0 step 133: training loss: 2145.9915079821353\n",
      "Epoch 0 step 134: training accuarcy: 0.627\n",
      "Epoch 0 step 134: training loss: 2130.4795425210323\n",
      "Epoch 0 step 135: training accuarcy: 0.6405\n",
      "Epoch 0 step 135: training loss: 2110.133863569372\n",
      "Epoch 0 step 136: training accuarcy: 0.6465\n",
      "Epoch 0 step 136: training loss: 2082.3747503607333\n",
      "Epoch 0 step 137: training accuarcy: 0.6575\n",
      "Epoch 0 step 137: training loss: 2075.102193680859\n",
      "Epoch 0 step 138: training accuarcy: 0.63\n",
      "Epoch 0 step 138: training loss: 2061.09942161161\n",
      "Epoch 0 step 139: training accuarcy: 0.631\n",
      "Epoch 0 step 139: training loss: 2043.4578919771309\n",
      "Epoch 0 step 140: training accuarcy: 0.623\n",
      "Epoch 0 step 140: training loss: 2001.3201503352518\n",
      "Epoch 0 step 141: training accuarcy: 0.648\n",
      "Epoch 0 step 141: training loss: 2004.3435338537886\n",
      "Epoch 0 step 142: training accuarcy: 0.6395000000000001\n",
      "Epoch 0 step 142: training loss: 1987.656797719363\n",
      "Epoch 0 step 143: training accuarcy: 0.6315000000000001\n",
      "Epoch 0 step 143: training loss: 1968.983318092427\n",
      "Epoch 0 step 144: training accuarcy: 0.636\n",
      "Epoch 0 step 144: training loss: 1965.9163415158828\n",
      "Epoch 0 step 145: training accuarcy: 0.64\n",
      "Epoch 0 step 145: training loss: 1931.0384516781223\n",
      "Epoch 0 step 146: training accuarcy: 0.6395000000000001\n",
      "Epoch 0 step 146: training loss: 1933.3886465697155\n",
      "Epoch 0 step 147: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 147: training loss: 1909.2760446354987\n",
      "Epoch 0 step 148: training accuarcy: 0.645\n",
      "Epoch 0 step 148: training loss: 1896.2998628046723\n",
      "Epoch 0 step 149: training accuarcy: 0.634\n",
      "Epoch 0 step 149: training loss: 1872.9942595789607\n",
      "Epoch 0 step 150: training accuarcy: 0.6605\n",
      "Epoch 0 step 150: training loss: 1874.5758848663145\n",
      "Epoch 0 step 151: training accuarcy: 0.6365000000000001\n",
      "Epoch 0 step 151: training loss: 1860.3512277494744\n",
      "Epoch 0 step 152: training accuarcy: 0.638\n",
      "Epoch 0 step 152: training loss: 1845.927747677313\n",
      "Epoch 0 step 153: training accuarcy: 0.6615\n",
      "Epoch 0 step 153: training loss: 1816.1892296995652\n",
      "Epoch 0 step 154: training accuarcy: 0.657\n",
      "Epoch 0 step 154: training loss: 1811.1268198528264\n",
      "Epoch 0 step 155: training accuarcy: 0.655\n",
      "Epoch 0 step 155: training loss: 1818.2316853300013\n",
      "Epoch 0 step 156: training accuarcy: 0.639\n",
      "Epoch 0 step 156: training loss: 1789.6810677889503\n",
      "Epoch 0 step 157: training accuarcy: 0.644\n",
      "Epoch 0 step 157: training loss: 1781.6739039834586\n",
      "Epoch 0 step 158: training accuarcy: 0.6525\n",
      "Epoch 0 step 158: training loss: 1772.7928175092684\n",
      "Epoch 0 step 159: training accuarcy: 0.6475\n",
      "Epoch 0 step 159: training loss: 1764.6239982692578\n",
      "Epoch 0 step 160: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 160: training loss: 1769.0485825605592\n",
      "Epoch 0 step 161: training accuarcy: 0.617\n",
      "Epoch 0 step 161: training loss: 1729.2427761348558\n",
      "Epoch 0 step 162: training accuarcy: 0.6485\n",
      "Epoch 0 step 162: training loss: 1740.5479742190482\n",
      "Epoch 0 step 163: training accuarcy: 0.6455\n",
      "Epoch 0 step 163: training loss: 1740.2754621693766\n",
      "Epoch 0 step 164: training accuarcy: 0.6405\n",
      "Epoch 0 step 164: training loss: 1708.2243931603273\n",
      "Epoch 0 step 165: training accuarcy: 0.6465\n",
      "Epoch 0 step 165: training loss: 1711.586361668013\n",
      "Epoch 0 step 166: training accuarcy: 0.635\n",
      "Epoch 0 step 166: training loss: 1700.3333193700153\n",
      "Epoch 0 step 167: training accuarcy: 0.637\n",
      "Epoch 0 step 167: training loss: 1702.6674802577859\n",
      "Epoch 0 step 168: training accuarcy: 0.622\n",
      "Epoch 0 step 168: training loss: 1691.6320790116106\n",
      "Epoch 0 step 169: training accuarcy: 0.6445\n",
      "Epoch 0 step 169: training loss: 1663.0151909255596\n",
      "Epoch 0 step 170: training accuarcy: 0.6495\n",
      "Epoch 0 step 170: training loss: 1683.2645776563918\n",
      "Epoch 0 step 171: training accuarcy: 0.6345000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 171: training loss: 1678.5417919913398\n",
      "Epoch 0 step 172: training accuarcy: 0.622\n",
      "Epoch 0 step 172: training loss: 1668.6281048016017\n",
      "Epoch 0 step 173: training accuarcy: 0.649\n",
      "Epoch 0 step 173: training loss: 1674.0068939601797\n",
      "Epoch 0 step 174: training accuarcy: 0.628\n",
      "Epoch 0 step 174: training loss: 1647.0013929377897\n",
      "Epoch 0 step 175: training accuarcy: 0.6155\n",
      "Epoch 0 step 175: training loss: 1633.9893575118667\n",
      "Epoch 0 step 176: training accuarcy: 0.64\n",
      "Epoch 0 step 176: training loss: 1635.0645925763888\n",
      "Epoch 0 step 177: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 177: training loss: 1614.1180871273991\n",
      "Epoch 0 step 178: training accuarcy: 0.6525\n",
      "Epoch 0 step 178: training loss: 1629.1897962537098\n",
      "Epoch 0 step 179: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 179: training loss: 1592.4721621138196\n",
      "Epoch 0 step 180: training accuarcy: 0.6495\n",
      "Epoch 0 step 180: training loss: 1602.3045706193288\n",
      "Epoch 0 step 181: training accuarcy: 0.6495\n",
      "Epoch 0 step 181: training loss: 1604.1267912137407\n",
      "Epoch 0 step 182: training accuarcy: 0.6335000000000001\n",
      "Epoch 0 step 182: training loss: 1594.950332726965\n",
      "Epoch 0 step 183: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 183: training loss: 1594.1927007420197\n",
      "Epoch 0 step 184: training accuarcy: 0.6215\n",
      "Epoch 0 step 184: training loss: 1576.1309604629762\n",
      "Epoch 0 step 185: training accuarcy: 0.644\n",
      "Epoch 0 step 185: training loss: 1582.858909082782\n",
      "Epoch 0 step 186: training accuarcy: 0.629\n",
      "Epoch 0 step 186: training loss: 1598.9081264359418\n",
      "Epoch 0 step 187: training accuarcy: 0.6195\n",
      "Epoch 0 step 187: training loss: 1574.8036490553618\n",
      "Epoch 0 step 188: training accuarcy: 0.623\n",
      "Epoch 0 step 188: training loss: 1567.2722907042032\n",
      "Epoch 0 step 189: training accuarcy: 0.628\n",
      "Epoch 0 step 189: training loss: 1559.9834196659735\n",
      "Epoch 0 step 190: training accuarcy: 0.626\n",
      "Epoch 0 step 190: training loss: 1556.9596865448054\n",
      "Epoch 0 step 191: training accuarcy: 0.626\n",
      "Epoch 0 step 191: training loss: 1538.6276467430782\n",
      "Epoch 0 step 192: training accuarcy: 0.6255000000000001\n",
      "Epoch 0 step 192: training loss: 1550.9450418524389\n",
      "Epoch 0 step 193: training accuarcy: 0.6255000000000001\n",
      "Epoch 0 step 193: training loss: 1535.5848933053412\n",
      "Epoch 0 step 194: training accuarcy: 0.641\n",
      "Epoch 0 step 194: training loss: 1544.9909960989467\n",
      "Epoch 0 step 195: training accuarcy: 0.621\n",
      "Epoch 0 step 195: training loss: 1542.4600956210056\n",
      "Epoch 0 step 196: training accuarcy: 0.615\n",
      "Epoch 0 step 196: training loss: 1515.4746962260824\n",
      "Epoch 0 step 197: training accuarcy: 0.6365000000000001\n",
      "Epoch 0 step 197: training loss: 1514.8360196936164\n",
      "Epoch 0 step 198: training accuarcy: 0.66\n",
      "Epoch 0 step 198: training loss: 1514.4958925540839\n",
      "Epoch 0 step 199: training accuarcy: 0.6405\n",
      "Epoch 0 step 199: training loss: 1505.7057040990958\n",
      "Epoch 0 step 200: training accuarcy: 0.6435\n",
      "Epoch 0 step 200: training loss: 1514.9716339593629\n",
      "Epoch 0 step 201: training accuarcy: 0.611\n",
      "Epoch 0 step 201: training loss: 1509.4620789840242\n",
      "Epoch 0 step 202: training accuarcy: 0.65\n",
      "Epoch 0 step 202: training loss: 1495.9174981072147\n",
      "Epoch 0 step 203: training accuarcy: 0.626\n",
      "Epoch 0 step 203: training loss: 1510.859021850583\n",
      "Epoch 0 step 204: training accuarcy: 0.6325000000000001\n",
      "Epoch 0 step 204: training loss: 1520.5413502696415\n",
      "Epoch 0 step 205: training accuarcy: 0.637\n",
      "Epoch 0 step 205: training loss: 1503.5211685322224\n",
      "Epoch 0 step 206: training accuarcy: 0.6405\n",
      "Epoch 0 step 206: training loss: 1506.0219008361391\n",
      "Epoch 0 step 207: training accuarcy: 0.6265000000000001\n",
      "Epoch 0 step 207: training loss: 1489.1271411535283\n",
      "Epoch 0 step 208: training accuarcy: 0.6335000000000001\n",
      "Epoch 0 step 208: training loss: 1506.1366412606585\n",
      "Epoch 0 step 209: training accuarcy: 0.601\n",
      "Epoch 0 step 209: training loss: 1466.4477153627313\n",
      "Epoch 0 step 210: training accuarcy: 0.642\n",
      "Epoch 0 step 210: training loss: 1474.0845807481503\n",
      "Epoch 0 step 211: training accuarcy: 0.6135\n",
      "Epoch 0 step 211: training loss: 1493.2524326762882\n",
      "Epoch 0 step 212: training accuarcy: 0.633\n",
      "Epoch 0 step 212: training loss: 1471.7028465153123\n",
      "Epoch 0 step 213: training accuarcy: 0.6415\n",
      "Epoch 0 step 213: training loss: 1468.1189134890692\n",
      "Epoch 0 step 214: training accuarcy: 0.629\n",
      "Epoch 0 step 214: training loss: 1470.508749900753\n",
      "Epoch 0 step 215: training accuarcy: 0.629\n",
      "Epoch 0 step 215: training loss: 1473.3111358122837\n",
      "Epoch 0 step 216: training accuarcy: 0.63\n",
      "Epoch 0 step 216: training loss: 1466.5251225917232\n",
      "Epoch 0 step 217: training accuarcy: 0.604\n",
      "Epoch 0 step 217: training loss: 1452.5596037930745\n",
      "Epoch 0 step 218: training accuarcy: 0.6355000000000001\n",
      "Epoch 0 step 218: training loss: 1457.7681081807032\n",
      "Epoch 0 step 219: training accuarcy: 0.625\n",
      "Epoch 0 step 219: training loss: 1446.142236969551\n",
      "Epoch 0 step 220: training accuarcy: 0.637\n",
      "Epoch 0 step 220: training loss: 1459.9288855031323\n",
      "Epoch 0 step 221: training accuarcy: 0.632\n",
      "Epoch 0 step 221: training loss: 1471.5858602790838\n",
      "Epoch 0 step 222: training accuarcy: 0.627\n",
      "Epoch 0 step 222: training loss: 1442.596646837299\n",
      "Epoch 0 step 223: training accuarcy: 0.629\n",
      "Epoch 0 step 223: training loss: 1449.7897302558686\n",
      "Epoch 0 step 224: training accuarcy: 0.616\n",
      "Epoch 0 step 224: training loss: 1451.2658156029331\n",
      "Epoch 0 step 225: training accuarcy: 0.6155\n",
      "Epoch 0 step 225: training loss: 1445.4638941606622\n",
      "Epoch 0 step 226: training accuarcy: 0.6115\n",
      "Epoch 0 step 226: training loss: 1445.5845607400897\n",
      "Epoch 0 step 227: training accuarcy: 0.6245\n",
      "Epoch 0 step 227: training loss: 1431.594708025666\n",
      "Epoch 0 step 228: training accuarcy: 0.631\n",
      "Epoch 0 step 228: training loss: 1443.4320651069906\n",
      "Epoch 0 step 229: training accuarcy: 0.6245\n",
      "Epoch 0 step 229: training loss: 1432.2517404816879\n",
      "Epoch 0 step 230: training accuarcy: 0.6225\n",
      "Epoch 0 step 230: training loss: 1418.899522753791\n",
      "Epoch 0 step 231: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 231: training loss: 1427.4802278079603\n",
      "Epoch 0 step 232: training accuarcy: 0.632\n",
      "Epoch 0 step 232: training loss: 1427.0108791145046\n",
      "Epoch 0 step 233: training accuarcy: 0.6145\n",
      "Epoch 0 step 233: training loss: 1424.5806536248365\n",
      "Epoch 0 step 234: training accuarcy: 0.6215\n",
      "Epoch 0 step 234: training loss: 1431.2566345903392\n",
      "Epoch 0 step 235: training accuarcy: 0.614\n",
      "Epoch 0 step 235: training loss: 1416.1516231051826\n",
      "Epoch 0 step 236: training accuarcy: 0.6385000000000001\n",
      "Epoch 0 step 236: training loss: 1417.650011917325\n",
      "Epoch 0 step 237: training accuarcy: 0.6185\n",
      "Epoch 0 step 237: training loss: 1415.2937988881981\n",
      "Epoch 0 step 238: training accuarcy: 0.626\n",
      "Epoch 0 step 238: training loss: 1419.9457063113\n",
      "Epoch 0 step 239: training accuarcy: 0.629\n",
      "Epoch 0 step 239: training loss: 1414.359643586185\n",
      "Epoch 0 step 240: training accuarcy: 0.633\n",
      "Epoch 0 step 240: training loss: 1422.6312142142087\n",
      "Epoch 0 step 241: training accuarcy: 0.608\n",
      "Epoch 0 step 241: training loss: 1409.35383602847\n",
      "Epoch 0 step 242: training accuarcy: 0.632\n",
      "Epoch 0 step 242: training loss: 1412.9967703487382\n",
      "Epoch 0 step 243: training accuarcy: 0.622\n",
      "Epoch 0 step 243: training loss: 1403.603230271708\n",
      "Epoch 0 step 244: training accuarcy: 0.6255000000000001\n",
      "Epoch 0 step 244: training loss: 1416.1583582312423\n",
      "Epoch 0 step 245: training accuarcy: 0.618\n",
      "Epoch 0 step 245: training loss: 1399.4145263820026\n",
      "Epoch 0 step 246: training accuarcy: 0.6205\n",
      "Epoch 0 step 246: training loss: 1413.8871594190073\n",
      "Epoch 0 step 247: training accuarcy: 0.614\n",
      "Epoch 0 step 247: training loss: 1415.1136872481452\n",
      "Epoch 0 step 248: training accuarcy: 0.6095\n",
      "Epoch 0 step 248: training loss: 1417.4025161547913\n",
      "Epoch 0 step 249: training accuarcy: 0.6075\n",
      "Epoch 0 step 249: training loss: 1387.5755909925717\n",
      "Epoch 0 step 250: training accuarcy: 0.6515\n",
      "Epoch 0 step 250: training loss: 1408.7961744575018\n",
      "Epoch 0 step 251: training accuarcy: 0.623\n",
      "Epoch 0 step 251: training loss: 1392.9053651700913\n",
      "Epoch 0 step 252: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 252: training loss: 1425.5747820575803\n",
      "Epoch 0 step 253: training accuarcy: 0.617\n",
      "Epoch 0 step 253: training loss: 1387.6189483805745\n",
      "Epoch 0 step 254: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 254: training loss: 1398.0573646884022\n",
      "Epoch 0 step 255: training accuarcy: 0.6115\n",
      "Epoch 0 step 255: training loss: 1379.1809693922469\n",
      "Epoch 0 step 256: training accuarcy: 0.639\n",
      "Epoch 0 step 256: training loss: 1398.3789644647784\n",
      "Epoch 0 step 257: training accuarcy: 0.626\n",
      "Epoch 0 step 257: training loss: 1392.1287283324239\n",
      "Epoch 0 step 258: training accuarcy: 0.624\n",
      "Epoch 0 step 258: training loss: 1402.460511608305\n",
      "Epoch 0 step 259: training accuarcy: 0.622\n",
      "Epoch 0 step 259: training loss: 1415.101527706496\n",
      "Epoch 0 step 260: training accuarcy: 0.6085\n",
      "Epoch 0 step 260: training loss: 1397.3951568410282\n",
      "Epoch 0 step 261: training accuarcy: 0.6215\n",
      "Epoch 0 step 261: training loss: 1384.800175814963\n",
      "Epoch 0 step 262: training accuarcy: 0.6245\n",
      "Epoch 0 step 262: training loss: 583.1636826798696\n",
      "Epoch 0 step 263: training accuarcy: 0.6115384615384616\n",
      "Epoch 0: train loss 7125.070112833991, train accuarcy 0.6078327894210815\n",
      "Epoch 0: valid loss 6660.448652538619, valid accuarcy 0.5964019298553467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▍                                                                                                                                         | 1/5 [04:54<19:37, 294.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1391.3742324237164\n",
      "Epoch 1 step 264: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 264: training loss: 1386.7022409815424\n",
      "Epoch 1 step 265: training accuarcy: 0.625\n",
      "Epoch 1 step 265: training loss: 1375.0216078713688\n",
      "Epoch 1 step 266: training accuarcy: 0.62\n",
      "Epoch 1 step 266: training loss: 1397.4907678312215\n",
      "Epoch 1 step 267: training accuarcy: 0.615\n",
      "Epoch 1 step 267: training loss: 1390.1119370827646\n",
      "Epoch 1 step 268: training accuarcy: 0.6125\n",
      "Epoch 1 step 268: training loss: 1389.4605440270732\n",
      "Epoch 1 step 269: training accuarcy: 0.63\n",
      "Epoch 1 step 269: training loss: 1369.4495607659544\n",
      "Epoch 1 step 270: training accuarcy: 0.6325000000000001\n",
      "Epoch 1 step 270: training loss: 1381.6163563773262\n",
      "Epoch 1 step 271: training accuarcy: 0.621\n",
      "Epoch 1 step 271: training loss: 1384.3328648470238\n",
      "Epoch 1 step 272: training accuarcy: 0.606\n",
      "Epoch 1 step 272: training loss: 1379.2392102268109\n",
      "Epoch 1 step 273: training accuarcy: 0.628\n",
      "Epoch 1 step 273: training loss: 1374.1226711203103\n",
      "Epoch 1 step 274: training accuarcy: 0.629\n",
      "Epoch 1 step 274: training loss: 1397.8406171850456\n",
      "Epoch 1 step 275: training accuarcy: 0.6035\n",
      "Epoch 1 step 275: training loss: 1389.7330763175748\n",
      "Epoch 1 step 276: training accuarcy: 0.6195\n",
      "Epoch 1 step 276: training loss: 1367.352262628753\n",
      "Epoch 1 step 277: training accuarcy: 0.636\n",
      "Epoch 1 step 277: training loss: 1388.2379627417906\n",
      "Epoch 1 step 278: training accuarcy: 0.607\n",
      "Epoch 1 step 278: training loss: 1383.645925300104\n",
      "Epoch 1 step 279: training accuarcy: 0.6195\n",
      "Epoch 1 step 279: training loss: 1371.667175120285\n",
      "Epoch 1 step 280: training accuarcy: 0.64\n",
      "Epoch 1 step 280: training loss: 1373.6159131570457\n",
      "Epoch 1 step 281: training accuarcy: 0.6305000000000001\n",
      "Epoch 1 step 281: training loss: 1375.5270134514822\n",
      "Epoch 1 step 282: training accuarcy: 0.611\n",
      "Epoch 1 step 282: training loss: 1356.377547656465\n",
      "Epoch 1 step 283: training accuarcy: 0.626\n",
      "Epoch 1 step 283: training loss: 1373.9253911367703\n",
      "Epoch 1 step 284: training accuarcy: 0.6335000000000001\n",
      "Epoch 1 step 284: training loss: 1363.9760350801046\n",
      "Epoch 1 step 285: training accuarcy: 0.6285000000000001\n",
      "Epoch 1 step 285: training loss: 1365.8559930646961\n",
      "Epoch 1 step 286: training accuarcy: 0.634\n",
      "Epoch 1 step 286: training loss: 1375.1733487674942\n",
      "Epoch 1 step 287: training accuarcy: 0.6245\n",
      "Epoch 1 step 287: training loss: 1378.5521789472468\n",
      "Epoch 1 step 288: training accuarcy: 0.621\n",
      "Epoch 1 step 288: training loss: 1364.180495516002\n",
      "Epoch 1 step 289: training accuarcy: 0.622\n",
      "Epoch 1 step 289: training loss: 1367.8327159727821\n",
      "Epoch 1 step 290: training accuarcy: 0.6125\n",
      "Epoch 1 step 290: training loss: 1356.6968945563406\n",
      "Epoch 1 step 291: training accuarcy: 0.626\n",
      "Epoch 1 step 291: training loss: 1373.4881520946667\n",
      "Epoch 1 step 292: training accuarcy: 0.611\n",
      "Epoch 1 step 292: training loss: 1361.3495294739287\n",
      "Epoch 1 step 293: training accuarcy: 0.626\n",
      "Epoch 1 step 293: training loss: 1400.8284692996415\n",
      "Epoch 1 step 294: training accuarcy: 0.595\n",
      "Epoch 1 step 294: training loss: 1359.0622220576097\n",
      "Epoch 1 step 295: training accuarcy: 0.6365000000000001\n",
      "Epoch 1 step 295: training loss: 1372.9043489807473\n",
      "Epoch 1 step 296: training accuarcy: 0.635\n",
      "Epoch 1 step 296: training loss: 1374.9652025916917\n",
      "Epoch 1 step 297: training accuarcy: 0.622\n",
      "Epoch 1 step 297: training loss: 1374.6163626634218\n",
      "Epoch 1 step 298: training accuarcy: 0.61\n",
      "Epoch 1 step 298: training loss: 1372.4483135928156\n",
      "Epoch 1 step 299: training accuarcy: 0.616\n",
      "Epoch 1 step 299: training loss: 1377.9681429403956\n",
      "Epoch 1 step 300: training accuarcy: 0.6125\n",
      "Epoch 1 step 300: training loss: 1375.4012781762442\n",
      "Epoch 1 step 301: training accuarcy: 0.611\n",
      "Epoch 1 step 301: training loss: 1372.5177849308627\n",
      "Epoch 1 step 302: training accuarcy: 0.5935\n",
      "Epoch 1 step 302: training loss: 1362.4161267356212\n",
      "Epoch 1 step 303: training accuarcy: 0.631\n",
      "Epoch 1 step 303: training loss: 1358.2589167270185\n",
      "Epoch 1 step 304: training accuarcy: 0.6135\n",
      "Epoch 1 step 304: training loss: 1367.730402402322\n",
      "Epoch 1 step 305: training accuarcy: 0.63\n",
      "Epoch 1 step 305: training loss: 1372.404972179277\n",
      "Epoch 1 step 306: training accuarcy: 0.61\n",
      "Epoch 1 step 306: training loss: 1358.542164143313\n",
      "Epoch 1 step 307: training accuarcy: 0.6165\n",
      "Epoch 1 step 307: training loss: 1352.268413207091\n",
      "Epoch 1 step 308: training accuarcy: 0.643\n",
      "Epoch 1 step 308: training loss: 1371.9223689738812\n",
      "Epoch 1 step 309: training accuarcy: 0.598\n",
      "Epoch 1 step 309: training loss: 1370.1403326816662\n",
      "Epoch 1 step 310: training accuarcy: 0.6035\n",
      "Epoch 1 step 310: training loss: 1352.1884368865103\n",
      "Epoch 1 step 311: training accuarcy: 0.63\n",
      "Epoch 1 step 311: training loss: 1383.4335589877612\n",
      "Epoch 1 step 312: training accuarcy: 0.6155\n",
      "Epoch 1 step 312: training loss: 1358.377739840924\n",
      "Epoch 1 step 313: training accuarcy: 0.621\n",
      "Epoch 1 step 313: training loss: 1378.4680133667366\n",
      "Epoch 1 step 314: training accuarcy: 0.6215\n",
      "Epoch 1 step 314: training loss: 1361.0768657421286\n",
      "Epoch 1 step 315: training accuarcy: 0.614\n",
      "Epoch 1 step 315: training loss: 1358.3741482674736\n",
      "Epoch 1 step 316: training accuarcy: 0.617\n",
      "Epoch 1 step 316: training loss: 1357.3174031888757\n",
      "Epoch 1 step 317: training accuarcy: 0.6235\n",
      "Epoch 1 step 317: training loss: 1349.8062161893674\n",
      "Epoch 1 step 318: training accuarcy: 0.641\n",
      "Epoch 1 step 318: training loss: 1355.073934708932\n",
      "Epoch 1 step 319: training accuarcy: 0.6205\n",
      "Epoch 1 step 319: training loss: 1359.7627916414083\n",
      "Epoch 1 step 320: training accuarcy: 0.6115\n",
      "Epoch 1 step 320: training loss: 1357.9350177347721\n",
      "Epoch 1 step 321: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 321: training loss: 1355.9559256693226\n",
      "Epoch 1 step 322: training accuarcy: 0.618\n",
      "Epoch 1 step 322: training loss: 1369.284397488796\n",
      "Epoch 1 step 323: training accuarcy: 0.606\n",
      "Epoch 1 step 323: training loss: 1354.8463606501996\n",
      "Epoch 1 step 324: training accuarcy: 0.6145\n",
      "Epoch 1 step 324: training loss: 1360.1209578485893\n",
      "Epoch 1 step 325: training accuarcy: 0.6255000000000001\n",
      "Epoch 1 step 325: training loss: 1359.046003920708\n",
      "Epoch 1 step 326: training accuarcy: 0.6145\n",
      "Epoch 1 step 326: training loss: 1357.4552525458098\n",
      "Epoch 1 step 327: training accuarcy: 0.621\n",
      "Epoch 1 step 327: training loss: 1362.1743756301414\n",
      "Epoch 1 step 328: training accuarcy: 0.6125\n",
      "Epoch 1 step 328: training loss: 1359.183071942249\n",
      "Epoch 1 step 329: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 329: training loss: 1371.0733592421736\n",
      "Epoch 1 step 330: training accuarcy: 0.625\n",
      "Epoch 1 step 330: training loss: 1371.416967866761\n",
      "Epoch 1 step 331: training accuarcy: 0.624\n",
      "Epoch 1 step 331: training loss: 1358.3289463918595\n",
      "Epoch 1 step 332: training accuarcy: 0.6105\n",
      "Epoch 1 step 332: training loss: 1361.2635779793632\n",
      "Epoch 1 step 333: training accuarcy: 0.616\n",
      "Epoch 1 step 333: training loss: 1357.6497205356457\n",
      "Epoch 1 step 334: training accuarcy: 0.6215\n",
      "Epoch 1 step 334: training loss: 1348.1838708970545\n",
      "Epoch 1 step 335: training accuarcy: 0.6235\n",
      "Epoch 1 step 335: training loss: 1351.3490676489473\n",
      "Epoch 1 step 336: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 336: training loss: 1342.4907039740829\n",
      "Epoch 1 step 337: training accuarcy: 0.6145\n",
      "Epoch 1 step 337: training loss: 1350.5139049957274\n",
      "Epoch 1 step 338: training accuarcy: 0.6175\n",
      "Epoch 1 step 338: training loss: 1348.443947530684\n",
      "Epoch 1 step 339: training accuarcy: 0.639\n",
      "Epoch 1 step 339: training loss: 1366.063521303887\n",
      "Epoch 1 step 340: training accuarcy: 0.6135\n",
      "Epoch 1 step 340: training loss: 1357.942304032719\n",
      "Epoch 1 step 341: training accuarcy: 0.6245\n",
      "Epoch 1 step 341: training loss: 1350.6836837993853\n",
      "Epoch 1 step 342: training accuarcy: 0.6135\n",
      "Epoch 1 step 342: training loss: 1345.7883701778055\n",
      "Epoch 1 step 343: training accuarcy: 0.6295000000000001\n",
      "Epoch 1 step 343: training loss: 1354.8152909264281\n",
      "Epoch 1 step 344: training accuarcy: 0.612\n",
      "Epoch 1 step 344: training loss: 1354.6562812267616\n",
      "Epoch 1 step 345: training accuarcy: 0.619\n",
      "Epoch 1 step 345: training loss: 1344.0662058153594\n",
      "Epoch 1 step 346: training accuarcy: 0.6245\n",
      "Epoch 1 step 346: training loss: 1354.1359132464727\n",
      "Epoch 1 step 347: training accuarcy: 0.623\n",
      "Epoch 1 step 347: training loss: 1350.5938547543287\n",
      "Epoch 1 step 348: training accuarcy: 0.62\n",
      "Epoch 1 step 348: training loss: 1362.7345614514804\n",
      "Epoch 1 step 349: training accuarcy: 0.601\n",
      "Epoch 1 step 349: training loss: 1345.5864651519819\n",
      "Epoch 1 step 350: training accuarcy: 0.632\n",
      "Epoch 1 step 350: training loss: 1350.0030504689466\n",
      "Epoch 1 step 351: training accuarcy: 0.6365000000000001\n",
      "Epoch 1 step 351: training loss: 1352.6804739748086\n",
      "Epoch 1 step 352: training accuarcy: 0.62\n",
      "Epoch 1 step 352: training loss: 1341.382358194217\n",
      "Epoch 1 step 353: training accuarcy: 0.6295000000000001\n",
      "Epoch 1 step 353: training loss: 1360.7295070557516\n",
      "Epoch 1 step 354: training accuarcy: 0.604\n",
      "Epoch 1 step 354: training loss: 1337.993355621862\n",
      "Epoch 1 step 355: training accuarcy: 0.648\n",
      "Epoch 1 step 355: training loss: 1344.1602835164963\n",
      "Epoch 1 step 356: training accuarcy: 0.6355000000000001\n",
      "Epoch 1 step 356: training loss: 1362.7220564903582\n",
      "Epoch 1 step 357: training accuarcy: 0.6075\n",
      "Epoch 1 step 357: training loss: 1346.4654535905236\n",
      "Epoch 1 step 358: training accuarcy: 0.622\n",
      "Epoch 1 step 358: training loss: 1347.0160193834233\n",
      "Epoch 1 step 359: training accuarcy: 0.626\n",
      "Epoch 1 step 359: training loss: 1350.6310904382653\n",
      "Epoch 1 step 360: training accuarcy: 0.609\n",
      "Epoch 1 step 360: training loss: 1341.8892617400827\n",
      "Epoch 1 step 361: training accuarcy: 0.6325000000000001\n",
      "Epoch 1 step 361: training loss: 1350.818023309109\n",
      "Epoch 1 step 362: training accuarcy: 0.615\n",
      "Epoch 1 step 362: training loss: 1339.7048337181677\n",
      "Epoch 1 step 363: training accuarcy: 0.622\n",
      "Epoch 1 step 363: training loss: 1353.1976087032833\n",
      "Epoch 1 step 364: training accuarcy: 0.616\n",
      "Epoch 1 step 364: training loss: 1344.986635774063\n",
      "Epoch 1 step 365: training accuarcy: 0.6315000000000001\n",
      "Epoch 1 step 365: training loss: 1350.2538458203067\n",
      "Epoch 1 step 366: training accuarcy: 0.633\n",
      "Epoch 1 step 366: training loss: 1338.8311014584006\n",
      "Epoch 1 step 367: training accuarcy: 0.6215\n",
      "Epoch 1 step 367: training loss: 1356.262855540216\n",
      "Epoch 1 step 368: training accuarcy: 0.601\n",
      "Epoch 1 step 368: training loss: 1357.643047352738\n",
      "Epoch 1 step 369: training accuarcy: 0.616\n",
      "Epoch 1 step 369: training loss: 1366.3813786697337\n",
      "Epoch 1 step 370: training accuarcy: 0.6105\n",
      "Epoch 1 step 370: training loss: 1352.0960305950093\n",
      "Epoch 1 step 371: training accuarcy: 0.6145\n",
      "Epoch 1 step 371: training loss: 1347.6286387465225\n",
      "Epoch 1 step 372: training accuarcy: 0.6155\n",
      "Epoch 1 step 372: training loss: 1336.2069400149728\n",
      "Epoch 1 step 373: training accuarcy: 0.6165\n",
      "Epoch 1 step 373: training loss: 1356.7368437904888\n",
      "Epoch 1 step 374: training accuarcy: 0.612\n",
      "Epoch 1 step 374: training loss: 1351.1534019051849\n",
      "Epoch 1 step 375: training accuarcy: 0.61\n",
      "Epoch 1 step 375: training loss: 1339.3468303537595\n",
      "Epoch 1 step 376: training accuarcy: 0.633\n",
      "Epoch 1 step 376: training loss: 1349.4915984059353\n",
      "Epoch 1 step 377: training accuarcy: 0.6135\n",
      "Epoch 1 step 377: training loss: 1358.4908676721916\n",
      "Epoch 1 step 378: training accuarcy: 0.608\n",
      "Epoch 1 step 378: training loss: 1358.6054850228988\n",
      "Epoch 1 step 379: training accuarcy: 0.6055\n",
      "Epoch 1 step 379: training loss: 1351.4710372474144\n",
      "Epoch 1 step 380: training accuarcy: 0.6145\n",
      "Epoch 1 step 380: training loss: 1344.7431379028208\n",
      "Epoch 1 step 381: training accuarcy: 0.6225\n",
      "Epoch 1 step 381: training loss: 1345.8460419638182\n",
      "Epoch 1 step 382: training accuarcy: 0.6245\n",
      "Epoch 1 step 382: training loss: 1343.5474999226476\n",
      "Epoch 1 step 383: training accuarcy: 0.6225\n",
      "Epoch 1 step 383: training loss: 1349.463799501872\n",
      "Epoch 1 step 384: training accuarcy: 0.614\n",
      "Epoch 1 step 384: training loss: 1354.515798137142\n",
      "Epoch 1 step 385: training accuarcy: 0.624\n",
      "Epoch 1 step 385: training loss: 1346.775210637404\n",
      "Epoch 1 step 386: training accuarcy: 0.6395000000000001\n",
      "Epoch 1 step 386: training loss: 1336.1895166340516\n",
      "Epoch 1 step 387: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 387: training loss: 1336.7429949134275\n",
      "Epoch 1 step 388: training accuarcy: 0.6405\n",
      "Epoch 1 step 388: training loss: 1325.9918044663086\n",
      "Epoch 1 step 389: training accuarcy: 0.6425\n",
      "Epoch 1 step 389: training loss: 1339.7916140788418\n",
      "Epoch 1 step 390: training accuarcy: 0.636\n",
      "Epoch 1 step 390: training loss: 1345.5525031589361\n",
      "Epoch 1 step 391: training accuarcy: 0.621\n",
      "Epoch 1 step 391: training loss: 1356.7306178713627\n",
      "Epoch 1 step 392: training accuarcy: 0.6135\n",
      "Epoch 1 step 392: training loss: 1347.9614918028813\n",
      "Epoch 1 step 393: training accuarcy: 0.6225\n",
      "Epoch 1 step 393: training loss: 1351.2574555614779\n",
      "Epoch 1 step 394: training accuarcy: 0.608\n",
      "Epoch 1 step 394: training loss: 1355.4218317079044\n",
      "Epoch 1 step 395: training accuarcy: 0.617\n",
      "Epoch 1 step 395: training loss: 1348.0098969161427\n",
      "Epoch 1 step 396: training accuarcy: 0.6015\n",
      "Epoch 1 step 396: training loss: 1345.1750405858295\n",
      "Epoch 1 step 397: training accuarcy: 0.624\n",
      "Epoch 1 step 397: training loss: 1333.6382055659158\n",
      "Epoch 1 step 398: training accuarcy: 0.6365000000000001\n",
      "Epoch 1 step 398: training loss: 1326.5423925525329\n",
      "Epoch 1 step 399: training accuarcy: 0.6445\n",
      "Epoch 1 step 399: training loss: 1349.656057293268\n",
      "Epoch 1 step 400: training accuarcy: 0.6185\n",
      "Epoch 1 step 400: training loss: 1352.3601563040693\n",
      "Epoch 1 step 401: training accuarcy: 0.634\n",
      "Epoch 1 step 401: training loss: 1351.833321460536\n",
      "Epoch 1 step 402: training accuarcy: 0.61\n",
      "Epoch 1 step 402: training loss: 1339.2736086488965\n",
      "Epoch 1 step 403: training accuarcy: 0.625\n",
      "Epoch 1 step 403: training loss: 1349.8936775811933\n",
      "Epoch 1 step 404: training accuarcy: 0.6125\n",
      "Epoch 1 step 404: training loss: 1331.8660492132715\n",
      "Epoch 1 step 405: training accuarcy: 0.627\n",
      "Epoch 1 step 405: training loss: 1338.79483317358\n",
      "Epoch 1 step 406: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 406: training loss: 1342.906208823844\n",
      "Epoch 1 step 407: training accuarcy: 0.6195\n",
      "Epoch 1 step 407: training loss: 1340.6636833796633\n",
      "Epoch 1 step 408: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 408: training loss: 1338.5319319617852\n",
      "Epoch 1 step 409: training accuarcy: 0.614\n",
      "Epoch 1 step 409: training loss: 1338.8752691949355\n",
      "Epoch 1 step 410: training accuarcy: 0.6295000000000001\n",
      "Epoch 1 step 410: training loss: 1337.5094444459494\n",
      "Epoch 1 step 411: training accuarcy: 0.623\n",
      "Epoch 1 step 411: training loss: 1335.9552297512441\n",
      "Epoch 1 step 412: training accuarcy: 0.6255000000000001\n",
      "Epoch 1 step 412: training loss: 1364.014121864197\n",
      "Epoch 1 step 413: training accuarcy: 0.604\n",
      "Epoch 1 step 413: training loss: 1349.0709040108159\n",
      "Epoch 1 step 414: training accuarcy: 0.6135\n",
      "Epoch 1 step 414: training loss: 1342.1977268628382\n",
      "Epoch 1 step 415: training accuarcy: 0.624\n",
      "Epoch 1 step 415: training loss: 1334.5075786115049\n",
      "Epoch 1 step 416: training accuarcy: 0.618\n",
      "Epoch 1 step 416: training loss: 1350.1045789961274\n",
      "Epoch 1 step 417: training accuarcy: 0.6005\n",
      "Epoch 1 step 417: training loss: 1337.9520974722984\n",
      "Epoch 1 step 418: training accuarcy: 0.619\n",
      "Epoch 1 step 418: training loss: 1338.9958602154786\n",
      "Epoch 1 step 419: training accuarcy: 0.626\n",
      "Epoch 1 step 419: training loss: 1335.1468129516143\n",
      "Epoch 1 step 420: training accuarcy: 0.624\n",
      "Epoch 1 step 420: training loss: 1337.3159286477264\n",
      "Epoch 1 step 421: training accuarcy: 0.6155\n",
      "Epoch 1 step 421: training loss: 1369.3609148480093\n",
      "Epoch 1 step 422: training accuarcy: 0.607\n",
      "Epoch 1 step 422: training loss: 1344.7072389725101\n",
      "Epoch 1 step 423: training accuarcy: 0.617\n",
      "Epoch 1 step 423: training loss: 1357.9791955225896\n",
      "Epoch 1 step 424: training accuarcy: 0.6065\n",
      "Epoch 1 step 424: training loss: 1350.1601732967977\n",
      "Epoch 1 step 425: training accuarcy: 0.6045\n",
      "Epoch 1 step 425: training loss: 1344.7353557946424\n",
      "Epoch 1 step 426: training accuarcy: 0.6145\n",
      "Epoch 1 step 426: training loss: 1348.0757088463777\n",
      "Epoch 1 step 427: training accuarcy: 0.6165\n",
      "Epoch 1 step 427: training loss: 1339.0048197366564\n",
      "Epoch 1 step 428: training accuarcy: 0.626\n",
      "Epoch 1 step 428: training loss: 1344.0571677986816\n",
      "Epoch 1 step 429: training accuarcy: 0.6175\n",
      "Epoch 1 step 429: training loss: 1327.5847208250004\n",
      "Epoch 1 step 430: training accuarcy: 0.639\n",
      "Epoch 1 step 430: training loss: 1343.4646805413227\n",
      "Epoch 1 step 431: training accuarcy: 0.6185\n",
      "Epoch 1 step 431: training loss: 1350.5155075557352\n",
      "Epoch 1 step 432: training accuarcy: 0.606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 432: training loss: 1360.0977067865258\n",
      "Epoch 1 step 433: training accuarcy: 0.6145\n",
      "Epoch 1 step 433: training loss: 1354.0424675205932\n",
      "Epoch 1 step 434: training accuarcy: 0.6\n",
      "Epoch 1 step 434: training loss: 1345.3582938085935\n",
      "Epoch 1 step 435: training accuarcy: 0.6225\n",
      "Epoch 1 step 435: training loss: 1330.94058782339\n",
      "Epoch 1 step 436: training accuarcy: 0.63\n",
      "Epoch 1 step 436: training loss: 1350.669443159442\n",
      "Epoch 1 step 437: training accuarcy: 0.6185\n",
      "Epoch 1 step 437: training loss: 1330.8795638415359\n",
      "Epoch 1 step 438: training accuarcy: 0.623\n",
      "Epoch 1 step 438: training loss: 1334.9357336393027\n",
      "Epoch 1 step 439: training accuarcy: 0.639\n",
      "Epoch 1 step 439: training loss: 1347.12579560194\n",
      "Epoch 1 step 440: training accuarcy: 0.6285000000000001\n",
      "Epoch 1 step 440: training loss: 1336.5620370040167\n",
      "Epoch 1 step 441: training accuarcy: 0.618\n",
      "Epoch 1 step 441: training loss: 1338.128626721537\n",
      "Epoch 1 step 442: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 442: training loss: 1360.5635529095312\n",
      "Epoch 1 step 443: training accuarcy: 0.6045\n",
      "Epoch 1 step 443: training loss: 1356.2596101730142\n",
      "Epoch 1 step 444: training accuarcy: 0.608\n",
      "Epoch 1 step 444: training loss: 1350.135179483325\n",
      "Epoch 1 step 445: training accuarcy: 0.6295000000000001\n",
      "Epoch 1 step 445: training loss: 1344.8570759589938\n",
      "Epoch 1 step 446: training accuarcy: 0.6305000000000001\n",
      "Epoch 1 step 446: training loss: 1343.3598070272894\n",
      "Epoch 1 step 447: training accuarcy: 0.6345000000000001\n",
      "Epoch 1 step 447: training loss: 1338.2402442452421\n",
      "Epoch 1 step 448: training accuarcy: 0.615\n",
      "Epoch 1 step 448: training loss: 1331.4016089221388\n",
      "Epoch 1 step 449: training accuarcy: 0.6215\n",
      "Epoch 1 step 449: training loss: 1344.3247287839179\n",
      "Epoch 1 step 450: training accuarcy: 0.622\n",
      "Epoch 1 step 450: training loss: 1342.121004686608\n",
      "Epoch 1 step 451: training accuarcy: 0.605\n",
      "Epoch 1 step 451: training loss: 1343.3554818017894\n",
      "Epoch 1 step 452: training accuarcy: 0.628\n",
      "Epoch 1 step 452: training loss: 1349.3534609436335\n",
      "Epoch 1 step 453: training accuarcy: 0.6315000000000001\n",
      "Epoch 1 step 453: training loss: 1348.8299563767926\n",
      "Epoch 1 step 454: training accuarcy: 0.6305000000000001\n",
      "Epoch 1 step 454: training loss: 1348.4743561619705\n",
      "Epoch 1 step 455: training accuarcy: 0.6185\n",
      "Epoch 1 step 455: training loss: 1338.7300590698947\n",
      "Epoch 1 step 456: training accuarcy: 0.615\n",
      "Epoch 1 step 456: training loss: 1339.830330913582\n",
      "Epoch 1 step 457: training accuarcy: 0.6155\n",
      "Epoch 1 step 457: training loss: 1344.938127753319\n",
      "Epoch 1 step 458: training accuarcy: 0.609\n",
      "Epoch 1 step 458: training loss: 1352.876011833795\n",
      "Epoch 1 step 459: training accuarcy: 0.6\n",
      "Epoch 1 step 459: training loss: 1343.436176530655\n",
      "Epoch 1 step 460: training accuarcy: 0.612\n",
      "Epoch 1 step 460: training loss: 1331.50971399535\n",
      "Epoch 1 step 461: training accuarcy: 0.6295000000000001\n",
      "Epoch 1 step 461: training loss: 1348.0687065619816\n",
      "Epoch 1 step 462: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 462: training loss: 1352.686905610257\n",
      "Epoch 1 step 463: training accuarcy: 0.617\n",
      "Epoch 1 step 463: training loss: 1324.7435580255456\n",
      "Epoch 1 step 464: training accuarcy: 0.631\n",
      "Epoch 1 step 464: training loss: 1349.696135466324\n",
      "Epoch 1 step 465: training accuarcy: 0.6085\n",
      "Epoch 1 step 465: training loss: 1346.3522088735426\n",
      "Epoch 1 step 466: training accuarcy: 0.6135\n",
      "Epoch 1 step 466: training loss: 1338.5489145573354\n",
      "Epoch 1 step 467: training accuarcy: 0.632\n",
      "Epoch 1 step 467: training loss: 1339.7169628894578\n",
      "Epoch 1 step 468: training accuarcy: 0.6165\n",
      "Epoch 1 step 468: training loss: 1342.306772105257\n",
      "Epoch 1 step 469: training accuarcy: 0.6095\n",
      "Epoch 1 step 469: training loss: 1349.0506591380524\n",
      "Epoch 1 step 470: training accuarcy: 0.6\n",
      "Epoch 1 step 470: training loss: 1338.2207743259976\n",
      "Epoch 1 step 471: training accuarcy: 0.6155\n",
      "Epoch 1 step 471: training loss: 1341.215273953567\n",
      "Epoch 1 step 472: training accuarcy: 0.624\n",
      "Epoch 1 step 472: training loss: 1342.9719011993802\n",
      "Epoch 1 step 473: training accuarcy: 0.613\n",
      "Epoch 1 step 473: training loss: 1331.9031989721732\n",
      "Epoch 1 step 474: training accuarcy: 0.619\n",
      "Epoch 1 step 474: training loss: 1350.3272122697174\n",
      "Epoch 1 step 475: training accuarcy: 0.617\n",
      "Epoch 1 step 475: training loss: 1341.117245297882\n",
      "Epoch 1 step 476: training accuarcy: 0.619\n",
      "Epoch 1 step 476: training loss: 1347.00973232023\n",
      "Epoch 1 step 477: training accuarcy: 0.6355000000000001\n",
      "Epoch 1 step 477: training loss: 1333.076021232571\n",
      "Epoch 1 step 478: training accuarcy: 0.614\n",
      "Epoch 1 step 478: training loss: 1348.3323100683615\n",
      "Epoch 1 step 479: training accuarcy: 0.617\n",
      "Epoch 1 step 479: training loss: 1345.260199229848\n",
      "Epoch 1 step 480: training accuarcy: 0.6035\n",
      "Epoch 1 step 480: training loss: 1348.6108942628819\n",
      "Epoch 1 step 481: training accuarcy: 0.6225\n",
      "Epoch 1 step 481: training loss: 1325.419891957659\n",
      "Epoch 1 step 482: training accuarcy: 0.6335000000000001\n",
      "Epoch 1 step 482: training loss: 1345.2256229472944\n",
      "Epoch 1 step 483: training accuarcy: 0.6095\n",
      "Epoch 1 step 483: training loss: 1356.9300857036333\n",
      "Epoch 1 step 484: training accuarcy: 0.598\n",
      "Epoch 1 step 484: training loss: 1336.0646360698734\n",
      "Epoch 1 step 485: training accuarcy: 0.623\n",
      "Epoch 1 step 485: training loss: 1342.623115525085\n",
      "Epoch 1 step 486: training accuarcy: 0.627\n",
      "Epoch 1 step 486: training loss: 1336.6633789549182\n",
      "Epoch 1 step 487: training accuarcy: 0.6105\n",
      "Epoch 1 step 487: training loss: 1329.5676944378326\n",
      "Epoch 1 step 488: training accuarcy: 0.629\n",
      "Epoch 1 step 488: training loss: 1348.9429720072794\n",
      "Epoch 1 step 489: training accuarcy: 0.62\n",
      "Epoch 1 step 489: training loss: 1341.279149024457\n",
      "Epoch 1 step 490: training accuarcy: 0.613\n",
      "Epoch 1 step 490: training loss: 1335.5207815210504\n",
      "Epoch 1 step 491: training accuarcy: 0.6215\n",
      "Epoch 1 step 491: training loss: 1347.4036172358335\n",
      "Epoch 1 step 492: training accuarcy: 0.62\n",
      "Epoch 1 step 492: training loss: 1342.0937850445648\n",
      "Epoch 1 step 493: training accuarcy: 0.6185\n",
      "Epoch 1 step 493: training loss: 1344.1626340148407\n",
      "Epoch 1 step 494: training accuarcy: 0.6095\n",
      "Epoch 1 step 494: training loss: 1343.2323757936526\n",
      "Epoch 1 step 495: training accuarcy: 0.616\n",
      "Epoch 1 step 495: training loss: 1350.0667797988272\n",
      "Epoch 1 step 496: training accuarcy: 0.607\n",
      "Epoch 1 step 496: training loss: 1324.4930863838576\n",
      "Epoch 1 step 497: training accuarcy: 0.6395000000000001\n",
      "Epoch 1 step 497: training loss: 1341.2265229222776\n",
      "Epoch 1 step 498: training accuarcy: 0.621\n",
      "Epoch 1 step 498: training loss: 1342.825550629416\n",
      "Epoch 1 step 499: training accuarcy: 0.613\n",
      "Epoch 1 step 499: training loss: 1363.0102365600048\n",
      "Epoch 1 step 500: training accuarcy: 0.598\n",
      "Epoch 1 step 500: training loss: 1352.2334777152073\n",
      "Epoch 1 step 501: training accuarcy: 0.595\n",
      "Epoch 1 step 501: training loss: 1349.0902522227732\n",
      "Epoch 1 step 502: training accuarcy: 0.6225\n",
      "Epoch 1 step 502: training loss: 1354.9992683229061\n",
      "Epoch 1 step 503: training accuarcy: 0.5985\n",
      "Epoch 1 step 503: training loss: 1342.0164618709643\n",
      "Epoch 1 step 504: training accuarcy: 0.6135\n",
      "Epoch 1 step 504: training loss: 1335.8647659828434\n",
      "Epoch 1 step 505: training accuarcy: 0.6045\n",
      "Epoch 1 step 505: training loss: 1335.3435538227316\n",
      "Epoch 1 step 506: training accuarcy: 0.6205\n",
      "Epoch 1 step 506: training loss: 1339.9316091978328\n",
      "Epoch 1 step 507: training accuarcy: 0.6175\n",
      "Epoch 1 step 507: training loss: 1343.956153235532\n",
      "Epoch 1 step 508: training accuarcy: 0.6285000000000001\n",
      "Epoch 1 step 508: training loss: 1340.3332967898914\n",
      "Epoch 1 step 509: training accuarcy: 0.615\n",
      "Epoch 1 step 509: training loss: 1346.3233584946372\n",
      "Epoch 1 step 510: training accuarcy: 0.6075\n",
      "Epoch 1 step 510: training loss: 1352.883216874006\n",
      "Epoch 1 step 511: training accuarcy: 0.6035\n",
      "Epoch 1 step 511: training loss: 1352.387797904514\n",
      "Epoch 1 step 512: training accuarcy: 0.594\n",
      "Epoch 1 step 512: training loss: 1346.7240057902254\n",
      "Epoch 1 step 513: training accuarcy: 0.616\n",
      "Epoch 1 step 513: training loss: 1336.5080955618384\n",
      "Epoch 1 step 514: training accuarcy: 0.6185\n",
      "Epoch 1 step 514: training loss: 1354.132967081046\n",
      "Epoch 1 step 515: training accuarcy: 0.6145\n",
      "Epoch 1 step 515: training loss: 1353.4402262106846\n",
      "Epoch 1 step 516: training accuarcy: 0.6165\n",
      "Epoch 1 step 516: training loss: 1341.6123214105423\n",
      "Epoch 1 step 517: training accuarcy: 0.6255000000000001\n",
      "Epoch 1 step 517: training loss: 1350.127417363914\n",
      "Epoch 1 step 518: training accuarcy: 0.6145\n",
      "Epoch 1 step 518: training loss: 1343.9128287177139\n",
      "Epoch 1 step 519: training accuarcy: 0.626\n",
      "Epoch 1 step 519: training loss: 1333.9341008406116\n",
      "Epoch 1 step 520: training accuarcy: 0.619\n",
      "Epoch 1 step 520: training loss: 1330.7228270484745\n",
      "Epoch 1 step 521: training accuarcy: 0.628\n",
      "Epoch 1 step 521: training loss: 1337.122572962741\n",
      "Epoch 1 step 522: training accuarcy: 0.613\n",
      "Epoch 1 step 522: training loss: 1336.1409456209517\n",
      "Epoch 1 step 523: training accuarcy: 0.6225\n",
      "Epoch 1 step 523: training loss: 1329.53963304701\n",
      "Epoch 1 step 524: training accuarcy: 0.6205\n",
      "Epoch 1 step 524: training loss: 1359.7894247154668\n",
      "Epoch 1 step 525: training accuarcy: 0.5975\n",
      "Epoch 1 step 525: training loss: 533.1210495761068\n",
      "Epoch 1 step 526: training accuarcy: 0.6217948717948718\n",
      "Epoch 1: train loss 1348.8648298213614, train accuarcy 0.6167920231819153\n",
      "Epoch 1: valid loss 6663.005302087583, valid accuarcy 0.5936529040336609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████████████████████████████▊                                                                                                       | 2/5 [09:45<14:39, 293.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 1331.1787062767032\n",
      "Epoch 2 step 527: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 527: training loss: 1342.4345398485145\n",
      "Epoch 2 step 528: training accuarcy: 0.6055\n",
      "Epoch 2 step 528: training loss: 1329.571049162581\n",
      "Epoch 2 step 529: training accuarcy: 0.625\n",
      "Epoch 2 step 529: training loss: 1332.1532642413129\n",
      "Epoch 2 step 530: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 530: training loss: 1335.4126864829227\n",
      "Epoch 2 step 531: training accuarcy: 0.6315000000000001\n",
      "Epoch 2 step 531: training loss: 1336.137793508341\n",
      "Epoch 2 step 532: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 532: training loss: 1339.4771086707353\n",
      "Epoch 2 step 533: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 533: training loss: 1329.8322244684296\n",
      "Epoch 2 step 534: training accuarcy: 0.632\n",
      "Epoch 2 step 534: training loss: 1334.6539118929163\n",
      "Epoch 2 step 535: training accuarcy: 0.631\n",
      "Epoch 2 step 535: training loss: 1339.7383493127218\n",
      "Epoch 2 step 536: training accuarcy: 0.6175\n",
      "Epoch 2 step 536: training loss: 1326.54586197195\n",
      "Epoch 2 step 537: training accuarcy: 0.624\n",
      "Epoch 2 step 537: training loss: 1347.6075096586799\n",
      "Epoch 2 step 538: training accuarcy: 0.629\n",
      "Epoch 2 step 538: training loss: 1340.657845963827\n",
      "Epoch 2 step 539: training accuarcy: 0.615\n",
      "Epoch 2 step 539: training loss: 1341.9010097305247\n",
      "Epoch 2 step 540: training accuarcy: 0.616\n",
      "Epoch 2 step 540: training loss: 1337.2735302209653\n",
      "Epoch 2 step 541: training accuarcy: 0.6205\n",
      "Epoch 2 step 541: training loss: 1343.8753091692724\n",
      "Epoch 2 step 542: training accuarcy: 0.6275000000000001\n",
      "Epoch 2 step 542: training loss: 1327.4193380044487\n",
      "Epoch 2 step 543: training accuarcy: 0.626\n",
      "Epoch 2 step 543: training loss: 1362.4989169831465\n",
      "Epoch 2 step 544: training accuarcy: 0.6025\n",
      "Epoch 2 step 544: training loss: 1348.5206582317583\n",
      "Epoch 2 step 545: training accuarcy: 0.6075\n",
      "Epoch 2 step 545: training loss: 1346.0789794526918\n",
      "Epoch 2 step 546: training accuarcy: 0.623\n",
      "Epoch 2 step 546: training loss: 1343.8483550692324\n",
      "Epoch 2 step 547: training accuarcy: 0.6125\n",
      "Epoch 2 step 547: training loss: 1351.8828836110222\n",
      "Epoch 2 step 548: training accuarcy: 0.608\n",
      "Epoch 2 step 548: training loss: 1352.9123974053898\n",
      "Epoch 2 step 549: training accuarcy: 0.624\n",
      "Epoch 2 step 549: training loss: 1353.286375324811\n",
      "Epoch 2 step 550: training accuarcy: 0.6045\n",
      "Epoch 2 step 550: training loss: 1333.1059485294743\n",
      "Epoch 2 step 551: training accuarcy: 0.617\n",
      "Epoch 2 step 551: training loss: 1343.417594084495\n",
      "Epoch 2 step 552: training accuarcy: 0.6205\n",
      "Epoch 2 step 552: training loss: 1349.4814925880664\n",
      "Epoch 2 step 553: training accuarcy: 0.6185\n",
      "Epoch 2 step 553: training loss: 1346.5944034733213\n",
      "Epoch 2 step 554: training accuarcy: 0.6125\n",
      "Epoch 2 step 554: training loss: 1343.3914233401563\n",
      "Epoch 2 step 555: training accuarcy: 0.6025\n",
      "Epoch 2 step 555: training loss: 1330.1281873506373\n",
      "Epoch 2 step 556: training accuarcy: 0.629\n",
      "Epoch 2 step 556: training loss: 1341.0025522214096\n",
      "Epoch 2 step 557: training accuarcy: 0.6105\n",
      "Epoch 2 step 557: training loss: 1340.5204914840142\n",
      "Epoch 2 step 558: training accuarcy: 0.624\n",
      "Epoch 2 step 558: training loss: 1342.3641244791056\n",
      "Epoch 2 step 559: training accuarcy: 0.6145\n",
      "Epoch 2 step 559: training loss: 1350.6987513396475\n",
      "Epoch 2 step 560: training accuarcy: 0.614\n",
      "Epoch 2 step 560: training loss: 1340.3925714137981\n",
      "Epoch 2 step 561: training accuarcy: 0.6065\n",
      "Epoch 2 step 561: training loss: 1353.8838599036092\n",
      "Epoch 2 step 562: training accuarcy: 0.6065\n",
      "Epoch 2 step 562: training loss: 1335.8458455158586\n",
      "Epoch 2 step 563: training accuarcy: 0.62\n",
      "Epoch 2 step 563: training loss: 1329.2715438698979\n",
      "Epoch 2 step 564: training accuarcy: 0.63\n",
      "Epoch 2 step 564: training loss: 1339.2485388258058\n",
      "Epoch 2 step 565: training accuarcy: 0.614\n",
      "Epoch 2 step 565: training loss: 1355.2683365350472\n",
      "Epoch 2 step 566: training accuarcy: 0.604\n",
      "Epoch 2 step 566: training loss: 1342.6144309582403\n",
      "Epoch 2 step 567: training accuarcy: 0.618\n",
      "Epoch 2 step 567: training loss: 1336.7828112516522\n",
      "Epoch 2 step 568: training accuarcy: 0.6175\n",
      "Epoch 2 step 568: training loss: 1348.7405443554642\n",
      "Epoch 2 step 569: training accuarcy: 0.6\n",
      "Epoch 2 step 569: training loss: 1323.2449546866947\n",
      "Epoch 2 step 570: training accuarcy: 0.642\n",
      "Epoch 2 step 570: training loss: 1355.8016955439025\n",
      "Epoch 2 step 571: training accuarcy: 0.6125\n",
      "Epoch 2 step 571: training loss: 1350.084134762363\n",
      "Epoch 2 step 572: training accuarcy: 0.6075\n",
      "Epoch 2 step 572: training loss: 1356.7015598994744\n",
      "Epoch 2 step 573: training accuarcy: 0.5975\n",
      "Epoch 2 step 573: training loss: 1332.6999039944992\n",
      "Epoch 2 step 574: training accuarcy: 0.628\n",
      "Epoch 2 step 574: training loss: 1332.8538650013327\n",
      "Epoch 2 step 575: training accuarcy: 0.6165\n",
      "Epoch 2 step 575: training loss: 1351.0244756691372\n",
      "Epoch 2 step 576: training accuarcy: 0.6115\n",
      "Epoch 2 step 576: training loss: 1342.2466908380866\n",
      "Epoch 2 step 577: training accuarcy: 0.62\n",
      "Epoch 2 step 577: training loss: 1329.9057750581485\n",
      "Epoch 2 step 578: training accuarcy: 0.632\n",
      "Epoch 2 step 578: training loss: 1340.787390879495\n",
      "Epoch 2 step 579: training accuarcy: 0.6155\n",
      "Epoch 2 step 579: training loss: 1342.15764697027\n",
      "Epoch 2 step 580: training accuarcy: 0.619\n",
      "Epoch 2 step 580: training loss: 1334.803574829613\n",
      "Epoch 2 step 581: training accuarcy: 0.6155\n",
      "Epoch 2 step 581: training loss: 1338.2003626654719\n",
      "Epoch 2 step 582: training accuarcy: 0.6225\n",
      "Epoch 2 step 582: training loss: 1346.2463164864032\n",
      "Epoch 2 step 583: training accuarcy: 0.6105\n",
      "Epoch 2 step 583: training loss: 1341.2924451487204\n",
      "Epoch 2 step 584: training accuarcy: 0.643\n",
      "Epoch 2 step 584: training loss: 1342.4764297498823\n",
      "Epoch 2 step 585: training accuarcy: 0.6135\n",
      "Epoch 2 step 585: training loss: 1355.0739535332266\n",
      "Epoch 2 step 586: training accuarcy: 0.6155\n",
      "Epoch 2 step 586: training loss: 1333.4921717281456\n",
      "Epoch 2 step 587: training accuarcy: 0.6335000000000001\n",
      "Epoch 2 step 587: training loss: 1340.4108126299504\n",
      "Epoch 2 step 588: training accuarcy: 0.6245\n",
      "Epoch 2 step 588: training loss: 1341.7966297957241\n",
      "Epoch 2 step 589: training accuarcy: 0.632\n",
      "Epoch 2 step 589: training loss: 1341.8168291850538\n",
      "Epoch 2 step 590: training accuarcy: 0.618\n",
      "Epoch 2 step 590: training loss: 1343.5177948972505\n",
      "Epoch 2 step 591: training accuarcy: 0.6155\n",
      "Epoch 2 step 591: training loss: 1347.728797559971\n",
      "Epoch 2 step 592: training accuarcy: 0.617\n",
      "Epoch 2 step 592: training loss: 1330.8568461397208\n",
      "Epoch 2 step 593: training accuarcy: 0.6355000000000001\n",
      "Epoch 2 step 593: training loss: 1341.12405672087\n",
      "Epoch 2 step 594: training accuarcy: 0.6175\n",
      "Epoch 2 step 594: training loss: 1349.3620531419135\n",
      "Epoch 2 step 595: training accuarcy: 0.616\n",
      "Epoch 2 step 595: training loss: 1337.2035118271622\n",
      "Epoch 2 step 596: training accuarcy: 0.632\n",
      "Epoch 2 step 596: training loss: 1336.7345743282385\n",
      "Epoch 2 step 597: training accuarcy: 0.618\n",
      "Epoch 2 step 597: training loss: 1342.2736920477057\n",
      "Epoch 2 step 598: training accuarcy: 0.616\n",
      "Epoch 2 step 598: training loss: 1357.1350932182395\n",
      "Epoch 2 step 599: training accuarcy: 0.602\n",
      "Epoch 2 step 599: training loss: 1333.0101141105197\n",
      "Epoch 2 step 600: training accuarcy: 0.636\n",
      "Epoch 2 step 600: training loss: 1326.1716949781526\n",
      "Epoch 2 step 601: training accuarcy: 0.634\n",
      "Epoch 2 step 601: training loss: 1330.6755746743415\n",
      "Epoch 2 step 602: training accuarcy: 0.6185\n",
      "Epoch 2 step 602: training loss: 1342.1485093767487\n",
      "Epoch 2 step 603: training accuarcy: 0.621\n",
      "Epoch 2 step 603: training loss: 1332.6264391723766\n",
      "Epoch 2 step 604: training accuarcy: 0.625\n",
      "Epoch 2 step 604: training loss: 1345.332086897162\n",
      "Epoch 2 step 605: training accuarcy: 0.635\n",
      "Epoch 2 step 605: training loss: 1343.524311271428\n",
      "Epoch 2 step 606: training accuarcy: 0.61\n",
      "Epoch 2 step 606: training loss: 1338.1155983919302\n",
      "Epoch 2 step 607: training accuarcy: 0.613\n",
      "Epoch 2 step 607: training loss: 1346.6616574600598\n",
      "Epoch 2 step 608: training accuarcy: 0.615\n",
      "Epoch 2 step 608: training loss: 1345.3255383134901\n",
      "Epoch 2 step 609: training accuarcy: 0.6185\n",
      "Epoch 2 step 609: training loss: 1328.6110465355414\n",
      "Epoch 2 step 610: training accuarcy: 0.64\n",
      "Epoch 2 step 610: training loss: 1343.0665348485447\n",
      "Epoch 2 step 611: training accuarcy: 0.622\n",
      "Epoch 2 step 611: training loss: 1339.7400933488777\n",
      "Epoch 2 step 612: training accuarcy: 0.617\n",
      "Epoch 2 step 612: training loss: 1346.1926480714103\n",
      "Epoch 2 step 613: training accuarcy: 0.61\n",
      "Epoch 2 step 613: training loss: 1338.9641023717554\n",
      "Epoch 2 step 614: training accuarcy: 0.6175\n",
      "Epoch 2 step 614: training loss: 1351.0460042024924\n",
      "Epoch 2 step 615: training accuarcy: 0.61\n",
      "Epoch 2 step 615: training loss: 1347.710669943678\n",
      "Epoch 2 step 616: training accuarcy: 0.605\n",
      "Epoch 2 step 616: training loss: 1328.5022021111406\n",
      "Epoch 2 step 617: training accuarcy: 0.63\n",
      "Epoch 2 step 617: training loss: 1337.2085881156104\n",
      "Epoch 2 step 618: training accuarcy: 0.635\n",
      "Epoch 2 step 618: training loss: 1337.8897439001616\n",
      "Epoch 2 step 619: training accuarcy: 0.633\n",
      "Epoch 2 step 619: training loss: 1336.2741810468929\n",
      "Epoch 2 step 620: training accuarcy: 0.635\n",
      "Epoch 2 step 620: training loss: 1329.7518425565256\n",
      "Epoch 2 step 621: training accuarcy: 0.624\n",
      "Epoch 2 step 621: training loss: 1332.8461153474686\n",
      "Epoch 2 step 622: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 622: training loss: 1333.2382654700527\n",
      "Epoch 2 step 623: training accuarcy: 0.6205\n",
      "Epoch 2 step 623: training loss: 1338.4823808519807\n",
      "Epoch 2 step 624: training accuarcy: 0.6195\n",
      "Epoch 2 step 624: training loss: 1337.2122159821902\n",
      "Epoch 2 step 625: training accuarcy: 0.612\n",
      "Epoch 2 step 625: training loss: 1335.62676769618\n",
      "Epoch 2 step 626: training accuarcy: 0.622\n",
      "Epoch 2 step 626: training loss: 1343.600359575341\n",
      "Epoch 2 step 627: training accuarcy: 0.61\n",
      "Epoch 2 step 627: training loss: 1343.4737420857064\n",
      "Epoch 2 step 628: training accuarcy: 0.6125\n",
      "Epoch 2 step 628: training loss: 1353.9878459237675\n",
      "Epoch 2 step 629: training accuarcy: 0.6\n",
      "Epoch 2 step 629: training loss: 1320.5797574663434\n",
      "Epoch 2 step 630: training accuarcy: 0.6435\n",
      "Epoch 2 step 630: training loss: 1327.1599510915796\n",
      "Epoch 2 step 631: training accuarcy: 0.6405\n",
      "Epoch 2 step 631: training loss: 1353.4969369646753\n",
      "Epoch 2 step 632: training accuarcy: 0.6105\n",
      "Epoch 2 step 632: training loss: 1338.5793600150823\n",
      "Epoch 2 step 633: training accuarcy: 0.633\n",
      "Epoch 2 step 633: training loss: 1349.990363028779\n",
      "Epoch 2 step 634: training accuarcy: 0.6105\n",
      "Epoch 2 step 634: training loss: 1339.0778159863319\n",
      "Epoch 2 step 635: training accuarcy: 0.629\n",
      "Epoch 2 step 635: training loss: 1347.492940251114\n",
      "Epoch 2 step 636: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 636: training loss: 1340.6986513984875\n",
      "Epoch 2 step 637: training accuarcy: 0.622\n",
      "Epoch 2 step 637: training loss: 1344.7078300533885\n",
      "Epoch 2 step 638: training accuarcy: 0.622\n",
      "Epoch 2 step 638: training loss: 1338.4194153760857\n",
      "Epoch 2 step 639: training accuarcy: 0.612\n",
      "Epoch 2 step 639: training loss: 1325.429053859724\n",
      "Epoch 2 step 640: training accuarcy: 0.629\n",
      "Epoch 2 step 640: training loss: 1341.2787520122001\n",
      "Epoch 2 step 641: training accuarcy: 0.614\n",
      "Epoch 2 step 641: training loss: 1338.4456739259765\n",
      "Epoch 2 step 642: training accuarcy: 0.6295000000000001\n",
      "Epoch 2 step 642: training loss: 1341.7278413392012\n",
      "Epoch 2 step 643: training accuarcy: 0.616\n",
      "Epoch 2 step 643: training loss: 1354.7646976090882\n",
      "Epoch 2 step 644: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 644: training loss: 1333.0005854213005\n",
      "Epoch 2 step 645: training accuarcy: 0.6205\n",
      "Epoch 2 step 645: training loss: 1340.2891434138512\n",
      "Epoch 2 step 646: training accuarcy: 0.6135\n",
      "Epoch 2 step 646: training loss: 1333.6735172786548\n",
      "Epoch 2 step 647: training accuarcy: 0.63\n",
      "Epoch 2 step 647: training loss: 1340.5323621177906\n",
      "Epoch 2 step 648: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 648: training loss: 1325.5813064609904\n",
      "Epoch 2 step 649: training accuarcy: 0.6365000000000001\n",
      "Epoch 2 step 649: training loss: 1331.011003420638\n",
      "Epoch 2 step 650: training accuarcy: 0.623\n",
      "Epoch 2 step 650: training loss: 1331.3741710576328\n",
      "Epoch 2 step 651: training accuarcy: 0.6345000000000001\n",
      "Epoch 2 step 651: training loss: 1350.8131583193453\n",
      "Epoch 2 step 652: training accuarcy: 0.6215\n",
      "Epoch 2 step 652: training loss: 1335.892773257433\n",
      "Epoch 2 step 653: training accuarcy: 0.6315000000000001\n",
      "Epoch 2 step 653: training loss: 1344.5717920651723\n",
      "Epoch 2 step 654: training accuarcy: 0.628\n",
      "Epoch 2 step 654: training loss: 1358.3710272364938\n",
      "Epoch 2 step 655: training accuarcy: 0.604\n",
      "Epoch 2 step 655: training loss: 1336.8032709896117\n",
      "Epoch 2 step 656: training accuarcy: 0.6335000000000001\n",
      "Epoch 2 step 656: training loss: 1335.8627312698359\n",
      "Epoch 2 step 657: training accuarcy: 0.6165\n",
      "Epoch 2 step 657: training loss: 1344.9725897287453\n",
      "Epoch 2 step 658: training accuarcy: 0.6215\n",
      "Epoch 2 step 658: training loss: 1338.516822245326\n",
      "Epoch 2 step 659: training accuarcy: 0.621\n",
      "Epoch 2 step 659: training loss: 1345.390775200355\n",
      "Epoch 2 step 660: training accuarcy: 0.6145\n",
      "Epoch 2 step 660: training loss: 1357.8247193737486\n",
      "Epoch 2 step 661: training accuarcy: 0.6065\n",
      "Epoch 2 step 661: training loss: 1351.1635305923814\n",
      "Epoch 2 step 662: training accuarcy: 0.5975\n",
      "Epoch 2 step 662: training loss: 1334.799148096756\n",
      "Epoch 2 step 663: training accuarcy: 0.608\n",
      "Epoch 2 step 663: training loss: 1338.9592918942135\n",
      "Epoch 2 step 664: training accuarcy: 0.629\n",
      "Epoch 2 step 664: training loss: 1351.4475711151772\n",
      "Epoch 2 step 665: training accuarcy: 0.6075\n",
      "Epoch 2 step 665: training loss: 1331.4043622414852\n",
      "Epoch 2 step 666: training accuarcy: 0.623\n",
      "Epoch 2 step 666: training loss: 1342.4270973322912\n",
      "Epoch 2 step 667: training accuarcy: 0.628\n",
      "Epoch 2 step 667: training loss: 1327.8601098125255\n",
      "Epoch 2 step 668: training accuarcy: 0.63\n",
      "Epoch 2 step 668: training loss: 1352.2243287833383\n",
      "Epoch 2 step 669: training accuarcy: 0.608\n",
      "Epoch 2 step 669: training loss: 1349.0408612800213\n",
      "Epoch 2 step 670: training accuarcy: 0.593\n",
      "Epoch 2 step 670: training loss: 1357.3689056027415\n",
      "Epoch 2 step 671: training accuarcy: 0.6\n",
      "Epoch 2 step 671: training loss: 1338.7148028174756\n",
      "Epoch 2 step 672: training accuarcy: 0.632\n",
      "Epoch 2 step 672: training loss: 1347.6804194096528\n",
      "Epoch 2 step 673: training accuarcy: 0.6235\n",
      "Epoch 2 step 673: training loss: 1344.2740727791177\n",
      "Epoch 2 step 674: training accuarcy: 0.614\n",
      "Epoch 2 step 674: training loss: 1326.320925636677\n",
      "Epoch 2 step 675: training accuarcy: 0.6435\n",
      "Epoch 2 step 675: training loss: 1349.79203940381\n",
      "Epoch 2 step 676: training accuarcy: 0.605\n",
      "Epoch 2 step 676: training loss: 1338.5176127739128\n",
      "Epoch 2 step 677: training accuarcy: 0.6155\n",
      "Epoch 2 step 677: training loss: 1346.4222794851946\n",
      "Epoch 2 step 678: training accuarcy: 0.614\n",
      "Epoch 2 step 678: training loss: 1337.808847969684\n",
      "Epoch 2 step 679: training accuarcy: 0.615\n",
      "Epoch 2 step 679: training loss: 1356.3013053781522\n",
      "Epoch 2 step 680: training accuarcy: 0.5905\n",
      "Epoch 2 step 680: training loss: 1350.0628341422198\n",
      "Epoch 2 step 681: training accuarcy: 0.613\n",
      "Epoch 2 step 681: training loss: 1340.421280660778\n",
      "Epoch 2 step 682: training accuarcy: 0.6135\n",
      "Epoch 2 step 682: training loss: 1339.5173526754295\n",
      "Epoch 2 step 683: training accuarcy: 0.627\n",
      "Epoch 2 step 683: training loss: 1327.8729967362494\n",
      "Epoch 2 step 684: training accuarcy: 0.6315000000000001\n",
      "Epoch 2 step 684: training loss: 1331.1391186110961\n",
      "Epoch 2 step 685: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 685: training loss: 1325.8327989728318\n",
      "Epoch 2 step 686: training accuarcy: 0.6315000000000001\n",
      "Epoch 2 step 686: training loss: 1346.2663820894545\n",
      "Epoch 2 step 687: training accuarcy: 0.616\n",
      "Epoch 2 step 687: training loss: 1332.3893765558462\n",
      "Epoch 2 step 688: training accuarcy: 0.636\n",
      "Epoch 2 step 688: training loss: 1338.9905837501256\n",
      "Epoch 2 step 689: training accuarcy: 0.614\n",
      "Epoch 2 step 689: training loss: 1356.992178665601\n",
      "Epoch 2 step 690: training accuarcy: 0.6105\n",
      "Epoch 2 step 690: training loss: 1348.3015707590769\n",
      "Epoch 2 step 691: training accuarcy: 0.6215\n",
      "Epoch 2 step 691: training loss: 1333.4774260402503\n",
      "Epoch 2 step 692: training accuarcy: 0.6395000000000001\n",
      "Epoch 2 step 692: training loss: 1331.8147664664334\n",
      "Epoch 2 step 693: training accuarcy: 0.6375000000000001\n",
      "Epoch 2 step 693: training loss: 1337.2636042267188\n",
      "Epoch 2 step 694: training accuarcy: 0.62\n",
      "Epoch 2 step 694: training loss: 1349.036415528568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 695: training accuarcy: 0.5965\n",
      "Epoch 2 step 695: training loss: 1336.9999174130585\n",
      "Epoch 2 step 696: training accuarcy: 0.607\n",
      "Epoch 2 step 696: training loss: 1339.6896405003527\n",
      "Epoch 2 step 697: training accuarcy: 0.6315000000000001\n",
      "Epoch 2 step 697: training loss: 1337.6300791459917\n",
      "Epoch 2 step 698: training accuarcy: 0.626\n",
      "Epoch 2 step 698: training loss: 1355.8023065213579\n",
      "Epoch 2 step 699: training accuarcy: 0.626\n",
      "Epoch 2 step 699: training loss: 1339.569092669095\n",
      "Epoch 2 step 700: training accuarcy: 0.6155\n",
      "Epoch 2 step 700: training loss: 1348.2755547057118\n",
      "Epoch 2 step 701: training accuarcy: 0.6075\n",
      "Epoch 2 step 701: training loss: 1328.0558919058712\n",
      "Epoch 2 step 702: training accuarcy: 0.643\n",
      "Epoch 2 step 702: training loss: 1353.701443400827\n",
      "Epoch 2 step 703: training accuarcy: 0.606\n",
      "Epoch 2 step 703: training loss: 1338.7242389048008\n",
      "Epoch 2 step 704: training accuarcy: 0.6145\n",
      "Epoch 2 step 704: training loss: 1335.4139622261562\n",
      "Epoch 2 step 705: training accuarcy: 0.6215\n",
      "Epoch 2 step 705: training loss: 1344.0022627077462\n",
      "Epoch 2 step 706: training accuarcy: 0.6105\n",
      "Epoch 2 step 706: training loss: 1338.6304991205282\n",
      "Epoch 2 step 707: training accuarcy: 0.613\n",
      "Epoch 2 step 707: training loss: 1326.4079971559418\n",
      "Epoch 2 step 708: training accuarcy: 0.636\n",
      "Epoch 2 step 708: training loss: 1347.7491476023717\n",
      "Epoch 2 step 709: training accuarcy: 0.601\n",
      "Epoch 2 step 709: training loss: 1327.9089942512048\n",
      "Epoch 2 step 710: training accuarcy: 0.642\n",
      "Epoch 2 step 710: training loss: 1338.154623353518\n",
      "Epoch 2 step 711: training accuarcy: 0.6185\n",
      "Epoch 2 step 711: training loss: 1353.0465666832924\n",
      "Epoch 2 step 712: training accuarcy: 0.62\n",
      "Epoch 2 step 712: training loss: 1364.9182233033223\n",
      "Epoch 2 step 713: training accuarcy: 0.606\n",
      "Epoch 2 step 713: training loss: 1328.2610029061248\n",
      "Epoch 2 step 714: training accuarcy: 0.625\n",
      "Epoch 2 step 714: training loss: 1347.5252815654667\n",
      "Epoch 2 step 715: training accuarcy: 0.6185\n",
      "Epoch 2 step 715: training loss: 1339.2382535199329\n",
      "Epoch 2 step 716: training accuarcy: 0.624\n",
      "Epoch 2 step 716: training loss: 1338.1035925503104\n",
      "Epoch 2 step 717: training accuarcy: 0.618\n",
      "Epoch 2 step 717: training loss: 1343.3105332123232\n",
      "Epoch 2 step 718: training accuarcy: 0.616\n",
      "Epoch 2 step 718: training loss: 1341.5922188225084\n",
      "Epoch 2 step 719: training accuarcy: 0.615\n",
      "Epoch 2 step 719: training loss: 1341.4316554719012\n",
      "Epoch 2 step 720: training accuarcy: 0.624\n",
      "Epoch 2 step 720: training loss: 1344.4973201547582\n",
      "Epoch 2 step 721: training accuarcy: 0.6095\n",
      "Epoch 2 step 721: training loss: 1339.5447385892255\n",
      "Epoch 2 step 722: training accuarcy: 0.6165\n",
      "Epoch 2 step 722: training loss: 1321.6382508155727\n",
      "Epoch 2 step 723: training accuarcy: 0.638\n",
      "Epoch 2 step 723: training loss: 1332.4733558170617\n",
      "Epoch 2 step 724: training accuarcy: 0.618\n",
      "Epoch 2 step 724: training loss: 1354.0641874385492\n",
      "Epoch 2 step 725: training accuarcy: 0.605\n",
      "Epoch 2 step 725: training loss: 1332.4345513645005\n",
      "Epoch 2 step 726: training accuarcy: 0.632\n",
      "Epoch 2 step 726: training loss: 1332.0591920622578\n",
      "Epoch 2 step 727: training accuarcy: 0.629\n",
      "Epoch 2 step 727: training loss: 1341.5327077184047\n",
      "Epoch 2 step 728: training accuarcy: 0.6065\n",
      "Epoch 2 step 728: training loss: 1345.134953217279\n",
      "Epoch 2 step 729: training accuarcy: 0.624\n",
      "Epoch 2 step 729: training loss: 1337.2010561166996\n",
      "Epoch 2 step 730: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 730: training loss: 1350.5257525404788\n",
      "Epoch 2 step 731: training accuarcy: 0.609\n",
      "Epoch 2 step 731: training loss: 1328.2253733988962\n",
      "Epoch 2 step 732: training accuarcy: 0.6275000000000001\n",
      "Epoch 2 step 732: training loss: 1350.647249401494\n",
      "Epoch 2 step 733: training accuarcy: 0.6145\n",
      "Epoch 2 step 733: training loss: 1341.0241847683005\n",
      "Epoch 2 step 734: training accuarcy: 0.6215\n",
      "Epoch 2 step 734: training loss: 1340.7512834284191\n",
      "Epoch 2 step 735: training accuarcy: 0.603\n",
      "Epoch 2 step 735: training loss: 1329.4233173059226\n",
      "Epoch 2 step 736: training accuarcy: 0.6385000000000001\n",
      "Epoch 2 step 736: training loss: 1334.9237064040015\n",
      "Epoch 2 step 737: training accuarcy: 0.63\n",
      "Epoch 2 step 737: training loss: 1334.9072661337332\n",
      "Epoch 2 step 738: training accuarcy: 0.62\n",
      "Epoch 2 step 738: training loss: 1336.3055250480256\n",
      "Epoch 2 step 739: training accuarcy: 0.6175\n",
      "Epoch 2 step 739: training loss: 1331.0895886434075\n",
      "Epoch 2 step 740: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 740: training loss: 1340.3346357886346\n",
      "Epoch 2 step 741: training accuarcy: 0.6085\n",
      "Epoch 2 step 741: training loss: 1340.3739730474906\n",
      "Epoch 2 step 742: training accuarcy: 0.616\n",
      "Epoch 2 step 742: training loss: 1340.1976430204234\n",
      "Epoch 2 step 743: training accuarcy: 0.6125\n",
      "Epoch 2 step 743: training loss: 1343.654788238623\n",
      "Epoch 2 step 744: training accuarcy: 0.621\n",
      "Epoch 2 step 744: training loss: 1330.5196934609073\n",
      "Epoch 2 step 745: training accuarcy: 0.6275000000000001\n",
      "Epoch 2 step 745: training loss: 1338.8489203753174\n",
      "Epoch 2 step 746: training accuarcy: 0.6125\n",
      "Epoch 2 step 746: training loss: 1358.3075732912532\n",
      "Epoch 2 step 747: training accuarcy: 0.6055\n",
      "Epoch 2 step 747: training loss: 1336.8910482033882\n",
      "Epoch 2 step 748: training accuarcy: 0.6165\n",
      "Epoch 2 step 748: training loss: 1321.548293233869\n",
      "Epoch 2 step 749: training accuarcy: 0.6375000000000001\n",
      "Epoch 2 step 749: training loss: 1340.1986583224493\n",
      "Epoch 2 step 750: training accuarcy: 0.619\n",
      "Epoch 2 step 750: training loss: 1343.7537415395245\n",
      "Epoch 2 step 751: training accuarcy: 0.6225\n",
      "Epoch 2 step 751: training loss: 1344.8490860423935\n",
      "Epoch 2 step 752: training accuarcy: 0.608\n",
      "Epoch 2 step 752: training loss: 1345.3656269039436\n",
      "Epoch 2 step 753: training accuarcy: 0.6155\n",
      "Epoch 2 step 753: training loss: 1349.4528038928354\n",
      "Epoch 2 step 754: training accuarcy: 0.61\n",
      "Epoch 2 step 754: training loss: 1357.1448187638798\n",
      "Epoch 2 step 755: training accuarcy: 0.602\n",
      "Epoch 2 step 755: training loss: 1335.8592888198846\n",
      "Epoch 2 step 756: training accuarcy: 0.625\n",
      "Epoch 2 step 756: training loss: 1342.379453642663\n",
      "Epoch 2 step 757: training accuarcy: 0.626\n",
      "Epoch 2 step 757: training loss: 1340.864342536108\n",
      "Epoch 2 step 758: training accuarcy: 0.6315000000000001\n",
      "Epoch 2 step 758: training loss: 1335.3652626270743\n",
      "Epoch 2 step 759: training accuarcy: 0.611\n",
      "Epoch 2 step 759: training loss: 1339.7043287080958\n",
      "Epoch 2 step 760: training accuarcy: 0.616\n",
      "Epoch 2 step 760: training loss: 1333.9353087122952\n",
      "Epoch 2 step 761: training accuarcy: 0.6185\n",
      "Epoch 2 step 761: training loss: 1336.8873755827349\n",
      "Epoch 2 step 762: training accuarcy: 0.6085\n",
      "Epoch 2 step 762: training loss: 1343.452126678688\n",
      "Epoch 2 step 763: training accuarcy: 0.6115\n",
      "Epoch 2 step 763: training loss: 1351.0896698769302\n",
      "Epoch 2 step 764: training accuarcy: 0.6075\n",
      "Epoch 2 step 764: training loss: 1346.884864062388\n",
      "Epoch 2 step 765: training accuarcy: 0.6105\n",
      "Epoch 2 step 765: training loss: 1341.9160899608398\n",
      "Epoch 2 step 766: training accuarcy: 0.64\n",
      "Epoch 2 step 766: training loss: 1342.3556768791354\n",
      "Epoch 2 step 767: training accuarcy: 0.6195\n",
      "Epoch 2 step 767: training loss: 1358.5157528240118\n",
      "Epoch 2 step 768: training accuarcy: 0.605\n",
      "Epoch 2 step 768: training loss: 1361.0848895677568\n",
      "Epoch 2 step 769: training accuarcy: 0.622\n",
      "Epoch 2 step 769: training loss: 1346.828979943603\n",
      "Epoch 2 step 770: training accuarcy: 0.612\n",
      "Epoch 2 step 770: training loss: 1344.0206349459795\n",
      "Epoch 2 step 771: training accuarcy: 0.613\n",
      "Epoch 2 step 771: training loss: 1346.3095381827904\n",
      "Epoch 2 step 772: training accuarcy: 0.6145\n",
      "Epoch 2 step 772: training loss: 1339.9743264719525\n",
      "Epoch 2 step 773: training accuarcy: 0.615\n",
      "Epoch 2 step 773: training loss: 1339.6270430862357\n",
      "Epoch 2 step 774: training accuarcy: 0.6245\n",
      "Epoch 2 step 774: training loss: 1327.9310471669328\n",
      "Epoch 2 step 775: training accuarcy: 0.6225\n",
      "Epoch 2 step 775: training loss: 1323.470637037783\n",
      "Epoch 2 step 776: training accuarcy: 0.6375000000000001\n",
      "Epoch 2 step 776: training loss: 1330.0818269601712\n",
      "Epoch 2 step 777: training accuarcy: 0.614\n",
      "Epoch 2 step 777: training loss: 1347.0653516285242\n",
      "Epoch 2 step 778: training accuarcy: 0.6235\n",
      "Epoch 2 step 778: training loss: 1351.6959243811561\n",
      "Epoch 2 step 779: training accuarcy: 0.613\n",
      "Epoch 2 step 779: training loss: 1349.752339863197\n",
      "Epoch 2 step 780: training accuarcy: 0.62\n",
      "Epoch 2 step 780: training loss: 1335.5917200921024\n",
      "Epoch 2 step 781: training accuarcy: 0.629\n",
      "Epoch 2 step 781: training loss: 1341.769326988012\n",
      "Epoch 2 step 782: training accuarcy: 0.616\n",
      "Epoch 2 step 782: training loss: 1330.5342381113387\n",
      "Epoch 2 step 783: training accuarcy: 0.635\n",
      "Epoch 2 step 783: training loss: 1339.6758767848899\n",
      "Epoch 2 step 784: training accuarcy: 0.631\n",
      "Epoch 2 step 784: training loss: 1341.9246594950996\n",
      "Epoch 2 step 785: training accuarcy: 0.6115\n",
      "Epoch 2 step 785: training loss: 1344.837432021713\n",
      "Epoch 2 step 786: training accuarcy: 0.6085\n",
      "Epoch 2 step 786: training loss: 1345.2651347760163\n",
      "Epoch 2 step 787: training accuarcy: 0.6145\n",
      "Epoch 2 step 787: training loss: 1341.0699078949551\n",
      "Epoch 2 step 788: training accuarcy: 0.6135\n",
      "Epoch 2 step 788: training loss: 538.631292376769\n",
      "Epoch 2 step 789: training accuarcy: 0.6153846153846154\n",
      "Epoch 2: train loss 1337.8034200382915, train accuarcy 0.6224855184555054\n",
      "Epoch 2: valid loss 6657.622671989344, valid accuarcy 0.5918335914611816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 3/5 [14:38<09:46, 293.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 1315.8605675423628\n",
      "Epoch 3 step 790: training accuarcy: 0.6445\n",
      "Epoch 3 step 790: training loss: 1336.7403449734009\n",
      "Epoch 3 step 791: training accuarcy: 0.6225\n",
      "Epoch 3 step 791: training loss: 1349.555964133602\n",
      "Epoch 3 step 792: training accuarcy: 0.614\n",
      "Epoch 3 step 792: training loss: 1360.144061465162\n",
      "Epoch 3 step 793: training accuarcy: 0.6115\n",
      "Epoch 3 step 793: training loss: 1360.5213440839862\n",
      "Epoch 3 step 794: training accuarcy: 0.596\n",
      "Epoch 3 step 794: training loss: 1339.9668445338452\n",
      "Epoch 3 step 795: training accuarcy: 0.627\n",
      "Epoch 3 step 795: training loss: 1328.4631547831843\n",
      "Epoch 3 step 796: training accuarcy: 0.647\n",
      "Epoch 3 step 796: training loss: 1334.6829829540227\n",
      "Epoch 3 step 797: training accuarcy: 0.639\n",
      "Epoch 3 step 797: training loss: 1339.5878852881563\n",
      "Epoch 3 step 798: training accuarcy: 0.6165\n",
      "Epoch 3 step 798: training loss: 1330.223708092958\n",
      "Epoch 3 step 799: training accuarcy: 0.6295000000000001\n",
      "Epoch 3 step 799: training loss: 1330.1245093733555\n",
      "Epoch 3 step 800: training accuarcy: 0.636\n",
      "Epoch 3 step 800: training loss: 1338.880648433624\n",
      "Epoch 3 step 801: training accuarcy: 0.6125\n",
      "Epoch 3 step 801: training loss: 1341.017596651169\n",
      "Epoch 3 step 802: training accuarcy: 0.628\n",
      "Epoch 3 step 802: training loss: 1351.5308999266172\n",
      "Epoch 3 step 803: training accuarcy: 0.6205\n",
      "Epoch 3 step 803: training loss: 1331.9309713845307\n",
      "Epoch 3 step 804: training accuarcy: 0.6205\n",
      "Epoch 3 step 804: training loss: 1342.122664167639\n",
      "Epoch 3 step 805: training accuarcy: 0.608\n",
      "Epoch 3 step 805: training loss: 1349.6018643660084\n",
      "Epoch 3 step 806: training accuarcy: 0.604\n",
      "Epoch 3 step 806: training loss: 1343.5350505091628\n",
      "Epoch 3 step 807: training accuarcy: 0.627\n",
      "Epoch 3 step 807: training loss: 1341.8489297235144\n",
      "Epoch 3 step 808: training accuarcy: 0.6125\n",
      "Epoch 3 step 808: training loss: 1323.9141993126937\n",
      "Epoch 3 step 809: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 809: training loss: 1325.8208046636105\n",
      "Epoch 3 step 810: training accuarcy: 0.625\n",
      "Epoch 3 step 810: training loss: 1327.5409718008743\n",
      "Epoch 3 step 811: training accuarcy: 0.6335000000000001\n",
      "Epoch 3 step 811: training loss: 1338.1795513409015\n",
      "Epoch 3 step 812: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 812: training loss: 1350.1560237577971\n",
      "Epoch 3 step 813: training accuarcy: 0.623\n",
      "Epoch 3 step 813: training loss: 1350.1692957373386\n",
      "Epoch 3 step 814: training accuarcy: 0.605\n",
      "Epoch 3 step 814: training loss: 1348.901738753245\n",
      "Epoch 3 step 815: training accuarcy: 0.614\n",
      "Epoch 3 step 815: training loss: 1353.6363336518054\n",
      "Epoch 3 step 816: training accuarcy: 0.6085\n",
      "Epoch 3 step 816: training loss: 1336.8566643010574\n",
      "Epoch 3 step 817: training accuarcy: 0.642\n",
      "Epoch 3 step 817: training loss: 1314.8843071436852\n",
      "Epoch 3 step 818: training accuarcy: 0.6475\n",
      "Epoch 3 step 818: training loss: 1339.0404469164475\n",
      "Epoch 3 step 819: training accuarcy: 0.631\n",
      "Epoch 3 step 819: training loss: 1361.569240662225\n",
      "Epoch 3 step 820: training accuarcy: 0.601\n",
      "Epoch 3 step 820: training loss: 1333.2048400307176\n",
      "Epoch 3 step 821: training accuarcy: 0.625\n",
      "Epoch 3 step 821: training loss: 1353.084597198244\n",
      "Epoch 3 step 822: training accuarcy: 0.6135\n",
      "Epoch 3 step 822: training loss: 1336.4317054027529\n",
      "Epoch 3 step 823: training accuarcy: 0.6385000000000001\n",
      "Epoch 3 step 823: training loss: 1335.4317419965087\n",
      "Epoch 3 step 824: training accuarcy: 0.632\n",
      "Epoch 3 step 824: training loss: 1327.2669215951682\n",
      "Epoch 3 step 825: training accuarcy: 0.6215\n",
      "Epoch 3 step 825: training loss: 1327.6474133546762\n",
      "Epoch 3 step 826: training accuarcy: 0.6485\n",
      "Epoch 3 step 826: training loss: 1343.6974006646933\n",
      "Epoch 3 step 827: training accuarcy: 0.6075\n",
      "Epoch 3 step 827: training loss: 1357.7804041263923\n",
      "Epoch 3 step 828: training accuarcy: 0.619\n",
      "Epoch 3 step 828: training loss: 1345.2229587445893\n",
      "Epoch 3 step 829: training accuarcy: 0.632\n",
      "Epoch 3 step 829: training loss: 1343.7704910398509\n",
      "Epoch 3 step 830: training accuarcy: 0.617\n",
      "Epoch 3 step 830: training loss: 1324.7790229100783\n",
      "Epoch 3 step 831: training accuarcy: 0.6415\n",
      "Epoch 3 step 831: training loss: 1339.7934393433536\n",
      "Epoch 3 step 832: training accuarcy: 0.6325000000000001\n",
      "Epoch 3 step 832: training loss: 1340.3304057933717\n",
      "Epoch 3 step 833: training accuarcy: 0.622\n",
      "Epoch 3 step 833: training loss: 1335.6914567790134\n",
      "Epoch 3 step 834: training accuarcy: 0.6185\n",
      "Epoch 3 step 834: training loss: 1341.2823910474028\n",
      "Epoch 3 step 835: training accuarcy: 0.6195\n",
      "Epoch 3 step 835: training loss: 1340.3616795101786\n",
      "Epoch 3 step 836: training accuarcy: 0.621\n",
      "Epoch 3 step 836: training loss: 1335.4181229070043\n",
      "Epoch 3 step 837: training accuarcy: 0.623\n",
      "Epoch 3 step 837: training loss: 1345.2457100324211\n",
      "Epoch 3 step 838: training accuarcy: 0.6285000000000001\n",
      "Epoch 3 step 838: training loss: 1348.1558848642858\n",
      "Epoch 3 step 839: training accuarcy: 0.618\n",
      "Epoch 3 step 839: training loss: 1333.231225182465\n",
      "Epoch 3 step 840: training accuarcy: 0.6295000000000001\n",
      "Epoch 3 step 840: training loss: 1342.6020293183112\n",
      "Epoch 3 step 841: training accuarcy: 0.6115\n",
      "Epoch 3 step 841: training loss: 1320.6180811504325\n",
      "Epoch 3 step 842: training accuarcy: 0.635\n",
      "Epoch 3 step 842: training loss: 1340.8070518613192\n",
      "Epoch 3 step 843: training accuarcy: 0.6065\n",
      "Epoch 3 step 843: training loss: 1331.9839270587536\n",
      "Epoch 3 step 844: training accuarcy: 0.625\n",
      "Epoch 3 step 844: training loss: 1340.5781673650797\n",
      "Epoch 3 step 845: training accuarcy: 0.62\n",
      "Epoch 3 step 845: training loss: 1349.289727019805\n",
      "Epoch 3 step 846: training accuarcy: 0.614\n",
      "Epoch 3 step 846: training loss: 1357.5391417422413\n",
      "Epoch 3 step 847: training accuarcy: 0.6135\n",
      "Epoch 3 step 847: training loss: 1335.8341169189857\n",
      "Epoch 3 step 848: training accuarcy: 0.632\n",
      "Epoch 3 step 848: training loss: 1336.3436898461719\n",
      "Epoch 3 step 849: training accuarcy: 0.625\n",
      "Epoch 3 step 849: training loss: 1335.2293343713702\n",
      "Epoch 3 step 850: training accuarcy: 0.6265000000000001\n",
      "Epoch 3 step 850: training loss: 1333.8860418559789\n",
      "Epoch 3 step 851: training accuarcy: 0.615\n",
      "Epoch 3 step 851: training loss: 1340.5761518231284\n",
      "Epoch 3 step 852: training accuarcy: 0.6205\n",
      "Epoch 3 step 852: training loss: 1335.467711894844\n",
      "Epoch 3 step 853: training accuarcy: 0.611\n",
      "Epoch 3 step 853: training loss: 1323.3292212507808\n",
      "Epoch 3 step 854: training accuarcy: 0.635\n",
      "Epoch 3 step 854: training loss: 1350.226997619878\n",
      "Epoch 3 step 855: training accuarcy: 0.6215\n",
      "Epoch 3 step 855: training loss: 1340.0415915346002\n",
      "Epoch 3 step 856: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 856: training loss: 1344.676460104479\n",
      "Epoch 3 step 857: training accuarcy: 0.6185\n",
      "Epoch 3 step 857: training loss: 1328.8694317991176\n",
      "Epoch 3 step 858: training accuarcy: 0.626\n",
      "Epoch 3 step 858: training loss: 1342.4680769800057\n",
      "Epoch 3 step 859: training accuarcy: 0.6155\n",
      "Epoch 3 step 859: training loss: 1346.6400239995926\n",
      "Epoch 3 step 860: training accuarcy: 0.6165\n",
      "Epoch 3 step 860: training loss: 1342.9225200494545\n",
      "Epoch 3 step 861: training accuarcy: 0.6345000000000001\n",
      "Epoch 3 step 861: training loss: 1344.3762287773955\n",
      "Epoch 3 step 862: training accuarcy: 0.622\n",
      "Epoch 3 step 862: training loss: 1354.396634106874\n",
      "Epoch 3 step 863: training accuarcy: 0.6115\n",
      "Epoch 3 step 863: training loss: 1341.4104965061938\n",
      "Epoch 3 step 864: training accuarcy: 0.621\n",
      "Epoch 3 step 864: training loss: 1323.1941927791338\n",
      "Epoch 3 step 865: training accuarcy: 0.6325000000000001\n",
      "Epoch 3 step 865: training loss: 1337.3022029005047\n",
      "Epoch 3 step 866: training accuarcy: 0.6385000000000001\n",
      "Epoch 3 step 866: training loss: 1349.5932443177794\n",
      "Epoch 3 step 867: training accuarcy: 0.629\n",
      "Epoch 3 step 867: training loss: 1332.2052523335258\n",
      "Epoch 3 step 868: training accuarcy: 0.633\n",
      "Epoch 3 step 868: training loss: 1346.265320360387\n",
      "Epoch 3 step 869: training accuarcy: 0.6205\n",
      "Epoch 3 step 869: training loss: 1331.5243546350832\n",
      "Epoch 3 step 870: training accuarcy: 0.6425\n",
      "Epoch 3 step 870: training loss: 1347.00915764558\n",
      "Epoch 3 step 871: training accuarcy: 0.627\n",
      "Epoch 3 step 871: training loss: 1321.4733789727693\n",
      "Epoch 3 step 872: training accuarcy: 0.6415\n",
      "Epoch 3 step 872: training loss: 1344.0377492666591\n",
      "Epoch 3 step 873: training accuarcy: 0.6115\n",
      "Epoch 3 step 873: training loss: 1334.6090750898409\n",
      "Epoch 3 step 874: training accuarcy: 0.628\n",
      "Epoch 3 step 874: training loss: 1347.1286227520072\n",
      "Epoch 3 step 875: training accuarcy: 0.63\n",
      "Epoch 3 step 875: training loss: 1348.8915129121847\n",
      "Epoch 3 step 876: training accuarcy: 0.605\n",
      "Epoch 3 step 876: training loss: 1346.896040071605\n",
      "Epoch 3 step 877: training accuarcy: 0.6145\n",
      "Epoch 3 step 877: training loss: 1340.0874653292485\n",
      "Epoch 3 step 878: training accuarcy: 0.624\n",
      "Epoch 3 step 878: training loss: 1334.4150601439812\n",
      "Epoch 3 step 879: training accuarcy: 0.621\n",
      "Epoch 3 step 879: training loss: 1339.6141234669908\n",
      "Epoch 3 step 880: training accuarcy: 0.619\n",
      "Epoch 3 step 880: training loss: 1337.8444096205515\n",
      "Epoch 3 step 881: training accuarcy: 0.623\n",
      "Epoch 3 step 881: training loss: 1347.095824937441\n",
      "Epoch 3 step 882: training accuarcy: 0.619\n",
      "Epoch 3 step 882: training loss: 1350.8186234852624\n",
      "Epoch 3 step 883: training accuarcy: 0.615\n",
      "Epoch 3 step 883: training loss: 1356.5577139750392\n",
      "Epoch 3 step 884: training accuarcy: 0.6145\n",
      "Epoch 3 step 884: training loss: 1334.715469679386\n",
      "Epoch 3 step 885: training accuarcy: 0.622\n",
      "Epoch 3 step 885: training loss: 1344.750549549238\n",
      "Epoch 3 step 886: training accuarcy: 0.6185\n",
      "Epoch 3 step 886: training loss: 1343.394149763433\n",
      "Epoch 3 step 887: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 887: training loss: 1349.632917030696\n",
      "Epoch 3 step 888: training accuarcy: 0.6185\n",
      "Epoch 3 step 888: training loss: 1348.6833296016946\n",
      "Epoch 3 step 889: training accuarcy: 0.6245\n",
      "Epoch 3 step 889: training loss: 1356.3553181405312\n",
      "Epoch 3 step 890: training accuarcy: 0.599\n",
      "Epoch 3 step 890: training loss: 1327.001700009739\n",
      "Epoch 3 step 891: training accuarcy: 0.624\n",
      "Epoch 3 step 891: training loss: 1338.8915459979844\n",
      "Epoch 3 step 892: training accuarcy: 0.625\n",
      "Epoch 3 step 892: training loss: 1341.5105350477002\n",
      "Epoch 3 step 893: training accuarcy: 0.6225\n",
      "Epoch 3 step 893: training loss: 1346.0437604852355\n",
      "Epoch 3 step 894: training accuarcy: 0.6035\n",
      "Epoch 3 step 894: training loss: 1345.5504125923517\n",
      "Epoch 3 step 895: training accuarcy: 0.6095\n",
      "Epoch 3 step 895: training loss: 1339.538982911897\n",
      "Epoch 3 step 896: training accuarcy: 0.615\n",
      "Epoch 3 step 896: training loss: 1342.1143761051278\n",
      "Epoch 3 step 897: training accuarcy: 0.613\n",
      "Epoch 3 step 897: training loss: 1348.4068510213874\n",
      "Epoch 3 step 898: training accuarcy: 0.6215\n",
      "Epoch 3 step 898: training loss: 1332.2134273427232\n",
      "Epoch 3 step 899: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 899: training loss: 1349.0397141830833\n",
      "Epoch 3 step 900: training accuarcy: 0.609\n",
      "Epoch 3 step 900: training loss: 1348.9120460280485\n",
      "Epoch 3 step 901: training accuarcy: 0.6105\n",
      "Epoch 3 step 901: training loss: 1350.2575323551941\n",
      "Epoch 3 step 902: training accuarcy: 0.61\n",
      "Epoch 3 step 902: training loss: 1340.4413373589855\n",
      "Epoch 3 step 903: training accuarcy: 0.6285000000000001\n",
      "Epoch 3 step 903: training loss: 1346.303205611217\n",
      "Epoch 3 step 904: training accuarcy: 0.6015\n",
      "Epoch 3 step 904: training loss: 1347.382364467183\n",
      "Epoch 3 step 905: training accuarcy: 0.609\n",
      "Epoch 3 step 905: training loss: 1335.443533898209\n",
      "Epoch 3 step 906: training accuarcy: 0.6325000000000001\n",
      "Epoch 3 step 906: training loss: 1354.2352732662846\n",
      "Epoch 3 step 907: training accuarcy: 0.607\n",
      "Epoch 3 step 907: training loss: 1340.878971929516\n",
      "Epoch 3 step 908: training accuarcy: 0.609\n",
      "Epoch 3 step 908: training loss: 1333.5688700590886\n",
      "Epoch 3 step 909: training accuarcy: 0.6315000000000001\n",
      "Epoch 3 step 909: training loss: 1339.7654389651425\n",
      "Epoch 3 step 910: training accuarcy: 0.625\n",
      "Epoch 3 step 910: training loss: 1325.3576021146894\n",
      "Epoch 3 step 911: training accuarcy: 0.6285000000000001\n",
      "Epoch 3 step 911: training loss: 1333.4789210304718\n",
      "Epoch 3 step 912: training accuarcy: 0.6365000000000001\n",
      "Epoch 3 step 912: training loss: 1337.1768433329823\n",
      "Epoch 3 step 913: training accuarcy: 0.622\n",
      "Epoch 3 step 913: training loss: 1341.140317749287\n",
      "Epoch 3 step 914: training accuarcy: 0.612\n",
      "Epoch 3 step 914: training loss: 1347.745259145248\n",
      "Epoch 3 step 915: training accuarcy: 0.6285000000000001\n",
      "Epoch 3 step 915: training loss: 1367.0010105192705\n",
      "Epoch 3 step 916: training accuarcy: 0.6165\n",
      "Epoch 3 step 916: training loss: 1331.1334642075908\n",
      "Epoch 3 step 917: training accuarcy: 0.614\n",
      "Epoch 3 step 917: training loss: 1340.0127964180674\n",
      "Epoch 3 step 918: training accuarcy: 0.621\n",
      "Epoch 3 step 918: training loss: 1342.044185678338\n",
      "Epoch 3 step 919: training accuarcy: 0.6045\n",
      "Epoch 3 step 919: training loss: 1363.3601388243874\n",
      "Epoch 3 step 920: training accuarcy: 0.5965\n",
      "Epoch 3 step 920: training loss: 1345.6961899871628\n",
      "Epoch 3 step 921: training accuarcy: 0.6295000000000001\n",
      "Epoch 3 step 921: training loss: 1348.4650756210715\n",
      "Epoch 3 step 922: training accuarcy: 0.6185\n",
      "Epoch 3 step 922: training loss: 1334.2736452814404\n",
      "Epoch 3 step 923: training accuarcy: 0.6235\n",
      "Epoch 3 step 923: training loss: 1330.0056571347566\n",
      "Epoch 3 step 924: training accuarcy: 0.632\n",
      "Epoch 3 step 924: training loss: 1347.1262094584915\n",
      "Epoch 3 step 925: training accuarcy: 0.608\n",
      "Epoch 3 step 925: training loss: 1332.3174454992363\n",
      "Epoch 3 step 926: training accuarcy: 0.6405\n",
      "Epoch 3 step 926: training loss: 1350.2074052969308\n",
      "Epoch 3 step 927: training accuarcy: 0.618\n",
      "Epoch 3 step 927: training loss: 1347.9158909841685\n",
      "Epoch 3 step 928: training accuarcy: 0.6105\n",
      "Epoch 3 step 928: training loss: 1337.8791828871374\n",
      "Epoch 3 step 929: training accuarcy: 0.6225\n",
      "Epoch 3 step 929: training loss: 1343.797691699529\n",
      "Epoch 3 step 930: training accuarcy: 0.629\n",
      "Epoch 3 step 930: training loss: 1350.7624412735956\n",
      "Epoch 3 step 931: training accuarcy: 0.612\n",
      "Epoch 3 step 931: training loss: 1348.1380783013944\n",
      "Epoch 3 step 932: training accuarcy: 0.617\n",
      "Epoch 3 step 932: training loss: 1334.1829050990177\n",
      "Epoch 3 step 933: training accuarcy: 0.623\n",
      "Epoch 3 step 933: training loss: 1351.9711032620905\n",
      "Epoch 3 step 934: training accuarcy: 0.6055\n",
      "Epoch 3 step 934: training loss: 1332.3435376670966\n",
      "Epoch 3 step 935: training accuarcy: 0.6355000000000001\n",
      "Epoch 3 step 935: training loss: 1345.1478142614153\n",
      "Epoch 3 step 936: training accuarcy: 0.6035\n",
      "Epoch 3 step 936: training loss: 1336.045133683432\n",
      "Epoch 3 step 937: training accuarcy: 0.614\n",
      "Epoch 3 step 937: training loss: 1352.1057927574018\n",
      "Epoch 3 step 938: training accuarcy: 0.6075\n",
      "Epoch 3 step 938: training loss: 1333.9130304306989\n",
      "Epoch 3 step 939: training accuarcy: 0.6335000000000001\n",
      "Epoch 3 step 939: training loss: 1345.8960467988093\n",
      "Epoch 3 step 940: training accuarcy: 0.6215\n",
      "Epoch 3 step 940: training loss: 1349.4446748720095\n",
      "Epoch 3 step 941: training accuarcy: 0.613\n",
      "Epoch 3 step 941: training loss: 1341.2954917105499\n",
      "Epoch 3 step 942: training accuarcy: 0.6325000000000001\n",
      "Epoch 3 step 942: training loss: 1344.8505046069185\n",
      "Epoch 3 step 943: training accuarcy: 0.6245\n",
      "Epoch 3 step 943: training loss: 1344.9102322240929\n",
      "Epoch 3 step 944: training accuarcy: 0.62\n",
      "Epoch 3 step 944: training loss: 1331.8841856727772\n",
      "Epoch 3 step 945: training accuarcy: 0.6225\n",
      "Epoch 3 step 945: training loss: 1348.0177909169083\n",
      "Epoch 3 step 946: training accuarcy: 0.6345000000000001\n",
      "Epoch 3 step 946: training loss: 1331.6540978909775\n",
      "Epoch 3 step 947: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 947: training loss: 1349.292955902074\n",
      "Epoch 3 step 948: training accuarcy: 0.6235\n",
      "Epoch 3 step 948: training loss: 1350.7273561223278\n",
      "Epoch 3 step 949: training accuarcy: 0.6115\n",
      "Epoch 3 step 949: training loss: 1348.532230070165\n",
      "Epoch 3 step 950: training accuarcy: 0.6125\n",
      "Epoch 3 step 950: training loss: 1340.2775985587218\n",
      "Epoch 3 step 951: training accuarcy: 0.616\n",
      "Epoch 3 step 951: training loss: 1334.2578112973229\n",
      "Epoch 3 step 952: training accuarcy: 0.612\n",
      "Epoch 3 step 952: training loss: 1344.706507558076\n",
      "Epoch 3 step 953: training accuarcy: 0.619\n",
      "Epoch 3 step 953: training loss: 1343.9908091096079\n",
      "Epoch 3 step 954: training accuarcy: 0.612\n",
      "Epoch 3 step 954: training loss: 1334.6250798658923\n",
      "Epoch 3 step 955: training accuarcy: 0.6335000000000001\n",
      "Epoch 3 step 955: training loss: 1344.0012204208213\n",
      "Epoch 3 step 956: training accuarcy: 0.622\n",
      "Epoch 3 step 956: training loss: 1333.5875003203466\n",
      "Epoch 3 step 957: training accuarcy: 0.6395000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 957: training loss: 1352.3688497546002\n",
      "Epoch 3 step 958: training accuarcy: 0.621\n",
      "Epoch 3 step 958: training loss: 1327.487915427688\n",
      "Epoch 3 step 959: training accuarcy: 0.6175\n",
      "Epoch 3 step 959: training loss: 1338.5488080333375\n",
      "Epoch 3 step 960: training accuarcy: 0.6125\n",
      "Epoch 3 step 960: training loss: 1351.4304674442187\n",
      "Epoch 3 step 961: training accuarcy: 0.6085\n",
      "Epoch 3 step 961: training loss: 1339.3711208486607\n",
      "Epoch 3 step 962: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 962: training loss: 1335.2871693526122\n",
      "Epoch 3 step 963: training accuarcy: 0.6245\n",
      "Epoch 3 step 963: training loss: 1356.1102947082109\n",
      "Epoch 3 step 964: training accuarcy: 0.613\n",
      "Epoch 3 step 964: training loss: 1355.3995785741363\n",
      "Epoch 3 step 965: training accuarcy: 0.606\n",
      "Epoch 3 step 965: training loss: 1337.9661806828433\n",
      "Epoch 3 step 966: training accuarcy: 0.621\n",
      "Epoch 3 step 966: training loss: 1346.6871976498844\n",
      "Epoch 3 step 967: training accuarcy: 0.6165\n",
      "Epoch 3 step 967: training loss: 1351.0115312493588\n",
      "Epoch 3 step 968: training accuarcy: 0.6095\n",
      "Epoch 3 step 968: training loss: 1342.1637904626566\n",
      "Epoch 3 step 969: training accuarcy: 0.6225\n",
      "Epoch 3 step 969: training loss: 1346.3183782705787\n",
      "Epoch 3 step 970: training accuarcy: 0.6185\n",
      "Epoch 3 step 970: training loss: 1346.3163712100354\n",
      "Epoch 3 step 971: training accuarcy: 0.6165\n",
      "Epoch 3 step 971: training loss: 1326.525713173456\n",
      "Epoch 3 step 972: training accuarcy: 0.6405\n",
      "Epoch 3 step 972: training loss: 1341.075380109292\n",
      "Epoch 3 step 973: training accuarcy: 0.62\n",
      "Epoch 3 step 973: training loss: 1348.8403875126105\n",
      "Epoch 3 step 974: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 974: training loss: 1346.0749520617196\n",
      "Epoch 3 step 975: training accuarcy: 0.624\n",
      "Epoch 3 step 975: training loss: 1342.1808363353475\n",
      "Epoch 3 step 976: training accuarcy: 0.619\n",
      "Epoch 3 step 976: training loss: 1336.8933083809397\n",
      "Epoch 3 step 977: training accuarcy: 0.614\n",
      "Epoch 3 step 977: training loss: 1334.6508965081816\n",
      "Epoch 3 step 978: training accuarcy: 0.6335000000000001\n",
      "Epoch 3 step 978: training loss: 1324.31768175273\n",
      "Epoch 3 step 979: training accuarcy: 0.645\n",
      "Epoch 3 step 979: training loss: 1331.1444555431988\n",
      "Epoch 3 step 980: training accuarcy: 0.631\n",
      "Epoch 3 step 980: training loss: 1336.090410339033\n",
      "Epoch 3 step 981: training accuarcy: 0.6165\n",
      "Epoch 3 step 981: training loss: 1352.0435479511468\n",
      "Epoch 3 step 982: training accuarcy: 0.6295000000000001\n",
      "Epoch 3 step 982: training loss: 1340.0604451530146\n",
      "Epoch 3 step 983: training accuarcy: 0.6185\n",
      "Epoch 3 step 983: training loss: 1332.5937017813715\n",
      "Epoch 3 step 984: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 984: training loss: 1343.6574688294784\n",
      "Epoch 3 step 985: training accuarcy: 0.6145\n",
      "Epoch 3 step 985: training loss: 1350.3288879298027\n",
      "Epoch 3 step 986: training accuarcy: 0.62\n",
      "Epoch 3 step 986: training loss: 1346.1554266525084\n",
      "Epoch 3 step 987: training accuarcy: 0.604\n",
      "Epoch 3 step 987: training loss: 1348.8146763235313\n",
      "Epoch 3 step 988: training accuarcy: 0.621\n",
      "Epoch 3 step 988: training loss: 1345.8144605491302\n",
      "Epoch 3 step 989: training accuarcy: 0.614\n",
      "Epoch 3 step 989: training loss: 1335.0431300465411\n",
      "Epoch 3 step 990: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 990: training loss: 1339.9815049180527\n",
      "Epoch 3 step 991: training accuarcy: 0.619\n",
      "Epoch 3 step 991: training loss: 1344.6988596092515\n",
      "Epoch 3 step 992: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 992: training loss: 1340.648183279046\n",
      "Epoch 3 step 993: training accuarcy: 0.627\n",
      "Epoch 3 step 993: training loss: 1335.7342234419457\n",
      "Epoch 3 step 994: training accuarcy: 0.6235\n",
      "Epoch 3 step 994: training loss: 1346.0673483372361\n",
      "Epoch 3 step 995: training accuarcy: 0.602\n",
      "Epoch 3 step 995: training loss: 1347.8363755229202\n",
      "Epoch 3 step 996: training accuarcy: 0.6075\n",
      "Epoch 3 step 996: training loss: 1341.0669456209528\n",
      "Epoch 3 step 997: training accuarcy: 0.613\n",
      "Epoch 3 step 997: training loss: 1346.139484898096\n",
      "Epoch 3 step 998: training accuarcy: 0.601\n",
      "Epoch 3 step 998: training loss: 1329.432887649036\n",
      "Epoch 3 step 999: training accuarcy: 0.63\n",
      "Epoch 3 step 999: training loss: 1344.324543092702\n",
      "Epoch 3 step 1000: training accuarcy: 0.623\n",
      "Epoch 3 step 1000: training loss: 1335.7113085820656\n",
      "Epoch 3 step 1001: training accuarcy: 0.615\n",
      "Epoch 3 step 1001: training loss: 1354.9012600368005\n",
      "Epoch 3 step 1002: training accuarcy: 0.605\n",
      "Epoch 3 step 1002: training loss: 1336.1556301394664\n",
      "Epoch 3 step 1003: training accuarcy: 0.6215\n",
      "Epoch 3 step 1003: training loss: 1345.6929348259152\n",
      "Epoch 3 step 1004: training accuarcy: 0.612\n",
      "Epoch 3 step 1004: training loss: 1344.9609879060083\n",
      "Epoch 3 step 1005: training accuarcy: 0.619\n",
      "Epoch 3 step 1005: training loss: 1325.123054504601\n",
      "Epoch 3 step 1006: training accuarcy: 0.6375000000000001\n",
      "Epoch 3 step 1006: training loss: 1348.2134974256924\n",
      "Epoch 3 step 1007: training accuarcy: 0.6115\n",
      "Epoch 3 step 1007: training loss: 1343.5044439041183\n",
      "Epoch 3 step 1008: training accuarcy: 0.6225\n",
      "Epoch 3 step 1008: training loss: 1346.1709887781378\n",
      "Epoch 3 step 1009: training accuarcy: 0.6115\n",
      "Epoch 3 step 1009: training loss: 1334.976330311977\n",
      "Epoch 3 step 1010: training accuarcy: 0.627\n",
      "Epoch 3 step 1010: training loss: 1358.944151744771\n",
      "Epoch 3 step 1011: training accuarcy: 0.6045\n",
      "Epoch 3 step 1011: training loss: 1332.673929374199\n",
      "Epoch 3 step 1012: training accuarcy: 0.626\n",
      "Epoch 3 step 1012: training loss: 1344.3092728439615\n",
      "Epoch 3 step 1013: training accuarcy: 0.618\n",
      "Epoch 3 step 1013: training loss: 1339.5392404994068\n",
      "Epoch 3 step 1014: training accuarcy: 0.6265000000000001\n",
      "Epoch 3 step 1014: training loss: 1335.0685434395064\n",
      "Epoch 3 step 1015: training accuarcy: 0.6365000000000001\n",
      "Epoch 3 step 1015: training loss: 1343.8222159822758\n",
      "Epoch 3 step 1016: training accuarcy: 0.6165\n",
      "Epoch 3 step 1016: training loss: 1349.90665051609\n",
      "Epoch 3 step 1017: training accuarcy: 0.612\n",
      "Epoch 3 step 1017: training loss: 1340.5518197693666\n",
      "Epoch 3 step 1018: training accuarcy: 0.6355000000000001\n",
      "Epoch 3 step 1018: training loss: 1347.9458284798168\n",
      "Epoch 3 step 1019: training accuarcy: 0.612\n",
      "Epoch 3 step 1019: training loss: 1352.506866500395\n",
      "Epoch 3 step 1020: training accuarcy: 0.6155\n",
      "Epoch 3 step 1020: training loss: 1340.7761902995169\n",
      "Epoch 3 step 1021: training accuarcy: 0.6105\n",
      "Epoch 3 step 1021: training loss: 1331.5584405742768\n",
      "Epoch 3 step 1022: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 1022: training loss: 1334.352205494845\n",
      "Epoch 3 step 1023: training accuarcy: 0.63\n",
      "Epoch 3 step 1023: training loss: 1356.3245895955615\n",
      "Epoch 3 step 1024: training accuarcy: 0.6005\n",
      "Epoch 3 step 1024: training loss: 1324.682065265298\n",
      "Epoch 3 step 1025: training accuarcy: 0.6375000000000001\n",
      "Epoch 3 step 1025: training loss: 1348.1968143301287\n",
      "Epoch 3 step 1026: training accuarcy: 0.6065\n",
      "Epoch 3 step 1026: training loss: 1336.9056564905702\n",
      "Epoch 3 step 1027: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 1027: training loss: 1344.85920909089\n",
      "Epoch 3 step 1028: training accuarcy: 0.61\n",
      "Epoch 3 step 1028: training loss: 1332.5154209595344\n",
      "Epoch 3 step 1029: training accuarcy: 0.623\n",
      "Epoch 3 step 1029: training loss: 1334.5840413574801\n",
      "Epoch 3 step 1030: training accuarcy: 0.6345000000000001\n",
      "Epoch 3 step 1030: training loss: 1349.4899374721883\n",
      "Epoch 3 step 1031: training accuarcy: 0.606\n",
      "Epoch 3 step 1031: training loss: 1329.7286404507424\n",
      "Epoch 3 step 1032: training accuarcy: 0.6405\n",
      "Epoch 3 step 1032: training loss: 1338.490287344731\n",
      "Epoch 3 step 1033: training accuarcy: 0.62\n",
      "Epoch 3 step 1033: training loss: 1353.8810343519297\n",
      "Epoch 3 step 1034: training accuarcy: 0.612\n",
      "Epoch 3 step 1034: training loss: 1328.7562991184402\n",
      "Epoch 3 step 1035: training accuarcy: 0.6225\n",
      "Epoch 3 step 1035: training loss: 1340.7573286964753\n",
      "Epoch 3 step 1036: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 1036: training loss: 1348.1169299002686\n",
      "Epoch 3 step 1037: training accuarcy: 0.613\n",
      "Epoch 3 step 1037: training loss: 1346.733575175413\n",
      "Epoch 3 step 1038: training accuarcy: 0.6135\n",
      "Epoch 3 step 1038: training loss: 1340.1394845228353\n",
      "Epoch 3 step 1039: training accuarcy: 0.6245\n",
      "Epoch 3 step 1039: training loss: 1354.238086135618\n",
      "Epoch 3 step 1040: training accuarcy: 0.6115\n",
      "Epoch 3 step 1040: training loss: 1349.949537943569\n",
      "Epoch 3 step 1041: training accuarcy: 0.6065\n",
      "Epoch 3 step 1041: training loss: 1346.6068398858097\n",
      "Epoch 3 step 1042: training accuarcy: 0.614\n",
      "Epoch 3 step 1042: training loss: 1362.0317297381189\n",
      "Epoch 3 step 1043: training accuarcy: 0.601\n",
      "Epoch 3 step 1043: training loss: 1331.7834129135363\n",
      "Epoch 3 step 1044: training accuarcy: 0.6205\n",
      "Epoch 3 step 1044: training loss: 1356.6146082031253\n",
      "Epoch 3 step 1045: training accuarcy: 0.6045\n",
      "Epoch 3 step 1045: training loss: 1341.5564318236966\n",
      "Epoch 3 step 1046: training accuarcy: 0.612\n",
      "Epoch 3 step 1046: training loss: 1337.4735504429684\n",
      "Epoch 3 step 1047: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 1047: training loss: 1339.6688044136006\n",
      "Epoch 3 step 1048: training accuarcy: 0.632\n",
      "Epoch 3 step 1048: training loss: 1338.1672917172882\n",
      "Epoch 3 step 1049: training accuarcy: 0.6315000000000001\n",
      "Epoch 3 step 1049: training loss: 1348.2503896894136\n",
      "Epoch 3 step 1050: training accuarcy: 0.5995\n",
      "Epoch 3 step 1050: training loss: 1331.1872548057167\n",
      "Epoch 3 step 1051: training accuarcy: 0.617\n",
      "Epoch 3 step 1051: training loss: 536.680579226246\n",
      "Epoch 3 step 1052: training accuarcy: 0.6358974358974359\n",
      "Epoch 3: train loss 1338.4778761969312, train accuarcy 0.619804322719574\n",
      "Epoch 3: valid loss 6680.409504609716, valid accuarcy 0.5861936807632446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 4/5 [19:30<04:52, 292.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 1347.1714314488063\n",
      "Epoch 4 step 1053: training accuarcy: 0.6175\n",
      "Epoch 4 step 1053: training loss: 1345.0089515616135\n",
      "Epoch 4 step 1054: training accuarcy: 0.617\n",
      "Epoch 4 step 1054: training loss: 1334.2267697433642\n",
      "Epoch 4 step 1055: training accuarcy: 0.6295000000000001\n",
      "Epoch 4 step 1055: training loss: 1335.8911666886925\n",
      "Epoch 4 step 1056: training accuarcy: 0.632\n",
      "Epoch 4 step 1056: training loss: 1343.3458264172255\n",
      "Epoch 4 step 1057: training accuarcy: 0.6175\n",
      "Epoch 4 step 1057: training loss: 1333.918976856889\n",
      "Epoch 4 step 1058: training accuarcy: 0.628\n",
      "Epoch 4 step 1058: training loss: 1339.7717908624807\n",
      "Epoch 4 step 1059: training accuarcy: 0.6195\n",
      "Epoch 4 step 1059: training loss: 1340.1125604707786\n",
      "Epoch 4 step 1060: training accuarcy: 0.6365000000000001\n",
      "Epoch 4 step 1060: training loss: 1350.3484880805643\n",
      "Epoch 4 step 1061: training accuarcy: 0.604\n",
      "Epoch 4 step 1061: training loss: 1341.2255989429673\n",
      "Epoch 4 step 1062: training accuarcy: 0.6355000000000001\n",
      "Epoch 4 step 1062: training loss: 1354.9623241748416\n",
      "Epoch 4 step 1063: training accuarcy: 0.602\n",
      "Epoch 4 step 1063: training loss: 1360.0071894316188\n",
      "Epoch 4 step 1064: training accuarcy: 0.6045\n",
      "Epoch 4 step 1064: training loss: 1336.8766748315577\n",
      "Epoch 4 step 1065: training accuarcy: 0.6245\n",
      "Epoch 4 step 1065: training loss: 1347.7698220983057\n",
      "Epoch 4 step 1066: training accuarcy: 0.6155\n",
      "Epoch 4 step 1066: training loss: 1364.6516705584252\n",
      "Epoch 4 step 1067: training accuarcy: 0.6165\n",
      "Epoch 4 step 1067: training loss: 1347.5497623545516\n",
      "Epoch 4 step 1068: training accuarcy: 0.6235\n",
      "Epoch 4 step 1068: training loss: 1350.2934462429023\n",
      "Epoch 4 step 1069: training accuarcy: 0.6095\n",
      "Epoch 4 step 1069: training loss: 1348.9530641654912\n",
      "Epoch 4 step 1070: training accuarcy: 0.617\n",
      "Epoch 4 step 1070: training loss: 1326.4214027391358\n",
      "Epoch 4 step 1071: training accuarcy: 0.6455\n",
      "Epoch 4 step 1071: training loss: 1362.4795237885548\n",
      "Epoch 4 step 1072: training accuarcy: 0.612\n",
      "Epoch 4 step 1072: training loss: 1335.1060886058715\n",
      "Epoch 4 step 1073: training accuarcy: 0.628\n",
      "Epoch 4 step 1073: training loss: 1347.120312936202\n",
      "Epoch 4 step 1074: training accuarcy: 0.626\n",
      "Epoch 4 step 1074: training loss: 1340.6598492556448\n",
      "Epoch 4 step 1075: training accuarcy: 0.632\n",
      "Epoch 4 step 1075: training loss: 1350.7766557507937\n",
      "Epoch 4 step 1076: training accuarcy: 0.6125\n",
      "Epoch 4 step 1076: training loss: 1356.426793475578\n",
      "Epoch 4 step 1077: training accuarcy: 0.6105\n",
      "Epoch 4 step 1077: training loss: 1331.6370648927798\n",
      "Epoch 4 step 1078: training accuarcy: 0.6295000000000001\n",
      "Epoch 4 step 1078: training loss: 1343.227658352693\n",
      "Epoch 4 step 1079: training accuarcy: 0.6205\n",
      "Epoch 4 step 1079: training loss: 1338.7773047239266\n",
      "Epoch 4 step 1080: training accuarcy: 0.631\n",
      "Epoch 4 step 1080: training loss: 1331.5404372307514\n",
      "Epoch 4 step 1081: training accuarcy: 0.638\n",
      "Epoch 4 step 1081: training loss: 1356.443159634555\n",
      "Epoch 4 step 1082: training accuarcy: 0.6055\n",
      "Epoch 4 step 1082: training loss: 1338.4579388017592\n",
      "Epoch 4 step 1083: training accuarcy: 0.6295000000000001\n",
      "Epoch 4 step 1083: training loss: 1326.6373679646454\n",
      "Epoch 4 step 1084: training accuarcy: 0.6385000000000001\n",
      "Epoch 4 step 1084: training loss: 1353.411742056663\n",
      "Epoch 4 step 1085: training accuarcy: 0.6175\n",
      "Epoch 4 step 1085: training loss: 1336.0461666180577\n",
      "Epoch 4 step 1086: training accuarcy: 0.649\n",
      "Epoch 4 step 1086: training loss: 1342.7156689962583\n",
      "Epoch 4 step 1087: training accuarcy: 0.6285000000000001\n",
      "Epoch 4 step 1087: training loss: 1335.8459892661515\n",
      "Epoch 4 step 1088: training accuarcy: 0.646\n",
      "Epoch 4 step 1088: training loss: 1353.2576140999652\n",
      "Epoch 4 step 1089: training accuarcy: 0.6155\n",
      "Epoch 4 step 1089: training loss: 1349.0119453255613\n",
      "Epoch 4 step 1090: training accuarcy: 0.612\n",
      "Epoch 4 step 1090: training loss: 1360.351315631437\n",
      "Epoch 4 step 1091: training accuarcy: 0.622\n",
      "Epoch 4 step 1091: training loss: 1341.1658188906215\n",
      "Epoch 4 step 1092: training accuarcy: 0.6225\n",
      "Epoch 4 step 1092: training loss: 1363.4069987770786\n",
      "Epoch 4 step 1093: training accuarcy: 0.6105\n",
      "Epoch 4 step 1093: training loss: 1354.0502619349702\n",
      "Epoch 4 step 1094: training accuarcy: 0.6235\n",
      "Epoch 4 step 1094: training loss: 1346.4882703655755\n",
      "Epoch 4 step 1095: training accuarcy: 0.6225\n",
      "Epoch 4 step 1095: training loss: 1348.160410636369\n",
      "Epoch 4 step 1096: training accuarcy: 0.6205\n",
      "Epoch 4 step 1096: training loss: 1357.6086179986044\n",
      "Epoch 4 step 1097: training accuarcy: 0.6045\n",
      "Epoch 4 step 1097: training loss: 1345.6427850601858\n",
      "Epoch 4 step 1098: training accuarcy: 0.6095\n",
      "Epoch 4 step 1098: training loss: 1348.6169536597056\n",
      "Epoch 4 step 1099: training accuarcy: 0.62\n",
      "Epoch 4 step 1099: training loss: 1334.5586390151734\n",
      "Epoch 4 step 1100: training accuarcy: 0.646\n",
      "Epoch 4 step 1100: training loss: 1326.0437111729168\n",
      "Epoch 4 step 1101: training accuarcy: 0.6265000000000001\n",
      "Epoch 4 step 1101: training loss: 1330.1177777141345\n",
      "Epoch 4 step 1102: training accuarcy: 0.646\n",
      "Epoch 4 step 1102: training loss: 1340.7122137783583\n",
      "Epoch 4 step 1103: training accuarcy: 0.622\n",
      "Epoch 4 step 1103: training loss: 1340.728260228017\n",
      "Epoch 4 step 1104: training accuarcy: 0.627\n",
      "Epoch 4 step 1104: training loss: 1348.7021933899991\n",
      "Epoch 4 step 1105: training accuarcy: 0.609\n",
      "Epoch 4 step 1105: training loss: 1347.1563974464361\n",
      "Epoch 4 step 1106: training accuarcy: 0.6215\n",
      "Epoch 4 step 1106: training loss: 1355.7994546984107\n",
      "Epoch 4 step 1107: training accuarcy: 0.6085\n",
      "Epoch 4 step 1107: training loss: 1342.9997166287717\n",
      "Epoch 4 step 1108: training accuarcy: 0.62\n",
      "Epoch 4 step 1108: training loss: 1344.3120851314159\n",
      "Epoch 4 step 1109: training accuarcy: 0.618\n",
      "Epoch 4 step 1109: training loss: 1346.2307475312696\n",
      "Epoch 4 step 1110: training accuarcy: 0.6235\n",
      "Epoch 4 step 1110: training loss: 1347.5584383808643\n",
      "Epoch 4 step 1111: training accuarcy: 0.6245\n",
      "Epoch 4 step 1111: training loss: 1347.4329255764394\n",
      "Epoch 4 step 1112: training accuarcy: 0.6255000000000001\n",
      "Epoch 4 step 1112: training loss: 1340.0562574864139\n",
      "Epoch 4 step 1113: training accuarcy: 0.625\n",
      "Epoch 4 step 1113: training loss: 1331.5814478648042\n",
      "Epoch 4 step 1114: training accuarcy: 0.631\n",
      "Epoch 4 step 1114: training loss: 1334.1275602627004\n",
      "Epoch 4 step 1115: training accuarcy: 0.6435\n",
      "Epoch 4 step 1115: training loss: 1328.5612029179706\n",
      "Epoch 4 step 1116: training accuarcy: 0.645\n",
      "Epoch 4 step 1116: training loss: 1344.6081536051947\n",
      "Epoch 4 step 1117: training accuarcy: 0.621\n",
      "Epoch 4 step 1117: training loss: 1341.1586501751617\n",
      "Epoch 4 step 1118: training accuarcy: 0.625\n",
      "Epoch 4 step 1118: training loss: 1347.904333218243\n",
      "Epoch 4 step 1119: training accuarcy: 0.632\n",
      "Epoch 4 step 1119: training loss: 1354.2120814537702\n",
      "Epoch 4 step 1120: training accuarcy: 0.601\n",
      "Epoch 4 step 1120: training loss: 1332.9848920567888\n",
      "Epoch 4 step 1121: training accuarcy: 0.628\n",
      "Epoch 4 step 1121: training loss: 1338.150163438359\n",
      "Epoch 4 step 1122: training accuarcy: 0.618\n",
      "Epoch 4 step 1122: training loss: 1330.6394469371987\n",
      "Epoch 4 step 1123: training accuarcy: 0.627\n",
      "Epoch 4 step 1123: training loss: 1358.0628907155576\n",
      "Epoch 4 step 1124: training accuarcy: 0.613\n",
      "Epoch 4 step 1124: training loss: 1336.2566362524099\n",
      "Epoch 4 step 1125: training accuarcy: 0.636\n",
      "Epoch 4 step 1125: training loss: 1339.7435054571386\n",
      "Epoch 4 step 1126: training accuarcy: 0.6145\n",
      "Epoch 4 step 1126: training loss: 1344.2646972918153\n",
      "Epoch 4 step 1127: training accuarcy: 0.634\n",
      "Epoch 4 step 1127: training loss: 1354.0308264607347\n",
      "Epoch 4 step 1128: training accuarcy: 0.6135\n",
      "Epoch 4 step 1128: training loss: 1339.9766920023535\n",
      "Epoch 4 step 1129: training accuarcy: 0.6175\n",
      "Epoch 4 step 1129: training loss: 1359.9811199616427\n",
      "Epoch 4 step 1130: training accuarcy: 0.6\n",
      "Epoch 4 step 1130: training loss: 1346.4640257104047\n",
      "Epoch 4 step 1131: training accuarcy: 0.619\n",
      "Epoch 4 step 1131: training loss: 1354.4142816941282\n",
      "Epoch 4 step 1132: training accuarcy: 0.622\n",
      "Epoch 4 step 1132: training loss: 1342.9803365464395\n",
      "Epoch 4 step 1133: training accuarcy: 0.629\n",
      "Epoch 4 step 1133: training loss: 1334.876725572709\n",
      "Epoch 4 step 1134: training accuarcy: 0.6345000000000001\n",
      "Epoch 4 step 1134: training loss: 1346.2249935910284\n",
      "Epoch 4 step 1135: training accuarcy: 0.608\n",
      "Epoch 4 step 1135: training loss: 1348.6889898837685\n",
      "Epoch 4 step 1136: training accuarcy: 0.613\n",
      "Epoch 4 step 1136: training loss: 1357.9108410462554\n",
      "Epoch 4 step 1137: training accuarcy: 0.606\n",
      "Epoch 4 step 1137: training loss: 1356.6359703906753\n",
      "Epoch 4 step 1138: training accuarcy: 0.6175\n",
      "Epoch 4 step 1138: training loss: 1345.11977578952\n",
      "Epoch 4 step 1139: training accuarcy: 0.622\n",
      "Epoch 4 step 1139: training loss: 1366.3503302111108\n",
      "Epoch 4 step 1140: training accuarcy: 0.602\n",
      "Epoch 4 step 1140: training loss: 1328.8874312209323\n",
      "Epoch 4 step 1141: training accuarcy: 0.6225\n",
      "Epoch 4 step 1141: training loss: 1339.0507855258206\n",
      "Epoch 4 step 1142: training accuarcy: 0.621\n",
      "Epoch 4 step 1142: training loss: 1342.3233048373402\n",
      "Epoch 4 step 1143: training accuarcy: 0.615\n",
      "Epoch 4 step 1143: training loss: 1340.9394964195642\n",
      "Epoch 4 step 1144: training accuarcy: 0.631\n",
      "Epoch 4 step 1144: training loss: 1330.1874487163245\n",
      "Epoch 4 step 1145: training accuarcy: 0.635\n",
      "Epoch 4 step 1145: training loss: 1350.9397533198\n",
      "Epoch 4 step 1146: training accuarcy: 0.619\n",
      "Epoch 4 step 1146: training loss: 1340.6959832464067\n",
      "Epoch 4 step 1147: training accuarcy: 0.634\n",
      "Epoch 4 step 1147: training loss: 1345.71634053763\n",
      "Epoch 4 step 1148: training accuarcy: 0.629\n",
      "Epoch 4 step 1148: training loss: 1350.8062524058334\n",
      "Epoch 4 step 1149: training accuarcy: 0.624\n",
      "Epoch 4 step 1149: training loss: 1357.2302727820413\n",
      "Epoch 4 step 1150: training accuarcy: 0.6045\n",
      "Epoch 4 step 1150: training loss: 1352.1005642976243\n",
      "Epoch 4 step 1151: training accuarcy: 0.617\n",
      "Epoch 4 step 1151: training loss: 1336.1313944024794\n",
      "Epoch 4 step 1152: training accuarcy: 0.6295000000000001\n",
      "Epoch 4 step 1152: training loss: 1339.4813387887802\n",
      "Epoch 4 step 1153: training accuarcy: 0.6245\n",
      "Epoch 4 step 1153: training loss: 1344.3671824510986\n",
      "Epoch 4 step 1154: training accuarcy: 0.629\n",
      "Epoch 4 step 1154: training loss: 1347.2668718406794\n",
      "Epoch 4 step 1155: training accuarcy: 0.61\n",
      "Epoch 4 step 1155: training loss: 1325.8086180662688\n",
      "Epoch 4 step 1156: training accuarcy: 0.6355000000000001\n",
      "Epoch 4 step 1156: training loss: 1342.973428489767\n",
      "Epoch 4 step 1157: training accuarcy: 0.638\n",
      "Epoch 4 step 1157: training loss: 1344.830780895994\n",
      "Epoch 4 step 1158: training accuarcy: 0.619\n",
      "Epoch 4 step 1158: training loss: 1350.8980487079484\n",
      "Epoch 4 step 1159: training accuarcy: 0.6185\n",
      "Epoch 4 step 1159: training loss: 1342.2410062235447\n",
      "Epoch 4 step 1160: training accuarcy: 0.612\n",
      "Epoch 4 step 1160: training loss: 1345.153705459142\n",
      "Epoch 4 step 1161: training accuarcy: 0.6195\n",
      "Epoch 4 step 1161: training loss: 1345.5537068513866\n",
      "Epoch 4 step 1162: training accuarcy: 0.6\n",
      "Epoch 4 step 1162: training loss: 1350.8501913026876\n",
      "Epoch 4 step 1163: training accuarcy: 0.6135\n",
      "Epoch 4 step 1163: training loss: 1343.571782151097\n",
      "Epoch 4 step 1164: training accuarcy: 0.62\n",
      "Epoch 4 step 1164: training loss: 1342.0631672204574\n",
      "Epoch 4 step 1165: training accuarcy: 0.63\n",
      "Epoch 4 step 1165: training loss: 1332.3245900050006\n",
      "Epoch 4 step 1166: training accuarcy: 0.6255000000000001\n",
      "Epoch 4 step 1166: training loss: 1338.446017437489\n",
      "Epoch 4 step 1167: training accuarcy: 0.6195\n",
      "Epoch 4 step 1167: training loss: 1361.1515666774012\n",
      "Epoch 4 step 1168: training accuarcy: 0.603\n",
      "Epoch 4 step 1168: training loss: 1343.1283371339696\n",
      "Epoch 4 step 1169: training accuarcy: 0.6255000000000001\n",
      "Epoch 4 step 1169: training loss: 1342.2035135421065\n",
      "Epoch 4 step 1170: training accuarcy: 0.6275000000000001\n",
      "Epoch 4 step 1170: training loss: 1343.600433358872\n",
      "Epoch 4 step 1171: training accuarcy: 0.628\n",
      "Epoch 4 step 1171: training loss: 1354.755524363342\n",
      "Epoch 4 step 1172: training accuarcy: 0.617\n",
      "Epoch 4 step 1172: training loss: 1346.149366886872\n",
      "Epoch 4 step 1173: training accuarcy: 0.626\n",
      "Epoch 4 step 1173: training loss: 1343.6996701787282\n",
      "Epoch 4 step 1174: training accuarcy: 0.618\n",
      "Epoch 4 step 1174: training loss: 1342.3793232374426\n",
      "Epoch 4 step 1175: training accuarcy: 0.6275000000000001\n",
      "Epoch 4 step 1175: training loss: 1344.3896294936164\n",
      "Epoch 4 step 1176: training accuarcy: 0.6255000000000001\n",
      "Epoch 4 step 1176: training loss: 1351.448911968794\n",
      "Epoch 4 step 1177: training accuarcy: 0.617\n",
      "Epoch 4 step 1177: training loss: 1342.9708704887712\n",
      "Epoch 4 step 1178: training accuarcy: 0.6385000000000001\n",
      "Epoch 4 step 1178: training loss: 1339.5614083211876\n",
      "Epoch 4 step 1179: training accuarcy: 0.631\n",
      "Epoch 4 step 1179: training loss: 1342.7625246777345\n",
      "Epoch 4 step 1180: training accuarcy: 0.636\n",
      "Epoch 4 step 1180: training loss: 1340.5641510838354\n",
      "Epoch 4 step 1181: training accuarcy: 0.6355000000000001\n",
      "Epoch 4 step 1181: training loss: 1343.5602529591276\n",
      "Epoch 4 step 1182: training accuarcy: 0.641\n",
      "Epoch 4 step 1182: training loss: 1338.7590833445993\n",
      "Epoch 4 step 1183: training accuarcy: 0.632\n",
      "Epoch 4 step 1183: training loss: 1355.0123669828954\n",
      "Epoch 4 step 1184: training accuarcy: 0.6075\n",
      "Epoch 4 step 1184: training loss: 1335.6884707473382\n",
      "Epoch 4 step 1185: training accuarcy: 0.639\n",
      "Epoch 4 step 1185: training loss: 1358.4006886096818\n",
      "Epoch 4 step 1186: training accuarcy: 0.6175\n",
      "Epoch 4 step 1186: training loss: 1348.8913923391924\n",
      "Epoch 4 step 1187: training accuarcy: 0.6155\n",
      "Epoch 4 step 1187: training loss: 1359.2172705079538\n",
      "Epoch 4 step 1188: training accuarcy: 0.6065\n",
      "Epoch 4 step 1188: training loss: 1331.2225681198179\n",
      "Epoch 4 step 1189: training accuarcy: 0.6305000000000001\n",
      "Epoch 4 step 1189: training loss: 1340.039501060361\n",
      "Epoch 4 step 1190: training accuarcy: 0.6085\n",
      "Epoch 4 step 1190: training loss: 1329.7633213124732\n",
      "Epoch 4 step 1191: training accuarcy: 0.63\n",
      "Epoch 4 step 1191: training loss: 1345.3592802952885\n",
      "Epoch 4 step 1192: training accuarcy: 0.6275000000000001\n",
      "Epoch 4 step 1192: training loss: 1349.0716258326365\n",
      "Epoch 4 step 1193: training accuarcy: 0.6225\n",
      "Epoch 4 step 1193: training loss: 1350.150094745626\n",
      "Epoch 4 step 1194: training accuarcy: 0.6235\n",
      "Epoch 4 step 1194: training loss: 1337.764341325558\n",
      "Epoch 4 step 1195: training accuarcy: 0.6385000000000001\n",
      "Epoch 4 step 1195: training loss: 1339.1448764308343\n",
      "Epoch 4 step 1196: training accuarcy: 0.6235\n",
      "Epoch 4 step 1196: training loss: 1349.8440528307874\n",
      "Epoch 4 step 1197: training accuarcy: 0.633\n",
      "Epoch 4 step 1197: training loss: 1359.542317543953\n",
      "Epoch 4 step 1198: training accuarcy: 0.587\n",
      "Epoch 4 step 1198: training loss: 1339.955291971953\n",
      "Epoch 4 step 1199: training accuarcy: 0.6285000000000001\n",
      "Epoch 4 step 1199: training loss: 1335.199280826829\n",
      "Epoch 4 step 1200: training accuarcy: 0.633\n",
      "Epoch 4 step 1200: training loss: 1345.221382649243\n",
      "Epoch 4 step 1201: training accuarcy: 0.619\n",
      "Epoch 4 step 1201: training loss: 1356.6721017685531\n",
      "Epoch 4 step 1202: training accuarcy: 0.6135\n",
      "Epoch 4 step 1202: training loss: 1349.7293011094278\n",
      "Epoch 4 step 1203: training accuarcy: 0.6295000000000001\n",
      "Epoch 4 step 1203: training loss: 1348.8230969911776\n",
      "Epoch 4 step 1204: training accuarcy: 0.6235\n",
      "Epoch 4 step 1204: training loss: 1337.3458539608696\n",
      "Epoch 4 step 1205: training accuarcy: 0.6325000000000001\n",
      "Epoch 4 step 1205: training loss: 1357.8229096230605\n",
      "Epoch 4 step 1206: training accuarcy: 0.6135\n",
      "Epoch 4 step 1206: training loss: 1364.350420754833\n",
      "Epoch 4 step 1207: training accuarcy: 0.606\n",
      "Epoch 4 step 1207: training loss: 1352.7196106062029\n",
      "Epoch 4 step 1208: training accuarcy: 0.6165\n",
      "Epoch 4 step 1208: training loss: 1346.8221185013722\n",
      "Epoch 4 step 1209: training accuarcy: 0.6175\n",
      "Epoch 4 step 1209: training loss: 1354.3629525963204\n",
      "Epoch 4 step 1210: training accuarcy: 0.608\n",
      "Epoch 4 step 1210: training loss: 1346.106350455985\n",
      "Epoch 4 step 1211: training accuarcy: 0.619\n",
      "Epoch 4 step 1211: training loss: 1339.5005355836513\n",
      "Epoch 4 step 1212: training accuarcy: 0.622\n",
      "Epoch 4 step 1212: training loss: 1344.385600425856\n",
      "Epoch 4 step 1213: training accuarcy: 0.6195\n",
      "Epoch 4 step 1213: training loss: 1341.2493505432872\n",
      "Epoch 4 step 1214: training accuarcy: 0.6355000000000001\n",
      "Epoch 4 step 1214: training loss: 1342.9242731505612\n",
      "Epoch 4 step 1215: training accuarcy: 0.6195\n",
      "Epoch 4 step 1215: training loss: 1351.804488312491\n",
      "Epoch 4 step 1216: training accuarcy: 0.6245\n",
      "Epoch 4 step 1216: training loss: 1321.475049792599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1217: training accuarcy: 0.6465\n",
      "Epoch 4 step 1217: training loss: 1334.6964352926996\n",
      "Epoch 4 step 1218: training accuarcy: 0.6335000000000001\n",
      "Epoch 4 step 1218: training loss: 1339.2602847043083\n",
      "Epoch 4 step 1219: training accuarcy: 0.616\n",
      "Epoch 4 step 1219: training loss: 1347.1658443645506\n",
      "Epoch 4 step 1220: training accuarcy: 0.6175\n",
      "Epoch 4 step 1220: training loss: 1338.238347604399\n",
      "Epoch 4 step 1221: training accuarcy: 0.632\n",
      "Epoch 4 step 1221: training loss: 1336.6486489915003\n",
      "Epoch 4 step 1222: training accuarcy: 0.626\n",
      "Epoch 4 step 1222: training loss: 1333.2472279340363\n",
      "Epoch 4 step 1223: training accuarcy: 0.642\n",
      "Epoch 4 step 1223: training loss: 1361.7043932116958\n",
      "Epoch 4 step 1224: training accuarcy: 0.615\n",
      "Epoch 4 step 1224: training loss: 1346.388730199401\n",
      "Epoch 4 step 1225: training accuarcy: 0.6245\n",
      "Epoch 4 step 1225: training loss: 1353.1917458528292\n",
      "Epoch 4 step 1226: training accuarcy: 0.612\n",
      "Epoch 4 step 1226: training loss: 1358.4421844763228\n",
      "Epoch 4 step 1227: training accuarcy: 0.6075\n",
      "Epoch 4 step 1227: training loss: 1331.497528611716\n",
      "Epoch 4 step 1228: training accuarcy: 0.637\n",
      "Epoch 4 step 1228: training loss: 1343.9410157030143\n",
      "Epoch 4 step 1229: training accuarcy: 0.629\n",
      "Epoch 4 step 1229: training loss: 1329.2268268434505\n",
      "Epoch 4 step 1230: training accuarcy: 0.6255000000000001\n",
      "Epoch 4 step 1230: training loss: 1339.4000417870263\n",
      "Epoch 4 step 1231: training accuarcy: 0.627\n",
      "Epoch 4 step 1231: training loss: 1362.11741471219\n",
      "Epoch 4 step 1232: training accuarcy: 0.6035\n",
      "Epoch 4 step 1232: training loss: 1333.4843331539714\n",
      "Epoch 4 step 1233: training accuarcy: 0.6265000000000001\n",
      "Epoch 4 step 1233: training loss: 1350.551369149195\n",
      "Epoch 4 step 1234: training accuarcy: 0.6015\n",
      "Epoch 4 step 1234: training loss: 1345.5825461174377\n",
      "Epoch 4 step 1235: training accuarcy: 0.6215\n",
      "Epoch 4 step 1235: training loss: 1342.9476127317753\n",
      "Epoch 4 step 1236: training accuarcy: 0.6165\n",
      "Epoch 4 step 1236: training loss: 1359.6759494556995\n",
      "Epoch 4 step 1237: training accuarcy: 0.621\n",
      "Epoch 4 step 1237: training loss: 1340.4706640217696\n",
      "Epoch 4 step 1238: training accuarcy: 0.6125\n",
      "Epoch 4 step 1238: training loss: 1356.6157497084\n",
      "Epoch 4 step 1239: training accuarcy: 0.608\n",
      "Epoch 4 step 1239: training loss: 1340.3683028289215\n",
      "Epoch 4 step 1240: training accuarcy: 0.622\n",
      "Epoch 4 step 1240: training loss: 1334.4303871358447\n",
      "Epoch 4 step 1241: training accuarcy: 0.633\n",
      "Epoch 4 step 1241: training loss: 1332.682142443919\n",
      "Epoch 4 step 1242: training accuarcy: 0.632\n",
      "Epoch 4 step 1242: training loss: 1355.1845599492294\n",
      "Epoch 4 step 1243: training accuarcy: 0.6225\n",
      "Epoch 4 step 1243: training loss: 1350.9342906970485\n",
      "Epoch 4 step 1244: training accuarcy: 0.62\n",
      "Epoch 4 step 1244: training loss: 1342.344572065413\n",
      "Epoch 4 step 1245: training accuarcy: 0.6165\n",
      "Epoch 4 step 1245: training loss: 1343.092690410239\n",
      "Epoch 4 step 1246: training accuarcy: 0.624\n",
      "Epoch 4 step 1246: training loss: 1350.6856945239633\n",
      "Epoch 4 step 1247: training accuarcy: 0.611\n",
      "Epoch 4 step 1247: training loss: 1348.5159454044865\n",
      "Epoch 4 step 1248: training accuarcy: 0.6215\n",
      "Epoch 4 step 1248: training loss: 1337.9789124792997\n",
      "Epoch 4 step 1249: training accuarcy: 0.6095\n",
      "Epoch 4 step 1249: training loss: 1336.946743727599\n",
      "Epoch 4 step 1250: training accuarcy: 0.622\n",
      "Epoch 4 step 1250: training loss: 1348.804195547477\n",
      "Epoch 4 step 1251: training accuarcy: 0.625\n",
      "Epoch 4 step 1251: training loss: 1342.0550686824072\n",
      "Epoch 4 step 1252: training accuarcy: 0.627\n",
      "Epoch 4 step 1252: training loss: 1337.3999641512823\n",
      "Epoch 4 step 1253: training accuarcy: 0.646\n",
      "Epoch 4 step 1253: training loss: 1340.0568587331672\n",
      "Epoch 4 step 1254: training accuarcy: 0.616\n",
      "Epoch 4 step 1254: training loss: 1347.3747293329805\n",
      "Epoch 4 step 1255: training accuarcy: 0.633\n",
      "Epoch 4 step 1255: training loss: 1354.4295061113212\n",
      "Epoch 4 step 1256: training accuarcy: 0.612\n",
      "Epoch 4 step 1256: training loss: 1352.958413972958\n",
      "Epoch 4 step 1257: training accuarcy: 0.6155\n",
      "Epoch 4 step 1257: training loss: 1363.7915263699497\n",
      "Epoch 4 step 1258: training accuarcy: 0.5995\n",
      "Epoch 4 step 1258: training loss: 1348.379752851174\n",
      "Epoch 4 step 1259: training accuarcy: 0.6145\n",
      "Epoch 4 step 1259: training loss: 1326.158718888168\n",
      "Epoch 4 step 1260: training accuarcy: 0.635\n",
      "Epoch 4 step 1260: training loss: 1346.742647309935\n",
      "Epoch 4 step 1261: training accuarcy: 0.6185\n",
      "Epoch 4 step 1261: training loss: 1354.8666370399046\n",
      "Epoch 4 step 1262: training accuarcy: 0.615\n",
      "Epoch 4 step 1262: training loss: 1356.7474246233212\n",
      "Epoch 4 step 1263: training accuarcy: 0.6145\n",
      "Epoch 4 step 1263: training loss: 1338.327626658051\n",
      "Epoch 4 step 1264: training accuarcy: 0.615\n",
      "Epoch 4 step 1264: training loss: 1346.0369776392865\n",
      "Epoch 4 step 1265: training accuarcy: 0.609\n",
      "Epoch 4 step 1265: training loss: 1354.3156323789965\n",
      "Epoch 4 step 1266: training accuarcy: 0.6205\n",
      "Epoch 4 step 1266: training loss: 1353.6712966675436\n",
      "Epoch 4 step 1267: training accuarcy: 0.623\n",
      "Epoch 4 step 1267: training loss: 1338.3171084817052\n",
      "Epoch 4 step 1268: training accuarcy: 0.628\n",
      "Epoch 4 step 1268: training loss: 1343.2703154708545\n",
      "Epoch 4 step 1269: training accuarcy: 0.6345000000000001\n",
      "Epoch 4 step 1269: training loss: 1332.421350886208\n",
      "Epoch 4 step 1270: training accuarcy: 0.6405\n",
      "Epoch 4 step 1270: training loss: 1353.21679116879\n",
      "Epoch 4 step 1271: training accuarcy: 0.612\n",
      "Epoch 4 step 1271: training loss: 1348.718957465313\n",
      "Epoch 4 step 1272: training accuarcy: 0.606\n",
      "Epoch 4 step 1272: training loss: 1343.0371236670558\n",
      "Epoch 4 step 1273: training accuarcy: 0.6265000000000001\n",
      "Epoch 4 step 1273: training loss: 1349.5934338018794\n",
      "Epoch 4 step 1274: training accuarcy: 0.618\n",
      "Epoch 4 step 1274: training loss: 1355.2964924187208\n",
      "Epoch 4 step 1275: training accuarcy: 0.625\n",
      "Epoch 4 step 1275: training loss: 1356.5085703417174\n",
      "Epoch 4 step 1276: training accuarcy: 0.588\n",
      "Epoch 4 step 1276: training loss: 1326.2518891618242\n",
      "Epoch 4 step 1277: training accuarcy: 0.6345000000000001\n",
      "Epoch 4 step 1277: training loss: 1332.9594093076857\n",
      "Epoch 4 step 1278: training accuarcy: 0.642\n",
      "Epoch 4 step 1278: training loss: 1345.696660769298\n",
      "Epoch 4 step 1279: training accuarcy: 0.619\n",
      "Epoch 4 step 1279: training loss: 1345.4384059023819\n",
      "Epoch 4 step 1280: training accuarcy: 0.623\n",
      "Epoch 4 step 1280: training loss: 1347.8971200061853\n",
      "Epoch 4 step 1281: training accuarcy: 0.612\n",
      "Epoch 4 step 1281: training loss: 1338.2026434700383\n",
      "Epoch 4 step 1282: training accuarcy: 0.6325000000000001\n",
      "Epoch 4 step 1282: training loss: 1339.918880959761\n",
      "Epoch 4 step 1283: training accuarcy: 0.614\n",
      "Epoch 4 step 1283: training loss: 1354.0458316709312\n",
      "Epoch 4 step 1284: training accuarcy: 0.6125\n",
      "Epoch 4 step 1284: training loss: 1352.6714888427468\n",
      "Epoch 4 step 1285: training accuarcy: 0.6085\n",
      "Epoch 4 step 1285: training loss: 1338.9913953871537\n",
      "Epoch 4 step 1286: training accuarcy: 0.635\n",
      "Epoch 4 step 1286: training loss: 1357.6750341640059\n",
      "Epoch 4 step 1287: training accuarcy: 0.6145\n",
      "Epoch 4 step 1287: training loss: 1347.4995032994923\n",
      "Epoch 4 step 1288: training accuarcy: 0.6145\n",
      "Epoch 4 step 1288: training loss: 1338.9876222962812\n",
      "Epoch 4 step 1289: training accuarcy: 0.613\n",
      "Epoch 4 step 1289: training loss: 1343.3189598482204\n",
      "Epoch 4 step 1290: training accuarcy: 0.631\n",
      "Epoch 4 step 1290: training loss: 1352.6429761457825\n",
      "Epoch 4 step 1291: training accuarcy: 0.6085\n",
      "Epoch 4 step 1291: training loss: 1319.1307538054912\n",
      "Epoch 4 step 1292: training accuarcy: 0.638\n",
      "Epoch 4 step 1292: training loss: 1353.606361460949\n",
      "Epoch 4 step 1293: training accuarcy: 0.6105\n",
      "Epoch 4 step 1293: training loss: 1352.3122732364063\n",
      "Epoch 4 step 1294: training accuarcy: 0.6075\n",
      "Epoch 4 step 1294: training loss: 1355.629794685793\n",
      "Epoch 4 step 1295: training accuarcy: 0.6\n",
      "Epoch 4 step 1295: training loss: 1346.4111295932958\n",
      "Epoch 4 step 1296: training accuarcy: 0.6305000000000001\n",
      "Epoch 4 step 1296: training loss: 1344.8772255743706\n",
      "Epoch 4 step 1297: training accuarcy: 0.628\n",
      "Epoch 4 step 1297: training loss: 1348.0236310622174\n",
      "Epoch 4 step 1298: training accuarcy: 0.6125\n",
      "Epoch 4 step 1298: training loss: 1337.4709704285763\n",
      "Epoch 4 step 1299: training accuarcy: 0.611\n",
      "Epoch 4 step 1299: training loss: 1353.9503726618004\n",
      "Epoch 4 step 1300: training accuarcy: 0.614\n",
      "Epoch 4 step 1300: training loss: 1332.4618595793186\n",
      "Epoch 4 step 1301: training accuarcy: 0.636\n",
      "Epoch 4 step 1301: training loss: 1335.7600795265976\n",
      "Epoch 4 step 1302: training accuarcy: 0.632\n",
      "Epoch 4 step 1302: training loss: 1335.8036433455986\n",
      "Epoch 4 step 1303: training accuarcy: 0.634\n",
      "Epoch 4 step 1303: training loss: 1344.3961290673228\n",
      "Epoch 4 step 1304: training accuarcy: 0.6215\n",
      "Epoch 4 step 1304: training loss: 1342.298207880617\n",
      "Epoch 4 step 1305: training accuarcy: 0.624\n",
      "Epoch 4 step 1305: training loss: 1359.3501837813474\n",
      "Epoch 4 step 1306: training accuarcy: 0.6135\n",
      "Epoch 4 step 1306: training loss: 1322.051329559603\n",
      "Epoch 4 step 1307: training accuarcy: 0.64\n",
      "Epoch 4 step 1307: training loss: 1349.6204309428522\n",
      "Epoch 4 step 1308: training accuarcy: 0.6055\n",
      "Epoch 4 step 1308: training loss: 1340.4382280586387\n",
      "Epoch 4 step 1309: training accuarcy: 0.614\n",
      "Epoch 4 step 1309: training loss: 1335.331432115869\n",
      "Epoch 4 step 1310: training accuarcy: 0.6285000000000001\n",
      "Epoch 4 step 1310: training loss: 1341.9251193455627\n",
      "Epoch 4 step 1311: training accuarcy: 0.617\n",
      "Epoch 4 step 1311: training loss: 1334.8873412586383\n",
      "Epoch 4 step 1312: training accuarcy: 0.6295000000000001\n",
      "Epoch 4 step 1312: training loss: 1339.2432401111193\n",
      "Epoch 4 step 1313: training accuarcy: 0.6265000000000001\n",
      "Epoch 4 step 1313: training loss: 1357.2015554401287\n",
      "Epoch 4 step 1314: training accuarcy: 0.608\n",
      "Epoch 4 step 1314: training loss: 546.0877356208739\n",
      "Epoch 4 step 1315: training accuarcy: 0.632051282051282\n",
      "Epoch 4: train loss 1341.7111266047311, train accuarcy 0.622305691242218\n",
      "Epoch 4: valid loss 6659.871462799861, valid accuarcy 0.5938752293586731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [24:42<00:00, 298.80s/it]\n"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=5,\n",
    "                 log_dir=get_log_dir('seq_topcoder', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:04:45.696347Z",
     "start_time": "2019-10-08T02:04:45.687352Z"
    }
   },
   "outputs": [],
   "source": [
    "del prme_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Trans FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:04:47.522735Z",
     "start_time": "2019-10-08T02:04:47.108733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTransFM()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model = TorchTransFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "trans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:04:48.504137Z",
     "start_time": "2019-10-08T02:04:48.501134Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(trans_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:04:49.488390Z",
     "start_time": "2019-10-08T02:04:49.429392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x29044d973c8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_learner = FMLearner(trans_model, adam_opt, schedular, db)\n",
    "trans_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:04:52.121159Z",
     "start_time": "2019-10-08T02:04:52.118159Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=trans_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:32:13.357974Z",
     "start_time": "2019-10-08T02:04:54.480089Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 48586.55751144697\n",
      "Epoch 0 step 1: training accuarcy: 0.5135\n",
      "Epoch 0 step 1: training loss: 46554.602668407955\n",
      "Epoch 0 step 2: training accuarcy: 0.5315\n",
      "Epoch 0 step 2: training loss: 46073.377841289795\n",
      "Epoch 0 step 3: training accuarcy: 0.5375\n",
      "Epoch 0 step 3: training loss: 44473.14710635728\n",
      "Epoch 0 step 4: training accuarcy: 0.5365\n",
      "Epoch 0 step 4: training loss: 43457.24741669615\n",
      "Epoch 0 step 5: training accuarcy: 0.5215\n",
      "Epoch 0 step 5: training loss: 42315.38672174617\n",
      "Epoch 0 step 6: training accuarcy: 0.5225\n",
      "Epoch 0 step 6: training loss: 41311.40883139911\n",
      "Epoch 0 step 7: training accuarcy: 0.54\n",
      "Epoch 0 step 7: training loss: 40070.476456635355\n",
      "Epoch 0 step 8: training accuarcy: 0.5225\n",
      "Epoch 0 step 8: training loss: 38971.2602908507\n",
      "Epoch 0 step 9: training accuarcy: 0.552\n",
      "Epoch 0 step 9: training loss: 37971.164600912336\n",
      "Epoch 0 step 10: training accuarcy: 0.5395\n",
      "Epoch 0 step 10: training loss: 36959.90561237274\n",
      "Epoch 0 step 11: training accuarcy: 0.535\n",
      "Epoch 0 step 11: training loss: 35668.01652299144\n",
      "Epoch 0 step 12: training accuarcy: 0.5455\n",
      "Epoch 0 step 12: training loss: 35273.96894860831\n",
      "Epoch 0 step 13: training accuarcy: 0.522\n",
      "Epoch 0 step 13: training loss: 33905.84976369\n",
      "Epoch 0 step 14: training accuarcy: 0.533\n",
      "Epoch 0 step 14: training loss: 33067.55974089354\n",
      "Epoch 0 step 15: training accuarcy: 0.5375\n",
      "Epoch 0 step 15: training loss: 31782.6475037668\n",
      "Epoch 0 step 16: training accuarcy: 0.5575\n",
      "Epoch 0 step 16: training loss: 30858.97694247512\n",
      "Epoch 0 step 17: training accuarcy: 0.534\n",
      "Epoch 0 step 17: training loss: 29686.937663994322\n",
      "Epoch 0 step 18: training accuarcy: 0.5575\n",
      "Epoch 0 step 18: training loss: 29441.381758486797\n",
      "Epoch 0 step 19: training accuarcy: 0.5185\n",
      "Epoch 0 step 19: training loss: 27943.06471719217\n",
      "Epoch 0 step 20: training accuarcy: 0.5385\n",
      "Epoch 0 step 20: training loss: 27743.524783866727\n",
      "Epoch 0 step 21: training accuarcy: 0.5335\n",
      "Epoch 0 step 21: training loss: 26177.726833698816\n",
      "Epoch 0 step 22: training accuarcy: 0.5425\n",
      "Epoch 0 step 22: training loss: 25730.20402331995\n",
      "Epoch 0 step 23: training accuarcy: 0.545\n",
      "Epoch 0 step 23: training loss: 25069.88082766243\n",
      "Epoch 0 step 24: training accuarcy: 0.528\n",
      "Epoch 0 step 24: training loss: 23388.666676257286\n",
      "Epoch 0 step 25: training accuarcy: 0.5415\n",
      "Epoch 0 step 25: training loss: 23028.16335463449\n",
      "Epoch 0 step 26: training accuarcy: 0.547\n",
      "Epoch 0 step 26: training loss: 22308.172135951267\n",
      "Epoch 0 step 27: training accuarcy: 0.545\n",
      "Epoch 0 step 27: training loss: 21780.987797935635\n",
      "Epoch 0 step 28: training accuarcy: 0.549\n",
      "Epoch 0 step 28: training loss: 20636.463376433476\n",
      "Epoch 0 step 29: training accuarcy: 0.5735\n",
      "Epoch 0 step 29: training loss: 20428.018573718946\n",
      "Epoch 0 step 30: training accuarcy: 0.5355\n",
      "Epoch 0 step 30: training loss: 19735.568487058226\n",
      "Epoch 0 step 31: training accuarcy: 0.548\n",
      "Epoch 0 step 31: training loss: 19028.042712771115\n",
      "Epoch 0 step 32: training accuarcy: 0.5525\n",
      "Epoch 0 step 32: training loss: 18437.774972814288\n",
      "Epoch 0 step 33: training accuarcy: 0.5435\n",
      "Epoch 0 step 33: training loss: 17854.68458930855\n",
      "Epoch 0 step 34: training accuarcy: 0.5385\n",
      "Epoch 0 step 34: training loss: 17041.034015758152\n",
      "Epoch 0 step 35: training accuarcy: 0.5525\n",
      "Epoch 0 step 35: training loss: 16479.768008065716\n",
      "Epoch 0 step 36: training accuarcy: 0.5505\n",
      "Epoch 0 step 36: training loss: 15902.638119257343\n",
      "Epoch 0 step 37: training accuarcy: 0.5485\n",
      "Epoch 0 step 37: training loss: 15303.711842137775\n",
      "Epoch 0 step 38: training accuarcy: 0.559\n",
      "Epoch 0 step 38: training loss: 14729.255909902371\n",
      "Epoch 0 step 39: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 39: training loss: 14497.579722062757\n",
      "Epoch 0 step 40: training accuarcy: 0.542\n",
      "Epoch 0 step 40: training loss: 13587.60985509853\n",
      "Epoch 0 step 41: training accuarcy: 0.5625\n",
      "Epoch 0 step 41: training loss: 13285.320978258113\n",
      "Epoch 0 step 42: training accuarcy: 0.5650000000000001\n",
      "Epoch 0 step 42: training loss: 12816.72824264243\n",
      "Epoch 0 step 43: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 43: training loss: 12381.521147756326\n",
      "Epoch 0 step 44: training accuarcy: 0.5825\n",
      "Epoch 0 step 44: training loss: 12007.373120126278\n",
      "Epoch 0 step 45: training accuarcy: 0.556\n",
      "Epoch 0 step 45: training loss: 11480.372616244658\n",
      "Epoch 0 step 46: training accuarcy: 0.5835\n",
      "Epoch 0 step 46: training loss: 11072.203884239994\n",
      "Epoch 0 step 47: training accuarcy: 0.578\n",
      "Epoch 0 step 47: training loss: 10779.255000102332\n",
      "Epoch 0 step 48: training accuarcy: 0.5835\n",
      "Epoch 0 step 48: training loss: 10371.291927513554\n",
      "Epoch 0 step 49: training accuarcy: 0.587\n",
      "Epoch 0 step 49: training loss: 9981.585145813082\n",
      "Epoch 0 step 50: training accuarcy: 0.5760000000000001\n",
      "Epoch 0 step 50: training loss: 9557.751112211788\n",
      "Epoch 0 step 51: training accuarcy: 0.6085\n",
      "Epoch 0 step 51: training loss: 9454.802852169974\n",
      "Epoch 0 step 52: training accuarcy: 0.5795\n",
      "Epoch 0 step 52: training loss: 9015.551313620032\n",
      "Epoch 0 step 53: training accuarcy: 0.609\n",
      "Epoch 0 step 53: training loss: 8708.890132804052\n",
      "Epoch 0 step 54: training accuarcy: 0.591\n",
      "Epoch 0 step 54: training loss: 8460.954736369167\n",
      "Epoch 0 step 55: training accuarcy: 0.6055\n",
      "Epoch 0 step 55: training loss: 8244.13671546845\n",
      "Epoch 0 step 56: training accuarcy: 0.5945\n",
      "Epoch 0 step 56: training loss: 7896.122520124621\n",
      "Epoch 0 step 57: training accuarcy: 0.619\n",
      "Epoch 0 step 57: training loss: 7605.36665335052\n",
      "Epoch 0 step 58: training accuarcy: 0.626\n",
      "Epoch 0 step 58: training loss: 7470.077372210512\n",
      "Epoch 0 step 59: training accuarcy: 0.6155\n",
      "Epoch 0 step 59: training loss: 7110.415274269539\n",
      "Epoch 0 step 60: training accuarcy: 0.639\n",
      "Epoch 0 step 60: training loss: 6957.03762413888\n",
      "Epoch 0 step 61: training accuarcy: 0.6355000000000001\n",
      "Epoch 0 step 61: training loss: 6747.749852507851\n",
      "Epoch 0 step 62: training accuarcy: 0.643\n",
      "Epoch 0 step 62: training loss: 6574.022428872973\n",
      "Epoch 0 step 63: training accuarcy: 0.627\n",
      "Epoch 0 step 63: training loss: 6310.575232816797\n",
      "Epoch 0 step 64: training accuarcy: 0.6565\n",
      "Epoch 0 step 64: training loss: 6161.93334610549\n",
      "Epoch 0 step 65: training accuarcy: 0.651\n",
      "Epoch 0 step 65: training loss: 6009.461017307053\n",
      "Epoch 0 step 66: training accuarcy: 0.6605\n",
      "Epoch 0 step 66: training loss: 5783.16096317387\n",
      "Epoch 0 step 67: training accuarcy: 0.6745\n",
      "Epoch 0 step 67: training loss: 5670.9661682014785\n",
      "Epoch 0 step 68: training accuarcy: 0.67\n",
      "Epoch 0 step 68: training loss: 5473.465049701296\n",
      "Epoch 0 step 69: training accuarcy: 0.6685\n",
      "Epoch 0 step 69: training loss: 5418.875827481837\n",
      "Epoch 0 step 70: training accuarcy: 0.6425\n",
      "Epoch 0 step 70: training loss: 5166.931195093896\n",
      "Epoch 0 step 71: training accuarcy: 0.668\n",
      "Epoch 0 step 71: training loss: 5016.498995734477\n",
      "Epoch 0 step 72: training accuarcy: 0.686\n",
      "Epoch 0 step 72: training loss: 4962.768152328033\n",
      "Epoch 0 step 73: training accuarcy: 0.669\n",
      "Epoch 0 step 73: training loss: 4757.49352015757\n",
      "Epoch 0 step 74: training accuarcy: 0.685\n",
      "Epoch 0 step 74: training loss: 4665.110288679927\n",
      "Epoch 0 step 75: training accuarcy: 0.6845\n",
      "Epoch 0 step 75: training loss: 4449.962611165212\n",
      "Epoch 0 step 76: training accuarcy: 0.7235\n",
      "Epoch 0 step 76: training loss: 4316.706497956886\n",
      "Epoch 0 step 77: training accuarcy: 0.7175\n",
      "Epoch 0 step 77: training loss: 4263.194902873718\n",
      "Epoch 0 step 78: training accuarcy: 0.6995\n",
      "Epoch 0 step 78: training loss: 4248.816311923771\n",
      "Epoch 0 step 79: training accuarcy: 0.679\n",
      "Epoch 0 step 79: training loss: 3997.8471682930776\n",
      "Epoch 0 step 80: training accuarcy: 0.7135\n",
      "Epoch 0 step 80: training loss: 3965.689906106801\n",
      "Epoch 0 step 81: training accuarcy: 0.6900000000000001\n",
      "Epoch 0 step 81: training loss: 3843.2441172238014\n",
      "Epoch 0 step 82: training accuarcy: 0.706\n",
      "Epoch 0 step 82: training loss: 3749.3243924359704\n",
      "Epoch 0 step 83: training accuarcy: 0.7135\n",
      "Epoch 0 step 83: training loss: 3649.7404777158536\n",
      "Epoch 0 step 84: training accuarcy: 0.715\n",
      "Epoch 0 step 84: training loss: 3564.97265927622\n",
      "Epoch 0 step 85: training accuarcy: 0.72\n",
      "Epoch 0 step 85: training loss: 3545.186126103704\n",
      "Epoch 0 step 86: training accuarcy: 0.6975\n",
      "Epoch 0 step 86: training loss: 3393.8305140491757\n",
      "Epoch 0 step 87: training accuarcy: 0.708\n",
      "Epoch 0 step 87: training loss: 3299.3061302705396\n",
      "Epoch 0 step 88: training accuarcy: 0.717\n",
      "Epoch 0 step 88: training loss: 3216.0678076747686\n",
      "Epoch 0 step 89: training accuarcy: 0.7275\n",
      "Epoch 0 step 89: training loss: 3197.4050258785737\n",
      "Epoch 0 step 90: training accuarcy: 0.713\n",
      "Epoch 0 step 90: training loss: 3104.3424921707256\n",
      "Epoch 0 step 91: training accuarcy: 0.725\n",
      "Epoch 0 step 91: training loss: 3069.00566618029\n",
      "Epoch 0 step 92: training accuarcy: 0.7215\n",
      "Epoch 0 step 92: training loss: 2964.987841273467\n",
      "Epoch 0 step 93: training accuarcy: 0.731\n",
      "Epoch 0 step 93: training loss: 2880.008439479798\n",
      "Epoch 0 step 94: training accuarcy: 0.731\n",
      "Epoch 0 step 94: training loss: 2796.913775909469\n",
      "Epoch 0 step 95: training accuarcy: 0.737\n",
      "Epoch 0 step 95: training loss: 2805.2269563324353\n",
      "Epoch 0 step 96: training accuarcy: 0.717\n",
      "Epoch 0 step 96: training loss: 2694.0044133243237\n",
      "Epoch 0 step 97: training accuarcy: 0.732\n",
      "Epoch 0 step 97: training loss: 2660.712908102922\n",
      "Epoch 0 step 98: training accuarcy: 0.733\n",
      "Epoch 0 step 98: training loss: 2642.707330519525\n",
      "Epoch 0 step 99: training accuarcy: 0.7235\n",
      "Epoch 0 step 99: training loss: 2582.9016891762403\n",
      "Epoch 0 step 100: training accuarcy: 0.7265\n",
      "Epoch 0 step 100: training loss: 2450.2035177762964\n",
      "Epoch 0 step 101: training accuarcy: 0.751\n",
      "Epoch 0 step 101: training loss: 2502.8791419996905\n",
      "Epoch 0 step 102: training accuarcy: 0.7235\n",
      "Epoch 0 step 102: training loss: 2363.8576810555805\n",
      "Epoch 0 step 103: training accuarcy: 0.7525000000000001\n",
      "Epoch 0 step 103: training loss: 2411.727908236211\n",
      "Epoch 0 step 104: training accuarcy: 0.739\n",
      "Epoch 0 step 104: training loss: 2309.641377818121\n",
      "Epoch 0 step 105: training accuarcy: 0.735\n",
      "Epoch 0 step 105: training loss: 2319.014239394606\n",
      "Epoch 0 step 106: training accuarcy: 0.7235\n",
      "Epoch 0 step 106: training loss: 2250.380290060333\n",
      "Epoch 0 step 107: training accuarcy: 0.736\n",
      "Epoch 0 step 107: training loss: 2201.296983970635\n",
      "Epoch 0 step 108: training accuarcy: 0.74\n",
      "Epoch 0 step 108: training loss: 2174.0802198595893\n",
      "Epoch 0 step 109: training accuarcy: 0.7375\n",
      "Epoch 0 step 109: training loss: 2144.7581812227\n",
      "Epoch 0 step 110: training accuarcy: 0.7425\n",
      "Epoch 0 step 110: training loss: 2124.4882520636475\n",
      "Epoch 0 step 111: training accuarcy: 0.739\n",
      "Epoch 0 step 111: training loss: 2023.3522639274142\n",
      "Epoch 0 step 112: training accuarcy: 0.75\n",
      "Epoch 0 step 112: training loss: 2042.713801236563\n",
      "Epoch 0 step 113: training accuarcy: 0.7415\n",
      "Epoch 0 step 113: training loss: 1952.7334098247895\n",
      "Epoch 0 step 114: training accuarcy: 0.7495\n",
      "Epoch 0 step 114: training loss: 1930.9449061574524\n",
      "Epoch 0 step 115: training accuarcy: 0.743\n",
      "Epoch 0 step 115: training loss: 1885.6813933666474\n",
      "Epoch 0 step 116: training accuarcy: 0.7485\n",
      "Epoch 0 step 116: training loss: 1862.485823992684\n",
      "Epoch 0 step 117: training accuarcy: 0.749\n",
      "Epoch 0 step 117: training loss: 1834.975597393353\n",
      "Epoch 0 step 118: training accuarcy: 0.754\n",
      "Epoch 0 step 118: training loss: 1851.4345104799727\n",
      "Epoch 0 step 119: training accuarcy: 0.742\n",
      "Epoch 0 step 119: training loss: 1785.7667862236904\n",
      "Epoch 0 step 120: training accuarcy: 0.755\n",
      "Epoch 0 step 120: training loss: 1776.941362480055\n",
      "Epoch 0 step 121: training accuarcy: 0.747\n",
      "Epoch 0 step 121: training loss: 1772.0534803443713\n",
      "Epoch 0 step 122: training accuarcy: 0.745\n",
      "Epoch 0 step 122: training loss: 1720.1383292109274\n",
      "Epoch 0 step 123: training accuarcy: 0.7485\n",
      "Epoch 0 step 123: training loss: 1689.005607118499\n",
      "Epoch 0 step 124: training accuarcy: 0.7575000000000001\n",
      "Epoch 0 step 124: training loss: 1685.742132465719\n",
      "Epoch 0 step 125: training accuarcy: 0.751\n",
      "Epoch 0 step 125: training loss: 1720.434861188534\n",
      "Epoch 0 step 126: training accuarcy: 0.737\n",
      "Epoch 0 step 126: training loss: 1663.5427425217204\n",
      "Epoch 0 step 127: training accuarcy: 0.7515000000000001\n",
      "Epoch 0 step 127: training loss: 1629.545949317902\n",
      "Epoch 0 step 128: training accuarcy: 0.759\n",
      "Epoch 0 step 128: training loss: 1571.825935652158\n",
      "Epoch 0 step 129: training accuarcy: 0.7705\n",
      "Epoch 0 step 129: training loss: 1555.5064987287396\n",
      "Epoch 0 step 130: training accuarcy: 0.765\n",
      "Epoch 0 step 130: training loss: 1555.793077163472\n",
      "Epoch 0 step 131: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 131: training loss: 1536.6580468584725\n",
      "Epoch 0 step 132: training accuarcy: 0.767\n",
      "Epoch 0 step 132: training loss: 1480.1022129651944\n",
      "Epoch 0 step 133: training accuarcy: 0.782\n",
      "Epoch 0 step 133: training loss: 1514.6294384642724\n",
      "Epoch 0 step 134: training accuarcy: 0.758\n",
      "Epoch 0 step 134: training loss: 1550.2156447506277\n",
      "Epoch 0 step 135: training accuarcy: 0.7435\n",
      "Epoch 0 step 135: training loss: 1484.6857209093278\n",
      "Epoch 0 step 136: training accuarcy: 0.761\n",
      "Epoch 0 step 136: training loss: 1429.0284876151652\n",
      "Epoch 0 step 137: training accuarcy: 0.783\n",
      "Epoch 0 step 137: training loss: 1451.241309603723\n",
      "Epoch 0 step 138: training accuarcy: 0.7725\n",
      "Epoch 0 step 138: training loss: 1428.7882157901313\n",
      "Epoch 0 step 139: training accuarcy: 0.771\n",
      "Epoch 0 step 139: training loss: 1409.9098427761871\n",
      "Epoch 0 step 140: training accuarcy: 0.772\n",
      "Epoch 0 step 140: training loss: 1387.5790390546906\n",
      "Epoch 0 step 141: training accuarcy: 0.7745\n",
      "Epoch 0 step 141: training loss: 1364.0231698761029\n",
      "Epoch 0 step 142: training accuarcy: 0.779\n",
      "Epoch 0 step 142: training loss: 1367.8629181347112\n",
      "Epoch 0 step 143: training accuarcy: 0.772\n",
      "Epoch 0 step 143: training loss: 1381.1227300116989\n",
      "Epoch 0 step 144: training accuarcy: 0.7725\n",
      "Epoch 0 step 144: training loss: 1392.6032401167288\n",
      "Epoch 0 step 145: training accuarcy: 0.7585000000000001\n",
      "Epoch 0 step 145: training loss: 1296.7941420814088\n",
      "Epoch 0 step 146: training accuarcy: 0.795\n",
      "Epoch 0 step 146: training loss: 1337.6349295801506\n",
      "Epoch 0 step 147: training accuarcy: 0.7665000000000001\n",
      "Epoch 0 step 147: training loss: 1296.0099483892882\n",
      "Epoch 0 step 148: training accuarcy: 0.787\n",
      "Epoch 0 step 148: training loss: 1333.1207928797817\n",
      "Epoch 0 step 149: training accuarcy: 0.769\n",
      "Epoch 0 step 149: training loss: 1321.8112054513963\n",
      "Epoch 0 step 150: training accuarcy: 0.768\n",
      "Epoch 0 step 150: training loss: 1274.260203378778\n",
      "Epoch 0 step 151: training accuarcy: 0.7755\n",
      "Epoch 0 step 151: training loss: 1276.3977239704254\n",
      "Epoch 0 step 152: training accuarcy: 0.7795\n",
      "Epoch 0 step 152: training loss: 1260.6513309513907\n",
      "Epoch 0 step 153: training accuarcy: 0.781\n",
      "Epoch 0 step 153: training loss: 1225.0608967328521\n",
      "Epoch 0 step 154: training accuarcy: 0.7925\n",
      "Epoch 0 step 154: training loss: 1248.11463131224\n",
      "Epoch 0 step 155: training accuarcy: 0.7815\n",
      "Epoch 0 step 155: training loss: 1269.3481200898927\n",
      "Epoch 0 step 156: training accuarcy: 0.7695\n",
      "Epoch 0 step 156: training loss: 1276.0424461170394\n",
      "Epoch 0 step 157: training accuarcy: 0.773\n",
      "Epoch 0 step 157: training loss: 1238.2284321955217\n",
      "Epoch 0 step 158: training accuarcy: 0.778\n",
      "Epoch 0 step 158: training loss: 1178.9407336759798\n",
      "Epoch 0 step 159: training accuarcy: 0.79\n",
      "Epoch 0 step 159: training loss: 1233.2442667443074\n",
      "Epoch 0 step 160: training accuarcy: 0.77\n",
      "Epoch 0 step 160: training loss: 1298.6290691766899\n",
      "Epoch 0 step 161: training accuarcy: 0.763\n",
      "Epoch 0 step 161: training loss: 1191.3492950274854\n",
      "Epoch 0 step 162: training accuarcy: 0.787\n",
      "Epoch 0 step 162: training loss: 1253.864506646513\n",
      "Epoch 0 step 163: training accuarcy: 0.7665000000000001\n",
      "Epoch 0 step 163: training loss: 1210.8446315435392\n",
      "Epoch 0 step 164: training accuarcy: 0.7795\n",
      "Epoch 0 step 164: training loss: 1165.674987523906\n",
      "Epoch 0 step 165: training accuarcy: 0.788\n",
      "Epoch 0 step 165: training loss: 1141.9666982480746\n",
      "Epoch 0 step 166: training accuarcy: 0.791\n",
      "Epoch 0 step 166: training loss: 1198.4821222683904\n",
      "Epoch 0 step 167: training accuarcy: 0.7725\n",
      "Epoch 0 step 167: training loss: 1146.70597233753\n",
      "Epoch 0 step 168: training accuarcy: 0.804\n",
      "Epoch 0 step 168: training loss: 1163.2411024487749\n",
      "Epoch 0 step 169: training accuarcy: 0.78\n",
      "Epoch 0 step 169: training loss: 1143.7943599146556\n",
      "Epoch 0 step 170: training accuarcy: 0.794\n",
      "Epoch 0 step 170: training loss: 1142.2816126385594\n",
      "Epoch 0 step 171: training accuarcy: 0.784\n",
      "Epoch 0 step 171: training loss: 1126.634305602738\n",
      "Epoch 0 step 172: training accuarcy: 0.7945\n",
      "Epoch 0 step 172: training loss: 1140.6546881989482\n",
      "Epoch 0 step 173: training accuarcy: 0.7895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 173: training loss: 1151.1784287771404\n",
      "Epoch 0 step 174: training accuarcy: 0.777\n",
      "Epoch 0 step 174: training loss: 1116.9814151445676\n",
      "Epoch 0 step 175: training accuarcy: 0.7945\n",
      "Epoch 0 step 175: training loss: 1140.5678941107212\n",
      "Epoch 0 step 176: training accuarcy: 0.7885\n",
      "Epoch 0 step 176: training loss: 1075.3674273548922\n",
      "Epoch 0 step 177: training accuarcy: 0.803\n",
      "Epoch 0 step 177: training loss: 1090.0428897173276\n",
      "Epoch 0 step 178: training accuarcy: 0.79\n",
      "Epoch 0 step 178: training loss: 1096.2791603129133\n",
      "Epoch 0 step 179: training accuarcy: 0.7905\n",
      "Epoch 0 step 179: training loss: 1057.0687897856774\n",
      "Epoch 0 step 180: training accuarcy: 0.8035\n",
      "Epoch 0 step 180: training loss: 1068.9123489402648\n",
      "Epoch 0 step 181: training accuarcy: 0.8055\n",
      "Epoch 0 step 181: training loss: 1093.2883667399813\n",
      "Epoch 0 step 182: training accuarcy: 0.7915\n",
      "Epoch 0 step 182: training loss: 1080.424352815765\n",
      "Epoch 0 step 183: training accuarcy: 0.7985\n",
      "Epoch 0 step 183: training loss: 1127.1094925375066\n",
      "Epoch 0 step 184: training accuarcy: 0.792\n",
      "Epoch 0 step 184: training loss: 1062.257977275384\n",
      "Epoch 0 step 185: training accuarcy: 0.8045\n",
      "Epoch 0 step 185: training loss: 1077.706946359\n",
      "Epoch 0 step 186: training accuarcy: 0.7945\n",
      "Epoch 0 step 186: training loss: 1070.9363893390014\n",
      "Epoch 0 step 187: training accuarcy: 0.793\n",
      "Epoch 0 step 187: training loss: 1063.324720710306\n",
      "Epoch 0 step 188: training accuarcy: 0.797\n",
      "Epoch 0 step 188: training loss: 1034.3637873746347\n",
      "Epoch 0 step 189: training accuarcy: 0.8130000000000001\n",
      "Epoch 0 step 189: training loss: 1096.4216938279349\n",
      "Epoch 0 step 190: training accuarcy: 0.7935\n",
      "Epoch 0 step 190: training loss: 1081.5027705640532\n",
      "Epoch 0 step 191: training accuarcy: 0.8005\n",
      "Epoch 0 step 191: training loss: 1085.2960747568436\n",
      "Epoch 0 step 192: training accuarcy: 0.794\n",
      "Epoch 0 step 192: training loss: 1063.5379501537536\n",
      "Epoch 0 step 193: training accuarcy: 0.8\n",
      "Epoch 0 step 193: training loss: 1065.1784410804935\n",
      "Epoch 0 step 194: training accuarcy: 0.808\n",
      "Epoch 0 step 194: training loss: 1063.9453193539732\n",
      "Epoch 0 step 195: training accuarcy: 0.804\n",
      "Epoch 0 step 195: training loss: 1088.0906820233292\n",
      "Epoch 0 step 196: training accuarcy: 0.784\n",
      "Epoch 0 step 196: training loss: 1032.997857279214\n",
      "Epoch 0 step 197: training accuarcy: 0.799\n",
      "Epoch 0 step 197: training loss: 1020.4968653148916\n",
      "Epoch 0 step 198: training accuarcy: 0.8165\n",
      "Epoch 0 step 198: training loss: 1055.301177897513\n",
      "Epoch 0 step 199: training accuarcy: 0.808\n",
      "Epoch 0 step 199: training loss: 1050.2901080099882\n",
      "Epoch 0 step 200: training accuarcy: 0.7935\n",
      "Epoch 0 step 200: training loss: 1025.6754669638474\n",
      "Epoch 0 step 201: training accuarcy: 0.8130000000000001\n",
      "Epoch 0 step 201: training loss: 1035.3732780404791\n",
      "Epoch 0 step 202: training accuarcy: 0.804\n",
      "Epoch 0 step 202: training loss: 980.300280654754\n",
      "Epoch 0 step 203: training accuarcy: 0.8215\n",
      "Epoch 0 step 203: training loss: 1067.103676081237\n",
      "Epoch 0 step 204: training accuarcy: 0.7905\n",
      "Epoch 0 step 204: training loss: 1019.4598484078692\n",
      "Epoch 0 step 205: training accuarcy: 0.8115\n",
      "Epoch 0 step 205: training loss: 1030.9281877462445\n",
      "Epoch 0 step 206: training accuarcy: 0.803\n",
      "Epoch 0 step 206: training loss: 1004.7153473999197\n",
      "Epoch 0 step 207: training accuarcy: 0.807\n",
      "Epoch 0 step 207: training loss: 1035.243955964259\n",
      "Epoch 0 step 208: training accuarcy: 0.79\n",
      "Epoch 0 step 208: training loss: 1021.6879050195885\n",
      "Epoch 0 step 209: training accuarcy: 0.796\n",
      "Epoch 0 step 209: training loss: 981.8613821758681\n",
      "Epoch 0 step 210: training accuarcy: 0.8160000000000001\n",
      "Epoch 0 step 210: training loss: 992.9878813686054\n",
      "Epoch 0 step 211: training accuarcy: 0.8095\n",
      "Epoch 0 step 211: training loss: 996.324380387026\n",
      "Epoch 0 step 212: training accuarcy: 0.8115\n",
      "Epoch 0 step 212: training loss: 971.440595880862\n",
      "Epoch 0 step 213: training accuarcy: 0.8075\n",
      "Epoch 0 step 213: training loss: 1009.2149632265985\n",
      "Epoch 0 step 214: training accuarcy: 0.811\n",
      "Epoch 0 step 214: training loss: 1013.7542700755477\n",
      "Epoch 0 step 215: training accuarcy: 0.8025\n",
      "Epoch 0 step 215: training loss: 1027.4573907488787\n",
      "Epoch 0 step 216: training accuarcy: 0.809\n",
      "Epoch 0 step 216: training loss: 947.2562268906985\n",
      "Epoch 0 step 217: training accuarcy: 0.8125\n",
      "Epoch 0 step 217: training loss: 1030.7552254143277\n",
      "Epoch 0 step 218: training accuarcy: 0.7965\n",
      "Epoch 0 step 218: training loss: 957.1175073826308\n",
      "Epoch 0 step 219: training accuarcy: 0.8135\n",
      "Epoch 0 step 219: training loss: 1007.562775989674\n",
      "Epoch 0 step 220: training accuarcy: 0.796\n",
      "Epoch 0 step 220: training loss: 961.3199155351442\n",
      "Epoch 0 step 221: training accuarcy: 0.8180000000000001\n",
      "Epoch 0 step 221: training loss: 981.5998292490156\n",
      "Epoch 0 step 222: training accuarcy: 0.8035\n",
      "Epoch 0 step 222: training loss: 1037.5157158059915\n",
      "Epoch 0 step 223: training accuarcy: 0.777\n",
      "Epoch 0 step 223: training loss: 927.7671669029214\n",
      "Epoch 0 step 224: training accuarcy: 0.8270000000000001\n",
      "Epoch 0 step 224: training loss: 1001.3889178032728\n",
      "Epoch 0 step 225: training accuarcy: 0.8075\n",
      "Epoch 0 step 225: training loss: 928.6810455310211\n",
      "Epoch 0 step 226: training accuarcy: 0.8300000000000001\n",
      "Epoch 0 step 226: training loss: 1002.108799518123\n",
      "Epoch 0 step 227: training accuarcy: 0.8005\n",
      "Epoch 0 step 227: training loss: 939.0796594699946\n",
      "Epoch 0 step 228: training accuarcy: 0.8255\n",
      "Epoch 0 step 228: training loss: 961.402901696932\n",
      "Epoch 0 step 229: training accuarcy: 0.8180000000000001\n",
      "Epoch 0 step 229: training loss: 972.1621408019339\n",
      "Epoch 0 step 230: training accuarcy: 0.803\n",
      "Epoch 0 step 230: training loss: 1006.9277158649378\n",
      "Epoch 0 step 231: training accuarcy: 0.8005\n",
      "Epoch 0 step 231: training loss: 956.5751656917532\n",
      "Epoch 0 step 232: training accuarcy: 0.8150000000000001\n",
      "Epoch 0 step 232: training loss: 992.4407063869435\n",
      "Epoch 0 step 233: training accuarcy: 0.8075\n",
      "Epoch 0 step 233: training loss: 964.8720364109488\n",
      "Epoch 0 step 234: training accuarcy: 0.8085\n",
      "Epoch 0 step 234: training loss: 946.3587369438683\n",
      "Epoch 0 step 235: training accuarcy: 0.8150000000000001\n",
      "Epoch 0 step 235: training loss: 929.296340575115\n",
      "Epoch 0 step 236: training accuarcy: 0.8265\n",
      "Epoch 0 step 236: training loss: 953.9266742175639\n",
      "Epoch 0 step 237: training accuarcy: 0.8190000000000001\n",
      "Epoch 0 step 237: training loss: 989.2511391254023\n",
      "Epoch 0 step 238: training accuarcy: 0.8045\n",
      "Epoch 0 step 238: training loss: 975.9308712039217\n",
      "Epoch 0 step 239: training accuarcy: 0.8145\n",
      "Epoch 0 step 239: training loss: 998.2794178673255\n",
      "Epoch 0 step 240: training accuarcy: 0.8085\n",
      "Epoch 0 step 240: training loss: 948.1781653003625\n",
      "Epoch 0 step 241: training accuarcy: 0.8170000000000001\n",
      "Epoch 0 step 241: training loss: 941.0014578923333\n",
      "Epoch 0 step 242: training accuarcy: 0.811\n",
      "Epoch 0 step 242: training loss: 965.1526549540346\n",
      "Epoch 0 step 243: training accuarcy: 0.809\n",
      "Epoch 0 step 243: training loss: 924.8805365681435\n",
      "Epoch 0 step 244: training accuarcy: 0.8200000000000001\n",
      "Epoch 0 step 244: training loss: 956.5790219683149\n",
      "Epoch 0 step 245: training accuarcy: 0.8105\n",
      "Epoch 0 step 245: training loss: 940.5381166168663\n",
      "Epoch 0 step 246: training accuarcy: 0.8170000000000001\n",
      "Epoch 0 step 246: training loss: 908.6571082382211\n",
      "Epoch 0 step 247: training accuarcy: 0.8280000000000001\n",
      "Epoch 0 step 247: training loss: 953.3527716585538\n",
      "Epoch 0 step 248: training accuarcy: 0.8215\n",
      "Epoch 0 step 248: training loss: 914.0742343643035\n",
      "Epoch 0 step 249: training accuarcy: 0.8290000000000001\n",
      "Epoch 0 step 249: training loss: 945.1273228004935\n",
      "Epoch 0 step 250: training accuarcy: 0.8115\n",
      "Epoch 0 step 250: training loss: 939.9764218492484\n",
      "Epoch 0 step 251: training accuarcy: 0.8135\n",
      "Epoch 0 step 251: training loss: 909.594492028925\n",
      "Epoch 0 step 252: training accuarcy: 0.8305\n",
      "Epoch 0 step 252: training loss: 909.8201499567634\n",
      "Epoch 0 step 253: training accuarcy: 0.8290000000000001\n",
      "Epoch 0 step 253: training loss: 917.4698763442904\n",
      "Epoch 0 step 254: training accuarcy: 0.8205\n",
      "Epoch 0 step 254: training loss: 907.4188914637263\n",
      "Epoch 0 step 255: training accuarcy: 0.838\n",
      "Epoch 0 step 255: training loss: 915.4802976227862\n",
      "Epoch 0 step 256: training accuarcy: 0.8265\n",
      "Epoch 0 step 256: training loss: 899.0249880605573\n",
      "Epoch 0 step 257: training accuarcy: 0.8285\n",
      "Epoch 0 step 257: training loss: 926.4986252078372\n",
      "Epoch 0 step 258: training accuarcy: 0.8200000000000001\n",
      "Epoch 0 step 258: training loss: 903.8879917115617\n",
      "Epoch 0 step 259: training accuarcy: 0.836\n",
      "Epoch 0 step 259: training loss: 929.8060604047384\n",
      "Epoch 0 step 260: training accuarcy: 0.8205\n",
      "Epoch 0 step 260: training loss: 923.8701002836611\n",
      "Epoch 0 step 261: training accuarcy: 0.8240000000000001\n",
      "Epoch 0 step 261: training loss: 927.7755014363437\n",
      "Epoch 0 step 262: training accuarcy: 0.8215\n",
      "Epoch 0 step 262: training loss: 497.6107484026444\n",
      "Epoch 0 step 263: training accuarcy: 0.7935897435897435\n",
      "Epoch 0: train loss 6602.544568617339, train accuarcy 0.6994403600692749\n",
      "Epoch 0: valid loss 4509.814084073633, valid accuarcy 0.7812214493751526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▍                                                                                                                                         | 1/5 [05:30<22:01, 330.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 893.8440470362575\n",
      "Epoch 1 step 264: training accuarcy: 0.8385\n",
      "Epoch 1 step 264: training loss: 880.6706700027167\n",
      "Epoch 1 step 265: training accuarcy: 0.8320000000000001\n",
      "Epoch 1 step 265: training loss: 883.6945793379331\n",
      "Epoch 1 step 266: training accuarcy: 0.8345\n",
      "Epoch 1 step 266: training loss: 884.2245551809732\n",
      "Epoch 1 step 267: training accuarcy: 0.8405\n",
      "Epoch 1 step 267: training loss: 905.0637195152724\n",
      "Epoch 1 step 268: training accuarcy: 0.8225\n",
      "Epoch 1 step 268: training loss: 873.3723081387003\n",
      "Epoch 1 step 269: training accuarcy: 0.8320000000000001\n",
      "Epoch 1 step 269: training loss: 889.42313099631\n",
      "Epoch 1 step 270: training accuarcy: 0.8335\n",
      "Epoch 1 step 270: training loss: 872.8751072640475\n",
      "Epoch 1 step 271: training accuarcy: 0.841\n",
      "Epoch 1 step 271: training loss: 886.7884586266921\n",
      "Epoch 1 step 272: training accuarcy: 0.838\n",
      "Epoch 1 step 272: training loss: 874.2252321386115\n",
      "Epoch 1 step 273: training accuarcy: 0.8415\n",
      "Epoch 1 step 273: training loss: 892.2415265161827\n",
      "Epoch 1 step 274: training accuarcy: 0.8245\n",
      "Epoch 1 step 274: training loss: 861.6781831791334\n",
      "Epoch 1 step 275: training accuarcy: 0.833\n",
      "Epoch 1 step 275: training loss: 897.2525292686845\n",
      "Epoch 1 step 276: training accuarcy: 0.8235\n",
      "Epoch 1 step 276: training loss: 854.4685540461476\n",
      "Epoch 1 step 277: training accuarcy: 0.839\n",
      "Epoch 1 step 277: training loss: 877.1725032351054\n",
      "Epoch 1 step 278: training accuarcy: 0.8365\n",
      "Epoch 1 step 278: training loss: 841.9022570406762\n",
      "Epoch 1 step 279: training accuarcy: 0.8435\n",
      "Epoch 1 step 279: training loss: 816.9097357611829\n",
      "Epoch 1 step 280: training accuarcy: 0.854\n",
      "Epoch 1 step 280: training loss: 828.1457650229104\n",
      "Epoch 1 step 281: training accuarcy: 0.845\n",
      "Epoch 1 step 281: training loss: 887.729863609374\n",
      "Epoch 1 step 282: training accuarcy: 0.836\n",
      "Epoch 1 step 282: training loss: 850.1939938979741\n",
      "Epoch 1 step 283: training accuarcy: 0.8335\n",
      "Epoch 1 step 283: training loss: 877.7477147594345\n",
      "Epoch 1 step 284: training accuarcy: 0.8270000000000001\n",
      "Epoch 1 step 284: training loss: 871.2612288035863\n",
      "Epoch 1 step 285: training accuarcy: 0.8415\n",
      "Epoch 1 step 285: training loss: 870.3164091255628\n",
      "Epoch 1 step 286: training accuarcy: 0.835\n",
      "Epoch 1 step 286: training loss: 852.2677788934363\n",
      "Epoch 1 step 287: training accuarcy: 0.8405\n",
      "Epoch 1 step 287: training loss: 869.1190890984979\n",
      "Epoch 1 step 288: training accuarcy: 0.8385\n",
      "Epoch 1 step 288: training loss: 896.4853754971546\n",
      "Epoch 1 step 289: training accuarcy: 0.8305\n",
      "Epoch 1 step 289: training loss: 829.6290033555678\n",
      "Epoch 1 step 290: training accuarcy: 0.851\n",
      "Epoch 1 step 290: training loss: 852.870514305635\n",
      "Epoch 1 step 291: training accuarcy: 0.8335\n",
      "Epoch 1 step 291: training loss: 852.0643263621606\n",
      "Epoch 1 step 292: training accuarcy: 0.8455\n",
      "Epoch 1 step 292: training loss: 843.8414629327583\n",
      "Epoch 1 step 293: training accuarcy: 0.843\n",
      "Epoch 1 step 293: training loss: 871.3710668851313\n",
      "Epoch 1 step 294: training accuarcy: 0.833\n",
      "Epoch 1 step 294: training loss: 871.4932700734433\n",
      "Epoch 1 step 295: training accuarcy: 0.8325\n",
      "Epoch 1 step 295: training loss: 859.8065863824339\n",
      "Epoch 1 step 296: training accuarcy: 0.8285\n",
      "Epoch 1 step 296: training loss: 887.357353082133\n",
      "Epoch 1 step 297: training accuarcy: 0.8325\n",
      "Epoch 1 step 297: training loss: 872.4837556425445\n",
      "Epoch 1 step 298: training accuarcy: 0.8260000000000001\n",
      "Epoch 1 step 298: training loss: 861.9834217421611\n",
      "Epoch 1 step 299: training accuarcy: 0.8405\n",
      "Epoch 1 step 299: training loss: 899.9676524474918\n",
      "Epoch 1 step 300: training accuarcy: 0.836\n",
      "Epoch 1 step 300: training loss: 824.1540050714952\n",
      "Epoch 1 step 301: training accuarcy: 0.8415\n",
      "Epoch 1 step 301: training loss: 885.7577999014139\n",
      "Epoch 1 step 302: training accuarcy: 0.8305\n",
      "Epoch 1 step 302: training loss: 836.6167883377037\n",
      "Epoch 1 step 303: training accuarcy: 0.849\n",
      "Epoch 1 step 303: training loss: 846.0800440391126\n",
      "Epoch 1 step 304: training accuarcy: 0.8415\n",
      "Epoch 1 step 304: training loss: 848.2586909451045\n",
      "Epoch 1 step 305: training accuarcy: 0.836\n",
      "Epoch 1 step 305: training loss: 884.4697710571179\n",
      "Epoch 1 step 306: training accuarcy: 0.835\n",
      "Epoch 1 step 306: training loss: 887.7556697005499\n",
      "Epoch 1 step 307: training accuarcy: 0.8425\n",
      "Epoch 1 step 307: training loss: 871.4962176821226\n",
      "Epoch 1 step 308: training accuarcy: 0.8345\n",
      "Epoch 1 step 308: training loss: 837.5010284717283\n",
      "Epoch 1 step 309: training accuarcy: 0.8415\n",
      "Epoch 1 step 309: training loss: 866.5273346835289\n",
      "Epoch 1 step 310: training accuarcy: 0.835\n",
      "Epoch 1 step 310: training loss: 853.9856046312852\n",
      "Epoch 1 step 311: training accuarcy: 0.8425\n",
      "Epoch 1 step 311: training loss: 844.3154007644736\n",
      "Epoch 1 step 312: training accuarcy: 0.842\n",
      "Epoch 1 step 312: training loss: 865.9888697556978\n",
      "Epoch 1 step 313: training accuarcy: 0.833\n",
      "Epoch 1 step 313: training loss: 861.8147919786473\n",
      "Epoch 1 step 314: training accuarcy: 0.844\n",
      "Epoch 1 step 314: training loss: 859.6249142401451\n",
      "Epoch 1 step 315: training accuarcy: 0.8365\n",
      "Epoch 1 step 315: training loss: 855.4297041992708\n",
      "Epoch 1 step 316: training accuarcy: 0.839\n",
      "Epoch 1 step 316: training loss: 828.1348826442825\n",
      "Epoch 1 step 317: training accuarcy: 0.846\n",
      "Epoch 1 step 317: training loss: 877.1257415365543\n",
      "Epoch 1 step 318: training accuarcy: 0.8345\n",
      "Epoch 1 step 318: training loss: 855.3516848067259\n",
      "Epoch 1 step 319: training accuarcy: 0.8375\n",
      "Epoch 1 step 319: training loss: 864.5670985667438\n",
      "Epoch 1 step 320: training accuarcy: 0.8375\n",
      "Epoch 1 step 320: training loss: 830.8473795770174\n",
      "Epoch 1 step 321: training accuarcy: 0.8435\n",
      "Epoch 1 step 321: training loss: 839.3234596598643\n",
      "Epoch 1 step 322: training accuarcy: 0.842\n",
      "Epoch 1 step 322: training loss: 841.0339470047861\n",
      "Epoch 1 step 323: training accuarcy: 0.837\n",
      "Epoch 1 step 323: training loss: 809.141201905003\n",
      "Epoch 1 step 324: training accuarcy: 0.853\n",
      "Epoch 1 step 324: training loss: 844.0920113458694\n",
      "Epoch 1 step 325: training accuarcy: 0.84\n",
      "Epoch 1 step 325: training loss: 877.0224852211326\n",
      "Epoch 1 step 326: training accuarcy: 0.841\n",
      "Epoch 1 step 326: training loss: 889.0189756479986\n",
      "Epoch 1 step 327: training accuarcy: 0.836\n",
      "Epoch 1 step 327: training loss: 865.4289426568835\n",
      "Epoch 1 step 328: training accuarcy: 0.838\n",
      "Epoch 1 step 328: training loss: 831.8907539122333\n",
      "Epoch 1 step 329: training accuarcy: 0.8385\n",
      "Epoch 1 step 329: training loss: 881.1922522032321\n",
      "Epoch 1 step 330: training accuarcy: 0.833\n",
      "Epoch 1 step 330: training loss: 842.9234874361023\n",
      "Epoch 1 step 331: training accuarcy: 0.8465\n",
      "Epoch 1 step 331: training loss: 815.4510625881677\n",
      "Epoch 1 step 332: training accuarcy: 0.85\n",
      "Epoch 1 step 332: training loss: 850.0016293582996\n",
      "Epoch 1 step 333: training accuarcy: 0.8395\n",
      "Epoch 1 step 333: training loss: 820.5170581464865\n",
      "Epoch 1 step 334: training accuarcy: 0.845\n",
      "Epoch 1 step 334: training loss: 844.8464634455161\n",
      "Epoch 1 step 335: training accuarcy: 0.845\n",
      "Epoch 1 step 335: training loss: 843.6037282647295\n",
      "Epoch 1 step 336: training accuarcy: 0.839\n",
      "Epoch 1 step 336: training loss: 836.6449096844367\n",
      "Epoch 1 step 337: training accuarcy: 0.8405\n",
      "Epoch 1 step 337: training loss: 848.3403391605505\n",
      "Epoch 1 step 338: training accuarcy: 0.8525\n",
      "Epoch 1 step 338: training loss: 869.354016233553\n",
      "Epoch 1 step 339: training accuarcy: 0.8265\n",
      "Epoch 1 step 339: training loss: 801.5304847374607\n",
      "Epoch 1 step 340: training accuarcy: 0.852\n",
      "Epoch 1 step 340: training loss: 812.2991189687505\n",
      "Epoch 1 step 341: training accuarcy: 0.8475\n",
      "Epoch 1 step 341: training loss: 844.3840687105595\n",
      "Epoch 1 step 342: training accuarcy: 0.8485\n",
      "Epoch 1 step 342: training loss: 842.4926616233391\n",
      "Epoch 1 step 343: training accuarcy: 0.841\n",
      "Epoch 1 step 343: training loss: 847.2177648293169\n",
      "Epoch 1 step 344: training accuarcy: 0.848\n",
      "Epoch 1 step 344: training loss: 849.2727313652406\n",
      "Epoch 1 step 345: training accuarcy: 0.849\n",
      "Epoch 1 step 345: training loss: 843.0236632572971\n",
      "Epoch 1 step 346: training accuarcy: 0.842\n",
      "Epoch 1 step 346: training loss: 828.9806637869158\n",
      "Epoch 1 step 347: training accuarcy: 0.8495\n",
      "Epoch 1 step 347: training loss: 861.0717769985962\n",
      "Epoch 1 step 348: training accuarcy: 0.841\n",
      "Epoch 1 step 348: training loss: 844.2992522823397\n",
      "Epoch 1 step 349: training accuarcy: 0.8425\n",
      "Epoch 1 step 349: training loss: 807.4401329222117\n",
      "Epoch 1 step 350: training accuarcy: 0.8475\n",
      "Epoch 1 step 350: training loss: 844.8798639380415\n",
      "Epoch 1 step 351: training accuarcy: 0.8385\n",
      "Epoch 1 step 351: training loss: 847.9051155132863\n",
      "Epoch 1 step 352: training accuarcy: 0.8445\n",
      "Epoch 1 step 352: training loss: 815.9216154122391\n",
      "Epoch 1 step 353: training accuarcy: 0.854\n",
      "Epoch 1 step 353: training loss: 800.9172430590311\n",
      "Epoch 1 step 354: training accuarcy: 0.859\n",
      "Epoch 1 step 354: training loss: 841.3542480996811\n",
      "Epoch 1 step 355: training accuarcy: 0.842\n",
      "Epoch 1 step 355: training loss: 851.8702077903171\n",
      "Epoch 1 step 356: training accuarcy: 0.8425\n",
      "Epoch 1 step 356: training loss: 850.2457208981078\n",
      "Epoch 1 step 357: training accuarcy: 0.833\n",
      "Epoch 1 step 357: training loss: 878.4485850223274\n",
      "Epoch 1 step 358: training accuarcy: 0.8270000000000001\n",
      "Epoch 1 step 358: training loss: 848.4152645552181\n",
      "Epoch 1 step 359: training accuarcy: 0.838\n",
      "Epoch 1 step 359: training loss: 779.8102155248232\n",
      "Epoch 1 step 360: training accuarcy: 0.8585\n",
      "Epoch 1 step 360: training loss: 836.3326308729553\n",
      "Epoch 1 step 361: training accuarcy: 0.842\n",
      "Epoch 1 step 361: training loss: 842.1381519717486\n",
      "Epoch 1 step 362: training accuarcy: 0.835\n",
      "Epoch 1 step 362: training loss: 807.1377994421294\n",
      "Epoch 1 step 363: training accuarcy: 0.8565\n",
      "Epoch 1 step 363: training loss: 802.5418190658045\n",
      "Epoch 1 step 364: training accuarcy: 0.849\n",
      "Epoch 1 step 364: training loss: 808.8709816228009\n",
      "Epoch 1 step 365: training accuarcy: 0.849\n",
      "Epoch 1 step 365: training loss: 809.873254309636\n",
      "Epoch 1 step 366: training accuarcy: 0.854\n",
      "Epoch 1 step 366: training loss: 847.4997870344687\n",
      "Epoch 1 step 367: training accuarcy: 0.8405\n",
      "Epoch 1 step 367: training loss: 845.4539669717584\n",
      "Epoch 1 step 368: training accuarcy: 0.849\n",
      "Epoch 1 step 368: training loss: 827.1128871639291\n",
      "Epoch 1 step 369: training accuarcy: 0.843\n",
      "Epoch 1 step 369: training loss: 833.1285322137021\n",
      "Epoch 1 step 370: training accuarcy: 0.8385\n",
      "Epoch 1 step 370: training loss: 808.8975356413015\n",
      "Epoch 1 step 371: training accuarcy: 0.8505\n",
      "Epoch 1 step 371: training loss: 847.9483294593292\n",
      "Epoch 1 step 372: training accuarcy: 0.838\n",
      "Epoch 1 step 372: training loss: 838.3646751922943\n",
      "Epoch 1 step 373: training accuarcy: 0.8465\n",
      "Epoch 1 step 373: training loss: 783.6231366174841\n",
      "Epoch 1 step 374: training accuarcy: 0.858\n",
      "Epoch 1 step 374: training loss: 806.3381116481248\n",
      "Epoch 1 step 375: training accuarcy: 0.8555\n",
      "Epoch 1 step 375: training loss: 846.975761001268\n",
      "Epoch 1 step 376: training accuarcy: 0.8485\n",
      "Epoch 1 step 376: training loss: 784.946823425832\n",
      "Epoch 1 step 377: training accuarcy: 0.8545\n",
      "Epoch 1 step 377: training loss: 845.2570643413169\n",
      "Epoch 1 step 378: training accuarcy: 0.842\n",
      "Epoch 1 step 378: training loss: 837.7285104812843\n",
      "Epoch 1 step 379: training accuarcy: 0.846\n",
      "Epoch 1 step 379: training loss: 827.6403266969569\n",
      "Epoch 1 step 380: training accuarcy: 0.8425\n",
      "Epoch 1 step 380: training loss: 815.342504053378\n",
      "Epoch 1 step 381: training accuarcy: 0.8535\n",
      "Epoch 1 step 381: training loss: 821.7740660849041\n",
      "Epoch 1 step 382: training accuarcy: 0.8435\n",
      "Epoch 1 step 382: training loss: 823.5927559263105\n",
      "Epoch 1 step 383: training accuarcy: 0.852\n",
      "Epoch 1 step 383: training loss: 802.6475807478012\n",
      "Epoch 1 step 384: training accuarcy: 0.8505\n",
      "Epoch 1 step 384: training loss: 857.9507371020816\n",
      "Epoch 1 step 385: training accuarcy: 0.8315\n",
      "Epoch 1 step 385: training loss: 800.8363612400431\n",
      "Epoch 1 step 386: training accuarcy: 0.853\n",
      "Epoch 1 step 386: training loss: 811.653606925975\n",
      "Epoch 1 step 387: training accuarcy: 0.8555\n",
      "Epoch 1 step 387: training loss: 841.8070611550996\n",
      "Epoch 1 step 388: training accuarcy: 0.8385\n",
      "Epoch 1 step 388: training loss: 799.2113459818911\n",
      "Epoch 1 step 389: training accuarcy: 0.853\n",
      "Epoch 1 step 389: training loss: 825.6821567936007\n",
      "Epoch 1 step 390: training accuarcy: 0.847\n",
      "Epoch 1 step 390: training loss: 837.7594396978906\n",
      "Epoch 1 step 391: training accuarcy: 0.8455\n",
      "Epoch 1 step 391: training loss: 810.2763601189047\n",
      "Epoch 1 step 392: training accuarcy: 0.857\n",
      "Epoch 1 step 392: training loss: 808.2433802276003\n",
      "Epoch 1 step 393: training accuarcy: 0.8515\n",
      "Epoch 1 step 393: training loss: 817.5976098493804\n",
      "Epoch 1 step 394: training accuarcy: 0.856\n",
      "Epoch 1 step 394: training loss: 858.4833669434356\n",
      "Epoch 1 step 395: training accuarcy: 0.8415\n",
      "Epoch 1 step 395: training loss: 813.8579036377926\n",
      "Epoch 1 step 396: training accuarcy: 0.8595\n",
      "Epoch 1 step 396: training loss: 791.8349909982893\n",
      "Epoch 1 step 397: training accuarcy: 0.854\n",
      "Epoch 1 step 397: training loss: 829.068777263466\n",
      "Epoch 1 step 398: training accuarcy: 0.8395\n",
      "Epoch 1 step 398: training loss: 821.1222904974019\n",
      "Epoch 1 step 399: training accuarcy: 0.8435\n",
      "Epoch 1 step 399: training loss: 847.724242496117\n",
      "Epoch 1 step 400: training accuarcy: 0.8475\n",
      "Epoch 1 step 400: training loss: 800.8777036937884\n",
      "Epoch 1 step 401: training accuarcy: 0.851\n",
      "Epoch 1 step 401: training loss: 790.4285858889007\n",
      "Epoch 1 step 402: training accuarcy: 0.859\n",
      "Epoch 1 step 402: training loss: 827.0328583189175\n",
      "Epoch 1 step 403: training accuarcy: 0.857\n",
      "Epoch 1 step 403: training loss: 824.0999137714036\n",
      "Epoch 1 step 404: training accuarcy: 0.856\n",
      "Epoch 1 step 404: training loss: 813.0055310580169\n",
      "Epoch 1 step 405: training accuarcy: 0.8485\n",
      "Epoch 1 step 405: training loss: 807.0607086387283\n",
      "Epoch 1 step 406: training accuarcy: 0.8495\n",
      "Epoch 1 step 406: training loss: 794.5899916159456\n",
      "Epoch 1 step 407: training accuarcy: 0.8595\n",
      "Epoch 1 step 407: training loss: 830.4609075803698\n",
      "Epoch 1 step 408: training accuarcy: 0.846\n",
      "Epoch 1 step 408: training loss: 842.0631237016916\n",
      "Epoch 1 step 409: training accuarcy: 0.8425\n",
      "Epoch 1 step 409: training loss: 849.6837757394694\n",
      "Epoch 1 step 410: training accuarcy: 0.833\n",
      "Epoch 1 step 410: training loss: 825.5706371715765\n",
      "Epoch 1 step 411: training accuarcy: 0.845\n",
      "Epoch 1 step 411: training loss: 815.1018601502884\n",
      "Epoch 1 step 412: training accuarcy: 0.848\n",
      "Epoch 1 step 412: training loss: 825.6225815775977\n",
      "Epoch 1 step 413: training accuarcy: 0.845\n",
      "Epoch 1 step 413: training loss: 840.9713473595658\n",
      "Epoch 1 step 414: training accuarcy: 0.8445\n",
      "Epoch 1 step 414: training loss: 823.3032373772131\n",
      "Epoch 1 step 415: training accuarcy: 0.849\n",
      "Epoch 1 step 415: training loss: 816.3283581558887\n",
      "Epoch 1 step 416: training accuarcy: 0.858\n",
      "Epoch 1 step 416: training loss: 822.4546622008376\n",
      "Epoch 1 step 417: training accuarcy: 0.8475\n",
      "Epoch 1 step 417: training loss: 806.5844465343417\n",
      "Epoch 1 step 418: training accuarcy: 0.852\n",
      "Epoch 1 step 418: training loss: 796.2348781426188\n",
      "Epoch 1 step 419: training accuarcy: 0.8555\n",
      "Epoch 1 step 419: training loss: 795.0492547038284\n",
      "Epoch 1 step 420: training accuarcy: 0.8505\n",
      "Epoch 1 step 420: training loss: 798.9962313808965\n",
      "Epoch 1 step 421: training accuarcy: 0.852\n",
      "Epoch 1 step 421: training loss: 818.2068575406247\n",
      "Epoch 1 step 422: training accuarcy: 0.8565\n",
      "Epoch 1 step 422: training loss: 849.1728138471061\n",
      "Epoch 1 step 423: training accuarcy: 0.838\n",
      "Epoch 1 step 423: training loss: 812.2523784761503\n",
      "Epoch 1 step 424: training accuarcy: 0.849\n",
      "Epoch 1 step 424: training loss: 819.3065981226489\n",
      "Epoch 1 step 425: training accuarcy: 0.848\n",
      "Epoch 1 step 425: training loss: 832.7390862553552\n",
      "Epoch 1 step 426: training accuarcy: 0.8405\n",
      "Epoch 1 step 426: training loss: 821.2324783591686\n",
      "Epoch 1 step 427: training accuarcy: 0.848\n",
      "Epoch 1 step 427: training loss: 829.9657468784449\n",
      "Epoch 1 step 428: training accuarcy: 0.85\n",
      "Epoch 1 step 428: training loss: 824.7547801157979\n",
      "Epoch 1 step 429: training accuarcy: 0.856\n",
      "Epoch 1 step 429: training loss: 823.151983469126\n",
      "Epoch 1 step 430: training accuarcy: 0.8535\n",
      "Epoch 1 step 430: training loss: 793.1098720578979\n",
      "Epoch 1 step 431: training accuarcy: 0.8635\n",
      "Epoch 1 step 431: training loss: 783.428383342406\n",
      "Epoch 1 step 432: training accuarcy: 0.8585\n",
      "Epoch 1 step 432: training loss: 820.317492033005\n",
      "Epoch 1 step 433: training accuarcy: 0.859\n",
      "Epoch 1 step 433: training loss: 801.8926573571264\n",
      "Epoch 1 step 434: training accuarcy: 0.8585\n",
      "Epoch 1 step 434: training loss: 830.9609172135233\n",
      "Epoch 1 step 435: training accuarcy: 0.8455\n",
      "Epoch 1 step 435: training loss: 829.8992051552887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 436: training accuarcy: 0.8465\n",
      "Epoch 1 step 436: training loss: 804.0164729016205\n",
      "Epoch 1 step 437: training accuarcy: 0.864\n",
      "Epoch 1 step 437: training loss: 856.2860340998391\n",
      "Epoch 1 step 438: training accuarcy: 0.8375\n",
      "Epoch 1 step 438: training loss: 755.4863209171469\n",
      "Epoch 1 step 439: training accuarcy: 0.8665\n",
      "Epoch 1 step 439: training loss: 809.1741931214162\n",
      "Epoch 1 step 440: training accuarcy: 0.8455\n",
      "Epoch 1 step 440: training loss: 820.6142935096415\n",
      "Epoch 1 step 441: training accuarcy: 0.8515\n",
      "Epoch 1 step 441: training loss: 813.561905797404\n",
      "Epoch 1 step 442: training accuarcy: 0.848\n",
      "Epoch 1 step 442: training loss: 814.2686491318643\n",
      "Epoch 1 step 443: training accuarcy: 0.849\n",
      "Epoch 1 step 443: training loss: 823.8053186257616\n",
      "Epoch 1 step 444: training accuarcy: 0.851\n",
      "Epoch 1 step 444: training loss: 821.8444857737215\n",
      "Epoch 1 step 445: training accuarcy: 0.8435\n",
      "Epoch 1 step 445: training loss: 819.8574794794114\n",
      "Epoch 1 step 446: training accuarcy: 0.852\n",
      "Epoch 1 step 446: training loss: 824.2462601377131\n",
      "Epoch 1 step 447: training accuarcy: 0.848\n",
      "Epoch 1 step 447: training loss: 804.3610685325714\n",
      "Epoch 1 step 448: training accuarcy: 0.8595\n",
      "Epoch 1 step 448: training loss: 837.8263810666349\n",
      "Epoch 1 step 449: training accuarcy: 0.8385\n",
      "Epoch 1 step 449: training loss: 835.1683600507666\n",
      "Epoch 1 step 450: training accuarcy: 0.8475\n",
      "Epoch 1 step 450: training loss: 814.308305413932\n",
      "Epoch 1 step 451: training accuarcy: 0.8555\n",
      "Epoch 1 step 451: training loss: 796.3068817203222\n",
      "Epoch 1 step 452: training accuarcy: 0.853\n",
      "Epoch 1 step 452: training loss: 769.8926819919666\n",
      "Epoch 1 step 453: training accuarcy: 0.859\n",
      "Epoch 1 step 453: training loss: 864.3290245874653\n",
      "Epoch 1 step 454: training accuarcy: 0.8320000000000001\n",
      "Epoch 1 step 454: training loss: 808.6506382361077\n",
      "Epoch 1 step 455: training accuarcy: 0.861\n",
      "Epoch 1 step 455: training loss: 824.2689611533077\n",
      "Epoch 1 step 456: training accuarcy: 0.846\n",
      "Epoch 1 step 456: training loss: 818.3625193202328\n",
      "Epoch 1 step 457: training accuarcy: 0.857\n",
      "Epoch 1 step 457: training loss: 803.6077476607104\n",
      "Epoch 1 step 458: training accuarcy: 0.8625\n",
      "Epoch 1 step 458: training loss: 803.0197898404593\n",
      "Epoch 1 step 459: training accuarcy: 0.86\n",
      "Epoch 1 step 459: training loss: 803.8275241974493\n",
      "Epoch 1 step 460: training accuarcy: 0.8555\n",
      "Epoch 1 step 460: training loss: 802.879746266691\n",
      "Epoch 1 step 461: training accuarcy: 0.863\n",
      "Epoch 1 step 461: training loss: 801.9743451938361\n",
      "Epoch 1 step 462: training accuarcy: 0.8565\n",
      "Epoch 1 step 462: training loss: 821.5714192303541\n",
      "Epoch 1 step 463: training accuarcy: 0.851\n",
      "Epoch 1 step 463: training loss: 811.9649983342956\n",
      "Epoch 1 step 464: training accuarcy: 0.855\n",
      "Epoch 1 step 464: training loss: 827.7519925987617\n",
      "Epoch 1 step 465: training accuarcy: 0.854\n",
      "Epoch 1 step 465: training loss: 768.6541803144432\n",
      "Epoch 1 step 466: training accuarcy: 0.854\n",
      "Epoch 1 step 466: training loss: 794.8690706761051\n",
      "Epoch 1 step 467: training accuarcy: 0.8555\n",
      "Epoch 1 step 467: training loss: 765.9361188199847\n",
      "Epoch 1 step 468: training accuarcy: 0.8645\n",
      "Epoch 1 step 468: training loss: 788.5968617101117\n",
      "Epoch 1 step 469: training accuarcy: 0.8525\n",
      "Epoch 1 step 469: training loss: 839.1434271107524\n",
      "Epoch 1 step 470: training accuarcy: 0.8455\n",
      "Epoch 1 step 470: training loss: 767.886251185724\n",
      "Epoch 1 step 471: training accuarcy: 0.87\n",
      "Epoch 1 step 471: training loss: 822.9011824737245\n",
      "Epoch 1 step 472: training accuarcy: 0.8475\n",
      "Epoch 1 step 472: training loss: 785.4406479603247\n",
      "Epoch 1 step 473: training accuarcy: 0.863\n",
      "Epoch 1 step 473: training loss: 824.5228617838796\n",
      "Epoch 1 step 474: training accuarcy: 0.8525\n",
      "Epoch 1 step 474: training loss: 810.3152355033602\n",
      "Epoch 1 step 475: training accuarcy: 0.8565\n",
      "Epoch 1 step 475: training loss: 807.613477465246\n",
      "Epoch 1 step 476: training accuarcy: 0.852\n",
      "Epoch 1 step 476: training loss: 787.3943522967977\n",
      "Epoch 1 step 477: training accuarcy: 0.858\n",
      "Epoch 1 step 477: training loss: 809.5835697230613\n",
      "Epoch 1 step 478: training accuarcy: 0.8535\n",
      "Epoch 1 step 478: training loss: 803.8787306020668\n",
      "Epoch 1 step 479: training accuarcy: 0.852\n",
      "Epoch 1 step 479: training loss: 795.5947352764545\n",
      "Epoch 1 step 480: training accuarcy: 0.859\n",
      "Epoch 1 step 480: training loss: 820.9216211823039\n",
      "Epoch 1 step 481: training accuarcy: 0.852\n",
      "Epoch 1 step 481: training loss: 835.7582999169011\n",
      "Epoch 1 step 482: training accuarcy: 0.838\n",
      "Epoch 1 step 482: training loss: 811.7260270079515\n",
      "Epoch 1 step 483: training accuarcy: 0.856\n",
      "Epoch 1 step 483: training loss: 790.7568909647634\n",
      "Epoch 1 step 484: training accuarcy: 0.8555\n",
      "Epoch 1 step 484: training loss: 795.7996923963058\n",
      "Epoch 1 step 485: training accuarcy: 0.8615\n",
      "Epoch 1 step 485: training loss: 752.5888974334439\n",
      "Epoch 1 step 486: training accuarcy: 0.8655\n",
      "Epoch 1 step 486: training loss: 770.6187427640591\n",
      "Epoch 1 step 487: training accuarcy: 0.861\n",
      "Epoch 1 step 487: training loss: 769.0446145771231\n",
      "Epoch 1 step 488: training accuarcy: 0.8695\n",
      "Epoch 1 step 488: training loss: 804.8243024671152\n",
      "Epoch 1 step 489: training accuarcy: 0.853\n",
      "Epoch 1 step 489: training loss: 794.0915191846661\n",
      "Epoch 1 step 490: training accuarcy: 0.8515\n",
      "Epoch 1 step 490: training loss: 813.9876198370554\n",
      "Epoch 1 step 491: training accuarcy: 0.8465\n",
      "Epoch 1 step 491: training loss: 816.3666886984314\n",
      "Epoch 1 step 492: training accuarcy: 0.8485\n",
      "Epoch 1 step 492: training loss: 807.0430056122646\n",
      "Epoch 1 step 493: training accuarcy: 0.8535\n",
      "Epoch 1 step 493: training loss: 819.4112009579754\n",
      "Epoch 1 step 494: training accuarcy: 0.844\n",
      "Epoch 1 step 494: training loss: 782.362128463137\n",
      "Epoch 1 step 495: training accuarcy: 0.858\n",
      "Epoch 1 step 495: training loss: 778.3115363507995\n",
      "Epoch 1 step 496: training accuarcy: 0.864\n",
      "Epoch 1 step 496: training loss: 777.8188256963351\n",
      "Epoch 1 step 497: training accuarcy: 0.865\n",
      "Epoch 1 step 497: training loss: 792.3376260509411\n",
      "Epoch 1 step 498: training accuarcy: 0.848\n",
      "Epoch 1 step 498: training loss: 783.3598508856859\n",
      "Epoch 1 step 499: training accuarcy: 0.849\n",
      "Epoch 1 step 499: training loss: 797.8375473247136\n",
      "Epoch 1 step 500: training accuarcy: 0.863\n",
      "Epoch 1 step 500: training loss: 809.5130074028842\n",
      "Epoch 1 step 501: training accuarcy: 0.855\n",
      "Epoch 1 step 501: training loss: 793.561041520171\n",
      "Epoch 1 step 502: training accuarcy: 0.8575\n",
      "Epoch 1 step 502: training loss: 816.9469756816278\n",
      "Epoch 1 step 503: training accuarcy: 0.8465\n",
      "Epoch 1 step 503: training loss: 799.1983753820551\n",
      "Epoch 1 step 504: training accuarcy: 0.8595\n",
      "Epoch 1 step 504: training loss: 798.791833967784\n",
      "Epoch 1 step 505: training accuarcy: 0.8455\n",
      "Epoch 1 step 505: training loss: 799.6489800190782\n",
      "Epoch 1 step 506: training accuarcy: 0.849\n",
      "Epoch 1 step 506: training loss: 802.227915726266\n",
      "Epoch 1 step 507: training accuarcy: 0.851\n",
      "Epoch 1 step 507: training loss: 789.5749073186513\n",
      "Epoch 1 step 508: training accuarcy: 0.862\n",
      "Epoch 1 step 508: training loss: 798.9680371155165\n",
      "Epoch 1 step 509: training accuarcy: 0.858\n",
      "Epoch 1 step 509: training loss: 856.7353299826752\n",
      "Epoch 1 step 510: training accuarcy: 0.838\n",
      "Epoch 1 step 510: training loss: 816.083843025225\n",
      "Epoch 1 step 511: training accuarcy: 0.857\n",
      "Epoch 1 step 511: training loss: 775.2417084626679\n",
      "Epoch 1 step 512: training accuarcy: 0.861\n",
      "Epoch 1 step 512: training loss: 770.6644729008584\n",
      "Epoch 1 step 513: training accuarcy: 0.8625\n",
      "Epoch 1 step 513: training loss: 818.3475808725223\n",
      "Epoch 1 step 514: training accuarcy: 0.854\n",
      "Epoch 1 step 514: training loss: 841.063273396437\n",
      "Epoch 1 step 515: training accuarcy: 0.848\n",
      "Epoch 1 step 515: training loss: 785.8711949317159\n",
      "Epoch 1 step 516: training accuarcy: 0.86\n",
      "Epoch 1 step 516: training loss: 804.2521798698843\n",
      "Epoch 1 step 517: training accuarcy: 0.8495\n",
      "Epoch 1 step 517: training loss: 786.973424185757\n",
      "Epoch 1 step 518: training accuarcy: 0.8585\n",
      "Epoch 1 step 518: training loss: 775.3266301898807\n",
      "Epoch 1 step 519: training accuarcy: 0.862\n",
      "Epoch 1 step 519: training loss: 766.7138028752802\n",
      "Epoch 1 step 520: training accuarcy: 0.864\n",
      "Epoch 1 step 520: training loss: 793.7161667423527\n",
      "Epoch 1 step 521: training accuarcy: 0.847\n",
      "Epoch 1 step 521: training loss: 788.9977416367598\n",
      "Epoch 1 step 522: training accuarcy: 0.8645\n",
      "Epoch 1 step 522: training loss: 790.0478123211773\n",
      "Epoch 1 step 523: training accuarcy: 0.8615\n",
      "Epoch 1 step 523: training loss: 829.648735956617\n",
      "Epoch 1 step 524: training accuarcy: 0.8445\n",
      "Epoch 1 step 524: training loss: 802.0962312427596\n",
      "Epoch 1 step 525: training accuarcy: 0.847\n",
      "Epoch 1 step 525: training loss: 406.49429815887174\n",
      "Epoch 1 step 526: training accuarcy: 0.8692307692307693\n",
      "Epoch 1: train loss 826.0333778833336, train accuarcy 0.8323061466217041\n",
      "Epoch 1: valid loss 3881.1533903455784, valid accuarcy 0.8284419178962708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████████████████████████████▊                                                                                                       | 2/5 [11:25<16:53, 337.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 784.2898922189256\n",
      "Epoch 2 step 527: training accuarcy: 0.8645\n",
      "Epoch 2 step 527: training loss: 753.4402067426277\n",
      "Epoch 2 step 528: training accuarcy: 0.868\n",
      "Epoch 2 step 528: training loss: 749.7542177181322\n",
      "Epoch 2 step 529: training accuarcy: 0.8785000000000001\n",
      "Epoch 2 step 529: training loss: 791.9179860098432\n",
      "Epoch 2 step 530: training accuarcy: 0.8585\n",
      "Epoch 2 step 530: training loss: 761.2608071142489\n",
      "Epoch 2 step 531: training accuarcy: 0.8625\n",
      "Epoch 2 step 531: training loss: 781.7669241719142\n",
      "Epoch 2 step 532: training accuarcy: 0.859\n",
      "Epoch 2 step 532: training loss: 771.0129542310128\n",
      "Epoch 2 step 533: training accuarcy: 0.8605\n",
      "Epoch 2 step 533: training loss: 803.0158494878227\n",
      "Epoch 2 step 534: training accuarcy: 0.8605\n",
      "Epoch 2 step 534: training loss: 759.2105011602422\n",
      "Epoch 2 step 535: training accuarcy: 0.865\n",
      "Epoch 2 step 535: training loss: 771.9554785520455\n",
      "Epoch 2 step 536: training accuarcy: 0.863\n",
      "Epoch 2 step 536: training loss: 756.9259726950918\n",
      "Epoch 2 step 537: training accuarcy: 0.8785000000000001\n",
      "Epoch 2 step 537: training loss: 744.5058738259268\n",
      "Epoch 2 step 538: training accuarcy: 0.877\n",
      "Epoch 2 step 538: training loss: 769.3257200125857\n",
      "Epoch 2 step 539: training accuarcy: 0.865\n",
      "Epoch 2 step 539: training loss: 740.4216883335052\n",
      "Epoch 2 step 540: training accuarcy: 0.8685\n",
      "Epoch 2 step 540: training loss: 792.1367576791126\n",
      "Epoch 2 step 541: training accuarcy: 0.861\n",
      "Epoch 2 step 541: training loss: 746.2611846284141\n",
      "Epoch 2 step 542: training accuarcy: 0.867\n",
      "Epoch 2 step 542: training loss: 799.9662512078567\n",
      "Epoch 2 step 543: training accuarcy: 0.861\n",
      "Epoch 2 step 543: training loss: 811.2984949996235\n",
      "Epoch 2 step 544: training accuarcy: 0.851\n",
      "Epoch 2 step 544: training loss: 786.9086673217041\n",
      "Epoch 2 step 545: training accuarcy: 0.858\n",
      "Epoch 2 step 545: training loss: 769.5374023152017\n",
      "Epoch 2 step 546: training accuarcy: 0.8625\n",
      "Epoch 2 step 546: training loss: 761.2657877406536\n",
      "Epoch 2 step 547: training accuarcy: 0.8615\n",
      "Epoch 2 step 547: training loss: 768.1832218876762\n",
      "Epoch 2 step 548: training accuarcy: 0.8575\n",
      "Epoch 2 step 548: training loss: 758.783548538669\n",
      "Epoch 2 step 549: training accuarcy: 0.869\n",
      "Epoch 2 step 549: training loss: 755.3588868809817\n",
      "Epoch 2 step 550: training accuarcy: 0.864\n",
      "Epoch 2 step 550: training loss: 762.7348187082246\n",
      "Epoch 2 step 551: training accuarcy: 0.867\n",
      "Epoch 2 step 551: training loss: 777.7272108049805\n",
      "Epoch 2 step 552: training accuarcy: 0.863\n",
      "Epoch 2 step 552: training loss: 741.1108999372393\n",
      "Epoch 2 step 553: training accuarcy: 0.87\n",
      "Epoch 2 step 553: training loss: 797.1115052037053\n",
      "Epoch 2 step 554: training accuarcy: 0.8505\n",
      "Epoch 2 step 554: training loss: 755.0403658706639\n",
      "Epoch 2 step 555: training accuarcy: 0.8755000000000001\n",
      "Epoch 2 step 555: training loss: 823.7105914022175\n",
      "Epoch 2 step 556: training accuarcy: 0.857\n",
      "Epoch 2 step 556: training loss: 739.8619389431999\n",
      "Epoch 2 step 557: training accuarcy: 0.8725\n",
      "Epoch 2 step 557: training loss: 779.6249127617883\n",
      "Epoch 2 step 558: training accuarcy: 0.856\n",
      "Epoch 2 step 558: training loss: 799.7329159064749\n",
      "Epoch 2 step 559: training accuarcy: 0.86\n",
      "Epoch 2 step 559: training loss: 760.8689187132004\n",
      "Epoch 2 step 560: training accuarcy: 0.866\n",
      "Epoch 2 step 560: training loss: 767.5269502790511\n",
      "Epoch 2 step 561: training accuarcy: 0.8635\n",
      "Epoch 2 step 561: training loss: 788.4907836902998\n",
      "Epoch 2 step 562: training accuarcy: 0.852\n",
      "Epoch 2 step 562: training loss: 772.0995021570184\n",
      "Epoch 2 step 563: training accuarcy: 0.8665\n",
      "Epoch 2 step 563: training loss: 781.6612629982579\n",
      "Epoch 2 step 564: training accuarcy: 0.8605\n",
      "Epoch 2 step 564: training loss: 774.4872131866654\n",
      "Epoch 2 step 565: training accuarcy: 0.864\n",
      "Epoch 2 step 565: training loss: 753.430607940593\n",
      "Epoch 2 step 566: training accuarcy: 0.875\n",
      "Epoch 2 step 566: training loss: 771.5496058824442\n",
      "Epoch 2 step 567: training accuarcy: 0.8595\n",
      "Epoch 2 step 567: training loss: 782.467933331286\n",
      "Epoch 2 step 568: training accuarcy: 0.854\n",
      "Epoch 2 step 568: training loss: 757.4711491745317\n",
      "Epoch 2 step 569: training accuarcy: 0.8665\n",
      "Epoch 2 step 569: training loss: 741.2709273540372\n",
      "Epoch 2 step 570: training accuarcy: 0.8595\n",
      "Epoch 2 step 570: training loss: 772.2427454441082\n",
      "Epoch 2 step 571: training accuarcy: 0.866\n",
      "Epoch 2 step 571: training loss: 772.3466250824412\n",
      "Epoch 2 step 572: training accuarcy: 0.861\n",
      "Epoch 2 step 572: training loss: 767.8634157059887\n",
      "Epoch 2 step 573: training accuarcy: 0.8625\n",
      "Epoch 2 step 573: training loss: 789.4334856016226\n",
      "Epoch 2 step 574: training accuarcy: 0.8575\n",
      "Epoch 2 step 574: training loss: 752.4041760976968\n",
      "Epoch 2 step 575: training accuarcy: 0.8765000000000001\n",
      "Epoch 2 step 575: training loss: 770.0835995191314\n",
      "Epoch 2 step 576: training accuarcy: 0.867\n",
      "Epoch 2 step 576: training loss: 792.1947375441488\n",
      "Epoch 2 step 577: training accuarcy: 0.8575\n",
      "Epoch 2 step 577: training loss: 797.2009789049969\n",
      "Epoch 2 step 578: training accuarcy: 0.858\n",
      "Epoch 2 step 578: training loss: 765.7344653321493\n",
      "Epoch 2 step 579: training accuarcy: 0.8675\n",
      "Epoch 2 step 579: training loss: 769.1457146561727\n",
      "Epoch 2 step 580: training accuarcy: 0.864\n",
      "Epoch 2 step 580: training loss: 758.7397320307978\n",
      "Epoch 2 step 581: training accuarcy: 0.8695\n",
      "Epoch 2 step 581: training loss: 749.2966606760316\n",
      "Epoch 2 step 582: training accuarcy: 0.873\n",
      "Epoch 2 step 582: training loss: 807.3099215093602\n",
      "Epoch 2 step 583: training accuarcy: 0.8535\n",
      "Epoch 2 step 583: training loss: 807.4760400870473\n",
      "Epoch 2 step 584: training accuarcy: 0.847\n",
      "Epoch 2 step 584: training loss: 793.6659758594562\n",
      "Epoch 2 step 585: training accuarcy: 0.85\n",
      "Epoch 2 step 585: training loss: 746.3864668807278\n",
      "Epoch 2 step 586: training accuarcy: 0.8845000000000001\n",
      "Epoch 2 step 586: training loss: 783.6517424633354\n",
      "Epoch 2 step 587: training accuarcy: 0.872\n",
      "Epoch 2 step 587: training loss: 811.4674018759807\n",
      "Epoch 2 step 588: training accuarcy: 0.859\n",
      "Epoch 2 step 588: training loss: 793.5649501050619\n",
      "Epoch 2 step 589: training accuarcy: 0.856\n",
      "Epoch 2 step 589: training loss: 800.0184984680869\n",
      "Epoch 2 step 590: training accuarcy: 0.857\n",
      "Epoch 2 step 590: training loss: 757.5768934324192\n",
      "Epoch 2 step 591: training accuarcy: 0.87\n",
      "Epoch 2 step 591: training loss: 784.6972025564114\n",
      "Epoch 2 step 592: training accuarcy: 0.866\n",
      "Epoch 2 step 592: training loss: 741.433714690438\n",
      "Epoch 2 step 593: training accuarcy: 0.871\n",
      "Epoch 2 step 593: training loss: 765.2390733051053\n",
      "Epoch 2 step 594: training accuarcy: 0.8775000000000001\n",
      "Epoch 2 step 594: training loss: 724.0725326929589\n",
      "Epoch 2 step 595: training accuarcy: 0.8775000000000001\n",
      "Epoch 2 step 595: training loss: 769.3243840592074\n",
      "Epoch 2 step 596: training accuarcy: 0.8695\n",
      "Epoch 2 step 596: training loss: 770.8616035251066\n",
      "Epoch 2 step 597: training accuarcy: 0.865\n",
      "Epoch 2 step 597: training loss: 772.5726678334588\n",
      "Epoch 2 step 598: training accuarcy: 0.8725\n",
      "Epoch 2 step 598: training loss: 784.6936417533933\n",
      "Epoch 2 step 599: training accuarcy: 0.867\n",
      "Epoch 2 step 599: training loss: 813.6950525940186\n",
      "Epoch 2 step 600: training accuarcy: 0.856\n",
      "Epoch 2 step 600: training loss: 762.23364048143\n",
      "Epoch 2 step 601: training accuarcy: 0.8585\n",
      "Epoch 2 step 601: training loss: 736.5928553006713\n",
      "Epoch 2 step 602: training accuarcy: 0.868\n",
      "Epoch 2 step 602: training loss: 767.8400758884836\n",
      "Epoch 2 step 603: training accuarcy: 0.8665\n",
      "Epoch 2 step 603: training loss: 787.1019779008501\n",
      "Epoch 2 step 604: training accuarcy: 0.861\n",
      "Epoch 2 step 604: training loss: 757.0103058413001\n",
      "Epoch 2 step 605: training accuarcy: 0.8655\n",
      "Epoch 2 step 605: training loss: 757.8158142755849\n",
      "Epoch 2 step 606: training accuarcy: 0.8685\n",
      "Epoch 2 step 606: training loss: 793.7575243725372\n",
      "Epoch 2 step 607: training accuarcy: 0.8525\n",
      "Epoch 2 step 607: training loss: 784.6967080493646\n",
      "Epoch 2 step 608: training accuarcy: 0.8675\n",
      "Epoch 2 step 608: training loss: 771.88120051588\n",
      "Epoch 2 step 609: training accuarcy: 0.8585\n",
      "Epoch 2 step 609: training loss: 832.086759160297\n",
      "Epoch 2 step 610: training accuarcy: 0.847\n",
      "Epoch 2 step 610: training loss: 787.4858561849933\n",
      "Epoch 2 step 611: training accuarcy: 0.85\n",
      "Epoch 2 step 611: training loss: 766.2049892591295\n",
      "Epoch 2 step 612: training accuarcy: 0.8695\n",
      "Epoch 2 step 612: training loss: 780.544988510383\n",
      "Epoch 2 step 613: training accuarcy: 0.86\n",
      "Epoch 2 step 613: training loss: 755.3603846660455\n",
      "Epoch 2 step 614: training accuarcy: 0.8635\n",
      "Epoch 2 step 614: training loss: 795.5359011898322\n",
      "Epoch 2 step 615: training accuarcy: 0.852\n",
      "Epoch 2 step 615: training loss: 781.5057915257029\n",
      "Epoch 2 step 616: training accuarcy: 0.866\n",
      "Epoch 2 step 616: training loss: 783.5926357519685\n",
      "Epoch 2 step 617: training accuarcy: 0.8675\n",
      "Epoch 2 step 617: training loss: 771.6103818659169\n",
      "Epoch 2 step 618: training accuarcy: 0.855\n",
      "Epoch 2 step 618: training loss: 764.3597318141699\n",
      "Epoch 2 step 619: training accuarcy: 0.8705\n",
      "Epoch 2 step 619: training loss: 778.5589412821676\n",
      "Epoch 2 step 620: training accuarcy: 0.8575\n",
      "Epoch 2 step 620: training loss: 758.8439028806009\n",
      "Epoch 2 step 621: training accuarcy: 0.863\n",
      "Epoch 2 step 621: training loss: 755.492862024968\n",
      "Epoch 2 step 622: training accuarcy: 0.865\n",
      "Epoch 2 step 622: training loss: 746.6835701827841\n",
      "Epoch 2 step 623: training accuarcy: 0.875\n",
      "Epoch 2 step 623: training loss: 794.2036246348376\n",
      "Epoch 2 step 624: training accuarcy: 0.8585\n",
      "Epoch 2 step 624: training loss: 751.8369447650128\n",
      "Epoch 2 step 625: training accuarcy: 0.876\n",
      "Epoch 2 step 625: training loss: 773.1359044188596\n",
      "Epoch 2 step 626: training accuarcy: 0.8675\n",
      "Epoch 2 step 626: training loss: 770.9356757136302\n",
      "Epoch 2 step 627: training accuarcy: 0.8605\n",
      "Epoch 2 step 627: training loss: 797.0423691712517\n",
      "Epoch 2 step 628: training accuarcy: 0.851\n",
      "Epoch 2 step 628: training loss: 759.188959847282\n",
      "Epoch 2 step 629: training accuarcy: 0.8665\n",
      "Epoch 2 step 629: training loss: 803.9353325240152\n",
      "Epoch 2 step 630: training accuarcy: 0.8565\n",
      "Epoch 2 step 630: training loss: 769.9089769443699\n",
      "Epoch 2 step 631: training accuarcy: 0.868\n",
      "Epoch 2 step 631: training loss: 801.5539888749474\n",
      "Epoch 2 step 632: training accuarcy: 0.8555\n",
      "Epoch 2 step 632: training loss: 790.4764760454532\n",
      "Epoch 2 step 633: training accuarcy: 0.866\n",
      "Epoch 2 step 633: training loss: 734.629191248868\n",
      "Epoch 2 step 634: training accuarcy: 0.8645\n",
      "Epoch 2 step 634: training loss: 760.7483707639424\n",
      "Epoch 2 step 635: training accuarcy: 0.8615\n",
      "Epoch 2 step 635: training loss: 766.5924200901759\n",
      "Epoch 2 step 636: training accuarcy: 0.866\n",
      "Epoch 2 step 636: training loss: 742.7655627523128\n",
      "Epoch 2 step 637: training accuarcy: 0.881\n",
      "Epoch 2 step 637: training loss: 772.3412447661842\n",
      "Epoch 2 step 638: training accuarcy: 0.866\n",
      "Epoch 2 step 638: training loss: 771.4329179305682\n",
      "Epoch 2 step 639: training accuarcy: 0.8575\n",
      "Epoch 2 step 639: training loss: 768.1610233956442\n",
      "Epoch 2 step 640: training accuarcy: 0.868\n",
      "Epoch 2 step 640: training loss: 794.9156853969729\n",
      "Epoch 2 step 641: training accuarcy: 0.857\n",
      "Epoch 2 step 641: training loss: 746.4713486465931\n",
      "Epoch 2 step 642: training accuarcy: 0.8765000000000001\n",
      "Epoch 2 step 642: training loss: 757.998933485912\n",
      "Epoch 2 step 643: training accuarcy: 0.8675\n",
      "Epoch 2 step 643: training loss: 743.5905105082403\n",
      "Epoch 2 step 644: training accuarcy: 0.87\n",
      "Epoch 2 step 644: training loss: 757.7284866981623\n",
      "Epoch 2 step 645: training accuarcy: 0.8655\n",
      "Epoch 2 step 645: training loss: 779.9724014489086\n",
      "Epoch 2 step 646: training accuarcy: 0.86\n",
      "Epoch 2 step 646: training loss: 776.8779357277865\n",
      "Epoch 2 step 647: training accuarcy: 0.864\n",
      "Epoch 2 step 647: training loss: 749.4943650783055\n",
      "Epoch 2 step 648: training accuarcy: 0.8625\n",
      "Epoch 2 step 648: training loss: 747.5811625922381\n",
      "Epoch 2 step 649: training accuarcy: 0.8685\n",
      "Epoch 2 step 649: training loss: 777.1311545169701\n",
      "Epoch 2 step 650: training accuarcy: 0.873\n",
      "Epoch 2 step 650: training loss: 764.3084277744877\n",
      "Epoch 2 step 651: training accuarcy: 0.8585\n",
      "Epoch 2 step 651: training loss: 807.8274918512816\n",
      "Epoch 2 step 652: training accuarcy: 0.858\n",
      "Epoch 2 step 652: training loss: 760.4673382831162\n",
      "Epoch 2 step 653: training accuarcy: 0.868\n",
      "Epoch 2 step 653: training loss: 743.3051109760329\n",
      "Epoch 2 step 654: training accuarcy: 0.8715\n",
      "Epoch 2 step 654: training loss: 808.5781542639224\n",
      "Epoch 2 step 655: training accuarcy: 0.8605\n",
      "Epoch 2 step 655: training loss: 749.2846614054707\n",
      "Epoch 2 step 656: training accuarcy: 0.8745\n",
      "Epoch 2 step 656: training loss: 770.2687239063614\n",
      "Epoch 2 step 657: training accuarcy: 0.8625\n",
      "Epoch 2 step 657: training loss: 786.4342675768613\n",
      "Epoch 2 step 658: training accuarcy: 0.8635\n",
      "Epoch 2 step 658: training loss: 732.7461740566491\n",
      "Epoch 2 step 659: training accuarcy: 0.8735\n",
      "Epoch 2 step 659: training loss: 755.9535449101395\n",
      "Epoch 2 step 660: training accuarcy: 0.8635\n",
      "Epoch 2 step 660: training loss: 749.3150562954756\n",
      "Epoch 2 step 661: training accuarcy: 0.8705\n",
      "Epoch 2 step 661: training loss: 774.4228251669805\n",
      "Epoch 2 step 662: training accuarcy: 0.8625\n",
      "Epoch 2 step 662: training loss: 743.8505228972047\n",
      "Epoch 2 step 663: training accuarcy: 0.8685\n",
      "Epoch 2 step 663: training loss: 767.7393171559639\n",
      "Epoch 2 step 664: training accuarcy: 0.8705\n",
      "Epoch 2 step 664: training loss: 772.871213115979\n",
      "Epoch 2 step 665: training accuarcy: 0.8675\n",
      "Epoch 2 step 665: training loss: 776.0390125732688\n",
      "Epoch 2 step 666: training accuarcy: 0.8625\n",
      "Epoch 2 step 666: training loss: 800.3850478900208\n",
      "Epoch 2 step 667: training accuarcy: 0.855\n",
      "Epoch 2 step 667: training loss: 727.9154377074941\n",
      "Epoch 2 step 668: training accuarcy: 0.884\n",
      "Epoch 2 step 668: training loss: 747.5735346538185\n",
      "Epoch 2 step 669: training accuarcy: 0.872\n",
      "Epoch 2 step 669: training loss: 833.8065820878412\n",
      "Epoch 2 step 670: training accuarcy: 0.847\n",
      "Epoch 2 step 670: training loss: 756.3762884958783\n",
      "Epoch 2 step 671: training accuarcy: 0.868\n",
      "Epoch 2 step 671: training loss: 767.553042137472\n",
      "Epoch 2 step 672: training accuarcy: 0.8705\n",
      "Epoch 2 step 672: training loss: 781.765627820253\n",
      "Epoch 2 step 673: training accuarcy: 0.8575\n",
      "Epoch 2 step 673: training loss: 737.8772458740955\n",
      "Epoch 2 step 674: training accuarcy: 0.879\n",
      "Epoch 2 step 674: training loss: 735.1452541427204\n",
      "Epoch 2 step 675: training accuarcy: 0.8705\n",
      "Epoch 2 step 675: training loss: 794.0588814671054\n",
      "Epoch 2 step 676: training accuarcy: 0.8615\n",
      "Epoch 2 step 676: training loss: 715.5060746434942\n",
      "Epoch 2 step 677: training accuarcy: 0.889\n",
      "Epoch 2 step 677: training loss: 733.1781038222167\n",
      "Epoch 2 step 678: training accuarcy: 0.8835000000000001\n",
      "Epoch 2 step 678: training loss: 786.1660505403835\n",
      "Epoch 2 step 679: training accuarcy: 0.8595\n",
      "Epoch 2 step 679: training loss: 766.6906020829949\n",
      "Epoch 2 step 680: training accuarcy: 0.869\n",
      "Epoch 2 step 680: training loss: 763.9478865275756\n",
      "Epoch 2 step 681: training accuarcy: 0.8645\n",
      "Epoch 2 step 681: training loss: 795.6688641860314\n",
      "Epoch 2 step 682: training accuarcy: 0.8675\n",
      "Epoch 2 step 682: training loss: 798.9925315393737\n",
      "Epoch 2 step 683: training accuarcy: 0.861\n",
      "Epoch 2 step 683: training loss: 768.5696226436424\n",
      "Epoch 2 step 684: training accuarcy: 0.8675\n",
      "Epoch 2 step 684: training loss: 817.1882085736643\n",
      "Epoch 2 step 685: training accuarcy: 0.855\n",
      "Epoch 2 step 685: training loss: 774.6097166861164\n",
      "Epoch 2 step 686: training accuarcy: 0.8655\n",
      "Epoch 2 step 686: training loss: 777.2753661601467\n",
      "Epoch 2 step 687: training accuarcy: 0.867\n",
      "Epoch 2 step 687: training loss: 795.8741910166685\n",
      "Epoch 2 step 688: training accuarcy: 0.8545\n",
      "Epoch 2 step 688: training loss: 738.7389079336\n",
      "Epoch 2 step 689: training accuarcy: 0.8715\n",
      "Epoch 2 step 689: training loss: 726.8811245208369\n",
      "Epoch 2 step 690: training accuarcy: 0.884\n",
      "Epoch 2 step 690: training loss: 775.740330535937\n",
      "Epoch 2 step 691: training accuarcy: 0.8695\n",
      "Epoch 2 step 691: training loss: 786.42063789502\n",
      "Epoch 2 step 692: training accuarcy: 0.857\n",
      "Epoch 2 step 692: training loss: 752.0576119177828\n",
      "Epoch 2 step 693: training accuarcy: 0.8765000000000001\n",
      "Epoch 2 step 693: training loss: 748.9686725894917\n",
      "Epoch 2 step 694: training accuarcy: 0.8675\n",
      "Epoch 2 step 694: training loss: 776.6103915785852\n",
      "Epoch 2 step 695: training accuarcy: 0.856\n",
      "Epoch 2 step 695: training loss: 782.3866417471922\n",
      "Epoch 2 step 696: training accuarcy: 0.8655\n",
      "Epoch 2 step 696: training loss: 752.0630505678209\n",
      "Epoch 2 step 697: training accuarcy: 0.8695\n",
      "Epoch 2 step 697: training loss: 709.7140098303046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 698: training accuarcy: 0.884\n",
      "Epoch 2 step 698: training loss: 780.3399071855326\n",
      "Epoch 2 step 699: training accuarcy: 0.874\n",
      "Epoch 2 step 699: training loss: 801.1737383269701\n",
      "Epoch 2 step 700: training accuarcy: 0.864\n",
      "Epoch 2 step 700: training loss: 757.1388994367992\n",
      "Epoch 2 step 701: training accuarcy: 0.863\n",
      "Epoch 2 step 701: training loss: 805.8627778904613\n",
      "Epoch 2 step 702: training accuarcy: 0.858\n",
      "Epoch 2 step 702: training loss: 784.5953445456287\n",
      "Epoch 2 step 703: training accuarcy: 0.8635\n",
      "Epoch 2 step 703: training loss: 753.6137714208736\n",
      "Epoch 2 step 704: training accuarcy: 0.866\n",
      "Epoch 2 step 704: training loss: 786.7245460122199\n",
      "Epoch 2 step 705: training accuarcy: 0.866\n",
      "Epoch 2 step 705: training loss: 774.0033544108808\n",
      "Epoch 2 step 706: training accuarcy: 0.858\n",
      "Epoch 2 step 706: training loss: 746.4023599632321\n",
      "Epoch 2 step 707: training accuarcy: 0.871\n",
      "Epoch 2 step 707: training loss: 748.69936662401\n",
      "Epoch 2 step 708: training accuarcy: 0.8655\n",
      "Epoch 2 step 708: training loss: 785.8265460336751\n",
      "Epoch 2 step 709: training accuarcy: 0.8695\n",
      "Epoch 2 step 709: training loss: 762.4334868031684\n",
      "Epoch 2 step 710: training accuarcy: 0.8635\n",
      "Epoch 2 step 710: training loss: 756.2467953053183\n",
      "Epoch 2 step 711: training accuarcy: 0.8665\n",
      "Epoch 2 step 711: training loss: 712.5151363943644\n",
      "Epoch 2 step 712: training accuarcy: 0.883\n",
      "Epoch 2 step 712: training loss: 740.6077035501925\n",
      "Epoch 2 step 713: training accuarcy: 0.8725\n",
      "Epoch 2 step 713: training loss: 719.8037671037799\n",
      "Epoch 2 step 714: training accuarcy: 0.876\n",
      "Epoch 2 step 714: training loss: 702.9931820607202\n",
      "Epoch 2 step 715: training accuarcy: 0.883\n",
      "Epoch 2 step 715: training loss: 777.9504099562818\n",
      "Epoch 2 step 716: training accuarcy: 0.8695\n",
      "Epoch 2 step 716: training loss: 750.1242631251063\n",
      "Epoch 2 step 717: training accuarcy: 0.8685\n",
      "Epoch 2 step 717: training loss: 758.2427587513591\n",
      "Epoch 2 step 718: training accuarcy: 0.8725\n",
      "Epoch 2 step 718: training loss: 769.5616734776852\n",
      "Epoch 2 step 719: training accuarcy: 0.87\n",
      "Epoch 2 step 719: training loss: 772.0719685633162\n",
      "Epoch 2 step 720: training accuarcy: 0.8535\n",
      "Epoch 2 step 720: training loss: 766.2109275308186\n",
      "Epoch 2 step 721: training accuarcy: 0.864\n",
      "Epoch 2 step 721: training loss: 751.9150729601204\n",
      "Epoch 2 step 722: training accuarcy: 0.8655\n",
      "Epoch 2 step 722: training loss: 772.4080749414662\n",
      "Epoch 2 step 723: training accuarcy: 0.863\n",
      "Epoch 2 step 723: training loss: 722.1766655167476\n",
      "Epoch 2 step 724: training accuarcy: 0.8785000000000001\n",
      "Epoch 2 step 724: training loss: 752.388108204867\n",
      "Epoch 2 step 725: training accuarcy: 0.868\n",
      "Epoch 2 step 725: training loss: 782.1281339442708\n",
      "Epoch 2 step 726: training accuarcy: 0.856\n",
      "Epoch 2 step 726: training loss: 766.8370338124976\n",
      "Epoch 2 step 727: training accuarcy: 0.8645\n",
      "Epoch 2 step 727: training loss: 789.1804962029731\n",
      "Epoch 2 step 728: training accuarcy: 0.8525\n",
      "Epoch 2 step 728: training loss: 779.3675841936271\n",
      "Epoch 2 step 729: training accuarcy: 0.865\n",
      "Epoch 2 step 729: training loss: 763.5539729687503\n",
      "Epoch 2 step 730: training accuarcy: 0.8645\n",
      "Epoch 2 step 730: training loss: 745.2944610868337\n",
      "Epoch 2 step 731: training accuarcy: 0.868\n",
      "Epoch 2 step 731: training loss: 769.9830086642626\n",
      "Epoch 2 step 732: training accuarcy: 0.8765000000000001\n",
      "Epoch 2 step 732: training loss: 762.4245614946412\n",
      "Epoch 2 step 733: training accuarcy: 0.868\n",
      "Epoch 2 step 733: training loss: 794.051927876064\n",
      "Epoch 2 step 734: training accuarcy: 0.857\n",
      "Epoch 2 step 734: training loss: 760.4416215860607\n",
      "Epoch 2 step 735: training accuarcy: 0.8655\n",
      "Epoch 2 step 735: training loss: 760.9151398901773\n",
      "Epoch 2 step 736: training accuarcy: 0.87\n",
      "Epoch 2 step 736: training loss: 779.8968851406864\n",
      "Epoch 2 step 737: training accuarcy: 0.857\n",
      "Epoch 2 step 737: training loss: 764.9094751779589\n",
      "Epoch 2 step 738: training accuarcy: 0.868\n",
      "Epoch 2 step 738: training loss: 785.1135256818077\n",
      "Epoch 2 step 739: training accuarcy: 0.861\n",
      "Epoch 2 step 739: training loss: 809.1898345087345\n",
      "Epoch 2 step 740: training accuarcy: 0.852\n",
      "Epoch 2 step 740: training loss: 770.9338235784898\n",
      "Epoch 2 step 741: training accuarcy: 0.8735\n",
      "Epoch 2 step 741: training loss: 775.4973845351391\n",
      "Epoch 2 step 742: training accuarcy: 0.8645\n",
      "Epoch 2 step 742: training loss: 753.0053165753436\n",
      "Epoch 2 step 743: training accuarcy: 0.867\n",
      "Epoch 2 step 743: training loss: 813.6416266469439\n",
      "Epoch 2 step 744: training accuarcy: 0.8565\n",
      "Epoch 2 step 744: training loss: 769.3063939733916\n",
      "Epoch 2 step 745: training accuarcy: 0.865\n",
      "Epoch 2 step 745: training loss: 752.9423465090895\n",
      "Epoch 2 step 746: training accuarcy: 0.862\n",
      "Epoch 2 step 746: training loss: 780.7676255668389\n",
      "Epoch 2 step 747: training accuarcy: 0.8645\n",
      "Epoch 2 step 747: training loss: 767.8194534306485\n",
      "Epoch 2 step 748: training accuarcy: 0.873\n",
      "Epoch 2 step 748: training loss: 753.3135530591569\n",
      "Epoch 2 step 749: training accuarcy: 0.865\n",
      "Epoch 2 step 749: training loss: 756.52533736417\n",
      "Epoch 2 step 750: training accuarcy: 0.873\n",
      "Epoch 2 step 750: training loss: 762.0281387634735\n",
      "Epoch 2 step 751: training accuarcy: 0.868\n",
      "Epoch 2 step 751: training loss: 758.4102148839822\n",
      "Epoch 2 step 752: training accuarcy: 0.867\n",
      "Epoch 2 step 752: training loss: 759.3986877213499\n",
      "Epoch 2 step 753: training accuarcy: 0.871\n",
      "Epoch 2 step 753: training loss: 738.3266028435868\n",
      "Epoch 2 step 754: training accuarcy: 0.8735\n",
      "Epoch 2 step 754: training loss: 771.1698354877126\n",
      "Epoch 2 step 755: training accuarcy: 0.866\n",
      "Epoch 2 step 755: training loss: 812.9155916896706\n",
      "Epoch 2 step 756: training accuarcy: 0.848\n",
      "Epoch 2 step 756: training loss: 746.649605309454\n",
      "Epoch 2 step 757: training accuarcy: 0.8715\n",
      "Epoch 2 step 757: training loss: 734.2915772952152\n",
      "Epoch 2 step 758: training accuarcy: 0.882\n",
      "Epoch 2 step 758: training loss: 762.2335435275606\n",
      "Epoch 2 step 759: training accuarcy: 0.8745\n",
      "Epoch 2 step 759: training loss: 738.5736882867204\n",
      "Epoch 2 step 760: training accuarcy: 0.874\n",
      "Epoch 2 step 760: training loss: 726.8293232883948\n",
      "Epoch 2 step 761: training accuarcy: 0.8775000000000001\n",
      "Epoch 2 step 761: training loss: 779.2104545823959\n",
      "Epoch 2 step 762: training accuarcy: 0.8615\n",
      "Epoch 2 step 762: training loss: 762.108730230338\n",
      "Epoch 2 step 763: training accuarcy: 0.8605\n",
      "Epoch 2 step 763: training loss: 732.0038200325066\n",
      "Epoch 2 step 764: training accuarcy: 0.876\n",
      "Epoch 2 step 764: training loss: 741.0586403872065\n",
      "Epoch 2 step 765: training accuarcy: 0.877\n",
      "Epoch 2 step 765: training loss: 770.1105210093186\n",
      "Epoch 2 step 766: training accuarcy: 0.8635\n",
      "Epoch 2 step 766: training loss: 754.579597126153\n",
      "Epoch 2 step 767: training accuarcy: 0.86\n",
      "Epoch 2 step 767: training loss: 757.0251361236501\n",
      "Epoch 2 step 768: training accuarcy: 0.87\n",
      "Epoch 2 step 768: training loss: 745.1685093455624\n",
      "Epoch 2 step 769: training accuarcy: 0.87\n",
      "Epoch 2 step 769: training loss: 717.7134111891047\n",
      "Epoch 2 step 770: training accuarcy: 0.8775000000000001\n",
      "Epoch 2 step 770: training loss: 802.1500451793042\n",
      "Epoch 2 step 771: training accuarcy: 0.8485\n",
      "Epoch 2 step 771: training loss: 732.3244347540406\n",
      "Epoch 2 step 772: training accuarcy: 0.8745\n",
      "Epoch 2 step 772: training loss: 776.6888316105033\n",
      "Epoch 2 step 773: training accuarcy: 0.863\n",
      "Epoch 2 step 773: training loss: 792.8120477132304\n",
      "Epoch 2 step 774: training accuarcy: 0.8575\n",
      "Epoch 2 step 774: training loss: 770.1750286786727\n",
      "Epoch 2 step 775: training accuarcy: 0.8665\n",
      "Epoch 2 step 775: training loss: 760.088307017754\n",
      "Epoch 2 step 776: training accuarcy: 0.867\n",
      "Epoch 2 step 776: training loss: 756.2527955963446\n",
      "Epoch 2 step 777: training accuarcy: 0.865\n",
      "Epoch 2 step 777: training loss: 745.9275271944463\n",
      "Epoch 2 step 778: training accuarcy: 0.88\n",
      "Epoch 2 step 778: training loss: 795.2003843393807\n",
      "Epoch 2 step 779: training accuarcy: 0.8565\n",
      "Epoch 2 step 779: training loss: 698.7829953926043\n",
      "Epoch 2 step 780: training accuarcy: 0.886\n",
      "Epoch 2 step 780: training loss: 727.0447223555718\n",
      "Epoch 2 step 781: training accuarcy: 0.879\n",
      "Epoch 2 step 781: training loss: 755.6921742122148\n",
      "Epoch 2 step 782: training accuarcy: 0.8755000000000001\n",
      "Epoch 2 step 782: training loss: 774.5151063963419\n",
      "Epoch 2 step 783: training accuarcy: 0.8545\n",
      "Epoch 2 step 783: training loss: 746.6499265297987\n",
      "Epoch 2 step 784: training accuarcy: 0.8685\n",
      "Epoch 2 step 784: training loss: 764.6400410911943\n",
      "Epoch 2 step 785: training accuarcy: 0.8665\n",
      "Epoch 2 step 785: training loss: 728.041211108568\n",
      "Epoch 2 step 786: training accuarcy: 0.865\n",
      "Epoch 2 step 786: training loss: 789.3986416246106\n",
      "Epoch 2 step 787: training accuarcy: 0.86\n",
      "Epoch 2 step 787: training loss: 779.7114488490893\n",
      "Epoch 2 step 788: training accuarcy: 0.866\n",
      "Epoch 2 step 788: training loss: 375.54944299066193\n",
      "Epoch 2 step 789: training accuarcy: 0.8987179487179487\n",
      "Epoch 2: train loss 766.1139369211614, train accuarcy 0.8627145886421204\n",
      "Epoch 2: valid loss 3722.8134852730363, valid accuarcy 0.837295651435852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 3/5 [16:47<11:06, 333.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 747.6857266983241\n",
      "Epoch 3 step 790: training accuarcy: 0.8745\n",
      "Epoch 3 step 790: training loss: 732.0710058126322\n",
      "Epoch 3 step 791: training accuarcy: 0.877\n",
      "Epoch 3 step 791: training loss: 741.2782437332831\n",
      "Epoch 3 step 792: training accuarcy: 0.872\n",
      "Epoch 3 step 792: training loss: 758.2552646081805\n",
      "Epoch 3 step 793: training accuarcy: 0.879\n",
      "Epoch 3 step 793: training loss: 744.1942600839736\n",
      "Epoch 3 step 794: training accuarcy: 0.878\n",
      "Epoch 3 step 794: training loss: 766.8813995961718\n",
      "Epoch 3 step 795: training accuarcy: 0.8695\n",
      "Epoch 3 step 795: training loss: 739.0812719642712\n",
      "Epoch 3 step 796: training accuarcy: 0.8745\n",
      "Epoch 3 step 796: training loss: 748.4699444927478\n",
      "Epoch 3 step 797: training accuarcy: 0.869\n",
      "Epoch 3 step 797: training loss: 705.6569705147551\n",
      "Epoch 3 step 798: training accuarcy: 0.8895000000000001\n",
      "Epoch 3 step 798: training loss: 773.4517409934587\n",
      "Epoch 3 step 799: training accuarcy: 0.866\n",
      "Epoch 3 step 799: training loss: 709.5831664203417\n",
      "Epoch 3 step 800: training accuarcy: 0.887\n",
      "Epoch 3 step 800: training loss: 702.8505391778518\n",
      "Epoch 3 step 801: training accuarcy: 0.8815000000000001\n",
      "Epoch 3 step 801: training loss: 739.3114552162391\n",
      "Epoch 3 step 802: training accuarcy: 0.8705\n",
      "Epoch 3 step 802: training loss: 743.5211131742017\n",
      "Epoch 3 step 803: training accuarcy: 0.8685\n",
      "Epoch 3 step 803: training loss: 690.5340133624486\n",
      "Epoch 3 step 804: training accuarcy: 0.8875000000000001\n",
      "Epoch 3 step 804: training loss: 734.8027465974974\n",
      "Epoch 3 step 805: training accuarcy: 0.868\n",
      "Epoch 3 step 805: training loss: 729.2342105048317\n",
      "Epoch 3 step 806: training accuarcy: 0.879\n",
      "Epoch 3 step 806: training loss: 704.1654203136751\n",
      "Epoch 3 step 807: training accuarcy: 0.8825000000000001\n",
      "Epoch 3 step 807: training loss: 707.5169279337945\n",
      "Epoch 3 step 808: training accuarcy: 0.879\n",
      "Epoch 3 step 808: training loss: 707.1947100849508\n",
      "Epoch 3 step 809: training accuarcy: 0.8835000000000001\n",
      "Epoch 3 step 809: training loss: 735.6471745046756\n",
      "Epoch 3 step 810: training accuarcy: 0.8815000000000001\n",
      "Epoch 3 step 810: training loss: 732.9257050519861\n",
      "Epoch 3 step 811: training accuarcy: 0.876\n",
      "Epoch 3 step 811: training loss: 727.3374621482469\n",
      "Epoch 3 step 812: training accuarcy: 0.887\n",
      "Epoch 3 step 812: training loss: 744.174408355074\n",
      "Epoch 3 step 813: training accuarcy: 0.868\n",
      "Epoch 3 step 813: training loss: 736.7055000122957\n",
      "Epoch 3 step 814: training accuarcy: 0.867\n",
      "Epoch 3 step 814: training loss: 753.6807905193225\n",
      "Epoch 3 step 815: training accuarcy: 0.872\n",
      "Epoch 3 step 815: training loss: 733.4516181746808\n",
      "Epoch 3 step 816: training accuarcy: 0.873\n",
      "Epoch 3 step 816: training loss: 758.4732162610942\n",
      "Epoch 3 step 817: training accuarcy: 0.8675\n",
      "Epoch 3 step 817: training loss: 766.3455991448208\n",
      "Epoch 3 step 818: training accuarcy: 0.8685\n",
      "Epoch 3 step 818: training loss: 717.1328000772387\n",
      "Epoch 3 step 819: training accuarcy: 0.886\n",
      "Epoch 3 step 819: training loss: 692.2736178747427\n",
      "Epoch 3 step 820: training accuarcy: 0.8805000000000001\n",
      "Epoch 3 step 820: training loss: 782.8833479893722\n",
      "Epoch 3 step 821: training accuarcy: 0.863\n",
      "Epoch 3 step 821: training loss: 747.179118401609\n",
      "Epoch 3 step 822: training accuarcy: 0.876\n",
      "Epoch 3 step 822: training loss: 717.3121169996987\n",
      "Epoch 3 step 823: training accuarcy: 0.878\n",
      "Epoch 3 step 823: training loss: 766.5943759206431\n",
      "Epoch 3 step 824: training accuarcy: 0.8565\n",
      "Epoch 3 step 824: training loss: 726.6742404814813\n",
      "Epoch 3 step 825: training accuarcy: 0.873\n",
      "Epoch 3 step 825: training loss: 716.2771472683011\n",
      "Epoch 3 step 826: training accuarcy: 0.8855000000000001\n",
      "Epoch 3 step 826: training loss: 717.9571369034797\n",
      "Epoch 3 step 827: training accuarcy: 0.877\n",
      "Epoch 3 step 827: training loss: 712.3384052308667\n",
      "Epoch 3 step 828: training accuarcy: 0.8885000000000001\n",
      "Epoch 3 step 828: training loss: 730.7561382395072\n",
      "Epoch 3 step 829: training accuarcy: 0.876\n",
      "Epoch 3 step 829: training loss: 743.4506748345839\n",
      "Epoch 3 step 830: training accuarcy: 0.8725\n",
      "Epoch 3 step 830: training loss: 769.212474842794\n",
      "Epoch 3 step 831: training accuarcy: 0.859\n",
      "Epoch 3 step 831: training loss: 721.0614740913586\n",
      "Epoch 3 step 832: training accuarcy: 0.89\n",
      "Epoch 3 step 832: training loss: 730.913439698019\n",
      "Epoch 3 step 833: training accuarcy: 0.875\n",
      "Epoch 3 step 833: training loss: 709.5882611962772\n",
      "Epoch 3 step 834: training accuarcy: 0.8875000000000001\n",
      "Epoch 3 step 834: training loss: 771.2675561079827\n",
      "Epoch 3 step 835: training accuarcy: 0.86\n",
      "Epoch 3 step 835: training loss: 743.2007706082039\n",
      "Epoch 3 step 836: training accuarcy: 0.8815000000000001\n",
      "Epoch 3 step 836: training loss: 740.1190615417596\n",
      "Epoch 3 step 837: training accuarcy: 0.872\n",
      "Epoch 3 step 837: training loss: 750.4478606094842\n",
      "Epoch 3 step 838: training accuarcy: 0.8615\n",
      "Epoch 3 step 838: training loss: 764.6260425001534\n",
      "Epoch 3 step 839: training accuarcy: 0.8635\n",
      "Epoch 3 step 839: training loss: 786.3757264826875\n",
      "Epoch 3 step 840: training accuarcy: 0.851\n",
      "Epoch 3 step 840: training loss: 723.9383227172267\n",
      "Epoch 3 step 841: training accuarcy: 0.886\n",
      "Epoch 3 step 841: training loss: 741.6805743252776\n",
      "Epoch 3 step 842: training accuarcy: 0.8755000000000001\n",
      "Epoch 3 step 842: training loss: 775.4499995942115\n",
      "Epoch 3 step 843: training accuarcy: 0.868\n",
      "Epoch 3 step 843: training loss: 716.9807680743588\n",
      "Epoch 3 step 844: training accuarcy: 0.871\n",
      "Epoch 3 step 844: training loss: 749.5084077290905\n",
      "Epoch 3 step 845: training accuarcy: 0.8705\n",
      "Epoch 3 step 845: training loss: 764.5690736757449\n",
      "Epoch 3 step 846: training accuarcy: 0.869\n",
      "Epoch 3 step 846: training loss: 744.2334189714055\n",
      "Epoch 3 step 847: training accuarcy: 0.875\n",
      "Epoch 3 step 847: training loss: 756.8696331995676\n",
      "Epoch 3 step 848: training accuarcy: 0.8725\n",
      "Epoch 3 step 848: training loss: 736.6863255966256\n",
      "Epoch 3 step 849: training accuarcy: 0.874\n",
      "Epoch 3 step 849: training loss: 752.2508728276762\n",
      "Epoch 3 step 850: training accuarcy: 0.875\n",
      "Epoch 3 step 850: training loss: 749.2432549837268\n",
      "Epoch 3 step 851: training accuarcy: 0.8675\n",
      "Epoch 3 step 851: training loss: 728.0178651062276\n",
      "Epoch 3 step 852: training accuarcy: 0.872\n",
      "Epoch 3 step 852: training loss: 747.2653213842167\n",
      "Epoch 3 step 853: training accuarcy: 0.8755000000000001\n",
      "Epoch 3 step 853: training loss: 745.3703699244268\n",
      "Epoch 3 step 854: training accuarcy: 0.8665\n",
      "Epoch 3 step 854: training loss: 729.5662438889779\n",
      "Epoch 3 step 855: training accuarcy: 0.8765000000000001\n",
      "Epoch 3 step 855: training loss: 788.4413600819338\n",
      "Epoch 3 step 856: training accuarcy: 0.8705\n",
      "Epoch 3 step 856: training loss: 769.1713381052348\n",
      "Epoch 3 step 857: training accuarcy: 0.862\n",
      "Epoch 3 step 857: training loss: 730.8815511016501\n",
      "Epoch 3 step 858: training accuarcy: 0.882\n",
      "Epoch 3 step 858: training loss: 749.2090965286932\n",
      "Epoch 3 step 859: training accuarcy: 0.8705\n",
      "Epoch 3 step 859: training loss: 727.7634778347983\n",
      "Epoch 3 step 860: training accuarcy: 0.8745\n",
      "Epoch 3 step 860: training loss: 749.0308225251225\n",
      "Epoch 3 step 861: training accuarcy: 0.869\n",
      "Epoch 3 step 861: training loss: 736.328513780237\n",
      "Epoch 3 step 862: training accuarcy: 0.8715\n",
      "Epoch 3 step 862: training loss: 762.3051425330312\n",
      "Epoch 3 step 863: training accuarcy: 0.8775000000000001\n",
      "Epoch 3 step 863: training loss: 746.1178191969159\n",
      "Epoch 3 step 864: training accuarcy: 0.8665\n",
      "Epoch 3 step 864: training loss: 724.390855931151\n",
      "Epoch 3 step 865: training accuarcy: 0.871\n",
      "Epoch 3 step 865: training loss: 716.3357684272099\n",
      "Epoch 3 step 866: training accuarcy: 0.884\n",
      "Epoch 3 step 866: training loss: 752.5756669023681\n",
      "Epoch 3 step 867: training accuarcy: 0.8695\n",
      "Epoch 3 step 867: training loss: 720.0766275219229\n",
      "Epoch 3 step 868: training accuarcy: 0.8795000000000001\n",
      "Epoch 3 step 868: training loss: 735.0681698467761\n",
      "Epoch 3 step 869: training accuarcy: 0.873\n",
      "Epoch 3 step 869: training loss: 721.0338907365946\n",
      "Epoch 3 step 870: training accuarcy: 0.8775000000000001\n",
      "Epoch 3 step 870: training loss: 736.6407804803534\n",
      "Epoch 3 step 871: training accuarcy: 0.879\n",
      "Epoch 3 step 871: training loss: 714.6052304558825\n",
      "Epoch 3 step 872: training accuarcy: 0.8755000000000001\n",
      "Epoch 3 step 872: training loss: 728.0888310433866\n",
      "Epoch 3 step 873: training accuarcy: 0.8735\n",
      "Epoch 3 step 873: training loss: 734.2505571515524\n",
      "Epoch 3 step 874: training accuarcy: 0.882\n",
      "Epoch 3 step 874: training loss: 715.4836057144679\n",
      "Epoch 3 step 875: training accuarcy: 0.889\n",
      "Epoch 3 step 875: training loss: 772.7546545840381\n",
      "Epoch 3 step 876: training accuarcy: 0.8615\n",
      "Epoch 3 step 876: training loss: 744.0600884427466\n",
      "Epoch 3 step 877: training accuarcy: 0.8655\n",
      "Epoch 3 step 877: training loss: 736.5403654273099\n",
      "Epoch 3 step 878: training accuarcy: 0.869\n",
      "Epoch 3 step 878: training loss: 745.7717193644361\n",
      "Epoch 3 step 879: training accuarcy: 0.881\n",
      "Epoch 3 step 879: training loss: 770.9592277909888\n",
      "Epoch 3 step 880: training accuarcy: 0.8535\n",
      "Epoch 3 step 880: training loss: 744.7589368872118\n",
      "Epoch 3 step 881: training accuarcy: 0.863\n",
      "Epoch 3 step 881: training loss: 771.06719442124\n",
      "Epoch 3 step 882: training accuarcy: 0.8695\n",
      "Epoch 3 step 882: training loss: 720.6067292382713\n",
      "Epoch 3 step 883: training accuarcy: 0.882\n",
      "Epoch 3 step 883: training loss: 735.3484739123695\n",
      "Epoch 3 step 884: training accuarcy: 0.8775000000000001\n",
      "Epoch 3 step 884: training loss: 759.1180538712541\n",
      "Epoch 3 step 885: training accuarcy: 0.867\n",
      "Epoch 3 step 885: training loss: 738.3500892039722\n",
      "Epoch 3 step 886: training accuarcy: 0.8675\n",
      "Epoch 3 step 886: training loss: 754.3877995992059\n",
      "Epoch 3 step 887: training accuarcy: 0.8585\n",
      "Epoch 3 step 887: training loss: 719.3540350988815\n",
      "Epoch 3 step 888: training accuarcy: 0.882\n",
      "Epoch 3 step 888: training loss: 717.0509711859629\n",
      "Epoch 3 step 889: training accuarcy: 0.879\n",
      "Epoch 3 step 889: training loss: 757.8215308864377\n",
      "Epoch 3 step 890: training accuarcy: 0.875\n",
      "Epoch 3 step 890: training loss: 751.6554587271623\n",
      "Epoch 3 step 891: training accuarcy: 0.867\n",
      "Epoch 3 step 891: training loss: 751.7146717787359\n",
      "Epoch 3 step 892: training accuarcy: 0.8765000000000001\n",
      "Epoch 3 step 892: training loss: 747.241726129966\n",
      "Epoch 3 step 893: training accuarcy: 0.866\n",
      "Epoch 3 step 893: training loss: 725.1614339080802\n",
      "Epoch 3 step 894: training accuarcy: 0.8785000000000001\n",
      "Epoch 3 step 894: training loss: 767.4417240360565\n",
      "Epoch 3 step 895: training accuarcy: 0.8655\n",
      "Epoch 3 step 895: training loss: 757.9144925109417\n",
      "Epoch 3 step 896: training accuarcy: 0.865\n",
      "Epoch 3 step 896: training loss: 737.9580934398518\n",
      "Epoch 3 step 897: training accuarcy: 0.8735\n",
      "Epoch 3 step 897: training loss: 730.1738694214087\n",
      "Epoch 3 step 898: training accuarcy: 0.873\n",
      "Epoch 3 step 898: training loss: 733.8957707904727\n",
      "Epoch 3 step 899: training accuarcy: 0.87\n",
      "Epoch 3 step 899: training loss: 729.5628044403263\n",
      "Epoch 3 step 900: training accuarcy: 0.875\n",
      "Epoch 3 step 900: training loss: 734.867693460942\n",
      "Epoch 3 step 901: training accuarcy: 0.872\n",
      "Epoch 3 step 901: training loss: 724.8536507145969\n",
      "Epoch 3 step 902: training accuarcy: 0.8725\n",
      "Epoch 3 step 902: training loss: 774.4804873820271\n",
      "Epoch 3 step 903: training accuarcy: 0.8645\n",
      "Epoch 3 step 903: training loss: 768.0689223366826\n",
      "Epoch 3 step 904: training accuarcy: 0.862\n",
      "Epoch 3 step 904: training loss: 731.784296611536\n",
      "Epoch 3 step 905: training accuarcy: 0.875\n",
      "Epoch 3 step 905: training loss: 733.9358187679919\n",
      "Epoch 3 step 906: training accuarcy: 0.875\n",
      "Epoch 3 step 906: training loss: 763.394336217062\n",
      "Epoch 3 step 907: training accuarcy: 0.855\n",
      "Epoch 3 step 907: training loss: 741.2172375230471\n",
      "Epoch 3 step 908: training accuarcy: 0.879\n",
      "Epoch 3 step 908: training loss: 734.5434930948563\n",
      "Epoch 3 step 909: training accuarcy: 0.874\n",
      "Epoch 3 step 909: training loss: 773.195546548408\n",
      "Epoch 3 step 910: training accuarcy: 0.8555\n",
      "Epoch 3 step 910: training loss: 755.0934596327663\n",
      "Epoch 3 step 911: training accuarcy: 0.873\n",
      "Epoch 3 step 911: training loss: 715.8936382079548\n",
      "Epoch 3 step 912: training accuarcy: 0.8795000000000001\n",
      "Epoch 3 step 912: training loss: 780.6757374953113\n",
      "Epoch 3 step 913: training accuarcy: 0.8555\n",
      "Epoch 3 step 913: training loss: 724.5429579527805\n",
      "Epoch 3 step 914: training accuarcy: 0.873\n",
      "Epoch 3 step 914: training loss: 758.3371512299314\n",
      "Epoch 3 step 915: training accuarcy: 0.865\n",
      "Epoch 3 step 915: training loss: 702.4803399718069\n",
      "Epoch 3 step 916: training accuarcy: 0.8825000000000001\n",
      "Epoch 3 step 916: training loss: 803.3806347156795\n",
      "Epoch 3 step 917: training accuarcy: 0.8515\n",
      "Epoch 3 step 917: training loss: 744.2114064857111\n",
      "Epoch 3 step 918: training accuarcy: 0.8735\n",
      "Epoch 3 step 918: training loss: 714.5833882641305\n",
      "Epoch 3 step 919: training accuarcy: 0.8685\n",
      "Epoch 3 step 919: training loss: 766.5004399058521\n",
      "Epoch 3 step 920: training accuarcy: 0.866\n",
      "Epoch 3 step 920: training loss: 743.9652156448633\n",
      "Epoch 3 step 921: training accuarcy: 0.8795000000000001\n",
      "Epoch 3 step 921: training loss: 743.5284327504585\n",
      "Epoch 3 step 922: training accuarcy: 0.8745\n",
      "Epoch 3 step 922: training loss: 734.035428725205\n",
      "Epoch 3 step 923: training accuarcy: 0.8665\n",
      "Epoch 3 step 923: training loss: 780.3318778239609\n",
      "Epoch 3 step 924: training accuarcy: 0.8635\n",
      "Epoch 3 step 924: training loss: 794.4733444260798\n",
      "Epoch 3 step 925: training accuarcy: 0.8605\n",
      "Epoch 3 step 925: training loss: 749.5531939036439\n",
      "Epoch 3 step 926: training accuarcy: 0.8645\n",
      "Epoch 3 step 926: training loss: 765.8701898480549\n",
      "Epoch 3 step 927: training accuarcy: 0.865\n",
      "Epoch 3 step 927: training loss: 781.1984737320334\n",
      "Epoch 3 step 928: training accuarcy: 0.869\n",
      "Epoch 3 step 928: training loss: 769.2227884680983\n",
      "Epoch 3 step 929: training accuarcy: 0.8655\n",
      "Epoch 3 step 929: training loss: 743.2087051677106\n",
      "Epoch 3 step 930: training accuarcy: 0.8755000000000001\n",
      "Epoch 3 step 930: training loss: 774.3259452015582\n",
      "Epoch 3 step 931: training accuarcy: 0.867\n",
      "Epoch 3 step 931: training loss: 733.2163844214666\n",
      "Epoch 3 step 932: training accuarcy: 0.874\n",
      "Epoch 3 step 932: training loss: 785.6516846779082\n",
      "Epoch 3 step 933: training accuarcy: 0.8705\n",
      "Epoch 3 step 933: training loss: 764.1739742868828\n",
      "Epoch 3 step 934: training accuarcy: 0.869\n",
      "Epoch 3 step 934: training loss: 729.9456996191078\n",
      "Epoch 3 step 935: training accuarcy: 0.8865000000000001\n",
      "Epoch 3 step 935: training loss: 774.3068527637979\n",
      "Epoch 3 step 936: training accuarcy: 0.857\n",
      "Epoch 3 step 936: training loss: 777.0866811763169\n",
      "Epoch 3 step 937: training accuarcy: 0.8615\n",
      "Epoch 3 step 937: training loss: 724.1382363578929\n",
      "Epoch 3 step 938: training accuarcy: 0.877\n",
      "Epoch 3 step 938: training loss: 732.6270292760008\n",
      "Epoch 3 step 939: training accuarcy: 0.8715\n",
      "Epoch 3 step 939: training loss: 733.5884260872864\n",
      "Epoch 3 step 940: training accuarcy: 0.8795000000000001\n",
      "Epoch 3 step 940: training loss: 761.8793270224415\n",
      "Epoch 3 step 941: training accuarcy: 0.8675\n",
      "Epoch 3 step 941: training loss: 741.1733021027893\n",
      "Epoch 3 step 942: training accuarcy: 0.877\n",
      "Epoch 3 step 942: training loss: 770.7621651816193\n",
      "Epoch 3 step 943: training accuarcy: 0.8645\n",
      "Epoch 3 step 943: training loss: 785.606544066694\n",
      "Epoch 3 step 944: training accuarcy: 0.86\n",
      "Epoch 3 step 944: training loss: 715.6054350834055\n",
      "Epoch 3 step 945: training accuarcy: 0.8805000000000001\n",
      "Epoch 3 step 945: training loss: 751.9791836305236\n",
      "Epoch 3 step 946: training accuarcy: 0.8705\n",
      "Epoch 3 step 946: training loss: 746.8451392009656\n",
      "Epoch 3 step 947: training accuarcy: 0.864\n",
      "Epoch 3 step 947: training loss: 742.4735446136376\n",
      "Epoch 3 step 948: training accuarcy: 0.871\n",
      "Epoch 3 step 948: training loss: 803.5501462784719\n",
      "Epoch 3 step 949: training accuarcy: 0.863\n",
      "Epoch 3 step 949: training loss: 734.4170709207984\n",
      "Epoch 3 step 950: training accuarcy: 0.8705\n",
      "Epoch 3 step 950: training loss: 765.8394005931295\n",
      "Epoch 3 step 951: training accuarcy: 0.8755000000000001\n",
      "Epoch 3 step 951: training loss: 760.6735534397301\n",
      "Epoch 3 step 952: training accuarcy: 0.8735\n",
      "Epoch 3 step 952: training loss: 711.9444367200479\n",
      "Epoch 3 step 953: training accuarcy: 0.867\n",
      "Epoch 3 step 953: training loss: 733.948629701878\n",
      "Epoch 3 step 954: training accuarcy: 0.876\n",
      "Epoch 3 step 954: training loss: 770.0378608949072\n",
      "Epoch 3 step 955: training accuarcy: 0.8655\n",
      "Epoch 3 step 955: training loss: 758.781156258286\n",
      "Epoch 3 step 956: training accuarcy: 0.8655\n",
      "Epoch 3 step 956: training loss: 741.4122685530538\n",
      "Epoch 3 step 957: training accuarcy: 0.8755000000000001\n",
      "Epoch 3 step 957: training loss: 710.3298099197189\n",
      "Epoch 3 step 958: training accuarcy: 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 958: training loss: 777.6858480979205\n",
      "Epoch 3 step 959: training accuarcy: 0.8605\n",
      "Epoch 3 step 959: training loss: 743.5984169279818\n",
      "Epoch 3 step 960: training accuarcy: 0.8735\n",
      "Epoch 3 step 960: training loss: 752.2032128194076\n",
      "Epoch 3 step 961: training accuarcy: 0.874\n",
      "Epoch 3 step 961: training loss: 754.8350933604228\n",
      "Epoch 3 step 962: training accuarcy: 0.871\n",
      "Epoch 3 step 962: training loss: 742.3114829549371\n",
      "Epoch 3 step 963: training accuarcy: 0.873\n",
      "Epoch 3 step 963: training loss: 768.9480889066733\n",
      "Epoch 3 step 964: training accuarcy: 0.8575\n",
      "Epoch 3 step 964: training loss: 735.3288141176668\n",
      "Epoch 3 step 965: training accuarcy: 0.874\n",
      "Epoch 3 step 965: training loss: 730.739718401121\n",
      "Epoch 3 step 966: training accuarcy: 0.877\n",
      "Epoch 3 step 966: training loss: 764.4535670722067\n",
      "Epoch 3 step 967: training accuarcy: 0.8645\n",
      "Epoch 3 step 967: training loss: 760.6797826546256\n",
      "Epoch 3 step 968: training accuarcy: 0.8595\n",
      "Epoch 3 step 968: training loss: 726.0403933882269\n",
      "Epoch 3 step 969: training accuarcy: 0.8845000000000001\n",
      "Epoch 3 step 969: training loss: 726.5599949325791\n",
      "Epoch 3 step 970: training accuarcy: 0.8855000000000001\n",
      "Epoch 3 step 970: training loss: 701.0363177332063\n",
      "Epoch 3 step 971: training accuarcy: 0.887\n",
      "Epoch 3 step 971: training loss: 728.1535146082559\n",
      "Epoch 3 step 972: training accuarcy: 0.873\n",
      "Epoch 3 step 972: training loss: 741.524931447809\n",
      "Epoch 3 step 973: training accuarcy: 0.8695\n",
      "Epoch 3 step 973: training loss: 790.8839344636406\n",
      "Epoch 3 step 974: training accuarcy: 0.8605\n",
      "Epoch 3 step 974: training loss: 712.8365112347578\n",
      "Epoch 3 step 975: training accuarcy: 0.878\n",
      "Epoch 3 step 975: training loss: 725.5192249239657\n",
      "Epoch 3 step 976: training accuarcy: 0.8725\n",
      "Epoch 3 step 976: training loss: 770.1967920831847\n",
      "Epoch 3 step 977: training accuarcy: 0.8675\n",
      "Epoch 3 step 977: training loss: 788.9310769661075\n",
      "Epoch 3 step 978: training accuarcy: 0.8665\n",
      "Epoch 3 step 978: training loss: 751.8907241016478\n",
      "Epoch 3 step 979: training accuarcy: 0.877\n",
      "Epoch 3 step 979: training loss: 743.7114504754006\n",
      "Epoch 3 step 980: training accuarcy: 0.874\n",
      "Epoch 3 step 980: training loss: 733.3259087771643\n",
      "Epoch 3 step 981: training accuarcy: 0.8645\n",
      "Epoch 3 step 981: training loss: 721.1929153183985\n",
      "Epoch 3 step 982: training accuarcy: 0.877\n",
      "Epoch 3 step 982: training loss: 719.51712359576\n",
      "Epoch 3 step 983: training accuarcy: 0.882\n",
      "Epoch 3 step 983: training loss: 723.450213035123\n",
      "Epoch 3 step 984: training accuarcy: 0.8715\n",
      "Epoch 3 step 984: training loss: 783.0373445250023\n",
      "Epoch 3 step 985: training accuarcy: 0.862\n",
      "Epoch 3 step 985: training loss: 717.6068591850718\n",
      "Epoch 3 step 986: training accuarcy: 0.885\n",
      "Epoch 3 step 986: training loss: 754.4144739317842\n",
      "Epoch 3 step 987: training accuarcy: 0.8615\n",
      "Epoch 3 step 987: training loss: 728.9764800562848\n",
      "Epoch 3 step 988: training accuarcy: 0.873\n",
      "Epoch 3 step 988: training loss: 783.8914662141146\n",
      "Epoch 3 step 989: training accuarcy: 0.856\n",
      "Epoch 3 step 989: training loss: 779.8637412379371\n",
      "Epoch 3 step 990: training accuarcy: 0.864\n",
      "Epoch 3 step 990: training loss: 751.5088544017652\n",
      "Epoch 3 step 991: training accuarcy: 0.8715\n",
      "Epoch 3 step 991: training loss: 757.593006275208\n",
      "Epoch 3 step 992: training accuarcy: 0.8675\n",
      "Epoch 3 step 992: training loss: 733.3477241216308\n",
      "Epoch 3 step 993: training accuarcy: 0.8815000000000001\n",
      "Epoch 3 step 993: training loss: 769.7349779040952\n",
      "Epoch 3 step 994: training accuarcy: 0.8735\n",
      "Epoch 3 step 994: training loss: 738.284347117524\n",
      "Epoch 3 step 995: training accuarcy: 0.8715\n",
      "Epoch 3 step 995: training loss: 749.8419985953896\n",
      "Epoch 3 step 996: training accuarcy: 0.863\n",
      "Epoch 3 step 996: training loss: 736.7629112447735\n",
      "Epoch 3 step 997: training accuarcy: 0.879\n",
      "Epoch 3 step 997: training loss: 746.5487030002316\n",
      "Epoch 3 step 998: training accuarcy: 0.8775000000000001\n",
      "Epoch 3 step 998: training loss: 740.8252771606584\n",
      "Epoch 3 step 999: training accuarcy: 0.8685\n",
      "Epoch 3 step 999: training loss: 733.3139300469403\n",
      "Epoch 3 step 1000: training accuarcy: 0.8785000000000001\n",
      "Epoch 3 step 1000: training loss: 717.7028621330683\n",
      "Epoch 3 step 1001: training accuarcy: 0.8795000000000001\n",
      "Epoch 3 step 1001: training loss: 771.8163073797393\n",
      "Epoch 3 step 1002: training accuarcy: 0.8545\n",
      "Epoch 3 step 1002: training loss: 757.9360095708824\n",
      "Epoch 3 step 1003: training accuarcy: 0.8655\n",
      "Epoch 3 step 1003: training loss: 741.3864835773134\n",
      "Epoch 3 step 1004: training accuarcy: 0.862\n",
      "Epoch 3 step 1004: training loss: 759.1488471882469\n",
      "Epoch 3 step 1005: training accuarcy: 0.867\n",
      "Epoch 3 step 1005: training loss: 730.6144436564568\n",
      "Epoch 3 step 1006: training accuarcy: 0.879\n",
      "Epoch 3 step 1006: training loss: 739.2483487014847\n",
      "Epoch 3 step 1007: training accuarcy: 0.8765000000000001\n",
      "Epoch 3 step 1007: training loss: 814.7886544078082\n",
      "Epoch 3 step 1008: training accuarcy: 0.853\n",
      "Epoch 3 step 1008: training loss: 713.4838540988524\n",
      "Epoch 3 step 1009: training accuarcy: 0.886\n",
      "Epoch 3 step 1009: training loss: 748.7477116217425\n",
      "Epoch 3 step 1010: training accuarcy: 0.8715\n",
      "Epoch 3 step 1010: training loss: 717.0860637466711\n",
      "Epoch 3 step 1011: training accuarcy: 0.884\n",
      "Epoch 3 step 1011: training loss: 753.6874717994777\n",
      "Epoch 3 step 1012: training accuarcy: 0.877\n",
      "Epoch 3 step 1012: training loss: 736.7637905732223\n",
      "Epoch 3 step 1013: training accuarcy: 0.8765000000000001\n",
      "Epoch 3 step 1013: training loss: 751.5788617908904\n",
      "Epoch 3 step 1014: training accuarcy: 0.873\n",
      "Epoch 3 step 1014: training loss: 729.7921925129876\n",
      "Epoch 3 step 1015: training accuarcy: 0.8715\n",
      "Epoch 3 step 1015: training loss: 720.1936096231277\n",
      "Epoch 3 step 1016: training accuarcy: 0.882\n",
      "Epoch 3 step 1016: training loss: 751.2745246089726\n",
      "Epoch 3 step 1017: training accuarcy: 0.8735\n",
      "Epoch 3 step 1017: training loss: 709.7647345817551\n",
      "Epoch 3 step 1018: training accuarcy: 0.8835000000000001\n",
      "Epoch 3 step 1018: training loss: 783.1230686222713\n",
      "Epoch 3 step 1019: training accuarcy: 0.8615\n",
      "Epoch 3 step 1019: training loss: 730.1320941413842\n",
      "Epoch 3 step 1020: training accuarcy: 0.874\n",
      "Epoch 3 step 1020: training loss: 779.4814815777744\n",
      "Epoch 3 step 1021: training accuarcy: 0.857\n",
      "Epoch 3 step 1021: training loss: 745.6417086320384\n",
      "Epoch 3 step 1022: training accuarcy: 0.8675\n",
      "Epoch 3 step 1022: training loss: 740.321505982911\n",
      "Epoch 3 step 1023: training accuarcy: 0.8715\n",
      "Epoch 3 step 1023: training loss: 720.4232752210672\n",
      "Epoch 3 step 1024: training accuarcy: 0.88\n",
      "Epoch 3 step 1024: training loss: 756.9147307249032\n",
      "Epoch 3 step 1025: training accuarcy: 0.872\n",
      "Epoch 3 step 1025: training loss: 746.0933422116973\n",
      "Epoch 3 step 1026: training accuarcy: 0.864\n",
      "Epoch 3 step 1026: training loss: 755.2030109826077\n",
      "Epoch 3 step 1027: training accuarcy: 0.8775000000000001\n",
      "Epoch 3 step 1027: training loss: 794.0306150108538\n",
      "Epoch 3 step 1028: training accuarcy: 0.8495\n",
      "Epoch 3 step 1028: training loss: 760.8467154451147\n",
      "Epoch 3 step 1029: training accuarcy: 0.8655\n",
      "Epoch 3 step 1029: training loss: 809.3876768255587\n",
      "Epoch 3 step 1030: training accuarcy: 0.847\n",
      "Epoch 3 step 1030: training loss: 770.6075233520617\n",
      "Epoch 3 step 1031: training accuarcy: 0.866\n",
      "Epoch 3 step 1031: training loss: 750.0297689129445\n",
      "Epoch 3 step 1032: training accuarcy: 0.86\n",
      "Epoch 3 step 1032: training loss: 739.3063754050283\n",
      "Epoch 3 step 1033: training accuarcy: 0.8685\n",
      "Epoch 3 step 1033: training loss: 728.8761668249233\n",
      "Epoch 3 step 1034: training accuarcy: 0.878\n",
      "Epoch 3 step 1034: training loss: 751.6361092740849\n",
      "Epoch 3 step 1035: training accuarcy: 0.8685\n",
      "Epoch 3 step 1035: training loss: 754.2511655142824\n",
      "Epoch 3 step 1036: training accuarcy: 0.869\n",
      "Epoch 3 step 1036: training loss: 721.7596617256911\n",
      "Epoch 3 step 1037: training accuarcy: 0.874\n",
      "Epoch 3 step 1037: training loss: 713.4420557073653\n",
      "Epoch 3 step 1038: training accuarcy: 0.883\n",
      "Epoch 3 step 1038: training loss: 770.5255671463243\n",
      "Epoch 3 step 1039: training accuarcy: 0.858\n",
      "Epoch 3 step 1039: training loss: 778.734465871067\n",
      "Epoch 3 step 1040: training accuarcy: 0.8565\n",
      "Epoch 3 step 1040: training loss: 753.5125141356142\n",
      "Epoch 3 step 1041: training accuarcy: 0.8745\n",
      "Epoch 3 step 1041: training loss: 775.467914787276\n",
      "Epoch 3 step 1042: training accuarcy: 0.8555\n",
      "Epoch 3 step 1042: training loss: 728.4352578165135\n",
      "Epoch 3 step 1043: training accuarcy: 0.884\n",
      "Epoch 3 step 1043: training loss: 778.6329136026786\n",
      "Epoch 3 step 1044: training accuarcy: 0.868\n",
      "Epoch 3 step 1044: training loss: 731.3243387744275\n",
      "Epoch 3 step 1045: training accuarcy: 0.872\n",
      "Epoch 3 step 1045: training loss: 733.4764310629516\n",
      "Epoch 3 step 1046: training accuarcy: 0.8895000000000001\n",
      "Epoch 3 step 1046: training loss: 787.1518895965319\n",
      "Epoch 3 step 1047: training accuarcy: 0.8615\n",
      "Epoch 3 step 1047: training loss: 742.3310731017017\n",
      "Epoch 3 step 1048: training accuarcy: 0.873\n",
      "Epoch 3 step 1048: training loss: 746.466626343008\n",
      "Epoch 3 step 1049: training accuarcy: 0.866\n",
      "Epoch 3 step 1049: training loss: 744.3178469997149\n",
      "Epoch 3 step 1050: training accuarcy: 0.8695\n",
      "Epoch 3 step 1050: training loss: 755.0850871496486\n",
      "Epoch 3 step 1051: training accuarcy: 0.869\n",
      "Epoch 3 step 1051: training loss: 407.02321956890296\n",
      "Epoch 3 step 1052: training accuarcy: 0.8576923076923076\n",
      "Epoch 3: train loss 744.0687686633124, train accuarcy 0.8666102886199951\n",
      "Epoch 3: valid loss 3585.5123918690515, valid accuarcy 0.8457452654838562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 4/5 [21:58<05:26, 326.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 767.0175590315823\n",
      "Epoch 4 step 1053: training accuarcy: 0.8585\n",
      "Epoch 4 step 1053: training loss: 737.7048019569734\n",
      "Epoch 4 step 1054: training accuarcy: 0.876\n",
      "Epoch 4 step 1054: training loss: 716.4961974492793\n",
      "Epoch 4 step 1055: training accuarcy: 0.879\n",
      "Epoch 4 step 1055: training loss: 734.5857265296753\n",
      "Epoch 4 step 1056: training accuarcy: 0.87\n",
      "Epoch 4 step 1056: training loss: 704.9380041077133\n",
      "Epoch 4 step 1057: training accuarcy: 0.884\n",
      "Epoch 4 step 1057: training loss: 770.1127786024069\n",
      "Epoch 4 step 1058: training accuarcy: 0.868\n",
      "Epoch 4 step 1058: training loss: 749.0727160168962\n",
      "Epoch 4 step 1059: training accuarcy: 0.8735\n",
      "Epoch 4 step 1059: training loss: 759.2212353324198\n",
      "Epoch 4 step 1060: training accuarcy: 0.8695\n",
      "Epoch 4 step 1060: training loss: 740.2415140687274\n",
      "Epoch 4 step 1061: training accuarcy: 0.879\n",
      "Epoch 4 step 1061: training loss: 747.1435214728915\n",
      "Epoch 4 step 1062: training accuarcy: 0.8735\n",
      "Epoch 4 step 1062: training loss: 714.3089116836958\n",
      "Epoch 4 step 1063: training accuarcy: 0.8775000000000001\n",
      "Epoch 4 step 1063: training loss: 788.3823359018792\n",
      "Epoch 4 step 1064: training accuarcy: 0.8765000000000001\n",
      "Epoch 4 step 1064: training loss: 732.9532100042153\n",
      "Epoch 4 step 1065: training accuarcy: 0.87\n",
      "Epoch 4 step 1065: training loss: 766.8158590752034\n",
      "Epoch 4 step 1066: training accuarcy: 0.868\n",
      "Epoch 4 step 1066: training loss: 750.3677376439891\n",
      "Epoch 4 step 1067: training accuarcy: 0.8725\n",
      "Epoch 4 step 1067: training loss: 697.703215438274\n",
      "Epoch 4 step 1068: training accuarcy: 0.888\n",
      "Epoch 4 step 1068: training loss: 751.8165787343008\n",
      "Epoch 4 step 1069: training accuarcy: 0.8745\n",
      "Epoch 4 step 1069: training loss: 751.9757572908609\n",
      "Epoch 4 step 1070: training accuarcy: 0.873\n",
      "Epoch 4 step 1070: training loss: 739.1999813947293\n",
      "Epoch 4 step 1071: training accuarcy: 0.876\n",
      "Epoch 4 step 1071: training loss: 766.5335783818795\n",
      "Epoch 4 step 1072: training accuarcy: 0.874\n",
      "Epoch 4 step 1072: training loss: 709.5651359933961\n",
      "Epoch 4 step 1073: training accuarcy: 0.8815000000000001\n",
      "Epoch 4 step 1073: training loss: 776.7083137595426\n",
      "Epoch 4 step 1074: training accuarcy: 0.8735\n",
      "Epoch 4 step 1074: training loss: 704.4031550691184\n",
      "Epoch 4 step 1075: training accuarcy: 0.885\n",
      "Epoch 4 step 1075: training loss: 737.9465826745034\n",
      "Epoch 4 step 1076: training accuarcy: 0.8715\n",
      "Epoch 4 step 1076: training loss: 741.0764431569344\n",
      "Epoch 4 step 1077: training accuarcy: 0.876\n",
      "Epoch 4 step 1077: training loss: 737.4724694182815\n",
      "Epoch 4 step 1078: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1078: training loss: 739.4309249728542\n",
      "Epoch 4 step 1079: training accuarcy: 0.877\n",
      "Epoch 4 step 1079: training loss: 729.8664010173807\n",
      "Epoch 4 step 1080: training accuarcy: 0.876\n",
      "Epoch 4 step 1080: training loss: 707.6907741334674\n",
      "Epoch 4 step 1081: training accuarcy: 0.885\n",
      "Epoch 4 step 1081: training loss: 709.2332547299786\n",
      "Epoch 4 step 1082: training accuarcy: 0.8835000000000001\n",
      "Epoch 4 step 1082: training loss: 714.9662691353939\n",
      "Epoch 4 step 1083: training accuarcy: 0.882\n",
      "Epoch 4 step 1083: training loss: 718.1826278127422\n",
      "Epoch 4 step 1084: training accuarcy: 0.8905000000000001\n",
      "Epoch 4 step 1084: training loss: 761.6381638875363\n",
      "Epoch 4 step 1085: training accuarcy: 0.8685\n",
      "Epoch 4 step 1085: training loss: 760.3019821200259\n",
      "Epoch 4 step 1086: training accuarcy: 0.871\n",
      "Epoch 4 step 1086: training loss: 725.1361688265401\n",
      "Epoch 4 step 1087: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1087: training loss: 749.7958959641101\n",
      "Epoch 4 step 1088: training accuarcy: 0.865\n",
      "Epoch 4 step 1088: training loss: 729.7220787459254\n",
      "Epoch 4 step 1089: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1089: training loss: 735.323048029575\n",
      "Epoch 4 step 1090: training accuarcy: 0.875\n",
      "Epoch 4 step 1090: training loss: 736.5761996979606\n",
      "Epoch 4 step 1091: training accuarcy: 0.869\n",
      "Epoch 4 step 1091: training loss: 705.4535086669316\n",
      "Epoch 4 step 1092: training accuarcy: 0.8765000000000001\n",
      "Epoch 4 step 1092: training loss: 758.7490171795324\n",
      "Epoch 4 step 1093: training accuarcy: 0.868\n",
      "Epoch 4 step 1093: training loss: 735.0748473406761\n",
      "Epoch 4 step 1094: training accuarcy: 0.877\n",
      "Epoch 4 step 1094: training loss: 767.8304768911416\n",
      "Epoch 4 step 1095: training accuarcy: 0.871\n",
      "Epoch 4 step 1095: training loss: 740.1104681854401\n",
      "Epoch 4 step 1096: training accuarcy: 0.872\n",
      "Epoch 4 step 1096: training loss: 754.9215668769995\n",
      "Epoch 4 step 1097: training accuarcy: 0.87\n",
      "Epoch 4 step 1097: training loss: 728.4455395270104\n",
      "Epoch 4 step 1098: training accuarcy: 0.874\n",
      "Epoch 4 step 1098: training loss: 727.9219873813233\n",
      "Epoch 4 step 1099: training accuarcy: 0.8695\n",
      "Epoch 4 step 1099: training loss: 748.480220022474\n",
      "Epoch 4 step 1100: training accuarcy: 0.8675\n",
      "Epoch 4 step 1100: training loss: 758.5429509655507\n",
      "Epoch 4 step 1101: training accuarcy: 0.865\n",
      "Epoch 4 step 1101: training loss: 772.8152830598196\n",
      "Epoch 4 step 1102: training accuarcy: 0.8665\n",
      "Epoch 4 step 1102: training loss: 694.5268343253135\n",
      "Epoch 4 step 1103: training accuarcy: 0.883\n",
      "Epoch 4 step 1103: training loss: 746.6213882232355\n",
      "Epoch 4 step 1104: training accuarcy: 0.8735\n",
      "Epoch 4 step 1104: training loss: 751.9510417935429\n",
      "Epoch 4 step 1105: training accuarcy: 0.869\n",
      "Epoch 4 step 1105: training loss: 768.0118702873683\n",
      "Epoch 4 step 1106: training accuarcy: 0.867\n",
      "Epoch 4 step 1106: training loss: 724.3196081326174\n",
      "Epoch 4 step 1107: training accuarcy: 0.8715\n",
      "Epoch 4 step 1107: training loss: 711.6457489428051\n",
      "Epoch 4 step 1108: training accuarcy: 0.8815000000000001\n",
      "Epoch 4 step 1108: training loss: 717.1775219057491\n",
      "Epoch 4 step 1109: training accuarcy: 0.882\n",
      "Epoch 4 step 1109: training loss: 746.3425096599834\n",
      "Epoch 4 step 1110: training accuarcy: 0.871\n",
      "Epoch 4 step 1110: training loss: 719.3558875434345\n",
      "Epoch 4 step 1111: training accuarcy: 0.878\n",
      "Epoch 4 step 1111: training loss: 726.5798365936549\n",
      "Epoch 4 step 1112: training accuarcy: 0.8685\n",
      "Epoch 4 step 1112: training loss: 743.1025802769466\n",
      "Epoch 4 step 1113: training accuarcy: 0.872\n",
      "Epoch 4 step 1113: training loss: 730.2450905460155\n",
      "Epoch 4 step 1114: training accuarcy: 0.8805000000000001\n",
      "Epoch 4 step 1114: training loss: 748.5521342362483\n",
      "Epoch 4 step 1115: training accuarcy: 0.8705\n",
      "Epoch 4 step 1115: training loss: 739.4804256958682\n",
      "Epoch 4 step 1116: training accuarcy: 0.871\n",
      "Epoch 4 step 1116: training loss: 717.6448757790856\n",
      "Epoch 4 step 1117: training accuarcy: 0.8805000000000001\n",
      "Epoch 4 step 1117: training loss: 744.790412038588\n",
      "Epoch 4 step 1118: training accuarcy: 0.869\n",
      "Epoch 4 step 1118: training loss: 755.4577366677798\n",
      "Epoch 4 step 1119: training accuarcy: 0.867\n",
      "Epoch 4 step 1119: training loss: 749.8777521872755\n",
      "Epoch 4 step 1120: training accuarcy: 0.8705\n",
      "Epoch 4 step 1120: training loss: 728.9647038448309\n",
      "Epoch 4 step 1121: training accuarcy: 0.8775000000000001\n",
      "Epoch 4 step 1121: training loss: 770.0564136533985\n",
      "Epoch 4 step 1122: training accuarcy: 0.8715\n",
      "Epoch 4 step 1122: training loss: 727.3782792873484\n",
      "Epoch 4 step 1123: training accuarcy: 0.875\n",
      "Epoch 4 step 1123: training loss: 743.4921501851085\n",
      "Epoch 4 step 1124: training accuarcy: 0.881\n",
      "Epoch 4 step 1124: training loss: 731.5253570816028\n",
      "Epoch 4 step 1125: training accuarcy: 0.882\n",
      "Epoch 4 step 1125: training loss: 783.7414514168303\n",
      "Epoch 4 step 1126: training accuarcy: 0.86\n",
      "Epoch 4 step 1126: training loss: 773.1880902394486\n",
      "Epoch 4 step 1127: training accuarcy: 0.8615\n",
      "Epoch 4 step 1127: training loss: 786.809938919568\n",
      "Epoch 4 step 1128: training accuarcy: 0.8655\n",
      "Epoch 4 step 1128: training loss: 719.9726080346222\n",
      "Epoch 4 step 1129: training accuarcy: 0.8805000000000001\n",
      "Epoch 4 step 1129: training loss: 750.4874555700865\n",
      "Epoch 4 step 1130: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1130: training loss: 774.3006361498074\n",
      "Epoch 4 step 1131: training accuarcy: 0.8685\n",
      "Epoch 4 step 1131: training loss: 741.2196127070724\n",
      "Epoch 4 step 1132: training accuarcy: 0.872\n",
      "Epoch 4 step 1132: training loss: 723.0615391785659\n",
      "Epoch 4 step 1133: training accuarcy: 0.876\n",
      "Epoch 4 step 1133: training loss: 719.8821898463827\n",
      "Epoch 4 step 1134: training accuarcy: 0.88\n",
      "Epoch 4 step 1134: training loss: 732.6867048575099\n",
      "Epoch 4 step 1135: training accuarcy: 0.8785000000000001\n",
      "Epoch 4 step 1135: training loss: 725.8598572856968\n",
      "Epoch 4 step 1136: training accuarcy: 0.887\n",
      "Epoch 4 step 1136: training loss: 723.0381262799331\n",
      "Epoch 4 step 1137: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1137: training loss: 747.0629551706909\n",
      "Epoch 4 step 1138: training accuarcy: 0.8765000000000001\n",
      "Epoch 4 step 1138: training loss: 748.4851658248709\n",
      "Epoch 4 step 1139: training accuarcy: 0.866\n",
      "Epoch 4 step 1139: training loss: 791.0283024792255\n",
      "Epoch 4 step 1140: training accuarcy: 0.861\n",
      "Epoch 4 step 1140: training loss: 728.8975240365711\n",
      "Epoch 4 step 1141: training accuarcy: 0.878\n",
      "Epoch 4 step 1141: training loss: 745.9434059972158\n",
      "Epoch 4 step 1142: training accuarcy: 0.8735\n",
      "Epoch 4 step 1142: training loss: 733.853914437929\n",
      "Epoch 4 step 1143: training accuarcy: 0.8735\n",
      "Epoch 4 step 1143: training loss: 756.081163476003\n",
      "Epoch 4 step 1144: training accuarcy: 0.8685\n",
      "Epoch 4 step 1144: training loss: 763.4134529592433\n",
      "Epoch 4 step 1145: training accuarcy: 0.8705\n",
      "Epoch 4 step 1145: training loss: 707.6207942649476\n",
      "Epoch 4 step 1146: training accuarcy: 0.875\n",
      "Epoch 4 step 1146: training loss: 792.2033140143152\n",
      "Epoch 4 step 1147: training accuarcy: 0.8545\n",
      "Epoch 4 step 1147: training loss: 750.9092917544064\n",
      "Epoch 4 step 1148: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1148: training loss: 719.7799780220096\n",
      "Epoch 4 step 1149: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1149: training loss: 745.1311377995382\n",
      "Epoch 4 step 1150: training accuarcy: 0.8635\n",
      "Epoch 4 step 1150: training loss: 748.4202476412383\n",
      "Epoch 4 step 1151: training accuarcy: 0.8685\n",
      "Epoch 4 step 1151: training loss: 739.0748351006437\n",
      "Epoch 4 step 1152: training accuarcy: 0.8665\n",
      "Epoch 4 step 1152: training loss: 758.0535286600691\n",
      "Epoch 4 step 1153: training accuarcy: 0.8645\n",
      "Epoch 4 step 1153: training loss: 797.5933111104719\n",
      "Epoch 4 step 1154: training accuarcy: 0.858\n",
      "Epoch 4 step 1154: training loss: 754.2700661303783\n",
      "Epoch 4 step 1155: training accuarcy: 0.8675\n",
      "Epoch 4 step 1155: training loss: 750.0241287280944\n",
      "Epoch 4 step 1156: training accuarcy: 0.8695\n",
      "Epoch 4 step 1156: training loss: 738.0692167075014\n",
      "Epoch 4 step 1157: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1157: training loss: 720.6159472643692\n",
      "Epoch 4 step 1158: training accuarcy: 0.8745\n",
      "Epoch 4 step 1158: training loss: 726.1357632616716\n",
      "Epoch 4 step 1159: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1159: training loss: 745.9852722673293\n",
      "Epoch 4 step 1160: training accuarcy: 0.8735\n",
      "Epoch 4 step 1160: training loss: 743.7102307320614\n",
      "Epoch 4 step 1161: training accuarcy: 0.872\n",
      "Epoch 4 step 1161: training loss: 802.8290827454748\n",
      "Epoch 4 step 1162: training accuarcy: 0.8595\n",
      "Epoch 4 step 1162: training loss: 708.7048835790015\n",
      "Epoch 4 step 1163: training accuarcy: 0.879\n",
      "Epoch 4 step 1163: training loss: 720.797301796299\n",
      "Epoch 4 step 1164: training accuarcy: 0.876\n",
      "Epoch 4 step 1164: training loss: 732.8832708394742\n",
      "Epoch 4 step 1165: training accuarcy: 0.881\n",
      "Epoch 4 step 1165: training loss: 709.1347357886036\n",
      "Epoch 4 step 1166: training accuarcy: 0.883\n",
      "Epoch 4 step 1166: training loss: 736.3135391114433\n",
      "Epoch 4 step 1167: training accuarcy: 0.8765000000000001\n",
      "Epoch 4 step 1167: training loss: 718.6432873990422\n",
      "Epoch 4 step 1168: training accuarcy: 0.873\n",
      "Epoch 4 step 1168: training loss: 715.1063273756175\n",
      "Epoch 4 step 1169: training accuarcy: 0.879\n",
      "Epoch 4 step 1169: training loss: 705.5430756758861\n",
      "Epoch 4 step 1170: training accuarcy: 0.878\n",
      "Epoch 4 step 1170: training loss: 745.6465507317455\n",
      "Epoch 4 step 1171: training accuarcy: 0.8695\n",
      "Epoch 4 step 1171: training loss: 760.7977234453296\n",
      "Epoch 4 step 1172: training accuarcy: 0.869\n",
      "Epoch 4 step 1172: training loss: 738.5449098023415\n",
      "Epoch 4 step 1173: training accuarcy: 0.87\n",
      "Epoch 4 step 1173: training loss: 764.2170527222615\n",
      "Epoch 4 step 1174: training accuarcy: 0.8605\n",
      "Epoch 4 step 1174: training loss: 729.762198695243\n",
      "Epoch 4 step 1175: training accuarcy: 0.88\n",
      "Epoch 4 step 1175: training loss: 764.2928760122272\n",
      "Epoch 4 step 1176: training accuarcy: 0.863\n",
      "Epoch 4 step 1176: training loss: 742.0855063686406\n",
      "Epoch 4 step 1177: training accuarcy: 0.875\n",
      "Epoch 4 step 1177: training loss: 753.1142237824434\n",
      "Epoch 4 step 1178: training accuarcy: 0.8725\n",
      "Epoch 4 step 1178: training loss: 722.1260336474811\n",
      "Epoch 4 step 1179: training accuarcy: 0.8715\n",
      "Epoch 4 step 1179: training loss: 760.3660485157658\n",
      "Epoch 4 step 1180: training accuarcy: 0.8625\n",
      "Epoch 4 step 1180: training loss: 748.1687607908332\n",
      "Epoch 4 step 1181: training accuarcy: 0.865\n",
      "Epoch 4 step 1181: training loss: 728.3924488346329\n",
      "Epoch 4 step 1182: training accuarcy: 0.8785000000000001\n",
      "Epoch 4 step 1182: training loss: 720.9746485067295\n",
      "Epoch 4 step 1183: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1183: training loss: 735.1233178451573\n",
      "Epoch 4 step 1184: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1184: training loss: 779.9419949056061\n",
      "Epoch 4 step 1185: training accuarcy: 0.8615\n",
      "Epoch 4 step 1185: training loss: 736.5484773760699\n",
      "Epoch 4 step 1186: training accuarcy: 0.872\n",
      "Epoch 4 step 1186: training loss: 748.754994658877\n",
      "Epoch 4 step 1187: training accuarcy: 0.864\n",
      "Epoch 4 step 1187: training loss: 721.2462593579689\n",
      "Epoch 4 step 1188: training accuarcy: 0.8805000000000001\n",
      "Epoch 4 step 1188: training loss: 755.0305303490328\n",
      "Epoch 4 step 1189: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1189: training loss: 711.2934739371053\n",
      "Epoch 4 step 1190: training accuarcy: 0.885\n",
      "Epoch 4 step 1190: training loss: 741.1267212591332\n",
      "Epoch 4 step 1191: training accuarcy: 0.874\n",
      "Epoch 4 step 1191: training loss: 721.9547723171802\n",
      "Epoch 4 step 1192: training accuarcy: 0.881\n",
      "Epoch 4 step 1192: training loss: 712.5129186200346\n",
      "Epoch 4 step 1193: training accuarcy: 0.872\n",
      "Epoch 4 step 1193: training loss: 773.194057429377\n",
      "Epoch 4 step 1194: training accuarcy: 0.861\n",
      "Epoch 4 step 1194: training loss: 726.790070229212\n",
      "Epoch 4 step 1195: training accuarcy: 0.876\n",
      "Epoch 4 step 1195: training loss: 771.1977432117711\n",
      "Epoch 4 step 1196: training accuarcy: 0.8705\n",
      "Epoch 4 step 1196: training loss: 766.904305249233\n",
      "Epoch 4 step 1197: training accuarcy: 0.867\n",
      "Epoch 4 step 1197: training loss: 749.3959086060822\n",
      "Epoch 4 step 1198: training accuarcy: 0.8725\n",
      "Epoch 4 step 1198: training loss: 763.7073832787124\n",
      "Epoch 4 step 1199: training accuarcy: 0.865\n",
      "Epoch 4 step 1199: training loss: 729.7716573156698\n",
      "Epoch 4 step 1200: training accuarcy: 0.876\n",
      "Epoch 4 step 1200: training loss: 742.580094530843\n",
      "Epoch 4 step 1201: training accuarcy: 0.873\n",
      "Epoch 4 step 1201: training loss: 772.1325940255449\n",
      "Epoch 4 step 1202: training accuarcy: 0.8615\n",
      "Epoch 4 step 1202: training loss: 765.9817808058771\n",
      "Epoch 4 step 1203: training accuarcy: 0.8615\n",
      "Epoch 4 step 1203: training loss: 744.9250580852863\n",
      "Epoch 4 step 1204: training accuarcy: 0.8685\n",
      "Epoch 4 step 1204: training loss: 756.8158903329222\n",
      "Epoch 4 step 1205: training accuarcy: 0.8735\n",
      "Epoch 4 step 1205: training loss: 756.2751608539944\n",
      "Epoch 4 step 1206: training accuarcy: 0.8615\n",
      "Epoch 4 step 1206: training loss: 751.400624541224\n",
      "Epoch 4 step 1207: training accuarcy: 0.874\n",
      "Epoch 4 step 1207: training loss: 727.9518450605858\n",
      "Epoch 4 step 1208: training accuarcy: 0.876\n",
      "Epoch 4 step 1208: training loss: 732.4648013969974\n",
      "Epoch 4 step 1209: training accuarcy: 0.874\n",
      "Epoch 4 step 1209: training loss: 764.206237837542\n",
      "Epoch 4 step 1210: training accuarcy: 0.868\n",
      "Epoch 4 step 1210: training loss: 741.9080869161226\n",
      "Epoch 4 step 1211: training accuarcy: 0.873\n",
      "Epoch 4 step 1211: training loss: 745.287241669668\n",
      "Epoch 4 step 1212: training accuarcy: 0.8715\n",
      "Epoch 4 step 1212: training loss: 731.460762146747\n",
      "Epoch 4 step 1213: training accuarcy: 0.875\n",
      "Epoch 4 step 1213: training loss: 744.336727367835\n",
      "Epoch 4 step 1214: training accuarcy: 0.8745\n",
      "Epoch 4 step 1214: training loss: 761.5767042323566\n",
      "Epoch 4 step 1215: training accuarcy: 0.864\n",
      "Epoch 4 step 1215: training loss: 734.2602443311569\n",
      "Epoch 4 step 1216: training accuarcy: 0.873\n",
      "Epoch 4 step 1216: training loss: 777.2118017091052\n",
      "Epoch 4 step 1217: training accuarcy: 0.871\n",
      "Epoch 4 step 1217: training loss: 747.0979897682532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1218: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1218: training loss: 733.5058235722876\n",
      "Epoch 4 step 1219: training accuarcy: 0.8805000000000001\n",
      "Epoch 4 step 1219: training loss: 768.2482503724679\n",
      "Epoch 4 step 1220: training accuarcy: 0.866\n",
      "Epoch 4 step 1220: training loss: 747.8459485672889\n",
      "Epoch 4 step 1221: training accuarcy: 0.8675\n",
      "Epoch 4 step 1221: training loss: 732.8843970368218\n",
      "Epoch 4 step 1222: training accuarcy: 0.8775000000000001\n",
      "Epoch 4 step 1222: training loss: 731.347330084879\n",
      "Epoch 4 step 1223: training accuarcy: 0.881\n",
      "Epoch 4 step 1223: training loss: 749.8136272840882\n",
      "Epoch 4 step 1224: training accuarcy: 0.864\n",
      "Epoch 4 step 1224: training loss: 747.1137312084136\n",
      "Epoch 4 step 1225: training accuarcy: 0.8775000000000001\n",
      "Epoch 4 step 1225: training loss: 794.4833948802295\n",
      "Epoch 4 step 1226: training accuarcy: 0.869\n",
      "Epoch 4 step 1226: training loss: 727.6877375651801\n",
      "Epoch 4 step 1227: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1227: training loss: 725.7308362543216\n",
      "Epoch 4 step 1228: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1228: training loss: 725.0696556042344\n",
      "Epoch 4 step 1229: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1229: training loss: 712.5210401647211\n",
      "Epoch 4 step 1230: training accuarcy: 0.8775000000000001\n",
      "Epoch 4 step 1230: training loss: 765.5452509286233\n",
      "Epoch 4 step 1231: training accuarcy: 0.8635\n",
      "Epoch 4 step 1231: training loss: 754.2166278442866\n",
      "Epoch 4 step 1232: training accuarcy: 0.8685\n",
      "Epoch 4 step 1232: training loss: 755.0480575934272\n",
      "Epoch 4 step 1233: training accuarcy: 0.8705\n",
      "Epoch 4 step 1233: training loss: 752.0674224146964\n",
      "Epoch 4 step 1234: training accuarcy: 0.8695\n",
      "Epoch 4 step 1234: training loss: 753.9217335404841\n",
      "Epoch 4 step 1235: training accuarcy: 0.871\n",
      "Epoch 4 step 1235: training loss: 776.0615682856089\n",
      "Epoch 4 step 1236: training accuarcy: 0.868\n",
      "Epoch 4 step 1236: training loss: 744.4824362014238\n",
      "Epoch 4 step 1237: training accuarcy: 0.8685\n",
      "Epoch 4 step 1237: training loss: 734.3381959670346\n",
      "Epoch 4 step 1238: training accuarcy: 0.8735\n",
      "Epoch 4 step 1238: training loss: 726.0674782142764\n",
      "Epoch 4 step 1239: training accuarcy: 0.874\n",
      "Epoch 4 step 1239: training loss: 744.2825805953415\n",
      "Epoch 4 step 1240: training accuarcy: 0.869\n",
      "Epoch 4 step 1240: training loss: 701.534791269993\n",
      "Epoch 4 step 1241: training accuarcy: 0.879\n",
      "Epoch 4 step 1241: training loss: 744.7026823313664\n",
      "Epoch 4 step 1242: training accuarcy: 0.873\n",
      "Epoch 4 step 1242: training loss: 704.3711023931526\n",
      "Epoch 4 step 1243: training accuarcy: 0.8825000000000001\n",
      "Epoch 4 step 1243: training loss: 718.0943760027862\n",
      "Epoch 4 step 1244: training accuarcy: 0.885\n",
      "Epoch 4 step 1244: training loss: 725.9406413496512\n",
      "Epoch 4 step 1245: training accuarcy: 0.881\n",
      "Epoch 4 step 1245: training loss: 780.1145515643746\n",
      "Epoch 4 step 1246: training accuarcy: 0.863\n",
      "Epoch 4 step 1246: training loss: 740.7928389793635\n",
      "Epoch 4 step 1247: training accuarcy: 0.8735\n",
      "Epoch 4 step 1247: training loss: 723.9662480421512\n",
      "Epoch 4 step 1248: training accuarcy: 0.876\n",
      "Epoch 4 step 1248: training loss: 765.1773918336553\n",
      "Epoch 4 step 1249: training accuarcy: 0.8715\n",
      "Epoch 4 step 1249: training loss: 736.3635086271215\n",
      "Epoch 4 step 1250: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1250: training loss: 670.8955227156264\n",
      "Epoch 4 step 1251: training accuarcy: 0.8825000000000001\n",
      "Epoch 4 step 1251: training loss: 754.3169291902752\n",
      "Epoch 4 step 1252: training accuarcy: 0.8725\n",
      "Epoch 4 step 1252: training loss: 774.1879920280074\n",
      "Epoch 4 step 1253: training accuarcy: 0.868\n",
      "Epoch 4 step 1253: training loss: 806.4863132135335\n",
      "Epoch 4 step 1254: training accuarcy: 0.869\n",
      "Epoch 4 step 1254: training loss: 754.696873944781\n",
      "Epoch 4 step 1255: training accuarcy: 0.8685\n",
      "Epoch 4 step 1255: training loss: 719.6686179467246\n",
      "Epoch 4 step 1256: training accuarcy: 0.879\n",
      "Epoch 4 step 1256: training loss: 755.9429064521619\n",
      "Epoch 4 step 1257: training accuarcy: 0.8615\n",
      "Epoch 4 step 1257: training loss: 712.1650514941396\n",
      "Epoch 4 step 1258: training accuarcy: 0.8865000000000001\n",
      "Epoch 4 step 1258: training loss: 759.4708767878728\n",
      "Epoch 4 step 1259: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1259: training loss: 747.4708068205168\n",
      "Epoch 4 step 1260: training accuarcy: 0.872\n",
      "Epoch 4 step 1260: training loss: 755.0136302441342\n",
      "Epoch 4 step 1261: training accuarcy: 0.8695\n",
      "Epoch 4 step 1261: training loss: 761.904736671656\n",
      "Epoch 4 step 1262: training accuarcy: 0.875\n",
      "Epoch 4 step 1262: training loss: 747.3639749893553\n",
      "Epoch 4 step 1263: training accuarcy: 0.873\n",
      "Epoch 4 step 1263: training loss: 721.2027874451329\n",
      "Epoch 4 step 1264: training accuarcy: 0.883\n",
      "Epoch 4 step 1264: training loss: 759.6133050866889\n",
      "Epoch 4 step 1265: training accuarcy: 0.8725\n",
      "Epoch 4 step 1265: training loss: 728.1947809915018\n",
      "Epoch 4 step 1266: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1266: training loss: 759.0343915725963\n",
      "Epoch 4 step 1267: training accuarcy: 0.866\n",
      "Epoch 4 step 1267: training loss: 783.9678504479152\n",
      "Epoch 4 step 1268: training accuarcy: 0.863\n",
      "Epoch 4 step 1268: training loss: 733.9121467708105\n",
      "Epoch 4 step 1269: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1269: training loss: 725.4447027206136\n",
      "Epoch 4 step 1270: training accuarcy: 0.873\n",
      "Epoch 4 step 1270: training loss: 770.8764463692022\n",
      "Epoch 4 step 1271: training accuarcy: 0.8645\n",
      "Epoch 4 step 1271: training loss: 751.3474102039463\n",
      "Epoch 4 step 1272: training accuarcy: 0.869\n",
      "Epoch 4 step 1272: training loss: 722.0410320624853\n",
      "Epoch 4 step 1273: training accuarcy: 0.8745\n",
      "Epoch 4 step 1273: training loss: 696.8764045513756\n",
      "Epoch 4 step 1274: training accuarcy: 0.8845000000000001\n",
      "Epoch 4 step 1274: training loss: 737.5873612980104\n",
      "Epoch 4 step 1275: training accuarcy: 0.875\n",
      "Epoch 4 step 1275: training loss: 726.7878879193843\n",
      "Epoch 4 step 1276: training accuarcy: 0.8795000000000001\n",
      "Epoch 4 step 1276: training loss: 764.9062122358873\n",
      "Epoch 4 step 1277: training accuarcy: 0.8685\n",
      "Epoch 4 step 1277: training loss: 742.5797215885768\n",
      "Epoch 4 step 1278: training accuarcy: 0.869\n",
      "Epoch 4 step 1278: training loss: 744.6885040791499\n",
      "Epoch 4 step 1279: training accuarcy: 0.8665\n",
      "Epoch 4 step 1279: training loss: 696.7681122595192\n",
      "Epoch 4 step 1280: training accuarcy: 0.8835000000000001\n",
      "Epoch 4 step 1280: training loss: 756.7522305778737\n",
      "Epoch 4 step 1281: training accuarcy: 0.87\n",
      "Epoch 4 step 1281: training loss: 725.4301570443158\n",
      "Epoch 4 step 1282: training accuarcy: 0.885\n",
      "Epoch 4 step 1282: training loss: 763.0558288861204\n",
      "Epoch 4 step 1283: training accuarcy: 0.876\n",
      "Epoch 4 step 1283: training loss: 711.4288789153127\n",
      "Epoch 4 step 1284: training accuarcy: 0.882\n",
      "Epoch 4 step 1284: training loss: 766.295758877704\n",
      "Epoch 4 step 1285: training accuarcy: 0.8715\n",
      "Epoch 4 step 1285: training loss: 751.3560438008616\n",
      "Epoch 4 step 1286: training accuarcy: 0.8755000000000001\n",
      "Epoch 4 step 1286: training loss: 712.7732934079598\n",
      "Epoch 4 step 1287: training accuarcy: 0.8785000000000001\n",
      "Epoch 4 step 1287: training loss: 694.3135930409875\n",
      "Epoch 4 step 1288: training accuarcy: 0.8825000000000001\n",
      "Epoch 4 step 1288: training loss: 729.1309945718277\n",
      "Epoch 4 step 1289: training accuarcy: 0.883\n",
      "Epoch 4 step 1289: training loss: 743.4913212254936\n",
      "Epoch 4 step 1290: training accuarcy: 0.873\n",
      "Epoch 4 step 1290: training loss: 742.1065488855552\n",
      "Epoch 4 step 1291: training accuarcy: 0.877\n",
      "Epoch 4 step 1291: training loss: 744.4149664199767\n",
      "Epoch 4 step 1292: training accuarcy: 0.867\n",
      "Epoch 4 step 1292: training loss: 782.1697456378074\n",
      "Epoch 4 step 1293: training accuarcy: 0.8665\n",
      "Epoch 4 step 1293: training loss: 767.106355247885\n",
      "Epoch 4 step 1294: training accuarcy: 0.8675\n",
      "Epoch 4 step 1294: training loss: 768.8109719270801\n",
      "Epoch 4 step 1295: training accuarcy: 0.866\n",
      "Epoch 4 step 1295: training loss: 736.7131990943179\n",
      "Epoch 4 step 1296: training accuarcy: 0.877\n",
      "Epoch 4 step 1296: training loss: 780.0308806034267\n",
      "Epoch 4 step 1297: training accuarcy: 0.866\n",
      "Epoch 4 step 1297: training loss: 760.7837393253385\n",
      "Epoch 4 step 1298: training accuarcy: 0.869\n",
      "Epoch 4 step 1298: training loss: 748.9192422309054\n",
      "Epoch 4 step 1299: training accuarcy: 0.8715\n",
      "Epoch 4 step 1299: training loss: 754.3866824501777\n",
      "Epoch 4 step 1300: training accuarcy: 0.872\n",
      "Epoch 4 step 1300: training loss: 756.4973603594458\n",
      "Epoch 4 step 1301: training accuarcy: 0.871\n",
      "Epoch 4 step 1301: training loss: 737.3758007807032\n",
      "Epoch 4 step 1302: training accuarcy: 0.863\n",
      "Epoch 4 step 1302: training loss: 738.8006985444026\n",
      "Epoch 4 step 1303: training accuarcy: 0.8675\n",
      "Epoch 4 step 1303: training loss: 753.6715817247134\n",
      "Epoch 4 step 1304: training accuarcy: 0.872\n",
      "Epoch 4 step 1304: training loss: 727.334907500064\n",
      "Epoch 4 step 1305: training accuarcy: 0.8835000000000001\n",
      "Epoch 4 step 1305: training loss: 738.5683801189239\n",
      "Epoch 4 step 1306: training accuarcy: 0.8745\n",
      "Epoch 4 step 1306: training loss: 778.447500151175\n",
      "Epoch 4 step 1307: training accuarcy: 0.859\n",
      "Epoch 4 step 1307: training loss: 776.4520892023776\n",
      "Epoch 4 step 1308: training accuarcy: 0.864\n",
      "Epoch 4 step 1308: training loss: 731.9257279783781\n",
      "Epoch 4 step 1309: training accuarcy: 0.873\n",
      "Epoch 4 step 1309: training loss: 785.9398997100859\n",
      "Epoch 4 step 1310: training accuarcy: 0.85\n",
      "Epoch 4 step 1310: training loss: 737.4142839721089\n",
      "Epoch 4 step 1311: training accuarcy: 0.8715\n",
      "Epoch 4 step 1311: training loss: 731.6518479917272\n",
      "Epoch 4 step 1312: training accuarcy: 0.873\n",
      "Epoch 4 step 1312: training loss: 718.033543761258\n",
      "Epoch 4 step 1313: training accuarcy: 0.883\n",
      "Epoch 4 step 1313: training loss: 754.8619145133034\n",
      "Epoch 4 step 1314: training accuarcy: 0.8705\n",
      "Epoch 4 step 1314: training loss: 385.6386598538486\n",
      "Epoch 4 step 1315: training accuarcy: 0.8782051282051282\n",
      "Epoch 4: train loss 741.6172478528799, train accuarcy 0.8695799708366394\n",
      "Epoch 4: valid loss 3590.2032129116515, valid accuarcy 0.8466346859931946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [27:18<00:00, 324.59s/it]\n"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=5,\n",
    "                  log_dir=get_log_dir('seq_topcoder', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:32:23.605411Z",
     "start_time": "2019-10-08T02:32:23.596442Z"
    }
   },
   "outputs": [],
   "source": [
    "del trans_model\n",
    "T.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
