{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:01:30.581717Z",
     "start_time": "2019-09-25T14:01:30.456659Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:36:26.101712Z",
     "start_time": "2019-09-25T12:36:26.094719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\python\\recommender\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:40:01.218847Z",
     "start_time": "2019-09-25T12:40:01.214849Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import get_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:00:19.883886Z",
     "start_time": "2019-09-25T14:00:19.880886Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import TorchMovielen10k, TorchTopcoder, DataBunch\n",
    "from models import FMLearner, TorchFM, TorchHrmFM, TorchPrmeFM, TorchTransFM\n",
    "from models.fm_learner import simple_loss, trans_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:36:36.044319Z",
     "start_time": "2019-09-25T12:36:35.839323Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = T.cuda.current_device()\n",
    "BATCH = 2000\n",
    "SHUFFLE = True\n",
    "WORKERS = 0\n",
    "REGS_PATH = Path(\"./inputs/topcoder/regs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:36:45.292848Z",
     "start_time": "2019-09-25T12:36:44.015836Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-25 20:36:44,356 - C:\\Projects\\python\\recommender\\utils.py - INFO - Read dataset in inputs\\topcoder\\regs.csv\n",
      "2019-09-25 20:36:44,356 - C:\\Projects\\python\\recommender\\utils.py - INFO - Read dataset in inputs\\topcoder\\regs.csv\n",
      "2019-09-25 20:36:44,356 - C:\\Projects\\python\\recommender\\utils.py - INFO - Read dataset in inputs\\topcoder\\regs.csv\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0925 20:36:44.356832  2240 torch_topcoder.py:47] Read dataset in inputs\\topcoder\\regs.csv\n",
      "2019-09-25 20:36:44,363 - C:\\Projects\\python\\recommender\\utils.py - INFO - Original regs shape: (610025, 3)\n",
      "2019-09-25 20:36:44,363 - C:\\Projects\\python\\recommender\\utils.py - INFO - Original regs shape: (610025, 3)\n",
      "2019-09-25 20:36:44,363 - C:\\Projects\\python\\recommender\\utils.py - INFO - Original regs shape: (610025, 3)\n",
      "I0925 20:36:44.363837  2240 torch_topcoder.py:48] Original regs shape: (610025, 3)\n",
      "2019-09-25 20:36:44,489 - C:\\Projects\\python\\recommender\\utils.py - INFO - Original registants size: 60017\n",
      "2019-09-25 20:36:44,489 - C:\\Projects\\python\\recommender\\utils.py - INFO - Original registants size: 60017\n",
      "2019-09-25 20:36:44,489 - C:\\Projects\\python\\recommender\\utils.py - INFO - Original registants size: 60017\n",
      "I0925 20:36:44.489835  2240 torch_topcoder.py:53] Original registants size: 60017\n",
      "2019-09-25 20:36:44,496 - C:\\Projects\\python\\recommender\\utils.py - INFO - Original challenges size: 39916\n",
      "2019-09-25 20:36:44,496 - C:\\Projects\\python\\recommender\\utils.py - INFO - Original challenges size: 39916\n",
      "2019-09-25 20:36:44,496 - C:\\Projects\\python\\recommender\\utils.py - INFO - Original challenges size: 39916\n",
      "I0925 20:36:44.496833  2240 torch_topcoder.py:54] Original challenges size: 39916\n",
      "2019-09-25 20:36:44,560 - C:\\Projects\\python\\recommender\\utils.py - INFO - Filter dataframe shape: (544568, 3)\n",
      "2019-09-25 20:36:44,560 - C:\\Projects\\python\\recommender\\utils.py - INFO - Filter dataframe shape: (544568, 3)\n",
      "2019-09-25 20:36:44,560 - C:\\Projects\\python\\recommender\\utils.py - INFO - Filter dataframe shape: (544568, 3)\n",
      "I0925 20:36:44.560834  2240 torch_topcoder.py:61] Filter dataframe shape: (544568, 3)\n",
      "C:\\Projects\\python\\recommender\\datasets\\torch_topcoder.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  regs_df['previousId'][first_mask] = -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datasets.torch_topcoder.TorchTopcoder at 0x1f26d1eeb38>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = TorchTopcoder(REGS_PATH)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:37:38.954279Z",
     "start_time": "2019-09-25T12:37:38.951275Z"
    }
   },
   "outputs": [],
   "source": [
    "db.config_db(batch_size=BATCH,\n",
    "                      shuffle=SHUFFLE,\n",
    "                      num_workers=WORKERS,\n",
    "                      device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:37:47.025907Z",
     "start_time": "2019-09-25T12:37:47.022907Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:38:20.308728Z",
     "start_time": "2019-09-25T12:38:20.305752Z"
    }
   },
   "outputs": [],
   "source": [
    "# regst setting\n",
    "LINEAR_REG = 1\n",
    "EMB_REG = 1\n",
    "TRANS_REG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:38:21.123168Z",
     "start_time": "2019-09-25T12:38:21.119166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_loss at 0x000001F26D1EAAE8>, 1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_loss_callback = partial(simple_loss, LINEAR_REG, EMB_REG)\n",
    "simple_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:38:31.940818Z",
     "start_time": "2019-09-25T12:38:31.935826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_loss at 0x000001F26D246BF8>, 1, 1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_loss_callback = partial(trans_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_loss_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "#### Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:39:01.186507Z",
     "start_time": "2019-09-25T12:39:01.183510Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:39:18.521944Z",
     "start_time": "2019-09-25T12:39:18.518939Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DECAY_FREQ = 1000\n",
    "DECAY_GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:39:19.151554Z",
     "start_time": "2019-09-25T12:39:18.926554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchFM()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model = TorchFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:39:28.723879Z",
     "start_time": "2019-09-25T12:39:28.720879Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(fm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt, step_size=DECAY_FREQ, gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:39:37.205240Z",
     "start_time": "2019-09-25T12:39:34.806272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1f27495e438>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_learner = FMLearner(fm_model, adam_opt, schedular, db)\n",
    "fm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T12:56:33.381255Z",
     "start_time": "2019-09-25T12:41:06.847584Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 37675.703864504314\n",
      "Epoch 0 step 1: training accuarcy: 0.325\n",
      "Epoch 0 step 1: training loss: 36599.736475448546\n",
      "Epoch 0 step 2: training accuarcy: 0.34750000000000003\n",
      "Epoch 0 step 2: training loss: 35615.31982925032\n",
      "Epoch 0 step 3: training accuarcy: 0.35100000000000003\n",
      "Epoch 0 step 3: training loss: 34615.4817054277\n",
      "Epoch 0 step 4: training accuarcy: 0.379\n",
      "Epoch 0 step 4: training loss: 33644.801296626974\n",
      "Epoch 0 step 5: training accuarcy: 0.395\n",
      "Epoch 0 step 5: training loss: 32720.449225403172\n",
      "Epoch 0 step 6: training accuarcy: 0.43\n",
      "Epoch 0 step 6: training loss: 31828.289722670477\n",
      "Epoch 0 step 7: training accuarcy: 0.41200000000000003\n",
      "Epoch 0 step 7: training loss: 30897.08321612749\n",
      "Epoch 0 step 8: training accuarcy: 0.47200000000000003\n",
      "Epoch 0 step 8: training loss: 30067.42791356266\n",
      "Epoch 0 step 9: training accuarcy: 0.47800000000000004\n",
      "Epoch 0 step 9: training loss: 29184.55817753484\n",
      "Epoch 0 step 10: training accuarcy: 0.537\n",
      "Epoch 0 step 10: training loss: 28402.863171674813\n",
      "Epoch 0 step 11: training accuarcy: 0.527\n",
      "Epoch 0 step 11: training loss: 27587.65938989829\n",
      "Epoch 0 step 12: training accuarcy: 0.5495\n",
      "Epoch 0 step 12: training loss: 26796.544569891023\n",
      "Epoch 0 step 13: training accuarcy: 0.5825\n",
      "Epoch 0 step 13: training loss: 26018.087672154397\n",
      "Epoch 0 step 14: training accuarcy: 0.6085\n",
      "Epoch 0 step 14: training loss: 25249.045988997277\n",
      "Epoch 0 step 15: training accuarcy: 0.666\n",
      "Epoch 0 step 15: training loss: 24542.218868633972\n",
      "Epoch 0 step 16: training accuarcy: 0.667\n",
      "Epoch 0 step 16: training loss: 23829.11875836119\n",
      "Epoch 0 step 17: training accuarcy: 0.671\n",
      "Epoch 0 step 17: training loss: 23176.187339620355\n",
      "Epoch 0 step 18: training accuarcy: 0.6795\n",
      "Epoch 0 step 18: training loss: 22479.443543104095\n",
      "Epoch 0 step 19: training accuarcy: 0.7055\n",
      "Epoch 0 step 19: training loss: 21815.253748958567\n",
      "Epoch 0 step 20: training accuarcy: 0.731\n",
      "Epoch 0 step 20: training loss: 21189.01976252261\n",
      "Epoch 0 step 21: training accuarcy: 0.7205\n",
      "Epoch 0 step 21: training loss: 20600.969749364154\n",
      "Epoch 0 step 22: training accuarcy: 0.7285\n",
      "Epoch 0 step 22: training loss: 19951.029104973688\n",
      "Epoch 0 step 23: training accuarcy: 0.774\n",
      "Epoch 0 step 23: training loss: 19346.98123727648\n",
      "Epoch 0 step 24: training accuarcy: 0.787\n",
      "Epoch 0 step 24: training loss: 18774.236455176397\n",
      "Epoch 0 step 25: training accuarcy: 0.7885\n",
      "Epoch 0 step 25: training loss: 18270.49420532085\n",
      "Epoch 0 step 26: training accuarcy: 0.7765\n",
      "Epoch 0 step 26: training loss: 17674.3437224983\n",
      "Epoch 0 step 27: training accuarcy: 0.805\n",
      "Epoch 0 step 27: training loss: 17162.433094315402\n",
      "Epoch 0 step 28: training accuarcy: 0.8170000000000001\n",
      "Epoch 0 step 28: training loss: 16643.577735154762\n",
      "Epoch 0 step 29: training accuarcy: 0.8210000000000001\n",
      "Epoch 0 step 29: training loss: 16146.386858412536\n",
      "Epoch 0 step 30: training accuarcy: 0.8285\n",
      "Epoch 0 step 30: training loss: 15672.803357054445\n",
      "Epoch 0 step 31: training accuarcy: 0.8280000000000001\n",
      "Epoch 0 step 31: training loss: 15203.06275965938\n",
      "Epoch 0 step 32: training accuarcy: 0.8240000000000001\n",
      "Epoch 0 step 32: training loss: 14734.830853054471\n",
      "Epoch 0 step 33: training accuarcy: 0.8365\n",
      "Epoch 0 step 33: training loss: 14275.167105804603\n",
      "Epoch 0 step 34: training accuarcy: 0.855\n",
      "Epoch 0 step 34: training loss: 13843.153888321041\n",
      "Epoch 0 step 35: training accuarcy: 0.8565\n",
      "Epoch 0 step 35: training loss: 13410.806902147386\n",
      "Epoch 0 step 36: training accuarcy: 0.874\n",
      "Epoch 0 step 36: training loss: 13006.918462171181\n",
      "Epoch 0 step 37: training accuarcy: 0.87\n",
      "Epoch 0 step 37: training loss: 12615.887842107744\n",
      "Epoch 0 step 38: training accuarcy: 0.867\n",
      "Epoch 0 step 38: training loss: 12223.895285710143\n",
      "Epoch 0 step 39: training accuarcy: 0.8775000000000001\n",
      "Epoch 0 step 39: training loss: 11841.889855460518\n",
      "Epoch 0 step 40: training accuarcy: 0.881\n",
      "Epoch 0 step 40: training loss: 11474.44514932592\n",
      "Epoch 0 step 41: training accuarcy: 0.884\n",
      "Epoch 0 step 41: training loss: 11113.81897144022\n",
      "Epoch 0 step 42: training accuarcy: 0.8905000000000001\n",
      "Epoch 0 step 42: training loss: 10780.689342247553\n",
      "Epoch 0 step 43: training accuarcy: 0.889\n",
      "Epoch 0 step 43: training loss: 10430.12120778025\n",
      "Epoch 0 step 44: training accuarcy: 0.8975\n",
      "Epoch 0 step 44: training loss: 10110.562951218471\n",
      "Epoch 0 step 45: training accuarcy: 0.896\n",
      "Epoch 0 step 45: training loss: 9803.711345980026\n",
      "Epoch 0 step 46: training accuarcy: 0.8885000000000001\n",
      "Epoch 0 step 46: training loss: 9489.283866363656\n",
      "Epoch 0 step 47: training accuarcy: 0.8985\n",
      "Epoch 0 step 47: training loss: 9184.444916468376\n",
      "Epoch 0 step 48: training accuarcy: 0.909\n",
      "Epoch 0 step 48: training loss: 8873.582933518532\n",
      "Epoch 0 step 49: training accuarcy: 0.919\n",
      "Epoch 0 step 49: training loss: 8637.231433962272\n",
      "Epoch 0 step 50: training accuarcy: 0.9025\n",
      "Epoch 0 step 50: training loss: 8324.267116697793\n",
      "Epoch 0 step 51: training accuarcy: 0.922\n",
      "Epoch 0 step 51: training loss: 8069.916103582809\n",
      "Epoch 0 step 52: training accuarcy: 0.92\n",
      "Epoch 0 step 52: training loss: 7819.626204402693\n",
      "Epoch 0 step 53: training accuarcy: 0.918\n",
      "Epoch 0 step 53: training loss: 7566.898562322524\n",
      "Epoch 0 step 54: training accuarcy: 0.9265\n",
      "Epoch 0 step 54: training loss: 7327.914871699613\n",
      "Epoch 0 step 55: training accuarcy: 0.926\n",
      "Epoch 0 step 55: training loss: 7100.649326217934\n",
      "Epoch 0 step 56: training accuarcy: 0.924\n",
      "Epoch 0 step 56: training loss: 6862.132729874674\n",
      "Epoch 0 step 57: training accuarcy: 0.9265\n",
      "Epoch 0 step 57: training loss: 6627.4914598035375\n",
      "Epoch 0 step 58: training accuarcy: 0.9400000000000001\n",
      "Epoch 0 step 58: training loss: 6451.936297776098\n",
      "Epoch 0 step 59: training accuarcy: 0.924\n",
      "Epoch 0 step 59: training loss: 6202.688192108857\n",
      "Epoch 0 step 60: training accuarcy: 0.9365\n",
      "Epoch 0 step 60: training loss: 6026.339319962665\n",
      "Epoch 0 step 61: training accuarcy: 0.9325\n",
      "Epoch 0 step 61: training loss: 5830.382098017379\n",
      "Epoch 0 step 62: training accuarcy: 0.9430000000000001\n",
      "Epoch 0 step 62: training loss: 5644.986903465495\n",
      "Epoch 0 step 63: training accuarcy: 0.9315\n",
      "Epoch 0 step 63: training loss: 5455.497989251372\n",
      "Epoch 0 step 64: training accuarcy: 0.9420000000000001\n",
      "Epoch 0 step 64: training loss: 5277.663052681811\n",
      "Epoch 0 step 65: training accuarcy: 0.9465\n",
      "Epoch 0 step 65: training loss: 5107.8208392132565\n",
      "Epoch 0 step 66: training accuarcy: 0.9450000000000001\n",
      "Epoch 0 step 66: training loss: 4933.474652264886\n",
      "Epoch 0 step 67: training accuarcy: 0.9470000000000001\n",
      "Epoch 0 step 67: training loss: 4782.83833898567\n",
      "Epoch 0 step 68: training accuarcy: 0.9430000000000001\n",
      "Epoch 0 step 68: training loss: 4637.047932739457\n",
      "Epoch 0 step 69: training accuarcy: 0.936\n",
      "Epoch 0 step 69: training loss: 4481.53248400834\n",
      "Epoch 0 step 70: training accuarcy: 0.9415\n",
      "Epoch 0 step 70: training loss: 4339.600679462951\n",
      "Epoch 0 step 71: training accuarcy: 0.9470000000000001\n",
      "Epoch 0 step 71: training loss: 4196.225454039379\n",
      "Epoch 0 step 72: training accuarcy: 0.9520000000000001\n",
      "Epoch 0 step 72: training loss: 4051.311195373192\n",
      "Epoch 0 step 73: training accuarcy: 0.9515\n",
      "Epoch 0 step 73: training loss: 3921.8615441024617\n",
      "Epoch 0 step 74: training accuarcy: 0.9490000000000001\n",
      "Epoch 0 step 74: training loss: 3794.8373472527924\n",
      "Epoch 0 step 75: training accuarcy: 0.9585\n",
      "Epoch 0 step 75: training loss: 3662.432588126274\n",
      "Epoch 0 step 76: training accuarcy: 0.961\n",
      "Epoch 0 step 76: training loss: 3561.3677262763094\n",
      "Epoch 0 step 77: training accuarcy: 0.9505\n",
      "Epoch 0 step 77: training loss: 3451.884954136055\n",
      "Epoch 0 step 78: training accuarcy: 0.9505\n",
      "Epoch 0 step 78: training loss: 3316.609405448229\n",
      "Epoch 0 step 79: training accuarcy: 0.9590000000000001\n",
      "Epoch 0 step 79: training loss: 3217.781976181056\n",
      "Epoch 0 step 80: training accuarcy: 0.9645\n",
      "Epoch 0 step 80: training loss: 3118.7490471604474\n",
      "Epoch 0 step 81: training accuarcy: 0.964\n",
      "Epoch 0 step 81: training loss: 3009.675274112687\n",
      "Epoch 0 step 82: training accuarcy: 0.9645\n",
      "Epoch 0 step 82: training loss: 2921.863942740029\n",
      "Epoch 0 step 83: training accuarcy: 0.9590000000000001\n",
      "Epoch 0 step 83: training loss: 2823.894699235148\n",
      "Epoch 0 step 84: training accuarcy: 0.97\n",
      "Epoch 0 step 84: training loss: 2733.0657051233707\n",
      "Epoch 0 step 85: training accuarcy: 0.962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 85: training loss: 2653.4429234977392\n",
      "Epoch 0 step 86: training accuarcy: 0.9635\n",
      "Epoch 0 step 86: training loss: 2550.409468341607\n",
      "Epoch 0 step 87: training accuarcy: 0.9715\n",
      "Epoch 0 step 87: training loss: 2469.2917007705064\n",
      "Epoch 0 step 88: training accuarcy: 0.9665\n",
      "Epoch 0 step 88: training loss: 2414.710345401264\n",
      "Epoch 0 step 89: training accuarcy: 0.9645\n",
      "Epoch 0 step 89: training loss: 2333.4486915904654\n",
      "Epoch 0 step 90: training accuarcy: 0.9635\n",
      "Epoch 0 step 90: training loss: 2267.7874215121824\n",
      "Epoch 0 step 91: training accuarcy: 0.961\n",
      "Epoch 0 step 91: training loss: 2183.3832112490873\n",
      "Epoch 0 step 92: training accuarcy: 0.9685\n",
      "Epoch 0 step 92: training loss: 2116.704745038552\n",
      "Epoch 0 step 93: training accuarcy: 0.9715\n",
      "Epoch 0 step 93: training loss: 2057.723475437113\n",
      "Epoch 0 step 94: training accuarcy: 0.9715\n",
      "Epoch 0 step 94: training loss: 1984.0326964775147\n",
      "Epoch 0 step 95: training accuarcy: 0.9725\n",
      "Epoch 0 step 95: training loss: 1936.757999925666\n",
      "Epoch 0 step 96: training accuarcy: 0.975\n",
      "Epoch 0 step 96: training loss: 1874.9597966857216\n",
      "Epoch 0 step 97: training accuarcy: 0.9695\n",
      "Epoch 0 step 97: training loss: 1802.106597574324\n",
      "Epoch 0 step 98: training accuarcy: 0.9815\n",
      "Epoch 0 step 98: training loss: 1769.972577910045\n",
      "Epoch 0 step 99: training accuarcy: 0.97\n",
      "Epoch 0 step 99: training loss: 1709.0010801996534\n",
      "Epoch 0 step 100: training accuarcy: 0.975\n",
      "Epoch 0 step 100: training loss: 1652.705955935413\n",
      "Epoch 0 step 101: training accuarcy: 0.9725\n",
      "Epoch 0 step 101: training loss: 1605.2913919268829\n",
      "Epoch 0 step 102: training accuarcy: 0.974\n",
      "Epoch 0 step 102: training loss: 1557.3114777657281\n",
      "Epoch 0 step 103: training accuarcy: 0.981\n",
      "Epoch 0 step 103: training loss: 1512.4025014877957\n",
      "Epoch 0 step 104: training accuarcy: 0.9755\n",
      "Epoch 0 step 104: training loss: 1489.1441778077888\n",
      "Epoch 0 step 105: training accuarcy: 0.9715\n",
      "Epoch 0 step 105: training loss: 1428.561932116973\n",
      "Epoch 0 step 106: training accuarcy: 0.9755\n",
      "Epoch 0 step 106: training loss: 1381.7084079438384\n",
      "Epoch 0 step 107: training accuarcy: 0.9795\n",
      "Epoch 0 step 107: training loss: 1341.6335816336568\n",
      "Epoch 0 step 108: training accuarcy: 0.9795\n",
      "Epoch 0 step 108: training loss: 1305.2491443024537\n",
      "Epoch 0 step 109: training accuarcy: 0.9845\n",
      "Epoch 0 step 109: training loss: 1260.5374334174815\n",
      "Epoch 0 step 110: training accuarcy: 0.98\n",
      "Epoch 0 step 110: training loss: 1230.3903415902255\n",
      "Epoch 0 step 111: training accuarcy: 0.988\n",
      "Epoch 0 step 111: training loss: 1207.421595506511\n",
      "Epoch 0 step 112: training accuarcy: 0.9805\n",
      "Epoch 0 step 112: training loss: 1157.5344879751658\n",
      "Epoch 0 step 113: training accuarcy: 0.983\n",
      "Epoch 0 step 113: training loss: 1133.607267104255\n",
      "Epoch 0 step 114: training accuarcy: 0.9815\n",
      "Epoch 0 step 114: training loss: 1106.965135882447\n",
      "Epoch 0 step 115: training accuarcy: 0.9795\n",
      "Epoch 0 step 115: training loss: 1071.1877612501207\n",
      "Epoch 0 step 116: training accuarcy: 0.9805\n",
      "Epoch 0 step 116: training loss: 1046.1382498480802\n",
      "Epoch 0 step 117: training accuarcy: 0.983\n",
      "Epoch 0 step 117: training loss: 1028.2596500551044\n",
      "Epoch 0 step 118: training accuarcy: 0.9825\n",
      "Epoch 0 step 118: training loss: 1011.20163925376\n",
      "Epoch 0 step 119: training accuarcy: 0.9855\n",
      "Epoch 0 step 119: training loss: 964.7844012374819\n",
      "Epoch 0 step 120: training accuarcy: 0.9875\n",
      "Epoch 0 step 120: training loss: 958.2758974969288\n",
      "Epoch 0 step 121: training accuarcy: 0.9875\n",
      "Epoch 0 step 121: training loss: 948.6955001500719\n",
      "Epoch 0 step 122: training accuarcy: 0.9805\n",
      "Epoch 0 step 122: training loss: 904.0946054713138\n",
      "Epoch 0 step 123: training accuarcy: 0.985\n",
      "Epoch 0 step 123: training loss: 880.1201868575879\n",
      "Epoch 0 step 124: training accuarcy: 0.9855\n",
      "Epoch 0 step 124: training loss: 864.9956992581338\n",
      "Epoch 0 step 125: training accuarcy: 0.9865\n",
      "Epoch 0 step 125: training loss: 836.0536699244512\n",
      "Epoch 0 step 126: training accuarcy: 0.983\n",
      "Epoch 0 step 126: training loss: 823.3652970460269\n",
      "Epoch 0 step 127: training accuarcy: 0.988\n",
      "Epoch 0 step 127: training loss: 812.2832050870057\n",
      "Epoch 0 step 128: training accuarcy: 0.987\n",
      "Epoch 0 step 128: training loss: 788.065698999595\n",
      "Epoch 0 step 129: training accuarcy: 0.981\n",
      "Epoch 0 step 129: training loss: 768.7098300947982\n",
      "Epoch 0 step 130: training accuarcy: 0.987\n",
      "Epoch 0 step 130: training loss: 755.807467543262\n",
      "Epoch 0 step 131: training accuarcy: 0.9885\n",
      "Epoch 0 step 131: training loss: 739.8486949945399\n",
      "Epoch 0 step 132: training accuarcy: 0.99\n",
      "Epoch 0 step 132: training loss: 732.3028056985571\n",
      "Epoch 0 step 133: training accuarcy: 0.986\n",
      "Epoch 0 step 133: training loss: 706.1485654973832\n",
      "Epoch 0 step 134: training accuarcy: 0.99\n",
      "Epoch 0 step 134: training loss: 686.2907932505013\n",
      "Epoch 0 step 135: training accuarcy: 0.9885\n",
      "Epoch 0 step 135: training loss: 684.3000064189571\n",
      "Epoch 0 step 136: training accuarcy: 0.987\n",
      "Epoch 0 step 136: training loss: 658.1373675043029\n",
      "Epoch 0 step 137: training accuarcy: 0.989\n",
      "Epoch 0 step 137: training loss: 633.3370108788656\n",
      "Epoch 0 step 138: training accuarcy: 0.9925\n",
      "Epoch 0 step 138: training loss: 638.2848877104648\n",
      "Epoch 0 step 139: training accuarcy: 0.9895\n",
      "Epoch 0 step 139: training loss: 624.9085635307657\n",
      "Epoch 0 step 140: training accuarcy: 0.9865\n",
      "Epoch 0 step 140: training loss: 621.1841456029524\n",
      "Epoch 0 step 141: training accuarcy: 0.9885\n",
      "Epoch 0 step 141: training loss: 613.2323118498135\n",
      "Epoch 0 step 142: training accuarcy: 0.9915\n",
      "Epoch 0 step 142: training loss: 601.3632992124435\n",
      "Epoch 0 step 143: training accuarcy: 0.99\n",
      "Epoch 0 step 143: training loss: 588.733973525819\n",
      "Epoch 0 step 144: training accuarcy: 0.9905\n",
      "Epoch 0 step 144: training loss: 571.7390842505285\n",
      "Epoch 0 step 145: training accuarcy: 0.987\n",
      "Epoch 0 step 145: training loss: 571.6207906779598\n",
      "Epoch 0 step 146: training accuarcy: 0.9875\n",
      "Epoch 0 step 146: training loss: 560.1214851821599\n",
      "Epoch 0 step 147: training accuarcy: 0.991\n",
      "Epoch 0 step 147: training loss: 556.7691266264392\n",
      "Epoch 0 step 148: training accuarcy: 0.984\n",
      "Epoch 0 step 148: training loss: 538.0814945981411\n",
      "Epoch 0 step 149: training accuarcy: 0.9905\n",
      "Epoch 0 step 149: training loss: 528.798626106147\n",
      "Epoch 0 step 150: training accuarcy: 0.989\n",
      "Epoch 0 step 150: training loss: 523.3246479331708\n",
      "Epoch 0 step 151: training accuarcy: 0.9915\n",
      "Epoch 0 step 151: training loss: 527.4578593413304\n",
      "Epoch 0 step 152: training accuarcy: 0.9935\n",
      "Epoch 0 step 152: training loss: 512.49981406232\n",
      "Epoch 0 step 153: training accuarcy: 0.9895\n",
      "Epoch 0 step 153: training loss: 515.1804106428508\n",
      "Epoch 0 step 154: training accuarcy: 0.9885\n",
      "Epoch 0 step 154: training loss: 502.56032587337666\n",
      "Epoch 0 step 155: training accuarcy: 0.9915\n",
      "Epoch 0 step 155: training loss: 493.19947118800553\n",
      "Epoch 0 step 156: training accuarcy: 0.9905\n",
      "Epoch 0 step 156: training loss: 489.13041223794403\n",
      "Epoch 0 step 157: training accuarcy: 0.9925\n",
      "Epoch 0 step 157: training loss: 489.00724148792\n",
      "Epoch 0 step 158: training accuarcy: 0.9865\n",
      "Epoch 0 step 158: training loss: 478.8679537961839\n",
      "Epoch 0 step 159: training accuarcy: 0.995\n",
      "Epoch 0 step 159: training loss: 473.5770505871375\n",
      "Epoch 0 step 160: training accuarcy: 0.9935\n",
      "Epoch 0 step 160: training loss: 476.3296086465237\n",
      "Epoch 0 step 161: training accuarcy: 0.9935\n",
      "Epoch 0 step 161: training loss: 470.1376437924289\n",
      "Epoch 0 step 162: training accuarcy: 0.992\n",
      "Epoch 0 step 162: training loss: 460.7957714251847\n",
      "Epoch 0 step 163: training accuarcy: 0.9915\n",
      "Epoch 0 step 163: training loss: 439.0257216512516\n",
      "Epoch 0 step 164: training accuarcy: 0.9925\n",
      "Epoch 0 step 164: training loss: 450.7956644108535\n",
      "Epoch 0 step 165: training accuarcy: 0.9915\n",
      "Epoch 0 step 165: training loss: 451.81822770229553\n",
      "Epoch 0 step 166: training accuarcy: 0.9925\n",
      "Epoch 0 step 166: training loss: 440.0686186105904\n",
      "Epoch 0 step 167: training accuarcy: 0.992\n",
      "Epoch 0 step 167: training loss: 442.4884064905039\n",
      "Epoch 0 step 168: training accuarcy: 0.9945\n",
      "Epoch 0 step 168: training loss: 443.1903536146309\n",
      "Epoch 0 step 169: training accuarcy: 0.9935\n",
      "Epoch 0 step 169: training loss: 420.75807103578086\n",
      "Epoch 0 step 170: training accuarcy: 0.9955\n",
      "Epoch 0 step 170: training loss: 427.8305652928816\n",
      "Epoch 0 step 171: training accuarcy: 0.9925\n",
      "Epoch 0 step 171: training loss: 419.0517177672075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 172: training accuarcy: 0.9945\n",
      "Epoch 0 step 172: training loss: 425.0257884882394\n",
      "Epoch 0 step 173: training accuarcy: 0.9945\n",
      "Epoch 0 step 173: training loss: 421.4626819664196\n",
      "Epoch 0 step 174: training accuarcy: 0.993\n",
      "Epoch 0 step 174: training loss: 414.43250600558804\n",
      "Epoch 0 step 175: training accuarcy: 0.9935\n",
      "Epoch 0 step 175: training loss: 409.8280806638817\n",
      "Epoch 0 step 176: training accuarcy: 0.9945\n",
      "Epoch 0 step 176: training loss: 407.12696780746546\n",
      "Epoch 0 step 177: training accuarcy: 0.9915\n",
      "Epoch 0 step 177: training loss: 399.084360623394\n",
      "Epoch 0 step 178: training accuarcy: 0.997\n",
      "Epoch 0 step 178: training loss: 394.00021115296954\n",
      "Epoch 0 step 179: training accuarcy: 0.9945\n",
      "Epoch 0 step 179: training loss: 403.04572063116314\n",
      "Epoch 0 step 180: training accuarcy: 0.9965\n",
      "Epoch 0 step 180: training loss: 398.4237869793317\n",
      "Epoch 0 step 181: training accuarcy: 0.993\n",
      "Epoch 0 step 181: training loss: 396.7086957094257\n",
      "Epoch 0 step 182: training accuarcy: 0.993\n",
      "Epoch 0 step 182: training loss: 389.31391084773384\n",
      "Epoch 0 step 183: training accuarcy: 0.993\n",
      "Epoch 0 step 183: training loss: 383.54150190522853\n",
      "Epoch 0 step 184: training accuarcy: 0.995\n",
      "Epoch 0 step 184: training loss: 383.17987146911105\n",
      "Epoch 0 step 185: training accuarcy: 0.991\n",
      "Epoch 0 step 185: training loss: 386.19413991034014\n",
      "Epoch 0 step 186: training accuarcy: 0.994\n",
      "Epoch 0 step 186: training loss: 374.8048216574948\n",
      "Epoch 0 step 187: training accuarcy: 0.9955\n",
      "Epoch 0 step 187: training loss: 386.1532888266203\n",
      "Epoch 0 step 188: training accuarcy: 0.9935\n",
      "Epoch 0 step 188: training loss: 374.7424573209728\n",
      "Epoch 0 step 189: training accuarcy: 0.9955\n",
      "Epoch 0 step 189: training loss: 383.469880431775\n",
      "Epoch 0 step 190: training accuarcy: 0.996\n",
      "Epoch 0 step 190: training loss: 383.5380806536708\n",
      "Epoch 0 step 191: training accuarcy: 0.9945\n",
      "Epoch 0 step 191: training loss: 393.1390630621038\n",
      "Epoch 0 step 192: training accuarcy: 0.9955\n",
      "Epoch 0 step 192: training loss: 370.6802549077964\n",
      "Epoch 0 step 193: training accuarcy: 0.997\n",
      "Epoch 0 step 193: training loss: 378.1312243134768\n",
      "Epoch 0 step 194: training accuarcy: 0.9955\n",
      "Epoch 0 step 194: training loss: 369.43100070398555\n",
      "Epoch 0 step 195: training accuarcy: 0.9945\n",
      "Epoch 0 step 195: training loss: 367.02507365229724\n",
      "Epoch 0 step 196: training accuarcy: 0.992\n",
      "Epoch 0 step 196: training loss: 368.12538878644716\n",
      "Epoch 0 step 197: training accuarcy: 0.9945\n",
      "Epoch 0 step 197: training loss: 359.0289656762759\n",
      "Epoch 0 step 198: training accuarcy: 0.996\n",
      "Epoch 0 step 198: training loss: 366.48300827024593\n",
      "Epoch 0 step 199: training accuarcy: 0.9965\n",
      "Epoch 0 step 199: training loss: 366.05063521371267\n",
      "Epoch 0 step 200: training accuarcy: 0.9945\n",
      "Epoch 0 step 200: training loss: 377.127547709684\n",
      "Epoch 0 step 201: training accuarcy: 0.995\n",
      "Epoch 0 step 201: training loss: 350.100534308692\n",
      "Epoch 0 step 202: training accuarcy: 0.9935\n",
      "Epoch 0 step 202: training loss: 361.67638248655504\n",
      "Epoch 0 step 203: training accuarcy: 0.995\n",
      "Epoch 0 step 203: training loss: 354.39364885784653\n",
      "Epoch 0 step 204: training accuarcy: 0.9945\n",
      "Epoch 0 step 204: training loss: 374.73791140440323\n",
      "Epoch 0 step 205: training accuarcy: 0.9925\n",
      "Epoch 0 step 205: training loss: 357.2133244782388\n",
      "Epoch 0 step 206: training accuarcy: 0.996\n",
      "Epoch 0 step 206: training loss: 358.63138514952095\n",
      "Epoch 0 step 207: training accuarcy: 0.9975\n",
      "Epoch 0 step 207: training loss: 354.45527499319485\n",
      "Epoch 0 step 208: training accuarcy: 0.994\n",
      "Epoch 0 step 208: training loss: 342.5490176237652\n",
      "Epoch 0 step 209: training accuarcy: 0.996\n",
      "Epoch 0 step 209: training loss: 348.6675675628132\n",
      "Epoch 0 step 210: training accuarcy: 0.9955\n",
      "Epoch 0 step 210: training loss: 354.1333517918728\n",
      "Epoch 0 step 211: training accuarcy: 0.9965\n",
      "Epoch 0 step 211: training loss: 358.3089547076722\n",
      "Epoch 0 step 212: training accuarcy: 0.9975\n",
      "Epoch 0 step 212: training loss: 346.2120491212412\n",
      "Epoch 0 step 213: training accuarcy: 0.9955\n",
      "Epoch 0 step 213: training loss: 348.61839632358055\n",
      "Epoch 0 step 214: training accuarcy: 0.9965\n",
      "Epoch 0 step 214: training loss: 339.8112494755216\n",
      "Epoch 0 step 215: training accuarcy: 0.9925\n",
      "Epoch 0 step 215: training loss: 351.6587707993141\n",
      "Epoch 0 step 216: training accuarcy: 0.9975\n",
      "Epoch 0 step 216: training loss: 347.7289518061594\n",
      "Epoch 0 step 217: training accuarcy: 0.994\n",
      "Epoch 0 step 217: training loss: 350.94677154389143\n",
      "Epoch 0 step 218: training accuarcy: 0.9945\n",
      "Epoch 0 step 218: training loss: 353.3460442887794\n",
      "Epoch 0 step 219: training accuarcy: 0.9945\n",
      "Epoch 0 step 219: training loss: 351.2288888270109\n",
      "Epoch 0 step 220: training accuarcy: 0.9975\n",
      "Epoch 0 step 220: training loss: 348.8117650114233\n",
      "Epoch 0 step 221: training accuarcy: 0.994\n",
      "Epoch 0 step 221: training loss: 345.9502054828799\n",
      "Epoch 0 step 222: training accuarcy: 0.9965\n",
      "Epoch 0 step 222: training loss: 339.38796929778607\n",
      "Epoch 0 step 223: training accuarcy: 0.997\n",
      "Epoch 0 step 223: training loss: 348.4996217914421\n",
      "Epoch 0 step 224: training accuarcy: 0.996\n",
      "Epoch 0 step 224: training loss: 339.96017022451076\n",
      "Epoch 0 step 225: training accuarcy: 0.9945\n",
      "Epoch 0 step 225: training loss: 338.7444690209111\n",
      "Epoch 0 step 226: training accuarcy: 0.995\n",
      "Epoch 0 step 226: training loss: 364.94943322723327\n",
      "Epoch 0 step 227: training accuarcy: 0.992\n",
      "Epoch 0 step 227: training loss: 327.88074212030244\n",
      "Epoch 0 step 228: training accuarcy: 0.9955\n",
      "Epoch 0 step 228: training loss: 328.7718769624237\n",
      "Epoch 0 step 229: training accuarcy: 0.9965\n",
      "Epoch 0 step 229: training loss: 339.52719727539784\n",
      "Epoch 0 step 230: training accuarcy: 0.9965\n",
      "Epoch 0 step 230: training loss: 325.4728614693997\n",
      "Epoch 0 step 231: training accuarcy: 0.9965\n",
      "Epoch 0 step 231: training loss: 338.89598465419954\n",
      "Epoch 0 step 232: training accuarcy: 0.9945\n",
      "Epoch 0 step 232: training loss: 339.084581472278\n",
      "Epoch 0 step 233: training accuarcy: 0.9975\n",
      "Epoch 0 step 233: training loss: 338.27982964958517\n",
      "Epoch 0 step 234: training accuarcy: 0.9965\n",
      "Epoch 0 step 234: training loss: 341.8413358921239\n",
      "Epoch 0 step 235: training accuarcy: 0.9945\n",
      "Epoch 0 step 235: training loss: 337.01831351198234\n",
      "Epoch 0 step 236: training accuarcy: 0.995\n",
      "Epoch 0 step 236: training loss: 334.4627689714082\n",
      "Epoch 0 step 237: training accuarcy: 0.997\n",
      "Epoch 0 step 237: training loss: 347.23340932701365\n",
      "Epoch 0 step 238: training accuarcy: 0.997\n",
      "Epoch 0 step 238: training loss: 336.9204491086222\n",
      "Epoch 0 step 239: training accuarcy: 0.9945\n",
      "Epoch 0 step 239: training loss: 341.95591418288797\n",
      "Epoch 0 step 240: training accuarcy: 0.9955\n",
      "Epoch 0 step 240: training loss: 340.07851348315137\n",
      "Epoch 0 step 241: training accuarcy: 0.9945\n",
      "Epoch 0 step 241: training loss: 333.8101833409452\n",
      "Epoch 0 step 242: training accuarcy: 0.997\n",
      "Epoch 0 step 242: training loss: 319.9669264815858\n",
      "Epoch 0 step 243: training accuarcy: 0.996\n",
      "Epoch 0 step 243: training loss: 334.71040823458134\n",
      "Epoch 0 step 244: training accuarcy: 0.996\n",
      "Epoch 0 step 244: training loss: 341.87262498510927\n",
      "Epoch 0 step 245: training accuarcy: 0.9975\n",
      "Epoch 0 step 245: training loss: 329.14904889928266\n",
      "Epoch 0 step 246: training accuarcy: 0.9975\n",
      "Epoch 0 step 246: training loss: 331.00898423881824\n",
      "Epoch 0 step 247: training accuarcy: 0.994\n",
      "Epoch 0 step 247: training loss: 352.81565713831554\n",
      "Epoch 0 step 248: training accuarcy: 0.993\n",
      "Epoch 0 step 248: training loss: 320.6611426116389\n",
      "Epoch 0 step 249: training accuarcy: 0.995\n",
      "Epoch 0 step 249: training loss: 334.4685900302777\n",
      "Epoch 0 step 250: training accuarcy: 0.993\n",
      "Epoch 0 step 250: training loss: 342.4056362807438\n",
      "Epoch 0 step 251: training accuarcy: 0.9975\n",
      "Epoch 0 step 251: training loss: 344.0635978953068\n",
      "Epoch 0 step 252: training accuarcy: 0.995\n",
      "Epoch 0 step 252: training loss: 332.062019081706\n",
      "Epoch 0 step 253: training accuarcy: 0.9945\n",
      "Epoch 0 step 253: training loss: 337.9039820004282\n",
      "Epoch 0 step 254: training accuarcy: 0.9945\n",
      "Epoch 0 step 254: training loss: 325.5362541231337\n",
      "Epoch 0 step 255: training accuarcy: 0.9955\n",
      "Epoch 0 step 255: training loss: 331.0768242933823\n",
      "Epoch 0 step 256: training accuarcy: 0.9945\n",
      "Epoch 0 step 256: training loss: 337.78622301632583\n",
      "Epoch 0 step 257: training accuarcy: 0.991\n",
      "Epoch 0 step 257: training loss: 335.3991209112995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 258: training accuarcy: 0.9965\n",
      "Epoch 0 step 258: training loss: 322.6649725226142\n",
      "Epoch 0 step 259: training accuarcy: 0.997\n",
      "Epoch 0 step 259: training loss: 317.7464473078878\n",
      "Epoch 0 step 260: training accuarcy: 0.999\n",
      "Epoch 0 step 260: training loss: 321.80251178393627\n",
      "Epoch 0 step 261: training accuarcy: 0.997\n",
      "Epoch 0 step 261: training loss: 326.00470199059305\n",
      "Epoch 0 step 262: training accuarcy: 0.995\n",
      "Epoch 0 step 262: training loss: 244.2342229596161\n",
      "Epoch 0 step 263: training accuarcy: 0.9948717948717949\n",
      "Epoch 0: train loss 4955.651903589199, train accuarcy 0.8019078373908997\n",
      "Epoch 0: valid loss 1073.815513813322, valid accuarcy 0.9551243185997009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████                                                                                                                                     | 1/8 [01:58<13:47, 118.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 309.1691628978947\n",
      "Epoch 1 step 264: training accuarcy: 0.998\n",
      "Epoch 1 step 264: training loss: 315.2735084747016\n",
      "Epoch 1 step 265: training accuarcy: 0.9975\n",
      "Epoch 1 step 265: training loss: 319.65269007600955\n",
      "Epoch 1 step 266: training accuarcy: 0.9975\n",
      "Epoch 1 step 266: training loss: 312.6047788514495\n",
      "Epoch 1 step 267: training accuarcy: 0.997\n",
      "Epoch 1 step 267: training loss: 328.70305530856047\n",
      "Epoch 1 step 268: training accuarcy: 0.998\n",
      "Epoch 1 step 268: training loss: 324.4337716981763\n",
      "Epoch 1 step 269: training accuarcy: 0.9955\n",
      "Epoch 1 step 269: training loss: 317.2617600904713\n",
      "Epoch 1 step 270: training accuarcy: 0.998\n",
      "Epoch 1 step 270: training loss: 309.15623288516286\n",
      "Epoch 1 step 271: training accuarcy: 0.996\n",
      "Epoch 1 step 271: training loss: 321.16579564425945\n",
      "Epoch 1 step 272: training accuarcy: 0.997\n",
      "Epoch 1 step 272: training loss: 313.3828973496984\n",
      "Epoch 1 step 273: training accuarcy: 0.9995\n",
      "Epoch 1 step 273: training loss: 305.61611317247184\n",
      "Epoch 1 step 274: training accuarcy: 0.996\n",
      "Epoch 1 step 274: training loss: 325.3370574299685\n",
      "Epoch 1 step 275: training accuarcy: 0.997\n",
      "Epoch 1 step 275: training loss: 319.23180477607866\n",
      "Epoch 1 step 276: training accuarcy: 0.9955\n",
      "Epoch 1 step 276: training loss: 322.43761377659985\n",
      "Epoch 1 step 277: training accuarcy: 0.996\n",
      "Epoch 1 step 277: training loss: 319.69486986228196\n",
      "Epoch 1 step 278: training accuarcy: 0.996\n",
      "Epoch 1 step 278: training loss: 314.24045042617354\n",
      "Epoch 1 step 279: training accuarcy: 0.997\n",
      "Epoch 1 step 279: training loss: 323.71685450297275\n",
      "Epoch 1 step 280: training accuarcy: 0.997\n",
      "Epoch 1 step 280: training loss: 317.5805323816529\n",
      "Epoch 1 step 281: training accuarcy: 0.997\n",
      "Epoch 1 step 281: training loss: 312.10722832691494\n",
      "Epoch 1 step 282: training accuarcy: 0.997\n",
      "Epoch 1 step 282: training loss: 313.68286854029293\n",
      "Epoch 1 step 283: training accuarcy: 0.9985\n",
      "Epoch 1 step 283: training loss: 305.38406768769835\n",
      "Epoch 1 step 284: training accuarcy: 0.9965\n",
      "Epoch 1 step 284: training loss: 312.4075732786182\n",
      "Epoch 1 step 285: training accuarcy: 0.9985\n",
      "Epoch 1 step 285: training loss: 327.0523365715097\n",
      "Epoch 1 step 286: training accuarcy: 0.9965\n",
      "Epoch 1 step 286: training loss: 326.1190791807231\n",
      "Epoch 1 step 287: training accuarcy: 0.995\n",
      "Epoch 1 step 287: training loss: 323.45347043917764\n",
      "Epoch 1 step 288: training accuarcy: 0.997\n",
      "Epoch 1 step 288: training loss: 318.5303262606365\n",
      "Epoch 1 step 289: training accuarcy: 0.9955\n",
      "Epoch 1 step 289: training loss: 328.3616607069202\n",
      "Epoch 1 step 290: training accuarcy: 0.997\n",
      "Epoch 1 step 290: training loss: 319.7744869799402\n",
      "Epoch 1 step 291: training accuarcy: 0.995\n",
      "Epoch 1 step 291: training loss: 325.1138409653865\n",
      "Epoch 1 step 292: training accuarcy: 0.997\n",
      "Epoch 1 step 292: training loss: 319.3168793149572\n",
      "Epoch 1 step 293: training accuarcy: 0.9965\n",
      "Epoch 1 step 293: training loss: 309.5521763755803\n",
      "Epoch 1 step 294: training accuarcy: 0.9975\n",
      "Epoch 1 step 294: training loss: 310.662648792456\n",
      "Epoch 1 step 295: training accuarcy: 0.9975\n",
      "Epoch 1 step 295: training loss: 304.33989148232274\n",
      "Epoch 1 step 296: training accuarcy: 0.9985\n",
      "Epoch 1 step 296: training loss: 307.00030830971485\n",
      "Epoch 1 step 297: training accuarcy: 0.9975\n",
      "Epoch 1 step 297: training loss: 316.1414789595814\n",
      "Epoch 1 step 298: training accuarcy: 0.9965\n",
      "Epoch 1 step 298: training loss: 314.781053768024\n",
      "Epoch 1 step 299: training accuarcy: 0.9955\n",
      "Epoch 1 step 299: training loss: 312.3118622704235\n",
      "Epoch 1 step 300: training accuarcy: 0.9955\n",
      "Epoch 1 step 300: training loss: 331.8623554403056\n",
      "Epoch 1 step 301: training accuarcy: 0.997\n",
      "Epoch 1 step 301: training loss: 318.730144658791\n",
      "Epoch 1 step 302: training accuarcy: 0.9975\n",
      "Epoch 1 step 302: training loss: 325.6813813546411\n",
      "Epoch 1 step 303: training accuarcy: 0.9955\n",
      "Epoch 1 step 303: training loss: 311.04656941300294\n",
      "Epoch 1 step 304: training accuarcy: 0.9925\n",
      "Epoch 1 step 304: training loss: 321.7233979241521\n",
      "Epoch 1 step 305: training accuarcy: 0.9975\n",
      "Epoch 1 step 305: training loss: 305.7736545864235\n",
      "Epoch 1 step 306: training accuarcy: 0.9965\n",
      "Epoch 1 step 306: training loss: 316.77202804194565\n",
      "Epoch 1 step 307: training accuarcy: 0.997\n",
      "Epoch 1 step 307: training loss: 315.54364698341493\n",
      "Epoch 1 step 308: training accuarcy: 0.9965\n",
      "Epoch 1 step 308: training loss: 317.0611559010331\n",
      "Epoch 1 step 309: training accuarcy: 0.9955\n",
      "Epoch 1 step 309: training loss: 328.0287246006276\n",
      "Epoch 1 step 310: training accuarcy: 0.9965\n",
      "Epoch 1 step 310: training loss: 325.65504027111484\n",
      "Epoch 1 step 311: training accuarcy: 0.995\n",
      "Epoch 1 step 311: training loss: 315.3091281509118\n",
      "Epoch 1 step 312: training accuarcy: 0.9975\n",
      "Epoch 1 step 312: training loss: 322.0716988728002\n",
      "Epoch 1 step 313: training accuarcy: 0.9975\n",
      "Epoch 1 step 313: training loss: 319.5107057815034\n",
      "Epoch 1 step 314: training accuarcy: 0.9965\n",
      "Epoch 1 step 314: training loss: 335.31016627192633\n",
      "Epoch 1 step 315: training accuarcy: 0.9945\n",
      "Epoch 1 step 315: training loss: 329.2110255567974\n",
      "Epoch 1 step 316: training accuarcy: 0.9945\n",
      "Epoch 1 step 316: training loss: 313.12645949570305\n",
      "Epoch 1 step 317: training accuarcy: 0.997\n",
      "Epoch 1 step 317: training loss: 325.0446224988939\n",
      "Epoch 1 step 318: training accuarcy: 0.992\n",
      "Epoch 1 step 318: training loss: 323.8226770572569\n",
      "Epoch 1 step 319: training accuarcy: 0.996\n",
      "Epoch 1 step 319: training loss: 324.1507949828183\n",
      "Epoch 1 step 320: training accuarcy: 0.995\n",
      "Epoch 1 step 320: training loss: 318.31216690921156\n",
      "Epoch 1 step 321: training accuarcy: 0.997\n",
      "Epoch 1 step 321: training loss: 324.5116867363581\n",
      "Epoch 1 step 322: training accuarcy: 0.999\n",
      "Epoch 1 step 322: training loss: 317.0913823309987\n",
      "Epoch 1 step 323: training accuarcy: 0.997\n",
      "Epoch 1 step 323: training loss: 318.5195803628356\n",
      "Epoch 1 step 324: training accuarcy: 0.9935\n",
      "Epoch 1 step 324: training loss: 319.27768757348065\n",
      "Epoch 1 step 325: training accuarcy: 0.996\n",
      "Epoch 1 step 325: training loss: 327.7103989867263\n",
      "Epoch 1 step 326: training accuarcy: 0.9975\n",
      "Epoch 1 step 326: training loss: 322.61267268035556\n",
      "Epoch 1 step 327: training accuarcy: 0.9945\n",
      "Epoch 1 step 327: training loss: 323.87269048509506\n",
      "Epoch 1 step 328: training accuarcy: 0.995\n",
      "Epoch 1 step 328: training loss: 323.9629282846684\n",
      "Epoch 1 step 329: training accuarcy: 0.996\n",
      "Epoch 1 step 329: training loss: 312.10339836409287\n",
      "Epoch 1 step 330: training accuarcy: 0.9965\n",
      "Epoch 1 step 330: training loss: 309.91559137271133\n",
      "Epoch 1 step 331: training accuarcy: 0.998\n",
      "Epoch 1 step 331: training loss: 321.8160271991851\n",
      "Epoch 1 step 332: training accuarcy: 0.9945\n",
      "Epoch 1 step 332: training loss: 314.67880031737775\n",
      "Epoch 1 step 333: training accuarcy: 0.996\n",
      "Epoch 1 step 333: training loss: 326.50801308942397\n",
      "Epoch 1 step 334: training accuarcy: 0.9945\n",
      "Epoch 1 step 334: training loss: 312.0954517736979\n",
      "Epoch 1 step 335: training accuarcy: 0.994\n",
      "Epoch 1 step 335: training loss: 319.880740499249\n",
      "Epoch 1 step 336: training accuarcy: 0.9965\n",
      "Epoch 1 step 336: training loss: 311.7754113635102\n",
      "Epoch 1 step 337: training accuarcy: 0.9955\n",
      "Epoch 1 step 337: training loss: 327.5709449929009\n",
      "Epoch 1 step 338: training accuarcy: 0.999\n",
      "Epoch 1 step 338: training loss: 310.8654114670135\n",
      "Epoch 1 step 339: training accuarcy: 0.997\n",
      "Epoch 1 step 339: training loss: 307.0607152024994\n",
      "Epoch 1 step 340: training accuarcy: 0.9995\n",
      "Epoch 1 step 340: training loss: 310.49555103293727\n",
      "Epoch 1 step 341: training accuarcy: 0.996\n",
      "Epoch 1 step 341: training loss: 308.0436086115724\n",
      "Epoch 1 step 342: training accuarcy: 0.995\n",
      "Epoch 1 step 342: training loss: 316.52649494301664\n",
      "Epoch 1 step 343: training accuarcy: 0.9955\n",
      "Epoch 1 step 343: training loss: 313.2364512758661\n",
      "Epoch 1 step 344: training accuarcy: 0.998\n",
      "Epoch 1 step 344: training loss: 312.56585522143865\n",
      "Epoch 1 step 345: training accuarcy: 0.995\n",
      "Epoch 1 step 345: training loss: 322.3452126150429\n",
      "Epoch 1 step 346: training accuarcy: 0.9945\n",
      "Epoch 1 step 346: training loss: 325.2488199539064\n",
      "Epoch 1 step 347: training accuarcy: 0.994\n",
      "Epoch 1 step 347: training loss: 325.80514509708087\n",
      "Epoch 1 step 348: training accuarcy: 0.9955\n",
      "Epoch 1 step 348: training loss: 318.67749294197154\n",
      "Epoch 1 step 349: training accuarcy: 0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 349: training loss: 316.5082103115984\n",
      "Epoch 1 step 350: training accuarcy: 0.9995\n",
      "Epoch 1 step 350: training loss: 321.30688448752596\n",
      "Epoch 1 step 351: training accuarcy: 0.9925\n",
      "Epoch 1 step 351: training loss: 311.8097373960985\n",
      "Epoch 1 step 352: training accuarcy: 0.996\n",
      "Epoch 1 step 352: training loss: 318.33969803620175\n",
      "Epoch 1 step 353: training accuarcy: 0.995\n",
      "Epoch 1 step 353: training loss: 312.16812255355273\n",
      "Epoch 1 step 354: training accuarcy: 0.996\n",
      "Epoch 1 step 354: training loss: 318.0854629421592\n",
      "Epoch 1 step 355: training accuarcy: 0.996\n",
      "Epoch 1 step 355: training loss: 304.59511971761333\n",
      "Epoch 1 step 356: training accuarcy: 0.996\n",
      "Epoch 1 step 356: training loss: 310.72339122395226\n",
      "Epoch 1 step 357: training accuarcy: 0.999\n",
      "Epoch 1 step 357: training loss: 305.9250289022699\n",
      "Epoch 1 step 358: training accuarcy: 0.998\n",
      "Epoch 1 step 358: training loss: 320.8448196466692\n",
      "Epoch 1 step 359: training accuarcy: 0.9965\n",
      "Epoch 1 step 359: training loss: 309.1239679152412\n",
      "Epoch 1 step 360: training accuarcy: 0.9945\n",
      "Epoch 1 step 360: training loss: 318.78381261692493\n",
      "Epoch 1 step 361: training accuarcy: 0.9955\n",
      "Epoch 1 step 361: training loss: 318.1191380314965\n",
      "Epoch 1 step 362: training accuarcy: 0.9955\n",
      "Epoch 1 step 362: training loss: 312.2958252736316\n",
      "Epoch 1 step 363: training accuarcy: 0.9955\n",
      "Epoch 1 step 363: training loss: 315.45947320584037\n",
      "Epoch 1 step 364: training accuarcy: 0.998\n",
      "Epoch 1 step 364: training loss: 308.511634432232\n",
      "Epoch 1 step 365: training accuarcy: 0.998\n",
      "Epoch 1 step 365: training loss: 320.00042691290196\n",
      "Epoch 1 step 366: training accuarcy: 0.9975\n",
      "Epoch 1 step 366: training loss: 310.29401259836914\n",
      "Epoch 1 step 367: training accuarcy: 0.996\n",
      "Epoch 1 step 367: training loss: 314.6108685514273\n",
      "Epoch 1 step 368: training accuarcy: 0.9975\n",
      "Epoch 1 step 368: training loss: 314.9995593067051\n",
      "Epoch 1 step 369: training accuarcy: 0.998\n",
      "Epoch 1 step 369: training loss: 316.3590894378512\n",
      "Epoch 1 step 370: training accuarcy: 0.9965\n",
      "Epoch 1 step 370: training loss: 322.4454059781998\n",
      "Epoch 1 step 371: training accuarcy: 0.9965\n",
      "Epoch 1 step 371: training loss: 321.98493525997435\n",
      "Epoch 1 step 372: training accuarcy: 0.9965\n",
      "Epoch 1 step 372: training loss: 315.19407059820634\n",
      "Epoch 1 step 373: training accuarcy: 0.998\n",
      "Epoch 1 step 373: training loss: 314.53712146842395\n",
      "Epoch 1 step 374: training accuarcy: 0.9965\n",
      "Epoch 1 step 374: training loss: 329.3917665648012\n",
      "Epoch 1 step 375: training accuarcy: 0.9925\n",
      "Epoch 1 step 375: training loss: 320.2648507357924\n",
      "Epoch 1 step 376: training accuarcy: 0.9965\n",
      "Epoch 1 step 376: training loss: 318.0108886846873\n",
      "Epoch 1 step 377: training accuarcy: 0.996\n",
      "Epoch 1 step 377: training loss: 319.5582564101114\n",
      "Epoch 1 step 378: training accuarcy: 0.9965\n",
      "Epoch 1 step 378: training loss: 320.67498011898095\n",
      "Epoch 1 step 379: training accuarcy: 0.997\n",
      "Epoch 1 step 379: training loss: 326.2238215516595\n",
      "Epoch 1 step 380: training accuarcy: 0.9965\n",
      "Epoch 1 step 380: training loss: 317.1824235002006\n",
      "Epoch 1 step 381: training accuarcy: 0.9955\n",
      "Epoch 1 step 381: training loss: 326.1867654641754\n",
      "Epoch 1 step 382: training accuarcy: 0.9965\n",
      "Epoch 1 step 382: training loss: 325.11625523037253\n",
      "Epoch 1 step 383: training accuarcy: 0.9965\n",
      "Epoch 1 step 383: training loss: 313.79741717573046\n",
      "Epoch 1 step 384: training accuarcy: 0.9945\n",
      "Epoch 1 step 384: training loss: 324.63639622360904\n",
      "Epoch 1 step 385: training accuarcy: 0.9975\n",
      "Epoch 1 step 385: training loss: 308.4893157812678\n",
      "Epoch 1 step 386: training accuarcy: 0.9955\n",
      "Epoch 1 step 386: training loss: 320.73451584869326\n",
      "Epoch 1 step 387: training accuarcy: 0.994\n",
      "Epoch 1 step 387: training loss: 324.0106978286507\n",
      "Epoch 1 step 388: training accuarcy: 0.997\n",
      "Epoch 1 step 388: training loss: 314.01871476609256\n",
      "Epoch 1 step 389: training accuarcy: 0.9975\n",
      "Epoch 1 step 389: training loss: 317.6907280096069\n",
      "Epoch 1 step 390: training accuarcy: 0.9945\n",
      "Epoch 1 step 390: training loss: 322.2853569483117\n",
      "Epoch 1 step 391: training accuarcy: 0.997\n",
      "Epoch 1 step 391: training loss: 308.25328443040615\n",
      "Epoch 1 step 392: training accuarcy: 0.9965\n",
      "Epoch 1 step 392: training loss: 325.680261461913\n",
      "Epoch 1 step 393: training accuarcy: 0.9955\n",
      "Epoch 1 step 393: training loss: 324.11096657957614\n",
      "Epoch 1 step 394: training accuarcy: 0.9965\n",
      "Epoch 1 step 394: training loss: 327.81964087297837\n",
      "Epoch 1 step 395: training accuarcy: 0.9955\n",
      "Epoch 1 step 395: training loss: 320.4095793373638\n",
      "Epoch 1 step 396: training accuarcy: 0.998\n",
      "Epoch 1 step 396: training loss: 322.9558072853813\n",
      "Epoch 1 step 397: training accuarcy: 0.996\n",
      "Epoch 1 step 397: training loss: 310.4150112013423\n",
      "Epoch 1 step 398: training accuarcy: 0.9965\n",
      "Epoch 1 step 398: training loss: 318.2519937429151\n",
      "Epoch 1 step 399: training accuarcy: 0.994\n",
      "Epoch 1 step 399: training loss: 326.9766025193221\n",
      "Epoch 1 step 400: training accuarcy: 0.995\n",
      "Epoch 1 step 400: training loss: 318.9699207149529\n",
      "Epoch 1 step 401: training accuarcy: 0.9955\n",
      "Epoch 1 step 401: training loss: 321.83355904837936\n",
      "Epoch 1 step 402: training accuarcy: 0.996\n",
      "Epoch 1 step 402: training loss: 321.7711374213654\n",
      "Epoch 1 step 403: training accuarcy: 0.994\n",
      "Epoch 1 step 403: training loss: 314.86770950096644\n",
      "Epoch 1 step 404: training accuarcy: 0.9945\n",
      "Epoch 1 step 404: training loss: 307.5289772267883\n",
      "Epoch 1 step 405: training accuarcy: 0.998\n",
      "Epoch 1 step 405: training loss: 331.9132481443867\n",
      "Epoch 1 step 406: training accuarcy: 0.996\n",
      "Epoch 1 step 406: training loss: 323.04985710768943\n",
      "Epoch 1 step 407: training accuarcy: 0.9945\n",
      "Epoch 1 step 407: training loss: 313.9166066075528\n",
      "Epoch 1 step 408: training accuarcy: 0.994\n",
      "Epoch 1 step 408: training loss: 325.96887160216716\n",
      "Epoch 1 step 409: training accuarcy: 0.997\n",
      "Epoch 1 step 409: training loss: 325.12326669012384\n",
      "Epoch 1 step 410: training accuarcy: 0.994\n",
      "Epoch 1 step 410: training loss: 329.1363120925339\n",
      "Epoch 1 step 411: training accuarcy: 0.996\n",
      "Epoch 1 step 411: training loss: 323.57949935994793\n",
      "Epoch 1 step 412: training accuarcy: 0.9945\n",
      "Epoch 1 step 412: training loss: 326.14067609613653\n",
      "Epoch 1 step 413: training accuarcy: 0.9945\n",
      "Epoch 1 step 413: training loss: 300.1237638656819\n",
      "Epoch 1 step 414: training accuarcy: 0.9975\n",
      "Epoch 1 step 414: training loss: 339.27684013925017\n",
      "Epoch 1 step 415: training accuarcy: 0.9955\n",
      "Epoch 1 step 415: training loss: 315.46970620893404\n",
      "Epoch 1 step 416: training accuarcy: 0.996\n",
      "Epoch 1 step 416: training loss: 327.52654438013315\n",
      "Epoch 1 step 417: training accuarcy: 0.9975\n",
      "Epoch 1 step 417: training loss: 323.74165614051446\n",
      "Epoch 1 step 418: training accuarcy: 0.995\n",
      "Epoch 1 step 418: training loss: 318.76438508490537\n",
      "Epoch 1 step 419: training accuarcy: 0.9955\n",
      "Epoch 1 step 419: training loss: 324.50252362260323\n",
      "Epoch 1 step 420: training accuarcy: 0.9975\n",
      "Epoch 1 step 420: training loss: 313.87739930595114\n",
      "Epoch 1 step 421: training accuarcy: 0.9955\n",
      "Epoch 1 step 421: training loss: 308.58950181940406\n",
      "Epoch 1 step 422: training accuarcy: 0.998\n",
      "Epoch 1 step 422: training loss: 327.19253336939846\n",
      "Epoch 1 step 423: training accuarcy: 0.9975\n",
      "Epoch 1 step 423: training loss: 312.1082476332145\n",
      "Epoch 1 step 424: training accuarcy: 0.9955\n",
      "Epoch 1 step 424: training loss: 321.46568978465496\n",
      "Epoch 1 step 425: training accuarcy: 0.9965\n",
      "Epoch 1 step 425: training loss: 322.65426193861583\n",
      "Epoch 1 step 426: training accuarcy: 0.998\n",
      "Epoch 1 step 426: training loss: 327.5685672933239\n",
      "Epoch 1 step 427: training accuarcy: 0.995\n",
      "Epoch 1 step 427: training loss: 319.3922521249632\n",
      "Epoch 1 step 428: training accuarcy: 0.9965\n",
      "Epoch 1 step 428: training loss: 328.5109039654649\n",
      "Epoch 1 step 429: training accuarcy: 0.996\n",
      "Epoch 1 step 429: training loss: 308.74821976204345\n",
      "Epoch 1 step 430: training accuarcy: 0.9955\n",
      "Epoch 1 step 430: training loss: 315.02757728051813\n",
      "Epoch 1 step 431: training accuarcy: 0.9955\n",
      "Epoch 1 step 431: training loss: 309.396083719374\n",
      "Epoch 1 step 432: training accuarcy: 0.9975\n",
      "Epoch 1 step 432: training loss: 317.7795326728782\n",
      "Epoch 1 step 433: training accuarcy: 0.9955\n",
      "Epoch 1 step 433: training loss: 317.27225851183744\n",
      "Epoch 1 step 434: training accuarcy: 0.9985\n",
      "Epoch 1 step 434: training loss: 310.7043555887692\n",
      "Epoch 1 step 435: training accuarcy: 0.9965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 435: training loss: 324.48042684495215\n",
      "Epoch 1 step 436: training accuarcy: 0.9955\n",
      "Epoch 1 step 436: training loss: 304.17924590749885\n",
      "Epoch 1 step 437: training accuarcy: 0.996\n",
      "Epoch 1 step 437: training loss: 315.96906691799165\n",
      "Epoch 1 step 438: training accuarcy: 0.997\n",
      "Epoch 1 step 438: training loss: 316.2006755559568\n",
      "Epoch 1 step 439: training accuarcy: 0.996\n",
      "Epoch 1 step 439: training loss: 308.84882102258524\n",
      "Epoch 1 step 440: training accuarcy: 0.997\n",
      "Epoch 1 step 440: training loss: 321.23936312305887\n",
      "Epoch 1 step 441: training accuarcy: 0.9955\n",
      "Epoch 1 step 441: training loss: 313.2956504290179\n",
      "Epoch 1 step 442: training accuarcy: 0.9965\n",
      "Epoch 1 step 442: training loss: 309.08133367924427\n",
      "Epoch 1 step 443: training accuarcy: 0.9965\n",
      "Epoch 1 step 443: training loss: 321.3998276253459\n",
      "Epoch 1 step 444: training accuarcy: 0.996\n",
      "Epoch 1 step 444: training loss: 314.88458205243865\n",
      "Epoch 1 step 445: training accuarcy: 0.9965\n",
      "Epoch 1 step 445: training loss: 323.4429357160144\n",
      "Epoch 1 step 446: training accuarcy: 0.9975\n",
      "Epoch 1 step 446: training loss: 323.91647505644966\n",
      "Epoch 1 step 447: training accuarcy: 0.995\n",
      "Epoch 1 step 447: training loss: 317.41040495692124\n",
      "Epoch 1 step 448: training accuarcy: 0.998\n",
      "Epoch 1 step 448: training loss: 323.0239071695752\n",
      "Epoch 1 step 449: training accuarcy: 1.0\n",
      "Epoch 1 step 449: training loss: 323.23524938599576\n",
      "Epoch 1 step 450: training accuarcy: 0.9965\n",
      "Epoch 1 step 450: training loss: 318.0988079378816\n",
      "Epoch 1 step 451: training accuarcy: 0.9955\n",
      "Epoch 1 step 451: training loss: 305.6778634765772\n",
      "Epoch 1 step 452: training accuarcy: 0.995\n",
      "Epoch 1 step 452: training loss: 333.09774168676825\n",
      "Epoch 1 step 453: training accuarcy: 0.9945\n",
      "Epoch 1 step 453: training loss: 315.89099780955587\n",
      "Epoch 1 step 454: training accuarcy: 0.995\n",
      "Epoch 1 step 454: training loss: 315.71546981166455\n",
      "Epoch 1 step 455: training accuarcy: 0.9965\n",
      "Epoch 1 step 455: training loss: 330.5953016847395\n",
      "Epoch 1 step 456: training accuarcy: 0.9975\n",
      "Epoch 1 step 456: training loss: 313.19111344362375\n",
      "Epoch 1 step 457: training accuarcy: 0.995\n",
      "Epoch 1 step 457: training loss: 320.2013974501497\n",
      "Epoch 1 step 458: training accuarcy: 0.9935\n",
      "Epoch 1 step 458: training loss: 320.4031171290213\n",
      "Epoch 1 step 459: training accuarcy: 0.9945\n",
      "Epoch 1 step 459: training loss: 313.5481505665074\n",
      "Epoch 1 step 460: training accuarcy: 0.9965\n",
      "Epoch 1 step 460: training loss: 311.4176132626407\n",
      "Epoch 1 step 461: training accuarcy: 0.996\n",
      "Epoch 1 step 461: training loss: 327.0324015518454\n",
      "Epoch 1 step 462: training accuarcy: 0.9955\n",
      "Epoch 1 step 462: training loss: 316.5303061921395\n",
      "Epoch 1 step 463: training accuarcy: 0.9955\n",
      "Epoch 1 step 463: training loss: 301.278087970893\n",
      "Epoch 1 step 464: training accuarcy: 0.998\n",
      "Epoch 1 step 464: training loss: 310.8377753040396\n",
      "Epoch 1 step 465: training accuarcy: 0.997\n",
      "Epoch 1 step 465: training loss: 323.2457771223576\n",
      "Epoch 1 step 466: training accuarcy: 0.997\n",
      "Epoch 1 step 466: training loss: 318.4686961530714\n",
      "Epoch 1 step 467: training accuarcy: 0.996\n",
      "Epoch 1 step 467: training loss: 320.25648073677144\n",
      "Epoch 1 step 468: training accuarcy: 0.9965\n",
      "Epoch 1 step 468: training loss: 324.7873212783803\n",
      "Epoch 1 step 469: training accuarcy: 0.9985\n",
      "Epoch 1 step 469: training loss: 327.2155503294608\n",
      "Epoch 1 step 470: training accuarcy: 0.9965\n",
      "Epoch 1 step 470: training loss: 332.6998368302228\n",
      "Epoch 1 step 471: training accuarcy: 0.994\n",
      "Epoch 1 step 471: training loss: 330.96982850395784\n",
      "Epoch 1 step 472: training accuarcy: 0.995\n",
      "Epoch 1 step 472: training loss: 313.88033582030084\n",
      "Epoch 1 step 473: training accuarcy: 0.995\n",
      "Epoch 1 step 473: training loss: 307.9620307128441\n",
      "Epoch 1 step 474: training accuarcy: 0.9935\n",
      "Epoch 1 step 474: training loss: 319.22232514590576\n",
      "Epoch 1 step 475: training accuarcy: 0.9975\n",
      "Epoch 1 step 475: training loss: 317.46124885761003\n",
      "Epoch 1 step 476: training accuarcy: 0.9955\n",
      "Epoch 1 step 476: training loss: 321.61396160485026\n",
      "Epoch 1 step 477: training accuarcy: 0.9955\n",
      "Epoch 1 step 477: training loss: 334.4189704302824\n",
      "Epoch 1 step 478: training accuarcy: 0.994\n",
      "Epoch 1 step 478: training loss: 308.74349036148607\n",
      "Epoch 1 step 479: training accuarcy: 0.997\n",
      "Epoch 1 step 479: training loss: 318.288155873085\n",
      "Epoch 1 step 480: training accuarcy: 0.996\n",
      "Epoch 1 step 480: training loss: 309.34664156847265\n",
      "Epoch 1 step 481: training accuarcy: 0.996\n",
      "Epoch 1 step 481: training loss: 329.0613864547199\n",
      "Epoch 1 step 482: training accuarcy: 0.995\n",
      "Epoch 1 step 482: training loss: 321.2401084332331\n",
      "Epoch 1 step 483: training accuarcy: 0.9975\n",
      "Epoch 1 step 483: training loss: 325.13660958900346\n",
      "Epoch 1 step 484: training accuarcy: 0.998\n",
      "Epoch 1 step 484: training loss: 320.43061465405697\n",
      "Epoch 1 step 485: training accuarcy: 0.9955\n",
      "Epoch 1 step 485: training loss: 322.2088826482909\n",
      "Epoch 1 step 486: training accuarcy: 0.995\n",
      "Epoch 1 step 486: training loss: 322.0332533606009\n",
      "Epoch 1 step 487: training accuarcy: 0.996\n",
      "Epoch 1 step 487: training loss: 310.3790735026999\n",
      "Epoch 1 step 488: training accuarcy: 0.9975\n",
      "Epoch 1 step 488: training loss: 316.4775686729852\n",
      "Epoch 1 step 489: training accuarcy: 0.999\n",
      "Epoch 1 step 489: training loss: 318.6403822392173\n",
      "Epoch 1 step 490: training accuarcy: 0.9965\n",
      "Epoch 1 step 490: training loss: 323.15380385171477\n",
      "Epoch 1 step 491: training accuarcy: 0.995\n",
      "Epoch 1 step 491: training loss: 315.1588818771546\n",
      "Epoch 1 step 492: training accuarcy: 0.9945\n",
      "Epoch 1 step 492: training loss: 321.67584254257605\n",
      "Epoch 1 step 493: training accuarcy: 0.9965\n",
      "Epoch 1 step 493: training loss: 315.41464177256836\n",
      "Epoch 1 step 494: training accuarcy: 0.9965\n",
      "Epoch 1 step 494: training loss: 323.77884111814143\n",
      "Epoch 1 step 495: training accuarcy: 0.9975\n",
      "Epoch 1 step 495: training loss: 311.5773470851335\n",
      "Epoch 1 step 496: training accuarcy: 0.996\n",
      "Epoch 1 step 496: training loss: 312.19836110806204\n",
      "Epoch 1 step 497: training accuarcy: 0.995\n",
      "Epoch 1 step 497: training loss: 319.0362136571906\n",
      "Epoch 1 step 498: training accuarcy: 0.9955\n",
      "Epoch 1 step 498: training loss: 319.45462350111825\n",
      "Epoch 1 step 499: training accuarcy: 0.9965\n",
      "Epoch 1 step 499: training loss: 302.9367868270233\n",
      "Epoch 1 step 500: training accuarcy: 0.997\n",
      "Epoch 1 step 500: training loss: 318.54159583323633\n",
      "Epoch 1 step 501: training accuarcy: 0.996\n",
      "Epoch 1 step 501: training loss: 321.85497251131346\n",
      "Epoch 1 step 502: training accuarcy: 0.9965\n",
      "Epoch 1 step 502: training loss: 316.1230398509035\n",
      "Epoch 1 step 503: training accuarcy: 0.9965\n",
      "Epoch 1 step 503: training loss: 326.0472976985806\n",
      "Epoch 1 step 504: training accuarcy: 0.9935\n",
      "Epoch 1 step 504: training loss: 320.6727074472003\n",
      "Epoch 1 step 505: training accuarcy: 0.9955\n",
      "Epoch 1 step 505: training loss: 325.54584930047724\n",
      "Epoch 1 step 506: training accuarcy: 0.9925\n",
      "Epoch 1 step 506: training loss: 324.19519940976045\n",
      "Epoch 1 step 507: training accuarcy: 0.9935\n",
      "Epoch 1 step 507: training loss: 307.47963283045004\n",
      "Epoch 1 step 508: training accuarcy: 0.999\n",
      "Epoch 1 step 508: training loss: 332.3933230295744\n",
      "Epoch 1 step 509: training accuarcy: 0.9965\n",
      "Epoch 1 step 509: training loss: 319.9033379486215\n",
      "Epoch 1 step 510: training accuarcy: 0.9955\n",
      "Epoch 1 step 510: training loss: 328.52871189490395\n",
      "Epoch 1 step 511: training accuarcy: 0.995\n",
      "Epoch 1 step 511: training loss: 322.45729105754157\n",
      "Epoch 1 step 512: training accuarcy: 0.995\n",
      "Epoch 1 step 512: training loss: 321.98461390128807\n",
      "Epoch 1 step 513: training accuarcy: 0.995\n",
      "Epoch 1 step 513: training loss: 329.28857661556594\n",
      "Epoch 1 step 514: training accuarcy: 0.9975\n",
      "Epoch 1 step 514: training loss: 329.7028782720412\n",
      "Epoch 1 step 515: training accuarcy: 0.996\n",
      "Epoch 1 step 515: training loss: 323.4677641042729\n",
      "Epoch 1 step 516: training accuarcy: 0.9965\n",
      "Epoch 1 step 516: training loss: 325.8315676487058\n",
      "Epoch 1 step 517: training accuarcy: 0.9965\n",
      "Epoch 1 step 517: training loss: 328.4871305797375\n",
      "Epoch 1 step 518: training accuarcy: 0.995\n",
      "Epoch 1 step 518: training loss: 326.045865351878\n",
      "Epoch 1 step 519: training accuarcy: 0.9975\n",
      "Epoch 1 step 519: training loss: 330.5668887266088\n",
      "Epoch 1 step 520: training accuarcy: 0.9975\n",
      "Epoch 1 step 520: training loss: 315.8163868200118\n",
      "Epoch 1 step 521: training accuarcy: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 521: training loss: 313.53631303783516\n",
      "Epoch 1 step 522: training accuarcy: 0.996\n",
      "Epoch 1 step 522: training loss: 321.8350621190158\n",
      "Epoch 1 step 523: training accuarcy: 0.996\n",
      "Epoch 1 step 523: training loss: 309.98878794692473\n",
      "Epoch 1 step 524: training accuarcy: 0.9955\n",
      "Epoch 1 step 524: training loss: 330.7124737356661\n",
      "Epoch 1 step 525: training accuarcy: 0.9955\n",
      "Epoch 1 step 525: training loss: 230.6820001954395\n",
      "Epoch 1 step 526: training accuarcy: 0.9987179487179487\n",
      "Epoch 1: train loss 318.5069418671539, train accuarcy 0.9515917301177979\n",
      "Epoch 1: valid loss 1041.2307226321534, valid accuarcy 0.9625025391578674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████                                                                                                                  | 2/8 [03:59<11:54, 119.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 306.4784648039956\n",
      "Epoch 2 step 527: training accuarcy: 0.9975\n",
      "Epoch 2 step 527: training loss: 319.4942709638066\n",
      "Epoch 2 step 528: training accuarcy: 0.993\n",
      "Epoch 2 step 528: training loss: 307.6946451425526\n",
      "Epoch 2 step 529: training accuarcy: 0.999\n",
      "Epoch 2 step 529: training loss: 299.4949892547625\n",
      "Epoch 2 step 530: training accuarcy: 0.998\n",
      "Epoch 2 step 530: training loss: 313.59830348378216\n",
      "Epoch 2 step 531: training accuarcy: 0.9955\n",
      "Epoch 2 step 531: training loss: 316.18446705271384\n",
      "Epoch 2 step 532: training accuarcy: 0.9945\n",
      "Epoch 2 step 532: training loss: 309.46603645860154\n",
      "Epoch 2 step 533: training accuarcy: 0.997\n",
      "Epoch 2 step 533: training loss: 313.3375216390581\n",
      "Epoch 2 step 534: training accuarcy: 0.9965\n",
      "Epoch 2 step 534: training loss: 320.2534815752594\n",
      "Epoch 2 step 535: training accuarcy: 0.997\n",
      "Epoch 2 step 535: training loss: 315.9165406368588\n",
      "Epoch 2 step 536: training accuarcy: 0.997\n",
      "Epoch 2 step 536: training loss: 318.5481019816134\n",
      "Epoch 2 step 537: training accuarcy: 0.9975\n",
      "Epoch 2 step 537: training loss: 317.0932020616919\n",
      "Epoch 2 step 538: training accuarcy: 0.9975\n",
      "Epoch 2 step 538: training loss: 317.32297522060776\n",
      "Epoch 2 step 539: training accuarcy: 0.997\n",
      "Epoch 2 step 539: training loss: 308.95458954221715\n",
      "Epoch 2 step 540: training accuarcy: 0.997\n",
      "Epoch 2 step 540: training loss: 320.239431018343\n",
      "Epoch 2 step 541: training accuarcy: 0.997\n",
      "Epoch 2 step 541: training loss: 311.9511283047036\n",
      "Epoch 2 step 542: training accuarcy: 0.996\n",
      "Epoch 2 step 542: training loss: 303.8897697419417\n",
      "Epoch 2 step 543: training accuarcy: 0.9965\n",
      "Epoch 2 step 543: training loss: 306.19948521479256\n",
      "Epoch 2 step 544: training accuarcy: 0.998\n",
      "Epoch 2 step 544: training loss: 312.3753952202562\n",
      "Epoch 2 step 545: training accuarcy: 0.9965\n",
      "Epoch 2 step 545: training loss: 314.3513455311547\n",
      "Epoch 2 step 546: training accuarcy: 0.9975\n",
      "Epoch 2 step 546: training loss: 300.7687177515985\n",
      "Epoch 2 step 547: training accuarcy: 0.995\n",
      "Epoch 2 step 547: training loss: 315.82714917822364\n",
      "Epoch 2 step 548: training accuarcy: 0.9945\n",
      "Epoch 2 step 548: training loss: 309.17619632330843\n",
      "Epoch 2 step 549: training accuarcy: 0.998\n",
      "Epoch 2 step 549: training loss: 320.87187816979156\n",
      "Epoch 2 step 550: training accuarcy: 0.997\n",
      "Epoch 2 step 550: training loss: 320.32431561468695\n",
      "Epoch 2 step 551: training accuarcy: 0.997\n",
      "Epoch 2 step 551: training loss: 326.2964885757546\n",
      "Epoch 2 step 552: training accuarcy: 0.998\n",
      "Epoch 2 step 552: training loss: 327.4855487892502\n",
      "Epoch 2 step 553: training accuarcy: 0.995\n",
      "Epoch 2 step 553: training loss: 306.58580703550206\n",
      "Epoch 2 step 554: training accuarcy: 0.998\n",
      "Epoch 2 step 554: training loss: 322.98843083417705\n",
      "Epoch 2 step 555: training accuarcy: 0.9985\n",
      "Epoch 2 step 555: training loss: 299.7690256678218\n",
      "Epoch 2 step 556: training accuarcy: 0.9965\n",
      "Epoch 2 step 556: training loss: 318.7351566969644\n",
      "Epoch 2 step 557: training accuarcy: 0.9965\n",
      "Epoch 2 step 557: training loss: 319.8535271985802\n",
      "Epoch 2 step 558: training accuarcy: 0.999\n",
      "Epoch 2 step 558: training loss: 309.4890869839938\n",
      "Epoch 2 step 559: training accuarcy: 0.9985\n",
      "Epoch 2 step 559: training loss: 316.42660826941943\n",
      "Epoch 2 step 560: training accuarcy: 0.997\n",
      "Epoch 2 step 560: training loss: 316.51811487678015\n",
      "Epoch 2 step 561: training accuarcy: 0.994\n",
      "Epoch 2 step 561: training loss: 322.83602087052975\n",
      "Epoch 2 step 562: training accuarcy: 0.9955\n",
      "Epoch 2 step 562: training loss: 320.86249607711306\n",
      "Epoch 2 step 563: training accuarcy: 0.997\n",
      "Epoch 2 step 563: training loss: 314.22958745523374\n",
      "Epoch 2 step 564: training accuarcy: 0.996\n",
      "Epoch 2 step 564: training loss: 310.2845314845497\n",
      "Epoch 2 step 565: training accuarcy: 0.9975\n",
      "Epoch 2 step 565: training loss: 306.02607847865914\n",
      "Epoch 2 step 566: training accuarcy: 0.9955\n",
      "Epoch 2 step 566: training loss: 319.9696954167666\n",
      "Epoch 2 step 567: training accuarcy: 0.997\n",
      "Epoch 2 step 567: training loss: 323.9040152507531\n",
      "Epoch 2 step 568: training accuarcy: 0.995\n",
      "Epoch 2 step 568: training loss: 307.81720263598174\n",
      "Epoch 2 step 569: training accuarcy: 0.9935\n",
      "Epoch 2 step 569: training loss: 316.0265767398756\n",
      "Epoch 2 step 570: training accuarcy: 0.994\n",
      "Epoch 2 step 570: training loss: 314.1465658795002\n",
      "Epoch 2 step 571: training accuarcy: 0.995\n",
      "Epoch 2 step 571: training loss: 312.5347522288208\n",
      "Epoch 2 step 572: training accuarcy: 0.996\n",
      "Epoch 2 step 572: training loss: 319.69049938447347\n",
      "Epoch 2 step 573: training accuarcy: 0.9975\n",
      "Epoch 2 step 573: training loss: 330.3166760762872\n",
      "Epoch 2 step 574: training accuarcy: 0.998\n",
      "Epoch 2 step 574: training loss: 320.69169266719166\n",
      "Epoch 2 step 575: training accuarcy: 0.996\n",
      "Epoch 2 step 575: training loss: 327.55084184111115\n",
      "Epoch 2 step 576: training accuarcy: 0.9965\n",
      "Epoch 2 step 576: training loss: 323.62980141661166\n",
      "Epoch 2 step 577: training accuarcy: 0.9955\n",
      "Epoch 2 step 577: training loss: 318.38237203336945\n",
      "Epoch 2 step 578: training accuarcy: 0.9955\n",
      "Epoch 2 step 578: training loss: 313.57814430930017\n",
      "Epoch 2 step 579: training accuarcy: 0.997\n",
      "Epoch 2 step 579: training loss: 332.5287327315734\n",
      "Epoch 2 step 580: training accuarcy: 0.9945\n",
      "Epoch 2 step 580: training loss: 318.2532709656258\n",
      "Epoch 2 step 581: training accuarcy: 0.9955\n",
      "Epoch 2 step 581: training loss: 322.2383280399937\n",
      "Epoch 2 step 582: training accuarcy: 0.9975\n",
      "Epoch 2 step 582: training loss: 323.26658755496715\n",
      "Epoch 2 step 583: training accuarcy: 0.9965\n",
      "Epoch 2 step 583: training loss: 324.4949228129487\n",
      "Epoch 2 step 584: training accuarcy: 0.9985\n",
      "Epoch 2 step 584: training loss: 320.2986781146659\n",
      "Epoch 2 step 585: training accuarcy: 0.995\n",
      "Epoch 2 step 585: training loss: 319.8885893016301\n",
      "Epoch 2 step 586: training accuarcy: 0.996\n",
      "Epoch 2 step 586: training loss: 319.26293656482466\n",
      "Epoch 2 step 587: training accuarcy: 0.997\n",
      "Epoch 2 step 587: training loss: 311.1621150124853\n",
      "Epoch 2 step 588: training accuarcy: 0.998\n",
      "Epoch 2 step 588: training loss: 317.42920135463874\n",
      "Epoch 2 step 589: training accuarcy: 0.9945\n",
      "Epoch 2 step 589: training loss: 315.89204086761356\n",
      "Epoch 2 step 590: training accuarcy: 0.9975\n",
      "Epoch 2 step 590: training loss: 310.5615867886389\n",
      "Epoch 2 step 591: training accuarcy: 0.997\n",
      "Epoch 2 step 591: training loss: 320.34573126638486\n",
      "Epoch 2 step 592: training accuarcy: 0.996\n",
      "Epoch 2 step 592: training loss: 324.35529573941176\n",
      "Epoch 2 step 593: training accuarcy: 0.996\n",
      "Epoch 2 step 593: training loss: 313.8682794141728\n",
      "Epoch 2 step 594: training accuarcy: 0.9955\n",
      "Epoch 2 step 594: training loss: 326.7411307688478\n",
      "Epoch 2 step 595: training accuarcy: 0.992\n",
      "Epoch 2 step 595: training loss: 321.1266746465642\n",
      "Epoch 2 step 596: training accuarcy: 0.9955\n",
      "Epoch 2 step 596: training loss: 319.5613190961687\n",
      "Epoch 2 step 597: training accuarcy: 0.999\n",
      "Epoch 2 step 597: training loss: 312.94401256453955\n",
      "Epoch 2 step 598: training accuarcy: 0.9935\n",
      "Epoch 2 step 598: training loss: 323.0074103216824\n",
      "Epoch 2 step 599: training accuarcy: 0.9975\n",
      "Epoch 2 step 599: training loss: 321.0804239138612\n",
      "Epoch 2 step 600: training accuarcy: 0.997\n",
      "Epoch 2 step 600: training loss: 322.18902471894194\n",
      "Epoch 2 step 601: training accuarcy: 0.9975\n",
      "Epoch 2 step 601: training loss: 308.1461799465816\n",
      "Epoch 2 step 602: training accuarcy: 0.998\n",
      "Epoch 2 step 602: training loss: 323.0002005855602\n",
      "Epoch 2 step 603: training accuarcy: 0.996\n",
      "Epoch 2 step 603: training loss: 326.6593981744095\n",
      "Epoch 2 step 604: training accuarcy: 0.9975\n",
      "Epoch 2 step 604: training loss: 309.07705189065496\n",
      "Epoch 2 step 605: training accuarcy: 0.9955\n",
      "Epoch 2 step 605: training loss: 313.145473504258\n",
      "Epoch 2 step 606: training accuarcy: 0.9965\n",
      "Epoch 2 step 606: training loss: 324.28617709864363\n",
      "Epoch 2 step 607: training accuarcy: 0.9935\n",
      "Epoch 2 step 607: training loss: 318.0155592068897\n",
      "Epoch 2 step 608: training accuarcy: 0.997\n",
      "Epoch 2 step 608: training loss: 323.1605853479916\n",
      "Epoch 2 step 609: training accuarcy: 0.9975\n",
      "Epoch 2 step 609: training loss: 330.1189910669994\n",
      "Epoch 2 step 610: training accuarcy: 0.998\n",
      "Epoch 2 step 610: training loss: 324.3862599137222\n",
      "Epoch 2 step 611: training accuarcy: 0.9955\n",
      "Epoch 2 step 611: training loss: 309.1247589593453\n",
      "Epoch 2 step 612: training accuarcy: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 612: training loss: 335.3512729490448\n",
      "Epoch 2 step 613: training accuarcy: 0.9945\n",
      "Epoch 2 step 613: training loss: 328.24863371124377\n",
      "Epoch 2 step 614: training accuarcy: 0.997\n",
      "Epoch 2 step 614: training loss: 313.0704500888211\n",
      "Epoch 2 step 615: training accuarcy: 0.9965\n",
      "Epoch 2 step 615: training loss: 314.64613084620754\n",
      "Epoch 2 step 616: training accuarcy: 0.997\n",
      "Epoch 2 step 616: training loss: 335.1671442164569\n",
      "Epoch 2 step 617: training accuarcy: 0.996\n",
      "Epoch 2 step 617: training loss: 324.79082520478624\n",
      "Epoch 2 step 618: training accuarcy: 0.995\n",
      "Epoch 2 step 618: training loss: 317.2795695637067\n",
      "Epoch 2 step 619: training accuarcy: 0.993\n",
      "Epoch 2 step 619: training loss: 316.524652789863\n",
      "Epoch 2 step 620: training accuarcy: 0.996\n",
      "Epoch 2 step 620: training loss: 321.8836638115931\n",
      "Epoch 2 step 621: training accuarcy: 0.9965\n",
      "Epoch 2 step 621: training loss: 311.42338813112076\n",
      "Epoch 2 step 622: training accuarcy: 0.997\n",
      "Epoch 2 step 622: training loss: 324.5668568387207\n",
      "Epoch 2 step 623: training accuarcy: 0.9975\n",
      "Epoch 2 step 623: training loss: 309.9491424139609\n",
      "Epoch 2 step 624: training accuarcy: 0.9965\n",
      "Epoch 2 step 624: training loss: 320.3856585134572\n",
      "Epoch 2 step 625: training accuarcy: 0.996\n",
      "Epoch 2 step 625: training loss: 324.08538784227267\n",
      "Epoch 2 step 626: training accuarcy: 0.997\n",
      "Epoch 2 step 626: training loss: 320.8717797608376\n",
      "Epoch 2 step 627: training accuarcy: 0.9975\n",
      "Epoch 2 step 627: training loss: 326.54970768270755\n",
      "Epoch 2 step 628: training accuarcy: 0.9945\n",
      "Epoch 2 step 628: training loss: 304.5424417666667\n",
      "Epoch 2 step 629: training accuarcy: 0.998\n",
      "Epoch 2 step 629: training loss: 317.06006387594687\n",
      "Epoch 2 step 630: training accuarcy: 0.9965\n",
      "Epoch 2 step 630: training loss: 326.64284642439543\n",
      "Epoch 2 step 631: training accuarcy: 0.995\n",
      "Epoch 2 step 631: training loss: 317.31652718458236\n",
      "Epoch 2 step 632: training accuarcy: 0.9965\n",
      "Epoch 2 step 632: training loss: 321.5036002738043\n",
      "Epoch 2 step 633: training accuarcy: 0.9965\n",
      "Epoch 2 step 633: training loss: 313.0883182984572\n",
      "Epoch 2 step 634: training accuarcy: 0.998\n",
      "Epoch 2 step 634: training loss: 326.77743486476555\n",
      "Epoch 2 step 635: training accuarcy: 0.997\n",
      "Epoch 2 step 635: training loss: 329.4928680698863\n",
      "Epoch 2 step 636: training accuarcy: 0.992\n",
      "Epoch 2 step 636: training loss: 319.0477432995368\n",
      "Epoch 2 step 637: training accuarcy: 0.9965\n",
      "Epoch 2 step 637: training loss: 328.471516711156\n",
      "Epoch 2 step 638: training accuarcy: 0.9985\n",
      "Epoch 2 step 638: training loss: 320.0659979157199\n",
      "Epoch 2 step 639: training accuarcy: 0.9945\n",
      "Epoch 2 step 639: training loss: 319.914911574052\n",
      "Epoch 2 step 640: training accuarcy: 0.999\n",
      "Epoch 2 step 640: training loss: 316.35891863938997\n",
      "Epoch 2 step 641: training accuarcy: 0.995\n",
      "Epoch 2 step 641: training loss: 320.8146795598442\n",
      "Epoch 2 step 642: training accuarcy: 0.9945\n",
      "Epoch 2 step 642: training loss: 319.3036654227705\n",
      "Epoch 2 step 643: training accuarcy: 0.9965\n",
      "Epoch 2 step 643: training loss: 307.08434382194173\n",
      "Epoch 2 step 644: training accuarcy: 0.9965\n",
      "Epoch 2 step 644: training loss: 308.9241660126823\n",
      "Epoch 2 step 645: training accuarcy: 0.995\n",
      "Epoch 2 step 645: training loss: 328.07951460617846\n",
      "Epoch 2 step 646: training accuarcy: 0.9945\n",
      "Epoch 2 step 646: training loss: 321.61195328745686\n",
      "Epoch 2 step 647: training accuarcy: 0.999\n",
      "Epoch 2 step 647: training loss: 322.0134418288633\n",
      "Epoch 2 step 648: training accuarcy: 0.996\n",
      "Epoch 2 step 648: training loss: 323.41928908298644\n",
      "Epoch 2 step 649: training accuarcy: 0.996\n",
      "Epoch 2 step 649: training loss: 304.662411371551\n",
      "Epoch 2 step 650: training accuarcy: 0.9975\n",
      "Epoch 2 step 650: training loss: 316.3590002299686\n",
      "Epoch 2 step 651: training accuarcy: 0.996\n",
      "Epoch 2 step 651: training loss: 317.2077936295232\n",
      "Epoch 2 step 652: training accuarcy: 0.994\n",
      "Epoch 2 step 652: training loss: 308.24093684768235\n",
      "Epoch 2 step 653: training accuarcy: 0.9975\n",
      "Epoch 2 step 653: training loss: 321.2701996568655\n",
      "Epoch 2 step 654: training accuarcy: 0.9945\n",
      "Epoch 2 step 654: training loss: 312.8109493022048\n",
      "Epoch 2 step 655: training accuarcy: 0.9985\n",
      "Epoch 2 step 655: training loss: 323.828869622863\n",
      "Epoch 2 step 656: training accuarcy: 0.9985\n",
      "Epoch 2 step 656: training loss: 330.30570340711677\n",
      "Epoch 2 step 657: training accuarcy: 0.9935\n",
      "Epoch 2 step 657: training loss: 329.0027348105589\n",
      "Epoch 2 step 658: training accuarcy: 0.9965\n",
      "Epoch 2 step 658: training loss: 312.7707732302323\n",
      "Epoch 2 step 659: training accuarcy: 0.995\n",
      "Epoch 2 step 659: training loss: 321.5701774920291\n",
      "Epoch 2 step 660: training accuarcy: 0.997\n",
      "Epoch 2 step 660: training loss: 321.00366530257753\n",
      "Epoch 2 step 661: training accuarcy: 0.994\n",
      "Epoch 2 step 661: training loss: 315.99665477223914\n",
      "Epoch 2 step 662: training accuarcy: 0.995\n",
      "Epoch 2 step 662: training loss: 318.1831921460529\n",
      "Epoch 2 step 663: training accuarcy: 0.997\n",
      "Epoch 2 step 663: training loss: 309.65255748659297\n",
      "Epoch 2 step 664: training accuarcy: 0.9985\n",
      "Epoch 2 step 664: training loss: 304.38254312978336\n",
      "Epoch 2 step 665: training accuarcy: 0.996\n",
      "Epoch 2 step 665: training loss: 319.85060308941104\n",
      "Epoch 2 step 666: training accuarcy: 0.9955\n",
      "Epoch 2 step 666: training loss: 314.16393298975737\n",
      "Epoch 2 step 667: training accuarcy: 0.9955\n",
      "Epoch 2 step 667: training loss: 324.2848080141038\n",
      "Epoch 2 step 668: training accuarcy: 0.997\n",
      "Epoch 2 step 668: training loss: 317.7897829657718\n",
      "Epoch 2 step 669: training accuarcy: 0.9955\n",
      "Epoch 2 step 669: training loss: 302.3566180275723\n",
      "Epoch 2 step 670: training accuarcy: 0.999\n",
      "Epoch 2 step 670: training loss: 312.45340008854953\n",
      "Epoch 2 step 671: training accuarcy: 0.9945\n",
      "Epoch 2 step 671: training loss: 327.45214136229686\n",
      "Epoch 2 step 672: training accuarcy: 0.9955\n",
      "Epoch 2 step 672: training loss: 319.3485546954288\n",
      "Epoch 2 step 673: training accuarcy: 0.9975\n",
      "Epoch 2 step 673: training loss: 313.7575717770005\n",
      "Epoch 2 step 674: training accuarcy: 0.998\n",
      "Epoch 2 step 674: training loss: 331.33762218948425\n",
      "Epoch 2 step 675: training accuarcy: 0.9945\n",
      "Epoch 2 step 675: training loss: 316.0587519893785\n",
      "Epoch 2 step 676: training accuarcy: 0.9975\n",
      "Epoch 2 step 676: training loss: 320.1885632375257\n",
      "Epoch 2 step 677: training accuarcy: 0.9975\n",
      "Epoch 2 step 677: training loss: 317.44543175026354\n",
      "Epoch 2 step 678: training accuarcy: 0.9955\n",
      "Epoch 2 step 678: training loss: 325.82974112163043\n",
      "Epoch 2 step 679: training accuarcy: 0.9975\n",
      "Epoch 2 step 679: training loss: 322.43043341792213\n",
      "Epoch 2 step 680: training accuarcy: 0.9975\n",
      "Epoch 2 step 680: training loss: 320.286354748001\n",
      "Epoch 2 step 681: training accuarcy: 0.997\n",
      "Epoch 2 step 681: training loss: 321.7282357654869\n",
      "Epoch 2 step 682: training accuarcy: 0.995\n",
      "Epoch 2 step 682: training loss: 333.5380229557385\n",
      "Epoch 2 step 683: training accuarcy: 0.991\n",
      "Epoch 2 step 683: training loss: 320.85350386962534\n",
      "Epoch 2 step 684: training accuarcy: 0.996\n",
      "Epoch 2 step 684: training loss: 318.831370653162\n",
      "Epoch 2 step 685: training accuarcy: 0.9955\n",
      "Epoch 2 step 685: training loss: 318.54927836057516\n",
      "Epoch 2 step 686: training accuarcy: 0.9935\n",
      "Epoch 2 step 686: training loss: 314.8428726162417\n",
      "Epoch 2 step 687: training accuarcy: 0.997\n",
      "Epoch 2 step 687: training loss: 323.96930030814696\n",
      "Epoch 2 step 688: training accuarcy: 0.9925\n",
      "Epoch 2 step 688: training loss: 323.1325966054892\n",
      "Epoch 2 step 689: training accuarcy: 0.997\n",
      "Epoch 2 step 689: training loss: 320.01956003053203\n",
      "Epoch 2 step 690: training accuarcy: 0.997\n",
      "Epoch 2 step 690: training loss: 328.66475700951503\n",
      "Epoch 2 step 691: training accuarcy: 0.9955\n",
      "Epoch 2 step 691: training loss: 319.1627357135359\n",
      "Epoch 2 step 692: training accuarcy: 0.9965\n",
      "Epoch 2 step 692: training loss: 325.8095865483356\n",
      "Epoch 2 step 693: training accuarcy: 0.997\n",
      "Epoch 2 step 693: training loss: 315.87462940431556\n",
      "Epoch 2 step 694: training accuarcy: 0.9945\n",
      "Epoch 2 step 694: training loss: 313.7280227001672\n",
      "Epoch 2 step 695: training accuarcy: 0.996\n",
      "Epoch 2 step 695: training loss: 317.020816136864\n",
      "Epoch 2 step 696: training accuarcy: 0.9955\n",
      "Epoch 2 step 696: training loss: 324.0569459436057\n",
      "Epoch 2 step 697: training accuarcy: 0.9965\n",
      "Epoch 2 step 697: training loss: 321.05659793904715\n",
      "Epoch 2 step 698: training accuarcy: 0.9965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 698: training loss: 322.0038442876637\n",
      "Epoch 2 step 699: training accuarcy: 0.995\n",
      "Epoch 2 step 699: training loss: 312.0435734269805\n",
      "Epoch 2 step 700: training accuarcy: 0.997\n",
      "Epoch 2 step 700: training loss: 318.85613161299625\n",
      "Epoch 2 step 701: training accuarcy: 0.9945\n",
      "Epoch 2 step 701: training loss: 341.679649477487\n",
      "Epoch 2 step 702: training accuarcy: 0.989\n",
      "Epoch 2 step 702: training loss: 316.8991271748211\n",
      "Epoch 2 step 703: training accuarcy: 0.994\n",
      "Epoch 2 step 703: training loss: 321.88526525589646\n",
      "Epoch 2 step 704: training accuarcy: 0.9965\n",
      "Epoch 2 step 704: training loss: 328.7915622176679\n",
      "Epoch 2 step 705: training accuarcy: 0.995\n",
      "Epoch 2 step 705: training loss: 323.26891578669847\n",
      "Epoch 2 step 706: training accuarcy: 0.997\n",
      "Epoch 2 step 706: training loss: 326.861486175447\n",
      "Epoch 2 step 707: training accuarcy: 0.996\n",
      "Epoch 2 step 707: training loss: 308.0951456016287\n",
      "Epoch 2 step 708: training accuarcy: 0.995\n",
      "Epoch 2 step 708: training loss: 319.57972280273066\n",
      "Epoch 2 step 709: training accuarcy: 0.9955\n",
      "Epoch 2 step 709: training loss: 324.9813360888443\n",
      "Epoch 2 step 710: training accuarcy: 0.996\n",
      "Epoch 2 step 710: training loss: 310.3791062671279\n",
      "Epoch 2 step 711: training accuarcy: 0.9955\n",
      "Epoch 2 step 711: training loss: 331.017990065087\n",
      "Epoch 2 step 712: training accuarcy: 0.9965\n",
      "Epoch 2 step 712: training loss: 325.85926478985243\n",
      "Epoch 2 step 713: training accuarcy: 0.9955\n",
      "Epoch 2 step 713: training loss: 317.64316353122996\n",
      "Epoch 2 step 714: training accuarcy: 0.9955\n",
      "Epoch 2 step 714: training loss: 324.85981733186884\n",
      "Epoch 2 step 715: training accuarcy: 0.9955\n",
      "Epoch 2 step 715: training loss: 320.71691525478263\n",
      "Epoch 2 step 716: training accuarcy: 0.998\n",
      "Epoch 2 step 716: training loss: 326.59127969575366\n",
      "Epoch 2 step 717: training accuarcy: 0.9975\n",
      "Epoch 2 step 717: training loss: 331.82025380990706\n",
      "Epoch 2 step 718: training accuarcy: 0.9965\n",
      "Epoch 2 step 718: training loss: 316.54617618184716\n",
      "Epoch 2 step 719: training accuarcy: 0.9965\n",
      "Epoch 2 step 719: training loss: 341.7328115664506\n",
      "Epoch 2 step 720: training accuarcy: 0.9945\n",
      "Epoch 2 step 720: training loss: 310.9789805657931\n",
      "Epoch 2 step 721: training accuarcy: 0.9975\n",
      "Epoch 2 step 721: training loss: 323.107700523809\n",
      "Epoch 2 step 722: training accuarcy: 0.9975\n",
      "Epoch 2 step 722: training loss: 313.6453025016931\n",
      "Epoch 2 step 723: training accuarcy: 0.997\n",
      "Epoch 2 step 723: training loss: 315.1487664798367\n",
      "Epoch 2 step 724: training accuarcy: 0.9965\n",
      "Epoch 2 step 724: training loss: 317.9053154511744\n",
      "Epoch 2 step 725: training accuarcy: 0.995\n",
      "Epoch 2 step 725: training loss: 332.24749973826295\n",
      "Epoch 2 step 726: training accuarcy: 0.994\n",
      "Epoch 2 step 726: training loss: 319.9746594661541\n",
      "Epoch 2 step 727: training accuarcy: 0.9975\n",
      "Epoch 2 step 727: training loss: 309.0941016096581\n",
      "Epoch 2 step 728: training accuarcy: 0.996\n",
      "Epoch 2 step 728: training loss: 326.7908265431414\n",
      "Epoch 2 step 729: training accuarcy: 0.9945\n",
      "Epoch 2 step 729: training loss: 331.06333787526006\n",
      "Epoch 2 step 730: training accuarcy: 0.998\n",
      "Epoch 2 step 730: training loss: 320.74587263826436\n",
      "Epoch 2 step 731: training accuarcy: 0.9955\n",
      "Epoch 2 step 731: training loss: 315.6702079546593\n",
      "Epoch 2 step 732: training accuarcy: 0.995\n",
      "Epoch 2 step 732: training loss: 323.6018673718344\n",
      "Epoch 2 step 733: training accuarcy: 0.9975\n",
      "Epoch 2 step 733: training loss: 306.9241656560906\n",
      "Epoch 2 step 734: training accuarcy: 0.9975\n",
      "Epoch 2 step 734: training loss: 318.06927153084496\n",
      "Epoch 2 step 735: training accuarcy: 0.997\n",
      "Epoch 2 step 735: training loss: 327.6550659517548\n",
      "Epoch 2 step 736: training accuarcy: 0.9945\n",
      "Epoch 2 step 736: training loss: 320.13922953262806\n",
      "Epoch 2 step 737: training accuarcy: 0.998\n",
      "Epoch 2 step 737: training loss: 325.456719449474\n",
      "Epoch 2 step 738: training accuarcy: 0.996\n",
      "Epoch 2 step 738: training loss: 329.83398835242656\n",
      "Epoch 2 step 739: training accuarcy: 0.996\n",
      "Epoch 2 step 739: training loss: 325.294853627051\n",
      "Epoch 2 step 740: training accuarcy: 0.994\n",
      "Epoch 2 step 740: training loss: 325.7577166327925\n",
      "Epoch 2 step 741: training accuarcy: 0.996\n",
      "Epoch 2 step 741: training loss: 321.76786828642224\n",
      "Epoch 2 step 742: training accuarcy: 0.9955\n",
      "Epoch 2 step 742: training loss: 327.69581976060283\n",
      "Epoch 2 step 743: training accuarcy: 0.9985\n",
      "Epoch 2 step 743: training loss: 312.7502314905709\n",
      "Epoch 2 step 744: training accuarcy: 0.9985\n",
      "Epoch 2 step 744: training loss: 331.36814228048627\n",
      "Epoch 2 step 745: training accuarcy: 0.996\n",
      "Epoch 2 step 745: training loss: 317.90039360104913\n",
      "Epoch 2 step 746: training accuarcy: 0.998\n",
      "Epoch 2 step 746: training loss: 326.31987786343313\n",
      "Epoch 2 step 747: training accuarcy: 0.995\n",
      "Epoch 2 step 747: training loss: 330.45885288521913\n",
      "Epoch 2 step 748: training accuarcy: 0.995\n",
      "Epoch 2 step 748: training loss: 312.30347791700456\n",
      "Epoch 2 step 749: training accuarcy: 0.9975\n",
      "Epoch 2 step 749: training loss: 304.6656759176723\n",
      "Epoch 2 step 750: training accuarcy: 0.9975\n",
      "Epoch 2 step 750: training loss: 314.1000419308359\n",
      "Epoch 2 step 751: training accuarcy: 0.9955\n",
      "Epoch 2 step 751: training loss: 319.19687935198124\n",
      "Epoch 2 step 752: training accuarcy: 0.997\n",
      "Epoch 2 step 752: training loss: 317.9287475029335\n",
      "Epoch 2 step 753: training accuarcy: 0.996\n",
      "Epoch 2 step 753: training loss: 321.04067116392156\n",
      "Epoch 2 step 754: training accuarcy: 0.996\n",
      "Epoch 2 step 754: training loss: 334.49585736682525\n",
      "Epoch 2 step 755: training accuarcy: 0.996\n",
      "Epoch 2 step 755: training loss: 326.3955172957769\n",
      "Epoch 2 step 756: training accuarcy: 0.9965\n",
      "Epoch 2 step 756: training loss: 311.6017348853985\n",
      "Epoch 2 step 757: training accuarcy: 0.994\n",
      "Epoch 2 step 757: training loss: 307.05401100193706\n",
      "Epoch 2 step 758: training accuarcy: 0.999\n",
      "Epoch 2 step 758: training loss: 317.5467501341295\n",
      "Epoch 2 step 759: training accuarcy: 0.9975\n",
      "Epoch 2 step 759: training loss: 320.1194832099811\n",
      "Epoch 2 step 760: training accuarcy: 0.995\n",
      "Epoch 2 step 760: training loss: 321.3084079695234\n",
      "Epoch 2 step 761: training accuarcy: 0.9945\n",
      "Epoch 2 step 761: training loss: 320.96002573980945\n",
      "Epoch 2 step 762: training accuarcy: 0.9975\n",
      "Epoch 2 step 762: training loss: 339.6576403465408\n",
      "Epoch 2 step 763: training accuarcy: 0.9945\n",
      "Epoch 2 step 763: training loss: 327.71695334570757\n",
      "Epoch 2 step 764: training accuarcy: 0.995\n",
      "Epoch 2 step 764: training loss: 326.7976738619251\n",
      "Epoch 2 step 765: training accuarcy: 0.9965\n",
      "Epoch 2 step 765: training loss: 314.6813384721286\n",
      "Epoch 2 step 766: training accuarcy: 0.9965\n",
      "Epoch 2 step 766: training loss: 317.727819535943\n",
      "Epoch 2 step 767: training accuarcy: 0.994\n",
      "Epoch 2 step 767: training loss: 349.4492657006962\n",
      "Epoch 2 step 768: training accuarcy: 0.995\n",
      "Epoch 2 step 768: training loss: 318.6947811435881\n",
      "Epoch 2 step 769: training accuarcy: 0.9975\n",
      "Epoch 2 step 769: training loss: 318.7223391374217\n",
      "Epoch 2 step 770: training accuarcy: 0.994\n",
      "Epoch 2 step 770: training loss: 318.1720778932763\n",
      "Epoch 2 step 771: training accuarcy: 0.9955\n",
      "Epoch 2 step 771: training loss: 319.890262165751\n",
      "Epoch 2 step 772: training accuarcy: 0.996\n",
      "Epoch 2 step 772: training loss: 317.3718810389153\n",
      "Epoch 2 step 773: training accuarcy: 0.9965\n",
      "Epoch 2 step 773: training loss: 334.17675735078114\n",
      "Epoch 2 step 774: training accuarcy: 0.9975\n",
      "Epoch 2 step 774: training loss: 333.02124279842263\n",
      "Epoch 2 step 775: training accuarcy: 0.9955\n",
      "Epoch 2 step 775: training loss: 331.17641032829374\n",
      "Epoch 2 step 776: training accuarcy: 0.9935\n",
      "Epoch 2 step 776: training loss: 324.9455580578552\n",
      "Epoch 2 step 777: training accuarcy: 0.9955\n",
      "Epoch 2 step 777: training loss: 311.2996219962133\n",
      "Epoch 2 step 778: training accuarcy: 0.9955\n",
      "Epoch 2 step 778: training loss: 331.3237752217273\n",
      "Epoch 2 step 779: training accuarcy: 0.9975\n",
      "Epoch 2 step 779: training loss: 325.86712185559935\n",
      "Epoch 2 step 780: training accuarcy: 0.996\n",
      "Epoch 2 step 780: training loss: 316.49577723926757\n",
      "Epoch 2 step 781: training accuarcy: 0.9995\n",
      "Epoch 2 step 781: training loss: 322.68571819506633\n",
      "Epoch 2 step 782: training accuarcy: 0.9955\n",
      "Epoch 2 step 782: training loss: 319.5825109179932\n",
      "Epoch 2 step 783: training accuarcy: 0.996\n",
      "Epoch 2 step 783: training loss: 336.22561125115806\n",
      "Epoch 2 step 784: training accuarcy: 0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 784: training loss: 319.28621600272703\n",
      "Epoch 2 step 785: training accuarcy: 0.9935\n",
      "Epoch 2 step 785: training loss: 315.3963780702619\n",
      "Epoch 2 step 786: training accuarcy: 0.9975\n",
      "Epoch 2 step 786: training loss: 321.0749453260568\n",
      "Epoch 2 step 787: training accuarcy: 0.996\n",
      "Epoch 2 step 787: training loss: 323.7255606776381\n",
      "Epoch 2 step 788: training accuarcy: 0.9955\n",
      "Epoch 2 step 788: training loss: 236.53689298205455\n",
      "Epoch 2 step 789: training accuarcy: 0.9961538461538462\n",
      "Epoch 2: train loss 319.33613336497325, train accuarcy 0.952167272567749\n",
      "Epoch 2: valid loss 1033.96088070751, valid accuarcy 0.9609864354133606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████                                                                                               | 3/8 [06:00<09:59, 119.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 309.65848292783585\n",
      "Epoch 3 step 790: training accuarcy: 0.997\n",
      "Epoch 3 step 790: training loss: 309.9977250349506\n",
      "Epoch 3 step 791: training accuarcy: 0.995\n",
      "Epoch 3 step 791: training loss: 324.94450242543166\n",
      "Epoch 3 step 792: training accuarcy: 0.9975\n",
      "Epoch 3 step 792: training loss: 300.41720849251215\n",
      "Epoch 3 step 793: training accuarcy: 0.998\n",
      "Epoch 3 step 793: training loss: 302.750009583625\n",
      "Epoch 3 step 794: training accuarcy: 0.9965\n",
      "Epoch 3 step 794: training loss: 309.11450940913477\n",
      "Epoch 3 step 795: training accuarcy: 0.995\n",
      "Epoch 3 step 795: training loss: 322.5879538555329\n",
      "Epoch 3 step 796: training accuarcy: 0.995\n",
      "Epoch 3 step 796: training loss: 304.9679905126494\n",
      "Epoch 3 step 797: training accuarcy: 0.9975\n",
      "Epoch 3 step 797: training loss: 319.77697752689426\n",
      "Epoch 3 step 798: training accuarcy: 0.9975\n",
      "Epoch 3 step 798: training loss: 308.3493692727157\n",
      "Epoch 3 step 799: training accuarcy: 0.9945\n",
      "Epoch 3 step 799: training loss: 316.7569106044861\n",
      "Epoch 3 step 800: training accuarcy: 0.997\n",
      "Epoch 3 step 800: training loss: 316.5985311863237\n",
      "Epoch 3 step 801: training accuarcy: 0.997\n",
      "Epoch 3 step 801: training loss: 307.73986246848676\n",
      "Epoch 3 step 802: training accuarcy: 0.9985\n",
      "Epoch 3 step 802: training loss: 302.9309163861677\n",
      "Epoch 3 step 803: training accuarcy: 0.9965\n",
      "Epoch 3 step 803: training loss: 321.78520829966857\n",
      "Epoch 3 step 804: training accuarcy: 0.996\n",
      "Epoch 3 step 804: training loss: 322.9658406735806\n",
      "Epoch 3 step 805: training accuarcy: 0.9955\n",
      "Epoch 3 step 805: training loss: 316.53258131669145\n",
      "Epoch 3 step 806: training accuarcy: 0.998\n",
      "Epoch 3 step 806: training loss: 299.33858341156736\n",
      "Epoch 3 step 807: training accuarcy: 0.998\n",
      "Epoch 3 step 807: training loss: 316.3916608108394\n",
      "Epoch 3 step 808: training accuarcy: 0.9955\n",
      "Epoch 3 step 808: training loss: 313.8058544966951\n",
      "Epoch 3 step 809: training accuarcy: 0.997\n",
      "Epoch 3 step 809: training loss: 316.3848567945682\n",
      "Epoch 3 step 810: training accuarcy: 0.9955\n",
      "Epoch 3 step 810: training loss: 313.5083100045476\n",
      "Epoch 3 step 811: training accuarcy: 0.999\n",
      "Epoch 3 step 811: training loss: 313.59741039864764\n",
      "Epoch 3 step 812: training accuarcy: 0.9945\n",
      "Epoch 3 step 812: training loss: 306.89067759438444\n",
      "Epoch 3 step 813: training accuarcy: 0.995\n",
      "Epoch 3 step 813: training loss: 325.26580659478435\n",
      "Epoch 3 step 814: training accuarcy: 0.9925\n",
      "Epoch 3 step 814: training loss: 317.7063436974065\n",
      "Epoch 3 step 815: training accuarcy: 0.996\n",
      "Epoch 3 step 815: training loss: 310.42552222446295\n",
      "Epoch 3 step 816: training accuarcy: 0.9985\n",
      "Epoch 3 step 816: training loss: 306.46313779967807\n",
      "Epoch 3 step 817: training accuarcy: 0.995\n",
      "Epoch 3 step 817: training loss: 326.90799238639147\n",
      "Epoch 3 step 818: training accuarcy: 0.9975\n",
      "Epoch 3 step 818: training loss: 311.20048536563263\n",
      "Epoch 3 step 819: training accuarcy: 0.9965\n",
      "Epoch 3 step 819: training loss: 324.18780560652465\n",
      "Epoch 3 step 820: training accuarcy: 0.995\n",
      "Epoch 3 step 820: training loss: 307.59957425971993\n",
      "Epoch 3 step 821: training accuarcy: 0.9965\n",
      "Epoch 3 step 821: training loss: 310.91976668439605\n",
      "Epoch 3 step 822: training accuarcy: 0.997\n",
      "Epoch 3 step 822: training loss: 311.5035451478092\n",
      "Epoch 3 step 823: training accuarcy: 0.9955\n",
      "Epoch 3 step 823: training loss: 326.59953077166494\n",
      "Epoch 3 step 824: training accuarcy: 0.9985\n",
      "Epoch 3 step 824: training loss: 304.35414963180733\n",
      "Epoch 3 step 825: training accuarcy: 0.9965\n",
      "Epoch 3 step 825: training loss: 327.2883312276874\n",
      "Epoch 3 step 826: training accuarcy: 0.9965\n",
      "Epoch 3 step 826: training loss: 322.25629801598785\n",
      "Epoch 3 step 827: training accuarcy: 0.9975\n",
      "Epoch 3 step 827: training loss: 315.5295533722061\n",
      "Epoch 3 step 828: training accuarcy: 0.997\n",
      "Epoch 3 step 828: training loss: 320.17111957706857\n",
      "Epoch 3 step 829: training accuarcy: 0.9985\n",
      "Epoch 3 step 829: training loss: 326.3339106864087\n",
      "Epoch 3 step 830: training accuarcy: 0.996\n",
      "Epoch 3 step 830: training loss: 333.3865489681662\n",
      "Epoch 3 step 831: training accuarcy: 0.9965\n",
      "Epoch 3 step 831: training loss: 326.91160239910243\n",
      "Epoch 3 step 832: training accuarcy: 0.996\n",
      "Epoch 3 step 832: training loss: 322.4992673870013\n",
      "Epoch 3 step 833: training accuarcy: 0.997\n",
      "Epoch 3 step 833: training loss: 315.8817224414053\n",
      "Epoch 3 step 834: training accuarcy: 0.9975\n",
      "Epoch 3 step 834: training loss: 317.41612833428076\n",
      "Epoch 3 step 835: training accuarcy: 0.996\n",
      "Epoch 3 step 835: training loss: 326.17103520397416\n",
      "Epoch 3 step 836: training accuarcy: 0.994\n",
      "Epoch 3 step 836: training loss: 309.0787487426629\n",
      "Epoch 3 step 837: training accuarcy: 0.998\n",
      "Epoch 3 step 837: training loss: 321.35334910089125\n",
      "Epoch 3 step 838: training accuarcy: 0.997\n",
      "Epoch 3 step 838: training loss: 319.036625094654\n",
      "Epoch 3 step 839: training accuarcy: 0.995\n",
      "Epoch 3 step 839: training loss: 316.52479616350536\n",
      "Epoch 3 step 840: training accuarcy: 0.995\n",
      "Epoch 3 step 840: training loss: 324.26623297616464\n",
      "Epoch 3 step 841: training accuarcy: 0.9965\n",
      "Epoch 3 step 841: training loss: 331.7334976272157\n",
      "Epoch 3 step 842: training accuarcy: 0.994\n",
      "Epoch 3 step 842: training loss: 325.8331980791045\n",
      "Epoch 3 step 843: training accuarcy: 0.9935\n",
      "Epoch 3 step 843: training loss: 328.363168470717\n",
      "Epoch 3 step 844: training accuarcy: 0.9935\n",
      "Epoch 3 step 844: training loss: 315.7656079801833\n",
      "Epoch 3 step 845: training accuarcy: 0.998\n",
      "Epoch 3 step 845: training loss: 334.3558763516509\n",
      "Epoch 3 step 846: training accuarcy: 0.9965\n",
      "Epoch 3 step 846: training loss: 327.8215211160035\n",
      "Epoch 3 step 847: training accuarcy: 0.9985\n",
      "Epoch 3 step 847: training loss: 317.7992004121344\n",
      "Epoch 3 step 848: training accuarcy: 0.997\n",
      "Epoch 3 step 848: training loss: 330.9556229412386\n",
      "Epoch 3 step 849: training accuarcy: 0.994\n",
      "Epoch 3 step 849: training loss: 329.4471857623698\n",
      "Epoch 3 step 850: training accuarcy: 0.9955\n",
      "Epoch 3 step 850: training loss: 323.5006939914533\n",
      "Epoch 3 step 851: training accuarcy: 0.995\n",
      "Epoch 3 step 851: training loss: 331.97810187386654\n",
      "Epoch 3 step 852: training accuarcy: 0.9905\n",
      "Epoch 3 step 852: training loss: 322.0563093986889\n",
      "Epoch 3 step 853: training accuarcy: 0.9975\n",
      "Epoch 3 step 853: training loss: 338.21738952249325\n",
      "Epoch 3 step 854: training accuarcy: 0.994\n",
      "Epoch 3 step 854: training loss: 318.23400668997175\n",
      "Epoch 3 step 855: training accuarcy: 0.998\n",
      "Epoch 3 step 855: training loss: 321.86479721820353\n",
      "Epoch 3 step 856: training accuarcy: 0.9975\n",
      "Epoch 3 step 856: training loss: 327.2135985229896\n",
      "Epoch 3 step 857: training accuarcy: 0.996\n",
      "Epoch 3 step 857: training loss: 335.0728664619178\n",
      "Epoch 3 step 858: training accuarcy: 0.996\n",
      "Epoch 3 step 858: training loss: 314.3302602867242\n",
      "Epoch 3 step 859: training accuarcy: 0.995\n",
      "Epoch 3 step 859: training loss: 320.74573134571085\n",
      "Epoch 3 step 860: training accuarcy: 0.9945\n",
      "Epoch 3 step 860: training loss: 318.39308904861343\n",
      "Epoch 3 step 861: training accuarcy: 0.996\n",
      "Epoch 3 step 861: training loss: 327.22209452160143\n",
      "Epoch 3 step 862: training accuarcy: 0.9945\n",
      "Epoch 3 step 862: training loss: 331.2482126239763\n",
      "Epoch 3 step 863: training accuarcy: 0.9955\n",
      "Epoch 3 step 863: training loss: 322.71264109747335\n",
      "Epoch 3 step 864: training accuarcy: 0.995\n",
      "Epoch 3 step 864: training loss: 324.6214220595655\n",
      "Epoch 3 step 865: training accuarcy: 0.9955\n",
      "Epoch 3 step 865: training loss: 326.28982129389476\n",
      "Epoch 3 step 866: training accuarcy: 0.996\n",
      "Epoch 3 step 866: training loss: 322.3451691300727\n",
      "Epoch 3 step 867: training accuarcy: 0.9985\n",
      "Epoch 3 step 867: training loss: 314.6669935397283\n",
      "Epoch 3 step 868: training accuarcy: 0.9985\n",
      "Epoch 3 step 868: training loss: 317.080301312953\n",
      "Epoch 3 step 869: training accuarcy: 0.9975\n",
      "Epoch 3 step 869: training loss: 327.4412559485328\n",
      "Epoch 3 step 870: training accuarcy: 0.9965\n",
      "Epoch 3 step 870: training loss: 319.9139887556326\n",
      "Epoch 3 step 871: training accuarcy: 0.9975\n",
      "Epoch 3 step 871: training loss: 325.3695828521759\n",
      "Epoch 3 step 872: training accuarcy: 0.993\n",
      "Epoch 3 step 872: training loss: 313.8202295553324\n",
      "Epoch 3 step 873: training accuarcy: 0.9995\n",
      "Epoch 3 step 873: training loss: 337.7032640486147\n",
      "Epoch 3 step 874: training accuarcy: 0.9965\n",
      "Epoch 3 step 874: training loss: 317.95416694954724\n",
      "Epoch 3 step 875: training accuarcy: 0.9955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 875: training loss: 325.8313165517123\n",
      "Epoch 3 step 876: training accuarcy: 0.994\n",
      "Epoch 3 step 876: training loss: 316.68664731604605\n",
      "Epoch 3 step 877: training accuarcy: 0.9985\n",
      "Epoch 3 step 877: training loss: 319.8295183403604\n",
      "Epoch 3 step 878: training accuarcy: 0.996\n",
      "Epoch 3 step 878: training loss: 313.61119126499807\n",
      "Epoch 3 step 879: training accuarcy: 0.998\n",
      "Epoch 3 step 879: training loss: 320.9779839784021\n",
      "Epoch 3 step 880: training accuarcy: 0.998\n",
      "Epoch 3 step 880: training loss: 329.09931322127034\n",
      "Epoch 3 step 881: training accuarcy: 0.9975\n",
      "Epoch 3 step 881: training loss: 315.49971653257023\n",
      "Epoch 3 step 882: training accuarcy: 0.9965\n",
      "Epoch 3 step 882: training loss: 313.2860025150304\n",
      "Epoch 3 step 883: training accuarcy: 0.9975\n",
      "Epoch 3 step 883: training loss: 330.23658252738323\n",
      "Epoch 3 step 884: training accuarcy: 0.9975\n",
      "Epoch 3 step 884: training loss: 322.04783865337254\n",
      "Epoch 3 step 885: training accuarcy: 0.998\n",
      "Epoch 3 step 885: training loss: 310.2426714273719\n",
      "Epoch 3 step 886: training accuarcy: 0.998\n",
      "Epoch 3 step 886: training loss: 328.17547893712447\n",
      "Epoch 3 step 887: training accuarcy: 0.9945\n",
      "Epoch 3 step 887: training loss: 308.76421072579274\n",
      "Epoch 3 step 888: training accuarcy: 0.9945\n",
      "Epoch 3 step 888: training loss: 318.67770800779493\n",
      "Epoch 3 step 889: training accuarcy: 0.997\n",
      "Epoch 3 step 889: training loss: 317.74687333087627\n",
      "Epoch 3 step 890: training accuarcy: 0.997\n",
      "Epoch 3 step 890: training loss: 328.9470889021518\n",
      "Epoch 3 step 891: training accuarcy: 0.998\n",
      "Epoch 3 step 891: training loss: 323.32258477433345\n",
      "Epoch 3 step 892: training accuarcy: 0.9965\n",
      "Epoch 3 step 892: training loss: 332.38295128115\n",
      "Epoch 3 step 893: training accuarcy: 0.997\n",
      "Epoch 3 step 893: training loss: 328.82312700485295\n",
      "Epoch 3 step 894: training accuarcy: 0.9945\n",
      "Epoch 3 step 894: training loss: 314.80334511\n",
      "Epoch 3 step 895: training accuarcy: 0.995\n",
      "Epoch 3 step 895: training loss: 310.8726525950525\n",
      "Epoch 3 step 896: training accuarcy: 0.9965\n",
      "Epoch 3 step 896: training loss: 312.44081762554146\n",
      "Epoch 3 step 897: training accuarcy: 0.9965\n",
      "Epoch 3 step 897: training loss: 322.5400815876194\n",
      "Epoch 3 step 898: training accuarcy: 0.996\n",
      "Epoch 3 step 898: training loss: 333.09205566348743\n",
      "Epoch 3 step 899: training accuarcy: 0.9965\n",
      "Epoch 3 step 899: training loss: 327.78919165482955\n",
      "Epoch 3 step 900: training accuarcy: 0.997\n",
      "Epoch 3 step 900: training loss: 329.7653129180478\n",
      "Epoch 3 step 901: training accuarcy: 0.9945\n",
      "Epoch 3 step 901: training loss: 327.8012202318945\n",
      "Epoch 3 step 902: training accuarcy: 0.997\n",
      "Epoch 3 step 902: training loss: 323.84871165494894\n",
      "Epoch 3 step 903: training accuarcy: 0.9975\n",
      "Epoch 3 step 903: training loss: 324.8126452591496\n",
      "Epoch 3 step 904: training accuarcy: 0.9945\n",
      "Epoch 3 step 904: training loss: 325.347929926407\n",
      "Epoch 3 step 905: training accuarcy: 0.9955\n",
      "Epoch 3 step 905: training loss: 323.76679322487723\n",
      "Epoch 3 step 906: training accuarcy: 0.9955\n",
      "Epoch 3 step 906: training loss: 326.1614081323879\n",
      "Epoch 3 step 907: training accuarcy: 0.993\n",
      "Epoch 3 step 907: training loss: 325.3077293525097\n",
      "Epoch 3 step 908: training accuarcy: 0.998\n",
      "Epoch 3 step 908: training loss: 322.5479560116216\n",
      "Epoch 3 step 909: training accuarcy: 0.9955\n",
      "Epoch 3 step 909: training loss: 319.8544326265936\n",
      "Epoch 3 step 910: training accuarcy: 0.9945\n",
      "Epoch 3 step 910: training loss: 331.59402727420667\n",
      "Epoch 3 step 911: training accuarcy: 0.994\n",
      "Epoch 3 step 911: training loss: 327.9331604877524\n",
      "Epoch 3 step 912: training accuarcy: 0.996\n",
      "Epoch 3 step 912: training loss: 328.5421768678591\n",
      "Epoch 3 step 913: training accuarcy: 0.995\n",
      "Epoch 3 step 913: training loss: 342.8007884340242\n",
      "Epoch 3 step 914: training accuarcy: 0.995\n",
      "Epoch 3 step 914: training loss: 331.2851142828688\n",
      "Epoch 3 step 915: training accuarcy: 0.996\n",
      "Epoch 3 step 915: training loss: 321.15086390098577\n",
      "Epoch 3 step 916: training accuarcy: 0.993\n",
      "Epoch 3 step 916: training loss: 334.0153612989436\n",
      "Epoch 3 step 917: training accuarcy: 0.9965\n",
      "Epoch 3 step 917: training loss: 319.08145577986755\n",
      "Epoch 3 step 918: training accuarcy: 0.996\n",
      "Epoch 3 step 918: training loss: 322.3196033352157\n",
      "Epoch 3 step 919: training accuarcy: 0.9965\n",
      "Epoch 3 step 919: training loss: 317.1988855979259\n",
      "Epoch 3 step 920: training accuarcy: 0.9955\n",
      "Epoch 3 step 920: training loss: 302.99920660307686\n",
      "Epoch 3 step 921: training accuarcy: 0.997\n",
      "Epoch 3 step 921: training loss: 335.711333648992\n",
      "Epoch 3 step 922: training accuarcy: 0.995\n",
      "Epoch 3 step 922: training loss: 320.31817782126063\n",
      "Epoch 3 step 923: training accuarcy: 0.998\n",
      "Epoch 3 step 923: training loss: 322.75467967545467\n",
      "Epoch 3 step 924: training accuarcy: 0.9975\n",
      "Epoch 3 step 924: training loss: 323.4625460106206\n",
      "Epoch 3 step 925: training accuarcy: 0.992\n",
      "Epoch 3 step 925: training loss: 335.0770209770669\n",
      "Epoch 3 step 926: training accuarcy: 0.9935\n",
      "Epoch 3 step 926: training loss: 321.61333940854274\n",
      "Epoch 3 step 927: training accuarcy: 0.995\n",
      "Epoch 3 step 927: training loss: 335.16708906952147\n",
      "Epoch 3 step 928: training accuarcy: 0.9955\n",
      "Epoch 3 step 928: training loss: 319.9951862234508\n",
      "Epoch 3 step 929: training accuarcy: 0.9965\n",
      "Epoch 3 step 929: training loss: 315.66823098932684\n",
      "Epoch 3 step 930: training accuarcy: 0.9975\n",
      "Epoch 3 step 930: training loss: 321.46726093833377\n",
      "Epoch 3 step 931: training accuarcy: 0.9975\n",
      "Epoch 3 step 931: training loss: 320.7792272232569\n",
      "Epoch 3 step 932: training accuarcy: 0.9935\n",
      "Epoch 3 step 932: training loss: 333.4100885669826\n",
      "Epoch 3 step 933: training accuarcy: 0.996\n",
      "Epoch 3 step 933: training loss: 320.22562372077357\n",
      "Epoch 3 step 934: training accuarcy: 0.996\n",
      "Epoch 3 step 934: training loss: 327.4638880966149\n",
      "Epoch 3 step 935: training accuarcy: 0.997\n",
      "Epoch 3 step 935: training loss: 319.78796808243\n",
      "Epoch 3 step 936: training accuarcy: 0.9955\n",
      "Epoch 3 step 936: training loss: 320.9001766765585\n",
      "Epoch 3 step 937: training accuarcy: 0.996\n",
      "Epoch 3 step 937: training loss: 331.10045744135874\n",
      "Epoch 3 step 938: training accuarcy: 0.9985\n",
      "Epoch 3 step 938: training loss: 320.70719414564974\n",
      "Epoch 3 step 939: training accuarcy: 0.996\n",
      "Epoch 3 step 939: training loss: 331.202210354993\n",
      "Epoch 3 step 940: training accuarcy: 0.9935\n",
      "Epoch 3 step 940: training loss: 329.2038049214976\n",
      "Epoch 3 step 941: training accuarcy: 0.9975\n",
      "Epoch 3 step 941: training loss: 331.2813422638002\n",
      "Epoch 3 step 942: training accuarcy: 0.9965\n",
      "Epoch 3 step 942: training loss: 325.21496161330526\n",
      "Epoch 3 step 943: training accuarcy: 0.998\n",
      "Epoch 3 step 943: training loss: 325.9642166723145\n",
      "Epoch 3 step 944: training accuarcy: 0.996\n",
      "Epoch 3 step 944: training loss: 323.02506742130856\n",
      "Epoch 3 step 945: training accuarcy: 0.995\n",
      "Epoch 3 step 945: training loss: 326.5744759765897\n",
      "Epoch 3 step 946: training accuarcy: 0.9965\n",
      "Epoch 3 step 946: training loss: 319.93555036593534\n",
      "Epoch 3 step 947: training accuarcy: 0.999\n",
      "Epoch 3 step 947: training loss: 320.77507589141567\n",
      "Epoch 3 step 948: training accuarcy: 0.9975\n",
      "Epoch 3 step 948: training loss: 310.650805454006\n",
      "Epoch 3 step 949: training accuarcy: 0.996\n",
      "Epoch 3 step 949: training loss: 311.44482281561613\n",
      "Epoch 3 step 950: training accuarcy: 0.9955\n",
      "Epoch 3 step 950: training loss: 309.57435627032316\n",
      "Epoch 3 step 951: training accuarcy: 0.9965\n",
      "Epoch 3 step 951: training loss: 329.03430600305876\n",
      "Epoch 3 step 952: training accuarcy: 0.9965\n",
      "Epoch 3 step 952: training loss: 334.8771441811484\n",
      "Epoch 3 step 953: training accuarcy: 0.995\n",
      "Epoch 3 step 953: training loss: 316.2436044787561\n",
      "Epoch 3 step 954: training accuarcy: 0.997\n",
      "Epoch 3 step 954: training loss: 321.69547993189457\n",
      "Epoch 3 step 955: training accuarcy: 0.9975\n",
      "Epoch 3 step 955: training loss: 321.3212905861137\n",
      "Epoch 3 step 956: training accuarcy: 0.9965\n",
      "Epoch 3 step 956: training loss: 324.9382303792777\n",
      "Epoch 3 step 957: training accuarcy: 0.9975\n",
      "Epoch 3 step 957: training loss: 317.303217347129\n",
      "Epoch 3 step 958: training accuarcy: 0.9965\n",
      "Epoch 3 step 958: training loss: 327.17567146229527\n",
      "Epoch 3 step 959: training accuarcy: 0.9955\n",
      "Epoch 3 step 959: training loss: 321.5580942662496\n",
      "Epoch 3 step 960: training accuarcy: 0.9965\n",
      "Epoch 3 step 960: training loss: 332.35568942108813\n",
      "Epoch 3 step 961: training accuarcy: 0.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 961: training loss: 315.10795976590293\n",
      "Epoch 3 step 962: training accuarcy: 0.9985\n",
      "Epoch 3 step 962: training loss: 322.17942949206616\n",
      "Epoch 3 step 963: training accuarcy: 0.996\n",
      "Epoch 3 step 963: training loss: 336.12249063814636\n",
      "Epoch 3 step 964: training accuarcy: 0.9975\n",
      "Epoch 3 step 964: training loss: 325.95476129312794\n",
      "Epoch 3 step 965: training accuarcy: 0.999\n",
      "Epoch 3 step 965: training loss: 331.7735338624691\n",
      "Epoch 3 step 966: training accuarcy: 0.9975\n",
      "Epoch 3 step 966: training loss: 321.3858923054422\n",
      "Epoch 3 step 967: training accuarcy: 0.996\n",
      "Epoch 3 step 967: training loss: 317.4350513110139\n",
      "Epoch 3 step 968: training accuarcy: 0.995\n",
      "Epoch 3 step 968: training loss: 330.81026620412433\n",
      "Epoch 3 step 969: training accuarcy: 0.997\n",
      "Epoch 3 step 969: training loss: 328.77890621360024\n",
      "Epoch 3 step 970: training accuarcy: 0.9955\n",
      "Epoch 3 step 970: training loss: 326.11887582023985\n",
      "Epoch 3 step 971: training accuarcy: 0.9975\n",
      "Epoch 3 step 971: training loss: 321.9104931511155\n",
      "Epoch 3 step 972: training accuarcy: 0.996\n",
      "Epoch 3 step 972: training loss: 339.7474665886022\n",
      "Epoch 3 step 973: training accuarcy: 0.995\n",
      "Epoch 3 step 973: training loss: 318.1556228742047\n",
      "Epoch 3 step 974: training accuarcy: 0.997\n",
      "Epoch 3 step 974: training loss: 326.81546873176455\n",
      "Epoch 3 step 975: training accuarcy: 0.998\n",
      "Epoch 3 step 975: training loss: 317.0353397056448\n",
      "Epoch 3 step 976: training accuarcy: 0.997\n",
      "Epoch 3 step 976: training loss: 324.2238293268183\n",
      "Epoch 3 step 977: training accuarcy: 0.998\n",
      "Epoch 3 step 977: training loss: 318.27473855338076\n",
      "Epoch 3 step 978: training accuarcy: 0.997\n",
      "Epoch 3 step 978: training loss: 330.70648898991806\n",
      "Epoch 3 step 979: training accuarcy: 0.997\n",
      "Epoch 3 step 979: training loss: 328.08063598900003\n",
      "Epoch 3 step 980: training accuarcy: 0.9965\n",
      "Epoch 3 step 980: training loss: 328.66401977372067\n",
      "Epoch 3 step 981: training accuarcy: 0.997\n",
      "Epoch 3 step 981: training loss: 322.41420258102585\n",
      "Epoch 3 step 982: training accuarcy: 0.9955\n",
      "Epoch 3 step 982: training loss: 317.69922032031036\n",
      "Epoch 3 step 983: training accuarcy: 0.9975\n",
      "Epoch 3 step 983: training loss: 318.04873907342164\n",
      "Epoch 3 step 984: training accuarcy: 0.9955\n",
      "Epoch 3 step 984: training loss: 326.3493974991561\n",
      "Epoch 3 step 985: training accuarcy: 0.994\n",
      "Epoch 3 step 985: training loss: 337.5076787727305\n",
      "Epoch 3 step 986: training accuarcy: 0.9955\n",
      "Epoch 3 step 986: training loss: 323.00114517693237\n",
      "Epoch 3 step 987: training accuarcy: 0.996\n",
      "Epoch 3 step 987: training loss: 325.59199398387585\n",
      "Epoch 3 step 988: training accuarcy: 0.995\n",
      "Epoch 3 step 988: training loss: 328.55068241160143\n",
      "Epoch 3 step 989: training accuarcy: 0.994\n",
      "Epoch 3 step 989: training loss: 327.94347536405024\n",
      "Epoch 3 step 990: training accuarcy: 0.995\n",
      "Epoch 3 step 990: training loss: 336.7428736085243\n",
      "Epoch 3 step 991: training accuarcy: 0.994\n",
      "Epoch 3 step 991: training loss: 328.02491331346255\n",
      "Epoch 3 step 992: training accuarcy: 0.997\n",
      "Epoch 3 step 992: training loss: 325.87005035331526\n",
      "Epoch 3 step 993: training accuarcy: 0.9955\n",
      "Epoch 3 step 993: training loss: 318.699399546927\n",
      "Epoch 3 step 994: training accuarcy: 0.9945\n",
      "Epoch 3 step 994: training loss: 323.05643590340026\n",
      "Epoch 3 step 995: training accuarcy: 0.996\n",
      "Epoch 3 step 995: training loss: 318.3727355510065\n",
      "Epoch 3 step 996: training accuarcy: 0.997\n",
      "Epoch 3 step 996: training loss: 330.60118793167817\n",
      "Epoch 3 step 997: training accuarcy: 0.996\n",
      "Epoch 3 step 997: training loss: 312.20143666883314\n",
      "Epoch 3 step 998: training accuarcy: 0.9975\n",
      "Epoch 3 step 998: training loss: 339.0753503694252\n",
      "Epoch 3 step 999: training accuarcy: 0.998\n",
      "Epoch 3 step 999: training loss: 336.4489582214406\n",
      "Epoch 3 step 1000: training accuarcy: 0.9975\n",
      "Epoch 3 step 1000: training loss: 327.90079267255\n",
      "Epoch 3 step 1001: training accuarcy: 0.9975\n",
      "Epoch 3 step 1001: training loss: 317.2832485497469\n",
      "Epoch 3 step 1002: training accuarcy: 0.9965\n",
      "Epoch 3 step 1002: training loss: 329.63914904939566\n",
      "Epoch 3 step 1003: training accuarcy: 0.9945\n",
      "Epoch 3 step 1003: training loss: 317.6105194927483\n",
      "Epoch 3 step 1004: training accuarcy: 0.995\n",
      "Epoch 3 step 1004: training loss: 328.6926375694061\n",
      "Epoch 3 step 1005: training accuarcy: 0.9955\n",
      "Epoch 3 step 1005: training loss: 329.4333112777508\n",
      "Epoch 3 step 1006: training accuarcy: 0.995\n",
      "Epoch 3 step 1006: training loss: 313.683101164666\n",
      "Epoch 3 step 1007: training accuarcy: 0.997\n",
      "Epoch 3 step 1007: training loss: 333.636253554132\n",
      "Epoch 3 step 1008: training accuarcy: 0.995\n",
      "Epoch 3 step 1008: training loss: 324.0732498144082\n",
      "Epoch 3 step 1009: training accuarcy: 0.996\n",
      "Epoch 3 step 1009: training loss: 320.8171084182904\n",
      "Epoch 3 step 1010: training accuarcy: 0.997\n",
      "Epoch 3 step 1010: training loss: 315.0285552876693\n",
      "Epoch 3 step 1011: training accuarcy: 0.998\n",
      "Epoch 3 step 1011: training loss: 325.695003086106\n",
      "Epoch 3 step 1012: training accuarcy: 0.9965\n",
      "Epoch 3 step 1012: training loss: 309.4855007093022\n",
      "Epoch 3 step 1013: training accuarcy: 0.9965\n",
      "Epoch 3 step 1013: training loss: 313.6494173482779\n",
      "Epoch 3 step 1014: training accuarcy: 0.995\n",
      "Epoch 3 step 1014: training loss: 328.9153784206285\n",
      "Epoch 3 step 1015: training accuarcy: 0.995\n",
      "Epoch 3 step 1015: training loss: 320.426171774891\n",
      "Epoch 3 step 1016: training accuarcy: 0.9975\n",
      "Epoch 3 step 1016: training loss: 316.98127312863943\n",
      "Epoch 3 step 1017: training accuarcy: 0.997\n",
      "Epoch 3 step 1017: training loss: 333.7255906102236\n",
      "Epoch 3 step 1018: training accuarcy: 0.9975\n",
      "Epoch 3 step 1018: training loss: 327.488362794556\n",
      "Epoch 3 step 1019: training accuarcy: 0.9975\n",
      "Epoch 3 step 1019: training loss: 320.45172866845024\n",
      "Epoch 3 step 1020: training accuarcy: 0.9935\n",
      "Epoch 3 step 1020: training loss: 321.26493959456263\n",
      "Epoch 3 step 1021: training accuarcy: 0.995\n",
      "Epoch 3 step 1021: training loss: 313.25354072706887\n",
      "Epoch 3 step 1022: training accuarcy: 0.9975\n",
      "Epoch 3 step 1022: training loss: 328.52528527579534\n",
      "Epoch 3 step 1023: training accuarcy: 0.9955\n",
      "Epoch 3 step 1023: training loss: 325.65287168155123\n",
      "Epoch 3 step 1024: training accuarcy: 0.997\n",
      "Epoch 3 step 1024: training loss: 323.0202127786756\n",
      "Epoch 3 step 1025: training accuarcy: 0.9975\n",
      "Epoch 3 step 1025: training loss: 325.0435386859069\n",
      "Epoch 3 step 1026: training accuarcy: 0.994\n",
      "Epoch 3 step 1026: training loss: 333.0168260660314\n",
      "Epoch 3 step 1027: training accuarcy: 0.9955\n",
      "Epoch 3 step 1027: training loss: 341.18419756776655\n",
      "Epoch 3 step 1028: training accuarcy: 0.9955\n",
      "Epoch 3 step 1028: training loss: 319.7544450315174\n",
      "Epoch 3 step 1029: training accuarcy: 0.9965\n",
      "Epoch 3 step 1029: training loss: 321.26093998884926\n",
      "Epoch 3 step 1030: training accuarcy: 0.9965\n",
      "Epoch 3 step 1030: training loss: 332.158613658943\n",
      "Epoch 3 step 1031: training accuarcy: 0.9985\n",
      "Epoch 3 step 1031: training loss: 309.7070802687026\n",
      "Epoch 3 step 1032: training accuarcy: 0.9965\n",
      "Epoch 3 step 1032: training loss: 320.38888422289193\n",
      "Epoch 3 step 1033: training accuarcy: 0.996\n",
      "Epoch 3 step 1033: training loss: 327.8247946840604\n",
      "Epoch 3 step 1034: training accuarcy: 0.9965\n",
      "Epoch 3 step 1034: training loss: 327.9881503209713\n",
      "Epoch 3 step 1035: training accuarcy: 0.9955\n",
      "Epoch 3 step 1035: training loss: 321.9646892854571\n",
      "Epoch 3 step 1036: training accuarcy: 0.995\n",
      "Epoch 3 step 1036: training loss: 325.3892678982086\n",
      "Epoch 3 step 1037: training accuarcy: 0.996\n",
      "Epoch 3 step 1037: training loss: 331.1950423530111\n",
      "Epoch 3 step 1038: training accuarcy: 0.996\n",
      "Epoch 3 step 1038: training loss: 320.1610360984811\n",
      "Epoch 3 step 1039: training accuarcy: 0.996\n",
      "Epoch 3 step 1039: training loss: 333.60636081801067\n",
      "Epoch 3 step 1040: training accuarcy: 0.993\n",
      "Epoch 3 step 1040: training loss: 324.4335673992417\n",
      "Epoch 3 step 1041: training accuarcy: 0.9975\n",
      "Epoch 3 step 1041: training loss: 318.60543029014855\n",
      "Epoch 3 step 1042: training accuarcy: 0.9975\n",
      "Epoch 3 step 1042: training loss: 321.789010897375\n",
      "Epoch 3 step 1043: training accuarcy: 0.997\n",
      "Epoch 3 step 1043: training loss: 313.40340868788303\n",
      "Epoch 3 step 1044: training accuarcy: 0.9965\n",
      "Epoch 3 step 1044: training loss: 322.40250893827647\n",
      "Epoch 3 step 1045: training accuarcy: 0.9975\n",
      "Epoch 3 step 1045: training loss: 313.89226373344525\n",
      "Epoch 3 step 1046: training accuarcy: 0.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 1046: training loss: 319.65152435874745\n",
      "Epoch 3 step 1047: training accuarcy: 0.996\n",
      "Epoch 3 step 1047: training loss: 319.87076043147556\n",
      "Epoch 3 step 1048: training accuarcy: 0.9965\n",
      "Epoch 3 step 1048: training loss: 319.6107963018667\n",
      "Epoch 3 step 1049: training accuarcy: 0.9965\n",
      "Epoch 3 step 1049: training loss: 320.80166476879174\n",
      "Epoch 3 step 1050: training accuarcy: 0.9975\n",
      "Epoch 3 step 1050: training loss: 340.15429999419575\n",
      "Epoch 3 step 1051: training accuarcy: 0.996\n",
      "Epoch 3 step 1051: training loss: 238.53677211808397\n",
      "Epoch 3 step 1052: training accuarcy: 0.9987179487179487\n",
      "Epoch 3: train loss 322.1046866738541, train accuarcy 0.9522033333778381\n",
      "Epoch 3: valid loss 1027.8526387736672, valid accuarcy 0.9556296467781067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 4/8 [08:00<07:59, 119.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 319.21165766951145\n",
      "Epoch 4 step 1053: training accuarcy: 0.9975\n",
      "Epoch 4 step 1053: training loss: 309.45178519129286\n",
      "Epoch 4 step 1054: training accuarcy: 0.997\n",
      "Epoch 4 step 1054: training loss: 309.81004367733107\n",
      "Epoch 4 step 1055: training accuarcy: 0.996\n",
      "Epoch 4 step 1055: training loss: 310.5198722611402\n",
      "Epoch 4 step 1056: training accuarcy: 0.997\n",
      "Epoch 4 step 1056: training loss: 323.1899221318025\n",
      "Epoch 4 step 1057: training accuarcy: 0.997\n",
      "Epoch 4 step 1057: training loss: 318.6667397092086\n",
      "Epoch 4 step 1058: training accuarcy: 0.9955\n",
      "Epoch 4 step 1058: training loss: 311.5701856276213\n",
      "Epoch 4 step 1059: training accuarcy: 0.996\n",
      "Epoch 4 step 1059: training loss: 317.8293964409372\n",
      "Epoch 4 step 1060: training accuarcy: 0.999\n",
      "Epoch 4 step 1060: training loss: 308.6348871294265\n",
      "Epoch 4 step 1061: training accuarcy: 0.998\n",
      "Epoch 4 step 1061: training loss: 319.7230498930651\n",
      "Epoch 4 step 1062: training accuarcy: 0.9955\n",
      "Epoch 4 step 1062: training loss: 316.98353091742877\n",
      "Epoch 4 step 1063: training accuarcy: 0.9965\n",
      "Epoch 4 step 1063: training loss: 319.9228491886979\n",
      "Epoch 4 step 1064: training accuarcy: 0.996\n",
      "Epoch 4 step 1064: training loss: 302.3456582152662\n",
      "Epoch 4 step 1065: training accuarcy: 0.998\n",
      "Epoch 4 step 1065: training loss: 307.83753452187875\n",
      "Epoch 4 step 1066: training accuarcy: 0.9965\n",
      "Epoch 4 step 1066: training loss: 310.7253239754607\n",
      "Epoch 4 step 1067: training accuarcy: 0.997\n",
      "Epoch 4 step 1067: training loss: 303.8753891757145\n",
      "Epoch 4 step 1068: training accuarcy: 0.9985\n",
      "Epoch 4 step 1068: training loss: 326.2440178412276\n",
      "Epoch 4 step 1069: training accuarcy: 0.996\n",
      "Epoch 4 step 1069: training loss: 315.3207191088313\n",
      "Epoch 4 step 1070: training accuarcy: 0.996\n",
      "Epoch 4 step 1070: training loss: 319.36161409032854\n",
      "Epoch 4 step 1071: training accuarcy: 0.997\n",
      "Epoch 4 step 1071: training loss: 326.68703976770814\n",
      "Epoch 4 step 1072: training accuarcy: 0.9965\n",
      "Epoch 4 step 1072: training loss: 326.2077576131439\n",
      "Epoch 4 step 1073: training accuarcy: 0.9945\n",
      "Epoch 4 step 1073: training loss: 315.9045940209834\n",
      "Epoch 4 step 1074: training accuarcy: 0.9965\n",
      "Epoch 4 step 1074: training loss: 306.56765345497996\n",
      "Epoch 4 step 1075: training accuarcy: 0.9965\n",
      "Epoch 4 step 1075: training loss: 335.20524634767844\n",
      "Epoch 4 step 1076: training accuarcy: 0.9935\n",
      "Epoch 4 step 1076: training loss: 316.0868303948372\n",
      "Epoch 4 step 1077: training accuarcy: 0.9965\n",
      "Epoch 4 step 1077: training loss: 319.42534050663096\n",
      "Epoch 4 step 1078: training accuarcy: 0.9945\n",
      "Epoch 4 step 1078: training loss: 311.69949554703874\n",
      "Epoch 4 step 1079: training accuarcy: 0.9955\n",
      "Epoch 4 step 1079: training loss: 317.2923615982673\n",
      "Epoch 4 step 1080: training accuarcy: 0.995\n",
      "Epoch 4 step 1080: training loss: 322.92939352361935\n",
      "Epoch 4 step 1081: training accuarcy: 0.996\n",
      "Epoch 4 step 1081: training loss: 312.55529376108666\n",
      "Epoch 4 step 1082: training accuarcy: 0.996\n",
      "Epoch 4 step 1082: training loss: 318.362644530023\n",
      "Epoch 4 step 1083: training accuarcy: 0.996\n",
      "Epoch 4 step 1083: training loss: 332.13120654628995\n",
      "Epoch 4 step 1084: training accuarcy: 0.9955\n",
      "Epoch 4 step 1084: training loss: 314.12594059249807\n",
      "Epoch 4 step 1085: training accuarcy: 0.997\n",
      "Epoch 4 step 1085: training loss: 319.48980976901794\n",
      "Epoch 4 step 1086: training accuarcy: 0.9995\n",
      "Epoch 4 step 1086: training loss: 345.72368974253345\n",
      "Epoch 4 step 1087: training accuarcy: 0.9945\n",
      "Epoch 4 step 1087: training loss: 323.88191213537124\n",
      "Epoch 4 step 1088: training accuarcy: 0.995\n",
      "Epoch 4 step 1088: training loss: 319.1171394347035\n",
      "Epoch 4 step 1089: training accuarcy: 0.996\n",
      "Epoch 4 step 1089: training loss: 328.1627922681323\n",
      "Epoch 4 step 1090: training accuarcy: 0.9965\n",
      "Epoch 4 step 1090: training loss: 329.22398361157707\n",
      "Epoch 4 step 1091: training accuarcy: 0.994\n",
      "Epoch 4 step 1091: training loss: 324.0856239075108\n",
      "Epoch 4 step 1092: training accuarcy: 0.9965\n",
      "Epoch 4 step 1092: training loss: 337.58686835004494\n",
      "Epoch 4 step 1093: training accuarcy: 0.9945\n",
      "Epoch 4 step 1093: training loss: 333.8403630485135\n",
      "Epoch 4 step 1094: training accuarcy: 0.9975\n",
      "Epoch 4 step 1094: training loss: 325.124762184958\n",
      "Epoch 4 step 1095: training accuarcy: 0.992\n",
      "Epoch 4 step 1095: training loss: 326.25822978880956\n",
      "Epoch 4 step 1096: training accuarcy: 0.9955\n",
      "Epoch 4 step 1096: training loss: 308.5407530936945\n",
      "Epoch 4 step 1097: training accuarcy: 0.998\n",
      "Epoch 4 step 1097: training loss: 326.1675619430807\n",
      "Epoch 4 step 1098: training accuarcy: 0.9945\n",
      "Epoch 4 step 1098: training loss: 329.80081882642526\n",
      "Epoch 4 step 1099: training accuarcy: 0.9985\n",
      "Epoch 4 step 1099: training loss: 323.7368398345565\n",
      "Epoch 4 step 1100: training accuarcy: 0.9965\n",
      "Epoch 4 step 1100: training loss: 333.9924942726019\n",
      "Epoch 4 step 1101: training accuarcy: 0.9985\n",
      "Epoch 4 step 1101: training loss: 319.2688942696685\n",
      "Epoch 4 step 1102: training accuarcy: 0.996\n",
      "Epoch 4 step 1102: training loss: 325.64907231043725\n",
      "Epoch 4 step 1103: training accuarcy: 0.9955\n",
      "Epoch 4 step 1103: training loss: 338.46339365326827\n",
      "Epoch 4 step 1104: training accuarcy: 0.9955\n",
      "Epoch 4 step 1104: training loss: 318.4381736464733\n",
      "Epoch 4 step 1105: training accuarcy: 0.998\n",
      "Epoch 4 step 1105: training loss: 318.6907265453508\n",
      "Epoch 4 step 1106: training accuarcy: 0.995\n",
      "Epoch 4 step 1106: training loss: 329.5272446889382\n",
      "Epoch 4 step 1107: training accuarcy: 0.996\n",
      "Epoch 4 step 1107: training loss: 324.99359180101254\n",
      "Epoch 4 step 1108: training accuarcy: 0.997\n",
      "Epoch 4 step 1108: training loss: 326.22081663367135\n",
      "Epoch 4 step 1109: training accuarcy: 0.996\n",
      "Epoch 4 step 1109: training loss: 320.5394695907156\n",
      "Epoch 4 step 1110: training accuarcy: 0.9955\n",
      "Epoch 4 step 1110: training loss: 321.9974993255241\n",
      "Epoch 4 step 1111: training accuarcy: 0.995\n",
      "Epoch 4 step 1111: training loss: 312.6478930839187\n",
      "Epoch 4 step 1112: training accuarcy: 0.998\n",
      "Epoch 4 step 1112: training loss: 324.60930280351397\n",
      "Epoch 4 step 1113: training accuarcy: 0.997\n",
      "Epoch 4 step 1113: training loss: 323.1997738921497\n",
      "Epoch 4 step 1114: training accuarcy: 0.999\n",
      "Epoch 4 step 1114: training loss: 326.9244118233745\n",
      "Epoch 4 step 1115: training accuarcy: 0.994\n",
      "Epoch 4 step 1115: training loss: 321.3275180551178\n",
      "Epoch 4 step 1116: training accuarcy: 0.9955\n",
      "Epoch 4 step 1116: training loss: 325.79262401154836\n",
      "Epoch 4 step 1117: training accuarcy: 0.998\n",
      "Epoch 4 step 1117: training loss: 333.7703258585444\n",
      "Epoch 4 step 1118: training accuarcy: 0.9955\n",
      "Epoch 4 step 1118: training loss: 319.64989183390566\n",
      "Epoch 4 step 1119: training accuarcy: 0.9965\n",
      "Epoch 4 step 1119: training loss: 319.24601093252477\n",
      "Epoch 4 step 1120: training accuarcy: 0.996\n",
      "Epoch 4 step 1120: training loss: 328.2630865106751\n",
      "Epoch 4 step 1121: training accuarcy: 0.9945\n",
      "Epoch 4 step 1121: training loss: 334.3552292139618\n",
      "Epoch 4 step 1122: training accuarcy: 0.996\n",
      "Epoch 4 step 1122: training loss: 331.8049844502563\n",
      "Epoch 4 step 1123: training accuarcy: 0.9965\n",
      "Epoch 4 step 1123: training loss: 322.4244291191677\n",
      "Epoch 4 step 1124: training accuarcy: 0.9975\n",
      "Epoch 4 step 1124: training loss: 315.1830668644717\n",
      "Epoch 4 step 1125: training accuarcy: 0.9985\n",
      "Epoch 4 step 1125: training loss: 321.1956165163436\n",
      "Epoch 4 step 1126: training accuarcy: 0.9935\n",
      "Epoch 4 step 1126: training loss: 315.36441617980495\n",
      "Epoch 4 step 1127: training accuarcy: 0.999\n",
      "Epoch 4 step 1127: training loss: 317.1479631400226\n",
      "Epoch 4 step 1128: training accuarcy: 0.995\n",
      "Epoch 4 step 1128: training loss: 330.399148365687\n",
      "Epoch 4 step 1129: training accuarcy: 0.9965\n",
      "Epoch 4 step 1129: training loss: 322.47161027285347\n",
      "Epoch 4 step 1130: training accuarcy: 0.9965\n",
      "Epoch 4 step 1130: training loss: 323.3787198913179\n",
      "Epoch 4 step 1131: training accuarcy: 0.9965\n",
      "Epoch 4 step 1131: training loss: 320.59813644856\n",
      "Epoch 4 step 1132: training accuarcy: 0.9955\n",
      "Epoch 4 step 1132: training loss: 323.90315839303423\n",
      "Epoch 4 step 1133: training accuarcy: 0.9945\n",
      "Epoch 4 step 1133: training loss: 326.1208170971161\n",
      "Epoch 4 step 1134: training accuarcy: 0.9975\n",
      "Epoch 4 step 1134: training loss: 326.2597704329937\n",
      "Epoch 4 step 1135: training accuarcy: 0.9975\n",
      "Epoch 4 step 1135: training loss: 319.23050756783\n",
      "Epoch 4 step 1136: training accuarcy: 0.9945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1136: training loss: 324.5529075077276\n",
      "Epoch 4 step 1137: training accuarcy: 0.9945\n",
      "Epoch 4 step 1137: training loss: 316.77602515282\n",
      "Epoch 4 step 1138: training accuarcy: 0.9975\n",
      "Epoch 4 step 1138: training loss: 317.26011122298974\n",
      "Epoch 4 step 1139: training accuarcy: 0.991\n",
      "Epoch 4 step 1139: training loss: 327.899051025331\n",
      "Epoch 4 step 1140: training accuarcy: 0.995\n",
      "Epoch 4 step 1140: training loss: 323.5658737617201\n",
      "Epoch 4 step 1141: training accuarcy: 0.993\n",
      "Epoch 4 step 1141: training loss: 317.68139775264194\n",
      "Epoch 4 step 1142: training accuarcy: 0.996\n",
      "Epoch 4 step 1142: training loss: 334.4911517491739\n",
      "Epoch 4 step 1143: training accuarcy: 0.9975\n",
      "Epoch 4 step 1143: training loss: 318.94899711786655\n",
      "Epoch 4 step 1144: training accuarcy: 0.9955\n",
      "Epoch 4 step 1144: training loss: 335.60331938779166\n",
      "Epoch 4 step 1145: training accuarcy: 0.9955\n",
      "Epoch 4 step 1145: training loss: 308.6023671127402\n",
      "Epoch 4 step 1146: training accuarcy: 0.9985\n",
      "Epoch 4 step 1146: training loss: 329.1607847853253\n",
      "Epoch 4 step 1147: training accuarcy: 0.9935\n",
      "Epoch 4 step 1147: training loss: 321.2104973998978\n",
      "Epoch 4 step 1148: training accuarcy: 0.996\n",
      "Epoch 4 step 1148: training loss: 320.4212198548613\n",
      "Epoch 4 step 1149: training accuarcy: 0.997\n",
      "Epoch 4 step 1149: training loss: 319.5958167964852\n",
      "Epoch 4 step 1150: training accuarcy: 0.9975\n",
      "Epoch 4 step 1150: training loss: 317.64462536916005\n",
      "Epoch 4 step 1151: training accuarcy: 0.993\n",
      "Epoch 4 step 1151: training loss: 341.1587757862214\n",
      "Epoch 4 step 1152: training accuarcy: 0.9955\n",
      "Epoch 4 step 1152: training loss: 323.1153220067497\n",
      "Epoch 4 step 1153: training accuarcy: 0.9965\n",
      "Epoch 4 step 1153: training loss: 325.48870324741796\n",
      "Epoch 4 step 1154: training accuarcy: 0.994\n",
      "Epoch 4 step 1154: training loss: 331.28544570437396\n",
      "Epoch 4 step 1155: training accuarcy: 0.995\n",
      "Epoch 4 step 1155: training loss: 320.8859174381697\n",
      "Epoch 4 step 1156: training accuarcy: 0.998\n",
      "Epoch 4 step 1156: training loss: 322.9207651308112\n",
      "Epoch 4 step 1157: training accuarcy: 0.996\n",
      "Epoch 4 step 1157: training loss: 326.1111382720168\n",
      "Epoch 4 step 1158: training accuarcy: 0.9945\n",
      "Epoch 4 step 1158: training loss: 318.3786181362956\n",
      "Epoch 4 step 1159: training accuarcy: 0.9965\n",
      "Epoch 4 step 1159: training loss: 331.9993122345047\n",
      "Epoch 4 step 1160: training accuarcy: 0.994\n",
      "Epoch 4 step 1160: training loss: 331.4779279711154\n",
      "Epoch 4 step 1161: training accuarcy: 0.9945\n",
      "Epoch 4 step 1161: training loss: 322.1541047215611\n",
      "Epoch 4 step 1162: training accuarcy: 0.996\n",
      "Epoch 4 step 1162: training loss: 330.331083321882\n",
      "Epoch 4 step 1163: training accuarcy: 0.998\n",
      "Epoch 4 step 1163: training loss: 324.75418412534805\n",
      "Epoch 4 step 1164: training accuarcy: 0.998\n",
      "Epoch 4 step 1164: training loss: 345.06150824817297\n",
      "Epoch 4 step 1165: training accuarcy: 0.9965\n",
      "Epoch 4 step 1165: training loss: 314.1291770445754\n",
      "Epoch 4 step 1166: training accuarcy: 0.9975\n",
      "Epoch 4 step 1166: training loss: 326.0189980000754\n",
      "Epoch 4 step 1167: training accuarcy: 0.9965\n",
      "Epoch 4 step 1167: training loss: 323.4889788511682\n",
      "Epoch 4 step 1168: training accuarcy: 0.996\n",
      "Epoch 4 step 1168: training loss: 322.6230041926907\n",
      "Epoch 4 step 1169: training accuarcy: 0.996\n",
      "Epoch 4 step 1169: training loss: 331.3870220508113\n",
      "Epoch 4 step 1170: training accuarcy: 0.9965\n",
      "Epoch 4 step 1170: training loss: 314.25221436317327\n",
      "Epoch 4 step 1171: training accuarcy: 0.9965\n",
      "Epoch 4 step 1171: training loss: 315.389009299253\n",
      "Epoch 4 step 1172: training accuarcy: 0.995\n",
      "Epoch 4 step 1172: training loss: 332.3578923041032\n",
      "Epoch 4 step 1173: training accuarcy: 0.996\n",
      "Epoch 4 step 1173: training loss: 324.06874010056674\n",
      "Epoch 4 step 1174: training accuarcy: 0.9955\n",
      "Epoch 4 step 1174: training loss: 318.1676454540283\n",
      "Epoch 4 step 1175: training accuarcy: 0.9955\n",
      "Epoch 4 step 1175: training loss: 331.21575925940243\n",
      "Epoch 4 step 1176: training accuarcy: 0.9965\n",
      "Epoch 4 step 1176: training loss: 329.55926373053154\n",
      "Epoch 4 step 1177: training accuarcy: 0.9985\n",
      "Epoch 4 step 1177: training loss: 322.4887072174399\n",
      "Epoch 4 step 1178: training accuarcy: 0.9975\n",
      "Epoch 4 step 1178: training loss: 320.9872908105673\n",
      "Epoch 4 step 1179: training accuarcy: 0.9955\n",
      "Epoch 4 step 1179: training loss: 320.81241914400414\n",
      "Epoch 4 step 1180: training accuarcy: 0.9965\n",
      "Epoch 4 step 1180: training loss: 327.0428034927621\n",
      "Epoch 4 step 1181: training accuarcy: 0.9945\n",
      "Epoch 4 step 1181: training loss: 330.1629170997948\n",
      "Epoch 4 step 1182: training accuarcy: 0.996\n",
      "Epoch 4 step 1182: training loss: 334.15484452477494\n",
      "Epoch 4 step 1183: training accuarcy: 0.9965\n",
      "Epoch 4 step 1183: training loss: 329.07845811427546\n",
      "Epoch 4 step 1184: training accuarcy: 0.9935\n",
      "Epoch 4 step 1184: training loss: 321.1895667747858\n",
      "Epoch 4 step 1185: training accuarcy: 0.9985\n",
      "Epoch 4 step 1185: training loss: 327.07544489710176\n",
      "Epoch 4 step 1186: training accuarcy: 0.9985\n",
      "Epoch 4 step 1186: training loss: 338.11858521546526\n",
      "Epoch 4 step 1187: training accuarcy: 0.9935\n",
      "Epoch 4 step 1187: training loss: 329.80512806633396\n",
      "Epoch 4 step 1188: training accuarcy: 0.996\n",
      "Epoch 4 step 1188: training loss: 316.09576213659784\n",
      "Epoch 4 step 1189: training accuarcy: 0.996\n",
      "Epoch 4 step 1189: training loss: 314.7584548534438\n",
      "Epoch 4 step 1190: training accuarcy: 0.996\n",
      "Epoch 4 step 1190: training loss: 318.6964832804982\n",
      "Epoch 4 step 1191: training accuarcy: 0.996\n",
      "Epoch 4 step 1191: training loss: 327.8751333547817\n",
      "Epoch 4 step 1192: training accuarcy: 0.997\n",
      "Epoch 4 step 1192: training loss: 330.51833402975524\n",
      "Epoch 4 step 1193: training accuarcy: 0.995\n",
      "Epoch 4 step 1193: training loss: 322.92622916212605\n",
      "Epoch 4 step 1194: training accuarcy: 0.993\n",
      "Epoch 4 step 1194: training loss: 349.28303643223796\n",
      "Epoch 4 step 1195: training accuarcy: 0.995\n",
      "Epoch 4 step 1195: training loss: 319.9397091580902\n",
      "Epoch 4 step 1196: training accuarcy: 0.9995\n",
      "Epoch 4 step 1196: training loss: 336.2861459472832\n",
      "Epoch 4 step 1197: training accuarcy: 0.995\n",
      "Epoch 4 step 1197: training loss: 333.6969418944944\n",
      "Epoch 4 step 1198: training accuarcy: 0.9945\n",
      "Epoch 4 step 1198: training loss: 320.71719615075597\n",
      "Epoch 4 step 1199: training accuarcy: 0.996\n",
      "Epoch 4 step 1199: training loss: 326.1364240521324\n",
      "Epoch 4 step 1200: training accuarcy: 0.997\n",
      "Epoch 4 step 1200: training loss: 334.7988932458234\n",
      "Epoch 4 step 1201: training accuarcy: 0.9965\n",
      "Epoch 4 step 1201: training loss: 318.56422512684986\n",
      "Epoch 4 step 1202: training accuarcy: 0.994\n",
      "Epoch 4 step 1202: training loss: 323.315294193591\n",
      "Epoch 4 step 1203: training accuarcy: 0.994\n",
      "Epoch 4 step 1203: training loss: 326.57620817183044\n",
      "Epoch 4 step 1204: training accuarcy: 0.997\n",
      "Epoch 4 step 1204: training loss: 322.8370632859471\n",
      "Epoch 4 step 1205: training accuarcy: 0.995\n",
      "Epoch 4 step 1205: training loss: 326.8211982492609\n",
      "Epoch 4 step 1206: training accuarcy: 0.997\n",
      "Epoch 4 step 1206: training loss: 326.3010375408793\n",
      "Epoch 4 step 1207: training accuarcy: 0.9945\n",
      "Epoch 4 step 1207: training loss: 324.797619846998\n",
      "Epoch 4 step 1208: training accuarcy: 0.9945\n",
      "Epoch 4 step 1208: training loss: 318.0653779814067\n",
      "Epoch 4 step 1209: training accuarcy: 0.997\n",
      "Epoch 4 step 1209: training loss: 320.52741310980167\n",
      "Epoch 4 step 1210: training accuarcy: 0.994\n",
      "Epoch 4 step 1210: training loss: 334.21123219767657\n",
      "Epoch 4 step 1211: training accuarcy: 0.9965\n",
      "Epoch 4 step 1211: training loss: 325.2919484013102\n",
      "Epoch 4 step 1212: training accuarcy: 0.9955\n",
      "Epoch 4 step 1212: training loss: 328.87019045768454\n",
      "Epoch 4 step 1213: training accuarcy: 0.994\n",
      "Epoch 4 step 1213: training loss: 333.6892222002864\n",
      "Epoch 4 step 1214: training accuarcy: 0.997\n",
      "Epoch 4 step 1214: training loss: 326.81698536602687\n",
      "Epoch 4 step 1215: training accuarcy: 0.9955\n",
      "Epoch 4 step 1215: training loss: 323.5159035945594\n",
      "Epoch 4 step 1216: training accuarcy: 0.9935\n",
      "Epoch 4 step 1216: training loss: 344.67547228796946\n",
      "Epoch 4 step 1217: training accuarcy: 0.9945\n",
      "Epoch 4 step 1217: training loss: 325.27082077241573\n",
      "Epoch 4 step 1218: training accuarcy: 0.998\n",
      "Epoch 4 step 1218: training loss: 320.5264628593594\n",
      "Epoch 4 step 1219: training accuarcy: 0.9975\n",
      "Epoch 4 step 1219: training loss: 333.4916463631333\n",
      "Epoch 4 step 1220: training accuarcy: 0.9935\n",
      "Epoch 4 step 1220: training loss: 334.2998618659474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1221: training accuarcy: 0.9945\n",
      "Epoch 4 step 1221: training loss: 334.85419178521283\n",
      "Epoch 4 step 1222: training accuarcy: 0.996\n",
      "Epoch 4 step 1222: training loss: 323.01065349691623\n",
      "Epoch 4 step 1223: training accuarcy: 0.9965\n",
      "Epoch 4 step 1223: training loss: 321.86714807538544\n",
      "Epoch 4 step 1224: training accuarcy: 0.9955\n",
      "Epoch 4 step 1224: training loss: 332.78325092166824\n",
      "Epoch 4 step 1225: training accuarcy: 0.9965\n",
      "Epoch 4 step 1225: training loss: 311.8968427532475\n",
      "Epoch 4 step 1226: training accuarcy: 0.9975\n",
      "Epoch 4 step 1226: training loss: 333.61774512533475\n",
      "Epoch 4 step 1227: training accuarcy: 0.996\n",
      "Epoch 4 step 1227: training loss: 326.6551326529108\n",
      "Epoch 4 step 1228: training accuarcy: 0.9955\n",
      "Epoch 4 step 1228: training loss: 317.50267456543395\n",
      "Epoch 4 step 1229: training accuarcy: 0.9955\n",
      "Epoch 4 step 1229: training loss: 324.97275796019886\n",
      "Epoch 4 step 1230: training accuarcy: 0.997\n",
      "Epoch 4 step 1230: training loss: 324.329392598824\n",
      "Epoch 4 step 1231: training accuarcy: 0.9955\n",
      "Epoch 4 step 1231: training loss: 335.29465885866523\n",
      "Epoch 4 step 1232: training accuarcy: 0.9945\n",
      "Epoch 4 step 1232: training loss: 327.15625500868373\n",
      "Epoch 4 step 1233: training accuarcy: 0.9975\n",
      "Epoch 4 step 1233: training loss: 325.74656501482934\n",
      "Epoch 4 step 1234: training accuarcy: 0.9975\n",
      "Epoch 4 step 1234: training loss: 309.8862769650866\n",
      "Epoch 4 step 1235: training accuarcy: 0.996\n",
      "Epoch 4 step 1235: training loss: 328.30650659543113\n",
      "Epoch 4 step 1236: training accuarcy: 0.997\n",
      "Epoch 4 step 1236: training loss: 324.1849082833513\n",
      "Epoch 4 step 1237: training accuarcy: 0.996\n",
      "Epoch 4 step 1237: training loss: 318.61566026247283\n",
      "Epoch 4 step 1238: training accuarcy: 0.9975\n",
      "Epoch 4 step 1238: training loss: 324.1699162947434\n",
      "Epoch 4 step 1239: training accuarcy: 0.9965\n",
      "Epoch 4 step 1239: training loss: 326.2888135246219\n",
      "Epoch 4 step 1240: training accuarcy: 0.9955\n",
      "Epoch 4 step 1240: training loss: 318.9529677119966\n",
      "Epoch 4 step 1241: training accuarcy: 0.997\n",
      "Epoch 4 step 1241: training loss: 330.0934364774006\n",
      "Epoch 4 step 1242: training accuarcy: 0.996\n",
      "Epoch 4 step 1242: training loss: 325.8145536511702\n",
      "Epoch 4 step 1243: training accuarcy: 0.994\n",
      "Epoch 4 step 1243: training loss: 322.61839397158457\n",
      "Epoch 4 step 1244: training accuarcy: 0.997\n",
      "Epoch 4 step 1244: training loss: 322.030895808331\n",
      "Epoch 4 step 1245: training accuarcy: 0.9965\n",
      "Epoch 4 step 1245: training loss: 326.29977055887935\n",
      "Epoch 4 step 1246: training accuarcy: 0.9955\n",
      "Epoch 4 step 1246: training loss: 336.62880253866126\n",
      "Epoch 4 step 1247: training accuarcy: 0.9965\n",
      "Epoch 4 step 1247: training loss: 320.76513404729275\n",
      "Epoch 4 step 1248: training accuarcy: 0.9975\n",
      "Epoch 4 step 1248: training loss: 321.43643461094723\n",
      "Epoch 4 step 1249: training accuarcy: 0.996\n",
      "Epoch 4 step 1249: training loss: 335.39070154466987\n",
      "Epoch 4 step 1250: training accuarcy: 0.9945\n",
      "Epoch 4 step 1250: training loss: 337.56207054625804\n",
      "Epoch 4 step 1251: training accuarcy: 0.996\n",
      "Epoch 4 step 1251: training loss: 330.33218830085957\n",
      "Epoch 4 step 1252: training accuarcy: 0.9945\n",
      "Epoch 4 step 1252: training loss: 329.3981689677795\n",
      "Epoch 4 step 1253: training accuarcy: 0.9935\n",
      "Epoch 4 step 1253: training loss: 330.4797235843871\n",
      "Epoch 4 step 1254: training accuarcy: 0.9975\n",
      "Epoch 4 step 1254: training loss: 328.98240018716285\n",
      "Epoch 4 step 1255: training accuarcy: 0.9955\n",
      "Epoch 4 step 1255: training loss: 321.02634432137313\n",
      "Epoch 4 step 1256: training accuarcy: 0.995\n",
      "Epoch 4 step 1256: training loss: 334.82869849467784\n",
      "Epoch 4 step 1257: training accuarcy: 0.9945\n",
      "Epoch 4 step 1257: training loss: 326.49603096003807\n",
      "Epoch 4 step 1258: training accuarcy: 0.997\n",
      "Epoch 4 step 1258: training loss: 328.4616124568928\n",
      "Epoch 4 step 1259: training accuarcy: 0.998\n",
      "Epoch 4 step 1259: training loss: 319.02796396039037\n",
      "Epoch 4 step 1260: training accuarcy: 0.9955\n",
      "Epoch 4 step 1260: training loss: 318.6909377998753\n",
      "Epoch 4 step 1261: training accuarcy: 0.998\n",
      "Epoch 4 step 1261: training loss: 320.5988172794123\n",
      "Epoch 4 step 1262: training accuarcy: 0.997\n",
      "Epoch 4 step 1262: training loss: 312.43501979416\n",
      "Epoch 4 step 1263: training accuarcy: 0.996\n",
      "Epoch 4 step 1263: training loss: 331.87174714540845\n",
      "Epoch 4 step 1264: training accuarcy: 0.9965\n",
      "Epoch 4 step 1264: training loss: 325.90307687669025\n",
      "Epoch 4 step 1265: training accuarcy: 0.998\n",
      "Epoch 4 step 1265: training loss: 330.438280215781\n",
      "Epoch 4 step 1266: training accuarcy: 0.997\n",
      "Epoch 4 step 1266: training loss: 321.2842086252566\n",
      "Epoch 4 step 1267: training accuarcy: 0.997\n",
      "Epoch 4 step 1267: training loss: 337.04744021176555\n",
      "Epoch 4 step 1268: training accuarcy: 0.996\n",
      "Epoch 4 step 1268: training loss: 336.9532644185333\n",
      "Epoch 4 step 1269: training accuarcy: 0.995\n",
      "Epoch 4 step 1269: training loss: 324.3752942765025\n",
      "Epoch 4 step 1270: training accuarcy: 0.995\n",
      "Epoch 4 step 1270: training loss: 319.34024326848305\n",
      "Epoch 4 step 1271: training accuarcy: 0.9965\n",
      "Epoch 4 step 1271: training loss: 329.5581914544781\n",
      "Epoch 4 step 1272: training accuarcy: 0.999\n",
      "Epoch 4 step 1272: training loss: 327.60806763576045\n",
      "Epoch 4 step 1273: training accuarcy: 0.9975\n",
      "Epoch 4 step 1273: training loss: 332.15463315517695\n",
      "Epoch 4 step 1274: training accuarcy: 0.9955\n",
      "Epoch 4 step 1274: training loss: 328.21435589203406\n",
      "Epoch 4 step 1275: training accuarcy: 0.9985\n",
      "Epoch 4 step 1275: training loss: 318.5211743190879\n",
      "Epoch 4 step 1276: training accuarcy: 0.9985\n",
      "Epoch 4 step 1276: training loss: 306.5378381343543\n",
      "Epoch 4 step 1277: training accuarcy: 0.998\n",
      "Epoch 4 step 1277: training loss: 326.10019922343133\n",
      "Epoch 4 step 1278: training accuarcy: 0.9935\n",
      "Epoch 4 step 1278: training loss: 324.9740273369739\n",
      "Epoch 4 step 1279: training accuarcy: 0.998\n",
      "Epoch 4 step 1279: training loss: 330.6720750219622\n",
      "Epoch 4 step 1280: training accuarcy: 0.9955\n",
      "Epoch 4 step 1280: training loss: 332.38147279881116\n",
      "Epoch 4 step 1281: training accuarcy: 0.997\n",
      "Epoch 4 step 1281: training loss: 308.10320881228347\n",
      "Epoch 4 step 1282: training accuarcy: 0.996\n",
      "Epoch 4 step 1282: training loss: 330.9518894819404\n",
      "Epoch 4 step 1283: training accuarcy: 0.9965\n",
      "Epoch 4 step 1283: training loss: 328.7803307517686\n",
      "Epoch 4 step 1284: training accuarcy: 0.9945\n",
      "Epoch 4 step 1284: training loss: 331.94515348984464\n",
      "Epoch 4 step 1285: training accuarcy: 0.995\n",
      "Epoch 4 step 1285: training loss: 329.6009332141609\n",
      "Epoch 4 step 1286: training accuarcy: 0.9955\n",
      "Epoch 4 step 1286: training loss: 328.4005528274647\n",
      "Epoch 4 step 1287: training accuarcy: 0.9975\n",
      "Epoch 4 step 1287: training loss: 320.12049774269224\n",
      "Epoch 4 step 1288: training accuarcy: 0.998\n",
      "Epoch 4 step 1288: training loss: 320.1532631101366\n",
      "Epoch 4 step 1289: training accuarcy: 0.997\n",
      "Epoch 4 step 1289: training loss: 326.7305417931484\n",
      "Epoch 4 step 1290: training accuarcy: 0.9945\n",
      "Epoch 4 step 1290: training loss: 321.9359795580208\n",
      "Epoch 4 step 1291: training accuarcy: 0.996\n",
      "Epoch 4 step 1291: training loss: 322.1629772711234\n",
      "Epoch 4 step 1292: training accuarcy: 0.9965\n",
      "Epoch 4 step 1292: training loss: 324.94524763757863\n",
      "Epoch 4 step 1293: training accuarcy: 0.991\n",
      "Epoch 4 step 1293: training loss: 321.9701273551212\n",
      "Epoch 4 step 1294: training accuarcy: 0.9975\n",
      "Epoch 4 step 1294: training loss: 332.0641460306787\n",
      "Epoch 4 step 1295: training accuarcy: 0.995\n",
      "Epoch 4 step 1295: training loss: 331.2074983926829\n",
      "Epoch 4 step 1296: training accuarcy: 0.996\n",
      "Epoch 4 step 1296: training loss: 324.4571038464694\n",
      "Epoch 4 step 1297: training accuarcy: 0.995\n",
      "Epoch 4 step 1297: training loss: 326.7812507766598\n",
      "Epoch 4 step 1298: training accuarcy: 0.996\n",
      "Epoch 4 step 1298: training loss: 335.89985087665775\n",
      "Epoch 4 step 1299: training accuarcy: 0.9945\n",
      "Epoch 4 step 1299: training loss: 324.30322970077987\n",
      "Epoch 4 step 1300: training accuarcy: 0.9945\n",
      "Epoch 4 step 1300: training loss: 331.7974729507724\n",
      "Epoch 4 step 1301: training accuarcy: 0.995\n",
      "Epoch 4 step 1301: training loss: 311.6007771820019\n",
      "Epoch 4 step 1302: training accuarcy: 0.997\n",
      "Epoch 4 step 1302: training loss: 337.4672590190828\n",
      "Epoch 4 step 1303: training accuarcy: 0.9955\n",
      "Epoch 4 step 1303: training loss: 309.84493480360925\n",
      "Epoch 4 step 1304: training accuarcy: 0.997\n",
      "Epoch 4 step 1304: training loss: 320.9739274867849\n",
      "Epoch 4 step 1305: training accuarcy: 0.9985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1305: training loss: 322.75840928172306\n",
      "Epoch 4 step 1306: training accuarcy: 0.994\n",
      "Epoch 4 step 1306: training loss: 324.64641039305707\n",
      "Epoch 4 step 1307: training accuarcy: 0.9965\n",
      "Epoch 4 step 1307: training loss: 326.67384095468213\n",
      "Epoch 4 step 1308: training accuarcy: 0.996\n",
      "Epoch 4 step 1308: training loss: 329.53041262228925\n",
      "Epoch 4 step 1309: training accuarcy: 0.9955\n",
      "Epoch 4 step 1309: training loss: 325.08932070852615\n",
      "Epoch 4 step 1310: training accuarcy: 0.997\n",
      "Epoch 4 step 1310: training loss: 316.1974464146633\n",
      "Epoch 4 step 1311: training accuarcy: 0.997\n",
      "Epoch 4 step 1311: training loss: 334.3833373807645\n",
      "Epoch 4 step 1312: training accuarcy: 0.996\n",
      "Epoch 4 step 1312: training loss: 329.6752810008255\n",
      "Epoch 4 step 1313: training accuarcy: 0.9965\n",
      "Epoch 4 step 1313: training loss: 331.7108628456672\n",
      "Epoch 4 step 1314: training accuarcy: 0.9955\n",
      "Epoch 4 step 1314: training loss: 229.19482363461373\n",
      "Epoch 4 step 1315: training accuarcy: 0.9961538461538462\n",
      "Epoch 4: train loss 324.09880216730795, train accuarcy 0.9489166140556335\n",
      "Epoch 4: valid loss 1029.294599076205, valid accuarcy 0.95987468957901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                         | 5/8 [09:53<05:53, 117.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 1315: training loss: 300.19705339384643\n",
      "Epoch 5 step 1316: training accuarcy: 0.998\n",
      "Epoch 5 step 1316: training loss: 309.2830322771522\n",
      "Epoch 5 step 1317: training accuarcy: 0.997\n",
      "Epoch 5 step 1317: training loss: 310.63042680173066\n",
      "Epoch 5 step 1318: training accuarcy: 0.9975\n",
      "Epoch 5 step 1318: training loss: 315.26146567146594\n",
      "Epoch 5 step 1319: training accuarcy: 0.9965\n",
      "Epoch 5 step 1319: training loss: 319.5794830087975\n",
      "Epoch 5 step 1320: training accuarcy: 0.996\n",
      "Epoch 5 step 1320: training loss: 324.05532209742216\n",
      "Epoch 5 step 1321: training accuarcy: 0.998\n",
      "Epoch 5 step 1321: training loss: 314.7399272664992\n",
      "Epoch 5 step 1322: training accuarcy: 0.9955\n",
      "Epoch 5 step 1322: training loss: 310.401231960229\n",
      "Epoch 5 step 1323: training accuarcy: 0.998\n",
      "Epoch 5 step 1323: training loss: 315.6578238625169\n",
      "Epoch 5 step 1324: training accuarcy: 0.996\n",
      "Epoch 5 step 1324: training loss: 319.7482102806622\n",
      "Epoch 5 step 1325: training accuarcy: 0.9995\n",
      "Epoch 5 step 1325: training loss: 313.07511621274523\n",
      "Epoch 5 step 1326: training accuarcy: 0.9965\n",
      "Epoch 5 step 1326: training loss: 320.0354519915541\n",
      "Epoch 5 step 1327: training accuarcy: 0.9975\n",
      "Epoch 5 step 1327: training loss: 315.84631465451344\n",
      "Epoch 5 step 1328: training accuarcy: 0.9965\n",
      "Epoch 5 step 1328: training loss: 326.55417663267264\n",
      "Epoch 5 step 1329: training accuarcy: 0.996\n",
      "Epoch 5 step 1329: training loss: 321.10842995202245\n",
      "Epoch 5 step 1330: training accuarcy: 0.998\n",
      "Epoch 5 step 1330: training loss: 319.0151914321517\n",
      "Epoch 5 step 1331: training accuarcy: 0.9965\n",
      "Epoch 5 step 1331: training loss: 324.95992263227976\n",
      "Epoch 5 step 1332: training accuarcy: 0.9975\n",
      "Epoch 5 step 1332: training loss: 313.3937145856671\n",
      "Epoch 5 step 1333: training accuarcy: 0.996\n",
      "Epoch 5 step 1333: training loss: 311.00189307970777\n",
      "Epoch 5 step 1334: training accuarcy: 0.997\n",
      "Epoch 5 step 1334: training loss: 316.86668193751893\n",
      "Epoch 5 step 1335: training accuarcy: 0.9985\n",
      "Epoch 5 step 1335: training loss: 325.57010613819557\n",
      "Epoch 5 step 1336: training accuarcy: 0.9985\n",
      "Epoch 5 step 1336: training loss: 335.6460426297856\n",
      "Epoch 5 step 1337: training accuarcy: 0.995\n",
      "Epoch 5 step 1337: training loss: 309.25514399944234\n",
      "Epoch 5 step 1338: training accuarcy: 0.997\n",
      "Epoch 5 step 1338: training loss: 310.7673309209659\n",
      "Epoch 5 step 1339: training accuarcy: 0.9965\n",
      "Epoch 5 step 1339: training loss: 328.7119308703907\n",
      "Epoch 5 step 1340: training accuarcy: 0.9965\n",
      "Epoch 5 step 1340: training loss: 314.82072335986913\n",
      "Epoch 5 step 1341: training accuarcy: 0.998\n",
      "Epoch 5 step 1341: training loss: 315.4183915866542\n",
      "Epoch 5 step 1342: training accuarcy: 0.996\n",
      "Epoch 5 step 1342: training loss: 320.77142423654925\n",
      "Epoch 5 step 1343: training accuarcy: 0.9975\n",
      "Epoch 5 step 1343: training loss: 322.9433104801718\n",
      "Epoch 5 step 1344: training accuarcy: 0.9975\n",
      "Epoch 5 step 1344: training loss: 318.4676005338315\n",
      "Epoch 5 step 1345: training accuarcy: 0.996\n",
      "Epoch 5 step 1345: training loss: 320.98877094332494\n",
      "Epoch 5 step 1346: training accuarcy: 0.9935\n",
      "Epoch 5 step 1346: training loss: 338.2013213329905\n",
      "Epoch 5 step 1347: training accuarcy: 0.9955\n",
      "Epoch 5 step 1347: training loss: 320.86163725371625\n",
      "Epoch 5 step 1348: training accuarcy: 0.9965\n",
      "Epoch 5 step 1348: training loss: 333.09629520318094\n",
      "Epoch 5 step 1349: training accuarcy: 0.9965\n",
      "Epoch 5 step 1349: training loss: 332.9092877882772\n",
      "Epoch 5 step 1350: training accuarcy: 0.997\n",
      "Epoch 5 step 1350: training loss: 337.05073890877895\n",
      "Epoch 5 step 1351: training accuarcy: 0.997\n",
      "Epoch 5 step 1351: training loss: 324.469792585193\n",
      "Epoch 5 step 1352: training accuarcy: 0.9965\n",
      "Epoch 5 step 1352: training loss: 321.106638467586\n",
      "Epoch 5 step 1353: training accuarcy: 0.996\n",
      "Epoch 5 step 1353: training loss: 326.84572135888436\n",
      "Epoch 5 step 1354: training accuarcy: 0.9965\n",
      "Epoch 5 step 1354: training loss: 325.62871823533806\n",
      "Epoch 5 step 1355: training accuarcy: 0.9955\n",
      "Epoch 5 step 1355: training loss: 320.8926318965948\n",
      "Epoch 5 step 1356: training accuarcy: 0.997\n",
      "Epoch 5 step 1356: training loss: 329.92704737528277\n",
      "Epoch 5 step 1357: training accuarcy: 0.9975\n",
      "Epoch 5 step 1357: training loss: 321.9632768151031\n",
      "Epoch 5 step 1358: training accuarcy: 0.998\n",
      "Epoch 5 step 1358: training loss: 329.044040073231\n",
      "Epoch 5 step 1359: training accuarcy: 0.9975\n",
      "Epoch 5 step 1359: training loss: 323.8979927853078\n",
      "Epoch 5 step 1360: training accuarcy: 0.996\n",
      "Epoch 5 step 1360: training loss: 330.52487921827037\n",
      "Epoch 5 step 1361: training accuarcy: 0.9975\n",
      "Epoch 5 step 1361: training loss: 340.30893252800513\n",
      "Epoch 5 step 1362: training accuarcy: 0.9925\n",
      "Epoch 5 step 1362: training loss: 332.954887089499\n",
      "Epoch 5 step 1363: training accuarcy: 0.9965\n",
      "Epoch 5 step 1363: training loss: 312.464174564078\n",
      "Epoch 5 step 1364: training accuarcy: 0.9965\n",
      "Epoch 5 step 1364: training loss: 332.20156532746364\n",
      "Epoch 5 step 1365: training accuarcy: 0.9965\n",
      "Epoch 5 step 1365: training loss: 309.4402646508902\n",
      "Epoch 5 step 1366: training accuarcy: 0.9975\n",
      "Epoch 5 step 1366: training loss: 327.55917295813236\n",
      "Epoch 5 step 1367: training accuarcy: 0.9945\n",
      "Epoch 5 step 1367: training loss: 346.48704111762675\n",
      "Epoch 5 step 1368: training accuarcy: 0.997\n",
      "Epoch 5 step 1368: training loss: 329.96834758267164\n",
      "Epoch 5 step 1369: training accuarcy: 0.991\n",
      "Epoch 5 step 1369: training loss: 336.0116968769796\n",
      "Epoch 5 step 1370: training accuarcy: 0.995\n",
      "Epoch 5 step 1370: training loss: 326.64999096848055\n",
      "Epoch 5 step 1371: training accuarcy: 0.999\n",
      "Epoch 5 step 1371: training loss: 322.405122050975\n",
      "Epoch 5 step 1372: training accuarcy: 0.998\n",
      "Epoch 5 step 1372: training loss: 325.3335953840931\n",
      "Epoch 5 step 1373: training accuarcy: 0.994\n",
      "Epoch 5 step 1373: training loss: 338.53615522734685\n",
      "Epoch 5 step 1374: training accuarcy: 0.995\n",
      "Epoch 5 step 1374: training loss: 319.0890005040944\n",
      "Epoch 5 step 1375: training accuarcy: 0.9985\n",
      "Epoch 5 step 1375: training loss: 320.3492196616371\n",
      "Epoch 5 step 1376: training accuarcy: 0.9975\n",
      "Epoch 5 step 1376: training loss: 319.9169388382903\n",
      "Epoch 5 step 1377: training accuarcy: 0.996\n",
      "Epoch 5 step 1377: training loss: 324.9617483293245\n",
      "Epoch 5 step 1378: training accuarcy: 0.996\n",
      "Epoch 5 step 1378: training loss: 326.852067289719\n",
      "Epoch 5 step 1379: training accuarcy: 0.9965\n",
      "Epoch 5 step 1379: training loss: 326.1124628250973\n",
      "Epoch 5 step 1380: training accuarcy: 0.997\n",
      "Epoch 5 step 1380: training loss: 327.27072158043484\n",
      "Epoch 5 step 1381: training accuarcy: 0.996\n",
      "Epoch 5 step 1381: training loss: 313.9840240812808\n",
      "Epoch 5 step 1382: training accuarcy: 0.9975\n",
      "Epoch 5 step 1382: training loss: 332.1167707114462\n",
      "Epoch 5 step 1383: training accuarcy: 0.997\n",
      "Epoch 5 step 1383: training loss: 324.7892677722971\n",
      "Epoch 5 step 1384: training accuarcy: 0.9945\n",
      "Epoch 5 step 1384: training loss: 329.03725685093514\n",
      "Epoch 5 step 1385: training accuarcy: 0.9975\n",
      "Epoch 5 step 1385: training loss: 331.5087536280007\n",
      "Epoch 5 step 1386: training accuarcy: 0.995\n",
      "Epoch 5 step 1386: training loss: 331.3211656581059\n",
      "Epoch 5 step 1387: training accuarcy: 0.9965\n",
      "Epoch 5 step 1387: training loss: 339.40244701317533\n",
      "Epoch 5 step 1388: training accuarcy: 0.996\n",
      "Epoch 5 step 1388: training loss: 313.68200106305994\n",
      "Epoch 5 step 1389: training accuarcy: 0.9965\n",
      "Epoch 5 step 1389: training loss: 338.10183411336834\n",
      "Epoch 5 step 1390: training accuarcy: 0.9955\n",
      "Epoch 5 step 1390: training loss: 334.181549379255\n",
      "Epoch 5 step 1391: training accuarcy: 0.996\n",
      "Epoch 5 step 1391: training loss: 316.0594003483278\n",
      "Epoch 5 step 1392: training accuarcy: 0.9965\n",
      "Epoch 5 step 1392: training loss: 327.65194912846135\n",
      "Epoch 5 step 1393: training accuarcy: 0.993\n",
      "Epoch 5 step 1393: training loss: 334.5617962160321\n",
      "Epoch 5 step 1394: training accuarcy: 0.997\n",
      "Epoch 5 step 1394: training loss: 321.919765612861\n",
      "Epoch 5 step 1395: training accuarcy: 0.998\n",
      "Epoch 5 step 1395: training loss: 313.17104164502416\n",
      "Epoch 5 step 1396: training accuarcy: 0.998\n",
      "Epoch 5 step 1396: training loss: 328.65069294691034\n",
      "Epoch 5 step 1397: training accuarcy: 0.9965\n",
      "Epoch 5 step 1397: training loss: 335.0984473530232\n",
      "Epoch 5 step 1398: training accuarcy: 0.995\n",
      "Epoch 5 step 1398: training loss: 324.7054529202701\n",
      "Epoch 5 step 1399: training accuarcy: 0.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1399: training loss: 333.1297917416314\n",
      "Epoch 5 step 1400: training accuarcy: 0.9935\n",
      "Epoch 5 step 1400: training loss: 317.84959706139466\n",
      "Epoch 5 step 1401: training accuarcy: 0.995\n",
      "Epoch 5 step 1401: training loss: 333.7308168298234\n",
      "Epoch 5 step 1402: training accuarcy: 0.994\n",
      "Epoch 5 step 1402: training loss: 337.80462862497615\n",
      "Epoch 5 step 1403: training accuarcy: 0.996\n",
      "Epoch 5 step 1403: training loss: 327.70672903202126\n",
      "Epoch 5 step 1404: training accuarcy: 0.9965\n",
      "Epoch 5 step 1404: training loss: 322.03576450288915\n",
      "Epoch 5 step 1405: training accuarcy: 0.996\n",
      "Epoch 5 step 1405: training loss: 337.13389299473124\n",
      "Epoch 5 step 1406: training accuarcy: 0.9965\n",
      "Epoch 5 step 1406: training loss: 326.72214002075674\n",
      "Epoch 5 step 1407: training accuarcy: 0.9955\n",
      "Epoch 5 step 1407: training loss: 331.5398332722882\n",
      "Epoch 5 step 1408: training accuarcy: 0.9965\n",
      "Epoch 5 step 1408: training loss: 333.28833341334155\n",
      "Epoch 5 step 1409: training accuarcy: 0.996\n",
      "Epoch 5 step 1409: training loss: 316.17275956545285\n",
      "Epoch 5 step 1410: training accuarcy: 0.9975\n",
      "Epoch 5 step 1410: training loss: 335.5176336607291\n",
      "Epoch 5 step 1411: training accuarcy: 0.9945\n",
      "Epoch 5 step 1411: training loss: 325.97581671039745\n",
      "Epoch 5 step 1412: training accuarcy: 0.994\n",
      "Epoch 5 step 1412: training loss: 322.9285558874028\n",
      "Epoch 5 step 1413: training accuarcy: 0.9975\n",
      "Epoch 5 step 1413: training loss: 326.72379331534785\n",
      "Epoch 5 step 1414: training accuarcy: 0.9955\n",
      "Epoch 5 step 1414: training loss: 347.70822885445193\n",
      "Epoch 5 step 1415: training accuarcy: 0.995\n",
      "Epoch 5 step 1415: training loss: 325.99181727538866\n",
      "Epoch 5 step 1416: training accuarcy: 0.9955\n",
      "Epoch 5 step 1416: training loss: 319.2330983045122\n",
      "Epoch 5 step 1417: training accuarcy: 0.9975\n",
      "Epoch 5 step 1417: training loss: 335.02691700409036\n",
      "Epoch 5 step 1418: training accuarcy: 0.9965\n",
      "Epoch 5 step 1418: training loss: 318.7602729743562\n",
      "Epoch 5 step 1419: training accuarcy: 0.9965\n",
      "Epoch 5 step 1419: training loss: 314.0468946813221\n",
      "Epoch 5 step 1420: training accuarcy: 0.9985\n",
      "Epoch 5 step 1420: training loss: 335.67575081857467\n",
      "Epoch 5 step 1421: training accuarcy: 0.9945\n",
      "Epoch 5 step 1421: training loss: 317.2537262831916\n",
      "Epoch 5 step 1422: training accuarcy: 0.996\n",
      "Epoch 5 step 1422: training loss: 327.7163416588879\n",
      "Epoch 5 step 1423: training accuarcy: 0.9975\n",
      "Epoch 5 step 1423: training loss: 329.5963653699006\n",
      "Epoch 5 step 1424: training accuarcy: 0.9975\n",
      "Epoch 5 step 1424: training loss: 316.0407455063818\n",
      "Epoch 5 step 1425: training accuarcy: 0.997\n",
      "Epoch 5 step 1425: training loss: 328.6852930076271\n",
      "Epoch 5 step 1426: training accuarcy: 0.9975\n",
      "Epoch 5 step 1426: training loss: 319.44191106772666\n",
      "Epoch 5 step 1427: training accuarcy: 0.9955\n",
      "Epoch 5 step 1427: training loss: 326.99458790655655\n",
      "Epoch 5 step 1428: training accuarcy: 0.995\n",
      "Epoch 5 step 1428: training loss: 324.4569299426313\n",
      "Epoch 5 step 1429: training accuarcy: 0.995\n",
      "Epoch 5 step 1429: training loss: 322.0273964390145\n",
      "Epoch 5 step 1430: training accuarcy: 0.996\n",
      "Epoch 5 step 1430: training loss: 328.7848460234982\n",
      "Epoch 5 step 1431: training accuarcy: 0.9965\n",
      "Epoch 5 step 1431: training loss: 327.0276982996289\n",
      "Epoch 5 step 1432: training accuarcy: 0.997\n",
      "Epoch 5 step 1432: training loss: 322.1239768396401\n",
      "Epoch 5 step 1433: training accuarcy: 0.9965\n",
      "Epoch 5 step 1433: training loss: 326.7356697811657\n",
      "Epoch 5 step 1434: training accuarcy: 0.9975\n",
      "Epoch 5 step 1434: training loss: 324.8499819166117\n",
      "Epoch 5 step 1435: training accuarcy: 0.995\n",
      "Epoch 5 step 1435: training loss: 321.26852061793693\n",
      "Epoch 5 step 1436: training accuarcy: 0.9985\n",
      "Epoch 5 step 1436: training loss: 323.57407860617343\n",
      "Epoch 5 step 1437: training accuarcy: 0.994\n",
      "Epoch 5 step 1437: training loss: 332.78969658288287\n",
      "Epoch 5 step 1438: training accuarcy: 0.996\n",
      "Epoch 5 step 1438: training loss: 323.3802009547395\n",
      "Epoch 5 step 1439: training accuarcy: 0.9965\n",
      "Epoch 5 step 1439: training loss: 327.9719373872884\n",
      "Epoch 5 step 1440: training accuarcy: 0.996\n",
      "Epoch 5 step 1440: training loss: 321.8772150104321\n",
      "Epoch 5 step 1441: training accuarcy: 0.998\n",
      "Epoch 5 step 1441: training loss: 334.2774840414428\n",
      "Epoch 5 step 1442: training accuarcy: 0.9965\n",
      "Epoch 5 step 1442: training loss: 320.3777825154143\n",
      "Epoch 5 step 1443: training accuarcy: 0.997\n",
      "Epoch 5 step 1443: training loss: 339.7651931251585\n",
      "Epoch 5 step 1444: training accuarcy: 0.997\n",
      "Epoch 5 step 1444: training loss: 325.5208859485639\n",
      "Epoch 5 step 1445: training accuarcy: 0.997\n",
      "Epoch 5 step 1445: training loss: 341.20221768339724\n",
      "Epoch 5 step 1446: training accuarcy: 0.993\n",
      "Epoch 5 step 1446: training loss: 334.44597417322683\n",
      "Epoch 5 step 1447: training accuarcy: 0.994\n",
      "Epoch 5 step 1447: training loss: 320.0636180287921\n",
      "Epoch 5 step 1448: training accuarcy: 0.9955\n",
      "Epoch 5 step 1448: training loss: 318.6372335827489\n",
      "Epoch 5 step 1449: training accuarcy: 0.9955\n",
      "Epoch 5 step 1449: training loss: 325.2177858410277\n",
      "Epoch 5 step 1450: training accuarcy: 0.9955\n",
      "Epoch 5 step 1450: training loss: 334.95537489871384\n",
      "Epoch 5 step 1451: training accuarcy: 0.994\n",
      "Epoch 5 step 1451: training loss: 325.25089859388595\n",
      "Epoch 5 step 1452: training accuarcy: 0.995\n",
      "Epoch 5 step 1452: training loss: 320.8275148203396\n",
      "Epoch 5 step 1453: training accuarcy: 0.9945\n",
      "Epoch 5 step 1453: training loss: 328.6688280146009\n",
      "Epoch 5 step 1454: training accuarcy: 0.9965\n",
      "Epoch 5 step 1454: training loss: 313.1429366689081\n",
      "Epoch 5 step 1455: training accuarcy: 0.996\n",
      "Epoch 5 step 1455: training loss: 324.50559772566123\n",
      "Epoch 5 step 1456: training accuarcy: 0.997\n",
      "Epoch 5 step 1456: training loss: 320.65489923304045\n",
      "Epoch 5 step 1457: training accuarcy: 0.9975\n",
      "Epoch 5 step 1457: training loss: 323.06578048004917\n",
      "Epoch 5 step 1458: training accuarcy: 0.9955\n",
      "Epoch 5 step 1458: training loss: 327.4308296749935\n",
      "Epoch 5 step 1459: training accuarcy: 0.998\n",
      "Epoch 5 step 1459: training loss: 330.25376438573596\n",
      "Epoch 5 step 1460: training accuarcy: 0.996\n",
      "Epoch 5 step 1460: training loss: 319.2667587234202\n",
      "Epoch 5 step 1461: training accuarcy: 0.995\n",
      "Epoch 5 step 1461: training loss: 328.82907480856295\n",
      "Epoch 5 step 1462: training accuarcy: 0.9965\n",
      "Epoch 5 step 1462: training loss: 325.6599622627906\n",
      "Epoch 5 step 1463: training accuarcy: 0.9955\n",
      "Epoch 5 step 1463: training loss: 330.1217502629669\n",
      "Epoch 5 step 1464: training accuarcy: 0.9965\n",
      "Epoch 5 step 1464: training loss: 327.4816467653076\n",
      "Epoch 5 step 1465: training accuarcy: 0.996\n",
      "Epoch 5 step 1465: training loss: 319.00223972554465\n",
      "Epoch 5 step 1466: training accuarcy: 0.996\n",
      "Epoch 5 step 1466: training loss: 333.6873662411109\n",
      "Epoch 5 step 1467: training accuarcy: 0.9975\n",
      "Epoch 5 step 1467: training loss: 322.3414866661362\n",
      "Epoch 5 step 1468: training accuarcy: 0.995\n",
      "Epoch 5 step 1468: training loss: 323.2766695907566\n",
      "Epoch 5 step 1469: training accuarcy: 0.996\n",
      "Epoch 5 step 1469: training loss: 316.4720998968449\n",
      "Epoch 5 step 1470: training accuarcy: 0.9965\n",
      "Epoch 5 step 1470: training loss: 323.00198253590275\n",
      "Epoch 5 step 1471: training accuarcy: 0.9995\n",
      "Epoch 5 step 1471: training loss: 315.87201058405265\n",
      "Epoch 5 step 1472: training accuarcy: 0.996\n",
      "Epoch 5 step 1472: training loss: 338.07533832995745\n",
      "Epoch 5 step 1473: training accuarcy: 0.9945\n",
      "Epoch 5 step 1473: training loss: 325.2655553439366\n",
      "Epoch 5 step 1474: training accuarcy: 0.9975\n",
      "Epoch 5 step 1474: training loss: 318.74509436605877\n",
      "Epoch 5 step 1475: training accuarcy: 0.9975\n",
      "Epoch 5 step 1475: training loss: 315.18467149677457\n",
      "Epoch 5 step 1476: training accuarcy: 0.9985\n",
      "Epoch 5 step 1476: training loss: 319.84288959394576\n",
      "Epoch 5 step 1477: training accuarcy: 0.995\n",
      "Epoch 5 step 1477: training loss: 320.74885650414456\n",
      "Epoch 5 step 1478: training accuarcy: 0.9975\n",
      "Epoch 5 step 1478: training loss: 330.81374784042487\n",
      "Epoch 5 step 1479: training accuarcy: 0.9975\n",
      "Epoch 5 step 1479: training loss: 330.02306844032364\n",
      "Epoch 5 step 1480: training accuarcy: 0.9965\n",
      "Epoch 5 step 1480: training loss: 333.0489030953736\n",
      "Epoch 5 step 1481: training accuarcy: 0.9935\n",
      "Epoch 5 step 1481: training loss: 324.8605334720992\n",
      "Epoch 5 step 1482: training accuarcy: 0.997\n",
      "Epoch 5 step 1482: training loss: 335.8172871804595\n",
      "Epoch 5 step 1483: training accuarcy: 0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1483: training loss: 330.76137930032814\n",
      "Epoch 5 step 1484: training accuarcy: 0.993\n",
      "Epoch 5 step 1484: training loss: 333.51524889084703\n",
      "Epoch 5 step 1485: training accuarcy: 0.993\n",
      "Epoch 5 step 1485: training loss: 324.49771735477634\n",
      "Epoch 5 step 1486: training accuarcy: 0.9965\n",
      "Epoch 5 step 1486: training loss: 338.13683528146214\n",
      "Epoch 5 step 1487: training accuarcy: 0.9945\n",
      "Epoch 5 step 1487: training loss: 323.2803831897743\n",
      "Epoch 5 step 1488: training accuarcy: 0.997\n",
      "Epoch 5 step 1488: training loss: 323.26534601577896\n",
      "Epoch 5 step 1489: training accuarcy: 0.998\n",
      "Epoch 5 step 1489: training loss: 327.59787807575344\n",
      "Epoch 5 step 1490: training accuarcy: 0.995\n",
      "Epoch 5 step 1490: training loss: 330.1775619091519\n",
      "Epoch 5 step 1491: training accuarcy: 0.9965\n",
      "Epoch 5 step 1491: training loss: 338.83962460807874\n",
      "Epoch 5 step 1492: training accuarcy: 0.995\n",
      "Epoch 5 step 1492: training loss: 319.9235989924775\n",
      "Epoch 5 step 1493: training accuarcy: 0.9965\n",
      "Epoch 5 step 1493: training loss: 335.7901658642563\n",
      "Epoch 5 step 1494: training accuarcy: 0.998\n",
      "Epoch 5 step 1494: training loss: 328.63690301082875\n",
      "Epoch 5 step 1495: training accuarcy: 0.9935\n",
      "Epoch 5 step 1495: training loss: 329.75580301678065\n",
      "Epoch 5 step 1496: training accuarcy: 0.995\n",
      "Epoch 5 step 1496: training loss: 330.1912661845935\n",
      "Epoch 5 step 1497: training accuarcy: 0.9955\n",
      "Epoch 5 step 1497: training loss: 326.39448397550865\n",
      "Epoch 5 step 1498: training accuarcy: 0.9945\n",
      "Epoch 5 step 1498: training loss: 340.6699135127359\n",
      "Epoch 5 step 1499: training accuarcy: 0.9945\n",
      "Epoch 5 step 1499: training loss: 323.05919125333503\n",
      "Epoch 5 step 1500: training accuarcy: 0.9965\n",
      "Epoch 5 step 1500: training loss: 323.00001955228316\n",
      "Epoch 5 step 1501: training accuarcy: 0.996\n",
      "Epoch 5 step 1501: training loss: 326.34743182673793\n",
      "Epoch 5 step 1502: training accuarcy: 0.9955\n",
      "Epoch 5 step 1502: training loss: 320.7879809388294\n",
      "Epoch 5 step 1503: training accuarcy: 0.9975\n",
      "Epoch 5 step 1503: training loss: 329.5122315230293\n",
      "Epoch 5 step 1504: training accuarcy: 0.996\n",
      "Epoch 5 step 1504: training loss: 317.81002054284636\n",
      "Epoch 5 step 1505: training accuarcy: 0.9955\n",
      "Epoch 5 step 1505: training loss: 317.1166787158659\n",
      "Epoch 5 step 1506: training accuarcy: 0.9975\n",
      "Epoch 5 step 1506: training loss: 324.2603622036546\n",
      "Epoch 5 step 1507: training accuarcy: 0.9975\n",
      "Epoch 5 step 1507: training loss: 331.09734424478114\n",
      "Epoch 5 step 1508: training accuarcy: 0.9945\n",
      "Epoch 5 step 1508: training loss: 329.6654326575625\n",
      "Epoch 5 step 1509: training accuarcy: 0.9975\n",
      "Epoch 5 step 1509: training loss: 323.13146007935177\n",
      "Epoch 5 step 1510: training accuarcy: 0.994\n",
      "Epoch 5 step 1510: training loss: 338.3924147764344\n",
      "Epoch 5 step 1511: training accuarcy: 0.995\n",
      "Epoch 5 step 1511: training loss: 338.2910223322425\n",
      "Epoch 5 step 1512: training accuarcy: 0.995\n",
      "Epoch 5 step 1512: training loss: 325.9906205786216\n",
      "Epoch 5 step 1513: training accuarcy: 0.994\n",
      "Epoch 5 step 1513: training loss: 322.58897907903633\n",
      "Epoch 5 step 1514: training accuarcy: 0.997\n",
      "Epoch 5 step 1514: training loss: 326.59734592409427\n",
      "Epoch 5 step 1515: training accuarcy: 0.995\n",
      "Epoch 5 step 1515: training loss: 332.08298402477\n",
      "Epoch 5 step 1516: training accuarcy: 0.996\n",
      "Epoch 5 step 1516: training loss: 342.47849725755026\n",
      "Epoch 5 step 1517: training accuarcy: 0.993\n",
      "Epoch 5 step 1517: training loss: 323.23778210449063\n",
      "Epoch 5 step 1518: training accuarcy: 0.995\n",
      "Epoch 5 step 1518: training loss: 328.18512865448656\n",
      "Epoch 5 step 1519: training accuarcy: 0.9945\n",
      "Epoch 5 step 1519: training loss: 325.3070079632359\n",
      "Epoch 5 step 1520: training accuarcy: 0.9955\n",
      "Epoch 5 step 1520: training loss: 324.58985753335264\n",
      "Epoch 5 step 1521: training accuarcy: 0.9965\n",
      "Epoch 5 step 1521: training loss: 328.0398590761748\n",
      "Epoch 5 step 1522: training accuarcy: 0.9955\n",
      "Epoch 5 step 1522: training loss: 331.21063235807617\n",
      "Epoch 5 step 1523: training accuarcy: 0.9945\n",
      "Epoch 5 step 1523: training loss: 331.0233456812349\n",
      "Epoch 5 step 1524: training accuarcy: 0.9965\n",
      "Epoch 5 step 1524: training loss: 333.97103761555866\n",
      "Epoch 5 step 1525: training accuarcy: 0.9975\n",
      "Epoch 5 step 1525: training loss: 314.21874157601667\n",
      "Epoch 5 step 1526: training accuarcy: 0.997\n",
      "Epoch 5 step 1526: training loss: 325.7344409560127\n",
      "Epoch 5 step 1527: training accuarcy: 0.998\n",
      "Epoch 5 step 1527: training loss: 336.45020148315035\n",
      "Epoch 5 step 1528: training accuarcy: 0.9945\n",
      "Epoch 5 step 1528: training loss: 331.9014001971914\n",
      "Epoch 5 step 1529: training accuarcy: 0.9965\n",
      "Epoch 5 step 1529: training loss: 334.2966701143914\n",
      "Epoch 5 step 1530: training accuarcy: 0.9945\n",
      "Epoch 5 step 1530: training loss: 324.7067698051885\n",
      "Epoch 5 step 1531: training accuarcy: 0.994\n",
      "Epoch 5 step 1531: training loss: 326.4983384141899\n",
      "Epoch 5 step 1532: training accuarcy: 0.9955\n",
      "Epoch 5 step 1532: training loss: 327.1981652238726\n",
      "Epoch 5 step 1533: training accuarcy: 0.998\n",
      "Epoch 5 step 1533: training loss: 340.92483876664477\n",
      "Epoch 5 step 1534: training accuarcy: 0.995\n",
      "Epoch 5 step 1534: training loss: 333.60254061529633\n",
      "Epoch 5 step 1535: training accuarcy: 0.998\n",
      "Epoch 5 step 1535: training loss: 331.9274380295355\n",
      "Epoch 5 step 1536: training accuarcy: 0.9955\n",
      "Epoch 5 step 1536: training loss: 330.1066921614862\n",
      "Epoch 5 step 1537: training accuarcy: 0.994\n",
      "Epoch 5 step 1537: training loss: 318.8329637717103\n",
      "Epoch 5 step 1538: training accuarcy: 0.9975\n",
      "Epoch 5 step 1538: training loss: 313.985401664704\n",
      "Epoch 5 step 1539: training accuarcy: 0.9975\n",
      "Epoch 5 step 1539: training loss: 316.28273151111836\n",
      "Epoch 5 step 1540: training accuarcy: 0.996\n",
      "Epoch 5 step 1540: training loss: 328.69154623833003\n",
      "Epoch 5 step 1541: training accuarcy: 0.997\n",
      "Epoch 5 step 1541: training loss: 326.82421104021626\n",
      "Epoch 5 step 1542: training accuarcy: 0.997\n",
      "Epoch 5 step 1542: training loss: 330.67063075264656\n",
      "Epoch 5 step 1543: training accuarcy: 0.997\n",
      "Epoch 5 step 1543: training loss: 330.0000354930553\n",
      "Epoch 5 step 1544: training accuarcy: 0.9945\n",
      "Epoch 5 step 1544: training loss: 338.1308410081341\n",
      "Epoch 5 step 1545: training accuarcy: 0.9975\n",
      "Epoch 5 step 1545: training loss: 330.54504130782755\n",
      "Epoch 5 step 1546: training accuarcy: 0.9965\n",
      "Epoch 5 step 1546: training loss: 309.05660260130816\n",
      "Epoch 5 step 1547: training accuarcy: 0.9985\n",
      "Epoch 5 step 1547: training loss: 319.57926989503136\n",
      "Epoch 5 step 1548: training accuarcy: 0.996\n",
      "Epoch 5 step 1548: training loss: 332.13049546077684\n",
      "Epoch 5 step 1549: training accuarcy: 0.9945\n",
      "Epoch 5 step 1549: training loss: 336.8335466538614\n",
      "Epoch 5 step 1550: training accuarcy: 0.994\n",
      "Epoch 5 step 1550: training loss: 345.3058683737696\n",
      "Epoch 5 step 1551: training accuarcy: 0.995\n",
      "Epoch 5 step 1551: training loss: 344.5339280204142\n",
      "Epoch 5 step 1552: training accuarcy: 0.997\n",
      "Epoch 5 step 1552: training loss: 321.72779333428866\n",
      "Epoch 5 step 1553: training accuarcy: 0.995\n",
      "Epoch 5 step 1553: training loss: 327.3564414496252\n",
      "Epoch 5 step 1554: training accuarcy: 0.9965\n",
      "Epoch 5 step 1554: training loss: 327.3322980450682\n",
      "Epoch 5 step 1555: training accuarcy: 0.9965\n",
      "Epoch 5 step 1555: training loss: 334.42326059892764\n",
      "Epoch 5 step 1556: training accuarcy: 0.9975\n",
      "Epoch 5 step 1556: training loss: 320.47239785904367\n",
      "Epoch 5 step 1557: training accuarcy: 0.997\n",
      "Epoch 5 step 1557: training loss: 329.56732342727923\n",
      "Epoch 5 step 1558: training accuarcy: 0.997\n",
      "Epoch 5 step 1558: training loss: 329.34986978618383\n",
      "Epoch 5 step 1559: training accuarcy: 0.9965\n",
      "Epoch 5 step 1559: training loss: 328.24829922602623\n",
      "Epoch 5 step 1560: training accuarcy: 0.993\n",
      "Epoch 5 step 1560: training loss: 312.2856149454645\n",
      "Epoch 5 step 1561: training accuarcy: 0.996\n",
      "Epoch 5 step 1561: training loss: 323.5741066817012\n",
      "Epoch 5 step 1562: training accuarcy: 0.9975\n",
      "Epoch 5 step 1562: training loss: 320.7555020324527\n",
      "Epoch 5 step 1563: training accuarcy: 0.9965\n",
      "Epoch 5 step 1563: training loss: 330.96931169026715\n",
      "Epoch 5 step 1564: training accuarcy: 0.9965\n",
      "Epoch 5 step 1564: training loss: 321.92692102872945\n",
      "Epoch 5 step 1565: training accuarcy: 0.996\n",
      "Epoch 5 step 1565: training loss: 328.85342156290903\n",
      "Epoch 5 step 1566: training accuarcy: 0.9955\n",
      "Epoch 5 step 1566: training loss: 338.3281366753097\n",
      "Epoch 5 step 1567: training accuarcy: 0.9945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1567: training loss: 337.3905831498827\n",
      "Epoch 5 step 1568: training accuarcy: 0.9955\n",
      "Epoch 5 step 1568: training loss: 329.86748872902007\n",
      "Epoch 5 step 1569: training accuarcy: 0.994\n",
      "Epoch 5 step 1569: training loss: 322.45900910224543\n",
      "Epoch 5 step 1570: training accuarcy: 0.995\n",
      "Epoch 5 step 1570: training loss: 335.1481004471458\n",
      "Epoch 5 step 1571: training accuarcy: 0.997\n",
      "Epoch 5 step 1571: training loss: 335.2599903073251\n",
      "Epoch 5 step 1572: training accuarcy: 0.997\n",
      "Epoch 5 step 1572: training loss: 327.94504995961927\n",
      "Epoch 5 step 1573: training accuarcy: 0.998\n",
      "Epoch 5 step 1573: training loss: 323.6124569384159\n",
      "Epoch 5 step 1574: training accuarcy: 0.9985\n",
      "Epoch 5 step 1574: training loss: 319.8922358030462\n",
      "Epoch 5 step 1575: training accuarcy: 0.997\n",
      "Epoch 5 step 1575: training loss: 330.40251445351544\n",
      "Epoch 5 step 1576: training accuarcy: 0.9955\n",
      "Epoch 5 step 1576: training loss: 334.9631085848649\n",
      "Epoch 5 step 1577: training accuarcy: 0.997\n",
      "Epoch 5 step 1577: training loss: 244.04203886230718\n",
      "Epoch 5 step 1578: training accuarcy: 0.9974358974358974\n",
      "Epoch 5: train loss 325.90624931153, train accuarcy 0.9509204030036926\n",
      "Epoch 5: valid loss 1019.6165550831131, valid accuarcy 0.9605821967124939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                      | 6/8 [11:43<03:50, 115.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 1578: training loss: 316.62284789303715\n",
      "Epoch 6 step 1579: training accuarcy: 0.9935\n",
      "Epoch 6 step 1579: training loss: 329.9828304113389\n",
      "Epoch 6 step 1580: training accuarcy: 0.996\n",
      "Epoch 6 step 1580: training loss: 323.5796471669129\n",
      "Epoch 6 step 1581: training accuarcy: 0.998\n",
      "Epoch 6 step 1581: training loss: 321.6963268416048\n",
      "Epoch 6 step 1582: training accuarcy: 0.9965\n",
      "Epoch 6 step 1582: training loss: 313.2548561400763\n",
      "Epoch 6 step 1583: training accuarcy: 0.994\n",
      "Epoch 6 step 1583: training loss: 309.1014486234674\n",
      "Epoch 6 step 1584: training accuarcy: 0.998\n",
      "Epoch 6 step 1584: training loss: 338.18399636405434\n",
      "Epoch 6 step 1585: training accuarcy: 0.992\n",
      "Epoch 6 step 1585: training loss: 311.5317243289815\n",
      "Epoch 6 step 1586: training accuarcy: 0.996\n",
      "Epoch 6 step 1586: training loss: 313.8682328264628\n",
      "Epoch 6 step 1587: training accuarcy: 0.9955\n",
      "Epoch 6 step 1587: training loss: 319.0030924558677\n",
      "Epoch 6 step 1588: training accuarcy: 0.9955\n",
      "Epoch 6 step 1588: training loss: 309.1455186039011\n",
      "Epoch 6 step 1589: training accuarcy: 0.9975\n",
      "Epoch 6 step 1589: training loss: 321.5022974177293\n",
      "Epoch 6 step 1590: training accuarcy: 0.997\n",
      "Epoch 6 step 1590: training loss: 302.70791694487684\n",
      "Epoch 6 step 1591: training accuarcy: 0.9935\n",
      "Epoch 6 step 1591: training loss: 322.2352715063058\n",
      "Epoch 6 step 1592: training accuarcy: 0.9955\n",
      "Epoch 6 step 1592: training loss: 323.0418059525331\n",
      "Epoch 6 step 1593: training accuarcy: 0.9985\n",
      "Epoch 6 step 1593: training loss: 330.7101446670431\n",
      "Epoch 6 step 1594: training accuarcy: 0.996\n",
      "Epoch 6 step 1594: training loss: 315.4319435623763\n",
      "Epoch 6 step 1595: training accuarcy: 0.9975\n",
      "Epoch 6 step 1595: training loss: 319.140855805715\n",
      "Epoch 6 step 1596: training accuarcy: 0.996\n",
      "Epoch 6 step 1596: training loss: 310.9185446551762\n",
      "Epoch 6 step 1597: training accuarcy: 0.9955\n",
      "Epoch 6 step 1597: training loss: 320.6478607013221\n",
      "Epoch 6 step 1598: training accuarcy: 0.9975\n",
      "Epoch 6 step 1598: training loss: 321.31505312152797\n",
      "Epoch 6 step 1599: training accuarcy: 0.9955\n",
      "Epoch 6 step 1599: training loss: 328.855958231924\n",
      "Epoch 6 step 1600: training accuarcy: 0.996\n",
      "Epoch 6 step 1600: training loss: 332.1943706431844\n",
      "Epoch 6 step 1601: training accuarcy: 0.997\n",
      "Epoch 6 step 1601: training loss: 327.5549100780921\n",
      "Epoch 6 step 1602: training accuarcy: 0.9955\n",
      "Epoch 6 step 1602: training loss: 307.2431729891649\n",
      "Epoch 6 step 1603: training accuarcy: 0.996\n",
      "Epoch 6 step 1603: training loss: 314.5488059730343\n",
      "Epoch 6 step 1604: training accuarcy: 0.998\n",
      "Epoch 6 step 1604: training loss: 326.4783146045648\n",
      "Epoch 6 step 1605: training accuarcy: 0.9965\n",
      "Epoch 6 step 1605: training loss: 317.41306402262967\n",
      "Epoch 6 step 1606: training accuarcy: 0.998\n",
      "Epoch 6 step 1606: training loss: 331.6415784309271\n",
      "Epoch 6 step 1607: training accuarcy: 0.997\n",
      "Epoch 6 step 1607: training loss: 331.59788615082323\n",
      "Epoch 6 step 1608: training accuarcy: 0.996\n",
      "Epoch 6 step 1608: training loss: 324.0965097622842\n",
      "Epoch 6 step 1609: training accuarcy: 0.997\n",
      "Epoch 6 step 1609: training loss: 321.7508086589985\n",
      "Epoch 6 step 1610: training accuarcy: 0.9955\n",
      "Epoch 6 step 1610: training loss: 324.3372952852736\n",
      "Epoch 6 step 1611: training accuarcy: 0.9965\n",
      "Epoch 6 step 1611: training loss: 327.79578673325335\n",
      "Epoch 6 step 1612: training accuarcy: 0.995\n",
      "Epoch 6 step 1612: training loss: 322.91964405848194\n",
      "Epoch 6 step 1613: training accuarcy: 0.9965\n",
      "Epoch 6 step 1613: training loss: 333.6663462014633\n",
      "Epoch 6 step 1614: training accuarcy: 0.997\n",
      "Epoch 6 step 1614: training loss: 318.2290199447887\n",
      "Epoch 6 step 1615: training accuarcy: 0.9975\n",
      "Epoch 6 step 1615: training loss: 321.32074309743444\n",
      "Epoch 6 step 1616: training accuarcy: 0.996\n",
      "Epoch 6 step 1616: training loss: 327.08450666642585\n",
      "Epoch 6 step 1617: training accuarcy: 0.9975\n",
      "Epoch 6 step 1617: training loss: 327.04766278678204\n",
      "Epoch 6 step 1618: training accuarcy: 0.9955\n",
      "Epoch 6 step 1618: training loss: 327.28597082543547\n",
      "Epoch 6 step 1619: training accuarcy: 0.994\n",
      "Epoch 6 step 1619: training loss: 332.19876570366904\n",
      "Epoch 6 step 1620: training accuarcy: 0.9955\n",
      "Epoch 6 step 1620: training loss: 342.3059407020418\n",
      "Epoch 6 step 1621: training accuarcy: 0.996\n",
      "Epoch 6 step 1621: training loss: 334.4624973596183\n",
      "Epoch 6 step 1622: training accuarcy: 0.9965\n",
      "Epoch 6 step 1622: training loss: 330.0772862927141\n",
      "Epoch 6 step 1623: training accuarcy: 0.997\n",
      "Epoch 6 step 1623: training loss: 331.8525782538693\n",
      "Epoch 6 step 1624: training accuarcy: 0.9955\n",
      "Epoch 6 step 1624: training loss: 333.1452581766745\n",
      "Epoch 6 step 1625: training accuarcy: 0.9965\n",
      "Epoch 6 step 1625: training loss: 337.0998427733041\n",
      "Epoch 6 step 1626: training accuarcy: 0.9975\n",
      "Epoch 6 step 1626: training loss: 328.2193192997553\n",
      "Epoch 6 step 1627: training accuarcy: 0.996\n",
      "Epoch 6 step 1627: training loss: 328.2399787912326\n",
      "Epoch 6 step 1628: training accuarcy: 0.995\n",
      "Epoch 6 step 1628: training loss: 325.320792928929\n",
      "Epoch 6 step 1629: training accuarcy: 0.995\n",
      "Epoch 6 step 1629: training loss: 338.1618987812285\n",
      "Epoch 6 step 1630: training accuarcy: 0.9975\n",
      "Epoch 6 step 1630: training loss: 322.9221265307941\n",
      "Epoch 6 step 1631: training accuarcy: 0.9965\n",
      "Epoch 6 step 1631: training loss: 330.51431531147705\n",
      "Epoch 6 step 1632: training accuarcy: 0.9985\n",
      "Epoch 6 step 1632: training loss: 326.94887733050507\n",
      "Epoch 6 step 1633: training accuarcy: 0.9965\n",
      "Epoch 6 step 1633: training loss: 334.723202782846\n",
      "Epoch 6 step 1634: training accuarcy: 0.9935\n",
      "Epoch 6 step 1634: training loss: 328.1327855366275\n",
      "Epoch 6 step 1635: training accuarcy: 0.992\n",
      "Epoch 6 step 1635: training loss: 330.6160193412211\n",
      "Epoch 6 step 1636: training accuarcy: 0.9975\n",
      "Epoch 6 step 1636: training loss: 337.9802760084586\n",
      "Epoch 6 step 1637: training accuarcy: 0.9955\n",
      "Epoch 6 step 1637: training loss: 327.252846556358\n",
      "Epoch 6 step 1638: training accuarcy: 0.9975\n",
      "Epoch 6 step 1638: training loss: 324.9999882886332\n",
      "Epoch 6 step 1639: training accuarcy: 0.998\n",
      "Epoch 6 step 1639: training loss: 329.58433278296843\n",
      "Epoch 6 step 1640: training accuarcy: 0.996\n",
      "Epoch 6 step 1640: training loss: 311.5063673481296\n",
      "Epoch 6 step 1641: training accuarcy: 0.9985\n",
      "Epoch 6 step 1641: training loss: 331.63563480648673\n",
      "Epoch 6 step 1642: training accuarcy: 0.9965\n",
      "Epoch 6 step 1642: training loss: 342.26477804810264\n",
      "Epoch 6 step 1643: training accuarcy: 0.996\n",
      "Epoch 6 step 1643: training loss: 327.5594326849556\n",
      "Epoch 6 step 1644: training accuarcy: 0.9955\n",
      "Epoch 6 step 1644: training loss: 329.9774888372931\n",
      "Epoch 6 step 1645: training accuarcy: 0.996\n",
      "Epoch 6 step 1645: training loss: 319.1628923920755\n",
      "Epoch 6 step 1646: training accuarcy: 0.9965\n",
      "Epoch 6 step 1646: training loss: 329.6028165358929\n",
      "Epoch 6 step 1647: training accuarcy: 0.9965\n",
      "Epoch 6 step 1647: training loss: 321.5438980630055\n",
      "Epoch 6 step 1648: training accuarcy: 0.994\n",
      "Epoch 6 step 1648: training loss: 318.0033259398335\n",
      "Epoch 6 step 1649: training accuarcy: 0.9985\n",
      "Epoch 6 step 1649: training loss: 337.8971681465623\n",
      "Epoch 6 step 1650: training accuarcy: 0.9975\n",
      "Epoch 6 step 1650: training loss: 323.8948629567906\n",
      "Epoch 6 step 1651: training accuarcy: 0.997\n",
      "Epoch 6 step 1651: training loss: 319.1217179340349\n",
      "Epoch 6 step 1652: training accuarcy: 0.996\n",
      "Epoch 6 step 1652: training loss: 333.5198359512868\n",
      "Epoch 6 step 1653: training accuarcy: 0.9935\n",
      "Epoch 6 step 1653: training loss: 322.3488786758261\n",
      "Epoch 6 step 1654: training accuarcy: 0.9975\n",
      "Epoch 6 step 1654: training loss: 317.6910050280875\n",
      "Epoch 6 step 1655: training accuarcy: 0.996\n",
      "Epoch 6 step 1655: training loss: 333.9418287493787\n",
      "Epoch 6 step 1656: training accuarcy: 0.9975\n",
      "Epoch 6 step 1656: training loss: 340.99604672041886\n",
      "Epoch 6 step 1657: training accuarcy: 0.994\n",
      "Epoch 6 step 1657: training loss: 318.11608358989577\n",
      "Epoch 6 step 1658: training accuarcy: 0.9975\n",
      "Epoch 6 step 1658: training loss: 326.04980858882817\n",
      "Epoch 6 step 1659: training accuarcy: 0.9965\n",
      "Epoch 6 step 1659: training loss: 331.2767187516148\n",
      "Epoch 6 step 1660: training accuarcy: 0.9975\n",
      "Epoch 6 step 1660: training loss: 326.2268523433339\n",
      "Epoch 6 step 1661: training accuarcy: 0.9965\n",
      "Epoch 6 step 1661: training loss: 331.3990888081721\n",
      "Epoch 6 step 1662: training accuarcy: 0.9925\n",
      "Epoch 6 step 1662: training loss: 333.1808785512321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1663: training accuarcy: 0.9955\n",
      "Epoch 6 step 1663: training loss: 316.72572420519026\n",
      "Epoch 6 step 1664: training accuarcy: 0.9965\n",
      "Epoch 6 step 1664: training loss: 326.06756223222226\n",
      "Epoch 6 step 1665: training accuarcy: 0.998\n",
      "Epoch 6 step 1665: training loss: 317.7778645679872\n",
      "Epoch 6 step 1666: training accuarcy: 0.9945\n",
      "Epoch 6 step 1666: training loss: 327.84942508850867\n",
      "Epoch 6 step 1667: training accuarcy: 0.9965\n",
      "Epoch 6 step 1667: training loss: 341.0111929779638\n",
      "Epoch 6 step 1668: training accuarcy: 0.997\n",
      "Epoch 6 step 1668: training loss: 329.97753278433936\n",
      "Epoch 6 step 1669: training accuarcy: 0.9965\n",
      "Epoch 6 step 1669: training loss: 319.108649503233\n",
      "Epoch 6 step 1670: training accuarcy: 0.998\n",
      "Epoch 6 step 1670: training loss: 327.5941052089297\n",
      "Epoch 6 step 1671: training accuarcy: 0.9955\n",
      "Epoch 6 step 1671: training loss: 315.35471951055115\n",
      "Epoch 6 step 1672: training accuarcy: 0.995\n",
      "Epoch 6 step 1672: training loss: 319.80512298675774\n",
      "Epoch 6 step 1673: training accuarcy: 0.997\n",
      "Epoch 6 step 1673: training loss: 335.4088790678403\n",
      "Epoch 6 step 1674: training accuarcy: 0.9975\n",
      "Epoch 6 step 1674: training loss: 319.56427224089805\n",
      "Epoch 6 step 1675: training accuarcy: 0.9965\n",
      "Epoch 6 step 1675: training loss: 317.43582274324496\n",
      "Epoch 6 step 1676: training accuarcy: 0.9975\n",
      "Epoch 6 step 1676: training loss: 331.0937149903473\n",
      "Epoch 6 step 1677: training accuarcy: 0.997\n",
      "Epoch 6 step 1677: training loss: 333.84774442796936\n",
      "Epoch 6 step 1678: training accuarcy: 0.9965\n",
      "Epoch 6 step 1678: training loss: 315.61103351890745\n",
      "Epoch 6 step 1679: training accuarcy: 0.9985\n",
      "Epoch 6 step 1679: training loss: 321.7440007133391\n",
      "Epoch 6 step 1680: training accuarcy: 0.9965\n",
      "Epoch 6 step 1680: training loss: 340.51742015127974\n",
      "Epoch 6 step 1681: training accuarcy: 0.994\n",
      "Epoch 6 step 1681: training loss: 325.19357806391133\n",
      "Epoch 6 step 1682: training accuarcy: 0.9985\n",
      "Epoch 6 step 1682: training loss: 330.9666481919413\n",
      "Epoch 6 step 1683: training accuarcy: 0.9955\n",
      "Epoch 6 step 1683: training loss: 327.0095782976196\n",
      "Epoch 6 step 1684: training accuarcy: 0.9935\n",
      "Epoch 6 step 1684: training loss: 326.9296127176567\n",
      "Epoch 6 step 1685: training accuarcy: 0.9975\n",
      "Epoch 6 step 1685: training loss: 324.51788272998374\n",
      "Epoch 6 step 1686: training accuarcy: 0.9935\n",
      "Epoch 6 step 1686: training loss: 332.27312059223243\n",
      "Epoch 6 step 1687: training accuarcy: 0.9955\n",
      "Epoch 6 step 1687: training loss: 306.49648175836285\n",
      "Epoch 6 step 1688: training accuarcy: 0.997\n",
      "Epoch 6 step 1688: training loss: 320.56048321873584\n",
      "Epoch 6 step 1689: training accuarcy: 0.996\n",
      "Epoch 6 step 1689: training loss: 333.0063792307019\n",
      "Epoch 6 step 1690: training accuarcy: 0.997\n",
      "Epoch 6 step 1690: training loss: 329.48527760561547\n",
      "Epoch 6 step 1691: training accuarcy: 0.9945\n",
      "Epoch 6 step 1691: training loss: 319.77334600935114\n",
      "Epoch 6 step 1692: training accuarcy: 0.9945\n",
      "Epoch 6 step 1692: training loss: 332.8600773083435\n",
      "Epoch 6 step 1693: training accuarcy: 0.9975\n",
      "Epoch 6 step 1693: training loss: 330.59338817380507\n",
      "Epoch 6 step 1694: training accuarcy: 0.9975\n",
      "Epoch 6 step 1694: training loss: 322.0613350110198\n",
      "Epoch 6 step 1695: training accuarcy: 0.9965\n",
      "Epoch 6 step 1695: training loss: 346.2474530574385\n",
      "Epoch 6 step 1696: training accuarcy: 0.9975\n",
      "Epoch 6 step 1696: training loss: 345.92481397756933\n",
      "Epoch 6 step 1697: training accuarcy: 0.994\n",
      "Epoch 6 step 1697: training loss: 325.13585823922205\n",
      "Epoch 6 step 1698: training accuarcy: 0.9995\n",
      "Epoch 6 step 1698: training loss: 330.16922626309565\n",
      "Epoch 6 step 1699: training accuarcy: 0.9975\n",
      "Epoch 6 step 1699: training loss: 330.3677794046932\n",
      "Epoch 6 step 1700: training accuarcy: 0.9945\n",
      "Epoch 6 step 1700: training loss: 321.159051590712\n",
      "Epoch 6 step 1701: training accuarcy: 0.998\n",
      "Epoch 6 step 1701: training loss: 328.65115517822016\n",
      "Epoch 6 step 1702: training accuarcy: 0.998\n",
      "Epoch 6 step 1702: training loss: 324.97389327480494\n",
      "Epoch 6 step 1703: training accuarcy: 0.9955\n",
      "Epoch 6 step 1703: training loss: 323.8265781160161\n",
      "Epoch 6 step 1704: training accuarcy: 0.996\n",
      "Epoch 6 step 1704: training loss: 334.5948410379719\n",
      "Epoch 6 step 1705: training accuarcy: 0.9955\n",
      "Epoch 6 step 1705: training loss: 320.51784901999144\n",
      "Epoch 6 step 1706: training accuarcy: 0.9965\n",
      "Epoch 6 step 1706: training loss: 329.1129108757277\n",
      "Epoch 6 step 1707: training accuarcy: 0.9935\n",
      "Epoch 6 step 1707: training loss: 322.9318228681577\n",
      "Epoch 6 step 1708: training accuarcy: 0.996\n",
      "Epoch 6 step 1708: training loss: 340.0432782508359\n",
      "Epoch 6 step 1709: training accuarcy: 0.996\n",
      "Epoch 6 step 1709: training loss: 334.37716216285634\n",
      "Epoch 6 step 1710: training accuarcy: 0.993\n",
      "Epoch 6 step 1710: training loss: 329.4620829509945\n",
      "Epoch 6 step 1711: training accuarcy: 0.9985\n",
      "Epoch 6 step 1711: training loss: 331.0037880042728\n",
      "Epoch 6 step 1712: training accuarcy: 0.996\n",
      "Epoch 6 step 1712: training loss: 332.7276247702504\n",
      "Epoch 6 step 1713: training accuarcy: 0.9975\n",
      "Epoch 6 step 1713: training loss: 339.98396544322827\n",
      "Epoch 6 step 1714: training accuarcy: 0.9965\n",
      "Epoch 6 step 1714: training loss: 329.09932471356035\n",
      "Epoch 6 step 1715: training accuarcy: 0.9965\n",
      "Epoch 6 step 1715: training loss: 320.0609047077444\n",
      "Epoch 6 step 1716: training accuarcy: 0.996\n",
      "Epoch 6 step 1716: training loss: 330.5530406855338\n",
      "Epoch 6 step 1717: training accuarcy: 0.996\n",
      "Epoch 6 step 1717: training loss: 330.49854761038307\n",
      "Epoch 6 step 1718: training accuarcy: 0.9955\n",
      "Epoch 6 step 1718: training loss: 325.4119148148003\n",
      "Epoch 6 step 1719: training accuarcy: 0.997\n",
      "Epoch 6 step 1719: training loss: 337.0227117806686\n",
      "Epoch 6 step 1720: training accuarcy: 0.9955\n",
      "Epoch 6 step 1720: training loss: 330.0021456273401\n",
      "Epoch 6 step 1721: training accuarcy: 0.9955\n",
      "Epoch 6 step 1721: training loss: 333.71091393243387\n",
      "Epoch 6 step 1722: training accuarcy: 0.9965\n",
      "Epoch 6 step 1722: training loss: 315.07976794317676\n",
      "Epoch 6 step 1723: training accuarcy: 0.9965\n",
      "Epoch 6 step 1723: training loss: 322.11455030899526\n",
      "Epoch 6 step 1724: training accuarcy: 0.996\n",
      "Epoch 6 step 1724: training loss: 322.2752799603533\n",
      "Epoch 6 step 1725: training accuarcy: 0.996\n",
      "Epoch 6 step 1725: training loss: 329.04396163572187\n",
      "Epoch 6 step 1726: training accuarcy: 0.996\n",
      "Epoch 6 step 1726: training loss: 331.1481306319747\n",
      "Epoch 6 step 1727: training accuarcy: 0.996\n",
      "Epoch 6 step 1727: training loss: 323.4725456829857\n",
      "Epoch 6 step 1728: training accuarcy: 0.9935\n",
      "Epoch 6 step 1728: training loss: 340.2677447453975\n",
      "Epoch 6 step 1729: training accuarcy: 0.993\n",
      "Epoch 6 step 1729: training loss: 329.3854976983202\n",
      "Epoch 6 step 1730: training accuarcy: 0.9945\n",
      "Epoch 6 step 1730: training loss: 322.7391307090543\n",
      "Epoch 6 step 1731: training accuarcy: 0.995\n",
      "Epoch 6 step 1731: training loss: 327.1449932508516\n",
      "Epoch 6 step 1732: training accuarcy: 0.9935\n",
      "Epoch 6 step 1732: training loss: 314.6011957616727\n",
      "Epoch 6 step 1733: training accuarcy: 0.9975\n",
      "Epoch 6 step 1733: training loss: 332.63294596263756\n",
      "Epoch 6 step 1734: training accuarcy: 0.9935\n",
      "Epoch 6 step 1734: training loss: 335.58333233665877\n",
      "Epoch 6 step 1735: training accuarcy: 0.995\n",
      "Epoch 6 step 1735: training loss: 321.12960350039066\n",
      "Epoch 6 step 1736: training accuarcy: 0.9965\n",
      "Epoch 6 step 1736: training loss: 328.6157630885512\n",
      "Epoch 6 step 1737: training accuarcy: 0.995\n",
      "Epoch 6 step 1737: training loss: 327.45472934867377\n",
      "Epoch 6 step 1738: training accuarcy: 0.9965\n",
      "Epoch 6 step 1738: training loss: 321.1900574678599\n",
      "Epoch 6 step 1739: training accuarcy: 0.9945\n",
      "Epoch 6 step 1739: training loss: 334.8989557300325\n",
      "Epoch 6 step 1740: training accuarcy: 0.9955\n",
      "Epoch 6 step 1740: training loss: 322.37669957161484\n",
      "Epoch 6 step 1741: training accuarcy: 0.997\n",
      "Epoch 6 step 1741: training loss: 312.96401265192554\n",
      "Epoch 6 step 1742: training accuarcy: 0.998\n",
      "Epoch 6 step 1742: training loss: 322.1778719928243\n",
      "Epoch 6 step 1743: training accuarcy: 0.998\n",
      "Epoch 6 step 1743: training loss: 331.68509571575123\n",
      "Epoch 6 step 1744: training accuarcy: 0.9955\n",
      "Epoch 6 step 1744: training loss: 324.8059246595002\n",
      "Epoch 6 step 1745: training accuarcy: 0.9975\n",
      "Epoch 6 step 1745: training loss: 325.0775582929734\n",
      "Epoch 6 step 1746: training accuarcy: 0.998\n",
      "Epoch 6 step 1746: training loss: 334.59026809300667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1747: training accuarcy: 0.991\n",
      "Epoch 6 step 1747: training loss: 339.0494114740451\n",
      "Epoch 6 step 1748: training accuarcy: 0.9945\n",
      "Epoch 6 step 1748: training loss: 334.21966006896344\n",
      "Epoch 6 step 1749: training accuarcy: 0.9965\n",
      "Epoch 6 step 1749: training loss: 344.9077539813636\n",
      "Epoch 6 step 1750: training accuarcy: 0.995\n",
      "Epoch 6 step 1750: training loss: 322.35832226725796\n",
      "Epoch 6 step 1751: training accuarcy: 0.997\n",
      "Epoch 6 step 1751: training loss: 329.32460510797637\n",
      "Epoch 6 step 1752: training accuarcy: 0.9975\n",
      "Epoch 6 step 1752: training loss: 338.50033102981115\n",
      "Epoch 6 step 1753: training accuarcy: 0.9945\n",
      "Epoch 6 step 1753: training loss: 320.93068927961366\n",
      "Epoch 6 step 1754: training accuarcy: 0.997\n",
      "Epoch 6 step 1754: training loss: 331.932300544874\n",
      "Epoch 6 step 1755: training accuarcy: 0.9955\n",
      "Epoch 6 step 1755: training loss: 320.04520426288104\n",
      "Epoch 6 step 1756: training accuarcy: 0.9955\n",
      "Epoch 6 step 1756: training loss: 326.3571632940833\n",
      "Epoch 6 step 1757: training accuarcy: 0.9955\n",
      "Epoch 6 step 1757: training loss: 333.84055453631015\n",
      "Epoch 6 step 1758: training accuarcy: 0.9935\n",
      "Epoch 6 step 1758: training loss: 324.53397576976505\n",
      "Epoch 6 step 1759: training accuarcy: 0.997\n",
      "Epoch 6 step 1759: training loss: 316.0365246023234\n",
      "Epoch 6 step 1760: training accuarcy: 0.996\n",
      "Epoch 6 step 1760: training loss: 350.2411109125624\n",
      "Epoch 6 step 1761: training accuarcy: 0.997\n",
      "Epoch 6 step 1761: training loss: 333.66253166468766\n",
      "Epoch 6 step 1762: training accuarcy: 0.9945\n",
      "Epoch 6 step 1762: training loss: 328.5316555604062\n",
      "Epoch 6 step 1763: training accuarcy: 0.9955\n",
      "Epoch 6 step 1763: training loss: 325.33647864712293\n",
      "Epoch 6 step 1764: training accuarcy: 0.993\n",
      "Epoch 6 step 1764: training loss: 321.4497168264717\n",
      "Epoch 6 step 1765: training accuarcy: 0.9955\n",
      "Epoch 6 step 1765: training loss: 331.77123093426206\n",
      "Epoch 6 step 1766: training accuarcy: 0.995\n",
      "Epoch 6 step 1766: training loss: 334.3663586125782\n",
      "Epoch 6 step 1767: training accuarcy: 0.994\n",
      "Epoch 6 step 1767: training loss: 324.8821393258487\n",
      "Epoch 6 step 1768: training accuarcy: 0.9955\n",
      "Epoch 6 step 1768: training loss: 329.0503234273383\n",
      "Epoch 6 step 1769: training accuarcy: 0.995\n",
      "Epoch 6 step 1769: training loss: 334.11362400812493\n",
      "Epoch 6 step 1770: training accuarcy: 0.9965\n",
      "Epoch 6 step 1770: training loss: 337.53945783289356\n",
      "Epoch 6 step 1771: training accuarcy: 0.997\n",
      "Epoch 6 step 1771: training loss: 337.7808602336918\n",
      "Epoch 6 step 1772: training accuarcy: 0.9985\n",
      "Epoch 6 step 1772: training loss: 342.39417020992653\n",
      "Epoch 6 step 1773: training accuarcy: 0.993\n",
      "Epoch 6 step 1773: training loss: 326.3401402171902\n",
      "Epoch 6 step 1774: training accuarcy: 0.9975\n",
      "Epoch 6 step 1774: training loss: 338.9218285685369\n",
      "Epoch 6 step 1775: training accuarcy: 0.9935\n",
      "Epoch 6 step 1775: training loss: 327.3669201351147\n",
      "Epoch 6 step 1776: training accuarcy: 0.9965\n",
      "Epoch 6 step 1776: training loss: 323.92856147102646\n",
      "Epoch 6 step 1777: training accuarcy: 0.996\n",
      "Epoch 6 step 1777: training loss: 333.5345546140568\n",
      "Epoch 6 step 1778: training accuarcy: 0.9955\n",
      "Epoch 6 step 1778: training loss: 334.0735491081234\n",
      "Epoch 6 step 1779: training accuarcy: 0.998\n",
      "Epoch 6 step 1779: training loss: 326.38680517891163\n",
      "Epoch 6 step 1780: training accuarcy: 0.994\n",
      "Epoch 6 step 1780: training loss: 333.34256558599327\n",
      "Epoch 6 step 1781: training accuarcy: 0.9945\n",
      "Epoch 6 step 1781: training loss: 331.73601797871146\n",
      "Epoch 6 step 1782: training accuarcy: 0.9955\n",
      "Epoch 6 step 1782: training loss: 331.8362905812523\n",
      "Epoch 6 step 1783: training accuarcy: 0.994\n",
      "Epoch 6 step 1783: training loss: 322.8870274242447\n",
      "Epoch 6 step 1784: training accuarcy: 0.9955\n",
      "Epoch 6 step 1784: training loss: 328.89306204471484\n",
      "Epoch 6 step 1785: training accuarcy: 0.997\n",
      "Epoch 6 step 1785: training loss: 332.771655916956\n",
      "Epoch 6 step 1786: training accuarcy: 0.997\n",
      "Epoch 6 step 1786: training loss: 324.6917395382731\n",
      "Epoch 6 step 1787: training accuarcy: 0.9965\n",
      "Epoch 6 step 1787: training loss: 334.21263094026074\n",
      "Epoch 6 step 1788: training accuarcy: 0.9955\n",
      "Epoch 6 step 1788: training loss: 328.8406368830781\n",
      "Epoch 6 step 1789: training accuarcy: 0.997\n",
      "Epoch 6 step 1789: training loss: 327.38339563909483\n",
      "Epoch 6 step 1790: training accuarcy: 0.996\n",
      "Epoch 6 step 1790: training loss: 323.1775417603932\n",
      "Epoch 6 step 1791: training accuarcy: 0.9975\n",
      "Epoch 6 step 1791: training loss: 336.5084869700095\n",
      "Epoch 6 step 1792: training accuarcy: 0.9935\n",
      "Epoch 6 step 1792: training loss: 331.6649979997768\n",
      "Epoch 6 step 1793: training accuarcy: 0.997\n",
      "Epoch 6 step 1793: training loss: 335.1950024219383\n",
      "Epoch 6 step 1794: training accuarcy: 0.997\n",
      "Epoch 6 step 1794: training loss: 325.0336389011263\n",
      "Epoch 6 step 1795: training accuarcy: 0.9965\n",
      "Epoch 6 step 1795: training loss: 323.40101997297165\n",
      "Epoch 6 step 1796: training accuarcy: 0.9955\n",
      "Epoch 6 step 1796: training loss: 336.0464616870306\n",
      "Epoch 6 step 1797: training accuarcy: 0.996\n",
      "Epoch 6 step 1797: training loss: 323.68331334766117\n",
      "Epoch 6 step 1798: training accuarcy: 0.9955\n",
      "Epoch 6 step 1798: training loss: 332.8291371151621\n",
      "Epoch 6 step 1799: training accuarcy: 0.996\n",
      "Epoch 6 step 1799: training loss: 322.44617486238553\n",
      "Epoch 6 step 1800: training accuarcy: 0.996\n",
      "Epoch 6 step 1800: training loss: 340.37152022841315\n",
      "Epoch 6 step 1801: training accuarcy: 0.997\n",
      "Epoch 6 step 1801: training loss: 334.36442434438686\n",
      "Epoch 6 step 1802: training accuarcy: 0.996\n",
      "Epoch 6 step 1802: training loss: 340.83203135929864\n",
      "Epoch 6 step 1803: training accuarcy: 0.9945\n",
      "Epoch 6 step 1803: training loss: 326.8098692375699\n",
      "Epoch 6 step 1804: training accuarcy: 0.998\n",
      "Epoch 6 step 1804: training loss: 327.4493634764061\n",
      "Epoch 6 step 1805: training accuarcy: 0.997\n",
      "Epoch 6 step 1805: training loss: 322.74174090270714\n",
      "Epoch 6 step 1806: training accuarcy: 0.9935\n",
      "Epoch 6 step 1806: training loss: 319.6578880379581\n",
      "Epoch 6 step 1807: training accuarcy: 0.9955\n",
      "Epoch 6 step 1807: training loss: 321.93838465434277\n",
      "Epoch 6 step 1808: training accuarcy: 0.9965\n",
      "Epoch 6 step 1808: training loss: 320.12220059026623\n",
      "Epoch 6 step 1809: training accuarcy: 0.996\n",
      "Epoch 6 step 1809: training loss: 333.00716197300824\n",
      "Epoch 6 step 1810: training accuarcy: 0.996\n",
      "Epoch 6 step 1810: training loss: 319.80913334857894\n",
      "Epoch 6 step 1811: training accuarcy: 0.9995\n",
      "Epoch 6 step 1811: training loss: 326.85348860260615\n",
      "Epoch 6 step 1812: training accuarcy: 0.995\n",
      "Epoch 6 step 1812: training loss: 331.48341484545074\n",
      "Epoch 6 step 1813: training accuarcy: 0.998\n",
      "Epoch 6 step 1813: training loss: 323.91640180056436\n",
      "Epoch 6 step 1814: training accuarcy: 0.9985\n",
      "Epoch 6 step 1814: training loss: 322.86555161425883\n",
      "Epoch 6 step 1815: training accuarcy: 0.997\n",
      "Epoch 6 step 1815: training loss: 334.204956095306\n",
      "Epoch 6 step 1816: training accuarcy: 0.9925\n",
      "Epoch 6 step 1816: training loss: 321.77172786158087\n",
      "Epoch 6 step 1817: training accuarcy: 0.9975\n",
      "Epoch 6 step 1817: training loss: 329.64249097492444\n",
      "Epoch 6 step 1818: training accuarcy: 0.9955\n",
      "Epoch 6 step 1818: training loss: 320.7947769955208\n",
      "Epoch 6 step 1819: training accuarcy: 0.9945\n",
      "Epoch 6 step 1819: training loss: 320.5393495914028\n",
      "Epoch 6 step 1820: training accuarcy: 0.9955\n",
      "Epoch 6 step 1820: training loss: 332.3989447990093\n",
      "Epoch 6 step 1821: training accuarcy: 0.9965\n",
      "Epoch 6 step 1821: training loss: 321.6746531365151\n",
      "Epoch 6 step 1822: training accuarcy: 0.996\n",
      "Epoch 6 step 1822: training loss: 325.09528528618375\n",
      "Epoch 6 step 1823: training accuarcy: 0.9965\n",
      "Epoch 6 step 1823: training loss: 347.1153477906387\n",
      "Epoch 6 step 1824: training accuarcy: 0.9975\n",
      "Epoch 6 step 1824: training loss: 328.88609567099644\n",
      "Epoch 6 step 1825: training accuarcy: 0.995\n",
      "Epoch 6 step 1825: training loss: 318.4686340082261\n",
      "Epoch 6 step 1826: training accuarcy: 0.9975\n",
      "Epoch 6 step 1826: training loss: 338.79466447301894\n",
      "Epoch 6 step 1827: training accuarcy: 0.997\n",
      "Epoch 6 step 1827: training loss: 330.85019757465307\n",
      "Epoch 6 step 1828: training accuarcy: 0.996\n",
      "Epoch 6 step 1828: training loss: 331.2700318714078\n",
      "Epoch 6 step 1829: training accuarcy: 0.996\n",
      "Epoch 6 step 1829: training loss: 325.3204508612877\n",
      "Epoch 6 step 1830: training accuarcy: 0.9945\n",
      "Epoch 6 step 1830: training loss: 334.0409038851052\n",
      "Epoch 6 step 1831: training accuarcy: 0.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1831: training loss: 333.19700528619444\n",
      "Epoch 6 step 1832: training accuarcy: 0.9965\n",
      "Epoch 6 step 1832: training loss: 324.19385579663594\n",
      "Epoch 6 step 1833: training accuarcy: 0.9985\n",
      "Epoch 6 step 1833: training loss: 327.84905327590195\n",
      "Epoch 6 step 1834: training accuarcy: 0.9955\n",
      "Epoch 6 step 1834: training loss: 330.01950764959605\n",
      "Epoch 6 step 1835: training accuarcy: 0.9955\n",
      "Epoch 6 step 1835: training loss: 322.93696679825666\n",
      "Epoch 6 step 1836: training accuarcy: 0.996\n",
      "Epoch 6 step 1836: training loss: 333.40553465839264\n",
      "Epoch 6 step 1837: training accuarcy: 0.997\n",
      "Epoch 6 step 1837: training loss: 329.8539937748701\n",
      "Epoch 6 step 1838: training accuarcy: 0.996\n",
      "Epoch 6 step 1838: training loss: 320.8889296170819\n",
      "Epoch 6 step 1839: training accuarcy: 0.9975\n",
      "Epoch 6 step 1839: training loss: 321.0241347919924\n",
      "Epoch 6 step 1840: training accuarcy: 0.9985\n",
      "Epoch 6 step 1840: training loss: 240.53445059041778\n",
      "Epoch 6 step 1841: training accuarcy: 1.0\n",
      "Epoch 6: train loss 327.1250683287368, train accuarcy 0.9485164880752563\n",
      "Epoch 6: valid loss 1025.499146685137, valid accuarcy 0.9587628841400146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                   | 7/8 [13:33<01:53, 113.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 1841: training loss: 329.3220929687893\n",
      "Epoch 7 step 1842: training accuarcy: 0.993\n",
      "Epoch 7 step 1842: training loss: 316.8665933907949\n",
      "Epoch 7 step 1843: training accuarcy: 0.9955\n",
      "Epoch 7 step 1843: training loss: 309.30408222590887\n",
      "Epoch 7 step 1844: training accuarcy: 0.9965\n",
      "Epoch 7 step 1844: training loss: 316.34608045385573\n",
      "Epoch 7 step 1845: training accuarcy: 0.998\n",
      "Epoch 7 step 1845: training loss: 319.88587668972855\n",
      "Epoch 7 step 1846: training accuarcy: 0.998\n",
      "Epoch 7 step 1846: training loss: 321.54724121306856\n",
      "Epoch 7 step 1847: training accuarcy: 0.9985\n",
      "Epoch 7 step 1847: training loss: 312.80154667536164\n",
      "Epoch 7 step 1848: training accuarcy: 0.9975\n",
      "Epoch 7 step 1848: training loss: 319.02496557633333\n",
      "Epoch 7 step 1849: training accuarcy: 0.9965\n",
      "Epoch 7 step 1849: training loss: 311.1567180435996\n",
      "Epoch 7 step 1850: training accuarcy: 0.9975\n",
      "Epoch 7 step 1850: training loss: 313.3573495488681\n",
      "Epoch 7 step 1851: training accuarcy: 0.997\n",
      "Epoch 7 step 1851: training loss: 319.59786106702916\n",
      "Epoch 7 step 1852: training accuarcy: 0.997\n",
      "Epoch 7 step 1852: training loss: 323.75996599171754\n",
      "Epoch 7 step 1853: training accuarcy: 0.996\n",
      "Epoch 7 step 1853: training loss: 326.1249997605197\n",
      "Epoch 7 step 1854: training accuarcy: 0.9965\n",
      "Epoch 7 step 1854: training loss: 318.0707863715316\n",
      "Epoch 7 step 1855: training accuarcy: 0.9975\n",
      "Epoch 7 step 1855: training loss: 319.30602479224535\n",
      "Epoch 7 step 1856: training accuarcy: 0.9975\n",
      "Epoch 7 step 1856: training loss: 319.33198286482036\n",
      "Epoch 7 step 1857: training accuarcy: 0.997\n",
      "Epoch 7 step 1857: training loss: 318.4254824927264\n",
      "Epoch 7 step 1858: training accuarcy: 0.997\n",
      "Epoch 7 step 1858: training loss: 316.2439755838752\n",
      "Epoch 7 step 1859: training accuarcy: 0.997\n",
      "Epoch 7 step 1859: training loss: 314.16865013924735\n",
      "Epoch 7 step 1860: training accuarcy: 0.993\n",
      "Epoch 7 step 1860: training loss: 317.8842281349472\n",
      "Epoch 7 step 1861: training accuarcy: 0.9945\n",
      "Epoch 7 step 1861: training loss: 315.03067121454114\n",
      "Epoch 7 step 1862: training accuarcy: 0.9985\n",
      "Epoch 7 step 1862: training loss: 313.2846238744009\n",
      "Epoch 7 step 1863: training accuarcy: 0.996\n",
      "Epoch 7 step 1863: training loss: 322.43495671061913\n",
      "Epoch 7 step 1864: training accuarcy: 0.9975\n",
      "Epoch 7 step 1864: training loss: 323.1698340107414\n",
      "Epoch 7 step 1865: training accuarcy: 0.997\n",
      "Epoch 7 step 1865: training loss: 327.9625469172895\n",
      "Epoch 7 step 1866: training accuarcy: 0.996\n",
      "Epoch 7 step 1866: training loss: 334.2336502851263\n",
      "Epoch 7 step 1867: training accuarcy: 0.9985\n",
      "Epoch 7 step 1867: training loss: 331.04364834629445\n",
      "Epoch 7 step 1868: training accuarcy: 0.996\n",
      "Epoch 7 step 1868: training loss: 326.95351520411384\n",
      "Epoch 7 step 1869: training accuarcy: 0.9975\n",
      "Epoch 7 step 1869: training loss: 325.31273807481926\n",
      "Epoch 7 step 1870: training accuarcy: 0.994\n",
      "Epoch 7 step 1870: training loss: 318.76265586966554\n",
      "Epoch 7 step 1871: training accuarcy: 0.998\n",
      "Epoch 7 step 1871: training loss: 322.01807088418184\n",
      "Epoch 7 step 1872: training accuarcy: 0.9965\n",
      "Epoch 7 step 1872: training loss: 327.32993053912276\n",
      "Epoch 7 step 1873: training accuarcy: 0.9965\n",
      "Epoch 7 step 1873: training loss: 338.43887172006134\n",
      "Epoch 7 step 1874: training accuarcy: 0.9975\n",
      "Epoch 7 step 1874: training loss: 337.92600150969747\n",
      "Epoch 7 step 1875: training accuarcy: 0.994\n",
      "Epoch 7 step 1875: training loss: 325.88525751252837\n",
      "Epoch 7 step 1876: training accuarcy: 0.9955\n",
      "Epoch 7 step 1876: training loss: 328.0261226467254\n",
      "Epoch 7 step 1877: training accuarcy: 0.996\n",
      "Epoch 7 step 1877: training loss: 319.2299745894309\n",
      "Epoch 7 step 1878: training accuarcy: 0.996\n",
      "Epoch 7 step 1878: training loss: 332.4943759345305\n",
      "Epoch 7 step 1879: training accuarcy: 0.996\n",
      "Epoch 7 step 1879: training loss: 330.5341700216481\n",
      "Epoch 7 step 1880: training accuarcy: 0.997\n",
      "Epoch 7 step 1880: training loss: 329.22643508879804\n",
      "Epoch 7 step 1881: training accuarcy: 0.996\n",
      "Epoch 7 step 1881: training loss: 325.8401158961133\n",
      "Epoch 7 step 1882: training accuarcy: 0.996\n",
      "Epoch 7 step 1882: training loss: 334.7153003818592\n",
      "Epoch 7 step 1883: training accuarcy: 0.9965\n",
      "Epoch 7 step 1883: training loss: 324.1905666432334\n",
      "Epoch 7 step 1884: training accuarcy: 0.997\n",
      "Epoch 7 step 1884: training loss: 329.1967493164017\n",
      "Epoch 7 step 1885: training accuarcy: 0.996\n",
      "Epoch 7 step 1885: training loss: 329.0976668343643\n",
      "Epoch 7 step 1886: training accuarcy: 0.996\n",
      "Epoch 7 step 1886: training loss: 345.6550633804476\n",
      "Epoch 7 step 1887: training accuarcy: 0.9955\n",
      "Epoch 7 step 1887: training loss: 343.483878013594\n",
      "Epoch 7 step 1888: training accuarcy: 0.9975\n",
      "Epoch 7 step 1888: training loss: 342.3724749784328\n",
      "Epoch 7 step 1889: training accuarcy: 0.991\n",
      "Epoch 7 step 1889: training loss: 322.5493975083598\n",
      "Epoch 7 step 1890: training accuarcy: 0.993\n",
      "Epoch 7 step 1890: training loss: 319.9390277042951\n",
      "Epoch 7 step 1891: training accuarcy: 0.9985\n",
      "Epoch 7 step 1891: training loss: 323.43386940822245\n",
      "Epoch 7 step 1892: training accuarcy: 0.998\n",
      "Epoch 7 step 1892: training loss: 336.08568482572457\n",
      "Epoch 7 step 1893: training accuarcy: 0.997\n",
      "Epoch 7 step 1893: training loss: 327.13158294226764\n",
      "Epoch 7 step 1894: training accuarcy: 0.9965\n",
      "Epoch 7 step 1894: training loss: 322.68665627942386\n",
      "Epoch 7 step 1895: training accuarcy: 0.9965\n",
      "Epoch 7 step 1895: training loss: 344.576285492675\n",
      "Epoch 7 step 1896: training accuarcy: 0.9975\n",
      "Epoch 7 step 1896: training loss: 331.47888631422063\n",
      "Epoch 7 step 1897: training accuarcy: 0.9945\n",
      "Epoch 7 step 1897: training loss: 333.18303280106295\n",
      "Epoch 7 step 1898: training accuarcy: 0.9955\n",
      "Epoch 7 step 1898: training loss: 340.09914667319765\n",
      "Epoch 7 step 1899: training accuarcy: 0.995\n",
      "Epoch 7 step 1899: training loss: 333.20035062010277\n",
      "Epoch 7 step 1900: training accuarcy: 0.9945\n",
      "Epoch 7 step 1900: training loss: 324.3215919691489\n",
      "Epoch 7 step 1901: training accuarcy: 0.9965\n",
      "Epoch 7 step 1901: training loss: 328.88697493126335\n",
      "Epoch 7 step 1902: training accuarcy: 0.997\n",
      "Epoch 7 step 1902: training loss: 334.16026998293535\n",
      "Epoch 7 step 1903: training accuarcy: 0.996\n",
      "Epoch 7 step 1903: training loss: 325.0225537288938\n",
      "Epoch 7 step 1904: training accuarcy: 0.997\n",
      "Epoch 7 step 1904: training loss: 317.40249413251775\n",
      "Epoch 7 step 1905: training accuarcy: 0.997\n",
      "Epoch 7 step 1905: training loss: 326.0513172756696\n",
      "Epoch 7 step 1906: training accuarcy: 0.9965\n",
      "Epoch 7 step 1906: training loss: 328.21634038140394\n",
      "Epoch 7 step 1907: training accuarcy: 0.9965\n",
      "Epoch 7 step 1907: training loss: 320.4877336579448\n",
      "Epoch 7 step 1908: training accuarcy: 0.9945\n",
      "Epoch 7 step 1908: training loss: 331.85388539723016\n",
      "Epoch 7 step 1909: training accuarcy: 0.996\n",
      "Epoch 7 step 1909: training loss: 322.807559183373\n",
      "Epoch 7 step 1910: training accuarcy: 0.994\n",
      "Epoch 7 step 1910: training loss: 337.32504200929566\n",
      "Epoch 7 step 1911: training accuarcy: 0.996\n",
      "Epoch 7 step 1911: training loss: 330.20708495227336\n",
      "Epoch 7 step 1912: training accuarcy: 0.9985\n",
      "Epoch 7 step 1912: training loss: 323.6888371477214\n",
      "Epoch 7 step 1913: training accuarcy: 0.9945\n",
      "Epoch 7 step 1913: training loss: 316.34298109696545\n",
      "Epoch 7 step 1914: training accuarcy: 0.9985\n",
      "Epoch 7 step 1914: training loss: 334.5760172421289\n",
      "Epoch 7 step 1915: training accuarcy: 0.9955\n",
      "Epoch 7 step 1915: training loss: 328.83650608075175\n",
      "Epoch 7 step 1916: training accuarcy: 0.996\n",
      "Epoch 7 step 1916: training loss: 331.9827856888985\n",
      "Epoch 7 step 1917: training accuarcy: 0.997\n",
      "Epoch 7 step 1917: training loss: 331.8566530495941\n",
      "Epoch 7 step 1918: training accuarcy: 0.996\n",
      "Epoch 7 step 1918: training loss: 324.2523002861424\n",
      "Epoch 7 step 1919: training accuarcy: 0.9965\n",
      "Epoch 7 step 1919: training loss: 323.57877038746653\n",
      "Epoch 7 step 1920: training accuarcy: 0.996\n",
      "Epoch 7 step 1920: training loss: 330.5836247766174\n",
      "Epoch 7 step 1921: training accuarcy: 0.9955\n",
      "Epoch 7 step 1921: training loss: 336.3081946503353\n",
      "Epoch 7 step 1922: training accuarcy: 0.9955\n",
      "Epoch 7 step 1922: training loss: 331.65431210712376\n",
      "Epoch 7 step 1923: training accuarcy: 0.993\n",
      "Epoch 7 step 1923: training loss: 331.2494104577895\n",
      "Epoch 7 step 1924: training accuarcy: 0.9975\n",
      "Epoch 7 step 1924: training loss: 320.2102876084175\n",
      "Epoch 7 step 1925: training accuarcy: 0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 1925: training loss: 332.9843082352826\n",
      "Epoch 7 step 1926: training accuarcy: 0.994\n",
      "Epoch 7 step 1926: training loss: 331.4446077903759\n",
      "Epoch 7 step 1927: training accuarcy: 0.9945\n",
      "Epoch 7 step 1927: training loss: 325.7790233012583\n",
      "Epoch 7 step 1928: training accuarcy: 0.9955\n",
      "Epoch 7 step 1928: training loss: 344.44421882884114\n",
      "Epoch 7 step 1929: training accuarcy: 0.9915\n",
      "Epoch 7 step 1929: training loss: 333.33931172099693\n",
      "Epoch 7 step 1930: training accuarcy: 0.995\n",
      "Epoch 7 step 1930: training loss: 320.5830591366987\n",
      "Epoch 7 step 1931: training accuarcy: 0.998\n",
      "Epoch 7 step 1931: training loss: 335.8779391274442\n",
      "Epoch 7 step 1932: training accuarcy: 0.9945\n",
      "Epoch 7 step 1932: training loss: 316.46374005818694\n",
      "Epoch 7 step 1933: training accuarcy: 0.9965\n",
      "Epoch 7 step 1933: training loss: 334.56897552039834\n",
      "Epoch 7 step 1934: training accuarcy: 0.9965\n",
      "Epoch 7 step 1934: training loss: 323.9347197086603\n",
      "Epoch 7 step 1935: training accuarcy: 0.994\n",
      "Epoch 7 step 1935: training loss: 330.67382756347286\n",
      "Epoch 7 step 1936: training accuarcy: 0.997\n",
      "Epoch 7 step 1936: training loss: 341.8937572785812\n",
      "Epoch 7 step 1937: training accuarcy: 0.9985\n",
      "Epoch 7 step 1937: training loss: 324.2320549133948\n",
      "Epoch 7 step 1938: training accuarcy: 0.9955\n",
      "Epoch 7 step 1938: training loss: 318.1238651974583\n",
      "Epoch 7 step 1939: training accuarcy: 0.998\n",
      "Epoch 7 step 1939: training loss: 324.649357880225\n",
      "Epoch 7 step 1940: training accuarcy: 0.994\n",
      "Epoch 7 step 1940: training loss: 330.4086700326902\n",
      "Epoch 7 step 1941: training accuarcy: 0.992\n",
      "Epoch 7 step 1941: training loss: 322.62642930054557\n",
      "Epoch 7 step 1942: training accuarcy: 0.998\n",
      "Epoch 7 step 1942: training loss: 331.9003452220642\n",
      "Epoch 7 step 1943: training accuarcy: 0.9935\n",
      "Epoch 7 step 1943: training loss: 326.586397245052\n",
      "Epoch 7 step 1944: training accuarcy: 0.9955\n",
      "Epoch 7 step 1944: training loss: 329.25928714733107\n",
      "Epoch 7 step 1945: training accuarcy: 0.9955\n",
      "Epoch 7 step 1945: training loss: 326.07112489415255\n",
      "Epoch 7 step 1946: training accuarcy: 0.9985\n",
      "Epoch 7 step 1946: training loss: 327.7077328880872\n",
      "Epoch 7 step 1947: training accuarcy: 0.9965\n",
      "Epoch 7 step 1947: training loss: 311.0016765117811\n",
      "Epoch 7 step 1948: training accuarcy: 0.9985\n",
      "Epoch 7 step 1948: training loss: 322.6993897601039\n",
      "Epoch 7 step 1949: training accuarcy: 0.999\n",
      "Epoch 7 step 1949: training loss: 320.4194981961915\n",
      "Epoch 7 step 1950: training accuarcy: 0.9975\n",
      "Epoch 7 step 1950: training loss: 330.0106237762326\n",
      "Epoch 7 step 1951: training accuarcy: 0.992\n",
      "Epoch 7 step 1951: training loss: 339.51567343860177\n",
      "Epoch 7 step 1952: training accuarcy: 0.9945\n",
      "Epoch 7 step 1952: training loss: 328.91384656754997\n",
      "Epoch 7 step 1953: training accuarcy: 0.9965\n",
      "Epoch 7 step 1953: training loss: 317.8677745199979\n",
      "Epoch 7 step 1954: training accuarcy: 0.9975\n",
      "Epoch 7 step 1954: training loss: 339.6800479848795\n",
      "Epoch 7 step 1955: training accuarcy: 0.995\n",
      "Epoch 7 step 1955: training loss: 323.7149833603212\n",
      "Epoch 7 step 1956: training accuarcy: 0.996\n",
      "Epoch 7 step 1956: training loss: 339.46727507146056\n",
      "Epoch 7 step 1957: training accuarcy: 0.9935\n",
      "Epoch 7 step 1957: training loss: 327.03093200304477\n",
      "Epoch 7 step 1958: training accuarcy: 0.9975\n",
      "Epoch 7 step 1958: training loss: 342.0741514797804\n",
      "Epoch 7 step 1959: training accuarcy: 0.993\n",
      "Epoch 7 step 1959: training loss: 330.3094009939155\n",
      "Epoch 7 step 1960: training accuarcy: 0.9945\n",
      "Epoch 7 step 1960: training loss: 330.2273029240761\n",
      "Epoch 7 step 1961: training accuarcy: 0.998\n",
      "Epoch 7 step 1961: training loss: 328.55251106698097\n",
      "Epoch 7 step 1962: training accuarcy: 0.997\n",
      "Epoch 7 step 1962: training loss: 336.3214368954764\n",
      "Epoch 7 step 1963: training accuarcy: 0.995\n",
      "Epoch 7 step 1963: training loss: 327.0030535591011\n",
      "Epoch 7 step 1964: training accuarcy: 0.9985\n",
      "Epoch 7 step 1964: training loss: 336.94000625201784\n",
      "Epoch 7 step 1965: training accuarcy: 0.995\n",
      "Epoch 7 step 1965: training loss: 337.58350931210305\n",
      "Epoch 7 step 1966: training accuarcy: 0.9965\n",
      "Epoch 7 step 1966: training loss: 330.1181743978363\n",
      "Epoch 7 step 1967: training accuarcy: 0.9965\n",
      "Epoch 7 step 1967: training loss: 311.86282901119193\n",
      "Epoch 7 step 1968: training accuarcy: 0.9955\n",
      "Epoch 7 step 1968: training loss: 333.6240833523259\n",
      "Epoch 7 step 1969: training accuarcy: 0.9955\n",
      "Epoch 7 step 1969: training loss: 331.9398641066571\n",
      "Epoch 7 step 1970: training accuarcy: 0.9975\n",
      "Epoch 7 step 1970: training loss: 333.6950325460257\n",
      "Epoch 7 step 1971: training accuarcy: 0.997\n",
      "Epoch 7 step 1971: training loss: 327.98147253321486\n",
      "Epoch 7 step 1972: training accuarcy: 0.9945\n",
      "Epoch 7 step 1972: training loss: 330.2698484234427\n",
      "Epoch 7 step 1973: training accuarcy: 0.9975\n",
      "Epoch 7 step 1973: training loss: 316.4395745694877\n",
      "Epoch 7 step 1974: training accuarcy: 0.996\n",
      "Epoch 7 step 1974: training loss: 334.23453570623644\n",
      "Epoch 7 step 1975: training accuarcy: 0.997\n",
      "Epoch 7 step 1975: training loss: 325.49081493400365\n",
      "Epoch 7 step 1976: training accuarcy: 0.9975\n",
      "Epoch 7 step 1976: training loss: 323.4504784645241\n",
      "Epoch 7 step 1977: training accuarcy: 0.995\n",
      "Epoch 7 step 1977: training loss: 331.609219784783\n",
      "Epoch 7 step 1978: training accuarcy: 0.995\n",
      "Epoch 7 step 1978: training loss: 322.0616245955517\n",
      "Epoch 7 step 1979: training accuarcy: 0.9965\n",
      "Epoch 7 step 1979: training loss: 318.67225498970976\n",
      "Epoch 7 step 1980: training accuarcy: 0.9955\n",
      "Epoch 7 step 1980: training loss: 325.35336051880654\n",
      "Epoch 7 step 1981: training accuarcy: 0.9965\n",
      "Epoch 7 step 1981: training loss: 316.7106945166479\n",
      "Epoch 7 step 1982: training accuarcy: 0.9955\n",
      "Epoch 7 step 1982: training loss: 315.34503473962485\n",
      "Epoch 7 step 1983: training accuarcy: 0.9965\n",
      "Epoch 7 step 1983: training loss: 325.80894596640434\n",
      "Epoch 7 step 1984: training accuarcy: 0.996\n",
      "Epoch 7 step 1984: training loss: 321.89040252976156\n",
      "Epoch 7 step 1985: training accuarcy: 0.9965\n",
      "Epoch 7 step 1985: training loss: 328.0256278060623\n",
      "Epoch 7 step 1986: training accuarcy: 0.993\n",
      "Epoch 7 step 1986: training loss: 315.9785791005104\n",
      "Epoch 7 step 1987: training accuarcy: 0.996\n",
      "Epoch 7 step 1987: training loss: 329.5582349730371\n",
      "Epoch 7 step 1988: training accuarcy: 0.995\n",
      "Epoch 7 step 1988: training loss: 319.63036420303837\n",
      "Epoch 7 step 1989: training accuarcy: 1.0\n",
      "Epoch 7 step 1989: training loss: 315.853220572814\n",
      "Epoch 7 step 1990: training accuarcy: 0.997\n",
      "Epoch 7 step 1990: training loss: 337.50360214386023\n",
      "Epoch 7 step 1991: training accuarcy: 0.996\n",
      "Epoch 7 step 1991: training loss: 317.26097976086567\n",
      "Epoch 7 step 1992: training accuarcy: 0.9945\n",
      "Epoch 7 step 1992: training loss: 336.2478790760624\n",
      "Epoch 7 step 1993: training accuarcy: 0.997\n",
      "Epoch 7 step 1993: training loss: 337.1214547768192\n",
      "Epoch 7 step 1994: training accuarcy: 0.994\n",
      "Epoch 7 step 1994: training loss: 348.6277458812139\n",
      "Epoch 7 step 1995: training accuarcy: 0.994\n",
      "Epoch 7 step 1995: training loss: 339.2118199971791\n",
      "Epoch 7 step 1996: training accuarcy: 0.9955\n",
      "Epoch 7 step 1996: training loss: 337.05128704169636\n",
      "Epoch 7 step 1997: training accuarcy: 0.996\n",
      "Epoch 7 step 1997: training loss: 341.7860232288109\n",
      "Epoch 7 step 1998: training accuarcy: 0.998\n",
      "Epoch 7 step 1998: training loss: 330.4419922389138\n",
      "Epoch 7 step 1999: training accuarcy: 0.9965\n",
      "Epoch 7 step 1999: training loss: 331.15530249561476\n",
      "Epoch 7 step 2000: training accuarcy: 0.996\n",
      "Epoch 7 step 2000: training loss: 330.10902749646186\n",
      "Epoch 7 step 2001: training accuarcy: 0.996\n",
      "Epoch 7 step 2001: training loss: 320.10085559467404\n",
      "Epoch 7 step 2002: training accuarcy: 0.998\n",
      "Epoch 7 step 2002: training loss: 329.6313825150472\n",
      "Epoch 7 step 2003: training accuarcy: 0.9955\n",
      "Epoch 7 step 2003: training loss: 325.93003939818834\n",
      "Epoch 7 step 2004: training accuarcy: 0.996\n",
      "Epoch 7 step 2004: training loss: 335.04850238492054\n",
      "Epoch 7 step 2005: training accuarcy: 0.9955\n",
      "Epoch 7 step 2005: training loss: 335.84584985642795\n",
      "Epoch 7 step 2006: training accuarcy: 0.9955\n",
      "Epoch 7 step 2006: training loss: 339.9508659809449\n",
      "Epoch 7 step 2007: training accuarcy: 0.995\n",
      "Epoch 7 step 2007: training loss: 334.83018281999074\n",
      "Epoch 7 step 2008: training accuarcy: 0.9935\n",
      "Epoch 7 step 2008: training loss: 322.46359587843205\n",
      "Epoch 7 step 2009: training accuarcy: 0.9985\n",
      "Epoch 7 step 2009: training loss: 330.68198008781405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2010: training accuarcy: 0.9955\n",
      "Epoch 7 step 2010: training loss: 328.935751366856\n",
      "Epoch 7 step 2011: training accuarcy: 0.996\n",
      "Epoch 7 step 2011: training loss: 325.5385763191102\n",
      "Epoch 7 step 2012: training accuarcy: 0.9975\n",
      "Epoch 7 step 2012: training loss: 337.1156319602303\n",
      "Epoch 7 step 2013: training accuarcy: 0.995\n",
      "Epoch 7 step 2013: training loss: 326.8206010556179\n",
      "Epoch 7 step 2014: training accuarcy: 0.9955\n",
      "Epoch 7 step 2014: training loss: 318.550511375767\n",
      "Epoch 7 step 2015: training accuarcy: 0.996\n",
      "Epoch 7 step 2015: training loss: 324.5610541237395\n",
      "Epoch 7 step 2016: training accuarcy: 0.998\n",
      "Epoch 7 step 2016: training loss: 336.9042692020139\n",
      "Epoch 7 step 2017: training accuarcy: 0.995\n",
      "Epoch 7 step 2017: training loss: 331.64046277004104\n",
      "Epoch 7 step 2018: training accuarcy: 0.995\n",
      "Epoch 7 step 2018: training loss: 327.00539135199955\n",
      "Epoch 7 step 2019: training accuarcy: 0.9985\n",
      "Epoch 7 step 2019: training loss: 330.69742573703854\n",
      "Epoch 7 step 2020: training accuarcy: 0.9965\n",
      "Epoch 7 step 2020: training loss: 327.1708298865557\n",
      "Epoch 7 step 2021: training accuarcy: 0.9945\n",
      "Epoch 7 step 2021: training loss: 335.71175579410976\n",
      "Epoch 7 step 2022: training accuarcy: 0.9965\n",
      "Epoch 7 step 2022: training loss: 326.97485090852257\n",
      "Epoch 7 step 2023: training accuarcy: 0.997\n",
      "Epoch 7 step 2023: training loss: 342.65349248214284\n",
      "Epoch 7 step 2024: training accuarcy: 0.996\n",
      "Epoch 7 step 2024: training loss: 330.9406767915215\n",
      "Epoch 7 step 2025: training accuarcy: 0.994\n",
      "Epoch 7 step 2025: training loss: 329.4030165835329\n",
      "Epoch 7 step 2026: training accuarcy: 0.994\n",
      "Epoch 7 step 2026: training loss: 319.2626094484565\n",
      "Epoch 7 step 2027: training accuarcy: 0.9985\n",
      "Epoch 7 step 2027: training loss: 327.57458615342347\n",
      "Epoch 7 step 2028: training accuarcy: 0.9965\n",
      "Epoch 7 step 2028: training loss: 336.8014418632206\n",
      "Epoch 7 step 2029: training accuarcy: 0.996\n",
      "Epoch 7 step 2029: training loss: 337.3988362295743\n",
      "Epoch 7 step 2030: training accuarcy: 0.997\n",
      "Epoch 7 step 2030: training loss: 323.28105948228676\n",
      "Epoch 7 step 2031: training accuarcy: 0.996\n",
      "Epoch 7 step 2031: training loss: 333.1890296186756\n",
      "Epoch 7 step 2032: training accuarcy: 0.9965\n",
      "Epoch 7 step 2032: training loss: 331.9814473893165\n",
      "Epoch 7 step 2033: training accuarcy: 0.996\n",
      "Epoch 7 step 2033: training loss: 324.0986740827133\n",
      "Epoch 7 step 2034: training accuarcy: 0.9955\n",
      "Epoch 7 step 2034: training loss: 332.49430078109424\n",
      "Epoch 7 step 2035: training accuarcy: 0.997\n",
      "Epoch 7 step 2035: training loss: 331.9351508175703\n",
      "Epoch 7 step 2036: training accuarcy: 0.996\n",
      "Epoch 7 step 2036: training loss: 331.2797796534634\n",
      "Epoch 7 step 2037: training accuarcy: 0.996\n",
      "Epoch 7 step 2037: training loss: 317.8142546320362\n",
      "Epoch 7 step 2038: training accuarcy: 0.9985\n",
      "Epoch 7 step 2038: training loss: 330.7306156647221\n",
      "Epoch 7 step 2039: training accuarcy: 0.994\n",
      "Epoch 7 step 2039: training loss: 336.66034199344745\n",
      "Epoch 7 step 2040: training accuarcy: 0.996\n",
      "Epoch 7 step 2040: training loss: 324.3998347721322\n",
      "Epoch 7 step 2041: training accuarcy: 0.995\n",
      "Epoch 7 step 2041: training loss: 341.76824852595337\n",
      "Epoch 7 step 2042: training accuarcy: 0.9955\n",
      "Epoch 7 step 2042: training loss: 319.77139654427316\n",
      "Epoch 7 step 2043: training accuarcy: 0.996\n",
      "Epoch 7 step 2043: training loss: 331.8591164986442\n",
      "Epoch 7 step 2044: training accuarcy: 0.995\n",
      "Epoch 7 step 2044: training loss: 320.9344237241014\n",
      "Epoch 7 step 2045: training accuarcy: 0.997\n",
      "Epoch 7 step 2045: training loss: 329.7383273985074\n",
      "Epoch 7 step 2046: training accuarcy: 0.9965\n",
      "Epoch 7 step 2046: training loss: 330.7222848650964\n",
      "Epoch 7 step 2047: training accuarcy: 0.9965\n",
      "Epoch 7 step 2047: training loss: 327.50575419962934\n",
      "Epoch 7 step 2048: training accuarcy: 0.9955\n",
      "Epoch 7 step 2048: training loss: 328.15281103272\n",
      "Epoch 7 step 2049: training accuarcy: 0.994\n",
      "Epoch 7 step 2049: training loss: 340.23496175807736\n",
      "Epoch 7 step 2050: training accuarcy: 0.9955\n",
      "Epoch 7 step 2050: training loss: 342.2768309366564\n",
      "Epoch 7 step 2051: training accuarcy: 0.996\n",
      "Epoch 7 step 2051: training loss: 322.52821929400193\n",
      "Epoch 7 step 2052: training accuarcy: 0.9965\n",
      "Epoch 7 step 2052: training loss: 321.5957726412272\n",
      "Epoch 7 step 2053: training accuarcy: 0.9955\n",
      "Epoch 7 step 2053: training loss: 325.7177758917988\n",
      "Epoch 7 step 2054: training accuarcy: 0.995\n",
      "Epoch 7 step 2054: training loss: 317.7186888150587\n",
      "Epoch 7 step 2055: training accuarcy: 0.9965\n",
      "Epoch 7 step 2055: training loss: 347.45859729426803\n",
      "Epoch 7 step 2056: training accuarcy: 0.996\n",
      "Epoch 7 step 2056: training loss: 331.37932890928334\n",
      "Epoch 7 step 2057: training accuarcy: 0.995\n",
      "Epoch 7 step 2057: training loss: 339.7643237134628\n",
      "Epoch 7 step 2058: training accuarcy: 0.9955\n",
      "Epoch 7 step 2058: training loss: 343.8177050558995\n",
      "Epoch 7 step 2059: training accuarcy: 0.995\n",
      "Epoch 7 step 2059: training loss: 334.89885541676324\n",
      "Epoch 7 step 2060: training accuarcy: 0.9945\n",
      "Epoch 7 step 2060: training loss: 330.77401875252013\n",
      "Epoch 7 step 2061: training accuarcy: 0.998\n",
      "Epoch 7 step 2061: training loss: 329.75135881330306\n",
      "Epoch 7 step 2062: training accuarcy: 0.997\n",
      "Epoch 7 step 2062: training loss: 347.15737344437986\n",
      "Epoch 7 step 2063: training accuarcy: 0.996\n",
      "Epoch 7 step 2063: training loss: 320.57180512068226\n",
      "Epoch 7 step 2064: training accuarcy: 0.9975\n",
      "Epoch 7 step 2064: training loss: 335.410909088268\n",
      "Epoch 7 step 2065: training accuarcy: 0.996\n",
      "Epoch 7 step 2065: training loss: 321.0378844219575\n",
      "Epoch 7 step 2066: training accuarcy: 0.9955\n",
      "Epoch 7 step 2066: training loss: 319.680665079045\n",
      "Epoch 7 step 2067: training accuarcy: 0.996\n",
      "Epoch 7 step 2067: training loss: 333.1525071597193\n",
      "Epoch 7 step 2068: training accuarcy: 0.9945\n",
      "Epoch 7 step 2068: training loss: 316.5600432745485\n",
      "Epoch 7 step 2069: training accuarcy: 0.997\n",
      "Epoch 7 step 2069: training loss: 322.0235131897855\n",
      "Epoch 7 step 2070: training accuarcy: 0.995\n",
      "Epoch 7 step 2070: training loss: 332.8123767998661\n",
      "Epoch 7 step 2071: training accuarcy: 0.997\n",
      "Epoch 7 step 2071: training loss: 324.19777732460705\n",
      "Epoch 7 step 2072: training accuarcy: 0.9955\n",
      "Epoch 7 step 2072: training loss: 326.2086280120508\n",
      "Epoch 7 step 2073: training accuarcy: 0.995\n",
      "Epoch 7 step 2073: training loss: 322.9879517951866\n",
      "Epoch 7 step 2074: training accuarcy: 0.995\n",
      "Epoch 7 step 2074: training loss: 336.55123036471275\n",
      "Epoch 7 step 2075: training accuarcy: 0.9965\n",
      "Epoch 7 step 2075: training loss: 326.7661944901911\n",
      "Epoch 7 step 2076: training accuarcy: 0.9945\n",
      "Epoch 7 step 2076: training loss: 342.6614619167118\n",
      "Epoch 7 step 2077: training accuarcy: 0.9955\n",
      "Epoch 7 step 2077: training loss: 334.0733861279481\n",
      "Epoch 7 step 2078: training accuarcy: 0.994\n",
      "Epoch 7 step 2078: training loss: 315.96280879238054\n",
      "Epoch 7 step 2079: training accuarcy: 0.9965\n",
      "Epoch 7 step 2079: training loss: 327.8310937421221\n",
      "Epoch 7 step 2080: training accuarcy: 0.9965\n",
      "Epoch 7 step 2080: training loss: 330.68532817762093\n",
      "Epoch 7 step 2081: training accuarcy: 0.994\n",
      "Epoch 7 step 2081: training loss: 329.0019337114172\n",
      "Epoch 7 step 2082: training accuarcy: 0.997\n",
      "Epoch 7 step 2082: training loss: 328.8095163952638\n",
      "Epoch 7 step 2083: training accuarcy: 0.997\n",
      "Epoch 7 step 2083: training loss: 334.3217508421048\n",
      "Epoch 7 step 2084: training accuarcy: 0.994\n",
      "Epoch 7 step 2084: training loss: 343.1656976532297\n",
      "Epoch 7 step 2085: training accuarcy: 0.993\n",
      "Epoch 7 step 2085: training loss: 335.59510626504334\n",
      "Epoch 7 step 2086: training accuarcy: 0.9955\n",
      "Epoch 7 step 2086: training loss: 334.66370543593405\n",
      "Epoch 7 step 2087: training accuarcy: 0.9955\n",
      "Epoch 7 step 2087: training loss: 321.77792536416155\n",
      "Epoch 7 step 2088: training accuarcy: 0.995\n",
      "Epoch 7 step 2088: training loss: 327.1111591977958\n",
      "Epoch 7 step 2089: training accuarcy: 0.997\n",
      "Epoch 7 step 2089: training loss: 321.1361355392569\n",
      "Epoch 7 step 2090: training accuarcy: 0.998\n",
      "Epoch 7 step 2090: training loss: 337.5573239356702\n",
      "Epoch 7 step 2091: training accuarcy: 0.9945\n",
      "Epoch 7 step 2091: training loss: 338.9524823801006\n",
      "Epoch 7 step 2092: training accuarcy: 0.9965\n",
      "Epoch 7 step 2092: training loss: 330.02465817296775\n",
      "Epoch 7 step 2093: training accuarcy: 0.9965\n",
      "Epoch 7 step 2093: training loss: 323.8466270779463\n",
      "Epoch 7 step 2094: training accuarcy: 0.9965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2094: training loss: 323.4826474826256\n",
      "Epoch 7 step 2095: training accuarcy: 0.997\n",
      "Epoch 7 step 2095: training loss: 333.4686184369758\n",
      "Epoch 7 step 2096: training accuarcy: 0.9965\n",
      "Epoch 7 step 2096: training loss: 339.2355380450729\n",
      "Epoch 7 step 2097: training accuarcy: 0.9975\n",
      "Epoch 7 step 2097: training loss: 341.9979638092247\n",
      "Epoch 7 step 2098: training accuarcy: 0.995\n",
      "Epoch 7 step 2098: training loss: 330.81652240053904\n",
      "Epoch 7 step 2099: training accuarcy: 0.995\n",
      "Epoch 7 step 2099: training loss: 319.15309088569927\n",
      "Epoch 7 step 2100: training accuarcy: 0.9945\n",
      "Epoch 7 step 2100: training loss: 312.6963901203337\n",
      "Epoch 7 step 2101: training accuarcy: 0.9965\n",
      "Epoch 7 step 2101: training loss: 321.20695039244686\n",
      "Epoch 7 step 2102: training accuarcy: 0.9965\n",
      "Epoch 7 step 2102: training loss: 317.5384766887032\n",
      "Epoch 7 step 2103: training accuarcy: 0.9975\n",
      "Epoch 7 step 2103: training loss: 242.85923543437607\n",
      "Epoch 7 step 2104: training accuarcy: 0.9910256410256411\n",
      "Epoch 7: train loss 327.926774757249, train accuarcy 0.949459969997406\n",
      "Epoch 7: valid loss 1021.149462706732, valid accuarcy 0.9609864354133606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [15:26<00:00, 113.57s/it]\n"
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=8, loss_callback=simple_loss_callback, log_dir=get_log_dir('topcoder', 'fm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:03:40.695915Z",
     "start_time": "2019-09-25T13:03:40.688884Z"
    }
   },
   "outputs": [],
   "source": [
    "del fm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HRM FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:03:55.333484Z",
     "start_time": "2019-09-25T13:03:55.097484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchHrmFM()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_model = TorchHrmFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "hrm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:04:00.445005Z",
     "start_time": "2019-09-25T13:04:00.440006Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(hrm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:04:06.227891Z",
     "start_time": "2019-09-25T13:04:06.183888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1f20046bcc0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_learner = FMLearner(hrm_model, adam_opt, schedular, db)\n",
    "hrm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:20:01.712810Z",
     "start_time": "2019-09-25T13:04:29.831807Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 37508.48394862066\n",
      "Epoch 0 step 1: training accuarcy: 0.489\n",
      "Epoch 0 step 1: training loss: 36442.54428888903\n",
      "Epoch 0 step 2: training accuarcy: 0.49\n",
      "Epoch 0 step 2: training loss: 35401.56785084879\n",
      "Epoch 0 step 3: training accuarcy: 0.4995\n",
      "Epoch 0 step 3: training loss: 34378.68944804763\n",
      "Epoch 0 step 4: training accuarcy: 0.5035000000000001\n",
      "Epoch 0 step 4: training loss: 33382.10234287656\n",
      "Epoch 0 step 5: training accuarcy: 0.5075000000000001\n",
      "Epoch 0 step 5: training loss: 32409.860371521718\n",
      "Epoch 0 step 6: training accuarcy: 0.504\n",
      "Epoch 0 step 6: training loss: 31457.042214567897\n",
      "Epoch 0 step 7: training accuarcy: 0.496\n",
      "Epoch 0 step 7: training loss: 30524.22628953661\n",
      "Epoch 0 step 8: training accuarcy: 0.496\n",
      "Epoch 0 step 8: training loss: 29613.44862392991\n",
      "Epoch 0 step 9: training accuarcy: 0.5225\n",
      "Epoch 0 step 9: training loss: 28726.668180471803\n",
      "Epoch 0 step 10: training accuarcy: 0.5075000000000001\n",
      "Epoch 0 step 10: training loss: 27861.619561225154\n",
      "Epoch 0 step 11: training accuarcy: 0.504\n",
      "Epoch 0 step 11: training loss: 27016.211203514173\n",
      "Epoch 0 step 12: training accuarcy: 0.502\n",
      "Epoch 0 step 12: training loss: 26195.05301216478\n",
      "Epoch 0 step 13: training accuarcy: 0.49\n",
      "Epoch 0 step 13: training loss: 25388.091380591195\n",
      "Epoch 0 step 14: training accuarcy: 0.505\n",
      "Epoch 0 step 14: training loss: 24603.63423140444\n",
      "Epoch 0 step 15: training accuarcy: 0.5095000000000001\n",
      "Epoch 0 step 15: training loss: 23842.018778721293\n",
      "Epoch 0 step 16: training accuarcy: 0.5095000000000001\n",
      "Epoch 0 step 16: training loss: 23096.930909857954\n",
      "Epoch 0 step 17: training accuarcy: 0.5305\n",
      "Epoch 0 step 17: training loss: 22377.6179543002\n",
      "Epoch 0 step 18: training accuarcy: 0.5125\n",
      "Epoch 0 step 18: training loss: 21671.986120129346\n",
      "Epoch 0 step 19: training accuarcy: 0.5225\n",
      "Epoch 0 step 19: training loss: 20991.049079682998\n",
      "Epoch 0 step 20: training accuarcy: 0.503\n",
      "Epoch 0 step 20: training loss: 20325.199764813722\n",
      "Epoch 0 step 21: training accuarcy: 0.5115000000000001\n",
      "Epoch 0 step 21: training loss: 19677.30278299788\n",
      "Epoch 0 step 22: training accuarcy: 0.519\n",
      "Epoch 0 step 22: training loss: 19048.04161345385\n",
      "Epoch 0 step 23: training accuarcy: 0.5345\n",
      "Epoch 0 step 23: training loss: 18443.336795414976\n",
      "Epoch 0 step 24: training accuarcy: 0.495\n",
      "Epoch 0 step 24: training loss: 17851.39086805281\n",
      "Epoch 0 step 25: training accuarcy: 0.504\n",
      "Epoch 0 step 25: training loss: 17275.560518874794\n",
      "Epoch 0 step 26: training accuarcy: 0.503\n",
      "Epoch 0 step 26: training loss: 16715.590004308764\n",
      "Epoch 0 step 27: training accuarcy: 0.5215\n",
      "Epoch 0 step 27: training loss: 16176.093885912167\n",
      "Epoch 0 step 28: training accuarcy: 0.523\n",
      "Epoch 0 step 28: training loss: 15654.336818056163\n",
      "Epoch 0 step 29: training accuarcy: 0.5005000000000001\n",
      "Epoch 0 step 29: training loss: 15142.277039667662\n",
      "Epoch 0 step 30: training accuarcy: 0.5235\n",
      "Epoch 0 step 30: training loss: 14651.456991062478\n",
      "Epoch 0 step 31: training accuarcy: 0.501\n",
      "Epoch 0 step 31: training loss: 14173.143970975583\n",
      "Epoch 0 step 32: training accuarcy: 0.5165\n",
      "Epoch 0 step 32: training loss: 13710.38335420449\n",
      "Epoch 0 step 33: training accuarcy: 0.523\n",
      "Epoch 0 step 33: training loss: 13263.043474494349\n",
      "Epoch 0 step 34: training accuarcy: 0.507\n",
      "Epoch 0 step 34: training loss: 12827.643057731158\n",
      "Epoch 0 step 35: training accuarcy: 0.5215\n",
      "Epoch 0 step 35: training loss: 12407.864031285428\n",
      "Epoch 0 step 36: training accuarcy: 0.519\n",
      "Epoch 0 step 36: training loss: 12000.584791816722\n",
      "Epoch 0 step 37: training accuarcy: 0.525\n",
      "Epoch 0 step 37: training loss: 11606.598047534566\n",
      "Epoch 0 step 38: training accuarcy: 0.5235\n",
      "Epoch 0 step 38: training loss: 11226.053720376736\n",
      "Epoch 0 step 39: training accuarcy: 0.53\n",
      "Epoch 0 step 39: training loss: 10859.477990220954\n",
      "Epoch 0 step 40: training accuarcy: 0.515\n",
      "Epoch 0 step 40: training loss: 10503.564242216387\n",
      "Epoch 0 step 41: training accuarcy: 0.5085000000000001\n",
      "Epoch 0 step 41: training loss: 10159.67649604515\n",
      "Epoch 0 step 42: training accuarcy: 0.518\n",
      "Epoch 0 step 42: training loss: 9826.162333293825\n",
      "Epoch 0 step 43: training accuarcy: 0.523\n",
      "Epoch 0 step 43: training loss: 9505.59404108122\n",
      "Epoch 0 step 44: training accuarcy: 0.5315\n",
      "Epoch 0 step 44: training loss: 9195.022599131898\n",
      "Epoch 0 step 45: training accuarcy: 0.511\n",
      "Epoch 0 step 45: training loss: 8893.221365540629\n",
      "Epoch 0 step 46: training accuarcy: 0.5425\n",
      "Epoch 0 step 46: training loss: 8605.678986294379\n",
      "Epoch 0 step 47: training accuarcy: 0.5225\n",
      "Epoch 0 step 47: training loss: 8326.675259813794\n",
      "Epoch 0 step 48: training accuarcy: 0.522\n",
      "Epoch 0 step 48: training loss: 8055.623563999568\n",
      "Epoch 0 step 49: training accuarcy: 0.544\n",
      "Epoch 0 step 49: training loss: 7795.950600732639\n",
      "Epoch 0 step 50: training accuarcy: 0.532\n",
      "Epoch 0 step 50: training loss: 7545.368847245045\n",
      "Epoch 0 step 51: training accuarcy: 0.525\n",
      "Epoch 0 step 51: training loss: 7301.635079841218\n",
      "Epoch 0 step 52: training accuarcy: 0.53\n",
      "Epoch 0 step 52: training loss: 7068.297001146215\n",
      "Epoch 0 step 53: training accuarcy: 0.528\n",
      "Epoch 0 step 53: training loss: 6841.226516838312\n",
      "Epoch 0 step 54: training accuarcy: 0.548\n",
      "Epoch 0 step 54: training loss: 6626.191199714913\n",
      "Epoch 0 step 55: training accuarcy: 0.518\n",
      "Epoch 0 step 55: training loss: 6414.200403002356\n",
      "Epoch 0 step 56: training accuarcy: 0.532\n",
      "Epoch 0 step 56: training loss: 6210.756358580467\n",
      "Epoch 0 step 57: training accuarcy: 0.555\n",
      "Epoch 0 step 57: training loss: 6016.86089614625\n",
      "Epoch 0 step 58: training accuarcy: 0.5415\n",
      "Epoch 0 step 58: training loss: 5830.287207100822\n",
      "Epoch 0 step 59: training accuarcy: 0.5265\n",
      "Epoch 0 step 59: training loss: 5648.02560623494\n",
      "Epoch 0 step 60: training accuarcy: 0.5405\n",
      "Epoch 0 step 60: training loss: 5476.308947002211\n",
      "Epoch 0 step 61: training accuarcy: 0.526\n",
      "Epoch 0 step 61: training loss: 5307.492070376284\n",
      "Epoch 0 step 62: training accuarcy: 0.5345\n",
      "Epoch 0 step 62: training loss: 5145.409286577451\n",
      "Epoch 0 step 63: training accuarcy: 0.5255\n",
      "Epoch 0 step 63: training loss: 4988.095313385069\n",
      "Epoch 0 step 64: training accuarcy: 0.558\n",
      "Epoch 0 step 64: training loss: 4839.34015379617\n",
      "Epoch 0 step 65: training accuarcy: 0.545\n",
      "Epoch 0 step 65: training loss: 4693.991967851242\n",
      "Epoch 0 step 66: training accuarcy: 0.5505\n",
      "Epoch 0 step 66: training loss: 4557.040898758015\n",
      "Epoch 0 step 67: training accuarcy: 0.5535\n",
      "Epoch 0 step 67: training loss: 4425.089557006058\n",
      "Epoch 0 step 68: training accuarcy: 0.5315\n",
      "Epoch 0 step 68: training loss: 4295.717602197856\n",
      "Epoch 0 step 69: training accuarcy: 0.5375\n",
      "Epoch 0 step 69: training loss: 4171.045219471573\n",
      "Epoch 0 step 70: training accuarcy: 0.56\n",
      "Epoch 0 step 70: training loss: 4052.732943843802\n",
      "Epoch 0 step 71: training accuarcy: 0.557\n",
      "Epoch 0 step 71: training loss: 3939.975532569247\n",
      "Epoch 0 step 72: training accuarcy: 0.5555\n",
      "Epoch 0 step 72: training loss: 3828.8698579864895\n",
      "Epoch 0 step 73: training accuarcy: 0.5565\n",
      "Epoch 0 step 73: training loss: 3724.4564815104386\n",
      "Epoch 0 step 74: training accuarcy: 0.5525\n",
      "Epoch 0 step 74: training loss: 3623.6365012026\n",
      "Epoch 0 step 75: training accuarcy: 0.5405\n",
      "Epoch 0 step 75: training loss: 3527.574028780693\n",
      "Epoch 0 step 76: training accuarcy: 0.5315\n",
      "Epoch 0 step 76: training loss: 3431.750732265642\n",
      "Epoch 0 step 77: training accuarcy: 0.5585\n",
      "Epoch 0 step 77: training loss: 3343.1424005610324\n",
      "Epoch 0 step 78: training accuarcy: 0.541\n",
      "Epoch 0 step 78: training loss: 3255.436446692102\n",
      "Epoch 0 step 79: training accuarcy: 0.56\n",
      "Epoch 0 step 79: training loss: 3175.2621917601027\n",
      "Epoch 0 step 80: training accuarcy: 0.5505\n",
      "Epoch 0 step 80: training loss: 3096.0176735139435\n",
      "Epoch 0 step 81: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 81: training loss: 3020.9218121930744\n",
      "Epoch 0 step 82: training accuarcy: 0.528\n",
      "Epoch 0 step 82: training loss: 2947.04438900239\n",
      "Epoch 0 step 83: training accuarcy: 0.54\n",
      "Epoch 0 step 83: training loss: 2876.44959119138\n",
      "Epoch 0 step 84: training accuarcy: 0.558\n",
      "Epoch 0 step 84: training loss: 2809.9597198547535\n",
      "Epoch 0 step 85: training accuarcy: 0.5455\n",
      "Epoch 0 step 85: training loss: 2745.2308242642584\n",
      "Epoch 0 step 86: training accuarcy: 0.5625\n",
      "Epoch 0 step 86: training loss: 2681.0612534797065\n",
      "Epoch 0 step 87: training accuarcy: 0.582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 87: training loss: 2623.9082366251396\n",
      "Epoch 0 step 88: training accuarcy: 0.5555\n",
      "Epoch 0 step 88: training loss: 2566.6791349567147\n",
      "Epoch 0 step 89: training accuarcy: 0.5655\n",
      "Epoch 0 step 89: training loss: 2512.556097300625\n",
      "Epoch 0 step 90: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 90: training loss: 2461.505345053759\n",
      "Epoch 0 step 91: training accuarcy: 0.578\n",
      "Epoch 0 step 91: training loss: 2411.553838109179\n",
      "Epoch 0 step 92: training accuarcy: 0.551\n",
      "Epoch 0 step 92: training loss: 2364.202697669666\n",
      "Epoch 0 step 93: training accuarcy: 0.5565\n",
      "Epoch 0 step 93: training loss: 2317.773404728965\n",
      "Epoch 0 step 94: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 94: training loss: 2276.52178179392\n",
      "Epoch 0 step 95: training accuarcy: 0.5235\n",
      "Epoch 0 step 95: training loss: 2232.1307986384318\n",
      "Epoch 0 step 96: training accuarcy: 0.5630000000000001\n",
      "Epoch 0 step 96: training loss: 2192.625100020993\n",
      "Epoch 0 step 97: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 97: training loss: 2155.2106195743554\n",
      "Epoch 0 step 98: training accuarcy: 0.553\n",
      "Epoch 0 step 98: training loss: 2117.488308188649\n",
      "Epoch 0 step 99: training accuarcy: 0.578\n",
      "Epoch 0 step 99: training loss: 2083.6941414266453\n",
      "Epoch 0 step 100: training accuarcy: 0.56\n",
      "Epoch 0 step 100: training loss: 2048.182189297741\n",
      "Epoch 0 step 101: training accuarcy: 0.5795\n",
      "Epoch 0 step 101: training loss: 2017.5877375530256\n",
      "Epoch 0 step 102: training accuarcy: 0.5615\n",
      "Epoch 0 step 102: training loss: 1987.1328277776615\n",
      "Epoch 0 step 103: training accuarcy: 0.5665\n",
      "Epoch 0 step 103: training loss: 1956.859635640069\n",
      "Epoch 0 step 104: training accuarcy: 0.5925\n",
      "Epoch 0 step 104: training loss: 1930.029294209096\n",
      "Epoch 0 step 105: training accuarcy: 0.5745\n",
      "Epoch 0 step 105: training loss: 1904.0552996336007\n",
      "Epoch 0 step 106: training accuarcy: 0.5675\n",
      "Epoch 0 step 106: training loss: 1878.7465476539235\n",
      "Epoch 0 step 107: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 107: training loss: 1853.9483096029285\n",
      "Epoch 0 step 108: training accuarcy: 0.577\n",
      "Epoch 0 step 108: training loss: 1831.4410779647499\n",
      "Epoch 0 step 109: training accuarcy: 0.5700000000000001\n",
      "Epoch 0 step 109: training loss: 1807.968443134478\n",
      "Epoch 0 step 110: training accuarcy: 0.5765\n",
      "Epoch 0 step 110: training loss: 1786.8159527748453\n",
      "Epoch 0 step 111: training accuarcy: 0.5845\n",
      "Epoch 0 step 111: training loss: 1768.6683794083126\n",
      "Epoch 0 step 112: training accuarcy: 0.544\n",
      "Epoch 0 step 112: training loss: 1748.0297658811842\n",
      "Epoch 0 step 113: training accuarcy: 0.578\n",
      "Epoch 0 step 113: training loss: 1728.562490499794\n",
      "Epoch 0 step 114: training accuarcy: 0.6065\n",
      "Epoch 0 step 114: training loss: 1713.2739428224554\n",
      "Epoch 0 step 115: training accuarcy: 0.5625\n",
      "Epoch 0 step 115: training loss: 1696.0196774712947\n",
      "Epoch 0 step 116: training accuarcy: 0.5765\n",
      "Epoch 0 step 116: training loss: 1679.974575716717\n",
      "Epoch 0 step 117: training accuarcy: 0.5805\n",
      "Epoch 0 step 117: training loss: 1664.630352219506\n",
      "Epoch 0 step 118: training accuarcy: 0.58\n",
      "Epoch 0 step 118: training loss: 1650.150834111001\n",
      "Epoch 0 step 119: training accuarcy: 0.5805\n",
      "Epoch 0 step 119: training loss: 1636.3499475784402\n",
      "Epoch 0 step 120: training accuarcy: 0.605\n",
      "Epoch 0 step 120: training loss: 1623.5631279706088\n",
      "Epoch 0 step 121: training accuarcy: 0.592\n",
      "Epoch 0 step 121: training loss: 1611.3084180763117\n",
      "Epoch 0 step 122: training accuarcy: 0.586\n",
      "Epoch 0 step 122: training loss: 1599.267253235142\n",
      "Epoch 0 step 123: training accuarcy: 0.587\n",
      "Epoch 0 step 123: training loss: 1588.8827164427032\n",
      "Epoch 0 step 124: training accuarcy: 0.5805\n",
      "Epoch 0 step 124: training loss: 1577.6997200433443\n",
      "Epoch 0 step 125: training accuarcy: 0.5995\n",
      "Epoch 0 step 125: training loss: 1568.300206681239\n",
      "Epoch 0 step 126: training accuarcy: 0.5755\n",
      "Epoch 0 step 126: training loss: 1557.5178756917517\n",
      "Epoch 0 step 127: training accuarcy: 0.589\n",
      "Epoch 0 step 127: training loss: 1548.7979336927056\n",
      "Epoch 0 step 128: training accuarcy: 0.5905\n",
      "Epoch 0 step 128: training loss: 1539.1471400042806\n",
      "Epoch 0 step 129: training accuarcy: 0.609\n",
      "Epoch 0 step 129: training loss: 1531.8790296947182\n",
      "Epoch 0 step 130: training accuarcy: 0.583\n",
      "Epoch 0 step 130: training loss: 1524.9050044483997\n",
      "Epoch 0 step 131: training accuarcy: 0.577\n",
      "Epoch 0 step 131: training loss: 1515.0569326344685\n",
      "Epoch 0 step 132: training accuarcy: 0.6175\n",
      "Epoch 0 step 132: training loss: 1508.9900436295745\n",
      "Epoch 0 step 133: training accuarcy: 0.604\n",
      "Epoch 0 step 133: training loss: 1502.2296434140949\n",
      "Epoch 0 step 134: training accuarcy: 0.604\n",
      "Epoch 0 step 134: training loss: 1496.5087684191594\n",
      "Epoch 0 step 135: training accuarcy: 0.606\n",
      "Epoch 0 step 135: training loss: 1489.7591050146057\n",
      "Epoch 0 step 136: training accuarcy: 0.5985\n",
      "Epoch 0 step 136: training loss: 1484.1301084569106\n",
      "Epoch 0 step 137: training accuarcy: 0.5935\n",
      "Epoch 0 step 137: training loss: 1478.0025267334488\n",
      "Epoch 0 step 138: training accuarcy: 0.6185\n",
      "Epoch 0 step 138: training loss: 1473.4626548379347\n",
      "Epoch 0 step 139: training accuarcy: 0.61\n",
      "Epoch 0 step 139: training loss: 1469.1498011708713\n",
      "Epoch 0 step 140: training accuarcy: 0.591\n",
      "Epoch 0 step 140: training loss: 1463.9721026482152\n",
      "Epoch 0 step 141: training accuarcy: 0.6\n",
      "Epoch 0 step 141: training loss: 1459.2383942003084\n",
      "Epoch 0 step 142: training accuarcy: 0.6155\n",
      "Epoch 0 step 142: training loss: 1455.9588336898419\n",
      "Epoch 0 step 143: training accuarcy: 0.5965\n",
      "Epoch 0 step 143: training loss: 1451.5184895089144\n",
      "Epoch 0 step 144: training accuarcy: 0.607\n",
      "Epoch 0 step 144: training loss: 1447.3882249788087\n",
      "Epoch 0 step 145: training accuarcy: 0.623\n",
      "Epoch 0 step 145: training loss: 1443.9449190690593\n",
      "Epoch 0 step 146: training accuarcy: 0.611\n",
      "Epoch 0 step 146: training loss: 1440.8622525866906\n",
      "Epoch 0 step 147: training accuarcy: 0.6265000000000001\n",
      "Epoch 0 step 147: training loss: 1437.9271078668592\n",
      "Epoch 0 step 148: training accuarcy: 0.612\n",
      "Epoch 0 step 148: training loss: 1434.897097298834\n",
      "Epoch 0 step 149: training accuarcy: 0.6205\n",
      "Epoch 0 step 149: training loss: 1432.1191711228482\n",
      "Epoch 0 step 150: training accuarcy: 0.615\n",
      "Epoch 0 step 150: training loss: 1428.676626445799\n",
      "Epoch 0 step 151: training accuarcy: 0.624\n",
      "Epoch 0 step 151: training loss: 1426.6890188995906\n",
      "Epoch 0 step 152: training accuarcy: 0.6145\n",
      "Epoch 0 step 152: training loss: 1424.6743854404008\n",
      "Epoch 0 step 153: training accuarcy: 0.6175\n",
      "Epoch 0 step 153: training loss: 1421.8521974782304\n",
      "Epoch 0 step 154: training accuarcy: 0.616\n",
      "Epoch 0 step 154: training loss: 1420.0255439776668\n",
      "Epoch 0 step 155: training accuarcy: 0.6095\n",
      "Epoch 0 step 155: training loss: 1417.2543214515192\n",
      "Epoch 0 step 156: training accuarcy: 0.6215\n",
      "Epoch 0 step 156: training loss: 1415.612112899163\n",
      "Epoch 0 step 157: training accuarcy: 0.6295000000000001\n",
      "Epoch 0 step 157: training loss: 1413.447620467856\n",
      "Epoch 0 step 158: training accuarcy: 0.6335000000000001\n",
      "Epoch 0 step 158: training loss: 1412.2750045658354\n",
      "Epoch 0 step 159: training accuarcy: 0.629\n",
      "Epoch 0 step 159: training loss: 1410.3200599026393\n",
      "Epoch 0 step 160: training accuarcy: 0.6435\n",
      "Epoch 0 step 160: training loss: 1407.8694327504656\n",
      "Epoch 0 step 161: training accuarcy: 0.639\n",
      "Epoch 0 step 161: training loss: 1407.8184096358045\n",
      "Epoch 0 step 162: training accuarcy: 0.6375000000000001\n",
      "Epoch 0 step 162: training loss: 1405.9328929100366\n",
      "Epoch 0 step 163: training accuarcy: 0.642\n",
      "Epoch 0 step 163: training loss: 1404.2085124741443\n",
      "Epoch 0 step 164: training accuarcy: 0.638\n",
      "Epoch 0 step 164: training loss: 1403.4551501877236\n",
      "Epoch 0 step 165: training accuarcy: 0.639\n",
      "Epoch 0 step 165: training loss: 1402.5574723433929\n",
      "Epoch 0 step 166: training accuarcy: 0.6385000000000001\n",
      "Epoch 0 step 166: training loss: 1400.9421036755882\n",
      "Epoch 0 step 167: training accuarcy: 0.6455\n",
      "Epoch 0 step 167: training loss: 1400.6985362428884\n",
      "Epoch 0 step 168: training accuarcy: 0.638\n",
      "Epoch 0 step 168: training loss: 1399.247213424011\n",
      "Epoch 0 step 169: training accuarcy: 0.642\n",
      "Epoch 0 step 169: training loss: 1398.1913490776433\n",
      "Epoch 0 step 170: training accuarcy: 0.6365000000000001\n",
      "Epoch 0 step 170: training loss: 1397.4463109542419\n",
      "Epoch 0 step 171: training accuarcy: 0.6465\n",
      "Epoch 0 step 171: training loss: 1396.566831150004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 172: training accuarcy: 0.6435\n",
      "Epoch 0 step 172: training loss: 1395.9451786591947\n",
      "Epoch 0 step 173: training accuarcy: 0.6495\n",
      "Epoch 0 step 173: training loss: 1395.700017244339\n",
      "Epoch 0 step 174: training accuarcy: 0.633\n",
      "Epoch 0 step 174: training loss: 1394.4504209212423\n",
      "Epoch 0 step 175: training accuarcy: 0.6525\n",
      "Epoch 0 step 175: training loss: 1394.3784052799422\n",
      "Epoch 0 step 176: training accuarcy: 0.633\n",
      "Epoch 0 step 176: training loss: 1393.6568341847271\n",
      "Epoch 0 step 177: training accuarcy: 0.6415\n",
      "Epoch 0 step 177: training loss: 1392.4720800108526\n",
      "Epoch 0 step 178: training accuarcy: 0.669\n",
      "Epoch 0 step 178: training loss: 1391.5860948089157\n",
      "Epoch 0 step 179: training accuarcy: 0.6575\n",
      "Epoch 0 step 179: training loss: 1391.6421523297481\n",
      "Epoch 0 step 180: training accuarcy: 0.6375000000000001\n",
      "Epoch 0 step 180: training loss: 1391.7604724936352\n",
      "Epoch 0 step 181: training accuarcy: 0.644\n",
      "Epoch 0 step 181: training loss: 1390.3580626694768\n",
      "Epoch 0 step 182: training accuarcy: 0.6625\n",
      "Epoch 0 step 182: training loss: 1390.11082185267\n",
      "Epoch 0 step 183: training accuarcy: 0.653\n",
      "Epoch 0 step 183: training loss: 1390.1118967133843\n",
      "Epoch 0 step 184: training accuarcy: 0.662\n",
      "Epoch 0 step 184: training loss: 1389.8931586502\n",
      "Epoch 0 step 185: training accuarcy: 0.6555\n",
      "Epoch 0 step 185: training loss: 1389.408892872928\n",
      "Epoch 0 step 186: training accuarcy: 0.6445\n",
      "Epoch 0 step 186: training loss: 1389.242514240251\n",
      "Epoch 0 step 187: training accuarcy: 0.6475\n",
      "Epoch 0 step 187: training loss: 1388.1562238050437\n",
      "Epoch 0 step 188: training accuarcy: 0.6645\n",
      "Epoch 0 step 188: training loss: 1388.7393541571298\n",
      "Epoch 0 step 189: training accuarcy: 0.6535\n",
      "Epoch 0 step 189: training loss: 1388.079570605442\n",
      "Epoch 0 step 190: training accuarcy: 0.6635\n",
      "Epoch 0 step 190: training loss: 1387.987710482103\n",
      "Epoch 0 step 191: training accuarcy: 0.6605\n",
      "Epoch 0 step 191: training loss: 1387.7871226328334\n",
      "Epoch 0 step 192: training accuarcy: 0.65\n",
      "Epoch 0 step 192: training loss: 1387.916747625886\n",
      "Epoch 0 step 193: training accuarcy: 0.6435\n",
      "Epoch 0 step 193: training loss: 1386.702612097039\n",
      "Epoch 0 step 194: training accuarcy: 0.6585\n",
      "Epoch 0 step 194: training loss: 1387.2392931193795\n",
      "Epoch 0 step 195: training accuarcy: 0.6625\n",
      "Epoch 0 step 195: training loss: 1387.1200899250325\n",
      "Epoch 0 step 196: training accuarcy: 0.6585\n",
      "Epoch 0 step 196: training loss: 1386.732524973032\n",
      "Epoch 0 step 197: training accuarcy: 0.6425\n",
      "Epoch 0 step 197: training loss: 1386.1372995533018\n",
      "Epoch 0 step 198: training accuarcy: 0.6595\n",
      "Epoch 0 step 198: training loss: 1385.1827732940492\n",
      "Epoch 0 step 199: training accuarcy: 0.687\n",
      "Epoch 0 step 199: training loss: 1385.9718901109486\n",
      "Epoch 0 step 200: training accuarcy: 0.6625\n",
      "Epoch 0 step 200: training loss: 1386.2285693434912\n",
      "Epoch 0 step 201: training accuarcy: 0.6635\n",
      "Epoch 0 step 201: training loss: 1385.582620696416\n",
      "Epoch 0 step 202: training accuarcy: 0.6775\n",
      "Epoch 0 step 202: training loss: 1385.3141317760037\n",
      "Epoch 0 step 203: training accuarcy: 0.673\n",
      "Epoch 0 step 203: training loss: 1385.2774831404417\n",
      "Epoch 0 step 204: training accuarcy: 0.6805\n",
      "Epoch 0 step 204: training loss: 1385.355530175382\n",
      "Epoch 0 step 205: training accuarcy: 0.663\n",
      "Epoch 0 step 205: training loss: 1385.4147240828127\n",
      "Epoch 0 step 206: training accuarcy: 0.6665\n",
      "Epoch 0 step 206: training loss: 1385.5507694483013\n",
      "Epoch 0 step 207: training accuarcy: 0.672\n",
      "Epoch 0 step 207: training loss: 1385.0191374985181\n",
      "Epoch 0 step 208: training accuarcy: 0.6775\n",
      "Epoch 0 step 208: training loss: 1384.3233643863246\n",
      "Epoch 0 step 209: training accuarcy: 0.6945\n",
      "Epoch 0 step 209: training loss: 1385.029138614179\n",
      "Epoch 0 step 210: training accuarcy: 0.6735\n",
      "Epoch 0 step 210: training loss: 1384.905618247013\n",
      "Epoch 0 step 211: training accuarcy: 0.6805\n",
      "Epoch 0 step 211: training loss: 1384.56836172904\n",
      "Epoch 0 step 212: training accuarcy: 0.681\n",
      "Epoch 0 step 212: training loss: 1384.586231525303\n",
      "Epoch 0 step 213: training accuarcy: 0.682\n",
      "Epoch 0 step 213: training loss: 1384.8954317361809\n",
      "Epoch 0 step 214: training accuarcy: 0.6775\n",
      "Epoch 0 step 214: training loss: 1384.024286433621\n",
      "Epoch 0 step 215: training accuarcy: 0.6930000000000001\n",
      "Epoch 0 step 215: training loss: 1384.880686782525\n",
      "Epoch 0 step 216: training accuarcy: 0.662\n",
      "Epoch 0 step 216: training loss: 1383.751149413638\n",
      "Epoch 0 step 217: training accuarcy: 0.6815\n",
      "Epoch 0 step 217: training loss: 1384.6576560598896\n",
      "Epoch 0 step 218: training accuarcy: 0.675\n",
      "Epoch 0 step 218: training loss: 1385.0362267959363\n",
      "Epoch 0 step 219: training accuarcy: 0.6615\n",
      "Epoch 0 step 219: training loss: 1384.1751812268674\n",
      "Epoch 0 step 220: training accuarcy: 0.676\n",
      "Epoch 0 step 220: training loss: 1383.749916983071\n",
      "Epoch 0 step 221: training accuarcy: 0.686\n",
      "Epoch 0 step 221: training loss: 1384.122471034128\n",
      "Epoch 0 step 222: training accuarcy: 0.687\n",
      "Epoch 0 step 222: training loss: 1384.1660647369347\n",
      "Epoch 0 step 223: training accuarcy: 0.6685\n",
      "Epoch 0 step 223: training loss: 1384.5191791640004\n",
      "Epoch 0 step 224: training accuarcy: 0.6625\n",
      "Epoch 0 step 224: training loss: 1383.91830872152\n",
      "Epoch 0 step 225: training accuarcy: 0.684\n",
      "Epoch 0 step 225: training loss: 1383.9410470641053\n",
      "Epoch 0 step 226: training accuarcy: 0.682\n",
      "Epoch 0 step 226: training loss: 1383.875989873419\n",
      "Epoch 0 step 227: training accuarcy: 0.684\n",
      "Epoch 0 step 227: training loss: 1384.8408279860346\n",
      "Epoch 0 step 228: training accuarcy: 0.6505\n",
      "Epoch 0 step 228: training loss: 1383.784251380308\n",
      "Epoch 0 step 229: training accuarcy: 0.682\n",
      "Epoch 0 step 229: training loss: 1384.3461591982739\n",
      "Epoch 0 step 230: training accuarcy: 0.664\n",
      "Epoch 0 step 230: training loss: 1384.1344687628966\n",
      "Epoch 0 step 231: training accuarcy: 0.6705\n",
      "Epoch 0 step 231: training loss: 1383.5981590049676\n",
      "Epoch 0 step 232: training accuarcy: 0.6785\n",
      "Epoch 0 step 232: training loss: 1383.2756170757964\n",
      "Epoch 0 step 233: training accuarcy: 0.6935\n",
      "Epoch 0 step 233: training loss: 1384.1454925163976\n",
      "Epoch 0 step 234: training accuarcy: 0.674\n",
      "Epoch 0 step 234: training loss: 1382.9776673958113\n",
      "Epoch 0 step 235: training accuarcy: 0.682\n",
      "Epoch 0 step 235: training loss: 1384.493988635651\n",
      "Epoch 0 step 236: training accuarcy: 0.66\n",
      "Epoch 0 step 236: training loss: 1383.8267508084705\n",
      "Epoch 0 step 237: training accuarcy: 0.6715\n",
      "Epoch 0 step 237: training loss: 1384.2861279088536\n",
      "Epoch 0 step 238: training accuarcy: 0.673\n",
      "Epoch 0 step 238: training loss: 1384.0470875193557\n",
      "Epoch 0 step 239: training accuarcy: 0.6645\n",
      "Epoch 0 step 239: training loss: 1383.9043636203091\n",
      "Epoch 0 step 240: training accuarcy: 0.684\n",
      "Epoch 0 step 240: training loss: 1384.1821387319128\n",
      "Epoch 0 step 241: training accuarcy: 0.678\n",
      "Epoch 0 step 241: training loss: 1383.526250122348\n",
      "Epoch 0 step 242: training accuarcy: 0.6955\n",
      "Epoch 0 step 242: training loss: 1383.638021709905\n",
      "Epoch 0 step 243: training accuarcy: 0.6745\n",
      "Epoch 0 step 243: training loss: 1384.1052314243188\n",
      "Epoch 0 step 244: training accuarcy: 0.682\n",
      "Epoch 0 step 244: training loss: 1383.624442842596\n",
      "Epoch 0 step 245: training accuarcy: 0.683\n",
      "Epoch 0 step 245: training loss: 1383.402385351955\n",
      "Epoch 0 step 246: training accuarcy: 0.682\n",
      "Epoch 0 step 246: training loss: 1383.431816912633\n",
      "Epoch 0 step 247: training accuarcy: 0.684\n",
      "Epoch 0 step 247: training loss: 1383.541548125753\n",
      "Epoch 0 step 248: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 248: training loss: 1384.0082070517467\n",
      "Epoch 0 step 249: training accuarcy: 0.685\n",
      "Epoch 0 step 249: training loss: 1383.9463882769276\n",
      "Epoch 0 step 250: training accuarcy: 0.678\n",
      "Epoch 0 step 250: training loss: 1382.3454331938883\n",
      "Epoch 0 step 251: training accuarcy: 0.7055\n",
      "Epoch 0 step 251: training loss: 1383.2728034639297\n",
      "Epoch 0 step 252: training accuarcy: 0.683\n",
      "Epoch 0 step 252: training loss: 1383.9357088544934\n",
      "Epoch 0 step 253: training accuarcy: 0.673\n",
      "Epoch 0 step 253: training loss: 1383.685683191207\n",
      "Epoch 0 step 254: training accuarcy: 0.6975\n",
      "Epoch 0 step 254: training loss: 1384.0112685668716\n",
      "Epoch 0 step 255: training accuarcy: 0.675\n",
      "Epoch 0 step 255: training loss: 1383.460146662415\n",
      "Epoch 0 step 256: training accuarcy: 0.682\n",
      "Epoch 0 step 256: training loss: 1382.545645859507\n",
      "Epoch 0 step 257: training accuarcy: 0.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 257: training loss: 1383.2192283649413\n",
      "Epoch 0 step 258: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 258: training loss: 1383.1231538035497\n",
      "Epoch 0 step 259: training accuarcy: 0.6945\n",
      "Epoch 0 step 259: training loss: 1382.7370542033436\n",
      "Epoch 0 step 260: training accuarcy: 0.6945\n",
      "Epoch 0 step 260: training loss: 1383.1388806180219\n",
      "Epoch 0 step 261: training accuarcy: 0.6950000000000001\n",
      "Epoch 0 step 261: training loss: 1383.837933860504\n",
      "Epoch 0 step 262: training accuarcy: 0.682\n",
      "Epoch 0 step 262: training loss: 542.5687598480471\n",
      "Epoch 0 step 263: training accuarcy: 0.7\n",
      "Epoch 0: train loss 5339.627408337506, train accuarcy 0.6063984632492065\n",
      "Epoch 0: valid loss 1365.690356413305, valid accuarcy 0.7049726843833923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████                                                                                                                                     | 1/8 [01:57<13:40, 117.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1382.2335485599704\n",
      "Epoch 1 step 264: training accuarcy: 0.7145\n",
      "Epoch 1 step 264: training loss: 1382.9602796375345\n",
      "Epoch 1 step 265: training accuarcy: 0.6935\n",
      "Epoch 1 step 265: training loss: 1382.3812017308117\n",
      "Epoch 1 step 266: training accuarcy: 0.6970000000000001\n",
      "Epoch 1 step 266: training loss: 1382.3396746286323\n",
      "Epoch 1 step 267: training accuarcy: 0.7055\n",
      "Epoch 1 step 267: training loss: 1382.8357907608686\n",
      "Epoch 1 step 268: training accuarcy: 0.6910000000000001\n",
      "Epoch 1 step 268: training loss: 1383.0191315930865\n",
      "Epoch 1 step 269: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 269: training loss: 1382.7444156974543\n",
      "Epoch 1 step 270: training accuarcy: 0.7035\n",
      "Epoch 1 step 270: training loss: 1382.4613726292046\n",
      "Epoch 1 step 271: training accuarcy: 0.712\n",
      "Epoch 1 step 271: training loss: 1382.3746117934768\n",
      "Epoch 1 step 272: training accuarcy: 0.713\n",
      "Epoch 1 step 272: training loss: 1381.9780242194338\n",
      "Epoch 1 step 273: training accuarcy: 0.71\n",
      "Epoch 1 step 273: training loss: 1382.4016348952387\n",
      "Epoch 1 step 274: training accuarcy: 0.709\n",
      "Epoch 1 step 274: training loss: 1382.4881816467027\n",
      "Epoch 1 step 275: training accuarcy: 0.7105\n",
      "Epoch 1 step 275: training loss: 1382.5728814959002\n",
      "Epoch 1 step 276: training accuarcy: 0.6975\n",
      "Epoch 1 step 276: training loss: 1382.6694043164919\n",
      "Epoch 1 step 277: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 277: training loss: 1382.8725825351626\n",
      "Epoch 1 step 278: training accuarcy: 0.6990000000000001\n",
      "Epoch 1 step 278: training loss: 1382.5722281911612\n",
      "Epoch 1 step 279: training accuarcy: 0.711\n",
      "Epoch 1 step 279: training loss: 1382.989986540595\n",
      "Epoch 1 step 280: training accuarcy: 0.6970000000000001\n",
      "Epoch 1 step 280: training loss: 1382.4498250495112\n",
      "Epoch 1 step 281: training accuarcy: 0.7000000000000001\n",
      "Epoch 1 step 281: training loss: 1382.6148870418656\n",
      "Epoch 1 step 282: training accuarcy: 0.7075\n",
      "Epoch 1 step 282: training loss: 1382.8973441892415\n",
      "Epoch 1 step 283: training accuarcy: 0.6985\n",
      "Epoch 1 step 283: training loss: 1382.342745854437\n",
      "Epoch 1 step 284: training accuarcy: 0.715\n",
      "Epoch 1 step 284: training loss: 1382.8101472039484\n",
      "Epoch 1 step 285: training accuarcy: 0.6995\n",
      "Epoch 1 step 285: training loss: 1382.559408989927\n",
      "Epoch 1 step 286: training accuarcy: 0.7055\n",
      "Epoch 1 step 286: training loss: 1382.3836590881988\n",
      "Epoch 1 step 287: training accuarcy: 0.728\n",
      "Epoch 1 step 287: training loss: 1383.24032873992\n",
      "Epoch 1 step 288: training accuarcy: 0.7015\n",
      "Epoch 1 step 288: training loss: 1383.059556101414\n",
      "Epoch 1 step 289: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 289: training loss: 1382.221875370961\n",
      "Epoch 1 step 290: training accuarcy: 0.7145\n",
      "Epoch 1 step 290: training loss: 1382.3136501655038\n",
      "Epoch 1 step 291: training accuarcy: 0.7115\n",
      "Epoch 1 step 291: training loss: 1382.8100911063511\n",
      "Epoch 1 step 292: training accuarcy: 0.7005\n",
      "Epoch 1 step 292: training loss: 1382.2895316955096\n",
      "Epoch 1 step 293: training accuarcy: 0.7195\n",
      "Epoch 1 step 293: training loss: 1382.8631075888304\n",
      "Epoch 1 step 294: training accuarcy: 0.708\n",
      "Epoch 1 step 294: training loss: 1383.0049576945264\n",
      "Epoch 1 step 295: training accuarcy: 0.6985\n",
      "Epoch 1 step 295: training loss: 1382.7720611203777\n",
      "Epoch 1 step 296: training accuarcy: 0.6945\n",
      "Epoch 1 step 296: training loss: 1382.8479229339248\n",
      "Epoch 1 step 297: training accuarcy: 0.6955\n",
      "Epoch 1 step 297: training loss: 1382.9980209847804\n",
      "Epoch 1 step 298: training accuarcy: 0.7055\n",
      "Epoch 1 step 298: training loss: 1382.7520864304465\n",
      "Epoch 1 step 299: training accuarcy: 0.6925\n",
      "Epoch 1 step 299: training loss: 1382.4786751564586\n",
      "Epoch 1 step 300: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 300: training loss: 1382.1290875367706\n",
      "Epoch 1 step 301: training accuarcy: 0.728\n",
      "Epoch 1 step 301: training loss: 1382.4974313738633\n",
      "Epoch 1 step 302: training accuarcy: 0.6975\n",
      "Epoch 1 step 302: training loss: 1382.6770032753593\n",
      "Epoch 1 step 303: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 303: training loss: 1382.7713600237364\n",
      "Epoch 1 step 304: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 304: training loss: 1382.8621337097077\n",
      "Epoch 1 step 305: training accuarcy: 0.707\n",
      "Epoch 1 step 305: training loss: 1383.3242615397905\n",
      "Epoch 1 step 306: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 306: training loss: 1383.5934527122013\n",
      "Epoch 1 step 307: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 307: training loss: 1381.824855883459\n",
      "Epoch 1 step 308: training accuarcy: 0.7275\n",
      "Epoch 1 step 308: training loss: 1382.3930434646102\n",
      "Epoch 1 step 309: training accuarcy: 0.7065\n",
      "Epoch 1 step 309: training loss: 1382.6583327582653\n",
      "Epoch 1 step 310: training accuarcy: 0.7045\n",
      "Epoch 1 step 310: training loss: 1382.0209303601848\n",
      "Epoch 1 step 311: training accuarcy: 0.7115\n",
      "Epoch 1 step 311: training loss: 1382.7977039989469\n",
      "Epoch 1 step 312: training accuarcy: 0.685\n",
      "Epoch 1 step 312: training loss: 1383.019080395842\n",
      "Epoch 1 step 313: training accuarcy: 0.6945\n",
      "Epoch 1 step 313: training loss: 1382.5358367435758\n",
      "Epoch 1 step 314: training accuarcy: 0.7085\n",
      "Epoch 1 step 314: training loss: 1383.2737207765365\n",
      "Epoch 1 step 315: training accuarcy: 0.6950000000000001\n",
      "Epoch 1 step 315: training loss: 1382.6671606854472\n",
      "Epoch 1 step 316: training accuarcy: 0.7045\n",
      "Epoch 1 step 316: training loss: 1383.0869907240597\n",
      "Epoch 1 step 317: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 317: training loss: 1383.1435214181229\n",
      "Epoch 1 step 318: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 318: training loss: 1382.3714474553335\n",
      "Epoch 1 step 319: training accuarcy: 0.6970000000000001\n",
      "Epoch 1 step 319: training loss: 1383.2786791672784\n",
      "Epoch 1 step 320: training accuarcy: 0.685\n",
      "Epoch 1 step 320: training loss: 1382.3989402902384\n",
      "Epoch 1 step 321: training accuarcy: 0.7115\n",
      "Epoch 1 step 321: training loss: 1382.7363632474098\n",
      "Epoch 1 step 322: training accuarcy: 0.7015\n",
      "Epoch 1 step 322: training loss: 1382.863381901058\n",
      "Epoch 1 step 323: training accuarcy: 0.6975\n",
      "Epoch 1 step 323: training loss: 1383.127186157935\n",
      "Epoch 1 step 324: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 324: training loss: 1382.7163447496152\n",
      "Epoch 1 step 325: training accuarcy: 0.714\n",
      "Epoch 1 step 325: training loss: 1382.7420923135467\n",
      "Epoch 1 step 326: training accuarcy: 0.685\n",
      "Epoch 1 step 326: training loss: 1382.1180049430075\n",
      "Epoch 1 step 327: training accuarcy: 0.7035\n",
      "Epoch 1 step 327: training loss: 1382.4481858064817\n",
      "Epoch 1 step 328: training accuarcy: 0.709\n",
      "Epoch 1 step 328: training loss: 1382.5765228475843\n",
      "Epoch 1 step 329: training accuarcy: 0.7105\n",
      "Epoch 1 step 329: training loss: 1382.7503380738178\n",
      "Epoch 1 step 330: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 330: training loss: 1382.1778726630978\n",
      "Epoch 1 step 331: training accuarcy: 0.7035\n",
      "Epoch 1 step 331: training loss: 1382.9450188672738\n",
      "Epoch 1 step 332: training accuarcy: 0.7025\n",
      "Epoch 1 step 332: training loss: 1382.4529152500518\n",
      "Epoch 1 step 333: training accuarcy: 0.7055\n",
      "Epoch 1 step 333: training loss: 1382.763677533553\n",
      "Epoch 1 step 334: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 334: training loss: 1382.0700314988462\n",
      "Epoch 1 step 335: training accuarcy: 0.7095\n",
      "Epoch 1 step 335: training loss: 1382.5343524906066\n",
      "Epoch 1 step 336: training accuarcy: 0.7065\n",
      "Epoch 1 step 336: training loss: 1383.5428459462762\n",
      "Epoch 1 step 337: training accuarcy: 0.6955\n",
      "Epoch 1 step 337: training loss: 1383.683054127298\n",
      "Epoch 1 step 338: training accuarcy: 0.666\n",
      "Epoch 1 step 338: training loss: 1383.7439754024083\n",
      "Epoch 1 step 339: training accuarcy: 0.6915\n",
      "Epoch 1 step 339: training loss: 1382.953564442908\n",
      "Epoch 1 step 340: training accuarcy: 0.6965\n",
      "Epoch 1 step 340: training loss: 1382.45249259378\n",
      "Epoch 1 step 341: training accuarcy: 0.7095\n",
      "Epoch 1 step 341: training loss: 1382.8236741342278\n",
      "Epoch 1 step 342: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 342: training loss: 1382.3631677533524\n",
      "Epoch 1 step 343: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 343: training loss: 1382.1858651741215\n",
      "Epoch 1 step 344: training accuarcy: 0.7145\n",
      "Epoch 1 step 344: training loss: 1383.3693580686609\n",
      "Epoch 1 step 345: training accuarcy: 0.681\n",
      "Epoch 1 step 345: training loss: 1382.2752891915509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 346: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 346: training loss: 1383.0737209036376\n",
      "Epoch 1 step 347: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 347: training loss: 1382.6014479690425\n",
      "Epoch 1 step 348: training accuarcy: 0.711\n",
      "Epoch 1 step 348: training loss: 1383.448127953567\n",
      "Epoch 1 step 349: training accuarcy: 0.6865\n",
      "Epoch 1 step 349: training loss: 1382.8779757852094\n",
      "Epoch 1 step 350: training accuarcy: 0.6955\n",
      "Epoch 1 step 350: training loss: 1382.9031838050273\n",
      "Epoch 1 step 351: training accuarcy: 0.6935\n",
      "Epoch 1 step 351: training loss: 1382.0983419460795\n",
      "Epoch 1 step 352: training accuarcy: 0.7135\n",
      "Epoch 1 step 352: training loss: 1382.3576242372903\n",
      "Epoch 1 step 353: training accuarcy: 0.6955\n",
      "Epoch 1 step 353: training loss: 1382.0952845453785\n",
      "Epoch 1 step 354: training accuarcy: 0.714\n",
      "Epoch 1 step 354: training loss: 1383.0207625291544\n",
      "Epoch 1 step 355: training accuarcy: 0.687\n",
      "Epoch 1 step 355: training loss: 1382.7337704577392\n",
      "Epoch 1 step 356: training accuarcy: 0.709\n",
      "Epoch 1 step 356: training loss: 1382.4965608894433\n",
      "Epoch 1 step 357: training accuarcy: 0.7045\n",
      "Epoch 1 step 357: training loss: 1382.2940541751395\n",
      "Epoch 1 step 358: training accuarcy: 0.706\n",
      "Epoch 1 step 358: training loss: 1382.8596365774904\n",
      "Epoch 1 step 359: training accuarcy: 0.708\n",
      "Epoch 1 step 359: training loss: 1382.575812009294\n",
      "Epoch 1 step 360: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 360: training loss: 1383.5251096622308\n",
      "Epoch 1 step 361: training accuarcy: 0.6845\n",
      "Epoch 1 step 361: training loss: 1382.2257348685675\n",
      "Epoch 1 step 362: training accuarcy: 0.7015\n",
      "Epoch 1 step 362: training loss: 1383.1397472585068\n",
      "Epoch 1 step 363: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 363: training loss: 1382.3878863994446\n",
      "Epoch 1 step 364: training accuarcy: 0.7015\n",
      "Epoch 1 step 364: training loss: 1383.6576216053254\n",
      "Epoch 1 step 365: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 365: training loss: 1382.2877322484983\n",
      "Epoch 1 step 366: training accuarcy: 0.7095\n",
      "Epoch 1 step 366: training loss: 1383.2693544120646\n",
      "Epoch 1 step 367: training accuarcy: 0.6895\n",
      "Epoch 1 step 367: training loss: 1382.7290806584544\n",
      "Epoch 1 step 368: training accuarcy: 0.7045\n",
      "Epoch 1 step 368: training loss: 1382.080162766959\n",
      "Epoch 1 step 369: training accuarcy: 0.6925\n",
      "Epoch 1 step 369: training loss: 1383.0366648887891\n",
      "Epoch 1 step 370: training accuarcy: 0.7055\n",
      "Epoch 1 step 370: training loss: 1383.13635880676\n",
      "Epoch 1 step 371: training accuarcy: 0.6905\n",
      "Epoch 1 step 371: training loss: 1383.216293951977\n",
      "Epoch 1 step 372: training accuarcy: 0.687\n",
      "Epoch 1 step 372: training loss: 1383.2228579565333\n",
      "Epoch 1 step 373: training accuarcy: 0.6925\n",
      "Epoch 1 step 373: training loss: 1383.4373859572088\n",
      "Epoch 1 step 374: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 374: training loss: 1383.1629780382711\n",
      "Epoch 1 step 375: training accuarcy: 0.6895\n",
      "Epoch 1 step 375: training loss: 1382.8312046117308\n",
      "Epoch 1 step 376: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 376: training loss: 1383.7600539712373\n",
      "Epoch 1 step 377: training accuarcy: 0.671\n",
      "Epoch 1 step 377: training loss: 1382.017460866032\n",
      "Epoch 1 step 378: training accuarcy: 0.7085\n",
      "Epoch 1 step 378: training loss: 1382.39506489246\n",
      "Epoch 1 step 379: training accuarcy: 0.716\n",
      "Epoch 1 step 379: training loss: 1382.6688022609305\n",
      "Epoch 1 step 380: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 380: training loss: 1383.1800888142582\n",
      "Epoch 1 step 381: training accuarcy: 0.6955\n",
      "Epoch 1 step 381: training loss: 1383.0396582095807\n",
      "Epoch 1 step 382: training accuarcy: 0.6935\n",
      "Epoch 1 step 382: training loss: 1383.6150883038658\n",
      "Epoch 1 step 383: training accuarcy: 0.6785\n",
      "Epoch 1 step 383: training loss: 1382.6747286756415\n",
      "Epoch 1 step 384: training accuarcy: 0.713\n",
      "Epoch 1 step 384: training loss: 1383.1454294624407\n",
      "Epoch 1 step 385: training accuarcy: 0.6970000000000001\n",
      "Epoch 1 step 385: training loss: 1383.09339387665\n",
      "Epoch 1 step 386: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 386: training loss: 1382.8052056708325\n",
      "Epoch 1 step 387: training accuarcy: 0.7000000000000001\n",
      "Epoch 1 step 387: training loss: 1383.2468892734469\n",
      "Epoch 1 step 388: training accuarcy: 0.7015\n",
      "Epoch 1 step 388: training loss: 1382.1019031836345\n",
      "Epoch 1 step 389: training accuarcy: 0.711\n",
      "Epoch 1 step 389: training loss: 1383.1006975954606\n",
      "Epoch 1 step 390: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 390: training loss: 1383.197075367573\n",
      "Epoch 1 step 391: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 391: training loss: 1382.8877492674628\n",
      "Epoch 1 step 392: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 392: training loss: 1382.6046529970356\n",
      "Epoch 1 step 393: training accuarcy: 0.7165\n",
      "Epoch 1 step 393: training loss: 1382.5622770353077\n",
      "Epoch 1 step 394: training accuarcy: 0.6865\n",
      "Epoch 1 step 394: training loss: 1383.6587184191258\n",
      "Epoch 1 step 395: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 395: training loss: 1382.5434318900743\n",
      "Epoch 1 step 396: training accuarcy: 0.7025\n",
      "Epoch 1 step 396: training loss: 1382.1802086228213\n",
      "Epoch 1 step 397: training accuarcy: 0.7075\n",
      "Epoch 1 step 397: training loss: 1381.8930079338775\n",
      "Epoch 1 step 398: training accuarcy: 0.72\n",
      "Epoch 1 step 398: training loss: 1382.9755007556744\n",
      "Epoch 1 step 399: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 399: training loss: 1382.2606658550285\n",
      "Epoch 1 step 400: training accuarcy: 0.719\n",
      "Epoch 1 step 400: training loss: 1382.9426975568358\n",
      "Epoch 1 step 401: training accuarcy: 0.6925\n",
      "Epoch 1 step 401: training loss: 1382.8972360005935\n",
      "Epoch 1 step 402: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 402: training loss: 1381.9043574478885\n",
      "Epoch 1 step 403: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 403: training loss: 1383.3694652858446\n",
      "Epoch 1 step 404: training accuarcy: 0.7015\n",
      "Epoch 1 step 404: training loss: 1382.8721934055147\n",
      "Epoch 1 step 405: training accuarcy: 0.7115\n",
      "Epoch 1 step 405: training loss: 1383.192417259419\n",
      "Epoch 1 step 406: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 406: training loss: 1382.9457741651177\n",
      "Epoch 1 step 407: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 407: training loss: 1382.334513148169\n",
      "Epoch 1 step 408: training accuarcy: 0.708\n",
      "Epoch 1 step 408: training loss: 1382.6530303203574\n",
      "Epoch 1 step 409: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 409: training loss: 1383.0165562848129\n",
      "Epoch 1 step 410: training accuarcy: 0.6965\n",
      "Epoch 1 step 410: training loss: 1382.6679105277717\n",
      "Epoch 1 step 411: training accuarcy: 0.685\n",
      "Epoch 1 step 411: training loss: 1382.968518655626\n",
      "Epoch 1 step 412: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 412: training loss: 1382.6678004728678\n",
      "Epoch 1 step 413: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 413: training loss: 1383.037949608211\n",
      "Epoch 1 step 414: training accuarcy: 0.6915\n",
      "Epoch 1 step 414: training loss: 1383.0069907682682\n",
      "Epoch 1 step 415: training accuarcy: 0.6865\n",
      "Epoch 1 step 415: training loss: 1383.6353376523057\n",
      "Epoch 1 step 416: training accuarcy: 0.6970000000000001\n",
      "Epoch 1 step 416: training loss: 1383.1530910329955\n",
      "Epoch 1 step 417: training accuarcy: 0.6910000000000001\n",
      "Epoch 1 step 417: training loss: 1383.0379156894774\n",
      "Epoch 1 step 418: training accuarcy: 0.6930000000000001\n",
      "Epoch 1 step 418: training loss: 1382.9825885040232\n",
      "Epoch 1 step 419: training accuarcy: 0.6955\n",
      "Epoch 1 step 419: training loss: 1383.2352241459691\n",
      "Epoch 1 step 420: training accuarcy: 0.687\n",
      "Epoch 1 step 420: training loss: 1383.1750794394839\n",
      "Epoch 1 step 421: training accuarcy: 0.6955\n",
      "Epoch 1 step 421: training loss: 1383.8178843020044\n",
      "Epoch 1 step 422: training accuarcy: 0.681\n",
      "Epoch 1 step 422: training loss: 1382.2066945484269\n",
      "Epoch 1 step 423: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 423: training loss: 1383.208916557179\n",
      "Epoch 1 step 424: training accuarcy: 0.7000000000000001\n",
      "Epoch 1 step 424: training loss: 1382.8335207150758\n",
      "Epoch 1 step 425: training accuarcy: 0.6915\n",
      "Epoch 1 step 425: training loss: 1382.8253609762783\n",
      "Epoch 1 step 426: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 426: training loss: 1383.0911935566878\n",
      "Epoch 1 step 427: training accuarcy: 0.7000000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 427: training loss: 1383.3466869210233\n",
      "Epoch 1 step 428: training accuarcy: 0.6815\n",
      "Epoch 1 step 428: training loss: 1383.741513599892\n",
      "Epoch 1 step 429: training accuarcy: 0.6875\n",
      "Epoch 1 step 429: training loss: 1383.1631343863094\n",
      "Epoch 1 step 430: training accuarcy: 0.6995\n",
      "Epoch 1 step 430: training loss: 1383.1306379368655\n",
      "Epoch 1 step 431: training accuarcy: 0.6885\n",
      "Epoch 1 step 431: training loss: 1381.902200007858\n",
      "Epoch 1 step 432: training accuarcy: 0.6975\n",
      "Epoch 1 step 432: training loss: 1383.310444263779\n",
      "Epoch 1 step 433: training accuarcy: 0.705\n",
      "Epoch 1 step 433: training loss: 1384.044912644757\n",
      "Epoch 1 step 434: training accuarcy: 0.6745\n",
      "Epoch 1 step 434: training loss: 1383.2501243331633\n",
      "Epoch 1 step 435: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 435: training loss: 1382.6929187817564\n",
      "Epoch 1 step 436: training accuarcy: 0.7135\n",
      "Epoch 1 step 436: training loss: 1383.4889490217479\n",
      "Epoch 1 step 437: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 437: training loss: 1382.5983746280212\n",
      "Epoch 1 step 438: training accuarcy: 0.6975\n",
      "Epoch 1 step 438: training loss: 1383.0484553735173\n",
      "Epoch 1 step 439: training accuarcy: 0.7000000000000001\n",
      "Epoch 1 step 439: training loss: 1382.4796159246912\n",
      "Epoch 1 step 440: training accuarcy: 0.6975\n",
      "Epoch 1 step 440: training loss: 1383.2089222315794\n",
      "Epoch 1 step 441: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 441: training loss: 1382.7868809349316\n",
      "Epoch 1 step 442: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 442: training loss: 1383.501629422133\n",
      "Epoch 1 step 443: training accuarcy: 0.6945\n",
      "Epoch 1 step 443: training loss: 1383.27204802967\n",
      "Epoch 1 step 444: training accuarcy: 0.71\n",
      "Epoch 1 step 444: training loss: 1383.6037987756545\n",
      "Epoch 1 step 445: training accuarcy: 0.6825\n",
      "Epoch 1 step 445: training loss: 1382.3527247993493\n",
      "Epoch 1 step 446: training accuarcy: 0.7035\n",
      "Epoch 1 step 446: training loss: 1382.8795905608645\n",
      "Epoch 1 step 447: training accuarcy: 0.6815\n",
      "Epoch 1 step 447: training loss: 1383.3682995038446\n",
      "Epoch 1 step 448: training accuarcy: 0.681\n",
      "Epoch 1 step 448: training loss: 1383.0222570054732\n",
      "Epoch 1 step 449: training accuarcy: 0.6910000000000001\n",
      "Epoch 1 step 449: training loss: 1382.6765149366038\n",
      "Epoch 1 step 450: training accuarcy: 0.6925\n",
      "Epoch 1 step 450: training loss: 1382.0780305080893\n",
      "Epoch 1 step 451: training accuarcy: 0.6970000000000001\n",
      "Epoch 1 step 451: training loss: 1383.0048741282967\n",
      "Epoch 1 step 452: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 452: training loss: 1382.4427864079748\n",
      "Epoch 1 step 453: training accuarcy: 0.7085\n",
      "Epoch 1 step 453: training loss: 1383.2482235557782\n",
      "Epoch 1 step 454: training accuarcy: 0.6985\n",
      "Epoch 1 step 454: training loss: 1382.2227429408886\n",
      "Epoch 1 step 455: training accuarcy: 0.714\n",
      "Epoch 1 step 455: training loss: 1382.13873042107\n",
      "Epoch 1 step 456: training accuarcy: 0.707\n",
      "Epoch 1 step 456: training loss: 1383.9647455766876\n",
      "Epoch 1 step 457: training accuarcy: 0.685\n",
      "Epoch 1 step 457: training loss: 1382.894271393186\n",
      "Epoch 1 step 458: training accuarcy: 0.7015\n",
      "Epoch 1 step 458: training loss: 1383.3385348865959\n",
      "Epoch 1 step 459: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 459: training loss: 1383.0008218716857\n",
      "Epoch 1 step 460: training accuarcy: 0.6995\n",
      "Epoch 1 step 460: training loss: 1383.1791208634188\n",
      "Epoch 1 step 461: training accuarcy: 0.685\n",
      "Epoch 1 step 461: training loss: 1383.4586759449742\n",
      "Epoch 1 step 462: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 462: training loss: 1382.62379794524\n",
      "Epoch 1 step 463: training accuarcy: 0.718\n",
      "Epoch 1 step 463: training loss: 1383.2217016853879\n",
      "Epoch 1 step 464: training accuarcy: 0.7035\n",
      "Epoch 1 step 464: training loss: 1382.5555346987512\n",
      "Epoch 1 step 465: training accuarcy: 0.6895\n",
      "Epoch 1 step 465: training loss: 1383.0138803321786\n",
      "Epoch 1 step 466: training accuarcy: 0.706\n",
      "Epoch 1 step 466: training loss: 1382.3310412948965\n",
      "Epoch 1 step 467: training accuarcy: 0.7105\n",
      "Epoch 1 step 467: training loss: 1382.9428929700418\n",
      "Epoch 1 step 468: training accuarcy: 0.7045\n",
      "Epoch 1 step 468: training loss: 1382.5725271686658\n",
      "Epoch 1 step 469: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 469: training loss: 1383.8987442653838\n",
      "Epoch 1 step 470: training accuarcy: 0.682\n",
      "Epoch 1 step 470: training loss: 1383.4131042640026\n",
      "Epoch 1 step 471: training accuarcy: 0.679\n",
      "Epoch 1 step 471: training loss: 1382.691231233909\n",
      "Epoch 1 step 472: training accuarcy: 0.7015\n",
      "Epoch 1 step 472: training loss: 1383.4551545298882\n",
      "Epoch 1 step 473: training accuarcy: 0.6935\n",
      "Epoch 1 step 473: training loss: 1382.5159310096724\n",
      "Epoch 1 step 474: training accuarcy: 0.7025\n",
      "Epoch 1 step 474: training loss: 1382.8413755605877\n",
      "Epoch 1 step 475: training accuarcy: 0.71\n",
      "Epoch 1 step 475: training loss: 1382.926600124024\n",
      "Epoch 1 step 476: training accuarcy: 0.683\n",
      "Epoch 1 step 476: training loss: 1383.3568569169138\n",
      "Epoch 1 step 477: training accuarcy: 0.6995\n",
      "Epoch 1 step 477: training loss: 1383.4657379382372\n",
      "Epoch 1 step 478: training accuarcy: 0.676\n",
      "Epoch 1 step 478: training loss: 1383.3837510732283\n",
      "Epoch 1 step 479: training accuarcy: 0.68\n",
      "Epoch 1 step 479: training loss: 1382.9754060574157\n",
      "Epoch 1 step 480: training accuarcy: 0.6915\n",
      "Epoch 1 step 480: training loss: 1382.7376034405331\n",
      "Epoch 1 step 481: training accuarcy: 0.6945\n",
      "Epoch 1 step 481: training loss: 1383.711862901423\n",
      "Epoch 1 step 482: training accuarcy: 0.676\n",
      "Epoch 1 step 482: training loss: 1382.9778005896583\n",
      "Epoch 1 step 483: training accuarcy: 0.6885\n",
      "Epoch 1 step 483: training loss: 1382.240726181429\n",
      "Epoch 1 step 484: training accuarcy: 0.6945\n",
      "Epoch 1 step 484: training loss: 1383.0200628720393\n",
      "Epoch 1 step 485: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 485: training loss: 1382.3287240013503\n",
      "Epoch 1 step 486: training accuarcy: 0.7105\n",
      "Epoch 1 step 486: training loss: 1383.2292830814238\n",
      "Epoch 1 step 487: training accuarcy: 0.7005\n",
      "Epoch 1 step 487: training loss: 1383.310697967615\n",
      "Epoch 1 step 488: training accuarcy: 0.6855\n",
      "Epoch 1 step 488: training loss: 1383.0478169201106\n",
      "Epoch 1 step 489: training accuarcy: 0.687\n",
      "Epoch 1 step 489: training loss: 1382.7229366430063\n",
      "Epoch 1 step 490: training accuarcy: 0.7035\n",
      "Epoch 1 step 490: training loss: 1383.3305153669746\n",
      "Epoch 1 step 491: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 491: training loss: 1382.4909595672632\n",
      "Epoch 1 step 492: training accuarcy: 0.6970000000000001\n",
      "Epoch 1 step 492: training loss: 1384.1247198793494\n",
      "Epoch 1 step 493: training accuarcy: 0.6855\n",
      "Epoch 1 step 493: training loss: 1382.870808050913\n",
      "Epoch 1 step 494: training accuarcy: 0.686\n",
      "Epoch 1 step 494: training loss: 1383.4520878969145\n",
      "Epoch 1 step 495: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 495: training loss: 1382.482116896001\n",
      "Epoch 1 step 496: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 496: training loss: 1382.6741643172743\n",
      "Epoch 1 step 497: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 497: training loss: 1382.6123173076855\n",
      "Epoch 1 step 498: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 498: training loss: 1383.262913677613\n",
      "Epoch 1 step 499: training accuarcy: 0.7015\n",
      "Epoch 1 step 499: training loss: 1382.7353460410077\n",
      "Epoch 1 step 500: training accuarcy: 0.7015\n",
      "Epoch 1 step 500: training loss: 1383.3102944623404\n",
      "Epoch 1 step 501: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 501: training loss: 1382.3374971166725\n",
      "Epoch 1 step 502: training accuarcy: 0.7075\n",
      "Epoch 1 step 502: training loss: 1382.9520612164092\n",
      "Epoch 1 step 503: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 503: training loss: 1382.5131134954433\n",
      "Epoch 1 step 504: training accuarcy: 0.7125\n",
      "Epoch 1 step 504: training loss: 1383.2898261773478\n",
      "Epoch 1 step 505: training accuarcy: 0.6855\n",
      "Epoch 1 step 505: training loss: 1382.4251830485991\n",
      "Epoch 1 step 506: training accuarcy: 0.71\n",
      "Epoch 1 step 506: training loss: 1383.8341708290247\n",
      "Epoch 1 step 507: training accuarcy: 0.6955\n",
      "Epoch 1 step 507: training loss: 1383.3657993944628\n",
      "Epoch 1 step 508: training accuarcy: 0.6910000000000001\n",
      "Epoch 1 step 508: training loss: 1382.8014005951234\n",
      "Epoch 1 step 509: training accuarcy: 0.6885\n",
      "Epoch 1 step 509: training loss: 1382.8335264081325\n",
      "Epoch 1 step 510: training accuarcy: 0.7020000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 510: training loss: 1383.2182823765247\n",
      "Epoch 1 step 511: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 511: training loss: 1382.0845721749126\n",
      "Epoch 1 step 512: training accuarcy: 0.7125\n",
      "Epoch 1 step 512: training loss: 1383.2221426235692\n",
      "Epoch 1 step 513: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 513: training loss: 1382.761634046808\n",
      "Epoch 1 step 514: training accuarcy: 0.713\n",
      "Epoch 1 step 514: training loss: 1382.4995598793453\n",
      "Epoch 1 step 515: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 515: training loss: 1382.8448034510623\n",
      "Epoch 1 step 516: training accuarcy: 0.6955\n",
      "Epoch 1 step 516: training loss: 1383.624820750143\n",
      "Epoch 1 step 517: training accuarcy: 0.6835\n",
      "Epoch 1 step 517: training loss: 1382.6092741058642\n",
      "Epoch 1 step 518: training accuarcy: 0.7035\n",
      "Epoch 1 step 518: training loss: 1382.8900977488643\n",
      "Epoch 1 step 519: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 519: training loss: 1383.3443141860794\n",
      "Epoch 1 step 520: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 520: training loss: 1382.8077312997416\n",
      "Epoch 1 step 521: training accuarcy: 0.7015\n",
      "Epoch 1 step 521: training loss: 1382.337547015297\n",
      "Epoch 1 step 522: training accuarcy: 0.7045\n",
      "Epoch 1 step 522: training loss: 1382.344978572229\n",
      "Epoch 1 step 523: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 523: training loss: 1382.5593279877924\n",
      "Epoch 1 step 524: training accuarcy: 0.7085\n",
      "Epoch 1 step 524: training loss: 1382.928439288782\n",
      "Epoch 1 step 525: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 525: training loss: 542.8579437306774\n",
      "Epoch 1 step 526: training accuarcy: 0.7025641025641025\n",
      "Epoch 1: train loss 1379.6555326962869, train accuarcy 0.7024531960487366\n",
      "Epoch 1: valid loss 1364.385207584681, valid accuarcy 0.7165958881378174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████                                                                                                                  | 2/8 [03:53<11:41, 116.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 1380.687579611831\n",
      "Epoch 2 step 527: training accuarcy: 0.726\n",
      "Epoch 2 step 527: training loss: 1381.6223539363955\n",
      "Epoch 2 step 528: training accuarcy: 0.718\n",
      "Epoch 2 step 528: training loss: 1382.3753080623578\n",
      "Epoch 2 step 529: training accuarcy: 0.7075\n",
      "Epoch 2 step 529: training loss: 1380.9742153913714\n",
      "Epoch 2 step 530: training accuarcy: 0.7365\n",
      "Epoch 2 step 530: training loss: 1382.264849161149\n",
      "Epoch 2 step 531: training accuarcy: 0.719\n",
      "Epoch 2 step 531: training loss: 1382.0788115044886\n",
      "Epoch 2 step 532: training accuarcy: 0.7205\n",
      "Epoch 2 step 532: training loss: 1382.347236856899\n",
      "Epoch 2 step 533: training accuarcy: 0.7135\n",
      "Epoch 2 step 533: training loss: 1382.0310448776056\n",
      "Epoch 2 step 534: training accuarcy: 0.7125\n",
      "Epoch 2 step 534: training loss: 1381.951551471578\n",
      "Epoch 2 step 535: training accuarcy: 0.717\n",
      "Epoch 2 step 535: training loss: 1383.1778070658934\n",
      "Epoch 2 step 536: training accuarcy: 0.714\n",
      "Epoch 2 step 536: training loss: 1382.11294472534\n",
      "Epoch 2 step 537: training accuarcy: 0.714\n",
      "Epoch 2 step 537: training loss: 1383.0968911604707\n",
      "Epoch 2 step 538: training accuarcy: 0.6855\n",
      "Epoch 2 step 538: training loss: 1381.4659468826935\n",
      "Epoch 2 step 539: training accuarcy: 0.721\n",
      "Epoch 2 step 539: training loss: 1382.3767332695409\n",
      "Epoch 2 step 540: training accuarcy: 0.711\n",
      "Epoch 2 step 540: training loss: 1382.5331763581437\n",
      "Epoch 2 step 541: training accuarcy: 0.707\n",
      "Epoch 2 step 541: training loss: 1382.0122593997971\n",
      "Epoch 2 step 542: training accuarcy: 0.7225\n",
      "Epoch 2 step 542: training loss: 1382.1466957313744\n",
      "Epoch 2 step 543: training accuarcy: 0.7105\n",
      "Epoch 2 step 543: training loss: 1382.2511985664764\n",
      "Epoch 2 step 544: training accuarcy: 0.6935\n",
      "Epoch 2 step 544: training loss: 1382.1139378688033\n",
      "Epoch 2 step 545: training accuarcy: 0.7175\n",
      "Epoch 2 step 545: training loss: 1382.253244985578\n",
      "Epoch 2 step 546: training accuarcy: 0.7025\n",
      "Epoch 2 step 546: training loss: 1381.8480712185853\n",
      "Epoch 2 step 547: training accuarcy: 0.708\n",
      "Epoch 2 step 547: training loss: 1381.8320166780802\n",
      "Epoch 2 step 548: training accuarcy: 0.723\n",
      "Epoch 2 step 548: training loss: 1382.5006795356599\n",
      "Epoch 2 step 549: training accuarcy: 0.704\n",
      "Epoch 2 step 549: training loss: 1382.636105200843\n",
      "Epoch 2 step 550: training accuarcy: 0.7025\n",
      "Epoch 2 step 550: training loss: 1382.6116800785737\n",
      "Epoch 2 step 551: training accuarcy: 0.709\n",
      "Epoch 2 step 551: training loss: 1383.0403481960384\n",
      "Epoch 2 step 552: training accuarcy: 0.7005\n",
      "Epoch 2 step 552: training loss: 1382.2311441274373\n",
      "Epoch 2 step 553: training accuarcy: 0.705\n",
      "Epoch 2 step 553: training loss: 1382.066300774188\n",
      "Epoch 2 step 554: training accuarcy: 0.7225\n",
      "Epoch 2 step 554: training loss: 1381.4901900153852\n",
      "Epoch 2 step 555: training accuarcy: 0.709\n",
      "Epoch 2 step 555: training loss: 1382.3617396392506\n",
      "Epoch 2 step 556: training accuarcy: 0.7075\n",
      "Epoch 2 step 556: training loss: 1382.8502070241857\n",
      "Epoch 2 step 557: training accuarcy: 0.7055\n",
      "Epoch 2 step 557: training loss: 1382.6562078203413\n",
      "Epoch 2 step 558: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 558: training loss: 1382.7857349889803\n",
      "Epoch 2 step 559: training accuarcy: 0.707\n",
      "Epoch 2 step 559: training loss: 1382.464327882489\n",
      "Epoch 2 step 560: training accuarcy: 0.714\n",
      "Epoch 2 step 560: training loss: 1382.5468369783082\n",
      "Epoch 2 step 561: training accuarcy: 0.6925\n",
      "Epoch 2 step 561: training loss: 1382.7249397727292\n",
      "Epoch 2 step 562: training accuarcy: 0.6975\n",
      "Epoch 2 step 562: training loss: 1382.621869379442\n",
      "Epoch 2 step 563: training accuarcy: 0.6915\n",
      "Epoch 2 step 563: training loss: 1382.2406268149182\n",
      "Epoch 2 step 564: training accuarcy: 0.717\n",
      "Epoch 2 step 564: training loss: 1382.4465658535487\n",
      "Epoch 2 step 565: training accuarcy: 0.709\n",
      "Epoch 2 step 565: training loss: 1382.781173676847\n",
      "Epoch 2 step 566: training accuarcy: 0.7115\n",
      "Epoch 2 step 566: training loss: 1382.1406377135704\n",
      "Epoch 2 step 567: training accuarcy: 0.707\n",
      "Epoch 2 step 567: training loss: 1382.8330104528284\n",
      "Epoch 2 step 568: training accuarcy: 0.704\n",
      "Epoch 2 step 568: training loss: 1383.7514187657619\n",
      "Epoch 2 step 569: training accuarcy: 0.683\n",
      "Epoch 2 step 569: training loss: 1382.4964598115585\n",
      "Epoch 2 step 570: training accuarcy: 0.709\n",
      "Epoch 2 step 570: training loss: 1382.7296514717593\n",
      "Epoch 2 step 571: training accuarcy: 0.7035\n",
      "Epoch 2 step 571: training loss: 1382.6287390883883\n",
      "Epoch 2 step 572: training accuarcy: 0.7015\n",
      "Epoch 2 step 572: training loss: 1382.183717995656\n",
      "Epoch 2 step 573: training accuarcy: 0.724\n",
      "Epoch 2 step 573: training loss: 1383.4412924297346\n",
      "Epoch 2 step 574: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 574: training loss: 1382.280386508988\n",
      "Epoch 2 step 575: training accuarcy: 0.709\n",
      "Epoch 2 step 575: training loss: 1382.6247428883044\n",
      "Epoch 2 step 576: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 576: training loss: 1383.6041027627089\n",
      "Epoch 2 step 577: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 577: training loss: 1382.9357257646602\n",
      "Epoch 2 step 578: training accuarcy: 0.6900000000000001\n",
      "Epoch 2 step 578: training loss: 1382.9843356084314\n",
      "Epoch 2 step 579: training accuarcy: 0.6875\n",
      "Epoch 2 step 579: training loss: 1383.2250297942492\n",
      "Epoch 2 step 580: training accuarcy: 0.6950000000000001\n",
      "Epoch 2 step 580: training loss: 1382.6848461934387\n",
      "Epoch 2 step 581: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 581: training loss: 1382.6446627784096\n",
      "Epoch 2 step 582: training accuarcy: 0.7055\n",
      "Epoch 2 step 582: training loss: 1382.713327640486\n",
      "Epoch 2 step 583: training accuarcy: 0.7095\n",
      "Epoch 2 step 583: training loss: 1382.4941104682473\n",
      "Epoch 2 step 584: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 584: training loss: 1382.8202813494918\n",
      "Epoch 2 step 585: training accuarcy: 0.6825\n",
      "Epoch 2 step 585: training loss: 1383.2314248466098\n",
      "Epoch 2 step 586: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 586: training loss: 1382.184171534309\n",
      "Epoch 2 step 587: training accuarcy: 0.717\n",
      "Epoch 2 step 587: training loss: 1382.6079100559402\n",
      "Epoch 2 step 588: training accuarcy: 0.7035\n",
      "Epoch 2 step 588: training loss: 1382.6277760243433\n",
      "Epoch 2 step 589: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 589: training loss: 1382.8690183874298\n",
      "Epoch 2 step 590: training accuarcy: 0.6955\n",
      "Epoch 2 step 590: training loss: 1382.2641976159364\n",
      "Epoch 2 step 591: training accuarcy: 0.714\n",
      "Epoch 2 step 591: training loss: 1382.1502366004363\n",
      "Epoch 2 step 592: training accuarcy: 0.7095\n",
      "Epoch 2 step 592: training loss: 1382.4819692965025\n",
      "Epoch 2 step 593: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 593: training loss: 1382.7418708707908\n",
      "Epoch 2 step 594: training accuarcy: 0.7005\n",
      "Epoch 2 step 594: training loss: 1382.5909794605532\n",
      "Epoch 2 step 595: training accuarcy: 0.7025\n",
      "Epoch 2 step 595: training loss: 1382.8096903413327\n",
      "Epoch 2 step 596: training accuarcy: 0.713\n",
      "Epoch 2 step 596: training loss: 1382.7563528972714\n",
      "Epoch 2 step 597: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 597: training loss: 1382.4950020040665\n",
      "Epoch 2 step 598: training accuarcy: 0.7065\n",
      "Epoch 2 step 598: training loss: 1382.9648445357216\n",
      "Epoch 2 step 599: training accuarcy: 0.708\n",
      "Epoch 2 step 599: training loss: 1383.1488517470088\n",
      "Epoch 2 step 600: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 600: training loss: 1381.162064161002\n",
      "Epoch 2 step 601: training accuarcy: 0.721\n",
      "Epoch 2 step 601: training loss: 1382.1729062097154\n",
      "Epoch 2 step 602: training accuarcy: 0.711\n",
      "Epoch 2 step 602: training loss: 1383.2496545751492\n",
      "Epoch 2 step 603: training accuarcy: 0.6775\n",
      "Epoch 2 step 603: training loss: 1382.5564789821956\n",
      "Epoch 2 step 604: training accuarcy: 0.708\n",
      "Epoch 2 step 604: training loss: 1382.6908993489776\n",
      "Epoch 2 step 605: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 605: training loss: 1382.5464576328561\n",
      "Epoch 2 step 606: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 606: training loss: 1383.0955182594537\n",
      "Epoch 2 step 607: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 607: training loss: 1383.2338103868403\n",
      "Epoch 2 step 608: training accuarcy: 0.6825\n",
      "Epoch 2 step 608: training loss: 1383.4057667532886\n",
      "Epoch 2 step 609: training accuarcy: 0.6965\n",
      "Epoch 2 step 609: training loss: 1382.4166307575626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 610: training accuarcy: 0.6995\n",
      "Epoch 2 step 610: training loss: 1382.8919874299904\n",
      "Epoch 2 step 611: training accuarcy: 0.6995\n",
      "Epoch 2 step 611: training loss: 1382.8247351634031\n",
      "Epoch 2 step 612: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 612: training loss: 1382.9986814176298\n",
      "Epoch 2 step 613: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 613: training loss: 1382.9701774573261\n",
      "Epoch 2 step 614: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 614: training loss: 1382.219830732438\n",
      "Epoch 2 step 615: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 615: training loss: 1382.2563906466617\n",
      "Epoch 2 step 616: training accuarcy: 0.7065\n",
      "Epoch 2 step 616: training loss: 1383.0526136180931\n",
      "Epoch 2 step 617: training accuarcy: 0.6935\n",
      "Epoch 2 step 617: training loss: 1382.526183047369\n",
      "Epoch 2 step 618: training accuarcy: 0.704\n",
      "Epoch 2 step 618: training loss: 1382.515421642378\n",
      "Epoch 2 step 619: training accuarcy: 0.7145\n",
      "Epoch 2 step 619: training loss: 1383.3689870500598\n",
      "Epoch 2 step 620: training accuarcy: 0.674\n",
      "Epoch 2 step 620: training loss: 1382.8204461863193\n",
      "Epoch 2 step 621: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 621: training loss: 1382.5779588906612\n",
      "Epoch 2 step 622: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 622: training loss: 1382.4091069674785\n",
      "Epoch 2 step 623: training accuarcy: 0.714\n",
      "Epoch 2 step 623: training loss: 1381.9371294836888\n",
      "Epoch 2 step 624: training accuarcy: 0.6925\n",
      "Epoch 2 step 624: training loss: 1382.115385148449\n",
      "Epoch 2 step 625: training accuarcy: 0.7115\n",
      "Epoch 2 step 625: training loss: 1382.6668283770896\n",
      "Epoch 2 step 626: training accuarcy: 0.704\n",
      "Epoch 2 step 626: training loss: 1383.5199539685034\n",
      "Epoch 2 step 627: training accuarcy: 0.683\n",
      "Epoch 2 step 627: training loss: 1382.7871360309948\n",
      "Epoch 2 step 628: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 628: training loss: 1382.642022860757\n",
      "Epoch 2 step 629: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 629: training loss: 1383.2492375752734\n",
      "Epoch 2 step 630: training accuarcy: 0.6900000000000001\n",
      "Epoch 2 step 630: training loss: 1382.6948122392198\n",
      "Epoch 2 step 631: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 631: training loss: 1382.8529326085584\n",
      "Epoch 2 step 632: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 632: training loss: 1382.5140310390495\n",
      "Epoch 2 step 633: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 633: training loss: 1382.7061232675214\n",
      "Epoch 2 step 634: training accuarcy: 0.7025\n",
      "Epoch 2 step 634: training loss: 1383.2003237528581\n",
      "Epoch 2 step 635: training accuarcy: 0.7095\n",
      "Epoch 2 step 635: training loss: 1382.8916655626917\n",
      "Epoch 2 step 636: training accuarcy: 0.6985\n",
      "Epoch 2 step 636: training loss: 1382.703642946172\n",
      "Epoch 2 step 637: training accuarcy: 0.6865\n",
      "Epoch 2 step 637: training loss: 1382.5722487922264\n",
      "Epoch 2 step 638: training accuarcy: 0.7135\n",
      "Epoch 2 step 638: training loss: 1383.2154083801477\n",
      "Epoch 2 step 639: training accuarcy: 0.708\n",
      "Epoch 2 step 639: training loss: 1382.7259715708144\n",
      "Epoch 2 step 640: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 640: training loss: 1382.1784200294749\n",
      "Epoch 2 step 641: training accuarcy: 0.71\n",
      "Epoch 2 step 641: training loss: 1382.723292525124\n",
      "Epoch 2 step 642: training accuarcy: 0.6945\n",
      "Epoch 2 step 642: training loss: 1383.336343839609\n",
      "Epoch 2 step 643: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 643: training loss: 1382.8592507406577\n",
      "Epoch 2 step 644: training accuarcy: 0.6950000000000001\n",
      "Epoch 2 step 644: training loss: 1382.631588658815\n",
      "Epoch 2 step 645: training accuarcy: 0.706\n",
      "Epoch 2 step 645: training loss: 1383.3049650405565\n",
      "Epoch 2 step 646: training accuarcy: 0.6935\n",
      "Epoch 2 step 646: training loss: 1383.5495335016674\n",
      "Epoch 2 step 647: training accuarcy: 0.6905\n",
      "Epoch 2 step 647: training loss: 1382.6577067079486\n",
      "Epoch 2 step 648: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 648: training loss: 1382.599734657492\n",
      "Epoch 2 step 649: training accuarcy: 0.6955\n",
      "Epoch 2 step 649: training loss: 1383.5138288936002\n",
      "Epoch 2 step 650: training accuarcy: 0.6905\n",
      "Epoch 2 step 650: training loss: 1383.3287123186026\n",
      "Epoch 2 step 651: training accuarcy: 0.684\n",
      "Epoch 2 step 651: training loss: 1383.8090169177267\n",
      "Epoch 2 step 652: training accuarcy: 0.687\n",
      "Epoch 2 step 652: training loss: 1383.6411824059014\n",
      "Epoch 2 step 653: training accuarcy: 0.6885\n",
      "Epoch 2 step 653: training loss: 1382.8503621824702\n",
      "Epoch 2 step 654: training accuarcy: 0.712\n",
      "Epoch 2 step 654: training loss: 1382.5999939348815\n",
      "Epoch 2 step 655: training accuarcy: 0.6985\n",
      "Epoch 2 step 655: training loss: 1382.021406075609\n",
      "Epoch 2 step 656: training accuarcy: 0.711\n",
      "Epoch 2 step 656: training loss: 1382.2230252851787\n",
      "Epoch 2 step 657: training accuarcy: 0.71\n",
      "Epoch 2 step 657: training loss: 1381.9946607525108\n",
      "Epoch 2 step 658: training accuarcy: 0.728\n",
      "Epoch 2 step 658: training loss: 1383.3272446115425\n",
      "Epoch 2 step 659: training accuarcy: 0.678\n",
      "Epoch 2 step 659: training loss: 1382.7943509349961\n",
      "Epoch 2 step 660: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 660: training loss: 1383.4790168793027\n",
      "Epoch 2 step 661: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 661: training loss: 1382.8031751513122\n",
      "Epoch 2 step 662: training accuarcy: 0.6995\n",
      "Epoch 2 step 662: training loss: 1382.5693467417736\n",
      "Epoch 2 step 663: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 663: training loss: 1383.3604309612574\n",
      "Epoch 2 step 664: training accuarcy: 0.6945\n",
      "Epoch 2 step 664: training loss: 1382.681805767517\n",
      "Epoch 2 step 665: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 665: training loss: 1382.259862438013\n",
      "Epoch 2 step 666: training accuarcy: 0.7135\n",
      "Epoch 2 step 666: training loss: 1383.0834992642217\n",
      "Epoch 2 step 667: training accuarcy: 0.7025\n",
      "Epoch 2 step 667: training loss: 1382.4684239295384\n",
      "Epoch 2 step 668: training accuarcy: 0.7125\n",
      "Epoch 2 step 668: training loss: 1382.167121282396\n",
      "Epoch 2 step 669: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 669: training loss: 1382.7590623802284\n",
      "Epoch 2 step 670: training accuarcy: 0.6925\n",
      "Epoch 2 step 670: training loss: 1383.0107076053268\n",
      "Epoch 2 step 671: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 671: training loss: 1383.3655328134744\n",
      "Epoch 2 step 672: training accuarcy: 0.683\n",
      "Epoch 2 step 672: training loss: 1383.4906014306364\n",
      "Epoch 2 step 673: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 673: training loss: 1382.2069262662203\n",
      "Epoch 2 step 674: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 674: training loss: 1382.6593691142612\n",
      "Epoch 2 step 675: training accuarcy: 0.72\n",
      "Epoch 2 step 675: training loss: 1382.9947232729744\n",
      "Epoch 2 step 676: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 676: training loss: 1382.904780579687\n",
      "Epoch 2 step 677: training accuarcy: 0.6955\n",
      "Epoch 2 step 677: training loss: 1383.6003360375835\n",
      "Epoch 2 step 678: training accuarcy: 0.6785\n",
      "Epoch 2 step 678: training loss: 1382.6417367179176\n",
      "Epoch 2 step 679: training accuarcy: 0.7065\n",
      "Epoch 2 step 679: training loss: 1383.4344681384646\n",
      "Epoch 2 step 680: training accuarcy: 0.6835\n",
      "Epoch 2 step 680: training loss: 1382.7880376740316\n",
      "Epoch 2 step 681: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 681: training loss: 1382.2373135803518\n",
      "Epoch 2 step 682: training accuarcy: 0.7045\n",
      "Epoch 2 step 682: training loss: 1382.418526939431\n",
      "Epoch 2 step 683: training accuarcy: 0.7025\n",
      "Epoch 2 step 683: training loss: 1383.9487506611454\n",
      "Epoch 2 step 684: training accuarcy: 0.6875\n",
      "Epoch 2 step 684: training loss: 1382.8336837715437\n",
      "Epoch 2 step 685: training accuarcy: 0.681\n",
      "Epoch 2 step 685: training loss: 1382.79741107623\n",
      "Epoch 2 step 686: training accuarcy: 0.6925\n",
      "Epoch 2 step 686: training loss: 1382.0208159512345\n",
      "Epoch 2 step 687: training accuarcy: 0.6975\n",
      "Epoch 2 step 687: training loss: 1381.7414545980791\n",
      "Epoch 2 step 688: training accuarcy: 0.713\n",
      "Epoch 2 step 688: training loss: 1383.7245799776967\n",
      "Epoch 2 step 689: training accuarcy: 0.6905\n",
      "Epoch 2 step 689: training loss: 1382.6752874826711\n",
      "Epoch 2 step 690: training accuarcy: 0.709\n",
      "Epoch 2 step 690: training loss: 1382.4995891654376\n",
      "Epoch 2 step 691: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 691: training loss: 1382.4072508340848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 692: training accuarcy: 0.709\n",
      "Epoch 2 step 692: training loss: 1382.5589543374374\n",
      "Epoch 2 step 693: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 693: training loss: 1383.364360076782\n",
      "Epoch 2 step 694: training accuarcy: 0.6885\n",
      "Epoch 2 step 694: training loss: 1382.750101116641\n",
      "Epoch 2 step 695: training accuarcy: 0.6795\n",
      "Epoch 2 step 695: training loss: 1382.4020238429368\n",
      "Epoch 2 step 696: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 696: training loss: 1382.924683947045\n",
      "Epoch 2 step 697: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 697: training loss: 1383.3500682963434\n",
      "Epoch 2 step 698: training accuarcy: 0.6895\n",
      "Epoch 2 step 698: training loss: 1383.96372903708\n",
      "Epoch 2 step 699: training accuarcy: 0.6785\n",
      "Epoch 2 step 699: training loss: 1383.542361172814\n",
      "Epoch 2 step 700: training accuarcy: 0.6925\n",
      "Epoch 2 step 700: training loss: 1383.2132949124157\n",
      "Epoch 2 step 701: training accuarcy: 0.6975\n",
      "Epoch 2 step 701: training loss: 1383.1017033906824\n",
      "Epoch 2 step 702: training accuarcy: 0.7015\n",
      "Epoch 2 step 702: training loss: 1382.4686277793628\n",
      "Epoch 2 step 703: training accuarcy: 0.6975\n",
      "Epoch 2 step 703: training loss: 1382.47743728911\n",
      "Epoch 2 step 704: training accuarcy: 0.7035\n",
      "Epoch 2 step 704: training loss: 1382.7964087732748\n",
      "Epoch 2 step 705: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 705: training loss: 1382.1206046395107\n",
      "Epoch 2 step 706: training accuarcy: 0.7135\n",
      "Epoch 2 step 706: training loss: 1383.0000485089352\n",
      "Epoch 2 step 707: training accuarcy: 0.6915\n",
      "Epoch 2 step 707: training loss: 1383.3151093021584\n",
      "Epoch 2 step 708: training accuarcy: 0.6845\n",
      "Epoch 2 step 708: training loss: 1382.984818954024\n",
      "Epoch 2 step 709: training accuarcy: 0.7045\n",
      "Epoch 2 step 709: training loss: 1382.9698323511109\n",
      "Epoch 2 step 710: training accuarcy: 0.6975\n",
      "Epoch 2 step 710: training loss: 1382.9958736957353\n",
      "Epoch 2 step 711: training accuarcy: 0.6815\n",
      "Epoch 2 step 711: training loss: 1382.8985792391718\n",
      "Epoch 2 step 712: training accuarcy: 0.6935\n",
      "Epoch 2 step 712: training loss: 1382.437061621238\n",
      "Epoch 2 step 713: training accuarcy: 0.7095\n",
      "Epoch 2 step 713: training loss: 1383.0068404023823\n",
      "Epoch 2 step 714: training accuarcy: 0.7025\n",
      "Epoch 2 step 714: training loss: 1383.4519352610164\n",
      "Epoch 2 step 715: training accuarcy: 0.686\n",
      "Epoch 2 step 715: training loss: 1383.4331655473065\n",
      "Epoch 2 step 716: training accuarcy: 0.6775\n",
      "Epoch 2 step 716: training loss: 1382.7631659604574\n",
      "Epoch 2 step 717: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 717: training loss: 1382.4535828057521\n",
      "Epoch 2 step 718: training accuarcy: 0.7135\n",
      "Epoch 2 step 718: training loss: 1383.6881338265664\n",
      "Epoch 2 step 719: training accuarcy: 0.677\n",
      "Epoch 2 step 719: training loss: 1382.6367898881267\n",
      "Epoch 2 step 720: training accuarcy: 0.6915\n",
      "Epoch 2 step 720: training loss: 1383.139229595813\n",
      "Epoch 2 step 721: training accuarcy: 0.6935\n",
      "Epoch 2 step 721: training loss: 1383.6205764971496\n",
      "Epoch 2 step 722: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 722: training loss: 1383.9093962060294\n",
      "Epoch 2 step 723: training accuarcy: 0.685\n",
      "Epoch 2 step 723: training loss: 1383.9524694565011\n",
      "Epoch 2 step 724: training accuarcy: 0.6735\n",
      "Epoch 2 step 724: training loss: 1382.9445324806848\n",
      "Epoch 2 step 725: training accuarcy: 0.7055\n",
      "Epoch 2 step 725: training loss: 1383.7090531701265\n",
      "Epoch 2 step 726: training accuarcy: 0.6825\n",
      "Epoch 2 step 726: training loss: 1382.2830178501745\n",
      "Epoch 2 step 727: training accuarcy: 0.7105\n",
      "Epoch 2 step 727: training loss: 1382.8394471378938\n",
      "Epoch 2 step 728: training accuarcy: 0.6950000000000001\n",
      "Epoch 2 step 728: training loss: 1383.1871385983566\n",
      "Epoch 2 step 729: training accuarcy: 0.6975\n",
      "Epoch 2 step 729: training loss: 1382.3403224660374\n",
      "Epoch 2 step 730: training accuarcy: 0.6965\n",
      "Epoch 2 step 730: training loss: 1383.30972571103\n",
      "Epoch 2 step 731: training accuarcy: 0.6815\n",
      "Epoch 2 step 731: training loss: 1383.2403296498953\n",
      "Epoch 2 step 732: training accuarcy: 0.6965\n",
      "Epoch 2 step 732: training loss: 1383.8682678599534\n",
      "Epoch 2 step 733: training accuarcy: 0.682\n",
      "Epoch 2 step 733: training loss: 1382.7324146469427\n",
      "Epoch 2 step 734: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 734: training loss: 1383.2579328938448\n",
      "Epoch 2 step 735: training accuarcy: 0.6875\n",
      "Epoch 2 step 735: training loss: 1383.2484285177454\n",
      "Epoch 2 step 736: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 736: training loss: 1382.5569118707635\n",
      "Epoch 2 step 737: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 737: training loss: 1382.7181539996384\n",
      "Epoch 2 step 738: training accuarcy: 0.6875\n",
      "Epoch 2 step 738: training loss: 1382.8034307978937\n",
      "Epoch 2 step 739: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 739: training loss: 1382.8611517683592\n",
      "Epoch 2 step 740: training accuarcy: 0.6995\n",
      "Epoch 2 step 740: training loss: 1383.295752432442\n",
      "Epoch 2 step 741: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 741: training loss: 1382.2939156168834\n",
      "Epoch 2 step 742: training accuarcy: 0.706\n",
      "Epoch 2 step 742: training loss: 1382.817411003817\n",
      "Epoch 2 step 743: training accuarcy: 0.708\n",
      "Epoch 2 step 743: training loss: 1382.8694898190122\n",
      "Epoch 2 step 744: training accuarcy: 0.7085\n",
      "Epoch 2 step 744: training loss: 1382.8792227453916\n",
      "Epoch 2 step 745: training accuarcy: 0.6935\n",
      "Epoch 2 step 745: training loss: 1383.00033967763\n",
      "Epoch 2 step 746: training accuarcy: 0.6885\n",
      "Epoch 2 step 746: training loss: 1383.3244662470456\n",
      "Epoch 2 step 747: training accuarcy: 0.6845\n",
      "Epoch 2 step 747: training loss: 1382.4666762338422\n",
      "Epoch 2 step 748: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 748: training loss: 1383.4791470656617\n",
      "Epoch 2 step 749: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 749: training loss: 1383.7941047342629\n",
      "Epoch 2 step 750: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 750: training loss: 1382.8411412876985\n",
      "Epoch 2 step 751: training accuarcy: 0.679\n",
      "Epoch 2 step 751: training loss: 1382.801945937396\n",
      "Epoch 2 step 752: training accuarcy: 0.6975\n",
      "Epoch 2 step 752: training loss: 1383.190592980542\n",
      "Epoch 2 step 753: training accuarcy: 0.6855\n",
      "Epoch 2 step 753: training loss: 1382.7252194788928\n",
      "Epoch 2 step 754: training accuarcy: 0.6900000000000001\n",
      "Epoch 2 step 754: training loss: 1382.9371160035303\n",
      "Epoch 2 step 755: training accuarcy: 0.6965\n",
      "Epoch 2 step 755: training loss: 1382.8560625464297\n",
      "Epoch 2 step 756: training accuarcy: 0.704\n",
      "Epoch 2 step 756: training loss: 1384.2401783217063\n",
      "Epoch 2 step 757: training accuarcy: 0.674\n",
      "Epoch 2 step 757: training loss: 1382.9195381228658\n",
      "Epoch 2 step 758: training accuarcy: 0.7065\n",
      "Epoch 2 step 758: training loss: 1381.5251781913228\n",
      "Epoch 2 step 759: training accuarcy: 0.7095\n",
      "Epoch 2 step 759: training loss: 1382.7042411144728\n",
      "Epoch 2 step 760: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 760: training loss: 1383.4554474863648\n",
      "Epoch 2 step 761: training accuarcy: 0.6935\n",
      "Epoch 2 step 761: training loss: 1383.0002446765232\n",
      "Epoch 2 step 762: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 762: training loss: 1382.998020607116\n",
      "Epoch 2 step 763: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 763: training loss: 1383.0860908545674\n",
      "Epoch 2 step 764: training accuarcy: 0.6915\n",
      "Epoch 2 step 764: training loss: 1382.9518570411576\n",
      "Epoch 2 step 765: training accuarcy: 0.6905\n",
      "Epoch 2 step 765: training loss: 1382.594024548284\n",
      "Epoch 2 step 766: training accuarcy: 0.706\n",
      "Epoch 2 step 766: training loss: 1382.6102820211327\n",
      "Epoch 2 step 767: training accuarcy: 0.6975\n",
      "Epoch 2 step 767: training loss: 1383.4193949672112\n",
      "Epoch 2 step 768: training accuarcy: 0.6845\n",
      "Epoch 2 step 768: training loss: 1383.4436463967534\n",
      "Epoch 2 step 769: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 769: training loss: 1382.4006985983578\n",
      "Epoch 2 step 770: training accuarcy: 0.7215\n",
      "Epoch 2 step 770: training loss: 1383.1360472185884\n",
      "Epoch 2 step 771: training accuarcy: 0.685\n",
      "Epoch 2 step 771: training loss: 1382.9554064130805\n",
      "Epoch 2 step 772: training accuarcy: 0.6995\n",
      "Epoch 2 step 772: training loss: 1383.6967287665457\n",
      "Epoch 2 step 773: training accuarcy: 0.685\n",
      "Epoch 2 step 773: training loss: 1384.0052978459698\n",
      "Epoch 2 step 774: training accuarcy: 0.685\n",
      "Epoch 2 step 774: training loss: 1383.2732672402506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 775: training accuarcy: 0.6895\n",
      "Epoch 2 step 775: training loss: 1382.7478025617104\n",
      "Epoch 2 step 776: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 776: training loss: 1383.6289528385523\n",
      "Epoch 2 step 777: training accuarcy: 0.684\n",
      "Epoch 2 step 777: training loss: 1382.1087440938288\n",
      "Epoch 2 step 778: training accuarcy: 0.7145\n",
      "Epoch 2 step 778: training loss: 1382.6784404400473\n",
      "Epoch 2 step 779: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 779: training loss: 1382.5811290413974\n",
      "Epoch 2 step 780: training accuarcy: 0.6925\n",
      "Epoch 2 step 780: training loss: 1382.85629720585\n",
      "Epoch 2 step 781: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 781: training loss: 1383.7141193619436\n",
      "Epoch 2 step 782: training accuarcy: 0.6905\n",
      "Epoch 2 step 782: training loss: 1382.4635407077728\n",
      "Epoch 2 step 783: training accuarcy: 0.6975\n",
      "Epoch 2 step 783: training loss: 1383.3754000244826\n",
      "Epoch 2 step 784: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 784: training loss: 1381.918467200503\n",
      "Epoch 2 step 785: training accuarcy: 0.7045\n",
      "Epoch 2 step 785: training loss: 1383.498481321271\n",
      "Epoch 2 step 786: training accuarcy: 0.6935\n",
      "Epoch 2 step 786: training loss: 1382.6783455696018\n",
      "Epoch 2 step 787: training accuarcy: 0.6955\n",
      "Epoch 2 step 787: training loss: 1382.4825527871935\n",
      "Epoch 2 step 788: training accuarcy: 0.6965\n",
      "Epoch 2 step 788: training loss: 542.8629306838751\n",
      "Epoch 2 step 789: training accuarcy: 0.7012820512820512\n",
      "Epoch 2: train loss 1379.588827995813, train accuarcy 0.7020944952964783\n",
      "Epoch 2: valid loss 1364.2819261504992, valid accuarcy 0.7132605910301208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████                                                                                               | 3/8 [05:50<09:44, 116.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 1381.359329905503\n",
      "Epoch 3 step 790: training accuarcy: 0.721\n",
      "Epoch 3 step 790: training loss: 1381.4663453497642\n",
      "Epoch 3 step 791: training accuarcy: 0.7285\n",
      "Epoch 3 step 791: training loss: 1381.6574088676011\n",
      "Epoch 3 step 792: training accuarcy: 0.7265\n",
      "Epoch 3 step 792: training loss: 1382.0332433024298\n",
      "Epoch 3 step 793: training accuarcy: 0.7135\n",
      "Epoch 3 step 793: training loss: 1382.277451512998\n",
      "Epoch 3 step 794: training accuarcy: 0.709\n",
      "Epoch 3 step 794: training loss: 1382.2713160837393\n",
      "Epoch 3 step 795: training accuarcy: 0.7135\n",
      "Epoch 3 step 795: training loss: 1381.6934251096998\n",
      "Epoch 3 step 796: training accuarcy: 0.715\n",
      "Epoch 3 step 796: training loss: 1382.816108734426\n",
      "Epoch 3 step 797: training accuarcy: 0.6975\n",
      "Epoch 3 step 797: training loss: 1381.3050231064317\n",
      "Epoch 3 step 798: training accuarcy: 0.717\n",
      "Epoch 3 step 798: training loss: 1381.1817076043144\n",
      "Epoch 3 step 799: training accuarcy: 0.7405\n",
      "Epoch 3 step 799: training loss: 1381.915115058996\n",
      "Epoch 3 step 800: training accuarcy: 0.7115\n",
      "Epoch 3 step 800: training loss: 1381.7899386009215\n",
      "Epoch 3 step 801: training accuarcy: 0.724\n",
      "Epoch 3 step 801: training loss: 1382.6529949416556\n",
      "Epoch 3 step 802: training accuarcy: 0.7105\n",
      "Epoch 3 step 802: training loss: 1382.5859147740553\n",
      "Epoch 3 step 803: training accuarcy: 0.7025\n",
      "Epoch 3 step 803: training loss: 1382.4085588420337\n",
      "Epoch 3 step 804: training accuarcy: 0.7155\n",
      "Epoch 3 step 804: training loss: 1382.1243850922242\n",
      "Epoch 3 step 805: training accuarcy: 0.7105\n",
      "Epoch 3 step 805: training loss: 1381.9824887806947\n",
      "Epoch 3 step 806: training accuarcy: 0.7055\n",
      "Epoch 3 step 806: training loss: 1382.181206165346\n",
      "Epoch 3 step 807: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 807: training loss: 1382.5297139624822\n",
      "Epoch 3 step 808: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 808: training loss: 1380.491392155805\n",
      "Epoch 3 step 809: training accuarcy: 0.726\n",
      "Epoch 3 step 809: training loss: 1382.7275469462568\n",
      "Epoch 3 step 810: training accuarcy: 0.704\n",
      "Epoch 3 step 810: training loss: 1382.1625415883595\n",
      "Epoch 3 step 811: training accuarcy: 0.7015\n",
      "Epoch 3 step 811: training loss: 1382.6722087134985\n",
      "Epoch 3 step 812: training accuarcy: 0.704\n",
      "Epoch 3 step 812: training loss: 1382.473890956701\n",
      "Epoch 3 step 813: training accuarcy: 0.71\n",
      "Epoch 3 step 813: training loss: 1382.9205832338823\n",
      "Epoch 3 step 814: training accuarcy: 0.7135\n",
      "Epoch 3 step 814: training loss: 1381.570383865883\n",
      "Epoch 3 step 815: training accuarcy: 0.725\n",
      "Epoch 3 step 815: training loss: 1382.2709083629122\n",
      "Epoch 3 step 816: training accuarcy: 0.7045\n",
      "Epoch 3 step 816: training loss: 1382.1481820978922\n",
      "Epoch 3 step 817: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 817: training loss: 1382.3808131439105\n",
      "Epoch 3 step 818: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 818: training loss: 1382.5003559543034\n",
      "Epoch 3 step 819: training accuarcy: 0.7145\n",
      "Epoch 3 step 819: training loss: 1381.9382969263565\n",
      "Epoch 3 step 820: training accuarcy: 0.7205\n",
      "Epoch 3 step 820: training loss: 1381.4439117104275\n",
      "Epoch 3 step 821: training accuarcy: 0.7405\n",
      "Epoch 3 step 821: training loss: 1382.6695942254798\n",
      "Epoch 3 step 822: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 822: training loss: 1383.2030783061434\n",
      "Epoch 3 step 823: training accuarcy: 0.71\n",
      "Epoch 3 step 823: training loss: 1382.0737211518956\n",
      "Epoch 3 step 824: training accuarcy: 0.7065\n",
      "Epoch 3 step 824: training loss: 1382.5929789496677\n",
      "Epoch 3 step 825: training accuarcy: 0.6955\n",
      "Epoch 3 step 825: training loss: 1382.5814041814208\n",
      "Epoch 3 step 826: training accuarcy: 0.7205\n",
      "Epoch 3 step 826: training loss: 1382.4038187838994\n",
      "Epoch 3 step 827: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 827: training loss: 1382.6532268810831\n",
      "Epoch 3 step 828: training accuarcy: 0.7065\n",
      "Epoch 3 step 828: training loss: 1383.3657261327194\n",
      "Epoch 3 step 829: training accuarcy: 0.6935\n",
      "Epoch 3 step 829: training loss: 1381.8336638536787\n",
      "Epoch 3 step 830: training accuarcy: 0.721\n",
      "Epoch 3 step 830: training loss: 1382.9032811611335\n",
      "Epoch 3 step 831: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 831: training loss: 1382.3006187857711\n",
      "Epoch 3 step 832: training accuarcy: 0.7115\n",
      "Epoch 3 step 832: training loss: 1383.1271880755962\n",
      "Epoch 3 step 833: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 833: training loss: 1382.582473909749\n",
      "Epoch 3 step 834: training accuarcy: 0.711\n",
      "Epoch 3 step 834: training loss: 1382.9418658724194\n",
      "Epoch 3 step 835: training accuarcy: 0.6905\n",
      "Epoch 3 step 835: training loss: 1382.225336355844\n",
      "Epoch 3 step 836: training accuarcy: 0.709\n",
      "Epoch 3 step 836: training loss: 1382.4231076726815\n",
      "Epoch 3 step 837: training accuarcy: 0.7185\n",
      "Epoch 3 step 837: training loss: 1382.1670145901178\n",
      "Epoch 3 step 838: training accuarcy: 0.7085\n",
      "Epoch 3 step 838: training loss: 1382.3245475648366\n",
      "Epoch 3 step 839: training accuarcy: 0.709\n",
      "Epoch 3 step 839: training loss: 1382.8299863113289\n",
      "Epoch 3 step 840: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 840: training loss: 1382.2436673718269\n",
      "Epoch 3 step 841: training accuarcy: 0.718\n",
      "Epoch 3 step 841: training loss: 1383.7166621677454\n",
      "Epoch 3 step 842: training accuarcy: 0.6825\n",
      "Epoch 3 step 842: training loss: 1382.1736311235968\n",
      "Epoch 3 step 843: training accuarcy: 0.7115\n",
      "Epoch 3 step 843: training loss: 1382.2163160743037\n",
      "Epoch 3 step 844: training accuarcy: 0.7115\n",
      "Epoch 3 step 844: training loss: 1382.6887808814572\n",
      "Epoch 3 step 845: training accuarcy: 0.7155\n",
      "Epoch 3 step 845: training loss: 1382.5491623093162\n",
      "Epoch 3 step 846: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 846: training loss: 1382.9787513297954\n",
      "Epoch 3 step 847: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 847: training loss: 1383.2318644855457\n",
      "Epoch 3 step 848: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 848: training loss: 1382.8082648088093\n",
      "Epoch 3 step 849: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 849: training loss: 1382.6607535258368\n",
      "Epoch 3 step 850: training accuarcy: 0.7005\n",
      "Epoch 3 step 850: training loss: 1383.0987355869734\n",
      "Epoch 3 step 851: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 851: training loss: 1383.336450714333\n",
      "Epoch 3 step 852: training accuarcy: 0.7045\n",
      "Epoch 3 step 852: training loss: 1382.5216780614626\n",
      "Epoch 3 step 853: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 853: training loss: 1382.9361296473032\n",
      "Epoch 3 step 854: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 854: training loss: 1382.0759255671437\n",
      "Epoch 3 step 855: training accuarcy: 0.7145\n",
      "Epoch 3 step 855: training loss: 1382.5460919670302\n",
      "Epoch 3 step 856: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 856: training loss: 1382.2358681589556\n",
      "Epoch 3 step 857: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 857: training loss: 1383.3674557431757\n",
      "Epoch 3 step 858: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 858: training loss: 1381.6525745769602\n",
      "Epoch 3 step 859: training accuarcy: 0.714\n",
      "Epoch 3 step 859: training loss: 1382.7104683529462\n",
      "Epoch 3 step 860: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 860: training loss: 1383.0589270731916\n",
      "Epoch 3 step 861: training accuarcy: 0.6935\n",
      "Epoch 3 step 861: training loss: 1383.3513062884315\n",
      "Epoch 3 step 862: training accuarcy: 0.6865\n",
      "Epoch 3 step 862: training loss: 1382.8988513084046\n",
      "Epoch 3 step 863: training accuarcy: 0.706\n",
      "Epoch 3 step 863: training loss: 1383.0668503078837\n",
      "Epoch 3 step 864: training accuarcy: 0.6845\n",
      "Epoch 3 step 864: training loss: 1382.1177862531542\n",
      "Epoch 3 step 865: training accuarcy: 0.6945\n",
      "Epoch 3 step 865: training loss: 1383.6070057097338\n",
      "Epoch 3 step 866: training accuarcy: 0.6975\n",
      "Epoch 3 step 866: training loss: 1382.678644740142\n",
      "Epoch 3 step 867: training accuarcy: 0.7085\n",
      "Epoch 3 step 867: training loss: 1381.578068798145\n",
      "Epoch 3 step 868: training accuarcy: 0.7045\n",
      "Epoch 3 step 868: training loss: 1382.532645559452\n",
      "Epoch 3 step 869: training accuarcy: 0.7115\n",
      "Epoch 3 step 869: training loss: 1383.0279121687602\n",
      "Epoch 3 step 870: training accuarcy: 0.6905\n",
      "Epoch 3 step 870: training loss: 1383.0137424126897\n",
      "Epoch 3 step 871: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 871: training loss: 1382.7563729968658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 872: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 872: training loss: 1382.8342172630805\n",
      "Epoch 3 step 873: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 873: training loss: 1382.408737870862\n",
      "Epoch 3 step 874: training accuarcy: 0.6910000000000001\n",
      "Epoch 3 step 874: training loss: 1382.7852380022237\n",
      "Epoch 3 step 875: training accuarcy: 0.722\n",
      "Epoch 3 step 875: training loss: 1382.8981096418468\n",
      "Epoch 3 step 876: training accuarcy: 0.7015\n",
      "Epoch 3 step 876: training loss: 1383.094732449932\n",
      "Epoch 3 step 877: training accuarcy: 0.7065\n",
      "Epoch 3 step 877: training loss: 1382.9133699775402\n",
      "Epoch 3 step 878: training accuarcy: 0.6985\n",
      "Epoch 3 step 878: training loss: 1383.3511092711428\n",
      "Epoch 3 step 879: training accuarcy: 0.7015\n",
      "Epoch 3 step 879: training loss: 1382.8888366873455\n",
      "Epoch 3 step 880: training accuarcy: 0.6965\n",
      "Epoch 3 step 880: training loss: 1383.2441821626264\n",
      "Epoch 3 step 881: training accuarcy: 0.6915\n",
      "Epoch 3 step 881: training loss: 1382.2240952187592\n",
      "Epoch 3 step 882: training accuarcy: 0.704\n",
      "Epoch 3 step 882: training loss: 1382.9778388178813\n",
      "Epoch 3 step 883: training accuarcy: 0.7085\n",
      "Epoch 3 step 883: training loss: 1382.270892890538\n",
      "Epoch 3 step 884: training accuarcy: 0.707\n",
      "Epoch 3 step 884: training loss: 1382.286375436162\n",
      "Epoch 3 step 885: training accuarcy: 0.714\n",
      "Epoch 3 step 885: training loss: 1382.2066124072142\n",
      "Epoch 3 step 886: training accuarcy: 0.71\n",
      "Epoch 3 step 886: training loss: 1383.14859016999\n",
      "Epoch 3 step 887: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 887: training loss: 1382.836500253523\n",
      "Epoch 3 step 888: training accuarcy: 0.706\n",
      "Epoch 3 step 888: training loss: 1383.435473042593\n",
      "Epoch 3 step 889: training accuarcy: 0.6910000000000001\n",
      "Epoch 3 step 889: training loss: 1382.8282221266973\n",
      "Epoch 3 step 890: training accuarcy: 0.7005\n",
      "Epoch 3 step 890: training loss: 1383.0620460494729\n",
      "Epoch 3 step 891: training accuarcy: 0.6845\n",
      "Epoch 3 step 891: training loss: 1382.6070489830277\n",
      "Epoch 3 step 892: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 892: training loss: 1382.7395155615457\n",
      "Epoch 3 step 893: training accuarcy: 0.6955\n",
      "Epoch 3 step 893: training loss: 1382.723173643032\n",
      "Epoch 3 step 894: training accuarcy: 0.6975\n",
      "Epoch 3 step 894: training loss: 1382.1144515439455\n",
      "Epoch 3 step 895: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 895: training loss: 1383.2648100836498\n",
      "Epoch 3 step 896: training accuarcy: 0.6925\n",
      "Epoch 3 step 896: training loss: 1382.4906668486483\n",
      "Epoch 3 step 897: training accuarcy: 0.716\n",
      "Epoch 3 step 897: training loss: 1381.8594946334777\n",
      "Epoch 3 step 898: training accuarcy: 0.718\n",
      "Epoch 3 step 898: training loss: 1382.424816098648\n",
      "Epoch 3 step 899: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 899: training loss: 1384.0302255760184\n",
      "Epoch 3 step 900: training accuarcy: 0.6905\n",
      "Epoch 3 step 900: training loss: 1382.646952281689\n",
      "Epoch 3 step 901: training accuarcy: 0.71\n",
      "Epoch 3 step 901: training loss: 1383.7026145305056\n",
      "Epoch 3 step 902: training accuarcy: 0.6835\n",
      "Epoch 3 step 902: training loss: 1382.7991278530626\n",
      "Epoch 3 step 903: training accuarcy: 0.7025\n",
      "Epoch 3 step 903: training loss: 1383.7690058583362\n",
      "Epoch 3 step 904: training accuarcy: 0.6925\n",
      "Epoch 3 step 904: training loss: 1382.252985687942\n",
      "Epoch 3 step 905: training accuarcy: 0.712\n",
      "Epoch 3 step 905: training loss: 1381.6995892601465\n",
      "Epoch 3 step 906: training accuarcy: 0.7115\n",
      "Epoch 3 step 906: training loss: 1383.3980957780755\n",
      "Epoch 3 step 907: training accuarcy: 0.6985\n",
      "Epoch 3 step 907: training loss: 1383.373160428702\n",
      "Epoch 3 step 908: training accuarcy: 0.7015\n",
      "Epoch 3 step 908: training loss: 1383.0868276039828\n",
      "Epoch 3 step 909: training accuarcy: 0.686\n",
      "Epoch 3 step 909: training loss: 1383.886344909808\n",
      "Epoch 3 step 910: training accuarcy: 0.6855\n",
      "Epoch 3 step 910: training loss: 1382.6856984325261\n",
      "Epoch 3 step 911: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 911: training loss: 1382.855977349579\n",
      "Epoch 3 step 912: training accuarcy: 0.7065\n",
      "Epoch 3 step 912: training loss: 1382.5746672115959\n",
      "Epoch 3 step 913: training accuarcy: 0.6905\n",
      "Epoch 3 step 913: training loss: 1383.2730849012612\n",
      "Epoch 3 step 914: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 914: training loss: 1383.6330511311464\n",
      "Epoch 3 step 915: training accuarcy: 0.6995\n",
      "Epoch 3 step 915: training loss: 1382.7757268437472\n",
      "Epoch 3 step 916: training accuarcy: 0.6995\n",
      "Epoch 3 step 916: training loss: 1382.8104584768273\n",
      "Epoch 3 step 917: training accuarcy: 0.6940000000000001\n",
      "Epoch 3 step 917: training loss: 1382.8211584871024\n",
      "Epoch 3 step 918: training accuarcy: 0.6945\n",
      "Epoch 3 step 918: training loss: 1382.3071684198821\n",
      "Epoch 3 step 919: training accuarcy: 0.7025\n",
      "Epoch 3 step 919: training loss: 1383.0678927748488\n",
      "Epoch 3 step 920: training accuarcy: 0.6965\n",
      "Epoch 3 step 920: training loss: 1382.6737125563009\n",
      "Epoch 3 step 921: training accuarcy: 0.7105\n",
      "Epoch 3 step 921: training loss: 1382.6674816843265\n",
      "Epoch 3 step 922: training accuarcy: 0.6875\n",
      "Epoch 3 step 922: training loss: 1383.1909084725569\n",
      "Epoch 3 step 923: training accuarcy: 0.6925\n",
      "Epoch 3 step 923: training loss: 1382.9214179614823\n",
      "Epoch 3 step 924: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 924: training loss: 1383.0064316877379\n",
      "Epoch 3 step 925: training accuarcy: 0.6855\n",
      "Epoch 3 step 925: training loss: 1382.19723155167\n",
      "Epoch 3 step 926: training accuarcy: 0.7115\n",
      "Epoch 3 step 926: training loss: 1382.6058053816241\n",
      "Epoch 3 step 927: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 927: training loss: 1382.7610809808627\n",
      "Epoch 3 step 928: training accuarcy: 0.7035\n",
      "Epoch 3 step 928: training loss: 1382.5399333943901\n",
      "Epoch 3 step 929: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 929: training loss: 1383.0350438627977\n",
      "Epoch 3 step 930: training accuarcy: 0.6885\n",
      "Epoch 3 step 930: training loss: 1382.7667223450044\n",
      "Epoch 3 step 931: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 931: training loss: 1382.76181521007\n",
      "Epoch 3 step 932: training accuarcy: 0.708\n",
      "Epoch 3 step 932: training loss: 1383.1712525812375\n",
      "Epoch 3 step 933: training accuarcy: 0.684\n",
      "Epoch 3 step 933: training loss: 1382.638149397846\n",
      "Epoch 3 step 934: training accuarcy: 0.6985\n",
      "Epoch 3 step 934: training loss: 1383.0165428986431\n",
      "Epoch 3 step 935: training accuarcy: 0.6925\n",
      "Epoch 3 step 935: training loss: 1382.791037919524\n",
      "Epoch 3 step 936: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 936: training loss: 1383.1886515909823\n",
      "Epoch 3 step 937: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 937: training loss: 1383.0077398785209\n",
      "Epoch 3 step 938: training accuarcy: 0.7025\n",
      "Epoch 3 step 938: training loss: 1382.9052282841883\n",
      "Epoch 3 step 939: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 939: training loss: 1382.324421712972\n",
      "Epoch 3 step 940: training accuarcy: 0.7085\n",
      "Epoch 3 step 940: training loss: 1384.1420219612053\n",
      "Epoch 3 step 941: training accuarcy: 0.674\n",
      "Epoch 3 step 941: training loss: 1382.961383371035\n",
      "Epoch 3 step 942: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 942: training loss: 1383.0064019404624\n",
      "Epoch 3 step 943: training accuarcy: 0.7015\n",
      "Epoch 3 step 943: training loss: 1382.9710300667837\n",
      "Epoch 3 step 944: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 944: training loss: 1383.0010477662436\n",
      "Epoch 3 step 945: training accuarcy: 0.6935\n",
      "Epoch 3 step 945: training loss: 1382.8225668691214\n",
      "Epoch 3 step 946: training accuarcy: 0.6875\n",
      "Epoch 3 step 946: training loss: 1382.183020021508\n",
      "Epoch 3 step 947: training accuarcy: 0.7085\n",
      "Epoch 3 step 947: training loss: 1382.9607252478415\n",
      "Epoch 3 step 948: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 948: training loss: 1383.013272272053\n",
      "Epoch 3 step 949: training accuarcy: 0.6995\n",
      "Epoch 3 step 949: training loss: 1381.6729534062224\n",
      "Epoch 3 step 950: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 950: training loss: 1381.9141564944518\n",
      "Epoch 3 step 951: training accuarcy: 0.704\n",
      "Epoch 3 step 951: training loss: 1383.0303112106706\n",
      "Epoch 3 step 952: training accuarcy: 0.6965\n",
      "Epoch 3 step 952: training loss: 1382.6950765287766\n",
      "Epoch 3 step 953: training accuarcy: 0.6805\n",
      "Epoch 3 step 953: training loss: 1382.5457939837033\n",
      "Epoch 3 step 954: training accuarcy: 0.7075\n",
      "Epoch 3 step 954: training loss: 1382.799820165191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 955: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 955: training loss: 1383.2117954111438\n",
      "Epoch 3 step 956: training accuarcy: 0.686\n",
      "Epoch 3 step 956: training loss: 1382.2238967924873\n",
      "Epoch 3 step 957: training accuarcy: 0.7035\n",
      "Epoch 3 step 957: training loss: 1382.562102176379\n",
      "Epoch 3 step 958: training accuarcy: 0.7005\n",
      "Epoch 3 step 958: training loss: 1382.9010781706747\n",
      "Epoch 3 step 959: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 959: training loss: 1383.313182998501\n",
      "Epoch 3 step 960: training accuarcy: 0.7035\n",
      "Epoch 3 step 960: training loss: 1383.4887704805974\n",
      "Epoch 3 step 961: training accuarcy: 0.6915\n",
      "Epoch 3 step 961: training loss: 1382.2393600231171\n",
      "Epoch 3 step 962: training accuarcy: 0.71\n",
      "Epoch 3 step 962: training loss: 1382.2856009137301\n",
      "Epoch 3 step 963: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 963: training loss: 1383.094539804581\n",
      "Epoch 3 step 964: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 964: training loss: 1383.0788073046192\n",
      "Epoch 3 step 965: training accuarcy: 0.687\n",
      "Epoch 3 step 965: training loss: 1382.28424676475\n",
      "Epoch 3 step 966: training accuarcy: 0.7065\n",
      "Epoch 3 step 966: training loss: 1382.3076725855196\n",
      "Epoch 3 step 967: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 967: training loss: 1382.5665534992572\n",
      "Epoch 3 step 968: training accuarcy: 0.6940000000000001\n",
      "Epoch 3 step 968: training loss: 1383.32931891928\n",
      "Epoch 3 step 969: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 969: training loss: 1383.4967185417622\n",
      "Epoch 3 step 970: training accuarcy: 0.6785\n",
      "Epoch 3 step 970: training loss: 1382.6937140648297\n",
      "Epoch 3 step 971: training accuarcy: 0.6905\n",
      "Epoch 3 step 971: training loss: 1383.458388776812\n",
      "Epoch 3 step 972: training accuarcy: 0.6885\n",
      "Epoch 3 step 972: training loss: 1382.0657942173018\n",
      "Epoch 3 step 973: training accuarcy: 0.7255\n",
      "Epoch 3 step 973: training loss: 1382.5216474264892\n",
      "Epoch 3 step 974: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 974: training loss: 1383.2220750862577\n",
      "Epoch 3 step 975: training accuarcy: 0.679\n",
      "Epoch 3 step 975: training loss: 1382.8981475457458\n",
      "Epoch 3 step 976: training accuarcy: 0.7055\n",
      "Epoch 3 step 976: training loss: 1383.8702974015584\n",
      "Epoch 3 step 977: training accuarcy: 0.687\n",
      "Epoch 3 step 977: training loss: 1384.0427006842617\n",
      "Epoch 3 step 978: training accuarcy: 0.684\n",
      "Epoch 3 step 978: training loss: 1382.8330556283822\n",
      "Epoch 3 step 979: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 979: training loss: 1383.0491373785007\n",
      "Epoch 3 step 980: training accuarcy: 0.715\n",
      "Epoch 3 step 980: training loss: 1382.229491907003\n",
      "Epoch 3 step 981: training accuarcy: 0.7055\n",
      "Epoch 3 step 981: training loss: 1383.1300181028962\n",
      "Epoch 3 step 982: training accuarcy: 0.6815\n",
      "Epoch 3 step 982: training loss: 1383.0384701135472\n",
      "Epoch 3 step 983: training accuarcy: 0.7015\n",
      "Epoch 3 step 983: training loss: 1382.8648615120971\n",
      "Epoch 3 step 984: training accuarcy: 0.6865\n",
      "Epoch 3 step 984: training loss: 1382.8902406771504\n",
      "Epoch 3 step 985: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 985: training loss: 1384.2517159471056\n",
      "Epoch 3 step 986: training accuarcy: 0.6825\n",
      "Epoch 3 step 986: training loss: 1383.1052203148015\n",
      "Epoch 3 step 987: training accuarcy: 0.6995\n",
      "Epoch 3 step 987: training loss: 1382.7845957122543\n",
      "Epoch 3 step 988: training accuarcy: 0.707\n",
      "Epoch 3 step 988: training loss: 1382.9003484280067\n",
      "Epoch 3 step 989: training accuarcy: 0.7085\n",
      "Epoch 3 step 989: training loss: 1382.7977215313485\n",
      "Epoch 3 step 990: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 990: training loss: 1383.193846758206\n",
      "Epoch 3 step 991: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 991: training loss: 1382.7952287177566\n",
      "Epoch 3 step 992: training accuarcy: 0.6835\n",
      "Epoch 3 step 992: training loss: 1381.7879477049942\n",
      "Epoch 3 step 993: training accuarcy: 0.723\n",
      "Epoch 3 step 993: training loss: 1382.9987745242627\n",
      "Epoch 3 step 994: training accuarcy: 0.705\n",
      "Epoch 3 step 994: training loss: 1384.1838052465152\n",
      "Epoch 3 step 995: training accuarcy: 0.6910000000000001\n",
      "Epoch 3 step 995: training loss: 1383.2856302603452\n",
      "Epoch 3 step 996: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 996: training loss: 1382.9027141251086\n",
      "Epoch 3 step 997: training accuarcy: 0.684\n",
      "Epoch 3 step 997: training loss: 1383.5822526622912\n",
      "Epoch 3 step 998: training accuarcy: 0.6795\n",
      "Epoch 3 step 998: training loss: 1382.042384904248\n",
      "Epoch 3 step 999: training accuarcy: 0.715\n",
      "Epoch 3 step 999: training loss: 1382.9154244497145\n",
      "Epoch 3 step 1000: training accuarcy: 0.7005\n",
      "Epoch 3 step 1000: training loss: 1382.2500689336312\n",
      "Epoch 3 step 1001: training accuarcy: 0.7065\n",
      "Epoch 3 step 1001: training loss: 1382.999278526902\n",
      "Epoch 3 step 1002: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 1002: training loss: 1383.836001767882\n",
      "Epoch 3 step 1003: training accuarcy: 0.664\n",
      "Epoch 3 step 1003: training loss: 1383.5297174903033\n",
      "Epoch 3 step 1004: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 1004: training loss: 1383.1320533678788\n",
      "Epoch 3 step 1005: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 1005: training loss: 1383.1646138047447\n",
      "Epoch 3 step 1006: training accuarcy: 0.7035\n",
      "Epoch 3 step 1006: training loss: 1383.1411846878002\n",
      "Epoch 3 step 1007: training accuarcy: 0.6785\n",
      "Epoch 3 step 1007: training loss: 1383.3459154391774\n",
      "Epoch 3 step 1008: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 1008: training loss: 1383.8817488634913\n",
      "Epoch 3 step 1009: training accuarcy: 0.6765\n",
      "Epoch 3 step 1009: training loss: 1383.3214185280294\n",
      "Epoch 3 step 1010: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 1010: training loss: 1382.6942946691113\n",
      "Epoch 3 step 1011: training accuarcy: 0.704\n",
      "Epoch 3 step 1011: training loss: 1383.0299990056005\n",
      "Epoch 3 step 1012: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 1012: training loss: 1384.6140814991807\n",
      "Epoch 3 step 1013: training accuarcy: 0.6855\n",
      "Epoch 3 step 1013: training loss: 1383.809552998218\n",
      "Epoch 3 step 1014: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 1014: training loss: 1383.5007070445852\n",
      "Epoch 3 step 1015: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 1015: training loss: 1383.163433689731\n",
      "Epoch 3 step 1016: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 1016: training loss: 1382.9009193448835\n",
      "Epoch 3 step 1017: training accuarcy: 0.7165\n",
      "Epoch 3 step 1017: training loss: 1383.0433105127934\n",
      "Epoch 3 step 1018: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 1018: training loss: 1383.23323952542\n",
      "Epoch 3 step 1019: training accuarcy: 0.6880000000000001\n",
      "Epoch 3 step 1019: training loss: 1381.8760521131464\n",
      "Epoch 3 step 1020: training accuarcy: 0.7145\n",
      "Epoch 3 step 1020: training loss: 1383.0344043271032\n",
      "Epoch 3 step 1021: training accuarcy: 0.6905\n",
      "Epoch 3 step 1021: training loss: 1383.8017549383255\n",
      "Epoch 3 step 1022: training accuarcy: 0.683\n",
      "Epoch 3 step 1022: training loss: 1383.6644115055801\n",
      "Epoch 3 step 1023: training accuarcy: 0.6795\n",
      "Epoch 3 step 1023: training loss: 1383.339609938045\n",
      "Epoch 3 step 1024: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 1024: training loss: 1383.2393661273838\n",
      "Epoch 3 step 1025: training accuarcy: 0.7005\n",
      "Epoch 3 step 1025: training loss: 1382.5957352573957\n",
      "Epoch 3 step 1026: training accuarcy: 0.708\n",
      "Epoch 3 step 1026: training loss: 1383.0978687858806\n",
      "Epoch 3 step 1027: training accuarcy: 0.681\n",
      "Epoch 3 step 1027: training loss: 1382.834971433045\n",
      "Epoch 3 step 1028: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 1028: training loss: 1383.4894891726897\n",
      "Epoch 3 step 1029: training accuarcy: 0.684\n",
      "Epoch 3 step 1029: training loss: 1383.1493862594352\n",
      "Epoch 3 step 1030: training accuarcy: 0.6925\n",
      "Epoch 3 step 1030: training loss: 1383.4707277282162\n",
      "Epoch 3 step 1031: training accuarcy: 0.6935\n",
      "Epoch 3 step 1031: training loss: 1383.0326711310763\n",
      "Epoch 3 step 1032: training accuarcy: 0.711\n",
      "Epoch 3 step 1032: training loss: 1382.4856997741156\n",
      "Epoch 3 step 1033: training accuarcy: 0.707\n",
      "Epoch 3 step 1033: training loss: 1383.006280029817\n",
      "Epoch 3 step 1034: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 1034: training loss: 1382.3768242842843\n",
      "Epoch 3 step 1035: training accuarcy: 0.6965\n",
      "Epoch 3 step 1035: training loss: 1383.107145373653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 1036: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 1036: training loss: 1383.0001820319944\n",
      "Epoch 3 step 1037: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 1037: training loss: 1382.9766155011955\n",
      "Epoch 3 step 1038: training accuarcy: 0.7135\n",
      "Epoch 3 step 1038: training loss: 1383.7114875687118\n",
      "Epoch 3 step 1039: training accuarcy: 0.6905\n",
      "Epoch 3 step 1039: training loss: 1382.321061804624\n",
      "Epoch 3 step 1040: training accuarcy: 0.7095\n",
      "Epoch 3 step 1040: training loss: 1383.4811931571169\n",
      "Epoch 3 step 1041: training accuarcy: 0.686\n",
      "Epoch 3 step 1041: training loss: 1382.7678398539358\n",
      "Epoch 3 step 1042: training accuarcy: 0.6935\n",
      "Epoch 3 step 1042: training loss: 1383.1578072251814\n",
      "Epoch 3 step 1043: training accuarcy: 0.6985\n",
      "Epoch 3 step 1043: training loss: 1383.43803277923\n",
      "Epoch 3 step 1044: training accuarcy: 0.682\n",
      "Epoch 3 step 1044: training loss: 1382.525838068188\n",
      "Epoch 3 step 1045: training accuarcy: 0.6935\n",
      "Epoch 3 step 1045: training loss: 1382.6732790175304\n",
      "Epoch 3 step 1046: training accuarcy: 0.704\n",
      "Epoch 3 step 1046: training loss: 1382.8003359749878\n",
      "Epoch 3 step 1047: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 1047: training loss: 1382.816166098336\n",
      "Epoch 3 step 1048: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 1048: training loss: 1382.398188364292\n",
      "Epoch 3 step 1049: training accuarcy: 0.704\n",
      "Epoch 3 step 1049: training loss: 1382.822563137731\n",
      "Epoch 3 step 1050: training accuarcy: 0.7035\n",
      "Epoch 3 step 1050: training loss: 1382.4781114178859\n",
      "Epoch 3 step 1051: training accuarcy: 0.7095\n",
      "Epoch 3 step 1051: training loss: 542.79263069976\n",
      "Epoch 3 step 1052: training accuarcy: 0.7128205128205128\n",
      "Epoch 3: train loss 1379.585195432144, train accuarcy 0.7019508481025696\n",
      "Epoch 3: valid loss 1364.4158714557275, valid accuarcy 0.7099252343177795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 4/8 [07:44<07:43, 115.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 1382.0408522295422\n",
      "Epoch 4 step 1053: training accuarcy: 0.7215\n",
      "Epoch 4 step 1053: training loss: 1381.8361811680559\n",
      "Epoch 4 step 1054: training accuarcy: 0.7075\n",
      "Epoch 4 step 1054: training loss: 1382.1049328887134\n",
      "Epoch 4 step 1055: training accuarcy: 0.708\n",
      "Epoch 4 step 1055: training loss: 1382.4317358418919\n",
      "Epoch 4 step 1056: training accuarcy: 0.707\n",
      "Epoch 4 step 1056: training loss: 1381.2602350573336\n",
      "Epoch 4 step 1057: training accuarcy: 0.7145\n",
      "Epoch 4 step 1057: training loss: 1381.6109753770697\n",
      "Epoch 4 step 1058: training accuarcy: 0.718\n",
      "Epoch 4 step 1058: training loss: 1382.4188737273682\n",
      "Epoch 4 step 1059: training accuarcy: 0.716\n",
      "Epoch 4 step 1059: training loss: 1382.6446113002419\n",
      "Epoch 4 step 1060: training accuarcy: 0.7035\n",
      "Epoch 4 step 1060: training loss: 1382.3513262279919\n",
      "Epoch 4 step 1061: training accuarcy: 0.709\n",
      "Epoch 4 step 1061: training loss: 1382.5711978479458\n",
      "Epoch 4 step 1062: training accuarcy: 0.7145\n",
      "Epoch 4 step 1062: training loss: 1382.6370699742972\n",
      "Epoch 4 step 1063: training accuarcy: 0.7055\n",
      "Epoch 4 step 1063: training loss: 1381.7702486708415\n",
      "Epoch 4 step 1064: training accuarcy: 0.725\n",
      "Epoch 4 step 1064: training loss: 1382.1036693384222\n",
      "Epoch 4 step 1065: training accuarcy: 0.712\n",
      "Epoch 4 step 1065: training loss: 1382.0950631766723\n",
      "Epoch 4 step 1066: training accuarcy: 0.7045\n",
      "Epoch 4 step 1066: training loss: 1381.4348646969552\n",
      "Epoch 4 step 1067: training accuarcy: 0.7135\n",
      "Epoch 4 step 1067: training loss: 1381.9386474740024\n",
      "Epoch 4 step 1068: training accuarcy: 0.6955\n",
      "Epoch 4 step 1068: training loss: 1382.5027824826077\n",
      "Epoch 4 step 1069: training accuarcy: 0.716\n",
      "Epoch 4 step 1069: training loss: 1382.1524245814546\n",
      "Epoch 4 step 1070: training accuarcy: 0.7185\n",
      "Epoch 4 step 1070: training loss: 1382.7597771004973\n",
      "Epoch 4 step 1071: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1071: training loss: 1382.6595984852477\n",
      "Epoch 4 step 1072: training accuarcy: 0.6995\n",
      "Epoch 4 step 1072: training loss: 1382.1108057256185\n",
      "Epoch 4 step 1073: training accuarcy: 0.726\n",
      "Epoch 4 step 1073: training loss: 1382.2963273902396\n",
      "Epoch 4 step 1074: training accuarcy: 0.7125\n",
      "Epoch 4 step 1074: training loss: 1380.9479618999455\n",
      "Epoch 4 step 1075: training accuarcy: 0.7155\n",
      "Epoch 4 step 1075: training loss: 1382.3038998407706\n",
      "Epoch 4 step 1076: training accuarcy: 0.705\n",
      "Epoch 4 step 1076: training loss: 1382.4641063026802\n",
      "Epoch 4 step 1077: training accuarcy: 0.722\n",
      "Epoch 4 step 1077: training loss: 1383.3382689971459\n",
      "Epoch 4 step 1078: training accuarcy: 0.686\n",
      "Epoch 4 step 1078: training loss: 1382.3728887809552\n",
      "Epoch 4 step 1079: training accuarcy: 0.705\n",
      "Epoch 4 step 1079: training loss: 1382.244441853588\n",
      "Epoch 4 step 1080: training accuarcy: 0.721\n",
      "Epoch 4 step 1080: training loss: 1381.0515257251823\n",
      "Epoch 4 step 1081: training accuarcy: 0.7145\n",
      "Epoch 4 step 1081: training loss: 1382.9402948046268\n",
      "Epoch 4 step 1082: training accuarcy: 0.7035\n",
      "Epoch 4 step 1082: training loss: 1381.668507139374\n",
      "Epoch 4 step 1083: training accuarcy: 0.706\n",
      "Epoch 4 step 1083: training loss: 1382.7167449541228\n",
      "Epoch 4 step 1084: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1084: training loss: 1382.6563131462606\n",
      "Epoch 4 step 1085: training accuarcy: 0.7005\n",
      "Epoch 4 step 1085: training loss: 1382.2217206439045\n",
      "Epoch 4 step 1086: training accuarcy: 0.72\n",
      "Epoch 4 step 1086: training loss: 1382.6128864477228\n",
      "Epoch 4 step 1087: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1087: training loss: 1382.5086463246819\n",
      "Epoch 4 step 1088: training accuarcy: 0.7025\n",
      "Epoch 4 step 1088: training loss: 1383.2433182865511\n",
      "Epoch 4 step 1089: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1089: training loss: 1382.5334095379383\n",
      "Epoch 4 step 1090: training accuarcy: 0.7125\n",
      "Epoch 4 step 1090: training loss: 1382.6897836964195\n",
      "Epoch 4 step 1091: training accuarcy: 0.7125\n",
      "Epoch 4 step 1091: training loss: 1383.264575030833\n",
      "Epoch 4 step 1092: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1092: training loss: 1383.5278439234382\n",
      "Epoch 4 step 1093: training accuarcy: 0.6900000000000001\n",
      "Epoch 4 step 1093: training loss: 1383.0338409837855\n",
      "Epoch 4 step 1094: training accuarcy: 0.7000000000000001\n",
      "Epoch 4 step 1094: training loss: 1382.0252184403696\n",
      "Epoch 4 step 1095: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1095: training loss: 1381.671399545857\n",
      "Epoch 4 step 1096: training accuarcy: 0.708\n",
      "Epoch 4 step 1096: training loss: 1381.0254283798752\n",
      "Epoch 4 step 1097: training accuarcy: 0.7195\n",
      "Epoch 4 step 1097: training loss: 1382.2658972011664\n",
      "Epoch 4 step 1098: training accuarcy: 0.7065\n",
      "Epoch 4 step 1098: training loss: 1382.436419370628\n",
      "Epoch 4 step 1099: training accuarcy: 0.6920000000000001\n",
      "Epoch 4 step 1099: training loss: 1382.3802946056821\n",
      "Epoch 4 step 1100: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1100: training loss: 1383.1868577572918\n",
      "Epoch 4 step 1101: training accuarcy: 0.6945\n",
      "Epoch 4 step 1101: training loss: 1382.2790942859044\n",
      "Epoch 4 step 1102: training accuarcy: 0.71\n",
      "Epoch 4 step 1102: training loss: 1381.8376261934384\n",
      "Epoch 4 step 1103: training accuarcy: 0.7245\n",
      "Epoch 4 step 1103: training loss: 1382.1063064634923\n",
      "Epoch 4 step 1104: training accuarcy: 0.7135\n",
      "Epoch 4 step 1104: training loss: 1382.3324762390791\n",
      "Epoch 4 step 1105: training accuarcy: 0.7145\n",
      "Epoch 4 step 1105: training loss: 1382.3451663674077\n",
      "Epoch 4 step 1106: training accuarcy: 0.707\n",
      "Epoch 4 step 1106: training loss: 1382.199452182408\n",
      "Epoch 4 step 1107: training accuarcy: 0.6985\n",
      "Epoch 4 step 1107: training loss: 1382.0856596266337\n",
      "Epoch 4 step 1108: training accuarcy: 0.7095\n",
      "Epoch 4 step 1108: training loss: 1382.5381841783706\n",
      "Epoch 4 step 1109: training accuarcy: 0.7005\n",
      "Epoch 4 step 1109: training loss: 1382.5049811309336\n",
      "Epoch 4 step 1110: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1110: training loss: 1381.9200493544536\n",
      "Epoch 4 step 1111: training accuarcy: 0.7195\n",
      "Epoch 4 step 1111: training loss: 1382.7539051514318\n",
      "Epoch 4 step 1112: training accuarcy: 0.712\n",
      "Epoch 4 step 1112: training loss: 1382.7635733204306\n",
      "Epoch 4 step 1113: training accuarcy: 0.717\n",
      "Epoch 4 step 1113: training loss: 1382.5372635154579\n",
      "Epoch 4 step 1114: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1114: training loss: 1382.6933262810674\n",
      "Epoch 4 step 1115: training accuarcy: 0.7045\n",
      "Epoch 4 step 1115: training loss: 1382.8062435915551\n",
      "Epoch 4 step 1116: training accuarcy: 0.6920000000000001\n",
      "Epoch 4 step 1116: training loss: 1382.9280177527046\n",
      "Epoch 4 step 1117: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1117: training loss: 1383.170455895367\n",
      "Epoch 4 step 1118: training accuarcy: 0.7000000000000001\n",
      "Epoch 4 step 1118: training loss: 1382.7900301895243\n",
      "Epoch 4 step 1119: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1119: training loss: 1382.6901803865169\n",
      "Epoch 4 step 1120: training accuarcy: 0.7000000000000001\n",
      "Epoch 4 step 1120: training loss: 1382.5421032521388\n",
      "Epoch 4 step 1121: training accuarcy: 0.7075\n",
      "Epoch 4 step 1121: training loss: 1382.9713248123144\n",
      "Epoch 4 step 1122: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1122: training loss: 1383.3004523528177\n",
      "Epoch 4 step 1123: training accuarcy: 0.6945\n",
      "Epoch 4 step 1123: training loss: 1382.4623460956145\n",
      "Epoch 4 step 1124: training accuarcy: 0.7135\n",
      "Epoch 4 step 1124: training loss: 1383.294161244926\n",
      "Epoch 4 step 1125: training accuarcy: 0.6995\n",
      "Epoch 4 step 1125: training loss: 1382.8773051449496\n",
      "Epoch 4 step 1126: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1126: training loss: 1383.2580680019246\n",
      "Epoch 4 step 1127: training accuarcy: 0.716\n",
      "Epoch 4 step 1127: training loss: 1383.4110086248704\n",
      "Epoch 4 step 1128: training accuarcy: 0.6965\n",
      "Epoch 4 step 1128: training loss: 1382.9298328743603\n",
      "Epoch 4 step 1129: training accuarcy: 0.714\n",
      "Epoch 4 step 1129: training loss: 1383.5943224298544\n",
      "Epoch 4 step 1130: training accuarcy: 0.6855\n",
      "Epoch 4 step 1130: training loss: 1382.4577559673005\n",
      "Epoch 4 step 1131: training accuarcy: 0.6995\n",
      "Epoch 4 step 1131: training loss: 1381.9722646732798\n",
      "Epoch 4 step 1132: training accuarcy: 0.7035\n",
      "Epoch 4 step 1132: training loss: 1382.539231605418\n",
      "Epoch 4 step 1133: training accuarcy: 0.6925\n",
      "Epoch 4 step 1133: training loss: 1382.349602170169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1134: training accuarcy: 0.7075\n",
      "Epoch 4 step 1134: training loss: 1383.6259785060001\n",
      "Epoch 4 step 1135: training accuarcy: 0.682\n",
      "Epoch 4 step 1135: training loss: 1383.0893388955708\n",
      "Epoch 4 step 1136: training accuarcy: 0.6885\n",
      "Epoch 4 step 1136: training loss: 1382.5168181051986\n",
      "Epoch 4 step 1137: training accuarcy: 0.714\n",
      "Epoch 4 step 1137: training loss: 1381.5929857023032\n",
      "Epoch 4 step 1138: training accuarcy: 0.6985\n",
      "Epoch 4 step 1138: training loss: 1382.6321361774842\n",
      "Epoch 4 step 1139: training accuarcy: 0.7155\n",
      "Epoch 4 step 1139: training loss: 1383.140800107942\n",
      "Epoch 4 step 1140: training accuarcy: 0.681\n",
      "Epoch 4 step 1140: training loss: 1383.1118173601556\n",
      "Epoch 4 step 1141: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1141: training loss: 1382.6128432318794\n",
      "Epoch 4 step 1142: training accuarcy: 0.7005\n",
      "Epoch 4 step 1142: training loss: 1382.907146712596\n",
      "Epoch 4 step 1143: training accuarcy: 0.7005\n",
      "Epoch 4 step 1143: training loss: 1382.5604008036662\n",
      "Epoch 4 step 1144: training accuarcy: 0.7105\n",
      "Epoch 4 step 1144: training loss: 1383.1968039082872\n",
      "Epoch 4 step 1145: training accuarcy: 0.6835\n",
      "Epoch 4 step 1145: training loss: 1382.3005572475088\n",
      "Epoch 4 step 1146: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1146: training loss: 1382.3015456170465\n",
      "Epoch 4 step 1147: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1147: training loss: 1382.864334119867\n",
      "Epoch 4 step 1148: training accuarcy: 0.6875\n",
      "Epoch 4 step 1148: training loss: 1383.019483756956\n",
      "Epoch 4 step 1149: training accuarcy: 0.6875\n",
      "Epoch 4 step 1149: training loss: 1381.9355122979537\n",
      "Epoch 4 step 1150: training accuarcy: 0.714\n",
      "Epoch 4 step 1150: training loss: 1382.1398792267885\n",
      "Epoch 4 step 1151: training accuarcy: 0.709\n",
      "Epoch 4 step 1151: training loss: 1383.1530534746835\n",
      "Epoch 4 step 1152: training accuarcy: 0.6875\n",
      "Epoch 4 step 1152: training loss: 1382.553800355447\n",
      "Epoch 4 step 1153: training accuarcy: 0.711\n",
      "Epoch 4 step 1153: training loss: 1383.4068684218535\n",
      "Epoch 4 step 1154: training accuarcy: 0.7055\n",
      "Epoch 4 step 1154: training loss: 1382.5108128943118\n",
      "Epoch 4 step 1155: training accuarcy: 0.719\n",
      "Epoch 4 step 1155: training loss: 1382.0941877363373\n",
      "Epoch 4 step 1156: training accuarcy: 0.6945\n",
      "Epoch 4 step 1156: training loss: 1383.3984704200607\n",
      "Epoch 4 step 1157: training accuarcy: 0.705\n",
      "Epoch 4 step 1157: training loss: 1382.7753730760753\n",
      "Epoch 4 step 1158: training accuarcy: 0.6995\n",
      "Epoch 4 step 1158: training loss: 1383.2397998661168\n",
      "Epoch 4 step 1159: training accuarcy: 0.7000000000000001\n",
      "Epoch 4 step 1159: training loss: 1384.0432125312343\n",
      "Epoch 4 step 1160: training accuarcy: 0.6595\n",
      "Epoch 4 step 1160: training loss: 1382.14257655773\n",
      "Epoch 4 step 1161: training accuarcy: 0.7000000000000001\n",
      "Epoch 4 step 1161: training loss: 1382.8274629301359\n",
      "Epoch 4 step 1162: training accuarcy: 0.7035\n",
      "Epoch 4 step 1162: training loss: 1382.8228281898782\n",
      "Epoch 4 step 1163: training accuarcy: 0.713\n",
      "Epoch 4 step 1163: training loss: 1383.288976871256\n",
      "Epoch 4 step 1164: training accuarcy: 0.6715\n",
      "Epoch 4 step 1164: training loss: 1382.248977621363\n",
      "Epoch 4 step 1165: training accuarcy: 0.7125\n",
      "Epoch 4 step 1165: training loss: 1383.5432215834467\n",
      "Epoch 4 step 1166: training accuarcy: 0.6945\n",
      "Epoch 4 step 1166: training loss: 1382.7156160306663\n",
      "Epoch 4 step 1167: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1167: training loss: 1382.1384791917808\n",
      "Epoch 4 step 1168: training accuarcy: 0.7015\n",
      "Epoch 4 step 1168: training loss: 1382.6546428436816\n",
      "Epoch 4 step 1169: training accuarcy: 0.708\n",
      "Epoch 4 step 1169: training loss: 1382.3004507597132\n",
      "Epoch 4 step 1170: training accuarcy: 0.7005\n",
      "Epoch 4 step 1170: training loss: 1382.6278232162874\n",
      "Epoch 4 step 1171: training accuarcy: 0.7125\n",
      "Epoch 4 step 1171: training loss: 1382.4909324676144\n",
      "Epoch 4 step 1172: training accuarcy: 0.686\n",
      "Epoch 4 step 1172: training loss: 1383.8036716539218\n",
      "Epoch 4 step 1173: training accuarcy: 0.6805\n",
      "Epoch 4 step 1173: training loss: 1382.6358640267383\n",
      "Epoch 4 step 1174: training accuarcy: 0.6935\n",
      "Epoch 4 step 1174: training loss: 1382.7185855769294\n",
      "Epoch 4 step 1175: training accuarcy: 0.705\n",
      "Epoch 4 step 1175: training loss: 1382.6147560092104\n",
      "Epoch 4 step 1176: training accuarcy: 0.6945\n",
      "Epoch 4 step 1176: training loss: 1382.5785231935522\n",
      "Epoch 4 step 1177: training accuarcy: 0.6965\n",
      "Epoch 4 step 1177: training loss: 1382.6148454280033\n",
      "Epoch 4 step 1178: training accuarcy: 0.7185\n",
      "Epoch 4 step 1178: training loss: 1383.0733314210381\n",
      "Epoch 4 step 1179: training accuarcy: 0.6945\n",
      "Epoch 4 step 1179: training loss: 1382.5065521947397\n",
      "Epoch 4 step 1180: training accuarcy: 0.71\n",
      "Epoch 4 step 1180: training loss: 1382.8840335387554\n",
      "Epoch 4 step 1181: training accuarcy: 0.6805\n",
      "Epoch 4 step 1181: training loss: 1382.2950250089848\n",
      "Epoch 4 step 1182: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1182: training loss: 1382.3077919326765\n",
      "Epoch 4 step 1183: training accuarcy: 0.6975\n",
      "Epoch 4 step 1183: training loss: 1384.2503971296162\n",
      "Epoch 4 step 1184: training accuarcy: 0.6585\n",
      "Epoch 4 step 1184: training loss: 1382.7940648537585\n",
      "Epoch 4 step 1185: training accuarcy: 0.705\n",
      "Epoch 4 step 1185: training loss: 1382.7585514339253\n",
      "Epoch 4 step 1186: training accuarcy: 0.6985\n",
      "Epoch 4 step 1186: training loss: 1382.6957180026068\n",
      "Epoch 4 step 1187: training accuarcy: 0.7005\n",
      "Epoch 4 step 1187: training loss: 1383.6405831623645\n",
      "Epoch 4 step 1188: training accuarcy: 0.686\n",
      "Epoch 4 step 1188: training loss: 1382.387350241877\n",
      "Epoch 4 step 1189: training accuarcy: 0.7215\n",
      "Epoch 4 step 1189: training loss: 1383.2613643825373\n",
      "Epoch 4 step 1190: training accuarcy: 0.6880000000000001\n",
      "Epoch 4 step 1190: training loss: 1383.2254687242432\n",
      "Epoch 4 step 1191: training accuarcy: 0.679\n",
      "Epoch 4 step 1191: training loss: 1383.360595925628\n",
      "Epoch 4 step 1192: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1192: training loss: 1383.285334871781\n",
      "Epoch 4 step 1193: training accuarcy: 0.673\n",
      "Epoch 4 step 1193: training loss: 1383.106441806997\n",
      "Epoch 4 step 1194: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1194: training loss: 1383.2774738563232\n",
      "Epoch 4 step 1195: training accuarcy: 0.679\n",
      "Epoch 4 step 1195: training loss: 1382.58136792945\n",
      "Epoch 4 step 1196: training accuarcy: 0.7035\n",
      "Epoch 4 step 1196: training loss: 1383.2987242003933\n",
      "Epoch 4 step 1197: training accuarcy: 0.709\n",
      "Epoch 4 step 1197: training loss: 1383.1379917481154\n",
      "Epoch 4 step 1198: training accuarcy: 0.6955\n",
      "Epoch 4 step 1198: training loss: 1383.3042990339136\n",
      "Epoch 4 step 1199: training accuarcy: 0.6925\n",
      "Epoch 4 step 1199: training loss: 1382.2565722492604\n",
      "Epoch 4 step 1200: training accuarcy: 0.714\n",
      "Epoch 4 step 1200: training loss: 1382.9728649113279\n",
      "Epoch 4 step 1201: training accuarcy: 0.6965\n",
      "Epoch 4 step 1201: training loss: 1382.862171769622\n",
      "Epoch 4 step 1202: training accuarcy: 0.713\n",
      "Epoch 4 step 1202: training loss: 1383.039895341282\n",
      "Epoch 4 step 1203: training accuarcy: 0.6880000000000001\n",
      "Epoch 4 step 1203: training loss: 1382.9805544388646\n",
      "Epoch 4 step 1204: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1204: training loss: 1382.9902922272825\n",
      "Epoch 4 step 1205: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1205: training loss: 1383.3706692324818\n",
      "Epoch 4 step 1206: training accuarcy: 0.6795\n",
      "Epoch 4 step 1206: training loss: 1384.3803585513235\n",
      "Epoch 4 step 1207: training accuarcy: 0.681\n",
      "Epoch 4 step 1207: training loss: 1382.5484271422924\n",
      "Epoch 4 step 1208: training accuarcy: 0.712\n",
      "Epoch 4 step 1208: training loss: 1383.637342731154\n",
      "Epoch 4 step 1209: training accuarcy: 0.6915\n",
      "Epoch 4 step 1209: training loss: 1382.8084664792468\n",
      "Epoch 4 step 1210: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1210: training loss: 1382.7662609721028\n",
      "Epoch 4 step 1211: training accuarcy: 0.6995\n",
      "Epoch 4 step 1211: training loss: 1383.7082728073947\n",
      "Epoch 4 step 1212: training accuarcy: 0.687\n",
      "Epoch 4 step 1212: training loss: 1382.548547144816\n",
      "Epoch 4 step 1213: training accuarcy: 0.7175\n",
      "Epoch 4 step 1213: training loss: 1381.7730573126219\n",
      "Epoch 4 step 1214: training accuarcy: 0.7185\n",
      "Epoch 4 step 1214: training loss: 1383.2880419920336\n",
      "Epoch 4 step 1215: training accuarcy: 0.6920000000000001\n",
      "Epoch 4 step 1215: training loss: 1383.7824272518217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1216: training accuarcy: 0.6835\n",
      "Epoch 4 step 1216: training loss: 1383.483215047522\n",
      "Epoch 4 step 1217: training accuarcy: 0.6985\n",
      "Epoch 4 step 1217: training loss: 1382.7371787746265\n",
      "Epoch 4 step 1218: training accuarcy: 0.707\n",
      "Epoch 4 step 1218: training loss: 1382.0514965918985\n",
      "Epoch 4 step 1219: training accuarcy: 0.719\n",
      "Epoch 4 step 1219: training loss: 1383.3726297815233\n",
      "Epoch 4 step 1220: training accuarcy: 0.6875\n",
      "Epoch 4 step 1220: training loss: 1383.191348447189\n",
      "Epoch 4 step 1221: training accuarcy: 0.675\n",
      "Epoch 4 step 1221: training loss: 1383.1023609766974\n",
      "Epoch 4 step 1222: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1222: training loss: 1382.7285639705317\n",
      "Epoch 4 step 1223: training accuarcy: 0.7015\n",
      "Epoch 4 step 1223: training loss: 1383.240924689382\n",
      "Epoch 4 step 1224: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1224: training loss: 1383.5253930049614\n",
      "Epoch 4 step 1225: training accuarcy: 0.6900000000000001\n",
      "Epoch 4 step 1225: training loss: 1383.078131555962\n",
      "Epoch 4 step 1226: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1226: training loss: 1382.903104797231\n",
      "Epoch 4 step 1227: training accuarcy: 0.6845\n",
      "Epoch 4 step 1227: training loss: 1383.2249200721242\n",
      "Epoch 4 step 1228: training accuarcy: 0.6945\n",
      "Epoch 4 step 1228: training loss: 1382.639423153832\n",
      "Epoch 4 step 1229: training accuarcy: 0.6945\n",
      "Epoch 4 step 1229: training loss: 1383.2013877120223\n",
      "Epoch 4 step 1230: training accuarcy: 0.684\n",
      "Epoch 4 step 1230: training loss: 1382.0160558509278\n",
      "Epoch 4 step 1231: training accuarcy: 0.705\n",
      "Epoch 4 step 1231: training loss: 1383.1450713646118\n",
      "Epoch 4 step 1232: training accuarcy: 0.687\n",
      "Epoch 4 step 1232: training loss: 1382.3709752240195\n",
      "Epoch 4 step 1233: training accuarcy: 0.708\n",
      "Epoch 4 step 1233: training loss: 1383.5344655563133\n",
      "Epoch 4 step 1234: training accuarcy: 0.6895\n",
      "Epoch 4 step 1234: training loss: 1382.9135271093685\n",
      "Epoch 4 step 1235: training accuarcy: 0.6920000000000001\n",
      "Epoch 4 step 1235: training loss: 1383.7310160586692\n",
      "Epoch 4 step 1236: training accuarcy: 0.6880000000000001\n",
      "Epoch 4 step 1236: training loss: 1382.9304049517457\n",
      "Epoch 4 step 1237: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1237: training loss: 1382.4546021117308\n",
      "Epoch 4 step 1238: training accuarcy: 0.7025\n",
      "Epoch 4 step 1238: training loss: 1384.1885986284851\n",
      "Epoch 4 step 1239: training accuarcy: 0.6915\n",
      "Epoch 4 step 1239: training loss: 1382.5232319909828\n",
      "Epoch 4 step 1240: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 1240: training loss: 1382.9208688725532\n",
      "Epoch 4 step 1241: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1241: training loss: 1383.0456330671168\n",
      "Epoch 4 step 1242: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 1242: training loss: 1383.5073691346624\n",
      "Epoch 4 step 1243: training accuarcy: 0.6865\n",
      "Epoch 4 step 1243: training loss: 1383.3719600797667\n",
      "Epoch 4 step 1244: training accuarcy: 0.6915\n",
      "Epoch 4 step 1244: training loss: 1382.1993748899208\n",
      "Epoch 4 step 1245: training accuarcy: 0.6985\n",
      "Epoch 4 step 1245: training loss: 1383.0414548286785\n",
      "Epoch 4 step 1246: training accuarcy: 0.6845\n",
      "Epoch 4 step 1246: training loss: 1382.64053882276\n",
      "Epoch 4 step 1247: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1247: training loss: 1382.734276959376\n",
      "Epoch 4 step 1248: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1248: training loss: 1382.9725052114063\n",
      "Epoch 4 step 1249: training accuarcy: 0.6995\n",
      "Epoch 4 step 1249: training loss: 1382.7707260888571\n",
      "Epoch 4 step 1250: training accuarcy: 0.684\n",
      "Epoch 4 step 1250: training loss: 1383.1324741080184\n",
      "Epoch 4 step 1251: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 1251: training loss: 1383.6451094883907\n",
      "Epoch 4 step 1252: training accuarcy: 0.6725\n",
      "Epoch 4 step 1252: training loss: 1382.619583201027\n",
      "Epoch 4 step 1253: training accuarcy: 0.707\n",
      "Epoch 4 step 1253: training loss: 1383.002527596621\n",
      "Epoch 4 step 1254: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1254: training loss: 1383.0763631503337\n",
      "Epoch 4 step 1255: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1255: training loss: 1382.366517027083\n",
      "Epoch 4 step 1256: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1256: training loss: 1383.3768492482873\n",
      "Epoch 4 step 1257: training accuarcy: 0.6695\n",
      "Epoch 4 step 1257: training loss: 1382.1976996246253\n",
      "Epoch 4 step 1258: training accuarcy: 0.704\n",
      "Epoch 4 step 1258: training loss: 1382.8857894384423\n",
      "Epoch 4 step 1259: training accuarcy: 0.685\n",
      "Epoch 4 step 1259: training loss: 1382.5843961171615\n",
      "Epoch 4 step 1260: training accuarcy: 0.7155\n",
      "Epoch 4 step 1260: training loss: 1382.4666590982195\n",
      "Epoch 4 step 1261: training accuarcy: 0.6985\n",
      "Epoch 4 step 1261: training loss: 1383.439008627457\n",
      "Epoch 4 step 1262: training accuarcy: 0.6835\n",
      "Epoch 4 step 1262: training loss: 1382.5443607476054\n",
      "Epoch 4 step 1263: training accuarcy: 0.6885\n",
      "Epoch 4 step 1263: training loss: 1382.7242833632138\n",
      "Epoch 4 step 1264: training accuarcy: 0.7000000000000001\n",
      "Epoch 4 step 1264: training loss: 1383.6920797059975\n",
      "Epoch 4 step 1265: training accuarcy: 0.6955\n",
      "Epoch 4 step 1265: training loss: 1383.785185008885\n",
      "Epoch 4 step 1266: training accuarcy: 0.6985\n",
      "Epoch 4 step 1266: training loss: 1383.4577737107923\n",
      "Epoch 4 step 1267: training accuarcy: 0.6715\n",
      "Epoch 4 step 1267: training loss: 1383.0535363316837\n",
      "Epoch 4 step 1268: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1268: training loss: 1382.6724732887992\n",
      "Epoch 4 step 1269: training accuarcy: 0.6905\n",
      "Epoch 4 step 1269: training loss: 1383.1957247922237\n",
      "Epoch 4 step 1270: training accuarcy: 0.683\n",
      "Epoch 4 step 1270: training loss: 1382.9820034940255\n",
      "Epoch 4 step 1271: training accuarcy: 0.7025\n",
      "Epoch 4 step 1271: training loss: 1382.5330091101666\n",
      "Epoch 4 step 1272: training accuarcy: 0.7025\n",
      "Epoch 4 step 1272: training loss: 1382.4415062923088\n",
      "Epoch 4 step 1273: training accuarcy: 0.707\n",
      "Epoch 4 step 1273: training loss: 1383.1223894278278\n",
      "Epoch 4 step 1274: training accuarcy: 0.709\n",
      "Epoch 4 step 1274: training loss: 1382.5567710801604\n",
      "Epoch 4 step 1275: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1275: training loss: 1382.9891943780824\n",
      "Epoch 4 step 1276: training accuarcy: 0.6935\n",
      "Epoch 4 step 1276: training loss: 1383.5082465704743\n",
      "Epoch 4 step 1277: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1277: training loss: 1382.7033175169163\n",
      "Epoch 4 step 1278: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1278: training loss: 1382.8045154373222\n",
      "Epoch 4 step 1279: training accuarcy: 0.6895\n",
      "Epoch 4 step 1279: training loss: 1382.797237837548\n",
      "Epoch 4 step 1280: training accuarcy: 0.707\n",
      "Epoch 4 step 1280: training loss: 1382.2419900059372\n",
      "Epoch 4 step 1281: training accuarcy: 0.6975\n",
      "Epoch 4 step 1281: training loss: 1382.5943111496877\n",
      "Epoch 4 step 1282: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1282: training loss: 1382.9548021261676\n",
      "Epoch 4 step 1283: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1283: training loss: 1383.4098502227932\n",
      "Epoch 4 step 1284: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 1284: training loss: 1382.4361032680931\n",
      "Epoch 4 step 1285: training accuarcy: 0.6995\n",
      "Epoch 4 step 1285: training loss: 1383.4772911818009\n",
      "Epoch 4 step 1286: training accuarcy: 0.682\n",
      "Epoch 4 step 1286: training loss: 1382.9399505973633\n",
      "Epoch 4 step 1287: training accuarcy: 0.6985\n",
      "Epoch 4 step 1287: training loss: 1383.010245239245\n",
      "Epoch 4 step 1288: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1288: training loss: 1383.1264868238225\n",
      "Epoch 4 step 1289: training accuarcy: 0.6925\n",
      "Epoch 4 step 1289: training loss: 1383.9251883566847\n",
      "Epoch 4 step 1290: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1290: training loss: 1382.5461534362153\n",
      "Epoch 4 step 1291: training accuarcy: 0.6995\n",
      "Epoch 4 step 1291: training loss: 1382.9877781068346\n",
      "Epoch 4 step 1292: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1292: training loss: 1381.976488312431\n",
      "Epoch 4 step 1293: training accuarcy: 0.7175\n",
      "Epoch 4 step 1293: training loss: 1382.8368635175266\n",
      "Epoch 4 step 1294: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1294: training loss: 1382.7406727479195\n",
      "Epoch 4 step 1295: training accuarcy: 0.7000000000000001\n",
      "Epoch 4 step 1295: training loss: 1383.5374920454517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1296: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1296: training loss: 1382.954776112143\n",
      "Epoch 4 step 1297: training accuarcy: 0.6905\n",
      "Epoch 4 step 1297: training loss: 1381.7904610671806\n",
      "Epoch 4 step 1298: training accuarcy: 0.704\n",
      "Epoch 4 step 1298: training loss: 1382.2964007470393\n",
      "Epoch 4 step 1299: training accuarcy: 0.7035\n",
      "Epoch 4 step 1299: training loss: 1383.2287193772556\n",
      "Epoch 4 step 1300: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 1300: training loss: 1382.359810497525\n",
      "Epoch 4 step 1301: training accuarcy: 0.6965\n",
      "Epoch 4 step 1301: training loss: 1382.0573018849736\n",
      "Epoch 4 step 1302: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1302: training loss: 1381.7302304098957\n",
      "Epoch 4 step 1303: training accuarcy: 0.708\n",
      "Epoch 4 step 1303: training loss: 1383.1892122366105\n",
      "Epoch 4 step 1304: training accuarcy: 0.6945\n",
      "Epoch 4 step 1304: training loss: 1383.231770240961\n",
      "Epoch 4 step 1305: training accuarcy: 0.6935\n",
      "Epoch 4 step 1305: training loss: 1384.1789616830029\n",
      "Epoch 4 step 1306: training accuarcy: 0.6825\n",
      "Epoch 4 step 1306: training loss: 1382.7732682372634\n",
      "Epoch 4 step 1307: training accuarcy: 0.6985\n",
      "Epoch 4 step 1307: training loss: 1383.1657852293254\n",
      "Epoch 4 step 1308: training accuarcy: 0.687\n",
      "Epoch 4 step 1308: training loss: 1382.2752592432305\n",
      "Epoch 4 step 1309: training accuarcy: 0.707\n",
      "Epoch 4 step 1309: training loss: 1382.4317810412974\n",
      "Epoch 4 step 1310: training accuarcy: 0.6905\n",
      "Epoch 4 step 1310: training loss: 1383.1666692055894\n",
      "Epoch 4 step 1311: training accuarcy: 0.6945\n",
      "Epoch 4 step 1311: training loss: 1383.223505694004\n",
      "Epoch 4 step 1312: training accuarcy: 0.684\n",
      "Epoch 4 step 1312: training loss: 1383.3351823538308\n",
      "Epoch 4 step 1313: training accuarcy: 0.6995\n",
      "Epoch 4 step 1313: training loss: 1382.759571122067\n",
      "Epoch 4 step 1314: training accuarcy: 0.713\n",
      "Epoch 4 step 1314: training loss: 543.2561019648554\n",
      "Epoch 4 step 1315: training accuarcy: 0.7\n",
      "Epoch 4: train loss 1379.5701182512662, train accuarcy 0.7008461356163025\n",
      "Epoch 4: valid loss 1364.1860325600442, valid accuarcy 0.7196280360221863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                         | 5/8 [09:36<05:44, 114.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 1315: training loss: 1381.6597103995423\n",
      "Epoch 5 step 1316: training accuarcy: 0.7205\n",
      "Epoch 5 step 1316: training loss: 1381.1796691800578\n",
      "Epoch 5 step 1317: training accuarcy: 0.7225\n",
      "Epoch 5 step 1317: training loss: 1382.3465677090928\n",
      "Epoch 5 step 1318: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1318: training loss: 1382.0706805574287\n",
      "Epoch 5 step 1319: training accuarcy: 0.719\n",
      "Epoch 5 step 1319: training loss: 1382.5387034344465\n",
      "Epoch 5 step 1320: training accuarcy: 0.6995\n",
      "Epoch 5 step 1320: training loss: 1381.8413893505965\n",
      "Epoch 5 step 1321: training accuarcy: 0.7145\n",
      "Epoch 5 step 1321: training loss: 1381.695640758426\n",
      "Epoch 5 step 1322: training accuarcy: 0.717\n",
      "Epoch 5 step 1322: training loss: 1381.9190678818773\n",
      "Epoch 5 step 1323: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1323: training loss: 1382.6374538043465\n",
      "Epoch 5 step 1324: training accuarcy: 0.723\n",
      "Epoch 5 step 1324: training loss: 1382.1536809648735\n",
      "Epoch 5 step 1325: training accuarcy: 0.7225\n",
      "Epoch 5 step 1325: training loss: 1381.8817372991714\n",
      "Epoch 5 step 1326: training accuarcy: 0.719\n",
      "Epoch 5 step 1326: training loss: 1382.2860393224646\n",
      "Epoch 5 step 1327: training accuarcy: 0.7025\n",
      "Epoch 5 step 1327: training loss: 1381.8745733175635\n",
      "Epoch 5 step 1328: training accuarcy: 0.7265\n",
      "Epoch 5 step 1328: training loss: 1382.7224388067891\n",
      "Epoch 5 step 1329: training accuarcy: 0.7025\n",
      "Epoch 5 step 1329: training loss: 1382.1347905967787\n",
      "Epoch 5 step 1330: training accuarcy: 0.708\n",
      "Epoch 5 step 1330: training loss: 1382.253474966368\n",
      "Epoch 5 step 1331: training accuarcy: 0.713\n",
      "Epoch 5 step 1331: training loss: 1381.249245803005\n",
      "Epoch 5 step 1332: training accuarcy: 0.716\n",
      "Epoch 5 step 1332: training loss: 1381.487280691727\n",
      "Epoch 5 step 1333: training accuarcy: 0.716\n",
      "Epoch 5 step 1333: training loss: 1381.73963177192\n",
      "Epoch 5 step 1334: training accuarcy: 0.717\n",
      "Epoch 5 step 1334: training loss: 1381.9570930226591\n",
      "Epoch 5 step 1335: training accuarcy: 0.7075\n",
      "Epoch 5 step 1335: training loss: 1382.2140903891582\n",
      "Epoch 5 step 1336: training accuarcy: 0.715\n",
      "Epoch 5 step 1336: training loss: 1382.4259332290028\n",
      "Epoch 5 step 1337: training accuarcy: 0.711\n",
      "Epoch 5 step 1337: training loss: 1382.5845567680146\n",
      "Epoch 5 step 1338: training accuarcy: 0.7115\n",
      "Epoch 5 step 1338: training loss: 1382.8581831181036\n",
      "Epoch 5 step 1339: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1339: training loss: 1382.2926585428552\n",
      "Epoch 5 step 1340: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1340: training loss: 1382.824734795836\n",
      "Epoch 5 step 1341: training accuarcy: 0.6955\n",
      "Epoch 5 step 1341: training loss: 1382.9416246117723\n",
      "Epoch 5 step 1342: training accuarcy: 0.6985\n",
      "Epoch 5 step 1342: training loss: 1382.4213172175357\n",
      "Epoch 5 step 1343: training accuarcy: 0.7075\n",
      "Epoch 5 step 1343: training loss: 1382.0252418235632\n",
      "Epoch 5 step 1344: training accuarcy: 0.7125\n",
      "Epoch 5 step 1344: training loss: 1381.7659936305997\n",
      "Epoch 5 step 1345: training accuarcy: 0.72\n",
      "Epoch 5 step 1345: training loss: 1382.0295402497732\n",
      "Epoch 5 step 1346: training accuarcy: 0.714\n",
      "Epoch 5 step 1346: training loss: 1382.5303873032149\n",
      "Epoch 5 step 1347: training accuarcy: 0.6955\n",
      "Epoch 5 step 1347: training loss: 1382.3889044985672\n",
      "Epoch 5 step 1348: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1348: training loss: 1381.8744079093158\n",
      "Epoch 5 step 1349: training accuarcy: 0.7105\n",
      "Epoch 5 step 1349: training loss: 1382.2094665226716\n",
      "Epoch 5 step 1350: training accuarcy: 0.7135\n",
      "Epoch 5 step 1350: training loss: 1383.556213653238\n",
      "Epoch 5 step 1351: training accuarcy: 0.686\n",
      "Epoch 5 step 1351: training loss: 1382.71851565407\n",
      "Epoch 5 step 1352: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1352: training loss: 1383.0043958681458\n",
      "Epoch 5 step 1353: training accuarcy: 0.708\n",
      "Epoch 5 step 1353: training loss: 1382.4121459730395\n",
      "Epoch 5 step 1354: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1354: training loss: 1382.4848522453347\n",
      "Epoch 5 step 1355: training accuarcy: 0.687\n",
      "Epoch 5 step 1355: training loss: 1381.5056628843467\n",
      "Epoch 5 step 1356: training accuarcy: 0.7245\n",
      "Epoch 5 step 1356: training loss: 1382.419465584379\n",
      "Epoch 5 step 1357: training accuarcy: 0.7045\n",
      "Epoch 5 step 1357: training loss: 1383.0149799127637\n",
      "Epoch 5 step 1358: training accuarcy: 0.7000000000000001\n",
      "Epoch 5 step 1358: training loss: 1382.5557874200063\n",
      "Epoch 5 step 1359: training accuarcy: 0.7045\n",
      "Epoch 5 step 1359: training loss: 1382.8421768207352\n",
      "Epoch 5 step 1360: training accuarcy: 0.7025\n",
      "Epoch 5 step 1360: training loss: 1382.5909946889306\n",
      "Epoch 5 step 1361: training accuarcy: 0.7165\n",
      "Epoch 5 step 1361: training loss: 1383.1049659981386\n",
      "Epoch 5 step 1362: training accuarcy: 0.7085\n",
      "Epoch 5 step 1362: training loss: 1382.9520362698056\n",
      "Epoch 5 step 1363: training accuarcy: 0.7095\n",
      "Epoch 5 step 1363: training loss: 1382.8432196282904\n",
      "Epoch 5 step 1364: training accuarcy: 0.685\n",
      "Epoch 5 step 1364: training loss: 1381.300692024968\n",
      "Epoch 5 step 1365: training accuarcy: 0.711\n",
      "Epoch 5 step 1365: training loss: 1383.1429447485382\n",
      "Epoch 5 step 1366: training accuarcy: 0.6895\n",
      "Epoch 5 step 1366: training loss: 1383.1832878262032\n",
      "Epoch 5 step 1367: training accuarcy: 0.6935\n",
      "Epoch 5 step 1367: training loss: 1382.8388201219452\n",
      "Epoch 5 step 1368: training accuarcy: 0.712\n",
      "Epoch 5 step 1368: training loss: 1383.4386102983824\n",
      "Epoch 5 step 1369: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1369: training loss: 1381.9843563544296\n",
      "Epoch 5 step 1370: training accuarcy: 0.705\n",
      "Epoch 5 step 1370: training loss: 1381.5572925548445\n",
      "Epoch 5 step 1371: training accuarcy: 0.726\n",
      "Epoch 5 step 1371: training loss: 1382.2506128315727\n",
      "Epoch 5 step 1372: training accuarcy: 0.708\n",
      "Epoch 5 step 1372: training loss: 1381.5917407802276\n",
      "Epoch 5 step 1373: training accuarcy: 0.7145\n",
      "Epoch 5 step 1373: training loss: 1382.4798600707015\n",
      "Epoch 5 step 1374: training accuarcy: 0.71\n",
      "Epoch 5 step 1374: training loss: 1382.1562580559314\n",
      "Epoch 5 step 1375: training accuarcy: 0.7075\n",
      "Epoch 5 step 1375: training loss: 1383.3192899011012\n",
      "Epoch 5 step 1376: training accuarcy: 0.6865\n",
      "Epoch 5 step 1376: training loss: 1383.4795720889808\n",
      "Epoch 5 step 1377: training accuarcy: 0.71\n",
      "Epoch 5 step 1377: training loss: 1383.000303299001\n",
      "Epoch 5 step 1378: training accuarcy: 0.7125\n",
      "Epoch 5 step 1378: training loss: 1383.1211591635542\n",
      "Epoch 5 step 1379: training accuarcy: 0.6955\n",
      "Epoch 5 step 1379: training loss: 1382.2011058214166\n",
      "Epoch 5 step 1380: training accuarcy: 0.7115\n",
      "Epoch 5 step 1380: training loss: 1382.2176023692746\n",
      "Epoch 5 step 1381: training accuarcy: 0.7095\n",
      "Epoch 5 step 1381: training loss: 1383.2589750104698\n",
      "Epoch 5 step 1382: training accuarcy: 0.6895\n",
      "Epoch 5 step 1382: training loss: 1383.7350993811347\n",
      "Epoch 5 step 1383: training accuarcy: 0.678\n",
      "Epoch 5 step 1383: training loss: 1382.3239646908953\n",
      "Epoch 5 step 1384: training accuarcy: 0.7075\n",
      "Epoch 5 step 1384: training loss: 1382.2275328013438\n",
      "Epoch 5 step 1385: training accuarcy: 0.714\n",
      "Epoch 5 step 1385: training loss: 1383.1743350605934\n",
      "Epoch 5 step 1386: training accuarcy: 0.6795\n",
      "Epoch 5 step 1386: training loss: 1383.6526615197067\n",
      "Epoch 5 step 1387: training accuarcy: 0.6910000000000001\n",
      "Epoch 5 step 1387: training loss: 1382.3436347445295\n",
      "Epoch 5 step 1388: training accuarcy: 0.704\n",
      "Epoch 5 step 1388: training loss: 1383.6185755038307\n",
      "Epoch 5 step 1389: training accuarcy: 0.6875\n",
      "Epoch 5 step 1389: training loss: 1383.6215543100132\n",
      "Epoch 5 step 1390: training accuarcy: 0.6845\n",
      "Epoch 5 step 1390: training loss: 1382.1023716415343\n",
      "Epoch 5 step 1391: training accuarcy: 0.705\n",
      "Epoch 5 step 1391: training loss: 1382.7105770519877\n",
      "Epoch 5 step 1392: training accuarcy: 0.709\n",
      "Epoch 5 step 1392: training loss: 1382.1006828696013\n",
      "Epoch 5 step 1393: training accuarcy: 0.7195\n",
      "Epoch 5 step 1393: training loss: 1383.2513940834276\n",
      "Epoch 5 step 1394: training accuarcy: 0.6935\n",
      "Epoch 5 step 1394: training loss: 1383.7252520657532\n",
      "Epoch 5 step 1395: training accuarcy: 0.7035\n",
      "Epoch 5 step 1395: training loss: 1382.885206525949\n",
      "Epoch 5 step 1396: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1396: training loss: 1383.0029632959358\n",
      "Epoch 5 step 1397: training accuarcy: 0.6940000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1397: training loss: 1382.7945020044776\n",
      "Epoch 5 step 1398: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1398: training loss: 1383.6372796714247\n",
      "Epoch 5 step 1399: training accuarcy: 0.6785\n",
      "Epoch 5 step 1399: training loss: 1383.2184372498675\n",
      "Epoch 5 step 1400: training accuarcy: 0.686\n",
      "Epoch 5 step 1400: training loss: 1383.534652456642\n",
      "Epoch 5 step 1401: training accuarcy: 0.6875\n",
      "Epoch 5 step 1401: training loss: 1383.205173735724\n",
      "Epoch 5 step 1402: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1402: training loss: 1383.0808009903628\n",
      "Epoch 5 step 1403: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1403: training loss: 1382.8344308978928\n",
      "Epoch 5 step 1404: training accuarcy: 0.7095\n",
      "Epoch 5 step 1404: training loss: 1383.0246024732446\n",
      "Epoch 5 step 1405: training accuarcy: 0.6935\n",
      "Epoch 5 step 1405: training loss: 1382.5396910768975\n",
      "Epoch 5 step 1406: training accuarcy: 0.7045\n",
      "Epoch 5 step 1406: training loss: 1382.212720652467\n",
      "Epoch 5 step 1407: training accuarcy: 0.705\n",
      "Epoch 5 step 1407: training loss: 1382.721979134749\n",
      "Epoch 5 step 1408: training accuarcy: 0.7015\n",
      "Epoch 5 step 1408: training loss: 1382.8998951119042\n",
      "Epoch 5 step 1409: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1409: training loss: 1383.1125914565212\n",
      "Epoch 5 step 1410: training accuarcy: 0.6985\n",
      "Epoch 5 step 1410: training loss: 1383.465600494396\n",
      "Epoch 5 step 1411: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1411: training loss: 1383.0245341389634\n",
      "Epoch 5 step 1412: training accuarcy: 0.7135\n",
      "Epoch 5 step 1412: training loss: 1382.8554788161002\n",
      "Epoch 5 step 1413: training accuarcy: 0.6925\n",
      "Epoch 5 step 1413: training loss: 1382.5683235254824\n",
      "Epoch 5 step 1414: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1414: training loss: 1382.1261617040939\n",
      "Epoch 5 step 1415: training accuarcy: 0.7155\n",
      "Epoch 5 step 1415: training loss: 1382.391840511819\n",
      "Epoch 5 step 1416: training accuarcy: 0.6955\n",
      "Epoch 5 step 1416: training loss: 1381.9551999677212\n",
      "Epoch 5 step 1417: training accuarcy: 0.7035\n",
      "Epoch 5 step 1417: training loss: 1382.8828736593568\n",
      "Epoch 5 step 1418: training accuarcy: 0.6945\n",
      "Epoch 5 step 1418: training loss: 1383.245110823356\n",
      "Epoch 5 step 1419: training accuarcy: 0.6935\n",
      "Epoch 5 step 1419: training loss: 1382.6791424410894\n",
      "Epoch 5 step 1420: training accuarcy: 0.7075\n",
      "Epoch 5 step 1420: training loss: 1383.2936536826587\n",
      "Epoch 5 step 1421: training accuarcy: 0.684\n",
      "Epoch 5 step 1421: training loss: 1383.3991779795601\n",
      "Epoch 5 step 1422: training accuarcy: 0.6885\n",
      "Epoch 5 step 1422: training loss: 1382.9145064665436\n",
      "Epoch 5 step 1423: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1423: training loss: 1382.2736177462177\n",
      "Epoch 5 step 1424: training accuarcy: 0.6925\n",
      "Epoch 5 step 1424: training loss: 1383.3050786551778\n",
      "Epoch 5 step 1425: training accuarcy: 0.6765\n",
      "Epoch 5 step 1425: training loss: 1382.5097403183536\n",
      "Epoch 5 step 1426: training accuarcy: 0.7005\n",
      "Epoch 5 step 1426: training loss: 1383.2152065684788\n",
      "Epoch 5 step 1427: training accuarcy: 0.6935\n",
      "Epoch 5 step 1427: training loss: 1382.5928721084958\n",
      "Epoch 5 step 1428: training accuarcy: 0.719\n",
      "Epoch 5 step 1428: training loss: 1383.2211782567372\n",
      "Epoch 5 step 1429: training accuarcy: 0.681\n",
      "Epoch 5 step 1429: training loss: 1383.4303510858815\n",
      "Epoch 5 step 1430: training accuarcy: 0.687\n",
      "Epoch 5 step 1430: training loss: 1383.7637701905026\n",
      "Epoch 5 step 1431: training accuarcy: 0.6915\n",
      "Epoch 5 step 1431: training loss: 1383.0453579402742\n",
      "Epoch 5 step 1432: training accuarcy: 0.7045\n",
      "Epoch 5 step 1432: training loss: 1382.2803858242771\n",
      "Epoch 5 step 1433: training accuarcy: 0.7025\n",
      "Epoch 5 step 1433: training loss: 1382.618725915586\n",
      "Epoch 5 step 1434: training accuarcy: 0.704\n",
      "Epoch 5 step 1434: training loss: 1382.4766018245764\n",
      "Epoch 5 step 1435: training accuarcy: 0.7075\n",
      "Epoch 5 step 1435: training loss: 1383.5323588713097\n",
      "Epoch 5 step 1436: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1436: training loss: 1383.5866401950839\n",
      "Epoch 5 step 1437: training accuarcy: 0.7000000000000001\n",
      "Epoch 5 step 1437: training loss: 1384.0355919327828\n",
      "Epoch 5 step 1438: training accuarcy: 0.6805\n",
      "Epoch 5 step 1438: training loss: 1382.183380155952\n",
      "Epoch 5 step 1439: training accuarcy: 0.7075\n",
      "Epoch 5 step 1439: training loss: 1382.8258772245222\n",
      "Epoch 5 step 1440: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1440: training loss: 1383.260851474671\n",
      "Epoch 5 step 1441: training accuarcy: 0.682\n",
      "Epoch 5 step 1441: training loss: 1382.8359844641204\n",
      "Epoch 5 step 1442: training accuarcy: 0.6975\n",
      "Epoch 5 step 1442: training loss: 1383.0773130975217\n",
      "Epoch 5 step 1443: training accuarcy: 0.6865\n",
      "Epoch 5 step 1443: training loss: 1383.2435036690565\n",
      "Epoch 5 step 1444: training accuarcy: 0.6950000000000001\n",
      "Epoch 5 step 1444: training loss: 1383.0793145866194\n",
      "Epoch 5 step 1445: training accuarcy: 0.7015\n",
      "Epoch 5 step 1445: training loss: 1382.8430322857944\n",
      "Epoch 5 step 1446: training accuarcy: 0.6910000000000001\n",
      "Epoch 5 step 1446: training loss: 1382.3700004758794\n",
      "Epoch 5 step 1447: training accuarcy: 0.7085\n",
      "Epoch 5 step 1447: training loss: 1383.9720665827813\n",
      "Epoch 5 step 1448: training accuarcy: 0.6845\n",
      "Epoch 5 step 1448: training loss: 1382.7521030422115\n",
      "Epoch 5 step 1449: training accuarcy: 0.705\n",
      "Epoch 5 step 1449: training loss: 1382.7075554490734\n",
      "Epoch 5 step 1450: training accuarcy: 0.7055\n",
      "Epoch 5 step 1450: training loss: 1382.0528878725552\n",
      "Epoch 5 step 1451: training accuarcy: 0.7115\n",
      "Epoch 5 step 1451: training loss: 1382.8429530409105\n",
      "Epoch 5 step 1452: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1452: training loss: 1382.4683499473429\n",
      "Epoch 5 step 1453: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1453: training loss: 1383.37784191939\n",
      "Epoch 5 step 1454: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1454: training loss: 1382.3794049678215\n",
      "Epoch 5 step 1455: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1455: training loss: 1383.1897031348753\n",
      "Epoch 5 step 1456: training accuarcy: 0.6905\n",
      "Epoch 5 step 1456: training loss: 1382.6294119732638\n",
      "Epoch 5 step 1457: training accuarcy: 0.71\n",
      "Epoch 5 step 1457: training loss: 1383.084284188712\n",
      "Epoch 5 step 1458: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1458: training loss: 1382.3523619091595\n",
      "Epoch 5 step 1459: training accuarcy: 0.706\n",
      "Epoch 5 step 1459: training loss: 1382.6127070917626\n",
      "Epoch 5 step 1460: training accuarcy: 0.7125\n",
      "Epoch 5 step 1460: training loss: 1383.0084587583078\n",
      "Epoch 5 step 1461: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1461: training loss: 1382.970086267769\n",
      "Epoch 5 step 1462: training accuarcy: 0.6835\n",
      "Epoch 5 step 1462: training loss: 1382.7762226984064\n",
      "Epoch 5 step 1463: training accuarcy: 0.7035\n",
      "Epoch 5 step 1463: training loss: 1382.7579145227626\n",
      "Epoch 5 step 1464: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1464: training loss: 1382.8332749038805\n",
      "Epoch 5 step 1465: training accuarcy: 0.704\n",
      "Epoch 5 step 1465: training loss: 1382.6171044165703\n",
      "Epoch 5 step 1466: training accuarcy: 0.704\n",
      "Epoch 5 step 1466: training loss: 1384.217999640926\n",
      "Epoch 5 step 1467: training accuarcy: 0.6595\n",
      "Epoch 5 step 1467: training loss: 1383.3982845617184\n",
      "Epoch 5 step 1468: training accuarcy: 0.6855\n",
      "Epoch 5 step 1468: training loss: 1383.5560057185564\n",
      "Epoch 5 step 1469: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1469: training loss: 1383.0257623565676\n",
      "Epoch 5 step 1470: training accuarcy: 0.6865\n",
      "Epoch 5 step 1470: training loss: 1381.9135210995175\n",
      "Epoch 5 step 1471: training accuarcy: 0.6985\n",
      "Epoch 5 step 1471: training loss: 1383.5632976301488\n",
      "Epoch 5 step 1472: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1472: training loss: 1382.9709476555354\n",
      "Epoch 5 step 1473: training accuarcy: 0.6910000000000001\n",
      "Epoch 5 step 1473: training loss: 1381.9913573456151\n",
      "Epoch 5 step 1474: training accuarcy: 0.7165\n",
      "Epoch 5 step 1474: training loss: 1382.8550329081825\n",
      "Epoch 5 step 1475: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1475: training loss: 1381.6407063979602\n",
      "Epoch 5 step 1476: training accuarcy: 0.705\n",
      "Epoch 5 step 1476: training loss: 1382.9003049096114\n",
      "Epoch 5 step 1477: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1477: training loss: 1382.6179268589958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1478: training accuarcy: 0.7125\n",
      "Epoch 5 step 1478: training loss: 1383.2708742177786\n",
      "Epoch 5 step 1479: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1479: training loss: 1383.3188534063843\n",
      "Epoch 5 step 1480: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1480: training loss: 1383.3949534418657\n",
      "Epoch 5 step 1481: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1481: training loss: 1382.6985485762311\n",
      "Epoch 5 step 1482: training accuarcy: 0.7005\n",
      "Epoch 5 step 1482: training loss: 1382.354370106939\n",
      "Epoch 5 step 1483: training accuarcy: 0.709\n",
      "Epoch 5 step 1483: training loss: 1383.5581736544277\n",
      "Epoch 5 step 1484: training accuarcy: 0.6955\n",
      "Epoch 5 step 1484: training loss: 1383.1264590856863\n",
      "Epoch 5 step 1485: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1485: training loss: 1382.9231519942368\n",
      "Epoch 5 step 1486: training accuarcy: 0.7020000000000001\n",
      "Epoch 5 step 1486: training loss: 1382.964920259646\n",
      "Epoch 5 step 1487: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1487: training loss: 1383.0864173495686\n",
      "Epoch 5 step 1488: training accuarcy: 0.6925\n",
      "Epoch 5 step 1488: training loss: 1383.2810719442484\n",
      "Epoch 5 step 1489: training accuarcy: 0.6975\n",
      "Epoch 5 step 1489: training loss: 1383.1934836299354\n",
      "Epoch 5 step 1490: training accuarcy: 0.687\n",
      "Epoch 5 step 1490: training loss: 1382.5827013708863\n",
      "Epoch 5 step 1491: training accuarcy: 0.708\n",
      "Epoch 5 step 1491: training loss: 1383.025865234599\n",
      "Epoch 5 step 1492: training accuarcy: 0.7035\n",
      "Epoch 5 step 1492: training loss: 1383.4518593899456\n",
      "Epoch 5 step 1493: training accuarcy: 0.6875\n",
      "Epoch 5 step 1493: training loss: 1382.4840341290665\n",
      "Epoch 5 step 1494: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1494: training loss: 1384.7130994787533\n",
      "Epoch 5 step 1495: training accuarcy: 0.6565\n",
      "Epoch 5 step 1495: training loss: 1383.1975857580287\n",
      "Epoch 5 step 1496: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1496: training loss: 1382.9546449533732\n",
      "Epoch 5 step 1497: training accuarcy: 0.6915\n",
      "Epoch 5 step 1497: training loss: 1383.288507458321\n",
      "Epoch 5 step 1498: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1498: training loss: 1383.3140131220125\n",
      "Epoch 5 step 1499: training accuarcy: 0.6955\n",
      "Epoch 5 step 1499: training loss: 1383.253706814501\n",
      "Epoch 5 step 1500: training accuarcy: 0.6975\n",
      "Epoch 5 step 1500: training loss: 1383.7456068976696\n",
      "Epoch 5 step 1501: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1501: training loss: 1382.6447677162312\n",
      "Epoch 5 step 1502: training accuarcy: 0.7005\n",
      "Epoch 5 step 1502: training loss: 1383.2490439530106\n",
      "Epoch 5 step 1503: training accuarcy: 0.6955\n",
      "Epoch 5 step 1503: training loss: 1382.0014163690573\n",
      "Epoch 5 step 1504: training accuarcy: 0.707\n",
      "Epoch 5 step 1504: training loss: 1382.8321064887018\n",
      "Epoch 5 step 1505: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1505: training loss: 1382.3149640988397\n",
      "Epoch 5 step 1506: training accuarcy: 0.7075\n",
      "Epoch 5 step 1506: training loss: 1383.5377592522568\n",
      "Epoch 5 step 1507: training accuarcy: 0.7005\n",
      "Epoch 5 step 1507: training loss: 1383.4463120109265\n",
      "Epoch 5 step 1508: training accuarcy: 0.6815\n",
      "Epoch 5 step 1508: training loss: 1382.8198800451794\n",
      "Epoch 5 step 1509: training accuarcy: 0.7055\n",
      "Epoch 5 step 1509: training loss: 1383.3386317351597\n",
      "Epoch 5 step 1510: training accuarcy: 0.6945\n",
      "Epoch 5 step 1510: training loss: 1383.1773780867566\n",
      "Epoch 5 step 1511: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1511: training loss: 1383.520629344692\n",
      "Epoch 5 step 1512: training accuarcy: 0.6890000000000001\n",
      "Epoch 5 step 1512: training loss: 1383.6426903953757\n",
      "Epoch 5 step 1513: training accuarcy: 0.6795\n",
      "Epoch 5 step 1513: training loss: 1382.7178148807893\n",
      "Epoch 5 step 1514: training accuarcy: 0.7075\n",
      "Epoch 5 step 1514: training loss: 1383.4092660911795\n",
      "Epoch 5 step 1515: training accuarcy: 0.6935\n",
      "Epoch 5 step 1515: training loss: 1382.325296984092\n",
      "Epoch 5 step 1516: training accuarcy: 0.6950000000000001\n",
      "Epoch 5 step 1516: training loss: 1382.7749253115187\n",
      "Epoch 5 step 1517: training accuarcy: 0.7035\n",
      "Epoch 5 step 1517: training loss: 1382.729346421717\n",
      "Epoch 5 step 1518: training accuarcy: 0.6890000000000001\n",
      "Epoch 5 step 1518: training loss: 1383.8126396083949\n",
      "Epoch 5 step 1519: training accuarcy: 0.6845\n",
      "Epoch 5 step 1519: training loss: 1382.932482257943\n",
      "Epoch 5 step 1520: training accuarcy: 0.7065\n",
      "Epoch 5 step 1520: training loss: 1383.1055587909066\n",
      "Epoch 5 step 1521: training accuarcy: 0.6935\n",
      "Epoch 5 step 1521: training loss: 1383.538933146923\n",
      "Epoch 5 step 1522: training accuarcy: 0.6835\n",
      "Epoch 5 step 1522: training loss: 1383.2099077941537\n",
      "Epoch 5 step 1523: training accuarcy: 0.686\n",
      "Epoch 5 step 1523: training loss: 1381.9531374478565\n",
      "Epoch 5 step 1524: training accuarcy: 0.707\n",
      "Epoch 5 step 1524: training loss: 1382.8317028814304\n",
      "Epoch 5 step 1525: training accuarcy: 0.682\n",
      "Epoch 5 step 1525: training loss: 1383.042612244699\n",
      "Epoch 5 step 1526: training accuarcy: 0.6905\n",
      "Epoch 5 step 1526: training loss: 1383.069856818149\n",
      "Epoch 5 step 1527: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1527: training loss: 1383.107459268924\n",
      "Epoch 5 step 1528: training accuarcy: 0.709\n",
      "Epoch 5 step 1528: training loss: 1383.1428478545727\n",
      "Epoch 5 step 1529: training accuarcy: 0.7005\n",
      "Epoch 5 step 1529: training loss: 1383.3530360286993\n",
      "Epoch 5 step 1530: training accuarcy: 0.685\n",
      "Epoch 5 step 1530: training loss: 1382.0315337648753\n",
      "Epoch 5 step 1531: training accuarcy: 0.706\n",
      "Epoch 5 step 1531: training loss: 1382.4027390361098\n",
      "Epoch 5 step 1532: training accuarcy: 0.707\n",
      "Epoch 5 step 1532: training loss: 1381.8305283581283\n",
      "Epoch 5 step 1533: training accuarcy: 0.7225\n",
      "Epoch 5 step 1533: training loss: 1382.8000479395512\n",
      "Epoch 5 step 1534: training accuarcy: 0.6955\n",
      "Epoch 5 step 1534: training loss: 1382.9643781270659\n",
      "Epoch 5 step 1535: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1535: training loss: 1383.2880966882565\n",
      "Epoch 5 step 1536: training accuarcy: 0.7055\n",
      "Epoch 5 step 1536: training loss: 1383.4776281465836\n",
      "Epoch 5 step 1537: training accuarcy: 0.678\n",
      "Epoch 5 step 1537: training loss: 1383.5796071441468\n",
      "Epoch 5 step 1538: training accuarcy: 0.6950000000000001\n",
      "Epoch 5 step 1538: training loss: 1382.5798332702054\n",
      "Epoch 5 step 1539: training accuarcy: 0.7065\n",
      "Epoch 5 step 1539: training loss: 1382.7043810991372\n",
      "Epoch 5 step 1540: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1540: training loss: 1382.3700706150848\n",
      "Epoch 5 step 1541: training accuarcy: 0.6955\n",
      "Epoch 5 step 1541: training loss: 1383.683929891693\n",
      "Epoch 5 step 1542: training accuarcy: 0.6815\n",
      "Epoch 5 step 1542: training loss: 1382.7079826699887\n",
      "Epoch 5 step 1543: training accuarcy: 0.71\n",
      "Epoch 5 step 1543: training loss: 1382.9893212129864\n",
      "Epoch 5 step 1544: training accuarcy: 0.6965\n",
      "Epoch 5 step 1544: training loss: 1382.9265375400184\n",
      "Epoch 5 step 1545: training accuarcy: 0.6965\n",
      "Epoch 5 step 1545: training loss: 1382.7831152836457\n",
      "Epoch 5 step 1546: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1546: training loss: 1382.7809334972444\n",
      "Epoch 5 step 1547: training accuarcy: 0.7005\n",
      "Epoch 5 step 1547: training loss: 1382.246404423317\n",
      "Epoch 5 step 1548: training accuarcy: 0.7035\n",
      "Epoch 5 step 1548: training loss: 1383.0015121859794\n",
      "Epoch 5 step 1549: training accuarcy: 0.7000000000000001\n",
      "Epoch 5 step 1549: training loss: 1382.5198723735186\n",
      "Epoch 5 step 1550: training accuarcy: 0.7105\n",
      "Epoch 5 step 1550: training loss: 1383.2676077504107\n",
      "Epoch 5 step 1551: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1551: training loss: 1382.9745313570252\n",
      "Epoch 5 step 1552: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1552: training loss: 1383.8791006648369\n",
      "Epoch 5 step 1553: training accuarcy: 0.6955\n",
      "Epoch 5 step 1553: training loss: 1382.9997124498757\n",
      "Epoch 5 step 1554: training accuarcy: 0.6855\n",
      "Epoch 5 step 1554: training loss: 1381.9620230937187\n",
      "Epoch 5 step 1555: training accuarcy: 0.724\n",
      "Epoch 5 step 1555: training loss: 1382.7023806985737\n",
      "Epoch 5 step 1556: training accuarcy: 0.6965\n",
      "Epoch 5 step 1556: training loss: 1382.6387291296412\n",
      "Epoch 5 step 1557: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1557: training loss: 1382.461306468879\n",
      "Epoch 5 step 1558: training accuarcy: 0.6920000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1558: training loss: 1383.2924504620373\n",
      "Epoch 5 step 1559: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1559: training loss: 1383.056869185014\n",
      "Epoch 5 step 1560: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1560: training loss: 1382.5950885108387\n",
      "Epoch 5 step 1561: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1561: training loss: 1382.2730691926108\n",
      "Epoch 5 step 1562: training accuarcy: 0.719\n",
      "Epoch 5 step 1562: training loss: 1382.0466550970325\n",
      "Epoch 5 step 1563: training accuarcy: 0.712\n",
      "Epoch 5 step 1563: training loss: 1383.3765520712768\n",
      "Epoch 5 step 1564: training accuarcy: 0.687\n",
      "Epoch 5 step 1564: training loss: 1382.581710501581\n",
      "Epoch 5 step 1565: training accuarcy: 0.6935\n",
      "Epoch 5 step 1565: training loss: 1382.8522371871832\n",
      "Epoch 5 step 1566: training accuarcy: 0.6895\n",
      "Epoch 5 step 1566: training loss: 1383.047410983072\n",
      "Epoch 5 step 1567: training accuarcy: 0.704\n",
      "Epoch 5 step 1567: training loss: 1381.6463631597583\n",
      "Epoch 5 step 1568: training accuarcy: 0.7115\n",
      "Epoch 5 step 1568: training loss: 1382.908567413493\n",
      "Epoch 5 step 1569: training accuarcy: 0.682\n",
      "Epoch 5 step 1569: training loss: 1382.841189910051\n",
      "Epoch 5 step 1570: training accuarcy: 0.6955\n",
      "Epoch 5 step 1570: training loss: 1382.1172283924313\n",
      "Epoch 5 step 1571: training accuarcy: 0.712\n",
      "Epoch 5 step 1571: training loss: 1383.5620454512084\n",
      "Epoch 5 step 1572: training accuarcy: 0.6890000000000001\n",
      "Epoch 5 step 1572: training loss: 1383.321617580208\n",
      "Epoch 5 step 1573: training accuarcy: 0.684\n",
      "Epoch 5 step 1573: training loss: 1383.0816953136257\n",
      "Epoch 5 step 1574: training accuarcy: 0.6875\n",
      "Epoch 5 step 1574: training loss: 1382.047221690814\n",
      "Epoch 5 step 1575: training accuarcy: 0.7275\n",
      "Epoch 5 step 1575: training loss: 1383.091389444523\n",
      "Epoch 5 step 1576: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1576: training loss: 1382.8201685495899\n",
      "Epoch 5 step 1577: training accuarcy: 0.7045\n",
      "Epoch 5 step 1577: training loss: 543.6330911684875\n",
      "Epoch 5 step 1578: training accuarcy: 0.6538461538461539\n",
      "Epoch 5: train loss 1379.599970795543, train accuarcy 0.6992306709289551\n",
      "Epoch 5: valid loss 1364.4523918428176, valid accuarcy 0.7078027129173279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                      | 6/8 [11:30<03:49, 114.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 1578: training loss: 1382.0416807791064\n",
      "Epoch 6 step 1579: training accuarcy: 0.72\n",
      "Epoch 6 step 1579: training loss: 1381.6716833373819\n",
      "Epoch 6 step 1580: training accuarcy: 0.73\n",
      "Epoch 6 step 1580: training loss: 1382.5053743233568\n",
      "Epoch 6 step 1581: training accuarcy: 0.7255\n",
      "Epoch 6 step 1581: training loss: 1381.7095520397102\n",
      "Epoch 6 step 1582: training accuarcy: 0.711\n",
      "Epoch 6 step 1582: training loss: 1381.6096645210694\n",
      "Epoch 6 step 1583: training accuarcy: 0.713\n",
      "Epoch 6 step 1583: training loss: 1382.1205756783447\n",
      "Epoch 6 step 1584: training accuarcy: 0.7145\n",
      "Epoch 6 step 1584: training loss: 1381.3016516701566\n",
      "Epoch 6 step 1585: training accuarcy: 0.7285\n",
      "Epoch 6 step 1585: training loss: 1381.761175331917\n",
      "Epoch 6 step 1586: training accuarcy: 0.7125\n",
      "Epoch 6 step 1586: training loss: 1382.847758280477\n",
      "Epoch 6 step 1587: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1587: training loss: 1381.39671459321\n",
      "Epoch 6 step 1588: training accuarcy: 0.7085\n",
      "Epoch 6 step 1588: training loss: 1382.1661577970242\n",
      "Epoch 6 step 1589: training accuarcy: 0.7095\n",
      "Epoch 6 step 1589: training loss: 1382.1250524535267\n",
      "Epoch 6 step 1590: training accuarcy: 0.706\n",
      "Epoch 6 step 1590: training loss: 1381.990018128052\n",
      "Epoch 6 step 1591: training accuarcy: 0.704\n",
      "Epoch 6 step 1591: training loss: 1382.0141168732032\n",
      "Epoch 6 step 1592: training accuarcy: 0.709\n",
      "Epoch 6 step 1592: training loss: 1382.7225293202807\n",
      "Epoch 6 step 1593: training accuarcy: 0.704\n",
      "Epoch 6 step 1593: training loss: 1383.0033900682058\n",
      "Epoch 6 step 1594: training accuarcy: 0.6885\n",
      "Epoch 6 step 1594: training loss: 1381.7768585832814\n",
      "Epoch 6 step 1595: training accuarcy: 0.714\n",
      "Epoch 6 step 1595: training loss: 1381.7087124027025\n",
      "Epoch 6 step 1596: training accuarcy: 0.716\n",
      "Epoch 6 step 1596: training loss: 1382.035357692969\n",
      "Epoch 6 step 1597: training accuarcy: 0.71\n",
      "Epoch 6 step 1597: training loss: 1382.0126657355645\n",
      "Epoch 6 step 1598: training accuarcy: 0.711\n",
      "Epoch 6 step 1598: training loss: 1382.8418665688084\n",
      "Epoch 6 step 1599: training accuarcy: 0.7135\n",
      "Epoch 6 step 1599: training loss: 1382.213273988261\n",
      "Epoch 6 step 1600: training accuarcy: 0.6985\n",
      "Epoch 6 step 1600: training loss: 1381.7153276181252\n",
      "Epoch 6 step 1601: training accuarcy: 0.7245\n",
      "Epoch 6 step 1601: training loss: 1382.7820971028273\n",
      "Epoch 6 step 1602: training accuarcy: 0.7125\n",
      "Epoch 6 step 1602: training loss: 1383.172550464344\n",
      "Epoch 6 step 1603: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1603: training loss: 1382.4236442042654\n",
      "Epoch 6 step 1604: training accuarcy: 0.7125\n",
      "Epoch 6 step 1604: training loss: 1382.179684040187\n",
      "Epoch 6 step 1605: training accuarcy: 0.723\n",
      "Epoch 6 step 1605: training loss: 1381.849530506176\n",
      "Epoch 6 step 1606: training accuarcy: 0.7245\n",
      "Epoch 6 step 1606: training loss: 1382.7275270067614\n",
      "Epoch 6 step 1607: training accuarcy: 0.7045\n",
      "Epoch 6 step 1607: training loss: 1382.0977228875643\n",
      "Epoch 6 step 1608: training accuarcy: 0.7175\n",
      "Epoch 6 step 1608: training loss: 1382.4238707072618\n",
      "Epoch 6 step 1609: training accuarcy: 0.7145\n",
      "Epoch 6 step 1609: training loss: 1382.3277516987403\n",
      "Epoch 6 step 1610: training accuarcy: 0.7025\n",
      "Epoch 6 step 1610: training loss: 1382.1661300740832\n",
      "Epoch 6 step 1611: training accuarcy: 0.7165\n",
      "Epoch 6 step 1611: training loss: 1382.026859054917\n",
      "Epoch 6 step 1612: training accuarcy: 0.7085\n",
      "Epoch 6 step 1612: training loss: 1383.019002483793\n",
      "Epoch 6 step 1613: training accuarcy: 0.7055\n",
      "Epoch 6 step 1613: training loss: 1382.146538817704\n",
      "Epoch 6 step 1614: training accuarcy: 0.7135\n",
      "Epoch 6 step 1614: training loss: 1382.5782983931133\n",
      "Epoch 6 step 1615: training accuarcy: 0.717\n",
      "Epoch 6 step 1615: training loss: 1382.2836986561038\n",
      "Epoch 6 step 1616: training accuarcy: 0.705\n",
      "Epoch 6 step 1616: training loss: 1383.3962406228347\n",
      "Epoch 6 step 1617: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1617: training loss: 1382.504609975773\n",
      "Epoch 6 step 1618: training accuarcy: 0.6925\n",
      "Epoch 6 step 1618: training loss: 1381.865227643498\n",
      "Epoch 6 step 1619: training accuarcy: 0.7115\n",
      "Epoch 6 step 1619: training loss: 1382.313834152334\n",
      "Epoch 6 step 1620: training accuarcy: 0.707\n",
      "Epoch 6 step 1620: training loss: 1381.7400695835986\n",
      "Epoch 6 step 1621: training accuarcy: 0.709\n",
      "Epoch 6 step 1621: training loss: 1383.1759997367437\n",
      "Epoch 6 step 1622: training accuarcy: 0.6755\n",
      "Epoch 6 step 1622: training loss: 1383.250460646374\n",
      "Epoch 6 step 1623: training accuarcy: 0.6865\n",
      "Epoch 6 step 1623: training loss: 1382.2491521064196\n",
      "Epoch 6 step 1624: training accuarcy: 0.7185\n",
      "Epoch 6 step 1624: training loss: 1383.1272737561724\n",
      "Epoch 6 step 1625: training accuarcy: 0.6815\n",
      "Epoch 6 step 1625: training loss: 1382.5422362862423\n",
      "Epoch 6 step 1626: training accuarcy: 0.704\n",
      "Epoch 6 step 1626: training loss: 1382.620068281107\n",
      "Epoch 6 step 1627: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1627: training loss: 1382.7018439603992\n",
      "Epoch 6 step 1628: training accuarcy: 0.6905\n",
      "Epoch 6 step 1628: training loss: 1382.205487290884\n",
      "Epoch 6 step 1629: training accuarcy: 0.7025\n",
      "Epoch 6 step 1629: training loss: 1382.5876761268244\n",
      "Epoch 6 step 1630: training accuarcy: 0.6995\n",
      "Epoch 6 step 1630: training loss: 1382.8683178733418\n",
      "Epoch 6 step 1631: training accuarcy: 0.6985\n",
      "Epoch 6 step 1631: training loss: 1382.5842952064088\n",
      "Epoch 6 step 1632: training accuarcy: 0.6880000000000001\n",
      "Epoch 6 step 1632: training loss: 1382.9047367897465\n",
      "Epoch 6 step 1633: training accuarcy: 0.6885\n",
      "Epoch 6 step 1633: training loss: 1382.3762538818903\n",
      "Epoch 6 step 1634: training accuarcy: 0.6955\n",
      "Epoch 6 step 1634: training loss: 1382.0308415541153\n",
      "Epoch 6 step 1635: training accuarcy: 0.7115\n",
      "Epoch 6 step 1635: training loss: 1382.975005164438\n",
      "Epoch 6 step 1636: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1636: training loss: 1382.6588303172782\n",
      "Epoch 6 step 1637: training accuarcy: 0.704\n",
      "Epoch 6 step 1637: training loss: 1383.1041487076097\n",
      "Epoch 6 step 1638: training accuarcy: 0.6945\n",
      "Epoch 6 step 1638: training loss: 1382.5040304971553\n",
      "Epoch 6 step 1639: training accuarcy: 0.709\n",
      "Epoch 6 step 1639: training loss: 1383.9865064805792\n",
      "Epoch 6 step 1640: training accuarcy: 0.6665\n",
      "Epoch 6 step 1640: training loss: 1382.711891007179\n",
      "Epoch 6 step 1641: training accuarcy: 0.687\n",
      "Epoch 6 step 1641: training loss: 1383.1622515149586\n",
      "Epoch 6 step 1642: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1642: training loss: 1381.2886878514005\n",
      "Epoch 6 step 1643: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1643: training loss: 1383.105229631786\n",
      "Epoch 6 step 1644: training accuarcy: 0.6910000000000001\n",
      "Epoch 6 step 1644: training loss: 1381.3860141305004\n",
      "Epoch 6 step 1645: training accuarcy: 0.7205\n",
      "Epoch 6 step 1645: training loss: 1382.2840881957882\n",
      "Epoch 6 step 1646: training accuarcy: 0.7025\n",
      "Epoch 6 step 1646: training loss: 1383.248995824921\n",
      "Epoch 6 step 1647: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1647: training loss: 1382.3018878943162\n",
      "Epoch 6 step 1648: training accuarcy: 0.714\n",
      "Epoch 6 step 1648: training loss: 1383.3637611402296\n",
      "Epoch 6 step 1649: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1649: training loss: 1383.2204709246303\n",
      "Epoch 6 step 1650: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1650: training loss: 1382.4867462776751\n",
      "Epoch 6 step 1651: training accuarcy: 0.7065\n",
      "Epoch 6 step 1651: training loss: 1384.2174862888428\n",
      "Epoch 6 step 1652: training accuarcy: 0.687\n",
      "Epoch 6 step 1652: training loss: 1382.9255876981313\n",
      "Epoch 6 step 1653: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1653: training loss: 1382.7030609695055\n",
      "Epoch 6 step 1654: training accuarcy: 0.6925\n",
      "Epoch 6 step 1654: training loss: 1382.4846386264146\n",
      "Epoch 6 step 1655: training accuarcy: 0.6955\n",
      "Epoch 6 step 1655: training loss: 1382.5358543054774\n",
      "Epoch 6 step 1656: training accuarcy: 0.7145\n",
      "Epoch 6 step 1656: training loss: 1382.905073084094\n",
      "Epoch 6 step 1657: training accuarcy: 0.6910000000000001\n",
      "Epoch 6 step 1657: training loss: 1383.380153904365\n",
      "Epoch 6 step 1658: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1658: training loss: 1382.9852175678295\n",
      "Epoch 6 step 1659: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1659: training loss: 1382.2068273754173\n",
      "Epoch 6 step 1660: training accuarcy: 0.7005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1660: training loss: 1382.7895176006652\n",
      "Epoch 6 step 1661: training accuarcy: 0.7030000000000001\n",
      "Epoch 6 step 1661: training loss: 1383.6638815351837\n",
      "Epoch 6 step 1662: training accuarcy: 0.687\n",
      "Epoch 6 step 1662: training loss: 1382.6111148458929\n",
      "Epoch 6 step 1663: training accuarcy: 0.7085\n",
      "Epoch 6 step 1663: training loss: 1382.8756824735635\n",
      "Epoch 6 step 1664: training accuarcy: 0.7025\n",
      "Epoch 6 step 1664: training loss: 1383.0679182382307\n",
      "Epoch 6 step 1665: training accuarcy: 0.6985\n",
      "Epoch 6 step 1665: training loss: 1382.710619387123\n",
      "Epoch 6 step 1666: training accuarcy: 0.705\n",
      "Epoch 6 step 1666: training loss: 1383.2180684538018\n",
      "Epoch 6 step 1667: training accuarcy: 0.6935\n",
      "Epoch 6 step 1667: training loss: 1382.397610811431\n",
      "Epoch 6 step 1668: training accuarcy: 0.7105\n",
      "Epoch 6 step 1668: training loss: 1382.260484270875\n",
      "Epoch 6 step 1669: training accuarcy: 0.719\n",
      "Epoch 6 step 1669: training loss: 1383.0546820841757\n",
      "Epoch 6 step 1670: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1670: training loss: 1382.7696274133177\n",
      "Epoch 6 step 1671: training accuarcy: 0.6970000000000001\n",
      "Epoch 6 step 1671: training loss: 1382.0880068155789\n",
      "Epoch 6 step 1672: training accuarcy: 0.7025\n",
      "Epoch 6 step 1672: training loss: 1383.1036795328757\n",
      "Epoch 6 step 1673: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1673: training loss: 1381.9661128791888\n",
      "Epoch 6 step 1674: training accuarcy: 0.711\n",
      "Epoch 6 step 1674: training loss: 1383.6014895141443\n",
      "Epoch 6 step 1675: training accuarcy: 0.7045\n",
      "Epoch 6 step 1675: training loss: 1383.1912851069433\n",
      "Epoch 6 step 1676: training accuarcy: 0.706\n",
      "Epoch 6 step 1676: training loss: 1383.0068017965004\n",
      "Epoch 6 step 1677: training accuarcy: 0.6935\n",
      "Epoch 6 step 1677: training loss: 1382.4262037865499\n",
      "Epoch 6 step 1678: training accuarcy: 0.7030000000000001\n",
      "Epoch 6 step 1678: training loss: 1382.4441093247794\n",
      "Epoch 6 step 1679: training accuarcy: 0.6970000000000001\n",
      "Epoch 6 step 1679: training loss: 1383.524251847216\n",
      "Epoch 6 step 1680: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1680: training loss: 1382.554377189068\n",
      "Epoch 6 step 1681: training accuarcy: 0.6995\n",
      "Epoch 6 step 1681: training loss: 1383.5997636324703\n",
      "Epoch 6 step 1682: training accuarcy: 0.6915\n",
      "Epoch 6 step 1682: training loss: 1381.6283357893967\n",
      "Epoch 6 step 1683: training accuarcy: 0.7045\n",
      "Epoch 6 step 1683: training loss: 1382.8755442173767\n",
      "Epoch 6 step 1684: training accuarcy: 0.6975\n",
      "Epoch 6 step 1684: training loss: 1382.2275358060836\n",
      "Epoch 6 step 1685: training accuarcy: 0.7125\n",
      "Epoch 6 step 1685: training loss: 1383.7851801701772\n",
      "Epoch 6 step 1686: training accuarcy: 0.684\n",
      "Epoch 6 step 1686: training loss: 1383.1790726201673\n",
      "Epoch 6 step 1687: training accuarcy: 0.6935\n",
      "Epoch 6 step 1687: training loss: 1382.9985416255906\n",
      "Epoch 6 step 1688: training accuarcy: 0.6985\n",
      "Epoch 6 step 1688: training loss: 1382.7564951429708\n",
      "Epoch 6 step 1689: training accuarcy: 0.711\n",
      "Epoch 6 step 1689: training loss: 1382.9136792928275\n",
      "Epoch 6 step 1690: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1690: training loss: 1382.5972058088462\n",
      "Epoch 6 step 1691: training accuarcy: 0.7030000000000001\n",
      "Epoch 6 step 1691: training loss: 1383.116532294852\n",
      "Epoch 6 step 1692: training accuarcy: 0.6890000000000001\n",
      "Epoch 6 step 1692: training loss: 1383.2796483028583\n",
      "Epoch 6 step 1693: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1693: training loss: 1382.4697110496118\n",
      "Epoch 6 step 1694: training accuarcy: 0.706\n",
      "Epoch 6 step 1694: training loss: 1383.1693597255803\n",
      "Epoch 6 step 1695: training accuarcy: 0.6880000000000001\n",
      "Epoch 6 step 1695: training loss: 1383.3467022533496\n",
      "Epoch 6 step 1696: training accuarcy: 0.6985\n",
      "Epoch 6 step 1696: training loss: 1383.1103658120992\n",
      "Epoch 6 step 1697: training accuarcy: 0.6925\n",
      "Epoch 6 step 1697: training loss: 1382.7463679465945\n",
      "Epoch 6 step 1698: training accuarcy: 0.6965\n",
      "Epoch 6 step 1698: training loss: 1382.5860750875884\n",
      "Epoch 6 step 1699: training accuarcy: 0.7085\n",
      "Epoch 6 step 1699: training loss: 1383.1224885007643\n",
      "Epoch 6 step 1700: training accuarcy: 0.686\n",
      "Epoch 6 step 1700: training loss: 1382.0775811348644\n",
      "Epoch 6 step 1701: training accuarcy: 0.7145\n",
      "Epoch 6 step 1701: training loss: 1382.5532698382906\n",
      "Epoch 6 step 1702: training accuarcy: 0.6985\n",
      "Epoch 6 step 1702: training loss: 1383.7447958696864\n",
      "Epoch 6 step 1703: training accuarcy: 0.6925\n",
      "Epoch 6 step 1703: training loss: 1382.309846290984\n",
      "Epoch 6 step 1704: training accuarcy: 0.7015\n",
      "Epoch 6 step 1704: training loss: 1383.0842598058728\n",
      "Epoch 6 step 1705: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1705: training loss: 1383.4199579926844\n",
      "Epoch 6 step 1706: training accuarcy: 0.6885\n",
      "Epoch 6 step 1706: training loss: 1382.7119316530323\n",
      "Epoch 6 step 1707: training accuarcy: 0.713\n",
      "Epoch 6 step 1707: training loss: 1383.462858313843\n",
      "Epoch 6 step 1708: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1708: training loss: 1383.3178237762015\n",
      "Epoch 6 step 1709: training accuarcy: 0.6995\n",
      "Epoch 6 step 1709: training loss: 1383.1709312534401\n",
      "Epoch 6 step 1710: training accuarcy: 0.7035\n",
      "Epoch 6 step 1710: training loss: 1383.2932899672787\n",
      "Epoch 6 step 1711: training accuarcy: 0.6965\n",
      "Epoch 6 step 1711: training loss: 1384.0044569647628\n",
      "Epoch 6 step 1712: training accuarcy: 0.6855\n",
      "Epoch 6 step 1712: training loss: 1382.5304179997054\n",
      "Epoch 6 step 1713: training accuarcy: 0.7055\n",
      "Epoch 6 step 1713: training loss: 1381.86512287988\n",
      "Epoch 6 step 1714: training accuarcy: 0.719\n",
      "Epoch 6 step 1714: training loss: 1382.9456335601883\n",
      "Epoch 6 step 1715: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1715: training loss: 1382.6513914536927\n",
      "Epoch 6 step 1716: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1716: training loss: 1383.1624348490486\n",
      "Epoch 6 step 1717: training accuarcy: 0.687\n",
      "Epoch 6 step 1717: training loss: 1383.0525379378694\n",
      "Epoch 6 step 1718: training accuarcy: 0.6935\n",
      "Epoch 6 step 1718: training loss: 1383.4425933449606\n",
      "Epoch 6 step 1719: training accuarcy: 0.681\n",
      "Epoch 6 step 1719: training loss: 1382.9100711541191\n",
      "Epoch 6 step 1720: training accuarcy: 0.7055\n",
      "Epoch 6 step 1720: training loss: 1382.5430650599924\n",
      "Epoch 6 step 1721: training accuarcy: 0.709\n",
      "Epoch 6 step 1721: training loss: 1382.647173760294\n",
      "Epoch 6 step 1722: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1722: training loss: 1383.6630829274118\n",
      "Epoch 6 step 1723: training accuarcy: 0.6845\n",
      "Epoch 6 step 1723: training loss: 1383.4081712525133\n",
      "Epoch 6 step 1724: training accuarcy: 0.705\n",
      "Epoch 6 step 1724: training loss: 1382.3816904958371\n",
      "Epoch 6 step 1725: training accuarcy: 0.7215\n",
      "Epoch 6 step 1725: training loss: 1383.915031558926\n",
      "Epoch 6 step 1726: training accuarcy: 0.68\n",
      "Epoch 6 step 1726: training loss: 1382.38186022857\n",
      "Epoch 6 step 1727: training accuarcy: 0.716\n",
      "Epoch 6 step 1727: training loss: 1383.0062834331563\n",
      "Epoch 6 step 1728: training accuarcy: 0.6935\n",
      "Epoch 6 step 1728: training loss: 1382.494646462749\n",
      "Epoch 6 step 1729: training accuarcy: 0.7105\n",
      "Epoch 6 step 1729: training loss: 1382.8828338931708\n",
      "Epoch 6 step 1730: training accuarcy: 0.709\n",
      "Epoch 6 step 1730: training loss: 1383.3921999672214\n",
      "Epoch 6 step 1731: training accuarcy: 0.6935\n",
      "Epoch 6 step 1731: training loss: 1381.9501929459548\n",
      "Epoch 6 step 1732: training accuarcy: 0.7195\n",
      "Epoch 6 step 1732: training loss: 1383.6665167423657\n",
      "Epoch 6 step 1733: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1733: training loss: 1383.4868902115854\n",
      "Epoch 6 step 1734: training accuarcy: 0.6845\n",
      "Epoch 6 step 1734: training loss: 1382.3709885013814\n",
      "Epoch 6 step 1735: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1735: training loss: 1382.8775777594028\n",
      "Epoch 6 step 1736: training accuarcy: 0.6970000000000001\n",
      "Epoch 6 step 1736: training loss: 1384.0799916819171\n",
      "Epoch 6 step 1737: training accuarcy: 0.674\n",
      "Epoch 6 step 1737: training loss: 1382.0732061213814\n",
      "Epoch 6 step 1738: training accuarcy: 0.7065\n",
      "Epoch 6 step 1738: training loss: 1382.7011279438282\n",
      "Epoch 6 step 1739: training accuarcy: 0.71\n",
      "Epoch 6 step 1739: training loss: 1382.5334002328764\n",
      "Epoch 6 step 1740: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1740: training loss: 1382.621791535481\n",
      "Epoch 6 step 1741: training accuarcy: 0.6990000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1741: training loss: 1382.6703116336673\n",
      "Epoch 6 step 1742: training accuarcy: 0.7105\n",
      "Epoch 6 step 1742: training loss: 1382.980924562956\n",
      "Epoch 6 step 1743: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1743: training loss: 1382.8738779442108\n",
      "Epoch 6 step 1744: training accuarcy: 0.706\n",
      "Epoch 6 step 1744: training loss: 1382.8284206021071\n",
      "Epoch 6 step 1745: training accuarcy: 0.712\n",
      "Epoch 6 step 1745: training loss: 1382.8851707959138\n",
      "Epoch 6 step 1746: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1746: training loss: 1382.7754425562712\n",
      "Epoch 6 step 1747: training accuarcy: 0.707\n",
      "Epoch 6 step 1747: training loss: 1382.566759800367\n",
      "Epoch 6 step 1748: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1748: training loss: 1383.3280967414867\n",
      "Epoch 6 step 1749: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1749: training loss: 1383.262365839473\n",
      "Epoch 6 step 1750: training accuarcy: 0.6970000000000001\n",
      "Epoch 6 step 1750: training loss: 1383.5459307362794\n",
      "Epoch 6 step 1751: training accuarcy: 0.6910000000000001\n",
      "Epoch 6 step 1751: training loss: 1382.8224237569702\n",
      "Epoch 6 step 1752: training accuarcy: 0.7005\n",
      "Epoch 6 step 1752: training loss: 1382.3019281526206\n",
      "Epoch 6 step 1753: training accuarcy: 0.704\n",
      "Epoch 6 step 1753: training loss: 1382.6860756368642\n",
      "Epoch 6 step 1754: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1754: training loss: 1382.5679502338926\n",
      "Epoch 6 step 1755: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1755: training loss: 1383.5968721444167\n",
      "Epoch 6 step 1756: training accuarcy: 0.7005\n",
      "Epoch 6 step 1756: training loss: 1382.6429114991133\n",
      "Epoch 6 step 1757: training accuarcy: 0.6895\n",
      "Epoch 6 step 1757: training loss: 1382.991258572155\n",
      "Epoch 6 step 1758: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1758: training loss: 1382.5212377188604\n",
      "Epoch 6 step 1759: training accuarcy: 0.6975\n",
      "Epoch 6 step 1759: training loss: 1383.425565338495\n",
      "Epoch 6 step 1760: training accuarcy: 0.6915\n",
      "Epoch 6 step 1760: training loss: 1382.4883199711578\n",
      "Epoch 6 step 1761: training accuarcy: 0.6975\n",
      "Epoch 6 step 1761: training loss: 1382.423831145022\n",
      "Epoch 6 step 1762: training accuarcy: 0.711\n",
      "Epoch 6 step 1762: training loss: 1382.7608378534385\n",
      "Epoch 6 step 1763: training accuarcy: 0.6925\n",
      "Epoch 6 step 1763: training loss: 1382.591763966565\n",
      "Epoch 6 step 1764: training accuarcy: 0.7035\n",
      "Epoch 6 step 1764: training loss: 1382.7597585607157\n",
      "Epoch 6 step 1765: training accuarcy: 0.685\n",
      "Epoch 6 step 1765: training loss: 1382.602020012257\n",
      "Epoch 6 step 1766: training accuarcy: 0.6895\n",
      "Epoch 6 step 1766: training loss: 1383.1756284697206\n",
      "Epoch 6 step 1767: training accuarcy: 0.684\n",
      "Epoch 6 step 1767: training loss: 1382.8890117030603\n",
      "Epoch 6 step 1768: training accuarcy: 0.7045\n",
      "Epoch 6 step 1768: training loss: 1383.3520480587574\n",
      "Epoch 6 step 1769: training accuarcy: 0.6880000000000001\n",
      "Epoch 6 step 1769: training loss: 1382.8659720020448\n",
      "Epoch 6 step 1770: training accuarcy: 0.704\n",
      "Epoch 6 step 1770: training loss: 1383.5048603274722\n",
      "Epoch 6 step 1771: training accuarcy: 0.6965\n",
      "Epoch 6 step 1771: training loss: 1383.6662560393104\n",
      "Epoch 6 step 1772: training accuarcy: 0.6895\n",
      "Epoch 6 step 1772: training loss: 1382.3949924839126\n",
      "Epoch 6 step 1773: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1773: training loss: 1383.3033367314663\n",
      "Epoch 6 step 1774: training accuarcy: 0.6945\n",
      "Epoch 6 step 1774: training loss: 1382.9007918352133\n",
      "Epoch 6 step 1775: training accuarcy: 0.7045\n",
      "Epoch 6 step 1775: training loss: 1383.404259398247\n",
      "Epoch 6 step 1776: training accuarcy: 0.6935\n",
      "Epoch 6 step 1776: training loss: 1383.0976908238874\n",
      "Epoch 6 step 1777: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1777: training loss: 1382.8583600000804\n",
      "Epoch 6 step 1778: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1778: training loss: 1383.4687902459443\n",
      "Epoch 6 step 1779: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1779: training loss: 1382.8597397688088\n",
      "Epoch 6 step 1780: training accuarcy: 0.71\n",
      "Epoch 6 step 1780: training loss: 1382.5821705889336\n",
      "Epoch 6 step 1781: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1781: training loss: 1382.766074920318\n",
      "Epoch 6 step 1782: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1782: training loss: 1382.8156231300497\n",
      "Epoch 6 step 1783: training accuarcy: 0.6985\n",
      "Epoch 6 step 1783: training loss: 1383.0514630172115\n",
      "Epoch 6 step 1784: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1784: training loss: 1383.3085377017812\n",
      "Epoch 6 step 1785: training accuarcy: 0.6890000000000001\n",
      "Epoch 6 step 1785: training loss: 1383.6671032395045\n",
      "Epoch 6 step 1786: training accuarcy: 0.681\n",
      "Epoch 6 step 1786: training loss: 1382.7493498315434\n",
      "Epoch 6 step 1787: training accuarcy: 0.7045\n",
      "Epoch 6 step 1787: training loss: 1382.4736792342815\n",
      "Epoch 6 step 1788: training accuarcy: 0.7030000000000001\n",
      "Epoch 6 step 1788: training loss: 1382.484030543996\n",
      "Epoch 6 step 1789: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1789: training loss: 1383.140227921376\n",
      "Epoch 6 step 1790: training accuarcy: 0.678\n",
      "Epoch 6 step 1790: training loss: 1382.450979418211\n",
      "Epoch 6 step 1791: training accuarcy: 0.707\n",
      "Epoch 6 step 1791: training loss: 1382.8950192025945\n",
      "Epoch 6 step 1792: training accuarcy: 0.7105\n",
      "Epoch 6 step 1792: training loss: 1383.4580313788379\n",
      "Epoch 6 step 1793: training accuarcy: 0.6915\n",
      "Epoch 6 step 1793: training loss: 1382.7500309243705\n",
      "Epoch 6 step 1794: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1794: training loss: 1382.7763141596674\n",
      "Epoch 6 step 1795: training accuarcy: 0.7045\n",
      "Epoch 6 step 1795: training loss: 1382.5458551153151\n",
      "Epoch 6 step 1796: training accuarcy: 0.6945\n",
      "Epoch 6 step 1796: training loss: 1382.9253573974966\n",
      "Epoch 6 step 1797: training accuarcy: 0.7055\n",
      "Epoch 6 step 1797: training loss: 1382.8113408788129\n",
      "Epoch 6 step 1798: training accuarcy: 0.6970000000000001\n",
      "Epoch 6 step 1798: training loss: 1383.8075794155134\n",
      "Epoch 6 step 1799: training accuarcy: 0.6865\n",
      "Epoch 6 step 1799: training loss: 1382.9667148589544\n",
      "Epoch 6 step 1800: training accuarcy: 0.6905\n",
      "Epoch 6 step 1800: training loss: 1382.5251934840949\n",
      "Epoch 6 step 1801: training accuarcy: 0.708\n",
      "Epoch 6 step 1801: training loss: 1383.1879258472839\n",
      "Epoch 6 step 1802: training accuarcy: 0.7085\n",
      "Epoch 6 step 1802: training loss: 1382.1930216222647\n",
      "Epoch 6 step 1803: training accuarcy: 0.684\n",
      "Epoch 6 step 1803: training loss: 1383.08555744547\n",
      "Epoch 6 step 1804: training accuarcy: 0.682\n",
      "Epoch 6 step 1804: training loss: 1383.0116106726125\n",
      "Epoch 6 step 1805: training accuarcy: 0.6915\n",
      "Epoch 6 step 1805: training loss: 1382.3625515770545\n",
      "Epoch 6 step 1806: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1806: training loss: 1382.3361898728888\n",
      "Epoch 6 step 1807: training accuarcy: 0.7065\n",
      "Epoch 6 step 1807: training loss: 1383.6308483269502\n",
      "Epoch 6 step 1808: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1808: training loss: 1382.8717114514704\n",
      "Epoch 6 step 1809: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1809: training loss: 1382.346978367474\n",
      "Epoch 6 step 1810: training accuarcy: 0.7145\n",
      "Epoch 6 step 1810: training loss: 1382.7362511359452\n",
      "Epoch 6 step 1811: training accuarcy: 0.6995\n",
      "Epoch 6 step 1811: training loss: 1383.4344040306617\n",
      "Epoch 6 step 1812: training accuarcy: 0.6975\n",
      "Epoch 6 step 1812: training loss: 1383.1220585253402\n",
      "Epoch 6 step 1813: training accuarcy: 0.6855\n",
      "Epoch 6 step 1813: training loss: 1383.2636431556382\n",
      "Epoch 6 step 1814: training accuarcy: 0.676\n",
      "Epoch 6 step 1814: training loss: 1382.6668924305566\n",
      "Epoch 6 step 1815: training accuarcy: 0.6895\n",
      "Epoch 6 step 1815: training loss: 1383.5009853386866\n",
      "Epoch 6 step 1816: training accuarcy: 0.7015\n",
      "Epoch 6 step 1816: training loss: 1383.5059652192535\n",
      "Epoch 6 step 1817: training accuarcy: 0.6895\n",
      "Epoch 6 step 1817: training loss: 1382.8004276273718\n",
      "Epoch 6 step 1818: training accuarcy: 0.7015\n",
      "Epoch 6 step 1818: training loss: 1383.138525832348\n",
      "Epoch 6 step 1819: training accuarcy: 0.686\n",
      "Epoch 6 step 1819: training loss: 1382.4582306814189\n",
      "Epoch 6 step 1820: training accuarcy: 0.6975\n",
      "Epoch 6 step 1820: training loss: 1382.9662799410503\n",
      "Epoch 6 step 1821: training accuarcy: 0.705\n",
      "Epoch 6 step 1821: training loss: 1383.2054970001204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1822: training accuarcy: 0.7055\n",
      "Epoch 6 step 1822: training loss: 1382.0399816179906\n",
      "Epoch 6 step 1823: training accuarcy: 0.726\n",
      "Epoch 6 step 1823: training loss: 1383.5294796207504\n",
      "Epoch 6 step 1824: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1824: training loss: 1382.8850290705232\n",
      "Epoch 6 step 1825: training accuarcy: 0.6935\n",
      "Epoch 6 step 1825: training loss: 1382.66015932258\n",
      "Epoch 6 step 1826: training accuarcy: 0.714\n",
      "Epoch 6 step 1826: training loss: 1383.1702468926608\n",
      "Epoch 6 step 1827: training accuarcy: 0.6945\n",
      "Epoch 6 step 1827: training loss: 1383.3619071618496\n",
      "Epoch 6 step 1828: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1828: training loss: 1382.6547395403131\n",
      "Epoch 6 step 1829: training accuarcy: 0.715\n",
      "Epoch 6 step 1829: training loss: 1383.0993106167298\n",
      "Epoch 6 step 1830: training accuarcy: 0.7045\n",
      "Epoch 6 step 1830: training loss: 1383.0868859466816\n",
      "Epoch 6 step 1831: training accuarcy: 0.6985\n",
      "Epoch 6 step 1831: training loss: 1383.250118486259\n",
      "Epoch 6 step 1832: training accuarcy: 0.71\n",
      "Epoch 6 step 1832: training loss: 1382.274917399039\n",
      "Epoch 6 step 1833: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1833: training loss: 1383.1721848828906\n",
      "Epoch 6 step 1834: training accuarcy: 0.6865\n",
      "Epoch 6 step 1834: training loss: 1382.5522957412552\n",
      "Epoch 6 step 1835: training accuarcy: 0.72\n",
      "Epoch 6 step 1835: training loss: 1382.916356097073\n",
      "Epoch 6 step 1836: training accuarcy: 0.686\n",
      "Epoch 6 step 1836: training loss: 1383.3023134807086\n",
      "Epoch 6 step 1837: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1837: training loss: 1382.4642385927036\n",
      "Epoch 6 step 1838: training accuarcy: 0.706\n",
      "Epoch 6 step 1838: training loss: 1383.0484138196307\n",
      "Epoch 6 step 1839: training accuarcy: 0.6925\n",
      "Epoch 6 step 1839: training loss: 1382.3715915167352\n",
      "Epoch 6 step 1840: training accuarcy: 0.7045\n",
      "Epoch 6 step 1840: training loss: 543.6855387987008\n",
      "Epoch 6 step 1841: training accuarcy: 0.6653846153846154\n",
      "Epoch 6: train loss 1379.5864636053755, train accuarcy 0.7036606073379517\n",
      "Epoch 6: valid loss 1364.403408335705, valid accuarcy 0.7081059217453003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                   | 7/8 [13:32<01:56, 116.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 1841: training loss: 1382.7192701712631\n",
      "Epoch 7 step 1842: training accuarcy: 0.7020000000000001\n",
      "Epoch 7 step 1842: training loss: 1382.7357072268237\n",
      "Epoch 7 step 1843: training accuarcy: 0.7125\n",
      "Epoch 7 step 1843: training loss: 1382.7605816459884\n",
      "Epoch 7 step 1844: training accuarcy: 0.71\n",
      "Epoch 7 step 1844: training loss: 1381.7718497909045\n",
      "Epoch 7 step 1845: training accuarcy: 0.7125\n",
      "Epoch 7 step 1845: training loss: 1382.2716846890207\n",
      "Epoch 7 step 1846: training accuarcy: 0.7075\n",
      "Epoch 7 step 1846: training loss: 1382.7309138310045\n",
      "Epoch 7 step 1847: training accuarcy: 0.7145\n",
      "Epoch 7 step 1847: training loss: 1382.247658817208\n",
      "Epoch 7 step 1848: training accuarcy: 0.706\n",
      "Epoch 7 step 1848: training loss: 1381.892354247379\n",
      "Epoch 7 step 1849: training accuarcy: 0.7105\n",
      "Epoch 7 step 1849: training loss: 1381.353685694988\n",
      "Epoch 7 step 1850: training accuarcy: 0.7345\n",
      "Epoch 7 step 1850: training loss: 1382.7014828403112\n",
      "Epoch 7 step 1851: training accuarcy: 0.713\n",
      "Epoch 7 step 1851: training loss: 1382.098408128021\n",
      "Epoch 7 step 1852: training accuarcy: 0.7165\n",
      "Epoch 7 step 1852: training loss: 1382.752614440052\n",
      "Epoch 7 step 1853: training accuarcy: 0.7035\n",
      "Epoch 7 step 1853: training loss: 1382.4016837653635\n",
      "Epoch 7 step 1854: training accuarcy: 0.6995\n",
      "Epoch 7 step 1854: training loss: 1382.5543208521551\n",
      "Epoch 7 step 1855: training accuarcy: 0.71\n",
      "Epoch 7 step 1855: training loss: 1382.2427379163335\n",
      "Epoch 7 step 1856: training accuarcy: 0.714\n",
      "Epoch 7 step 1856: training loss: 1382.6570436273842\n",
      "Epoch 7 step 1857: training accuarcy: 0.722\n",
      "Epoch 7 step 1857: training loss: 1381.4840577430803\n",
      "Epoch 7 step 1858: training accuarcy: 0.7235\n",
      "Epoch 7 step 1858: training loss: 1382.8167025889472\n",
      "Epoch 7 step 1859: training accuarcy: 0.7020000000000001\n",
      "Epoch 7 step 1859: training loss: 1381.9747704115296\n",
      "Epoch 7 step 1860: training accuarcy: 0.721\n",
      "Epoch 7 step 1860: training loss: 1382.1393353732033\n",
      "Epoch 7 step 1861: training accuarcy: 0.712\n",
      "Epoch 7 step 1861: training loss: 1382.6840532865544\n",
      "Epoch 7 step 1862: training accuarcy: 0.7085\n",
      "Epoch 7 step 1862: training loss: 1381.9601514492354\n",
      "Epoch 7 step 1863: training accuarcy: 0.721\n",
      "Epoch 7 step 1863: training loss: 1382.941011124284\n",
      "Epoch 7 step 1864: training accuarcy: 0.7055\n",
      "Epoch 7 step 1864: training loss: 1382.1323572766596\n",
      "Epoch 7 step 1865: training accuarcy: 0.713\n",
      "Epoch 7 step 1865: training loss: 1382.097904452112\n",
      "Epoch 7 step 1866: training accuarcy: 0.709\n",
      "Epoch 7 step 1866: training loss: 1381.10015748631\n",
      "Epoch 7 step 1867: training accuarcy: 0.729\n",
      "Epoch 7 step 1867: training loss: 1381.5953352240658\n",
      "Epoch 7 step 1868: training accuarcy: 0.723\n",
      "Epoch 7 step 1868: training loss: 1381.7635852552196\n",
      "Epoch 7 step 1869: training accuarcy: 0.7085\n",
      "Epoch 7 step 1869: training loss: 1382.3076841051754\n",
      "Epoch 7 step 1870: training accuarcy: 0.7095\n",
      "Epoch 7 step 1870: training loss: 1382.542904967893\n",
      "Epoch 7 step 1871: training accuarcy: 0.7115\n",
      "Epoch 7 step 1871: training loss: 1381.6462614360585\n",
      "Epoch 7 step 1872: training accuarcy: 0.7225\n",
      "Epoch 7 step 1872: training loss: 1382.309001443112\n",
      "Epoch 7 step 1873: training accuarcy: 0.7165\n",
      "Epoch 7 step 1873: training loss: 1382.519211604209\n",
      "Epoch 7 step 1874: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1874: training loss: 1382.232761752518\n",
      "Epoch 7 step 1875: training accuarcy: 0.706\n",
      "Epoch 7 step 1875: training loss: 1382.0146805976046\n",
      "Epoch 7 step 1876: training accuarcy: 0.7045\n",
      "Epoch 7 step 1876: training loss: 1382.2504412329138\n",
      "Epoch 7 step 1877: training accuarcy: 0.7005\n",
      "Epoch 7 step 1877: training loss: 1381.6921047107319\n",
      "Epoch 7 step 1878: training accuarcy: 0.728\n",
      "Epoch 7 step 1878: training loss: 1382.6693321010248\n",
      "Epoch 7 step 1879: training accuarcy: 0.707\n",
      "Epoch 7 step 1879: training loss: 1382.9294929404164\n",
      "Epoch 7 step 1880: training accuarcy: 0.7085\n",
      "Epoch 7 step 1880: training loss: 1382.7143081840993\n",
      "Epoch 7 step 1881: training accuarcy: 0.7045\n",
      "Epoch 7 step 1881: training loss: 1383.0576542571612\n",
      "Epoch 7 step 1882: training accuarcy: 0.7015\n",
      "Epoch 7 step 1882: training loss: 1383.2412807039368\n",
      "Epoch 7 step 1883: training accuarcy: 0.708\n",
      "Epoch 7 step 1883: training loss: 1383.650105962232\n",
      "Epoch 7 step 1884: training accuarcy: 0.6965\n",
      "Epoch 7 step 1884: training loss: 1382.4607740645315\n",
      "Epoch 7 step 1885: training accuarcy: 0.683\n",
      "Epoch 7 step 1885: training loss: 1381.332534461174\n",
      "Epoch 7 step 1886: training accuarcy: 0.704\n",
      "Epoch 7 step 1886: training loss: 1382.0576697768345\n",
      "Epoch 7 step 1887: training accuarcy: 0.726\n",
      "Epoch 7 step 1887: training loss: 1382.5723198013295\n",
      "Epoch 7 step 1888: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 1888: training loss: 1383.0693046905226\n",
      "Epoch 7 step 1889: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 1889: training loss: 1383.0465979405926\n",
      "Epoch 7 step 1890: training accuarcy: 0.7005\n",
      "Epoch 7 step 1890: training loss: 1382.556403503346\n",
      "Epoch 7 step 1891: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 1891: training loss: 1382.2992459295695\n",
      "Epoch 7 step 1892: training accuarcy: 0.7065\n",
      "Epoch 7 step 1892: training loss: 1381.144622999294\n",
      "Epoch 7 step 1893: training accuarcy: 0.715\n",
      "Epoch 7 step 1893: training loss: 1382.3855597699405\n",
      "Epoch 7 step 1894: training accuarcy: 0.715\n",
      "Epoch 7 step 1894: training loss: 1383.1822022856772\n",
      "Epoch 7 step 1895: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 1895: training loss: 1382.8493025741811\n",
      "Epoch 7 step 1896: training accuarcy: 0.7075\n",
      "Epoch 7 step 1896: training loss: 1382.943401082432\n",
      "Epoch 7 step 1897: training accuarcy: 0.705\n",
      "Epoch 7 step 1897: training loss: 1382.860514862208\n",
      "Epoch 7 step 1898: training accuarcy: 0.7075\n",
      "Epoch 7 step 1898: training loss: 1383.651524135591\n",
      "Epoch 7 step 1899: training accuarcy: 0.6935\n",
      "Epoch 7 step 1899: training loss: 1381.9795137857207\n",
      "Epoch 7 step 1900: training accuarcy: 0.7175\n",
      "Epoch 7 step 1900: training loss: 1382.2656270116552\n",
      "Epoch 7 step 1901: training accuarcy: 0.7165\n",
      "Epoch 7 step 1901: training loss: 1382.1041059873216\n",
      "Epoch 7 step 1902: training accuarcy: 0.716\n",
      "Epoch 7 step 1902: training loss: 1383.7157085616734\n",
      "Epoch 7 step 1903: training accuarcy: 0.674\n",
      "Epoch 7 step 1903: training loss: 1383.2760119673005\n",
      "Epoch 7 step 1904: training accuarcy: 0.6945\n",
      "Epoch 7 step 1904: training loss: 1383.3021789791765\n",
      "Epoch 7 step 1905: training accuarcy: 0.7075\n",
      "Epoch 7 step 1905: training loss: 1382.3883192287597\n",
      "Epoch 7 step 1906: training accuarcy: 0.7035\n",
      "Epoch 7 step 1906: training loss: 1382.7148108705526\n",
      "Epoch 7 step 1907: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 1907: training loss: 1383.017366715218\n",
      "Epoch 7 step 1908: training accuarcy: 0.7075\n",
      "Epoch 7 step 1908: training loss: 1382.1583476444816\n",
      "Epoch 7 step 1909: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 1909: training loss: 1383.1462470013873\n",
      "Epoch 7 step 1910: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 1910: training loss: 1382.6180607928948\n",
      "Epoch 7 step 1911: training accuarcy: 0.7065\n",
      "Epoch 7 step 1911: training loss: 1382.7183890374852\n",
      "Epoch 7 step 1912: training accuarcy: 0.7035\n",
      "Epoch 7 step 1912: training loss: 1382.8833566474323\n",
      "Epoch 7 step 1913: training accuarcy: 0.7035\n",
      "Epoch 7 step 1913: training loss: 1383.2354193937372\n",
      "Epoch 7 step 1914: training accuarcy: 0.6985\n",
      "Epoch 7 step 1914: training loss: 1383.4114582599914\n",
      "Epoch 7 step 1915: training accuarcy: 0.6905\n",
      "Epoch 7 step 1915: training loss: 1383.1710582535584\n",
      "Epoch 7 step 1916: training accuarcy: 0.686\n",
      "Epoch 7 step 1916: training loss: 1383.0801729611321\n",
      "Epoch 7 step 1917: training accuarcy: 0.6955\n",
      "Epoch 7 step 1917: training loss: 1383.0334244171045\n",
      "Epoch 7 step 1918: training accuarcy: 0.6955\n",
      "Epoch 7 step 1918: training loss: 1383.0407050897747\n",
      "Epoch 7 step 1919: training accuarcy: 0.711\n",
      "Epoch 7 step 1919: training loss: 1382.7339441227696\n",
      "Epoch 7 step 1920: training accuarcy: 0.7105\n",
      "Epoch 7 step 1920: training loss: 1382.5155921483743\n",
      "Epoch 7 step 1921: training accuarcy: 0.715\n",
      "Epoch 7 step 1921: training loss: 1382.3235618564547\n",
      "Epoch 7 step 1922: training accuarcy: 0.706\n",
      "Epoch 7 step 1922: training loss: 1381.7691918829541\n",
      "Epoch 7 step 1923: training accuarcy: 0.708\n",
      "Epoch 7 step 1923: training loss: 1383.5492953274086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 1924: training accuarcy: 0.683\n",
      "Epoch 7 step 1924: training loss: 1382.9508611854221\n",
      "Epoch 7 step 1925: training accuarcy: 0.6910000000000001\n",
      "Epoch 7 step 1925: training loss: 1383.3192341240579\n",
      "Epoch 7 step 1926: training accuarcy: 0.6950000000000001\n",
      "Epoch 7 step 1926: training loss: 1383.2744461996829\n",
      "Epoch 7 step 1927: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 1927: training loss: 1381.864013229542\n",
      "Epoch 7 step 1928: training accuarcy: 0.7145\n",
      "Epoch 7 step 1928: training loss: 1382.5911852878191\n",
      "Epoch 7 step 1929: training accuarcy: 0.7005\n",
      "Epoch 7 step 1929: training loss: 1383.1237428876707\n",
      "Epoch 7 step 1930: training accuarcy: 0.704\n",
      "Epoch 7 step 1930: training loss: 1382.2987460881814\n",
      "Epoch 7 step 1931: training accuarcy: 0.6875\n",
      "Epoch 7 step 1931: training loss: 1383.2594127302755\n",
      "Epoch 7 step 1932: training accuarcy: 0.7025\n",
      "Epoch 7 step 1932: training loss: 1382.8302645735441\n",
      "Epoch 7 step 1933: training accuarcy: 0.7115\n",
      "Epoch 7 step 1933: training loss: 1383.1950657367008\n",
      "Epoch 7 step 1934: training accuarcy: 0.6885\n",
      "Epoch 7 step 1934: training loss: 1382.7964665024876\n",
      "Epoch 7 step 1935: training accuarcy: 0.6975\n",
      "Epoch 7 step 1935: training loss: 1383.8563288343546\n",
      "Epoch 7 step 1936: training accuarcy: 0.6845\n",
      "Epoch 7 step 1936: training loss: 1383.885194968871\n",
      "Epoch 7 step 1937: training accuarcy: 0.673\n",
      "Epoch 7 step 1937: training loss: 1382.6558748012164\n",
      "Epoch 7 step 1938: training accuarcy: 0.687\n",
      "Epoch 7 step 1938: training loss: 1383.153193271693\n",
      "Epoch 7 step 1939: training accuarcy: 0.7015\n",
      "Epoch 7 step 1939: training loss: 1381.4631495945832\n",
      "Epoch 7 step 1940: training accuarcy: 0.717\n",
      "Epoch 7 step 1940: training loss: 1383.3167545224967\n",
      "Epoch 7 step 1941: training accuarcy: 0.6960000000000001\n",
      "Epoch 7 step 1941: training loss: 1382.6168744041274\n",
      "Epoch 7 step 1942: training accuarcy: 0.6925\n",
      "Epoch 7 step 1942: training loss: 1382.891313979762\n",
      "Epoch 7 step 1943: training accuarcy: 0.721\n",
      "Epoch 7 step 1943: training loss: 1383.5118547875402\n",
      "Epoch 7 step 1944: training accuarcy: 0.6945\n",
      "Epoch 7 step 1944: training loss: 1383.1339968007678\n",
      "Epoch 7 step 1945: training accuarcy: 0.678\n",
      "Epoch 7 step 1945: training loss: 1383.3115386295453\n",
      "Epoch 7 step 1946: training accuarcy: 0.6875\n",
      "Epoch 7 step 1946: training loss: 1382.6429689088643\n",
      "Epoch 7 step 1947: training accuarcy: 0.704\n",
      "Epoch 7 step 1947: training loss: 1382.8766659975956\n",
      "Epoch 7 step 1948: training accuarcy: 0.6965\n",
      "Epoch 7 step 1948: training loss: 1381.8724806093387\n",
      "Epoch 7 step 1949: training accuarcy: 0.722\n",
      "Epoch 7 step 1949: training loss: 1383.1425747942553\n",
      "Epoch 7 step 1950: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 1950: training loss: 1383.6869004981859\n",
      "Epoch 7 step 1951: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1951: training loss: 1383.0118129104756\n",
      "Epoch 7 step 1952: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1952: training loss: 1383.3650042638692\n",
      "Epoch 7 step 1953: training accuarcy: 0.683\n",
      "Epoch 7 step 1953: training loss: 1382.8990171192377\n",
      "Epoch 7 step 1954: training accuarcy: 0.6945\n",
      "Epoch 7 step 1954: training loss: 1382.4962270701428\n",
      "Epoch 7 step 1955: training accuarcy: 0.7055\n",
      "Epoch 7 step 1955: training loss: 1383.1535469268445\n",
      "Epoch 7 step 1956: training accuarcy: 0.7025\n",
      "Epoch 7 step 1956: training loss: 1382.8822718730123\n",
      "Epoch 7 step 1957: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1957: training loss: 1382.3996362332095\n",
      "Epoch 7 step 1958: training accuarcy: 0.7015\n",
      "Epoch 7 step 1958: training loss: 1382.9067003409164\n",
      "Epoch 7 step 1959: training accuarcy: 0.6915\n",
      "Epoch 7 step 1959: training loss: 1382.8476006863548\n",
      "Epoch 7 step 1960: training accuarcy: 0.7085\n",
      "Epoch 7 step 1960: training loss: 1382.6271599247941\n",
      "Epoch 7 step 1961: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1961: training loss: 1382.7015071785947\n",
      "Epoch 7 step 1962: training accuarcy: 0.712\n",
      "Epoch 7 step 1962: training loss: 1382.4520845624465\n",
      "Epoch 7 step 1963: training accuarcy: 0.711\n",
      "Epoch 7 step 1963: training loss: 1383.4255298800715\n",
      "Epoch 7 step 1964: training accuarcy: 0.684\n",
      "Epoch 7 step 1964: training loss: 1382.6846712863826\n",
      "Epoch 7 step 1965: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 1965: training loss: 1381.8159933053091\n",
      "Epoch 7 step 1966: training accuarcy: 0.7005\n",
      "Epoch 7 step 1966: training loss: 1383.0054888670747\n",
      "Epoch 7 step 1967: training accuarcy: 0.7015\n",
      "Epoch 7 step 1967: training loss: 1382.9477914384859\n",
      "Epoch 7 step 1968: training accuarcy: 0.716\n",
      "Epoch 7 step 1968: training loss: 1382.6817883344277\n",
      "Epoch 7 step 1969: training accuarcy: 0.6965\n",
      "Epoch 7 step 1969: training loss: 1382.5765486535006\n",
      "Epoch 7 step 1970: training accuarcy: 0.7025\n",
      "Epoch 7 step 1970: training loss: 1381.9773472553252\n",
      "Epoch 7 step 1971: training accuarcy: 0.7235\n",
      "Epoch 7 step 1971: training loss: 1381.787104800863\n",
      "Epoch 7 step 1972: training accuarcy: 0.6985\n",
      "Epoch 7 step 1972: training loss: 1381.8167922075936\n",
      "Epoch 7 step 1973: training accuarcy: 0.6935\n",
      "Epoch 7 step 1973: training loss: 1383.895002853778\n",
      "Epoch 7 step 1974: training accuarcy: 0.687\n",
      "Epoch 7 step 1974: training loss: 1383.3329190168354\n",
      "Epoch 7 step 1975: training accuarcy: 0.6880000000000001\n",
      "Epoch 7 step 1975: training loss: 1383.7219575532783\n",
      "Epoch 7 step 1976: training accuarcy: 0.683\n",
      "Epoch 7 step 1976: training loss: 1382.903770990946\n",
      "Epoch 7 step 1977: training accuarcy: 0.707\n",
      "Epoch 7 step 1977: training loss: 1383.4815940595367\n",
      "Epoch 7 step 1978: training accuarcy: 0.6845\n",
      "Epoch 7 step 1978: training loss: 1382.6183893951873\n",
      "Epoch 7 step 1979: training accuarcy: 0.7005\n",
      "Epoch 7 step 1979: training loss: 1383.056773695856\n",
      "Epoch 7 step 1980: training accuarcy: 0.6955\n",
      "Epoch 7 step 1980: training loss: 1382.8862953354103\n",
      "Epoch 7 step 1981: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 1981: training loss: 1382.3232439937253\n",
      "Epoch 7 step 1982: training accuarcy: 0.714\n",
      "Epoch 7 step 1982: training loss: 1383.469986151354\n",
      "Epoch 7 step 1983: training accuarcy: 0.6835\n",
      "Epoch 7 step 1983: training loss: 1382.882790272153\n",
      "Epoch 7 step 1984: training accuarcy: 0.7025\n",
      "Epoch 7 step 1984: training loss: 1383.751515499349\n",
      "Epoch 7 step 1985: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 1985: training loss: 1382.6402440101522\n",
      "Epoch 7 step 1986: training accuarcy: 0.6935\n",
      "Epoch 7 step 1986: training loss: 1383.5190472549752\n",
      "Epoch 7 step 1987: training accuarcy: 0.6955\n",
      "Epoch 7 step 1987: training loss: 1383.5884439207096\n",
      "Epoch 7 step 1988: training accuarcy: 0.684\n",
      "Epoch 7 step 1988: training loss: 1382.8597110231317\n",
      "Epoch 7 step 1989: training accuarcy: 0.687\n",
      "Epoch 7 step 1989: training loss: 1383.2417328317558\n",
      "Epoch 7 step 1990: training accuarcy: 0.6885\n",
      "Epoch 7 step 1990: training loss: 1382.9446705622825\n",
      "Epoch 7 step 1991: training accuarcy: 0.6975\n",
      "Epoch 7 step 1991: training loss: 1382.4043244983638\n",
      "Epoch 7 step 1992: training accuarcy: 0.7105\n",
      "Epoch 7 step 1992: training loss: 1383.0660181831174\n",
      "Epoch 7 step 1993: training accuarcy: 0.705\n",
      "Epoch 7 step 1993: training loss: 1383.662092792661\n",
      "Epoch 7 step 1994: training accuarcy: 0.6995\n",
      "Epoch 7 step 1994: training loss: 1382.1040300935776\n",
      "Epoch 7 step 1995: training accuarcy: 0.707\n",
      "Epoch 7 step 1995: training loss: 1382.6384188108411\n",
      "Epoch 7 step 1996: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 1996: training loss: 1383.741375171864\n",
      "Epoch 7 step 1997: training accuarcy: 0.676\n",
      "Epoch 7 step 1997: training loss: 1382.8846921017596\n",
      "Epoch 7 step 1998: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 1998: training loss: 1383.1607390656704\n",
      "Epoch 7 step 1999: training accuarcy: 0.6975\n",
      "Epoch 7 step 1999: training loss: 1382.5351034714643\n",
      "Epoch 7 step 2000: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 2000: training loss: 1383.4365488630233\n",
      "Epoch 7 step 2001: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 2001: training loss: 1383.3247732915772\n",
      "Epoch 7 step 2002: training accuarcy: 0.6825\n",
      "Epoch 7 step 2002: training loss: 1382.7304247487566\n",
      "Epoch 7 step 2003: training accuarcy: 0.6915\n",
      "Epoch 7 step 2003: training loss: 1383.3366469789369\n",
      "Epoch 7 step 2004: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 2004: training loss: 1382.5234708283856\n",
      "Epoch 7 step 2005: training accuarcy: 0.7055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2005: training loss: 1383.5513405245874\n",
      "Epoch 7 step 2006: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 2006: training loss: 1382.574918088769\n",
      "Epoch 7 step 2007: training accuarcy: 0.7015\n",
      "Epoch 7 step 2007: training loss: 1383.1070883076884\n",
      "Epoch 7 step 2008: training accuarcy: 0.6845\n",
      "Epoch 7 step 2008: training loss: 1382.1518069010247\n",
      "Epoch 7 step 2009: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 2009: training loss: 1383.5022730360529\n",
      "Epoch 7 step 2010: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 2010: training loss: 1382.775110817881\n",
      "Epoch 7 step 2011: training accuarcy: 0.683\n",
      "Epoch 7 step 2011: training loss: 1382.6307904970793\n",
      "Epoch 7 step 2012: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 2012: training loss: 1383.2086614715845\n",
      "Epoch 7 step 2013: training accuarcy: 0.6895\n",
      "Epoch 7 step 2013: training loss: 1383.4964817616806\n",
      "Epoch 7 step 2014: training accuarcy: 0.6935\n",
      "Epoch 7 step 2014: training loss: 1383.2985106309293\n",
      "Epoch 7 step 2015: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 2015: training loss: 1382.491487216259\n",
      "Epoch 7 step 2016: training accuarcy: 0.7025\n",
      "Epoch 7 step 2016: training loss: 1381.932063784365\n",
      "Epoch 7 step 2017: training accuarcy: 0.711\n",
      "Epoch 7 step 2017: training loss: 1382.840840424245\n",
      "Epoch 7 step 2018: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 2018: training loss: 1383.1169314728063\n",
      "Epoch 7 step 2019: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 2019: training loss: 1382.2078466027071\n",
      "Epoch 7 step 2020: training accuarcy: 0.6875\n",
      "Epoch 7 step 2020: training loss: 1382.1648321145346\n",
      "Epoch 7 step 2021: training accuarcy: 0.7055\n",
      "Epoch 7 step 2021: training loss: 1383.362412579985\n",
      "Epoch 7 step 2022: training accuarcy: 0.6875\n",
      "Epoch 7 step 2022: training loss: 1382.8027701131448\n",
      "Epoch 7 step 2023: training accuarcy: 0.6880000000000001\n",
      "Epoch 7 step 2023: training loss: 1383.8175650561516\n",
      "Epoch 7 step 2024: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 2024: training loss: 1382.1701878290842\n",
      "Epoch 7 step 2025: training accuarcy: 0.6950000000000001\n",
      "Epoch 7 step 2025: training loss: 1382.4960411397217\n",
      "Epoch 7 step 2026: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2026: training loss: 1381.7868002314497\n",
      "Epoch 7 step 2027: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 2027: training loss: 1382.2254029393748\n",
      "Epoch 7 step 2028: training accuarcy: 0.7055\n",
      "Epoch 7 step 2028: training loss: 1382.6945740361914\n",
      "Epoch 7 step 2029: training accuarcy: 0.7075\n",
      "Epoch 7 step 2029: training loss: 1383.4305399144591\n",
      "Epoch 7 step 2030: training accuarcy: 0.6835\n",
      "Epoch 7 step 2030: training loss: 1382.5337670892025\n",
      "Epoch 7 step 2031: training accuarcy: 0.7115\n",
      "Epoch 7 step 2031: training loss: 1382.8458732668687\n",
      "Epoch 7 step 2032: training accuarcy: 0.6865\n",
      "Epoch 7 step 2032: training loss: 1383.1675729126168\n",
      "Epoch 7 step 2033: training accuarcy: 0.6945\n",
      "Epoch 7 step 2033: training loss: 1382.6517587596763\n",
      "Epoch 7 step 2034: training accuarcy: 0.7055\n",
      "Epoch 7 step 2034: training loss: 1383.3411851929723\n",
      "Epoch 7 step 2035: training accuarcy: 0.686\n",
      "Epoch 7 step 2035: training loss: 1382.798137991498\n",
      "Epoch 7 step 2036: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 2036: training loss: 1382.6794093874776\n",
      "Epoch 7 step 2037: training accuarcy: 0.7005\n",
      "Epoch 7 step 2037: training loss: 1383.4144148370417\n",
      "Epoch 7 step 2038: training accuarcy: 0.6905\n",
      "Epoch 7 step 2038: training loss: 1382.7414425037134\n",
      "Epoch 7 step 2039: training accuarcy: 0.7095\n",
      "Epoch 7 step 2039: training loss: 1382.5113658497876\n",
      "Epoch 7 step 2040: training accuarcy: 0.681\n",
      "Epoch 7 step 2040: training loss: 1382.5308938015985\n",
      "Epoch 7 step 2041: training accuarcy: 0.7055\n",
      "Epoch 7 step 2041: training loss: 1382.713380185336\n",
      "Epoch 7 step 2042: training accuarcy: 0.7055\n",
      "Epoch 7 step 2042: training loss: 1381.9277257419772\n",
      "Epoch 7 step 2043: training accuarcy: 0.7045\n",
      "Epoch 7 step 2043: training loss: 1383.1777103121212\n",
      "Epoch 7 step 2044: training accuarcy: 0.6945\n",
      "Epoch 7 step 2044: training loss: 1382.9253774178064\n",
      "Epoch 7 step 2045: training accuarcy: 0.6905\n",
      "Epoch 7 step 2045: training loss: 1383.7534178777\n",
      "Epoch 7 step 2046: training accuarcy: 0.6905\n",
      "Epoch 7 step 2046: training loss: 1382.9582087013694\n",
      "Epoch 7 step 2047: training accuarcy: 0.6950000000000001\n",
      "Epoch 7 step 2047: training loss: 1382.9760983246924\n",
      "Epoch 7 step 2048: training accuarcy: 0.6785\n",
      "Epoch 7 step 2048: training loss: 1382.6612863935413\n",
      "Epoch 7 step 2049: training accuarcy: 0.71\n",
      "Epoch 7 step 2049: training loss: 1382.6678329106066\n",
      "Epoch 7 step 2050: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 2050: training loss: 1383.2694327146226\n",
      "Epoch 7 step 2051: training accuarcy: 0.6945\n",
      "Epoch 7 step 2051: training loss: 1383.4872360538664\n",
      "Epoch 7 step 2052: training accuarcy: 0.684\n",
      "Epoch 7 step 2052: training loss: 1382.0539016517428\n",
      "Epoch 7 step 2053: training accuarcy: 0.713\n",
      "Epoch 7 step 2053: training loss: 1382.8862742495085\n",
      "Epoch 7 step 2054: training accuarcy: 0.6905\n",
      "Epoch 7 step 2054: training loss: 1383.7375271552946\n",
      "Epoch 7 step 2055: training accuarcy: 0.68\n",
      "Epoch 7 step 2055: training loss: 1382.5093261591787\n",
      "Epoch 7 step 2056: training accuarcy: 0.705\n",
      "Epoch 7 step 2056: training loss: 1382.5260113822978\n",
      "Epoch 7 step 2057: training accuarcy: 0.704\n",
      "Epoch 7 step 2057: training loss: 1382.804495589734\n",
      "Epoch 7 step 2058: training accuarcy: 0.7015\n",
      "Epoch 7 step 2058: training loss: 1382.7172651361318\n",
      "Epoch 7 step 2059: training accuarcy: 0.7075\n",
      "Epoch 7 step 2059: training loss: 1382.996958300341\n",
      "Epoch 7 step 2060: training accuarcy: 0.6915\n",
      "Epoch 7 step 2060: training loss: 1383.016107235973\n",
      "Epoch 7 step 2061: training accuarcy: 0.7085\n",
      "Epoch 7 step 2061: training loss: 1383.2741752166123\n",
      "Epoch 7 step 2062: training accuarcy: 0.6875\n",
      "Epoch 7 step 2062: training loss: 1382.875907663824\n",
      "Epoch 7 step 2063: training accuarcy: 0.7005\n",
      "Epoch 7 step 2063: training loss: 1382.4941693897638\n",
      "Epoch 7 step 2064: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 2064: training loss: 1382.367782713466\n",
      "Epoch 7 step 2065: training accuarcy: 0.705\n",
      "Epoch 7 step 2065: training loss: 1383.4736174353147\n",
      "Epoch 7 step 2066: training accuarcy: 0.682\n",
      "Epoch 7 step 2066: training loss: 1382.0040545208224\n",
      "Epoch 7 step 2067: training accuarcy: 0.6915\n",
      "Epoch 7 step 2067: training loss: 1382.7858458266987\n",
      "Epoch 7 step 2068: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 2068: training loss: 1382.3680847341814\n",
      "Epoch 7 step 2069: training accuarcy: 0.704\n",
      "Epoch 7 step 2069: training loss: 1382.1841743712805\n",
      "Epoch 7 step 2070: training accuarcy: 0.7085\n",
      "Epoch 7 step 2070: training loss: 1383.066097638068\n",
      "Epoch 7 step 2071: training accuarcy: 0.6825\n",
      "Epoch 7 step 2071: training loss: 1383.016447201361\n",
      "Epoch 7 step 2072: training accuarcy: 0.6960000000000001\n",
      "Epoch 7 step 2072: training loss: 1383.3072598241736\n",
      "Epoch 7 step 2073: training accuarcy: 0.6865\n",
      "Epoch 7 step 2073: training loss: 1382.5285315368503\n",
      "Epoch 7 step 2074: training accuarcy: 0.7035\n",
      "Epoch 7 step 2074: training loss: 1383.1552598438132\n",
      "Epoch 7 step 2075: training accuarcy: 0.6910000000000001\n",
      "Epoch 7 step 2075: training loss: 1382.401087165033\n",
      "Epoch 7 step 2076: training accuarcy: 0.6960000000000001\n",
      "Epoch 7 step 2076: training loss: 1383.3346457327832\n",
      "Epoch 7 step 2077: training accuarcy: 0.679\n",
      "Epoch 7 step 2077: training loss: 1382.9190639982126\n",
      "Epoch 7 step 2078: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 2078: training loss: 1383.5413095928466\n",
      "Epoch 7 step 2079: training accuarcy: 0.6975\n",
      "Epoch 7 step 2079: training loss: 1382.1491617203578\n",
      "Epoch 7 step 2080: training accuarcy: 0.713\n",
      "Epoch 7 step 2080: training loss: 1382.9535600302524\n",
      "Epoch 7 step 2081: training accuarcy: 0.678\n",
      "Epoch 7 step 2081: training loss: 1382.537846532073\n",
      "Epoch 7 step 2082: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 2082: training loss: 1383.6040856032039\n",
      "Epoch 7 step 2083: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 2083: training loss: 1382.6917431837965\n",
      "Epoch 7 step 2084: training accuarcy: 0.7055\n",
      "Epoch 7 step 2084: training loss: 1383.1897927122088\n",
      "Epoch 7 step 2085: training accuarcy: 0.6875\n",
      "Epoch 7 step 2085: training loss: 1383.0018518892887\n",
      "Epoch 7 step 2086: training accuarcy: 0.7020000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2086: training loss: 1382.7805021995116\n",
      "Epoch 7 step 2087: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 2087: training loss: 1382.8178241724138\n",
      "Epoch 7 step 2088: training accuarcy: 0.6880000000000001\n",
      "Epoch 7 step 2088: training loss: 1382.1992907678505\n",
      "Epoch 7 step 2089: training accuarcy: 0.708\n",
      "Epoch 7 step 2089: training loss: 1382.9550620688879\n",
      "Epoch 7 step 2090: training accuarcy: 0.6915\n",
      "Epoch 7 step 2090: training loss: 1382.8500328882108\n",
      "Epoch 7 step 2091: training accuarcy: 0.6965\n",
      "Epoch 7 step 2091: training loss: 1383.0008869831283\n",
      "Epoch 7 step 2092: training accuarcy: 0.7075\n",
      "Epoch 7 step 2092: training loss: 1382.381436803192\n",
      "Epoch 7 step 2093: training accuarcy: 0.708\n",
      "Epoch 7 step 2093: training loss: 1383.3437918158495\n",
      "Epoch 7 step 2094: training accuarcy: 0.6895\n",
      "Epoch 7 step 2094: training loss: 1382.887745604684\n",
      "Epoch 7 step 2095: training accuarcy: 0.7085\n",
      "Epoch 7 step 2095: training loss: 1383.0170676577698\n",
      "Epoch 7 step 2096: training accuarcy: 0.6925\n",
      "Epoch 7 step 2096: training loss: 1382.505608009794\n",
      "Epoch 7 step 2097: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 2097: training loss: 1382.6082644714886\n",
      "Epoch 7 step 2098: training accuarcy: 0.706\n",
      "Epoch 7 step 2098: training loss: 1382.9115610998074\n",
      "Epoch 7 step 2099: training accuarcy: 0.6985\n",
      "Epoch 7 step 2099: training loss: 1382.9939270252798\n",
      "Epoch 7 step 2100: training accuarcy: 0.71\n",
      "Epoch 7 step 2100: training loss: 1382.1899307987155\n",
      "Epoch 7 step 2101: training accuarcy: 0.7045\n",
      "Epoch 7 step 2101: training loss: 1381.924073804021\n",
      "Epoch 7 step 2102: training accuarcy: 0.715\n",
      "Epoch 7 step 2102: training loss: 1382.4054560644884\n",
      "Epoch 7 step 2103: training accuarcy: 0.6855\n",
      "Epoch 7 step 2103: training loss: 543.4274880962402\n",
      "Epoch 7 step 2104: training accuarcy: 0.691025641025641\n",
      "Epoch 7: train loss 1379.5620385800378, train accuarcy 0.702333390712738\n",
      "Epoch 7: valid loss 1364.0601346115177, valid accuarcy 0.7207398414611816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [15:31<00:00, 117.59s/it]\n"
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=8,\n",
    "                loss_callback=simple_loss_callback,\n",
    "                log_dir=get_log_dir('topcoder', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:22:03.752840Z",
     "start_time": "2019-09-25T13:22:03.743839Z"
    }
   },
   "outputs": [],
   "source": [
    "del hrm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PRME FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:22:25.507537Z",
     "start_time": "2019-09-25T13:22:25.255536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchPrmeFM()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_model = TorchPrmeFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "prme_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:22:29.635222Z",
     "start_time": "2019-09-25T13:22:29.631221Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(prme_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:22:34.027515Z",
     "start_time": "2019-09-25T13:22:33.983528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1f20046b400>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_learner = FMLearner(prme_model, adam_opt, schedular, db)\n",
    "prme_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:39:05.138320Z",
     "start_time": "2019-09-25T13:22:57.640696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 37514.48137421203\n",
      "Epoch 0 step 1: training accuarcy: 0.527\n",
      "Epoch 0 step 1: training loss: 36473.747391143785\n",
      "Epoch 0 step 2: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 2: training loss: 35454.346101311516\n",
      "Epoch 0 step 3: training accuarcy: 0.5015000000000001\n",
      "Epoch 0 step 3: training loss: 34454.33436229627\n",
      "Epoch 0 step 4: training accuarcy: 0.484\n",
      "Epoch 0 step 4: training loss: 33472.234940401846\n",
      "Epoch 0 step 5: training accuarcy: 0.5105000000000001\n",
      "Epoch 0 step 5: training loss: 32522.437981548668\n",
      "Epoch 0 step 6: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 6: training loss: 31582.993115826408\n",
      "Epoch 0 step 7: training accuarcy: 0.5135\n",
      "Epoch 0 step 7: training loss: 30670.02176044439\n",
      "Epoch 0 step 8: training accuarcy: 0.5155\n",
      "Epoch 0 step 8: training loss: 29788.87466102316\n",
      "Epoch 0 step 9: training accuarcy: 0.507\n",
      "Epoch 0 step 9: training loss: 28914.7417543135\n",
      "Epoch 0 step 10: training accuarcy: 0.5165\n",
      "Epoch 0 step 10: training loss: 28062.646270543024\n",
      "Epoch 0 step 11: training accuarcy: 0.522\n",
      "Epoch 0 step 11: training loss: 27239.842394624946\n",
      "Epoch 0 step 12: training accuarcy: 0.4915\n",
      "Epoch 0 step 12: training loss: 26424.54360148426\n",
      "Epoch 0 step 13: training accuarcy: 0.518\n",
      "Epoch 0 step 13: training loss: 25632.1226669722\n",
      "Epoch 0 step 14: training accuarcy: 0.541\n",
      "Epoch 0 step 14: training loss: 24868.546024228413\n",
      "Epoch 0 step 15: training accuarcy: 0.508\n",
      "Epoch 0 step 15: training loss: 24111.908461133906\n",
      "Epoch 0 step 16: training accuarcy: 0.535\n",
      "Epoch 0 step 16: training loss: 23383.531547629347\n",
      "Epoch 0 step 17: training accuarcy: 0.5245\n",
      "Epoch 0 step 17: training loss: 22672.647465358255\n",
      "Epoch 0 step 18: training accuarcy: 0.5305\n",
      "Epoch 0 step 18: training loss: 21974.311731257494\n",
      "Epoch 0 step 19: training accuarcy: 0.536\n",
      "Epoch 0 step 19: training loss: 21299.627592291352\n",
      "Epoch 0 step 20: training accuarcy: 0.5355\n",
      "Epoch 0 step 20: training loss: 20644.2404076104\n",
      "Epoch 0 step 21: training accuarcy: 0.534\n",
      "Epoch 0 step 21: training loss: 20002.970165223396\n",
      "Epoch 0 step 22: training accuarcy: 0.5535\n",
      "Epoch 0 step 22: training loss: 19381.89133334017\n",
      "Epoch 0 step 23: training accuarcy: 0.547\n",
      "Epoch 0 step 23: training loss: 18772.904280142484\n",
      "Epoch 0 step 24: training accuarcy: 0.5615\n",
      "Epoch 0 step 24: training loss: 18192.552851703236\n",
      "Epoch 0 step 25: training accuarcy: 0.546\n",
      "Epoch 0 step 25: training loss: 17623.0104615171\n",
      "Epoch 0 step 26: training accuarcy: 0.5305\n",
      "Epoch 0 step 26: training loss: 17064.722150059475\n",
      "Epoch 0 step 27: training accuarcy: 0.5515\n",
      "Epoch 0 step 27: training loss: 16527.414671065348\n",
      "Epoch 0 step 28: training accuarcy: 0.546\n",
      "Epoch 0 step 28: training loss: 16005.169937778894\n",
      "Epoch 0 step 29: training accuarcy: 0.552\n",
      "Epoch 0 step 29: training loss: 15501.943430343252\n",
      "Epoch 0 step 30: training accuarcy: 0.5505\n",
      "Epoch 0 step 30: training loss: 15009.480675598992\n",
      "Epoch 0 step 31: training accuarcy: 0.5485\n",
      "Epoch 0 step 31: training loss: 14532.774574120012\n",
      "Epoch 0 step 32: training accuarcy: 0.5505\n",
      "Epoch 0 step 32: training loss: 14065.008685162817\n",
      "Epoch 0 step 33: training accuarcy: 0.5655\n",
      "Epoch 0 step 33: training loss: 13614.968682862615\n",
      "Epoch 0 step 34: training accuarcy: 0.5775\n",
      "Epoch 0 step 34: training loss: 13187.78595862876\n",
      "Epoch 0 step 35: training accuarcy: 0.5615\n",
      "Epoch 0 step 35: training loss: 12764.746933347671\n",
      "Epoch 0 step 36: training accuarcy: 0.558\n",
      "Epoch 0 step 36: training loss: 12358.952760678196\n",
      "Epoch 0 step 37: training accuarcy: 0.5535\n",
      "Epoch 0 step 37: training loss: 11957.139184556752\n",
      "Epoch 0 step 38: training accuarcy: 0.5785\n",
      "Epoch 0 step 38: training loss: 11578.10089165549\n",
      "Epoch 0 step 39: training accuarcy: 0.5645\n",
      "Epoch 0 step 39: training loss: 11210.170839158214\n",
      "Epoch 0 step 40: training accuarcy: 0.5725\n",
      "Epoch 0 step 40: training loss: 10848.559030422053\n",
      "Epoch 0 step 41: training accuarcy: 0.577\n",
      "Epoch 0 step 41: training loss: 10506.835521014327\n",
      "Epoch 0 step 42: training accuarcy: 0.5625\n",
      "Epoch 0 step 42: training loss: 10166.629123765779\n",
      "Epoch 0 step 43: training accuarcy: 0.5775\n",
      "Epoch 0 step 43: training loss: 9838.61839602091\n",
      "Epoch 0 step 44: training accuarcy: 0.6035\n",
      "Epoch 0 step 44: training loss: 9530.07888466716\n",
      "Epoch 0 step 45: training accuarcy: 0.5655\n",
      "Epoch 0 step 45: training loss: 9225.996267689341\n",
      "Epoch 0 step 46: training accuarcy: 0.5730000000000001\n",
      "Epoch 0 step 46: training loss: 8934.510124479884\n",
      "Epoch 0 step 47: training accuarcy: 0.555\n",
      "Epoch 0 step 47: training loss: 8649.634706762448\n",
      "Epoch 0 step 48: training accuarcy: 0.5750000000000001\n",
      "Epoch 0 step 48: training loss: 8376.078467023763\n",
      "Epoch 0 step 49: training accuarcy: 0.5750000000000001\n",
      "Epoch 0 step 49: training loss: 8110.682623572238\n",
      "Epoch 0 step 50: training accuarcy: 0.585\n",
      "Epoch 0 step 50: training loss: 7855.969278212591\n",
      "Epoch 0 step 51: training accuarcy: 0.58\n",
      "Epoch 0 step 51: training loss: 7610.1542268712565\n",
      "Epoch 0 step 52: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 52: training loss: 7371.133513344363\n",
      "Epoch 0 step 53: training accuarcy: 0.5635\n",
      "Epoch 0 step 53: training loss: 7142.285526240432\n",
      "Epoch 0 step 54: training accuarcy: 0.5655\n",
      "Epoch 0 step 54: training loss: 6914.717675149546\n",
      "Epoch 0 step 55: training accuarcy: 0.591\n",
      "Epoch 0 step 55: training loss: 6704.136983746221\n",
      "Epoch 0 step 56: training accuarcy: 0.5685\n",
      "Epoch 0 step 56: training loss: 6493.817573727631\n",
      "Epoch 0 step 57: training accuarcy: 0.5925\n",
      "Epoch 0 step 57: training loss: 6297.564206948133\n",
      "Epoch 0 step 58: training accuarcy: 0.5845\n",
      "Epoch 0 step 58: training loss: 6104.3439665957885\n",
      "Epoch 0 step 59: training accuarcy: 0.581\n",
      "Epoch 0 step 59: training loss: 5918.7684579295765\n",
      "Epoch 0 step 60: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 60: training loss: 5740.835450994186\n",
      "Epoch 0 step 61: training accuarcy: 0.5720000000000001\n",
      "Epoch 0 step 61: training loss: 5565.814478416676\n",
      "Epoch 0 step 62: training accuarcy: 0.578\n",
      "Epoch 0 step 62: training loss: 5398.3378888812895\n",
      "Epoch 0 step 63: training accuarcy: 0.5835\n",
      "Epoch 0 step 63: training loss: 5238.789459345968\n",
      "Epoch 0 step 64: training accuarcy: 0.5750000000000001\n",
      "Epoch 0 step 64: training loss: 5084.913463692763\n",
      "Epoch 0 step 65: training accuarcy: 0.5755\n",
      "Epoch 0 step 65: training loss: 4937.1446454835195\n",
      "Epoch 0 step 66: training accuarcy: 0.5635\n",
      "Epoch 0 step 66: training loss: 4791.225020637687\n",
      "Epoch 0 step 67: training accuarcy: 0.587\n",
      "Epoch 0 step 67: training loss: 4651.800419416375\n",
      "Epoch 0 step 68: training accuarcy: 0.606\n",
      "Epoch 0 step 68: training loss: 4520.711793959861\n",
      "Epoch 0 step 69: training accuarcy: 0.5865\n",
      "Epoch 0 step 69: training loss: 4390.584605515061\n",
      "Epoch 0 step 70: training accuarcy: 0.591\n",
      "Epoch 0 step 70: training loss: 4268.101368139625\n",
      "Epoch 0 step 71: training accuarcy: 0.5815\n",
      "Epoch 0 step 71: training loss: 4150.128345789615\n",
      "Epoch 0 step 72: training accuarcy: 0.5875\n",
      "Epoch 0 step 72: training loss: 4035.5357543067043\n",
      "Epoch 0 step 73: training accuarcy: 0.5915\n",
      "Epoch 0 step 73: training loss: 3926.8104214191408\n",
      "Epoch 0 step 74: training accuarcy: 0.577\n",
      "Epoch 0 step 74: training loss: 3818.1946579486416\n",
      "Epoch 0 step 75: training accuarcy: 0.604\n",
      "Epoch 0 step 75: training loss: 3719.2000771572893\n",
      "Epoch 0 step 76: training accuarcy: 0.577\n",
      "Epoch 0 step 76: training loss: 3620.328605774228\n",
      "Epoch 0 step 77: training accuarcy: 0.59\n",
      "Epoch 0 step 77: training loss: 3526.1575841736003\n",
      "Epoch 0 step 78: training accuarcy: 0.58\n",
      "Epoch 0 step 78: training loss: 3435.281575258935\n",
      "Epoch 0 step 79: training accuarcy: 0.5855\n",
      "Epoch 0 step 79: training loss: 3346.194536932623\n",
      "Epoch 0 step 80: training accuarcy: 0.5945\n",
      "Epoch 0 step 80: training loss: 3263.915735010875\n",
      "Epoch 0 step 81: training accuarcy: 0.5975\n",
      "Epoch 0 step 81: training loss: 3182.3496947119665\n",
      "Epoch 0 step 82: training accuarcy: 0.599\n",
      "Epoch 0 step 82: training loss: 3107.091412243074\n",
      "Epoch 0 step 83: training accuarcy: 0.585\n",
      "Epoch 0 step 83: training loss: 3032.524953609145\n",
      "Epoch 0 step 84: training accuarcy: 0.5855\n",
      "Epoch 0 step 84: training loss: 2963.495915734633\n",
      "Epoch 0 step 85: training accuarcy: 0.5725\n",
      "Epoch 0 step 85: training loss: 2894.1349756890495\n",
      "Epoch 0 step 86: training accuarcy: 0.5805\n",
      "Epoch 0 step 86: training loss: 2827.6093232192125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 87: training accuarcy: 0.5925\n",
      "Epoch 0 step 87: training loss: 2762.5663323613303\n",
      "Epoch 0 step 88: training accuarcy: 0.599\n",
      "Epoch 0 step 88: training loss: 2706.2634834567843\n",
      "Epoch 0 step 89: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 89: training loss: 2644.5764398264546\n",
      "Epoch 0 step 90: training accuarcy: 0.5945\n",
      "Epoch 0 step 90: training loss: 2588.3158711829783\n",
      "Epoch 0 step 91: training accuarcy: 0.6055\n",
      "Epoch 0 step 91: training loss: 2536.562902300486\n",
      "Epoch 0 step 92: training accuarcy: 0.5885\n",
      "Epoch 0 step 92: training loss: 2486.265884515271\n",
      "Epoch 0 step 93: training accuarcy: 0.5915\n",
      "Epoch 0 step 93: training loss: 2435.0182627281147\n",
      "Epoch 0 step 94: training accuarcy: 0.6\n",
      "Epoch 0 step 94: training loss: 2389.118089535354\n",
      "Epoch 0 step 95: training accuarcy: 0.5975\n",
      "Epoch 0 step 95: training loss: 2344.12374985393\n",
      "Epoch 0 step 96: training accuarcy: 0.583\n",
      "Epoch 0 step 96: training loss: 2300.0552278803657\n",
      "Epoch 0 step 97: training accuarcy: 0.5915\n",
      "Epoch 0 step 97: training loss: 2258.84169809021\n",
      "Epoch 0 step 98: training accuarcy: 0.592\n",
      "Epoch 0 step 98: training loss: 2218.261067480604\n",
      "Epoch 0 step 99: training accuarcy: 0.6025\n",
      "Epoch 0 step 99: training loss: 2180.3362250752325\n",
      "Epoch 0 step 100: training accuarcy: 0.6005\n",
      "Epoch 0 step 100: training loss: 2144.171357156447\n",
      "Epoch 0 step 101: training accuarcy: 0.601\n",
      "Epoch 0 step 101: training loss: 2109.1529840377966\n",
      "Epoch 0 step 102: training accuarcy: 0.6045\n",
      "Epoch 0 step 102: training loss: 2075.7088889420174\n",
      "Epoch 0 step 103: training accuarcy: 0.609\n",
      "Epoch 0 step 103: training loss: 2045.4750881160594\n",
      "Epoch 0 step 104: training accuarcy: 0.5765\n",
      "Epoch 0 step 104: training loss: 2013.3139381632864\n",
      "Epoch 0 step 105: training accuarcy: 0.6065\n",
      "Epoch 0 step 105: training loss: 1983.8180910582432\n",
      "Epoch 0 step 106: training accuarcy: 0.599\n",
      "Epoch 0 step 106: training loss: 1955.9047423117145\n",
      "Epoch 0 step 107: training accuarcy: 0.605\n",
      "Epoch 0 step 107: training loss: 1929.9172010057246\n",
      "Epoch 0 step 108: training accuarcy: 0.6065\n",
      "Epoch 0 step 108: training loss: 1905.5602735935536\n",
      "Epoch 0 step 109: training accuarcy: 0.5915\n",
      "Epoch 0 step 109: training loss: 1878.200254827054\n",
      "Epoch 0 step 110: training accuarcy: 0.6145\n",
      "Epoch 0 step 110: training loss: 1856.3194113299887\n",
      "Epoch 0 step 111: training accuarcy: 0.5845\n",
      "Epoch 0 step 111: training loss: 1833.1653562649064\n",
      "Epoch 0 step 112: training accuarcy: 0.599\n",
      "Epoch 0 step 112: training loss: 1810.730713724317\n",
      "Epoch 0 step 113: training accuarcy: 0.6175\n",
      "Epoch 0 step 113: training loss: 1793.0042847002098\n",
      "Epoch 0 step 114: training accuarcy: 0.582\n",
      "Epoch 0 step 114: training loss: 1771.9726215934288\n",
      "Epoch 0 step 115: training accuarcy: 0.6035\n",
      "Epoch 0 step 115: training loss: 1753.7958016076232\n",
      "Epoch 0 step 116: training accuarcy: 0.599\n",
      "Epoch 0 step 116: training loss: 1736.5814465476424\n",
      "Epoch 0 step 117: training accuarcy: 0.5945\n",
      "Epoch 0 step 117: training loss: 1718.2588169586531\n",
      "Epoch 0 step 118: training accuarcy: 0.597\n",
      "Epoch 0 step 118: training loss: 1702.2030619010243\n",
      "Epoch 0 step 119: training accuarcy: 0.6\n",
      "Epoch 0 step 119: training loss: 1686.032512447158\n",
      "Epoch 0 step 120: training accuarcy: 0.612\n",
      "Epoch 0 step 120: training loss: 1672.4098045612598\n",
      "Epoch 0 step 121: training accuarcy: 0.605\n",
      "Epoch 0 step 121: training loss: 1658.1689433751576\n",
      "Epoch 0 step 122: training accuarcy: 0.596\n",
      "Epoch 0 step 122: training loss: 1644.2551075219703\n",
      "Epoch 0 step 123: training accuarcy: 0.602\n",
      "Epoch 0 step 123: training loss: 1631.5806442417763\n",
      "Epoch 0 step 124: training accuarcy: 0.6025\n",
      "Epoch 0 step 124: training loss: 1619.496605629231\n",
      "Epoch 0 step 125: training accuarcy: 0.615\n",
      "Epoch 0 step 125: training loss: 1606.8976203691761\n",
      "Epoch 0 step 126: training accuarcy: 0.611\n",
      "Epoch 0 step 126: training loss: 1596.5024368539691\n",
      "Epoch 0 step 127: training accuarcy: 0.606\n",
      "Epoch 0 step 127: training loss: 1584.7921793138867\n",
      "Epoch 0 step 128: training accuarcy: 0.6175\n",
      "Epoch 0 step 128: training loss: 1575.2523159819411\n",
      "Epoch 0 step 129: training accuarcy: 0.613\n",
      "Epoch 0 step 129: training loss: 1565.032998178794\n",
      "Epoch 0 step 130: training accuarcy: 0.6095\n",
      "Epoch 0 step 130: training loss: 1556.002008025446\n",
      "Epoch 0 step 131: training accuarcy: 0.6265000000000001\n",
      "Epoch 0 step 131: training loss: 1548.338549085878\n",
      "Epoch 0 step 132: training accuarcy: 0.6005\n",
      "Epoch 0 step 132: training loss: 1540.1532660490186\n",
      "Epoch 0 step 133: training accuarcy: 0.598\n",
      "Epoch 0 step 133: training loss: 1531.8461731828952\n",
      "Epoch 0 step 134: training accuarcy: 0.6145\n",
      "Epoch 0 step 134: training loss: 1524.9353633168428\n",
      "Epoch 0 step 135: training accuarcy: 0.6165\n",
      "Epoch 0 step 135: training loss: 1517.6208562577463\n",
      "Epoch 0 step 136: training accuarcy: 0.6195\n",
      "Epoch 0 step 136: training loss: 1511.12274413154\n",
      "Epoch 0 step 137: training accuarcy: 0.597\n",
      "Epoch 0 step 137: training loss: 1503.8677052806174\n",
      "Epoch 0 step 138: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 138: training loss: 1497.5420560295756\n",
      "Epoch 0 step 139: training accuarcy: 0.621\n",
      "Epoch 0 step 139: training loss: 1491.7375420309577\n",
      "Epoch 0 step 140: training accuarcy: 0.6135\n",
      "Epoch 0 step 140: training loss: 1485.7492623522478\n",
      "Epoch 0 step 141: training accuarcy: 0.6105\n",
      "Epoch 0 step 141: training loss: 1481.2769489604746\n",
      "Epoch 0 step 142: training accuarcy: 0.6005\n",
      "Epoch 0 step 142: training loss: 1476.467237532925\n",
      "Epoch 0 step 143: training accuarcy: 0.6195\n",
      "Epoch 0 step 143: training loss: 1471.6510381013193\n",
      "Epoch 0 step 144: training accuarcy: 0.608\n",
      "Epoch 0 step 144: training loss: 1466.0691178540371\n",
      "Epoch 0 step 145: training accuarcy: 0.63\n",
      "Epoch 0 step 145: training loss: 1462.9219211200461\n",
      "Epoch 0 step 146: training accuarcy: 0.612\n",
      "Epoch 0 step 146: training loss: 1458.189675497529\n",
      "Epoch 0 step 147: training accuarcy: 0.6385000000000001\n",
      "Epoch 0 step 147: training loss: 1453.5068012605707\n",
      "Epoch 0 step 148: training accuarcy: 0.6205\n",
      "Epoch 0 step 148: training loss: 1451.155245880636\n",
      "Epoch 0 step 149: training accuarcy: 0.615\n",
      "Epoch 0 step 149: training loss: 1447.0502113295217\n",
      "Epoch 0 step 150: training accuarcy: 0.6135\n",
      "Epoch 0 step 150: training loss: 1443.4691819451198\n",
      "Epoch 0 step 151: training accuarcy: 0.6365000000000001\n",
      "Epoch 0 step 151: training loss: 1440.4613575803126\n",
      "Epoch 0 step 152: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 152: training loss: 1437.4472436845383\n",
      "Epoch 0 step 153: training accuarcy: 0.628\n",
      "Epoch 0 step 153: training loss: 1434.3991041258785\n",
      "Epoch 0 step 154: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 154: training loss: 1432.0644138231792\n",
      "Epoch 0 step 155: training accuarcy: 0.6295000000000001\n",
      "Epoch 0 step 155: training loss: 1428.8674241253038\n",
      "Epoch 0 step 156: training accuarcy: 0.649\n",
      "Epoch 0 step 156: training loss: 1425.812155732883\n",
      "Epoch 0 step 157: training accuarcy: 0.646\n",
      "Epoch 0 step 157: training loss: 1424.2723725892429\n",
      "Epoch 0 step 158: training accuarcy: 0.639\n",
      "Epoch 0 step 158: training loss: 1423.3856810206964\n",
      "Epoch 0 step 159: training accuarcy: 0.6245\n",
      "Epoch 0 step 159: training loss: 1420.274828981289\n",
      "Epoch 0 step 160: training accuarcy: 0.6315000000000001\n",
      "Epoch 0 step 160: training loss: 1417.0586752030079\n",
      "Epoch 0 step 161: training accuarcy: 0.646\n",
      "Epoch 0 step 161: training loss: 1414.9820000853038\n",
      "Epoch 0 step 162: training accuarcy: 0.6365000000000001\n",
      "Epoch 0 step 162: training loss: 1413.856593048961\n",
      "Epoch 0 step 163: training accuarcy: 0.647\n",
      "Epoch 0 step 163: training loss: 1414.112900027208\n",
      "Epoch 0 step 164: training accuarcy: 0.628\n",
      "Epoch 0 step 164: training loss: 1411.9763416426\n",
      "Epoch 0 step 165: training accuarcy: 0.642\n",
      "Epoch 0 step 165: training loss: 1410.295267599146\n",
      "Epoch 0 step 166: training accuarcy: 0.654\n",
      "Epoch 0 step 166: training loss: 1407.4389863884332\n",
      "Epoch 0 step 167: training accuarcy: 0.6715\n",
      "Epoch 0 step 167: training loss: 1407.827917840289\n",
      "Epoch 0 step 168: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 168: training loss: 1405.9620360226809\n",
      "Epoch 0 step 169: training accuarcy: 0.6195\n",
      "Epoch 0 step 169: training loss: 1403.8561005064464\n",
      "Epoch 0 step 170: training accuarcy: 0.645\n",
      "Epoch 0 step 170: training loss: 1403.9883835980431\n",
      "Epoch 0 step 171: training accuarcy: 0.6295000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 171: training loss: 1403.2463424379605\n",
      "Epoch 0 step 172: training accuarcy: 0.64\n",
      "Epoch 0 step 172: training loss: 1402.6984465602586\n",
      "Epoch 0 step 173: training accuarcy: 0.6355000000000001\n",
      "Epoch 0 step 173: training loss: 1400.553143916913\n",
      "Epoch 0 step 174: training accuarcy: 0.66\n",
      "Epoch 0 step 174: training loss: 1400.4041205164006\n",
      "Epoch 0 step 175: training accuarcy: 0.648\n",
      "Epoch 0 step 175: training loss: 1397.8913055722246\n",
      "Epoch 0 step 176: training accuarcy: 0.662\n",
      "Epoch 0 step 176: training loss: 1397.7636800453208\n",
      "Epoch 0 step 177: training accuarcy: 0.6465\n",
      "Epoch 0 step 177: training loss: 1397.2701884359565\n",
      "Epoch 0 step 178: training accuarcy: 0.655\n",
      "Epoch 0 step 178: training loss: 1395.899843048871\n",
      "Epoch 0 step 179: training accuarcy: 0.6735\n",
      "Epoch 0 step 179: training loss: 1395.2441839108803\n",
      "Epoch 0 step 180: training accuarcy: 0.6715\n",
      "Epoch 0 step 180: training loss: 1395.9703598188728\n",
      "Epoch 0 step 181: training accuarcy: 0.646\n",
      "Epoch 0 step 181: training loss: 1394.592959389261\n",
      "Epoch 0 step 182: training accuarcy: 0.6485\n",
      "Epoch 0 step 182: training loss: 1395.201715373398\n",
      "Epoch 0 step 183: training accuarcy: 0.6485\n",
      "Epoch 0 step 183: training loss: 1393.1725726477807\n",
      "Epoch 0 step 184: training accuarcy: 0.6555\n",
      "Epoch 0 step 184: training loss: 1391.962095155837\n",
      "Epoch 0 step 185: training accuarcy: 0.652\n",
      "Epoch 0 step 185: training loss: 1392.4973048455136\n",
      "Epoch 0 step 186: training accuarcy: 0.6565\n",
      "Epoch 0 step 186: training loss: 1392.1386841382366\n",
      "Epoch 0 step 187: training accuarcy: 0.655\n",
      "Epoch 0 step 187: training loss: 1391.2142942183445\n",
      "Epoch 0 step 188: training accuarcy: 0.6705\n",
      "Epoch 0 step 188: training loss: 1391.5419161541681\n",
      "Epoch 0 step 189: training accuarcy: 0.668\n",
      "Epoch 0 step 189: training loss: 1391.0606822053853\n",
      "Epoch 0 step 190: training accuarcy: 0.6445\n",
      "Epoch 0 step 190: training loss: 1390.6904420543508\n",
      "Epoch 0 step 191: training accuarcy: 0.6505\n",
      "Epoch 0 step 191: training loss: 1389.8660571168675\n",
      "Epoch 0 step 192: training accuarcy: 0.652\n",
      "Epoch 0 step 192: training loss: 1390.148036857197\n",
      "Epoch 0 step 193: training accuarcy: 0.6675\n",
      "Epoch 0 step 193: training loss: 1388.3544254957706\n",
      "Epoch 0 step 194: training accuarcy: 0.678\n",
      "Epoch 0 step 194: training loss: 1389.922636267287\n",
      "Epoch 0 step 195: training accuarcy: 0.6555\n",
      "Epoch 0 step 195: training loss: 1388.784817198794\n",
      "Epoch 0 step 196: training accuarcy: 0.6415\n",
      "Epoch 0 step 196: training loss: 1388.1465998855485\n",
      "Epoch 0 step 197: training accuarcy: 0.666\n",
      "Epoch 0 step 197: training loss: 1387.7719592600793\n",
      "Epoch 0 step 198: training accuarcy: 0.6535\n",
      "Epoch 0 step 198: training loss: 1387.0369209408702\n",
      "Epoch 0 step 199: training accuarcy: 0.6795\n",
      "Epoch 0 step 199: training loss: 1387.711369567633\n",
      "Epoch 0 step 200: training accuarcy: 0.6495\n",
      "Epoch 0 step 200: training loss: 1387.8649643403828\n",
      "Epoch 0 step 201: training accuarcy: 0.6675\n",
      "Epoch 0 step 201: training loss: 1388.5663885564832\n",
      "Epoch 0 step 202: training accuarcy: 0.657\n",
      "Epoch 0 step 202: training loss: 1387.858229467672\n",
      "Epoch 0 step 203: training accuarcy: 0.666\n",
      "Epoch 0 step 203: training loss: 1388.084465459926\n",
      "Epoch 0 step 204: training accuarcy: 0.6415\n",
      "Epoch 0 step 204: training loss: 1386.7887834233763\n",
      "Epoch 0 step 205: training accuarcy: 0.6705\n",
      "Epoch 0 step 205: training loss: 1386.752693567284\n",
      "Epoch 0 step 206: training accuarcy: 0.6695\n",
      "Epoch 0 step 206: training loss: 1386.3132633537475\n",
      "Epoch 0 step 207: training accuarcy: 0.6755\n",
      "Epoch 0 step 207: training loss: 1386.8654818405041\n",
      "Epoch 0 step 208: training accuarcy: 0.667\n",
      "Epoch 0 step 208: training loss: 1386.2672044149572\n",
      "Epoch 0 step 209: training accuarcy: 0.658\n",
      "Epoch 0 step 209: training loss: 1384.452850829269\n",
      "Epoch 0 step 210: training accuarcy: 0.679\n",
      "Epoch 0 step 210: training loss: 1385.7260906029376\n",
      "Epoch 0 step 211: training accuarcy: 0.672\n",
      "Epoch 0 step 211: training loss: 1385.7068522083412\n",
      "Epoch 0 step 212: training accuarcy: 0.6695\n",
      "Epoch 0 step 212: training loss: 1386.0931529673078\n",
      "Epoch 0 step 213: training accuarcy: 0.6625\n",
      "Epoch 0 step 213: training loss: 1386.1559585273556\n",
      "Epoch 0 step 214: training accuarcy: 0.6745\n",
      "Epoch 0 step 214: training loss: 1385.041405409517\n",
      "Epoch 0 step 215: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 215: training loss: 1385.4045834460128\n",
      "Epoch 0 step 216: training accuarcy: 0.679\n",
      "Epoch 0 step 216: training loss: 1384.2982671503091\n",
      "Epoch 0 step 217: training accuarcy: 0.6795\n",
      "Epoch 0 step 217: training loss: 1384.7107267817992\n",
      "Epoch 0 step 218: training accuarcy: 0.676\n",
      "Epoch 0 step 218: training loss: 1385.0711765370643\n",
      "Epoch 0 step 219: training accuarcy: 0.675\n",
      "Epoch 0 step 219: training loss: 1384.76502772176\n",
      "Epoch 0 step 220: training accuarcy: 0.677\n",
      "Epoch 0 step 220: training loss: 1384.728783680713\n",
      "Epoch 0 step 221: training accuarcy: 0.6795\n",
      "Epoch 0 step 221: training loss: 1385.1618638440732\n",
      "Epoch 0 step 222: training accuarcy: 0.676\n",
      "Epoch 0 step 222: training loss: 1384.4903209629824\n",
      "Epoch 0 step 223: training accuarcy: 0.6885\n",
      "Epoch 0 step 223: training loss: 1385.0367317190078\n",
      "Epoch 0 step 224: training accuarcy: 0.671\n",
      "Epoch 0 step 224: training loss: 1384.6729797127293\n",
      "Epoch 0 step 225: training accuarcy: 0.68\n",
      "Epoch 0 step 225: training loss: 1384.3502104933013\n",
      "Epoch 0 step 226: training accuarcy: 0.6915\n",
      "Epoch 0 step 226: training loss: 1384.0359873021218\n",
      "Epoch 0 step 227: training accuarcy: 0.679\n",
      "Epoch 0 step 227: training loss: 1384.681468922643\n",
      "Epoch 0 step 228: training accuarcy: 0.6815\n",
      "Epoch 0 step 228: training loss: 1384.3720839359112\n",
      "Epoch 0 step 229: training accuarcy: 0.6825\n",
      "Epoch 0 step 229: training loss: 1384.5894604278703\n",
      "Epoch 0 step 230: training accuarcy: 0.6645\n",
      "Epoch 0 step 230: training loss: 1384.1300663884522\n",
      "Epoch 0 step 231: training accuarcy: 0.673\n",
      "Epoch 0 step 231: training loss: 1384.5417797348362\n",
      "Epoch 0 step 232: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 232: training loss: 1384.6430926680066\n",
      "Epoch 0 step 233: training accuarcy: 0.6675\n",
      "Epoch 0 step 233: training loss: 1384.7564316778794\n",
      "Epoch 0 step 234: training accuarcy: 0.6735\n",
      "Epoch 0 step 234: training loss: 1383.814292920936\n",
      "Epoch 0 step 235: training accuarcy: 0.6880000000000001\n",
      "Epoch 0 step 235: training loss: 1383.4597822019607\n",
      "Epoch 0 step 236: training accuarcy: 0.6845\n",
      "Epoch 0 step 236: training loss: 1383.520036681344\n",
      "Epoch 0 step 237: training accuarcy: 0.6905\n",
      "Epoch 0 step 237: training loss: 1383.7466198490222\n",
      "Epoch 0 step 238: training accuarcy: 0.6985\n",
      "Epoch 0 step 238: training loss: 1383.7335338168095\n",
      "Epoch 0 step 239: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 239: training loss: 1383.7897956756701\n",
      "Epoch 0 step 240: training accuarcy: 0.673\n",
      "Epoch 0 step 240: training loss: 1383.7881327972102\n",
      "Epoch 0 step 241: training accuarcy: 0.6895\n",
      "Epoch 0 step 241: training loss: 1384.7168372776828\n",
      "Epoch 0 step 242: training accuarcy: 0.6665\n",
      "Epoch 0 step 242: training loss: 1382.8973482997887\n",
      "Epoch 0 step 243: training accuarcy: 0.685\n",
      "Epoch 0 step 243: training loss: 1384.1229099567252\n",
      "Epoch 0 step 244: training accuarcy: 0.6930000000000001\n",
      "Epoch 0 step 244: training loss: 1384.0828556763552\n",
      "Epoch 0 step 245: training accuarcy: 0.6785\n",
      "Epoch 0 step 245: training loss: 1383.657407813066\n",
      "Epoch 0 step 246: training accuarcy: 0.6935\n",
      "Epoch 0 step 246: training loss: 1383.9463871150879\n",
      "Epoch 0 step 247: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 247: training loss: 1383.6306263543565\n",
      "Epoch 0 step 248: training accuarcy: 0.6955\n",
      "Epoch 0 step 248: training loss: 1382.9814067511486\n",
      "Epoch 0 step 249: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 249: training loss: 1383.2591692810315\n",
      "Epoch 0 step 250: training accuarcy: 0.687\n",
      "Epoch 0 step 250: training loss: 1383.591079858919\n",
      "Epoch 0 step 251: training accuarcy: 0.676\n",
      "Epoch 0 step 251: training loss: 1383.3625601234835\n",
      "Epoch 0 step 252: training accuarcy: 0.6875\n",
      "Epoch 0 step 252: training loss: 1383.0751395459727\n",
      "Epoch 0 step 253: training accuarcy: 0.6980000000000001\n",
      "Epoch 0 step 253: training loss: 1383.900468432526\n",
      "Epoch 0 step 254: training accuarcy: 0.677\n",
      "Epoch 0 step 254: training loss: 1383.7509882327959\n",
      "Epoch 0 step 255: training accuarcy: 0.687\n",
      "Epoch 0 step 255: training loss: 1383.949941451294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 256: training accuarcy: 0.673\n",
      "Epoch 0 step 256: training loss: 1382.9072025880077\n",
      "Epoch 0 step 257: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 257: training loss: 1383.3938732460037\n",
      "Epoch 0 step 258: training accuarcy: 0.6945\n",
      "Epoch 0 step 258: training loss: 1383.6231843877981\n",
      "Epoch 0 step 259: training accuarcy: 0.7035\n",
      "Epoch 0 step 259: training loss: 1383.1637076390673\n",
      "Epoch 0 step 260: training accuarcy: 0.683\n",
      "Epoch 0 step 260: training loss: 1383.6408113188377\n",
      "Epoch 0 step 261: training accuarcy: 0.6915\n",
      "Epoch 0 step 261: training loss: 1383.8668465713042\n",
      "Epoch 0 step 262: training accuarcy: 0.6755\n",
      "Epoch 0 step 262: training loss: 543.4043466801741\n",
      "Epoch 0 step 263: training accuarcy: 0.6948717948717948\n",
      "Epoch 0: train loss 5440.148758324528, train accuarcy 0.6237958073616028\n",
      "Epoch 0: valid loss 1364.5724493932605, valid accuarcy 0.7122498750686646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████                                                                                                                                     | 1/8 [01:56<13:35, 116.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1381.9288631656204\n",
      "Epoch 1 step 264: training accuarcy: 0.717\n",
      "Epoch 1 step 264: training loss: 1382.8740386957263\n",
      "Epoch 1 step 265: training accuarcy: 0.7205\n",
      "Epoch 1 step 265: training loss: 1383.163731968013\n",
      "Epoch 1 step 266: training accuarcy: 0.6905\n",
      "Epoch 1 step 266: training loss: 1382.8545891421277\n",
      "Epoch 1 step 267: training accuarcy: 0.6885\n",
      "Epoch 1 step 267: training loss: 1382.4083080815135\n",
      "Epoch 1 step 268: training accuarcy: 0.6975\n",
      "Epoch 1 step 268: training loss: 1382.9286292596848\n",
      "Epoch 1 step 269: training accuarcy: 0.7055\n",
      "Epoch 1 step 269: training loss: 1382.8100783364525\n",
      "Epoch 1 step 270: training accuarcy: 0.713\n",
      "Epoch 1 step 270: training loss: 1383.6546622408448\n",
      "Epoch 1 step 271: training accuarcy: 0.707\n",
      "Epoch 1 step 271: training loss: 1382.8825608359637\n",
      "Epoch 1 step 272: training accuarcy: 0.705\n",
      "Epoch 1 step 272: training loss: 1383.230800394471\n",
      "Epoch 1 step 273: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 273: training loss: 1382.6469405966727\n",
      "Epoch 1 step 274: training accuarcy: 0.708\n",
      "Epoch 1 step 274: training loss: 1382.7744678003885\n",
      "Epoch 1 step 275: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 275: training loss: 1381.9167723042701\n",
      "Epoch 1 step 276: training accuarcy: 0.7025\n",
      "Epoch 1 step 276: training loss: 1383.7791531918742\n",
      "Epoch 1 step 277: training accuarcy: 0.6935\n",
      "Epoch 1 step 277: training loss: 1382.618443430913\n",
      "Epoch 1 step 278: training accuarcy: 0.686\n",
      "Epoch 1 step 278: training loss: 1382.897000637387\n",
      "Epoch 1 step 279: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 279: training loss: 1383.1727559333065\n",
      "Epoch 1 step 280: training accuarcy: 0.7155\n",
      "Epoch 1 step 280: training loss: 1382.4588360380956\n",
      "Epoch 1 step 281: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 281: training loss: 1382.0950391056456\n",
      "Epoch 1 step 282: training accuarcy: 0.7045\n",
      "Epoch 1 step 282: training loss: 1382.855965572646\n",
      "Epoch 1 step 283: training accuarcy: 0.6855\n",
      "Epoch 1 step 283: training loss: 1382.347269550663\n",
      "Epoch 1 step 284: training accuarcy: 0.7105\n",
      "Epoch 1 step 284: training loss: 1382.1051984514772\n",
      "Epoch 1 step 285: training accuarcy: 0.7115\n",
      "Epoch 1 step 285: training loss: 1382.740802266272\n",
      "Epoch 1 step 286: training accuarcy: 0.7025\n",
      "Epoch 1 step 286: training loss: 1382.604197804284\n",
      "Epoch 1 step 287: training accuarcy: 0.7035\n",
      "Epoch 1 step 287: training loss: 1382.7160219494215\n",
      "Epoch 1 step 288: training accuarcy: 0.687\n",
      "Epoch 1 step 288: training loss: 1382.603521280423\n",
      "Epoch 1 step 289: training accuarcy: 0.721\n",
      "Epoch 1 step 289: training loss: 1383.6716749734348\n",
      "Epoch 1 step 290: training accuarcy: 0.6945\n",
      "Epoch 1 step 290: training loss: 1381.4903735619505\n",
      "Epoch 1 step 291: training accuarcy: 0.7125\n",
      "Epoch 1 step 291: training loss: 1381.8795672906072\n",
      "Epoch 1 step 292: training accuarcy: 0.715\n",
      "Epoch 1 step 292: training loss: 1382.0403105492915\n",
      "Epoch 1 step 293: training accuarcy: 0.704\n",
      "Epoch 1 step 293: training loss: 1382.213735580636\n",
      "Epoch 1 step 294: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 294: training loss: 1381.6778264213972\n",
      "Epoch 1 step 295: training accuarcy: 0.7105\n",
      "Epoch 1 step 295: training loss: 1382.3367607082669\n",
      "Epoch 1 step 296: training accuarcy: 0.6985\n",
      "Epoch 1 step 296: training loss: 1382.3709388904763\n",
      "Epoch 1 step 297: training accuarcy: 0.7015\n",
      "Epoch 1 step 297: training loss: 1382.2225037767334\n",
      "Epoch 1 step 298: training accuarcy: 0.704\n",
      "Epoch 1 step 298: training loss: 1383.0822877574485\n",
      "Epoch 1 step 299: training accuarcy: 0.7075\n",
      "Epoch 1 step 299: training loss: 1382.1294283193467\n",
      "Epoch 1 step 300: training accuarcy: 0.706\n",
      "Epoch 1 step 300: training loss: 1382.7811838232303\n",
      "Epoch 1 step 301: training accuarcy: 0.7085\n",
      "Epoch 1 step 301: training loss: 1382.1801357742652\n",
      "Epoch 1 step 302: training accuarcy: 0.7045\n",
      "Epoch 1 step 302: training loss: 1383.6370542622221\n",
      "Epoch 1 step 303: training accuarcy: 0.6945\n",
      "Epoch 1 step 303: training loss: 1381.5967318983458\n",
      "Epoch 1 step 304: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 304: training loss: 1382.0156740190553\n",
      "Epoch 1 step 305: training accuarcy: 0.6945\n",
      "Epoch 1 step 305: training loss: 1382.914604367383\n",
      "Epoch 1 step 306: training accuarcy: 0.7135\n",
      "Epoch 1 step 306: training loss: 1382.4159644398544\n",
      "Epoch 1 step 307: training accuarcy: 0.6945\n",
      "Epoch 1 step 307: training loss: 1382.2066528834987\n",
      "Epoch 1 step 308: training accuarcy: 0.715\n",
      "Epoch 1 step 308: training loss: 1382.8782353445918\n",
      "Epoch 1 step 309: training accuarcy: 0.6985\n",
      "Epoch 1 step 309: training loss: 1383.1439764667423\n",
      "Epoch 1 step 310: training accuarcy: 0.7155\n",
      "Epoch 1 step 310: training loss: 1383.7972791931427\n",
      "Epoch 1 step 311: training accuarcy: 0.678\n",
      "Epoch 1 step 311: training loss: 1383.442139773331\n",
      "Epoch 1 step 312: training accuarcy: 0.6835\n",
      "Epoch 1 step 312: training loss: 1383.9943493990477\n",
      "Epoch 1 step 313: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 313: training loss: 1383.5508810386207\n",
      "Epoch 1 step 314: training accuarcy: 0.6925\n",
      "Epoch 1 step 314: training loss: 1381.7084569186156\n",
      "Epoch 1 step 315: training accuarcy: 0.707\n",
      "Epoch 1 step 315: training loss: 1383.1185781323384\n",
      "Epoch 1 step 316: training accuarcy: 0.676\n",
      "Epoch 1 step 316: training loss: 1382.6813425345351\n",
      "Epoch 1 step 317: training accuarcy: 0.7085\n",
      "Epoch 1 step 317: training loss: 1382.9401829074354\n",
      "Epoch 1 step 318: training accuarcy: 0.704\n",
      "Epoch 1 step 318: training loss: 1382.4810101800197\n",
      "Epoch 1 step 319: training accuarcy: 0.705\n",
      "Epoch 1 step 319: training loss: 1382.7056506569033\n",
      "Epoch 1 step 320: training accuarcy: 0.6975\n",
      "Epoch 1 step 320: training loss: 1384.1321980873631\n",
      "Epoch 1 step 321: training accuarcy: 0.6990000000000001\n",
      "Epoch 1 step 321: training loss: 1382.7558694759014\n",
      "Epoch 1 step 322: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 322: training loss: 1384.0358880329659\n",
      "Epoch 1 step 323: training accuarcy: 0.678\n",
      "Epoch 1 step 323: training loss: 1381.5237783865325\n",
      "Epoch 1 step 324: training accuarcy: 0.7175\n",
      "Epoch 1 step 324: training loss: 1383.970747610062\n",
      "Epoch 1 step 325: training accuarcy: 0.67\n",
      "Epoch 1 step 325: training loss: 1382.3103967729448\n",
      "Epoch 1 step 326: training accuarcy: 0.7045\n",
      "Epoch 1 step 326: training loss: 1383.5611217801684\n",
      "Epoch 1 step 327: training accuarcy: 0.6765\n",
      "Epoch 1 step 327: training loss: 1382.9128421449436\n",
      "Epoch 1 step 328: training accuarcy: 0.6895\n",
      "Epoch 1 step 328: training loss: 1383.605240968852\n",
      "Epoch 1 step 329: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 329: training loss: 1383.2134913442394\n",
      "Epoch 1 step 330: training accuarcy: 0.687\n",
      "Epoch 1 step 330: training loss: 1383.0159960287324\n",
      "Epoch 1 step 331: training accuarcy: 0.7105\n",
      "Epoch 1 step 331: training loss: 1382.6558428546018\n",
      "Epoch 1 step 332: training accuarcy: 0.6865\n",
      "Epoch 1 step 332: training loss: 1382.1965389165664\n",
      "Epoch 1 step 333: training accuarcy: 0.7215\n",
      "Epoch 1 step 333: training loss: 1383.8771682819663\n",
      "Epoch 1 step 334: training accuarcy: 0.6865\n",
      "Epoch 1 step 334: training loss: 1382.6534074027395\n",
      "Epoch 1 step 335: training accuarcy: 0.7115\n",
      "Epoch 1 step 335: training loss: 1382.5338400096393\n",
      "Epoch 1 step 336: training accuarcy: 0.6955\n",
      "Epoch 1 step 336: training loss: 1382.6527385162049\n",
      "Epoch 1 step 337: training accuarcy: 0.6985\n",
      "Epoch 1 step 337: training loss: 1382.54035960109\n",
      "Epoch 1 step 338: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 338: training loss: 1383.4101876197615\n",
      "Epoch 1 step 339: training accuarcy: 0.6885\n",
      "Epoch 1 step 339: training loss: 1383.8339781694206\n",
      "Epoch 1 step 340: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 340: training loss: 1382.0082540682347\n",
      "Epoch 1 step 341: training accuarcy: 0.7065\n",
      "Epoch 1 step 341: training loss: 1382.569257412533\n",
      "Epoch 1 step 342: training accuarcy: 0.7185\n",
      "Epoch 1 step 342: training loss: 1384.028029550414\n",
      "Epoch 1 step 343: training accuarcy: 0.6835\n",
      "Epoch 1 step 343: training loss: 1382.5080820907128\n",
      "Epoch 1 step 344: training accuarcy: 0.705\n",
      "Epoch 1 step 344: training loss: 1381.7664094446702\n",
      "Epoch 1 step 345: training accuarcy: 0.7085\n",
      "Epoch 1 step 345: training loss: 1382.826825193834\n",
      "Epoch 1 step 346: training accuarcy: 0.686\n",
      "Epoch 1 step 346: training loss: 1382.8671398346373\n",
      "Epoch 1 step 347: training accuarcy: 0.6920000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 347: training loss: 1383.0618994443694\n",
      "Epoch 1 step 348: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 348: training loss: 1382.913775212559\n",
      "Epoch 1 step 349: training accuarcy: 0.7135\n",
      "Epoch 1 step 349: training loss: 1382.9219114150665\n",
      "Epoch 1 step 350: training accuarcy: 0.676\n",
      "Epoch 1 step 350: training loss: 1382.7557897761963\n",
      "Epoch 1 step 351: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 351: training loss: 1383.6413111256625\n",
      "Epoch 1 step 352: training accuarcy: 0.7025\n",
      "Epoch 1 step 352: training loss: 1382.9694863399923\n",
      "Epoch 1 step 353: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 353: training loss: 1383.6165630754715\n",
      "Epoch 1 step 354: training accuarcy: 0.681\n",
      "Epoch 1 step 354: training loss: 1382.7907732011336\n",
      "Epoch 1 step 355: training accuarcy: 0.706\n",
      "Epoch 1 step 355: training loss: 1382.8123922637292\n",
      "Epoch 1 step 356: training accuarcy: 0.6995\n",
      "Epoch 1 step 356: training loss: 1382.4637338122902\n",
      "Epoch 1 step 357: training accuarcy: 0.7055\n",
      "Epoch 1 step 357: training loss: 1383.80813387469\n",
      "Epoch 1 step 358: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 358: training loss: 1382.7484392429235\n",
      "Epoch 1 step 359: training accuarcy: 0.7005\n",
      "Epoch 1 step 359: training loss: 1382.755986316608\n",
      "Epoch 1 step 360: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 360: training loss: 1383.4039634541498\n",
      "Epoch 1 step 361: training accuarcy: 0.6935\n",
      "Epoch 1 step 361: training loss: 1381.9585902691836\n",
      "Epoch 1 step 362: training accuarcy: 0.6935\n",
      "Epoch 1 step 362: training loss: 1382.4376920135655\n",
      "Epoch 1 step 363: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 363: training loss: 1383.142909074914\n",
      "Epoch 1 step 364: training accuarcy: 0.6915\n",
      "Epoch 1 step 364: training loss: 1382.7535364937187\n",
      "Epoch 1 step 365: training accuarcy: 0.683\n",
      "Epoch 1 step 365: training loss: 1382.480890714615\n",
      "Epoch 1 step 366: training accuarcy: 0.7055\n",
      "Epoch 1 step 366: training loss: 1382.3038037718766\n",
      "Epoch 1 step 367: training accuarcy: 0.709\n",
      "Epoch 1 step 367: training loss: 1382.739269251073\n",
      "Epoch 1 step 368: training accuarcy: 0.7175\n",
      "Epoch 1 step 368: training loss: 1383.689325831669\n",
      "Epoch 1 step 369: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 369: training loss: 1382.1653749408774\n",
      "Epoch 1 step 370: training accuarcy: 0.705\n",
      "Epoch 1 step 370: training loss: 1382.271805402761\n",
      "Epoch 1 step 371: training accuarcy: 0.6930000000000001\n",
      "Epoch 1 step 371: training loss: 1382.1772721829263\n",
      "Epoch 1 step 372: training accuarcy: 0.678\n",
      "Epoch 1 step 372: training loss: 1383.5498644290935\n",
      "Epoch 1 step 373: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 373: training loss: 1383.4072072534866\n",
      "Epoch 1 step 374: training accuarcy: 0.6975\n",
      "Epoch 1 step 374: training loss: 1383.7691793383644\n",
      "Epoch 1 step 375: training accuarcy: 0.6805\n",
      "Epoch 1 step 375: training loss: 1382.9624221801682\n",
      "Epoch 1 step 376: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 376: training loss: 1382.6337499822928\n",
      "Epoch 1 step 377: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 377: training loss: 1382.8181639895981\n",
      "Epoch 1 step 378: training accuarcy: 0.6855\n",
      "Epoch 1 step 378: training loss: 1382.8295721896102\n",
      "Epoch 1 step 379: training accuarcy: 0.6965\n",
      "Epoch 1 step 379: training loss: 1383.190071798451\n",
      "Epoch 1 step 380: training accuarcy: 0.686\n",
      "Epoch 1 step 380: training loss: 1383.2121395904833\n",
      "Epoch 1 step 381: training accuarcy: 0.6975\n",
      "Epoch 1 step 381: training loss: 1382.4927760719597\n",
      "Epoch 1 step 382: training accuarcy: 0.705\n",
      "Epoch 1 step 382: training loss: 1382.094002018633\n",
      "Epoch 1 step 383: training accuarcy: 0.6985\n",
      "Epoch 1 step 383: training loss: 1382.6909104015335\n",
      "Epoch 1 step 384: training accuarcy: 0.714\n",
      "Epoch 1 step 384: training loss: 1382.6558052446471\n",
      "Epoch 1 step 385: training accuarcy: 0.6925\n",
      "Epoch 1 step 385: training loss: 1382.9869926566366\n",
      "Epoch 1 step 386: training accuarcy: 0.7055\n",
      "Epoch 1 step 386: training loss: 1383.1745295630974\n",
      "Epoch 1 step 387: training accuarcy: 0.7015\n",
      "Epoch 1 step 387: training loss: 1383.740171688482\n",
      "Epoch 1 step 388: training accuarcy: 0.6815\n",
      "Epoch 1 step 388: training loss: 1382.3038971885746\n",
      "Epoch 1 step 389: training accuarcy: 0.6985\n",
      "Epoch 1 step 389: training loss: 1382.0784021321633\n",
      "Epoch 1 step 390: training accuarcy: 0.7125\n",
      "Epoch 1 step 390: training loss: 1382.910251339042\n",
      "Epoch 1 step 391: training accuarcy: 0.7025\n",
      "Epoch 1 step 391: training loss: 1382.7214430832714\n",
      "Epoch 1 step 392: training accuarcy: 0.7005\n",
      "Epoch 1 step 392: training loss: 1382.3957522901\n",
      "Epoch 1 step 393: training accuarcy: 0.6985\n",
      "Epoch 1 step 393: training loss: 1382.8067285333607\n",
      "Epoch 1 step 394: training accuarcy: 0.7015\n",
      "Epoch 1 step 394: training loss: 1383.1250562435052\n",
      "Epoch 1 step 395: training accuarcy: 0.6915\n",
      "Epoch 1 step 395: training loss: 1382.403334945549\n",
      "Epoch 1 step 396: training accuarcy: 0.685\n",
      "Epoch 1 step 396: training loss: 1383.9931185001024\n",
      "Epoch 1 step 397: training accuarcy: 0.68\n",
      "Epoch 1 step 397: training loss: 1383.5348718868042\n",
      "Epoch 1 step 398: training accuarcy: 0.6955\n",
      "Epoch 1 step 398: training loss: 1382.6526724172686\n",
      "Epoch 1 step 399: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 399: training loss: 1382.8618313583374\n",
      "Epoch 1 step 400: training accuarcy: 0.7000000000000001\n",
      "Epoch 1 step 400: training loss: 1381.962950276407\n",
      "Epoch 1 step 401: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 401: training loss: 1382.4095264174887\n",
      "Epoch 1 step 402: training accuarcy: 0.7065\n",
      "Epoch 1 step 402: training loss: 1382.7390575774025\n",
      "Epoch 1 step 403: training accuarcy: 0.704\n",
      "Epoch 1 step 403: training loss: 1382.163124460385\n",
      "Epoch 1 step 404: training accuarcy: 0.711\n",
      "Epoch 1 step 404: training loss: 1383.0447394999687\n",
      "Epoch 1 step 405: training accuarcy: 0.7065\n",
      "Epoch 1 step 405: training loss: 1382.676796858335\n",
      "Epoch 1 step 406: training accuarcy: 0.683\n",
      "Epoch 1 step 406: training loss: 1382.1000106814172\n",
      "Epoch 1 step 407: training accuarcy: 0.714\n",
      "Epoch 1 step 407: training loss: 1381.5575924586026\n",
      "Epoch 1 step 408: training accuarcy: 0.7065\n",
      "Epoch 1 step 408: training loss: 1383.2786066793713\n",
      "Epoch 1 step 409: training accuarcy: 0.6985\n",
      "Epoch 1 step 409: training loss: 1382.8534365679845\n",
      "Epoch 1 step 410: training accuarcy: 0.687\n",
      "Epoch 1 step 410: training loss: 1383.0992314519851\n",
      "Epoch 1 step 411: training accuarcy: 0.6825\n",
      "Epoch 1 step 411: training loss: 1383.1954309821247\n",
      "Epoch 1 step 412: training accuarcy: 0.685\n",
      "Epoch 1 step 412: training loss: 1382.0851836435083\n",
      "Epoch 1 step 413: training accuarcy: 0.7025\n",
      "Epoch 1 step 413: training loss: 1382.5531298113149\n",
      "Epoch 1 step 414: training accuarcy: 0.7115\n",
      "Epoch 1 step 414: training loss: 1384.082005387279\n",
      "Epoch 1 step 415: training accuarcy: 0.681\n",
      "Epoch 1 step 415: training loss: 1383.5540138643319\n",
      "Epoch 1 step 416: training accuarcy: 0.711\n",
      "Epoch 1 step 416: training loss: 1383.0056318061527\n",
      "Epoch 1 step 417: training accuarcy: 0.7035\n",
      "Epoch 1 step 417: training loss: 1382.7055028423792\n",
      "Epoch 1 step 418: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 418: training loss: 1383.7597999085824\n",
      "Epoch 1 step 419: training accuarcy: 0.6875\n",
      "Epoch 1 step 419: training loss: 1383.160496842933\n",
      "Epoch 1 step 420: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 420: training loss: 1382.4256275730318\n",
      "Epoch 1 step 421: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 421: training loss: 1382.6962969181238\n",
      "Epoch 1 step 422: training accuarcy: 0.6945\n",
      "Epoch 1 step 422: training loss: 1381.8012752851005\n",
      "Epoch 1 step 423: training accuarcy: 0.705\n",
      "Epoch 1 step 423: training loss: 1382.7861544697892\n",
      "Epoch 1 step 424: training accuarcy: 0.7165\n",
      "Epoch 1 step 424: training loss: 1382.6314436500554\n",
      "Epoch 1 step 425: training accuarcy: 0.6895\n",
      "Epoch 1 step 425: training loss: 1383.3840346164664\n",
      "Epoch 1 step 426: training accuarcy: 0.6805\n",
      "Epoch 1 step 426: training loss: 1383.318399585697\n",
      "Epoch 1 step 427: training accuarcy: 0.6930000000000001\n",
      "Epoch 1 step 427: training loss: 1382.7635216684082\n",
      "Epoch 1 step 428: training accuarcy: 0.6915\n",
      "Epoch 1 step 428: training loss: 1382.6812118961857\n",
      "Epoch 1 step 429: training accuarcy: 0.6985\n",
      "Epoch 1 step 429: training loss: 1381.9758926390182\n",
      "Epoch 1 step 430: training accuarcy: 0.708\n",
      "Epoch 1 step 430: training loss: 1384.1621375146858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 431: training accuarcy: 0.687\n",
      "Epoch 1 step 431: training loss: 1383.544529975551\n",
      "Epoch 1 step 432: training accuarcy: 0.6935\n",
      "Epoch 1 step 432: training loss: 1382.7406523463846\n",
      "Epoch 1 step 433: training accuarcy: 0.705\n",
      "Epoch 1 step 433: training loss: 1383.8302853839634\n",
      "Epoch 1 step 434: training accuarcy: 0.685\n",
      "Epoch 1 step 434: training loss: 1382.263804137278\n",
      "Epoch 1 step 435: training accuarcy: 0.7065\n",
      "Epoch 1 step 435: training loss: 1383.2367647967167\n",
      "Epoch 1 step 436: training accuarcy: 0.7015\n",
      "Epoch 1 step 436: training loss: 1382.6569232613456\n",
      "Epoch 1 step 437: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 437: training loss: 1382.3876202297586\n",
      "Epoch 1 step 438: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 438: training loss: 1382.6201850273494\n",
      "Epoch 1 step 439: training accuarcy: 0.7025\n",
      "Epoch 1 step 439: training loss: 1383.1003555990235\n",
      "Epoch 1 step 440: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 440: training loss: 1383.0934235754376\n",
      "Epoch 1 step 441: training accuarcy: 0.6815\n",
      "Epoch 1 step 441: training loss: 1381.3731106128469\n",
      "Epoch 1 step 442: training accuarcy: 0.7265\n",
      "Epoch 1 step 442: training loss: 1382.7451155627007\n",
      "Epoch 1 step 443: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 443: training loss: 1384.6850145328424\n",
      "Epoch 1 step 444: training accuarcy: 0.687\n",
      "Epoch 1 step 444: training loss: 1382.4231795191288\n",
      "Epoch 1 step 445: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 445: training loss: 1381.3712672140468\n",
      "Epoch 1 step 446: training accuarcy: 0.7075\n",
      "Epoch 1 step 446: training loss: 1382.635748590452\n",
      "Epoch 1 step 447: training accuarcy: 0.707\n",
      "Epoch 1 step 447: training loss: 1382.7256532186032\n",
      "Epoch 1 step 448: training accuarcy: 0.7035\n",
      "Epoch 1 step 448: training loss: 1382.8298777321188\n",
      "Epoch 1 step 449: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 449: training loss: 1382.5597717107937\n",
      "Epoch 1 step 450: training accuarcy: 0.7015\n",
      "Epoch 1 step 450: training loss: 1382.3024001630042\n",
      "Epoch 1 step 451: training accuarcy: 0.714\n",
      "Epoch 1 step 451: training loss: 1382.9823126706394\n",
      "Epoch 1 step 452: training accuarcy: 0.7095\n",
      "Epoch 1 step 452: training loss: 1383.359763906216\n",
      "Epoch 1 step 453: training accuarcy: 0.7075\n",
      "Epoch 1 step 453: training loss: 1382.7032253368802\n",
      "Epoch 1 step 454: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 454: training loss: 1383.0456283862827\n",
      "Epoch 1 step 455: training accuarcy: 0.7045\n",
      "Epoch 1 step 455: training loss: 1382.5595065311124\n",
      "Epoch 1 step 456: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 456: training loss: 1383.190646323468\n",
      "Epoch 1 step 457: training accuarcy: 0.6855\n",
      "Epoch 1 step 457: training loss: 1382.804726310822\n",
      "Epoch 1 step 458: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 458: training loss: 1382.5027377952918\n",
      "Epoch 1 step 459: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 459: training loss: 1382.3675509010604\n",
      "Epoch 1 step 460: training accuarcy: 0.7085\n",
      "Epoch 1 step 460: training loss: 1382.3358219454171\n",
      "Epoch 1 step 461: training accuarcy: 0.709\n",
      "Epoch 1 step 461: training loss: 1383.2184251041006\n",
      "Epoch 1 step 462: training accuarcy: 0.6925\n",
      "Epoch 1 step 462: training loss: 1382.2870642875325\n",
      "Epoch 1 step 463: training accuarcy: 0.6935\n",
      "Epoch 1 step 463: training loss: 1382.2643850882519\n",
      "Epoch 1 step 464: training accuarcy: 0.7045\n",
      "Epoch 1 step 464: training loss: 1383.1230374451648\n",
      "Epoch 1 step 465: training accuarcy: 0.6995\n",
      "Epoch 1 step 465: training loss: 1383.2987436631956\n",
      "Epoch 1 step 466: training accuarcy: 0.7045\n",
      "Epoch 1 step 466: training loss: 1382.7109683280892\n",
      "Epoch 1 step 467: training accuarcy: 0.7005\n",
      "Epoch 1 step 467: training loss: 1383.0758890406253\n",
      "Epoch 1 step 468: training accuarcy: 0.6825\n",
      "Epoch 1 step 468: training loss: 1382.9623144388481\n",
      "Epoch 1 step 469: training accuarcy: 0.6915\n",
      "Epoch 1 step 469: training loss: 1382.4193748619339\n",
      "Epoch 1 step 470: training accuarcy: 0.6990000000000001\n",
      "Epoch 1 step 470: training loss: 1383.2413406686633\n",
      "Epoch 1 step 471: training accuarcy: 0.705\n",
      "Epoch 1 step 471: training loss: 1382.8755779792227\n",
      "Epoch 1 step 472: training accuarcy: 0.71\n",
      "Epoch 1 step 472: training loss: 1384.5536326024753\n",
      "Epoch 1 step 473: training accuarcy: 0.7005\n",
      "Epoch 1 step 473: training loss: 1383.8650324625926\n",
      "Epoch 1 step 474: training accuarcy: 0.6825\n",
      "Epoch 1 step 474: training loss: 1382.5799854682343\n",
      "Epoch 1 step 475: training accuarcy: 0.7035\n",
      "Epoch 1 step 475: training loss: 1382.929384874539\n",
      "Epoch 1 step 476: training accuarcy: 0.6910000000000001\n",
      "Epoch 1 step 476: training loss: 1382.3162854235536\n",
      "Epoch 1 step 477: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 477: training loss: 1382.4377693822971\n",
      "Epoch 1 step 478: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 478: training loss: 1382.3635648881257\n",
      "Epoch 1 step 479: training accuarcy: 0.71\n",
      "Epoch 1 step 479: training loss: 1383.436659744837\n",
      "Epoch 1 step 480: training accuarcy: 0.7005\n",
      "Epoch 1 step 480: training loss: 1384.059067303122\n",
      "Epoch 1 step 481: training accuarcy: 0.6785\n",
      "Epoch 1 step 481: training loss: 1382.4524769815416\n",
      "Epoch 1 step 482: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 482: training loss: 1383.1409915844922\n",
      "Epoch 1 step 483: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 483: training loss: 1383.416869857891\n",
      "Epoch 1 step 484: training accuarcy: 0.6985\n",
      "Epoch 1 step 484: training loss: 1382.9798447799778\n",
      "Epoch 1 step 485: training accuarcy: 0.7005\n",
      "Epoch 1 step 485: training loss: 1383.5609336322314\n",
      "Epoch 1 step 486: training accuarcy: 0.6935\n",
      "Epoch 1 step 486: training loss: 1382.8419990154086\n",
      "Epoch 1 step 487: training accuarcy: 0.6955\n",
      "Epoch 1 step 487: training loss: 1382.589517115585\n",
      "Epoch 1 step 488: training accuarcy: 0.7035\n",
      "Epoch 1 step 488: training loss: 1383.3036605562438\n",
      "Epoch 1 step 489: training accuarcy: 0.6735\n",
      "Epoch 1 step 489: training loss: 1383.928445826889\n",
      "Epoch 1 step 490: training accuarcy: 0.6835\n",
      "Epoch 1 step 490: training loss: 1383.2556960525767\n",
      "Epoch 1 step 491: training accuarcy: 0.6835\n",
      "Epoch 1 step 491: training loss: 1382.718364897062\n",
      "Epoch 1 step 492: training accuarcy: 0.7035\n",
      "Epoch 1 step 492: training loss: 1383.609179760611\n",
      "Epoch 1 step 493: training accuarcy: 0.6950000000000001\n",
      "Epoch 1 step 493: training loss: 1382.4327484714192\n",
      "Epoch 1 step 494: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 494: training loss: 1383.6456773474176\n",
      "Epoch 1 step 495: training accuarcy: 0.681\n",
      "Epoch 1 step 495: training loss: 1383.8822177216502\n",
      "Epoch 1 step 496: training accuarcy: 0.6835\n",
      "Epoch 1 step 496: training loss: 1382.5479717040791\n",
      "Epoch 1 step 497: training accuarcy: 0.7000000000000001\n",
      "Epoch 1 step 497: training loss: 1383.143312785016\n",
      "Epoch 1 step 498: training accuarcy: 0.6985\n",
      "Epoch 1 step 498: training loss: 1383.096521693051\n",
      "Epoch 1 step 499: training accuarcy: 0.6895\n",
      "Epoch 1 step 499: training loss: 1383.738875247877\n",
      "Epoch 1 step 500: training accuarcy: 0.6855\n",
      "Epoch 1 step 500: training loss: 1382.5091192079044\n",
      "Epoch 1 step 501: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 501: training loss: 1382.6927263337775\n",
      "Epoch 1 step 502: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 502: training loss: 1382.9637716656537\n",
      "Epoch 1 step 503: training accuarcy: 0.6895\n",
      "Epoch 1 step 503: training loss: 1382.778721164391\n",
      "Epoch 1 step 504: training accuarcy: 0.7085\n",
      "Epoch 1 step 504: training loss: 1383.0861073809492\n",
      "Epoch 1 step 505: training accuarcy: 0.687\n",
      "Epoch 1 step 505: training loss: 1382.4815568495792\n",
      "Epoch 1 step 506: training accuarcy: 0.6955\n",
      "Epoch 1 step 506: training loss: 1383.5547116663681\n",
      "Epoch 1 step 507: training accuarcy: 0.6855\n",
      "Epoch 1 step 507: training loss: 1382.2612753187495\n",
      "Epoch 1 step 508: training accuarcy: 0.7035\n",
      "Epoch 1 step 508: training loss: 1383.7144544686528\n",
      "Epoch 1 step 509: training accuarcy: 0.6785\n",
      "Epoch 1 step 509: training loss: 1384.514737021985\n",
      "Epoch 1 step 510: training accuarcy: 0.681\n",
      "Epoch 1 step 510: training loss: 1382.6601887763309\n",
      "Epoch 1 step 511: training accuarcy: 0.7125\n",
      "Epoch 1 step 511: training loss: 1383.509457709399\n",
      "Epoch 1 step 512: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 512: training loss: 1382.3316493009982\n",
      "Epoch 1 step 513: training accuarcy: 0.6980000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 513: training loss: 1383.1139720090362\n",
      "Epoch 1 step 514: training accuarcy: 0.6895\n",
      "Epoch 1 step 514: training loss: 1383.1273729447241\n",
      "Epoch 1 step 515: training accuarcy: 0.6885\n",
      "Epoch 1 step 515: training loss: 1383.5136744847298\n",
      "Epoch 1 step 516: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 516: training loss: 1382.0365476969548\n",
      "Epoch 1 step 517: training accuarcy: 0.7185\n",
      "Epoch 1 step 517: training loss: 1383.3986803261205\n",
      "Epoch 1 step 518: training accuarcy: 0.6915\n",
      "Epoch 1 step 518: training loss: 1382.1109219179843\n",
      "Epoch 1 step 519: training accuarcy: 0.716\n",
      "Epoch 1 step 519: training loss: 1383.17402704786\n",
      "Epoch 1 step 520: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 520: training loss: 1382.6616721640212\n",
      "Epoch 1 step 521: training accuarcy: 0.6995\n",
      "Epoch 1 step 521: training loss: 1382.4970014507155\n",
      "Epoch 1 step 522: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 522: training loss: 1383.5249716979354\n",
      "Epoch 1 step 523: training accuarcy: 0.6975\n",
      "Epoch 1 step 523: training loss: 1383.379121082561\n",
      "Epoch 1 step 524: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 524: training loss: 1382.9835506134104\n",
      "Epoch 1 step 525: training accuarcy: 0.7015\n",
      "Epoch 1 step 525: training loss: 543.3559587859495\n",
      "Epoch 1 step 526: training accuarcy: 0.7141025641025641\n",
      "Epoch 1: train loss 1379.6690073786979, train accuarcy 0.6994702816009521\n",
      "Epoch 1: valid loss 1363.9291752894183, valid accuarcy 0.7103294730186462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████                                                                                                                  | 2/8 [03:49<11:32, 115.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 1382.0775223145868\n",
      "Epoch 2 step 527: training accuarcy: 0.7135\n",
      "Epoch 2 step 527: training loss: 1381.8366147226334\n",
      "Epoch 2 step 528: training accuarcy: 0.7035\n",
      "Epoch 2 step 528: training loss: 1381.4339110246892\n",
      "Epoch 2 step 529: training accuarcy: 0.724\n",
      "Epoch 2 step 529: training loss: 1381.3559395621166\n",
      "Epoch 2 step 530: training accuarcy: 0.7245\n",
      "Epoch 2 step 530: training loss: 1381.7553624288194\n",
      "Epoch 2 step 531: training accuarcy: 0.724\n",
      "Epoch 2 step 531: training loss: 1382.7913779384473\n",
      "Epoch 2 step 532: training accuarcy: 0.7095\n",
      "Epoch 2 step 532: training loss: 1382.1608668271333\n",
      "Epoch 2 step 533: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 533: training loss: 1382.5028539405237\n",
      "Epoch 2 step 534: training accuarcy: 0.736\n",
      "Epoch 2 step 534: training loss: 1381.686959127311\n",
      "Epoch 2 step 535: training accuarcy: 0.7035\n",
      "Epoch 2 step 535: training loss: 1382.1422925118645\n",
      "Epoch 2 step 536: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 536: training loss: 1380.8483829766956\n",
      "Epoch 2 step 537: training accuarcy: 0.74\n",
      "Epoch 2 step 537: training loss: 1381.459103506052\n",
      "Epoch 2 step 538: training accuarcy: 0.709\n",
      "Epoch 2 step 538: training loss: 1382.6506693698955\n",
      "Epoch 2 step 539: training accuarcy: 0.7135\n",
      "Epoch 2 step 539: training loss: 1382.946005336632\n",
      "Epoch 2 step 540: training accuarcy: 0.7015\n",
      "Epoch 2 step 540: training loss: 1382.425615885395\n",
      "Epoch 2 step 541: training accuarcy: 0.7025\n",
      "Epoch 2 step 541: training loss: 1382.1615843897287\n",
      "Epoch 2 step 542: training accuarcy: 0.723\n",
      "Epoch 2 step 542: training loss: 1381.2599388601911\n",
      "Epoch 2 step 543: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 543: training loss: 1382.3394062940797\n",
      "Epoch 2 step 544: training accuarcy: 0.7055\n",
      "Epoch 2 step 544: training loss: 1381.8490602263178\n",
      "Epoch 2 step 545: training accuarcy: 0.711\n",
      "Epoch 2 step 545: training loss: 1383.1380969045706\n",
      "Epoch 2 step 546: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 546: training loss: 1382.6217208065177\n",
      "Epoch 2 step 547: training accuarcy: 0.7055\n",
      "Epoch 2 step 547: training loss: 1382.3038080249248\n",
      "Epoch 2 step 548: training accuarcy: 0.7145\n",
      "Epoch 2 step 548: training loss: 1382.7017177726773\n",
      "Epoch 2 step 549: training accuarcy: 0.7175\n",
      "Epoch 2 step 549: training loss: 1382.1393434131985\n",
      "Epoch 2 step 550: training accuarcy: 0.7185\n",
      "Epoch 2 step 550: training loss: 1382.8644835368923\n",
      "Epoch 2 step 551: training accuarcy: 0.709\n",
      "Epoch 2 step 551: training loss: 1383.0499767099543\n",
      "Epoch 2 step 552: training accuarcy: 0.6945\n",
      "Epoch 2 step 552: training loss: 1382.7743284555895\n",
      "Epoch 2 step 553: training accuarcy: 0.715\n",
      "Epoch 2 step 553: training loss: 1382.1317963906006\n",
      "Epoch 2 step 554: training accuarcy: 0.7295\n",
      "Epoch 2 step 554: training loss: 1382.9600050890813\n",
      "Epoch 2 step 555: training accuarcy: 0.7115\n",
      "Epoch 2 step 555: training loss: 1380.7057692106812\n",
      "Epoch 2 step 556: training accuarcy: 0.7165\n",
      "Epoch 2 step 556: training loss: 1381.8868243608506\n",
      "Epoch 2 step 557: training accuarcy: 0.7025\n",
      "Epoch 2 step 557: training loss: 1382.0466368745497\n",
      "Epoch 2 step 558: training accuarcy: 0.7125\n",
      "Epoch 2 step 558: training loss: 1382.0817199329545\n",
      "Epoch 2 step 559: training accuarcy: 0.705\n",
      "Epoch 2 step 559: training loss: 1382.039657444853\n",
      "Epoch 2 step 560: training accuarcy: 0.707\n",
      "Epoch 2 step 560: training loss: 1382.9732657471932\n",
      "Epoch 2 step 561: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 561: training loss: 1382.456950194924\n",
      "Epoch 2 step 562: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 562: training loss: 1382.77142043073\n",
      "Epoch 2 step 563: training accuarcy: 0.7045\n",
      "Epoch 2 step 563: training loss: 1382.6063736406925\n",
      "Epoch 2 step 564: training accuarcy: 0.711\n",
      "Epoch 2 step 564: training loss: 1383.1295143213936\n",
      "Epoch 2 step 565: training accuarcy: 0.7025\n",
      "Epoch 2 step 565: training loss: 1382.5614031895532\n",
      "Epoch 2 step 566: training accuarcy: 0.7025\n",
      "Epoch 2 step 566: training loss: 1382.16583469489\n",
      "Epoch 2 step 567: training accuarcy: 0.71\n",
      "Epoch 2 step 567: training loss: 1382.887021447832\n",
      "Epoch 2 step 568: training accuarcy: 0.706\n",
      "Epoch 2 step 568: training loss: 1382.3241982947836\n",
      "Epoch 2 step 569: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 569: training loss: 1382.975910044699\n",
      "Epoch 2 step 570: training accuarcy: 0.6895\n",
      "Epoch 2 step 570: training loss: 1381.8256193225163\n",
      "Epoch 2 step 571: training accuarcy: 0.715\n",
      "Epoch 2 step 571: training loss: 1382.6514640708965\n",
      "Epoch 2 step 572: training accuarcy: 0.7135\n",
      "Epoch 2 step 572: training loss: 1382.413119063012\n",
      "Epoch 2 step 573: training accuarcy: 0.7005\n",
      "Epoch 2 step 573: training loss: 1382.3735657687503\n",
      "Epoch 2 step 574: training accuarcy: 0.708\n",
      "Epoch 2 step 574: training loss: 1382.5207195749767\n",
      "Epoch 2 step 575: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 575: training loss: 1383.2660588880467\n",
      "Epoch 2 step 576: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 576: training loss: 1382.0294121429442\n",
      "Epoch 2 step 577: training accuarcy: 0.7145\n",
      "Epoch 2 step 577: training loss: 1381.8400538532444\n",
      "Epoch 2 step 578: training accuarcy: 0.7105\n",
      "Epoch 2 step 578: training loss: 1382.6353533377367\n",
      "Epoch 2 step 579: training accuarcy: 0.6985\n",
      "Epoch 2 step 579: training loss: 1382.5802660502836\n",
      "Epoch 2 step 580: training accuarcy: 0.712\n",
      "Epoch 2 step 580: training loss: 1382.8443507822872\n",
      "Epoch 2 step 581: training accuarcy: 0.7025\n",
      "Epoch 2 step 581: training loss: 1382.743059209935\n",
      "Epoch 2 step 582: training accuarcy: 0.7155\n",
      "Epoch 2 step 582: training loss: 1382.1422385312494\n",
      "Epoch 2 step 583: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 583: training loss: 1382.5638950477844\n",
      "Epoch 2 step 584: training accuarcy: 0.7135\n",
      "Epoch 2 step 584: training loss: 1381.6426126380825\n",
      "Epoch 2 step 585: training accuarcy: 0.706\n",
      "Epoch 2 step 585: training loss: 1382.1235057687545\n",
      "Epoch 2 step 586: training accuarcy: 0.6985\n",
      "Epoch 2 step 586: training loss: 1381.985552313317\n",
      "Epoch 2 step 587: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 587: training loss: 1381.5211724574442\n",
      "Epoch 2 step 588: training accuarcy: 0.7215\n",
      "Epoch 2 step 588: training loss: 1383.3973662896042\n",
      "Epoch 2 step 589: training accuarcy: 0.706\n",
      "Epoch 2 step 589: training loss: 1382.9553879613293\n",
      "Epoch 2 step 590: training accuarcy: 0.7195\n",
      "Epoch 2 step 590: training loss: 1383.1469015628634\n",
      "Epoch 2 step 591: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 591: training loss: 1382.386559732936\n",
      "Epoch 2 step 592: training accuarcy: 0.7065\n",
      "Epoch 2 step 592: training loss: 1382.7432123827264\n",
      "Epoch 2 step 593: training accuarcy: 0.6915\n",
      "Epoch 2 step 593: training loss: 1383.4450026999712\n",
      "Epoch 2 step 594: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 594: training loss: 1382.7666111388662\n",
      "Epoch 2 step 595: training accuarcy: 0.7085\n",
      "Epoch 2 step 595: training loss: 1383.2883549794415\n",
      "Epoch 2 step 596: training accuarcy: 0.6975\n",
      "Epoch 2 step 596: training loss: 1382.6502074193475\n",
      "Epoch 2 step 597: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 597: training loss: 1383.322300938248\n",
      "Epoch 2 step 598: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 598: training loss: 1383.1889638852049\n",
      "Epoch 2 step 599: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 599: training loss: 1382.8534706634816\n",
      "Epoch 2 step 600: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 600: training loss: 1382.1403959761751\n",
      "Epoch 2 step 601: training accuarcy: 0.7205\n",
      "Epoch 2 step 601: training loss: 1382.6137563244217\n",
      "Epoch 2 step 602: training accuarcy: 0.6995\n",
      "Epoch 2 step 602: training loss: 1382.5339464218903\n",
      "Epoch 2 step 603: training accuarcy: 0.7185\n",
      "Epoch 2 step 603: training loss: 1383.6565471048868\n",
      "Epoch 2 step 604: training accuarcy: 0.687\n",
      "Epoch 2 step 604: training loss: 1383.6155928696694\n",
      "Epoch 2 step 605: training accuarcy: 0.6785\n",
      "Epoch 2 step 605: training loss: 1382.4643095977071\n",
      "Epoch 2 step 606: training accuarcy: 0.712\n",
      "Epoch 2 step 606: training loss: 1382.0833769192068\n",
      "Epoch 2 step 607: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 607: training loss: 1383.1045825302858\n",
      "Epoch 2 step 608: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 608: training loss: 1382.3655671648744\n",
      "Epoch 2 step 609: training accuarcy: 0.6900000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 609: training loss: 1384.0077439191318\n",
      "Epoch 2 step 610: training accuarcy: 0.681\n",
      "Epoch 2 step 610: training loss: 1383.333358696333\n",
      "Epoch 2 step 611: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 611: training loss: 1382.9604646548516\n",
      "Epoch 2 step 612: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 612: training loss: 1382.1609066388241\n",
      "Epoch 2 step 613: training accuarcy: 0.705\n",
      "Epoch 2 step 613: training loss: 1382.6911006912296\n",
      "Epoch 2 step 614: training accuarcy: 0.6995\n",
      "Epoch 2 step 614: training loss: 1383.4066355362904\n",
      "Epoch 2 step 615: training accuarcy: 0.707\n",
      "Epoch 2 step 615: training loss: 1381.4360403452197\n",
      "Epoch 2 step 616: training accuarcy: 0.7195\n",
      "Epoch 2 step 616: training loss: 1382.7716758911672\n",
      "Epoch 2 step 617: training accuarcy: 0.715\n",
      "Epoch 2 step 617: training loss: 1382.7695450744252\n",
      "Epoch 2 step 618: training accuarcy: 0.712\n",
      "Epoch 2 step 618: training loss: 1383.9049502025598\n",
      "Epoch 2 step 619: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 619: training loss: 1382.8561509041651\n",
      "Epoch 2 step 620: training accuarcy: 0.6905\n",
      "Epoch 2 step 620: training loss: 1383.9467310454277\n",
      "Epoch 2 step 621: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 621: training loss: 1383.438825777874\n",
      "Epoch 2 step 622: training accuarcy: 0.6945\n",
      "Epoch 2 step 622: training loss: 1383.0388627812924\n",
      "Epoch 2 step 623: training accuarcy: 0.687\n",
      "Epoch 2 step 623: training loss: 1382.7839710469343\n",
      "Epoch 2 step 624: training accuarcy: 0.6975\n",
      "Epoch 2 step 624: training loss: 1382.5185883554448\n",
      "Epoch 2 step 625: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 625: training loss: 1383.2299663341803\n",
      "Epoch 2 step 626: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 626: training loss: 1381.7158049788698\n",
      "Epoch 2 step 627: training accuarcy: 0.709\n",
      "Epoch 2 step 627: training loss: 1383.4066965292773\n",
      "Epoch 2 step 628: training accuarcy: 0.682\n",
      "Epoch 2 step 628: training loss: 1382.0366631340364\n",
      "Epoch 2 step 629: training accuarcy: 0.718\n",
      "Epoch 2 step 629: training loss: 1383.477557043961\n",
      "Epoch 2 step 630: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 630: training loss: 1382.2661696331543\n",
      "Epoch 2 step 631: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 631: training loss: 1382.9625735816937\n",
      "Epoch 2 step 632: training accuarcy: 0.6955\n",
      "Epoch 2 step 632: training loss: 1382.5169155251008\n",
      "Epoch 2 step 633: training accuarcy: 0.7125\n",
      "Epoch 2 step 633: training loss: 1382.723513317283\n",
      "Epoch 2 step 634: training accuarcy: 0.706\n",
      "Epoch 2 step 634: training loss: 1382.63985143381\n",
      "Epoch 2 step 635: training accuarcy: 0.71\n",
      "Epoch 2 step 635: training loss: 1383.184724507169\n",
      "Epoch 2 step 636: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 636: training loss: 1383.0448937385247\n",
      "Epoch 2 step 637: training accuarcy: 0.6885\n",
      "Epoch 2 step 637: training loss: 1383.111499028473\n",
      "Epoch 2 step 638: training accuarcy: 0.7115\n",
      "Epoch 2 step 638: training loss: 1383.422072297219\n",
      "Epoch 2 step 639: training accuarcy: 0.6895\n",
      "Epoch 2 step 639: training loss: 1381.909137281159\n",
      "Epoch 2 step 640: training accuarcy: 0.7025\n",
      "Epoch 2 step 640: training loss: 1383.3857015808896\n",
      "Epoch 2 step 641: training accuarcy: 0.6905\n",
      "Epoch 2 step 641: training loss: 1381.994210211764\n",
      "Epoch 2 step 642: training accuarcy: 0.6955\n",
      "Epoch 2 step 642: training loss: 1382.7115735933162\n",
      "Epoch 2 step 643: training accuarcy: 0.712\n",
      "Epoch 2 step 643: training loss: 1382.1240608224539\n",
      "Epoch 2 step 644: training accuarcy: 0.7005\n",
      "Epoch 2 step 644: training loss: 1382.6640700115422\n",
      "Epoch 2 step 645: training accuarcy: 0.7025\n",
      "Epoch 2 step 645: training loss: 1383.3575392864202\n",
      "Epoch 2 step 646: training accuarcy: 0.6855\n",
      "Epoch 2 step 646: training loss: 1383.1232837158523\n",
      "Epoch 2 step 647: training accuarcy: 0.6815\n",
      "Epoch 2 step 647: training loss: 1383.3870299829782\n",
      "Epoch 2 step 648: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 648: training loss: 1382.3930390341638\n",
      "Epoch 2 step 649: training accuarcy: 0.7065\n",
      "Epoch 2 step 649: training loss: 1382.713366768408\n",
      "Epoch 2 step 650: training accuarcy: 0.712\n",
      "Epoch 2 step 650: training loss: 1383.2375300458143\n",
      "Epoch 2 step 651: training accuarcy: 0.676\n",
      "Epoch 2 step 651: training loss: 1382.4881397677277\n",
      "Epoch 2 step 652: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 652: training loss: 1383.2343907496474\n",
      "Epoch 2 step 653: training accuarcy: 0.7135\n",
      "Epoch 2 step 653: training loss: 1383.0278906005592\n",
      "Epoch 2 step 654: training accuarcy: 0.6905\n",
      "Epoch 2 step 654: training loss: 1382.2635997499228\n",
      "Epoch 2 step 655: training accuarcy: 0.7055\n",
      "Epoch 2 step 655: training loss: 1382.266277434648\n",
      "Epoch 2 step 656: training accuarcy: 0.7075\n",
      "Epoch 2 step 656: training loss: 1381.5856212028843\n",
      "Epoch 2 step 657: training accuarcy: 0.724\n",
      "Epoch 2 step 657: training loss: 1383.784060053382\n",
      "Epoch 2 step 658: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 658: training loss: 1382.7713257827538\n",
      "Epoch 2 step 659: training accuarcy: 0.686\n",
      "Epoch 2 step 659: training loss: 1382.4929338972327\n",
      "Epoch 2 step 660: training accuarcy: 0.6985\n",
      "Epoch 2 step 660: training loss: 1383.0698699382676\n",
      "Epoch 2 step 661: training accuarcy: 0.6985\n",
      "Epoch 2 step 661: training loss: 1383.068184715993\n",
      "Epoch 2 step 662: training accuarcy: 0.6905\n",
      "Epoch 2 step 662: training loss: 1382.786669378819\n",
      "Epoch 2 step 663: training accuarcy: 0.6935\n",
      "Epoch 2 step 663: training loss: 1381.8544002856252\n",
      "Epoch 2 step 664: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 664: training loss: 1382.9262153634384\n",
      "Epoch 2 step 665: training accuarcy: 0.712\n",
      "Epoch 2 step 665: training loss: 1383.6546176590357\n",
      "Epoch 2 step 666: training accuarcy: 0.6805\n",
      "Epoch 2 step 666: training loss: 1382.853900414878\n",
      "Epoch 2 step 667: training accuarcy: 0.7075\n",
      "Epoch 2 step 667: training loss: 1383.4277721997291\n",
      "Epoch 2 step 668: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 668: training loss: 1382.2751378680553\n",
      "Epoch 2 step 669: training accuarcy: 0.687\n",
      "Epoch 2 step 669: training loss: 1382.118242463249\n",
      "Epoch 2 step 670: training accuarcy: 0.6845\n",
      "Epoch 2 step 670: training loss: 1383.2660212075289\n",
      "Epoch 2 step 671: training accuarcy: 0.6825\n",
      "Epoch 2 step 671: training loss: 1382.862946697054\n",
      "Epoch 2 step 672: training accuarcy: 0.6745\n",
      "Epoch 2 step 672: training loss: 1382.7569647408793\n",
      "Epoch 2 step 673: training accuarcy: 0.712\n",
      "Epoch 2 step 673: training loss: 1383.720277489204\n",
      "Epoch 2 step 674: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 674: training loss: 1382.314015513571\n",
      "Epoch 2 step 675: training accuarcy: 0.6945\n",
      "Epoch 2 step 675: training loss: 1383.6231542676721\n",
      "Epoch 2 step 676: training accuarcy: 0.6835\n",
      "Epoch 2 step 676: training loss: 1383.3245579038862\n",
      "Epoch 2 step 677: training accuarcy: 0.6985\n",
      "Epoch 2 step 677: training loss: 1382.8810695517268\n",
      "Epoch 2 step 678: training accuarcy: 0.6975\n",
      "Epoch 2 step 678: training loss: 1383.101287125229\n",
      "Epoch 2 step 679: training accuarcy: 0.6965\n",
      "Epoch 2 step 679: training loss: 1382.5151985538644\n",
      "Epoch 2 step 680: training accuarcy: 0.6905\n",
      "Epoch 2 step 680: training loss: 1382.5699876742058\n",
      "Epoch 2 step 681: training accuarcy: 0.7015\n",
      "Epoch 2 step 681: training loss: 1381.4214142330593\n",
      "Epoch 2 step 682: training accuarcy: 0.7115\n",
      "Epoch 2 step 682: training loss: 1380.981620048131\n",
      "Epoch 2 step 683: training accuarcy: 0.7155\n",
      "Epoch 2 step 683: training loss: 1383.8919607481384\n",
      "Epoch 2 step 684: training accuarcy: 0.6965\n",
      "Epoch 2 step 684: training loss: 1382.1721094873133\n",
      "Epoch 2 step 685: training accuarcy: 0.6955\n",
      "Epoch 2 step 685: training loss: 1382.5716028576571\n",
      "Epoch 2 step 686: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 686: training loss: 1382.2922710722569\n",
      "Epoch 2 step 687: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 687: training loss: 1381.9497010805062\n",
      "Epoch 2 step 688: training accuarcy: 0.7115\n",
      "Epoch 2 step 688: training loss: 1382.9749180794513\n",
      "Epoch 2 step 689: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 689: training loss: 1381.7073314019133\n",
      "Epoch 2 step 690: training accuarcy: 0.712\n",
      "Epoch 2 step 690: training loss: 1383.5030606348982\n",
      "Epoch 2 step 691: training accuarcy: 0.6965\n",
      "Epoch 2 step 691: training loss: 1383.700451284412\n",
      "Epoch 2 step 692: training accuarcy: 0.6925\n",
      "Epoch 2 step 692: training loss: 1382.7512948769697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 693: training accuarcy: 0.684\n",
      "Epoch 2 step 693: training loss: 1382.9571477016398\n",
      "Epoch 2 step 694: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 694: training loss: 1383.61796920351\n",
      "Epoch 2 step 695: training accuarcy: 0.7055\n",
      "Epoch 2 step 695: training loss: 1384.1448688496243\n",
      "Epoch 2 step 696: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 696: training loss: 1383.276184700827\n",
      "Epoch 2 step 697: training accuarcy: 0.6925\n",
      "Epoch 2 step 697: training loss: 1382.7589471041933\n",
      "Epoch 2 step 698: training accuarcy: 0.6995\n",
      "Epoch 2 step 698: training loss: 1382.4871790511083\n",
      "Epoch 2 step 699: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 699: training loss: 1383.3348101030208\n",
      "Epoch 2 step 700: training accuarcy: 0.6835\n",
      "Epoch 2 step 700: training loss: 1382.8498780024174\n",
      "Epoch 2 step 701: training accuarcy: 0.6895\n",
      "Epoch 2 step 701: training loss: 1383.3917578101277\n",
      "Epoch 2 step 702: training accuarcy: 0.6975\n",
      "Epoch 2 step 702: training loss: 1382.619333216925\n",
      "Epoch 2 step 703: training accuarcy: 0.719\n",
      "Epoch 2 step 703: training loss: 1382.0572340991391\n",
      "Epoch 2 step 704: training accuarcy: 0.716\n",
      "Epoch 2 step 704: training loss: 1383.6900303685347\n",
      "Epoch 2 step 705: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 705: training loss: 1383.483009930662\n",
      "Epoch 2 step 706: training accuarcy: 0.685\n",
      "Epoch 2 step 706: training loss: 1382.5487549335614\n",
      "Epoch 2 step 707: training accuarcy: 0.7045\n",
      "Epoch 2 step 707: training loss: 1383.2792872532432\n",
      "Epoch 2 step 708: training accuarcy: 0.6915\n",
      "Epoch 2 step 708: training loss: 1383.630087984506\n",
      "Epoch 2 step 709: training accuarcy: 0.674\n",
      "Epoch 2 step 709: training loss: 1383.621593057755\n",
      "Epoch 2 step 710: training accuarcy: 0.687\n",
      "Epoch 2 step 710: training loss: 1383.3854446729404\n",
      "Epoch 2 step 711: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 711: training loss: 1383.9101734286771\n",
      "Epoch 2 step 712: training accuarcy: 0.706\n",
      "Epoch 2 step 712: training loss: 1383.047037770683\n",
      "Epoch 2 step 713: training accuarcy: 0.6915\n",
      "Epoch 2 step 713: training loss: 1383.0945234712012\n",
      "Epoch 2 step 714: training accuarcy: 0.6885\n",
      "Epoch 2 step 714: training loss: 1382.2167168285225\n",
      "Epoch 2 step 715: training accuarcy: 0.7015\n",
      "Epoch 2 step 715: training loss: 1381.8566023919784\n",
      "Epoch 2 step 716: training accuarcy: 0.7055\n",
      "Epoch 2 step 716: training loss: 1383.8855838021896\n",
      "Epoch 2 step 717: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 717: training loss: 1382.930676277873\n",
      "Epoch 2 step 718: training accuarcy: 0.7015\n",
      "Epoch 2 step 718: training loss: 1382.6663793554126\n",
      "Epoch 2 step 719: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 719: training loss: 1382.630164400065\n",
      "Epoch 2 step 720: training accuarcy: 0.6945\n",
      "Epoch 2 step 720: training loss: 1383.85654167295\n",
      "Epoch 2 step 721: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 721: training loss: 1382.438181862424\n",
      "Epoch 2 step 722: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 722: training loss: 1383.1242114773725\n",
      "Epoch 2 step 723: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 723: training loss: 1382.0747549598202\n",
      "Epoch 2 step 724: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 724: training loss: 1383.0417988383463\n",
      "Epoch 2 step 725: training accuarcy: 0.706\n",
      "Epoch 2 step 725: training loss: 1382.6065320441373\n",
      "Epoch 2 step 726: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 726: training loss: 1382.2663111101717\n",
      "Epoch 2 step 727: training accuarcy: 0.7005\n",
      "Epoch 2 step 727: training loss: 1383.1156000194678\n",
      "Epoch 2 step 728: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 728: training loss: 1383.8236327660363\n",
      "Epoch 2 step 729: training accuarcy: 0.683\n",
      "Epoch 2 step 729: training loss: 1383.1849110341807\n",
      "Epoch 2 step 730: training accuarcy: 0.7115\n",
      "Epoch 2 step 730: training loss: 1382.8812223086727\n",
      "Epoch 2 step 731: training accuarcy: 0.7045\n",
      "Epoch 2 step 731: training loss: 1382.8431161375784\n",
      "Epoch 2 step 732: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 732: training loss: 1382.7289178267165\n",
      "Epoch 2 step 733: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 733: training loss: 1382.8594955317835\n",
      "Epoch 2 step 734: training accuarcy: 0.7025\n",
      "Epoch 2 step 734: training loss: 1383.5147204093903\n",
      "Epoch 2 step 735: training accuarcy: 0.6915\n",
      "Epoch 2 step 735: training loss: 1383.2558878275997\n",
      "Epoch 2 step 736: training accuarcy: 0.6785\n",
      "Epoch 2 step 736: training loss: 1382.389350707132\n",
      "Epoch 2 step 737: training accuarcy: 0.7095\n",
      "Epoch 2 step 737: training loss: 1383.3341908081356\n",
      "Epoch 2 step 738: training accuarcy: 0.679\n",
      "Epoch 2 step 738: training loss: 1383.7273234033594\n",
      "Epoch 2 step 739: training accuarcy: 0.6915\n",
      "Epoch 2 step 739: training loss: 1383.2129462259454\n",
      "Epoch 2 step 740: training accuarcy: 0.6855\n",
      "Epoch 2 step 740: training loss: 1383.1708572122272\n",
      "Epoch 2 step 741: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 741: training loss: 1381.9123069285404\n",
      "Epoch 2 step 742: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 742: training loss: 1382.42980147772\n",
      "Epoch 2 step 743: training accuarcy: 0.6945\n",
      "Epoch 2 step 743: training loss: 1383.8527552721646\n",
      "Epoch 2 step 744: training accuarcy: 0.6865\n",
      "Epoch 2 step 744: training loss: 1382.4811837693405\n",
      "Epoch 2 step 745: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 745: training loss: 1383.3403428957913\n",
      "Epoch 2 step 746: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 746: training loss: 1383.160598610122\n",
      "Epoch 2 step 747: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 747: training loss: 1381.4677640059065\n",
      "Epoch 2 step 748: training accuarcy: 0.7195\n",
      "Epoch 2 step 748: training loss: 1382.3217801705139\n",
      "Epoch 2 step 749: training accuarcy: 0.7005\n",
      "Epoch 2 step 749: training loss: 1383.0317836312995\n",
      "Epoch 2 step 750: training accuarcy: 0.7015\n",
      "Epoch 2 step 750: training loss: 1383.3715602547131\n",
      "Epoch 2 step 751: training accuarcy: 0.6935\n",
      "Epoch 2 step 751: training loss: 1382.3214357831785\n",
      "Epoch 2 step 752: training accuarcy: 0.708\n",
      "Epoch 2 step 752: training loss: 1382.4191779987086\n",
      "Epoch 2 step 753: training accuarcy: 0.7075\n",
      "Epoch 2 step 753: training loss: 1382.9380300031105\n",
      "Epoch 2 step 754: training accuarcy: 0.708\n",
      "Epoch 2 step 754: training loss: 1382.2137277395955\n",
      "Epoch 2 step 755: training accuarcy: 0.7065\n",
      "Epoch 2 step 755: training loss: 1382.201914099856\n",
      "Epoch 2 step 756: training accuarcy: 0.713\n",
      "Epoch 2 step 756: training loss: 1382.7409996478539\n",
      "Epoch 2 step 757: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 757: training loss: 1382.6704730866174\n",
      "Epoch 2 step 758: training accuarcy: 0.681\n",
      "Epoch 2 step 758: training loss: 1382.27105609461\n",
      "Epoch 2 step 759: training accuarcy: 0.7085\n",
      "Epoch 2 step 759: training loss: 1383.3237375352458\n",
      "Epoch 2 step 760: training accuarcy: 0.6905\n",
      "Epoch 2 step 760: training loss: 1383.0659820845704\n",
      "Epoch 2 step 761: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 761: training loss: 1383.4517798317593\n",
      "Epoch 2 step 762: training accuarcy: 0.7015\n",
      "Epoch 2 step 762: training loss: 1383.4246678550528\n",
      "Epoch 2 step 763: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 763: training loss: 1383.5126520810177\n",
      "Epoch 2 step 764: training accuarcy: 0.6895\n",
      "Epoch 2 step 764: training loss: 1382.9494004562514\n",
      "Epoch 2 step 765: training accuarcy: 0.6885\n",
      "Epoch 2 step 765: training loss: 1382.730898133878\n",
      "Epoch 2 step 766: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 766: training loss: 1382.795943056829\n",
      "Epoch 2 step 767: training accuarcy: 0.6995\n",
      "Epoch 2 step 767: training loss: 1383.3281892671148\n",
      "Epoch 2 step 768: training accuarcy: 0.6765\n",
      "Epoch 2 step 768: training loss: 1383.5152004113004\n",
      "Epoch 2 step 769: training accuarcy: 0.6985\n",
      "Epoch 2 step 769: training loss: 1382.8706314517642\n",
      "Epoch 2 step 770: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 770: training loss: 1383.0860897992056\n",
      "Epoch 2 step 771: training accuarcy: 0.6875\n",
      "Epoch 2 step 771: training loss: 1383.68985399313\n",
      "Epoch 2 step 772: training accuarcy: 0.6900000000000001\n",
      "Epoch 2 step 772: training loss: 1383.338439452954\n",
      "Epoch 2 step 773: training accuarcy: 0.6865\n",
      "Epoch 2 step 773: training loss: 1383.4581148854913\n",
      "Epoch 2 step 774: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 774: training loss: 1383.7470470966116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 775: training accuarcy: 0.7075\n",
      "Epoch 2 step 775: training loss: 1383.2530691823151\n",
      "Epoch 2 step 776: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 776: training loss: 1383.3415074586235\n",
      "Epoch 2 step 777: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 777: training loss: 1382.5645377528733\n",
      "Epoch 2 step 778: training accuarcy: 0.7035\n",
      "Epoch 2 step 778: training loss: 1383.3859154186994\n",
      "Epoch 2 step 779: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 779: training loss: 1384.055620287605\n",
      "Epoch 2 step 780: training accuarcy: 0.685\n",
      "Epoch 2 step 780: training loss: 1383.277577422761\n",
      "Epoch 2 step 781: training accuarcy: 0.6955\n",
      "Epoch 2 step 781: training loss: 1382.9202428339286\n",
      "Epoch 2 step 782: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 782: training loss: 1382.89904629041\n",
      "Epoch 2 step 783: training accuarcy: 0.678\n",
      "Epoch 2 step 783: training loss: 1383.291107787693\n",
      "Epoch 2 step 784: training accuarcy: 0.6795\n",
      "Epoch 2 step 784: training loss: 1382.7444881588337\n",
      "Epoch 2 step 785: training accuarcy: 0.706\n",
      "Epoch 2 step 785: training loss: 1383.4339932469034\n",
      "Epoch 2 step 786: training accuarcy: 0.7005\n",
      "Epoch 2 step 786: training loss: 1382.6893081396443\n",
      "Epoch 2 step 787: training accuarcy: 0.6950000000000001\n",
      "Epoch 2 step 787: training loss: 1382.6328515780692\n",
      "Epoch 2 step 788: training accuarcy: 0.6905\n",
      "Epoch 2 step 788: training loss: 542.4420208258769\n",
      "Epoch 2 step 789: training accuarcy: 0.7128205128205128\n",
      "Epoch 2: train loss 1379.5709567444717, train accuarcy 0.7031528353691101\n",
      "Epoch 2: valid loss 1363.9940319445864, valid accuarcy 0.7110369801521301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████                                                                                               | 3/8 [05:42<09:34, 114.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 1382.6948344196605\n",
      "Epoch 3 step 790: training accuarcy: 0.7185\n",
      "Epoch 3 step 790: training loss: 1381.2702644229284\n",
      "Epoch 3 step 791: training accuarcy: 0.7195\n",
      "Epoch 3 step 791: training loss: 1382.443284264446\n",
      "Epoch 3 step 792: training accuarcy: 0.7085\n",
      "Epoch 3 step 792: training loss: 1382.2035946184014\n",
      "Epoch 3 step 793: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 793: training loss: 1382.3513427074772\n",
      "Epoch 3 step 794: training accuarcy: 0.7025\n",
      "Epoch 3 step 794: training loss: 1380.8759247979915\n",
      "Epoch 3 step 795: training accuarcy: 0.72\n",
      "Epoch 3 step 795: training loss: 1381.8447831274518\n",
      "Epoch 3 step 796: training accuarcy: 0.7085\n",
      "Epoch 3 step 796: training loss: 1381.9371576360477\n",
      "Epoch 3 step 797: training accuarcy: 0.7055\n",
      "Epoch 3 step 797: training loss: 1381.8638093092452\n",
      "Epoch 3 step 798: training accuarcy: 0.73\n",
      "Epoch 3 step 798: training loss: 1381.8935691504855\n",
      "Epoch 3 step 799: training accuarcy: 0.707\n",
      "Epoch 3 step 799: training loss: 1383.0062404125374\n",
      "Epoch 3 step 800: training accuarcy: 0.6995\n",
      "Epoch 3 step 800: training loss: 1381.4162796636977\n",
      "Epoch 3 step 801: training accuarcy: 0.715\n",
      "Epoch 3 step 801: training loss: 1382.3762408700686\n",
      "Epoch 3 step 802: training accuarcy: 0.7085\n",
      "Epoch 3 step 802: training loss: 1382.436729670018\n",
      "Epoch 3 step 803: training accuarcy: 0.706\n",
      "Epoch 3 step 803: training loss: 1382.4072033791067\n",
      "Epoch 3 step 804: training accuarcy: 0.7215\n",
      "Epoch 3 step 804: training loss: 1381.5627637215528\n",
      "Epoch 3 step 805: training accuarcy: 0.718\n",
      "Epoch 3 step 805: training loss: 1381.9471944333077\n",
      "Epoch 3 step 806: training accuarcy: 0.7055\n",
      "Epoch 3 step 806: training loss: 1383.290355948219\n",
      "Epoch 3 step 807: training accuarcy: 0.6940000000000001\n",
      "Epoch 3 step 807: training loss: 1382.4786666844168\n",
      "Epoch 3 step 808: training accuarcy: 0.7045\n",
      "Epoch 3 step 808: training loss: 1381.993489265633\n",
      "Epoch 3 step 809: training accuarcy: 0.7135\n",
      "Epoch 3 step 809: training loss: 1382.0114748255169\n",
      "Epoch 3 step 810: training accuarcy: 0.7195\n",
      "Epoch 3 step 810: training loss: 1382.3311061714398\n",
      "Epoch 3 step 811: training accuarcy: 0.724\n",
      "Epoch 3 step 811: training loss: 1382.3417188313147\n",
      "Epoch 3 step 812: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 812: training loss: 1382.059928287919\n",
      "Epoch 3 step 813: training accuarcy: 0.7045\n",
      "Epoch 3 step 813: training loss: 1382.232057368389\n",
      "Epoch 3 step 814: training accuarcy: 0.716\n",
      "Epoch 3 step 814: training loss: 1382.1104185849138\n",
      "Epoch 3 step 815: training accuarcy: 0.711\n",
      "Epoch 3 step 815: training loss: 1382.9875853461879\n",
      "Epoch 3 step 816: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 816: training loss: 1381.952071188307\n",
      "Epoch 3 step 817: training accuarcy: 0.71\n",
      "Epoch 3 step 817: training loss: 1383.6543610686706\n",
      "Epoch 3 step 818: training accuarcy: 0.6915\n",
      "Epoch 3 step 818: training loss: 1382.4889731995065\n",
      "Epoch 3 step 819: training accuarcy: 0.7175\n",
      "Epoch 3 step 819: training loss: 1383.0124783187866\n",
      "Epoch 3 step 820: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 820: training loss: 1382.3075830426314\n",
      "Epoch 3 step 821: training accuarcy: 0.7035\n",
      "Epoch 3 step 821: training loss: 1382.1610015782426\n",
      "Epoch 3 step 822: training accuarcy: 0.6955\n",
      "Epoch 3 step 822: training loss: 1381.5979413878613\n",
      "Epoch 3 step 823: training accuarcy: 0.7205\n",
      "Epoch 3 step 823: training loss: 1381.702157244927\n",
      "Epoch 3 step 824: training accuarcy: 0.7125\n",
      "Epoch 3 step 824: training loss: 1381.9592829796295\n",
      "Epoch 3 step 825: training accuarcy: 0.709\n",
      "Epoch 3 step 825: training loss: 1382.9541043046072\n",
      "Epoch 3 step 826: training accuarcy: 0.6965\n",
      "Epoch 3 step 826: training loss: 1382.8674739008268\n",
      "Epoch 3 step 827: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 827: training loss: 1382.0412816992114\n",
      "Epoch 3 step 828: training accuarcy: 0.7025\n",
      "Epoch 3 step 828: training loss: 1381.2472111904608\n",
      "Epoch 3 step 829: training accuarcy: 0.7125\n",
      "Epoch 3 step 829: training loss: 1382.6269347032437\n",
      "Epoch 3 step 830: training accuarcy: 0.6815\n",
      "Epoch 3 step 830: training loss: 1383.4070396797306\n",
      "Epoch 3 step 831: training accuarcy: 0.7065\n",
      "Epoch 3 step 831: training loss: 1382.2561785881005\n",
      "Epoch 3 step 832: training accuarcy: 0.7135\n",
      "Epoch 3 step 832: training loss: 1381.9604599266534\n",
      "Epoch 3 step 833: training accuarcy: 0.7115\n",
      "Epoch 3 step 833: training loss: 1382.824231791628\n",
      "Epoch 3 step 834: training accuarcy: 0.7095\n",
      "Epoch 3 step 834: training loss: 1382.609160976341\n",
      "Epoch 3 step 835: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 835: training loss: 1382.6684239156696\n",
      "Epoch 3 step 836: training accuarcy: 0.7105\n",
      "Epoch 3 step 836: training loss: 1382.0486798001384\n",
      "Epoch 3 step 837: training accuarcy: 0.705\n",
      "Epoch 3 step 837: training loss: 1382.7273965976656\n",
      "Epoch 3 step 838: training accuarcy: 0.7035\n",
      "Epoch 3 step 838: training loss: 1382.2302404949387\n",
      "Epoch 3 step 839: training accuarcy: 0.687\n",
      "Epoch 3 step 839: training loss: 1381.197310886291\n",
      "Epoch 3 step 840: training accuarcy: 0.7075\n",
      "Epoch 3 step 840: training loss: 1382.6020572156374\n",
      "Epoch 3 step 841: training accuarcy: 0.7035\n",
      "Epoch 3 step 841: training loss: 1382.2088060722533\n",
      "Epoch 3 step 842: training accuarcy: 0.6975\n",
      "Epoch 3 step 842: training loss: 1381.3724331204344\n",
      "Epoch 3 step 843: training accuarcy: 0.711\n",
      "Epoch 3 step 843: training loss: 1382.568987908064\n",
      "Epoch 3 step 844: training accuarcy: 0.715\n",
      "Epoch 3 step 844: training loss: 1383.0828829722655\n",
      "Epoch 3 step 845: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 845: training loss: 1382.3933649995513\n",
      "Epoch 3 step 846: training accuarcy: 0.6965\n",
      "Epoch 3 step 846: training loss: 1383.3986447841646\n",
      "Epoch 3 step 847: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 847: training loss: 1381.6851929097438\n",
      "Epoch 3 step 848: training accuarcy: 0.712\n",
      "Epoch 3 step 848: training loss: 1381.340428116216\n",
      "Epoch 3 step 849: training accuarcy: 0.7005\n",
      "Epoch 3 step 849: training loss: 1382.7914767758134\n",
      "Epoch 3 step 850: training accuarcy: 0.7105\n",
      "Epoch 3 step 850: training loss: 1382.1773204483495\n",
      "Epoch 3 step 851: training accuarcy: 0.7185\n",
      "Epoch 3 step 851: training loss: 1382.882129734424\n",
      "Epoch 3 step 852: training accuarcy: 0.6965\n",
      "Epoch 3 step 852: training loss: 1382.1621333747162\n",
      "Epoch 3 step 853: training accuarcy: 0.718\n",
      "Epoch 3 step 853: training loss: 1382.930451597094\n",
      "Epoch 3 step 854: training accuarcy: 0.6985\n",
      "Epoch 3 step 854: training loss: 1383.1818834668686\n",
      "Epoch 3 step 855: training accuarcy: 0.683\n",
      "Epoch 3 step 855: training loss: 1382.257239184821\n",
      "Epoch 3 step 856: training accuarcy: 0.7045\n",
      "Epoch 3 step 856: training loss: 1382.6727422820557\n",
      "Epoch 3 step 857: training accuarcy: 0.677\n",
      "Epoch 3 step 857: training loss: 1382.9925376344847\n",
      "Epoch 3 step 858: training accuarcy: 0.678\n",
      "Epoch 3 step 858: training loss: 1382.5490840218563\n",
      "Epoch 3 step 859: training accuarcy: 0.709\n",
      "Epoch 3 step 859: training loss: 1382.6675609807726\n",
      "Epoch 3 step 860: training accuarcy: 0.7015\n",
      "Epoch 3 step 860: training loss: 1382.5070443067787\n",
      "Epoch 3 step 861: training accuarcy: 0.7035\n",
      "Epoch 3 step 861: training loss: 1382.1839989179332\n",
      "Epoch 3 step 862: training accuarcy: 0.7065\n",
      "Epoch 3 step 862: training loss: 1383.6449131176553\n",
      "Epoch 3 step 863: training accuarcy: 0.6925\n",
      "Epoch 3 step 863: training loss: 1382.142767088508\n",
      "Epoch 3 step 864: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 864: training loss: 1382.9263970865952\n",
      "Epoch 3 step 865: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 865: training loss: 1383.0190394736767\n",
      "Epoch 3 step 866: training accuarcy: 0.716\n",
      "Epoch 3 step 866: training loss: 1382.1665529692684\n",
      "Epoch 3 step 867: training accuarcy: 0.7085\n",
      "Epoch 3 step 867: training loss: 1382.0191066840862\n",
      "Epoch 3 step 868: training accuarcy: 0.714\n",
      "Epoch 3 step 868: training loss: 1382.817919359855\n",
      "Epoch 3 step 869: training accuarcy: 0.7115\n",
      "Epoch 3 step 869: training loss: 1382.8252443617507\n",
      "Epoch 3 step 870: training accuarcy: 0.6910000000000001\n",
      "Epoch 3 step 870: training loss: 1383.1009888372498\n",
      "Epoch 3 step 871: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 871: training loss: 1382.8875609471527\n",
      "Epoch 3 step 872: training accuarcy: 0.7065\n",
      "Epoch 3 step 872: training loss: 1383.3484837776637\n",
      "Epoch 3 step 873: training accuarcy: 0.706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 873: training loss: 1383.0455499420316\n",
      "Epoch 3 step 874: training accuarcy: 0.6905\n",
      "Epoch 3 step 874: training loss: 1384.7291653067205\n",
      "Epoch 3 step 875: training accuarcy: 0.683\n",
      "Epoch 3 step 875: training loss: 1382.9634338978026\n",
      "Epoch 3 step 876: training accuarcy: 0.6885\n",
      "Epoch 3 step 876: training loss: 1382.3797467458207\n",
      "Epoch 3 step 877: training accuarcy: 0.7095\n",
      "Epoch 3 step 877: training loss: 1382.240119717867\n",
      "Epoch 3 step 878: training accuarcy: 0.6945\n",
      "Epoch 3 step 878: training loss: 1382.7587815712109\n",
      "Epoch 3 step 879: training accuarcy: 0.6985\n",
      "Epoch 3 step 879: training loss: 1381.8477590733555\n",
      "Epoch 3 step 880: training accuarcy: 0.6965\n",
      "Epoch 3 step 880: training loss: 1382.4198697183351\n",
      "Epoch 3 step 881: training accuarcy: 0.7025\n",
      "Epoch 3 step 881: training loss: 1382.6321639297348\n",
      "Epoch 3 step 882: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 882: training loss: 1382.6172944492027\n",
      "Epoch 3 step 883: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 883: training loss: 1383.5870150992982\n",
      "Epoch 3 step 884: training accuarcy: 0.6985\n",
      "Epoch 3 step 884: training loss: 1382.5334045866082\n",
      "Epoch 3 step 885: training accuarcy: 0.707\n",
      "Epoch 3 step 885: training loss: 1382.3182945264884\n",
      "Epoch 3 step 886: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 886: training loss: 1383.3637047290097\n",
      "Epoch 3 step 887: training accuarcy: 0.6910000000000001\n",
      "Epoch 3 step 887: training loss: 1382.8062242373903\n",
      "Epoch 3 step 888: training accuarcy: 0.6855\n",
      "Epoch 3 step 888: training loss: 1382.1092789190898\n",
      "Epoch 3 step 889: training accuarcy: 0.7105\n",
      "Epoch 3 step 889: training loss: 1383.111706193071\n",
      "Epoch 3 step 890: training accuarcy: 0.7065\n",
      "Epoch 3 step 890: training loss: 1382.919473950621\n",
      "Epoch 3 step 891: training accuarcy: 0.7035\n",
      "Epoch 3 step 891: training loss: 1382.9288396693055\n",
      "Epoch 3 step 892: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 892: training loss: 1383.069284320961\n",
      "Epoch 3 step 893: training accuarcy: 0.7085\n",
      "Epoch 3 step 893: training loss: 1382.5702774782987\n",
      "Epoch 3 step 894: training accuarcy: 0.6925\n",
      "Epoch 3 step 894: training loss: 1382.8958104422898\n",
      "Epoch 3 step 895: training accuarcy: 0.6925\n",
      "Epoch 3 step 895: training loss: 1382.9467039305675\n",
      "Epoch 3 step 896: training accuarcy: 0.6955\n",
      "Epoch 3 step 896: training loss: 1383.7964706655775\n",
      "Epoch 3 step 897: training accuarcy: 0.676\n",
      "Epoch 3 step 897: training loss: 1382.831668982114\n",
      "Epoch 3 step 898: training accuarcy: 0.6965\n",
      "Epoch 3 step 898: training loss: 1383.5518723279465\n",
      "Epoch 3 step 899: training accuarcy: 0.6905\n",
      "Epoch 3 step 899: training loss: 1382.5748076920156\n",
      "Epoch 3 step 900: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 900: training loss: 1381.9121720666358\n",
      "Epoch 3 step 901: training accuarcy: 0.6885\n",
      "Epoch 3 step 901: training loss: 1381.9833688320064\n",
      "Epoch 3 step 902: training accuarcy: 0.712\n",
      "Epoch 3 step 902: training loss: 1383.3229266510514\n",
      "Epoch 3 step 903: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 903: training loss: 1382.8253120779461\n",
      "Epoch 3 step 904: training accuarcy: 0.7025\n",
      "Epoch 3 step 904: training loss: 1383.0698134445126\n",
      "Epoch 3 step 905: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 905: training loss: 1382.436745936149\n",
      "Epoch 3 step 906: training accuarcy: 0.6935\n",
      "Epoch 3 step 906: training loss: 1383.0046554447372\n",
      "Epoch 3 step 907: training accuarcy: 0.6865\n",
      "Epoch 3 step 907: training loss: 1382.7420924974788\n",
      "Epoch 3 step 908: training accuarcy: 0.711\n",
      "Epoch 3 step 908: training loss: 1383.6867555468925\n",
      "Epoch 3 step 909: training accuarcy: 0.6765\n",
      "Epoch 3 step 909: training loss: 1383.5013546589141\n",
      "Epoch 3 step 910: training accuarcy: 0.6985\n",
      "Epoch 3 step 910: training loss: 1382.7549085233888\n",
      "Epoch 3 step 911: training accuarcy: 0.6995\n",
      "Epoch 3 step 911: training loss: 1383.1762268149928\n",
      "Epoch 3 step 912: training accuarcy: 0.7015\n",
      "Epoch 3 step 912: training loss: 1383.35775599615\n",
      "Epoch 3 step 913: training accuarcy: 0.6965\n",
      "Epoch 3 step 913: training loss: 1383.6947818989813\n",
      "Epoch 3 step 914: training accuarcy: 0.6955\n",
      "Epoch 3 step 914: training loss: 1382.5507375260213\n",
      "Epoch 3 step 915: training accuarcy: 0.712\n",
      "Epoch 3 step 915: training loss: 1383.6291750861424\n",
      "Epoch 3 step 916: training accuarcy: 0.6875\n",
      "Epoch 3 step 916: training loss: 1381.5056480850585\n",
      "Epoch 3 step 917: training accuarcy: 0.7025\n",
      "Epoch 3 step 917: training loss: 1383.7461710324746\n",
      "Epoch 3 step 918: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 918: training loss: 1382.561356903427\n",
      "Epoch 3 step 919: training accuarcy: 0.7115\n",
      "Epoch 3 step 919: training loss: 1383.1089647632102\n",
      "Epoch 3 step 920: training accuarcy: 0.7105\n",
      "Epoch 3 step 920: training loss: 1382.5908451149348\n",
      "Epoch 3 step 921: training accuarcy: 0.7055\n",
      "Epoch 3 step 921: training loss: 1382.1643334180194\n",
      "Epoch 3 step 922: training accuarcy: 0.708\n",
      "Epoch 3 step 922: training loss: 1382.7128776047525\n",
      "Epoch 3 step 923: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 923: training loss: 1382.465129067771\n",
      "Epoch 3 step 924: training accuarcy: 0.709\n",
      "Epoch 3 step 924: training loss: 1383.0114337375305\n",
      "Epoch 3 step 925: training accuarcy: 0.681\n",
      "Epoch 3 step 925: training loss: 1383.1402963505118\n",
      "Epoch 3 step 926: training accuarcy: 0.6985\n",
      "Epoch 3 step 926: training loss: 1383.311093963953\n",
      "Epoch 3 step 927: training accuarcy: 0.674\n",
      "Epoch 3 step 927: training loss: 1383.6242357338529\n",
      "Epoch 3 step 928: training accuarcy: 0.7005\n",
      "Epoch 3 step 928: training loss: 1382.3069548905946\n",
      "Epoch 3 step 929: training accuarcy: 0.6955\n",
      "Epoch 3 step 929: training loss: 1383.0720708624895\n",
      "Epoch 3 step 930: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 930: training loss: 1383.4758446592693\n",
      "Epoch 3 step 931: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 931: training loss: 1383.6130644268026\n",
      "Epoch 3 step 932: training accuarcy: 0.7025\n",
      "Epoch 3 step 932: training loss: 1382.953999490703\n",
      "Epoch 3 step 933: training accuarcy: 0.676\n",
      "Epoch 3 step 933: training loss: 1382.962713989948\n",
      "Epoch 3 step 934: training accuarcy: 0.6910000000000001\n",
      "Epoch 3 step 934: training loss: 1382.4641433408817\n",
      "Epoch 3 step 935: training accuarcy: 0.713\n",
      "Epoch 3 step 935: training loss: 1384.378718073146\n",
      "Epoch 3 step 936: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 936: training loss: 1382.3860936987032\n",
      "Epoch 3 step 937: training accuarcy: 0.6825\n",
      "Epoch 3 step 937: training loss: 1382.588395295429\n",
      "Epoch 3 step 938: training accuarcy: 0.6975\n",
      "Epoch 3 step 938: training loss: 1382.7643271693728\n",
      "Epoch 3 step 939: training accuarcy: 0.6865\n",
      "Epoch 3 step 939: training loss: 1383.8943599431989\n",
      "Epoch 3 step 940: training accuarcy: 0.6755\n",
      "Epoch 3 step 940: training loss: 1384.047440217447\n",
      "Epoch 3 step 941: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 941: training loss: 1384.175308670123\n",
      "Epoch 3 step 942: training accuarcy: 0.678\n",
      "Epoch 3 step 942: training loss: 1383.514182054013\n",
      "Epoch 3 step 943: training accuarcy: 0.6995\n",
      "Epoch 3 step 943: training loss: 1382.6567378156003\n",
      "Epoch 3 step 944: training accuarcy: 0.7115\n",
      "Epoch 3 step 944: training loss: 1383.1009046282782\n",
      "Epoch 3 step 945: training accuarcy: 0.707\n",
      "Epoch 3 step 945: training loss: 1383.8306729243106\n",
      "Epoch 3 step 946: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 946: training loss: 1383.0571942667439\n",
      "Epoch 3 step 947: training accuarcy: 0.6975\n",
      "Epoch 3 step 947: training loss: 1383.1396157542192\n",
      "Epoch 3 step 948: training accuarcy: 0.7005\n",
      "Epoch 3 step 948: training loss: 1382.6826081295471\n",
      "Epoch 3 step 949: training accuarcy: 0.6880000000000001\n",
      "Epoch 3 step 949: training loss: 1381.6206933165613\n",
      "Epoch 3 step 950: training accuarcy: 0.7015\n",
      "Epoch 3 step 950: training loss: 1382.4136178445701\n",
      "Epoch 3 step 951: training accuarcy: 0.6905\n",
      "Epoch 3 step 951: training loss: 1382.0542590965604\n",
      "Epoch 3 step 952: training accuarcy: 0.7225\n",
      "Epoch 3 step 952: training loss: 1382.887392476947\n",
      "Epoch 3 step 953: training accuarcy: 0.709\n",
      "Epoch 3 step 953: training loss: 1381.5628755901766\n",
      "Epoch 3 step 954: training accuarcy: 0.715\n",
      "Epoch 3 step 954: training loss: 1383.0759659893295\n",
      "Epoch 3 step 955: training accuarcy: 0.706\n",
      "Epoch 3 step 955: training loss: 1383.3912401029754\n",
      "Epoch 3 step 956: training accuarcy: 0.6955\n",
      "Epoch 3 step 956: training loss: 1382.8625479279717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 957: training accuarcy: 0.704\n",
      "Epoch 3 step 957: training loss: 1382.2913814021301\n",
      "Epoch 3 step 958: training accuarcy: 0.7005\n",
      "Epoch 3 step 958: training loss: 1383.4684908042232\n",
      "Epoch 3 step 959: training accuarcy: 0.6905\n",
      "Epoch 3 step 959: training loss: 1383.0728250134239\n",
      "Epoch 3 step 960: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 960: training loss: 1382.8998016617154\n",
      "Epoch 3 step 961: training accuarcy: 0.6895\n",
      "Epoch 3 step 961: training loss: 1382.62635410225\n",
      "Epoch 3 step 962: training accuarcy: 0.7105\n",
      "Epoch 3 step 962: training loss: 1383.510197930131\n",
      "Epoch 3 step 963: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 963: training loss: 1382.782339482078\n",
      "Epoch 3 step 964: training accuarcy: 0.7015\n",
      "Epoch 3 step 964: training loss: 1383.5770460562608\n",
      "Epoch 3 step 965: training accuarcy: 0.687\n",
      "Epoch 3 step 965: training loss: 1382.8944597063394\n",
      "Epoch 3 step 966: training accuarcy: 0.7025\n",
      "Epoch 3 step 966: training loss: 1382.318713068834\n",
      "Epoch 3 step 967: training accuarcy: 0.707\n",
      "Epoch 3 step 967: training loss: 1383.0662093079281\n",
      "Epoch 3 step 968: training accuarcy: 0.6940000000000001\n",
      "Epoch 3 step 968: training loss: 1383.2480104373806\n",
      "Epoch 3 step 969: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 969: training loss: 1382.971607827981\n",
      "Epoch 3 step 970: training accuarcy: 0.6905\n",
      "Epoch 3 step 970: training loss: 1382.8152003317666\n",
      "Epoch 3 step 971: training accuarcy: 0.707\n",
      "Epoch 3 step 971: training loss: 1383.4458717431353\n",
      "Epoch 3 step 972: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 972: training loss: 1382.6169213295598\n",
      "Epoch 3 step 973: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 973: training loss: 1383.3465944106188\n",
      "Epoch 3 step 974: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 974: training loss: 1383.6554896361129\n",
      "Epoch 3 step 975: training accuarcy: 0.685\n",
      "Epoch 3 step 975: training loss: 1383.0547438285034\n",
      "Epoch 3 step 976: training accuarcy: 0.6825\n",
      "Epoch 3 step 976: training loss: 1383.3603825457649\n",
      "Epoch 3 step 977: training accuarcy: 0.6915\n",
      "Epoch 3 step 977: training loss: 1383.3145648816146\n",
      "Epoch 3 step 978: training accuarcy: 0.6925\n",
      "Epoch 3 step 978: training loss: 1383.1752022363073\n",
      "Epoch 3 step 979: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 979: training loss: 1382.0790909395025\n",
      "Epoch 3 step 980: training accuarcy: 0.6815\n",
      "Epoch 3 step 980: training loss: 1382.266964329617\n",
      "Epoch 3 step 981: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 981: training loss: 1382.7466577892214\n",
      "Epoch 3 step 982: training accuarcy: 0.6955\n",
      "Epoch 3 step 982: training loss: 1383.286507174455\n",
      "Epoch 3 step 983: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 983: training loss: 1382.840888553229\n",
      "Epoch 3 step 984: training accuarcy: 0.6895\n",
      "Epoch 3 step 984: training loss: 1383.0345142305453\n",
      "Epoch 3 step 985: training accuarcy: 0.7115\n",
      "Epoch 3 step 985: training loss: 1382.6716240186158\n",
      "Epoch 3 step 986: training accuarcy: 0.7045\n",
      "Epoch 3 step 986: training loss: 1383.8485199595264\n",
      "Epoch 3 step 987: training accuarcy: 0.6885\n",
      "Epoch 3 step 987: training loss: 1383.290461058194\n",
      "Epoch 3 step 988: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 988: training loss: 1382.9812338038041\n",
      "Epoch 3 step 989: training accuarcy: 0.6905\n",
      "Epoch 3 step 989: training loss: 1383.3954528738798\n",
      "Epoch 3 step 990: training accuarcy: 0.6855\n",
      "Epoch 3 step 990: training loss: 1383.1605792965977\n",
      "Epoch 3 step 991: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 991: training loss: 1383.3361774116165\n",
      "Epoch 3 step 992: training accuarcy: 0.7005\n",
      "Epoch 3 step 992: training loss: 1383.066577631763\n",
      "Epoch 3 step 993: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 993: training loss: 1383.7958552225402\n",
      "Epoch 3 step 994: training accuarcy: 0.6915\n",
      "Epoch 3 step 994: training loss: 1382.43566207849\n",
      "Epoch 3 step 995: training accuarcy: 0.706\n",
      "Epoch 3 step 995: training loss: 1383.6932125344035\n",
      "Epoch 3 step 996: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 996: training loss: 1382.7914348366435\n",
      "Epoch 3 step 997: training accuarcy: 0.6985\n",
      "Epoch 3 step 997: training loss: 1382.9609750316263\n",
      "Epoch 3 step 998: training accuarcy: 0.7055\n",
      "Epoch 3 step 998: training loss: 1383.2335140463342\n",
      "Epoch 3 step 999: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 999: training loss: 1383.0825438028542\n",
      "Epoch 3 step 1000: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 1000: training loss: 1383.1758631493433\n",
      "Epoch 3 step 1001: training accuarcy: 0.6985\n",
      "Epoch 3 step 1001: training loss: 1382.5943080610482\n",
      "Epoch 3 step 1002: training accuarcy: 0.708\n",
      "Epoch 3 step 1002: training loss: 1383.1712213548135\n",
      "Epoch 3 step 1003: training accuarcy: 0.6965\n",
      "Epoch 3 step 1003: training loss: 1381.6319600180461\n",
      "Epoch 3 step 1004: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 1004: training loss: 1383.7331695186301\n",
      "Epoch 3 step 1005: training accuarcy: 0.6745\n",
      "Epoch 3 step 1005: training loss: 1382.7037819886543\n",
      "Epoch 3 step 1006: training accuarcy: 0.6975\n",
      "Epoch 3 step 1006: training loss: 1383.3568136997765\n",
      "Epoch 3 step 1007: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 1007: training loss: 1382.6317009001962\n",
      "Epoch 3 step 1008: training accuarcy: 0.6905\n",
      "Epoch 3 step 1008: training loss: 1382.8640442224985\n",
      "Epoch 3 step 1009: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 1009: training loss: 1383.8827198516044\n",
      "Epoch 3 step 1010: training accuarcy: 0.6845\n",
      "Epoch 3 step 1010: training loss: 1382.3181600798202\n",
      "Epoch 3 step 1011: training accuarcy: 0.707\n",
      "Epoch 3 step 1011: training loss: 1382.367884250588\n",
      "Epoch 3 step 1012: training accuarcy: 0.715\n",
      "Epoch 3 step 1012: training loss: 1382.5813100818884\n",
      "Epoch 3 step 1013: training accuarcy: 0.7005\n",
      "Epoch 3 step 1013: training loss: 1382.8009129174284\n",
      "Epoch 3 step 1014: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 1014: training loss: 1381.3296184513003\n",
      "Epoch 3 step 1015: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 1015: training loss: 1382.9580956928673\n",
      "Epoch 3 step 1016: training accuarcy: 0.7105\n",
      "Epoch 3 step 1016: training loss: 1383.2268028367287\n",
      "Epoch 3 step 1017: training accuarcy: 0.686\n",
      "Epoch 3 step 1017: training loss: 1381.7648541411527\n",
      "Epoch 3 step 1018: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 1018: training loss: 1383.7410375505667\n",
      "Epoch 3 step 1019: training accuarcy: 0.6875\n",
      "Epoch 3 step 1019: training loss: 1383.3589404540758\n",
      "Epoch 3 step 1020: training accuarcy: 0.7015\n",
      "Epoch 3 step 1020: training loss: 1383.1176834308474\n",
      "Epoch 3 step 1021: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 1021: training loss: 1383.3478549687127\n",
      "Epoch 3 step 1022: training accuarcy: 0.6880000000000001\n",
      "Epoch 3 step 1022: training loss: 1381.8084800438196\n",
      "Epoch 3 step 1023: training accuarcy: 0.7045\n",
      "Epoch 3 step 1023: training loss: 1383.2007926355727\n",
      "Epoch 3 step 1024: training accuarcy: 0.6975\n",
      "Epoch 3 step 1024: training loss: 1382.4861545762933\n",
      "Epoch 3 step 1025: training accuarcy: 0.6965\n",
      "Epoch 3 step 1025: training loss: 1382.6511681624652\n",
      "Epoch 3 step 1026: training accuarcy: 0.7075\n",
      "Epoch 3 step 1026: training loss: 1382.6908408385661\n",
      "Epoch 3 step 1027: training accuarcy: 0.7065\n",
      "Epoch 3 step 1027: training loss: 1383.6487596329378\n",
      "Epoch 3 step 1028: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 1028: training loss: 1382.452610242877\n",
      "Epoch 3 step 1029: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 1029: training loss: 1382.1827777503354\n",
      "Epoch 3 step 1030: training accuarcy: 0.7055\n",
      "Epoch 3 step 1030: training loss: 1383.2447953854983\n",
      "Epoch 3 step 1031: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 1031: training loss: 1384.067887958729\n",
      "Epoch 3 step 1032: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 1032: training loss: 1383.670526632963\n",
      "Epoch 3 step 1033: training accuarcy: 0.6935\n",
      "Epoch 3 step 1033: training loss: 1383.0198082992313\n",
      "Epoch 3 step 1034: training accuarcy: 0.678\n",
      "Epoch 3 step 1034: training loss: 1383.4498935009985\n",
      "Epoch 3 step 1035: training accuarcy: 0.684\n",
      "Epoch 3 step 1035: training loss: 1382.6092098689064\n",
      "Epoch 3 step 1036: training accuarcy: 0.7075\n",
      "Epoch 3 step 1036: training loss: 1382.9767935032742\n",
      "Epoch 3 step 1037: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 1037: training loss: 1382.6044368688247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 1038: training accuarcy: 0.6985\n",
      "Epoch 3 step 1038: training loss: 1383.7750460755167\n",
      "Epoch 3 step 1039: training accuarcy: 0.686\n",
      "Epoch 3 step 1039: training loss: 1384.15248415367\n",
      "Epoch 3 step 1040: training accuarcy: 0.665\n",
      "Epoch 3 step 1040: training loss: 1383.3735926124186\n",
      "Epoch 3 step 1041: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 1041: training loss: 1382.433210542612\n",
      "Epoch 3 step 1042: training accuarcy: 0.686\n",
      "Epoch 3 step 1042: training loss: 1382.2463133346428\n",
      "Epoch 3 step 1043: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 1043: training loss: 1383.6066352128764\n",
      "Epoch 3 step 1044: training accuarcy: 0.684\n",
      "Epoch 3 step 1044: training loss: 1382.914305794032\n",
      "Epoch 3 step 1045: training accuarcy: 0.7045\n",
      "Epoch 3 step 1045: training loss: 1382.8207407805858\n",
      "Epoch 3 step 1046: training accuarcy: 0.7035\n",
      "Epoch 3 step 1046: training loss: 1383.5247245725898\n",
      "Epoch 3 step 1047: training accuarcy: 0.6995\n",
      "Epoch 3 step 1047: training loss: 1381.4452054903948\n",
      "Epoch 3 step 1048: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 1048: training loss: 1382.2720086335064\n",
      "Epoch 3 step 1049: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 1049: training loss: 1382.2614505558422\n",
      "Epoch 3 step 1050: training accuarcy: 0.7095\n",
      "Epoch 3 step 1050: training loss: 1381.9401037423122\n",
      "Epoch 3 step 1051: training accuarcy: 0.713\n",
      "Epoch 3 step 1051: training loss: 542.3112190109429\n",
      "Epoch 3 step 1052: training accuarcy: 0.6935897435897436\n",
      "Epoch 3: train loss 1379.568299516494, train accuarcy 0.700809895992279\n",
      "Epoch 3: valid loss 1363.6248650571988, valid accuarcy 0.7157873511314392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 4/8 [07:42<07:45, 116.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 1382.5246919652498\n",
      "Epoch 4 step 1053: training accuarcy: 0.71\n",
      "Epoch 4 step 1053: training loss: 1381.8458729592346\n",
      "Epoch 4 step 1054: training accuarcy: 0.708\n",
      "Epoch 4 step 1054: training loss: 1383.148648274755\n",
      "Epoch 4 step 1055: training accuarcy: 0.704\n",
      "Epoch 4 step 1055: training loss: 1382.3012138236195\n",
      "Epoch 4 step 1056: training accuarcy: 0.72\n",
      "Epoch 4 step 1056: training loss: 1381.511025055811\n",
      "Epoch 4 step 1057: training accuarcy: 0.717\n",
      "Epoch 4 step 1057: training loss: 1382.7251593311412\n",
      "Epoch 4 step 1058: training accuarcy: 0.71\n",
      "Epoch 4 step 1058: training loss: 1382.137966299077\n",
      "Epoch 4 step 1059: training accuarcy: 0.723\n",
      "Epoch 4 step 1059: training loss: 1381.7830035196864\n",
      "Epoch 4 step 1060: training accuarcy: 0.712\n",
      "Epoch 4 step 1060: training loss: 1382.1192834601509\n",
      "Epoch 4 step 1061: training accuarcy: 0.7035\n",
      "Epoch 4 step 1061: training loss: 1382.2242559358194\n",
      "Epoch 4 step 1062: training accuarcy: 0.708\n",
      "Epoch 4 step 1062: training loss: 1381.8645179256189\n",
      "Epoch 4 step 1063: training accuarcy: 0.7045\n",
      "Epoch 4 step 1063: training loss: 1382.870882602911\n",
      "Epoch 4 step 1064: training accuarcy: 0.713\n",
      "Epoch 4 step 1064: training loss: 1380.9165022224627\n",
      "Epoch 4 step 1065: training accuarcy: 0.7165\n",
      "Epoch 4 step 1065: training loss: 1383.3864889511324\n",
      "Epoch 4 step 1066: training accuarcy: 0.6855\n",
      "Epoch 4 step 1066: training loss: 1381.9812122531005\n",
      "Epoch 4 step 1067: training accuarcy: 0.6955\n",
      "Epoch 4 step 1067: training loss: 1380.3996422966327\n",
      "Epoch 4 step 1068: training accuarcy: 0.709\n",
      "Epoch 4 step 1068: training loss: 1382.0115797301853\n",
      "Epoch 4 step 1069: training accuarcy: 0.7165\n",
      "Epoch 4 step 1069: training loss: 1381.468797235042\n",
      "Epoch 4 step 1070: training accuarcy: 0.7085\n",
      "Epoch 4 step 1070: training loss: 1383.0875019043442\n",
      "Epoch 4 step 1071: training accuarcy: 0.707\n",
      "Epoch 4 step 1071: training loss: 1382.2472468907433\n",
      "Epoch 4 step 1072: training accuarcy: 0.7225\n",
      "Epoch 4 step 1072: training loss: 1382.1161021404057\n",
      "Epoch 4 step 1073: training accuarcy: 0.7025\n",
      "Epoch 4 step 1073: training loss: 1381.7533539434346\n",
      "Epoch 4 step 1074: training accuarcy: 0.706\n",
      "Epoch 4 step 1074: training loss: 1381.4559377676567\n",
      "Epoch 4 step 1075: training accuarcy: 0.7135\n",
      "Epoch 4 step 1075: training loss: 1381.0766771225485\n",
      "Epoch 4 step 1076: training accuarcy: 0.7255\n",
      "Epoch 4 step 1076: training loss: 1383.0991128015885\n",
      "Epoch 4 step 1077: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1077: training loss: 1382.8765839008304\n",
      "Epoch 4 step 1078: training accuarcy: 0.711\n",
      "Epoch 4 step 1078: training loss: 1382.69640618908\n",
      "Epoch 4 step 1079: training accuarcy: 0.708\n",
      "Epoch 4 step 1079: training loss: 1382.8883079140885\n",
      "Epoch 4 step 1080: training accuarcy: 0.6945\n",
      "Epoch 4 step 1080: training loss: 1383.5652293953833\n",
      "Epoch 4 step 1081: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1081: training loss: 1382.13966476656\n",
      "Epoch 4 step 1082: training accuarcy: 0.718\n",
      "Epoch 4 step 1082: training loss: 1382.6144966716636\n",
      "Epoch 4 step 1083: training accuarcy: 0.709\n",
      "Epoch 4 step 1083: training loss: 1382.375913267942\n",
      "Epoch 4 step 1084: training accuarcy: 0.713\n",
      "Epoch 4 step 1084: training loss: 1381.9375152084235\n",
      "Epoch 4 step 1085: training accuarcy: 0.711\n",
      "Epoch 4 step 1085: training loss: 1382.1021903922501\n",
      "Epoch 4 step 1086: training accuarcy: 0.7135\n",
      "Epoch 4 step 1086: training loss: 1382.7374352752017\n",
      "Epoch 4 step 1087: training accuarcy: 0.7055\n",
      "Epoch 4 step 1087: training loss: 1382.0080746180536\n",
      "Epoch 4 step 1088: training accuarcy: 0.707\n",
      "Epoch 4 step 1088: training loss: 1382.4070738828736\n",
      "Epoch 4 step 1089: training accuarcy: 0.709\n",
      "Epoch 4 step 1089: training loss: 1382.02985549696\n",
      "Epoch 4 step 1090: training accuarcy: 0.721\n",
      "Epoch 4 step 1090: training loss: 1383.453915892891\n",
      "Epoch 4 step 1091: training accuarcy: 0.7005\n",
      "Epoch 4 step 1091: training loss: 1382.0874838345137\n",
      "Epoch 4 step 1092: training accuarcy: 0.7215\n",
      "Epoch 4 step 1092: training loss: 1383.5476587448286\n",
      "Epoch 4 step 1093: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1093: training loss: 1382.0026208542454\n",
      "Epoch 4 step 1094: training accuarcy: 0.7065\n",
      "Epoch 4 step 1094: training loss: 1381.4363107130762\n",
      "Epoch 4 step 1095: training accuarcy: 0.718\n",
      "Epoch 4 step 1095: training loss: 1382.724268492269\n",
      "Epoch 4 step 1096: training accuarcy: 0.705\n",
      "Epoch 4 step 1096: training loss: 1381.8377170716412\n",
      "Epoch 4 step 1097: training accuarcy: 0.7075\n",
      "Epoch 4 step 1097: training loss: 1381.8158633546996\n",
      "Epoch 4 step 1098: training accuarcy: 0.7105\n",
      "Epoch 4 step 1098: training loss: 1382.4281433850108\n",
      "Epoch 4 step 1099: training accuarcy: 0.7105\n",
      "Epoch 4 step 1099: training loss: 1382.2433589947188\n",
      "Epoch 4 step 1100: training accuarcy: 0.718\n",
      "Epoch 4 step 1100: training loss: 1382.7753817829275\n",
      "Epoch 4 step 1101: training accuarcy: 0.7015\n",
      "Epoch 4 step 1101: training loss: 1382.001591552839\n",
      "Epoch 4 step 1102: training accuarcy: 0.705\n",
      "Epoch 4 step 1102: training loss: 1381.628451067273\n",
      "Epoch 4 step 1103: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1103: training loss: 1383.2671988989164\n",
      "Epoch 4 step 1104: training accuarcy: 0.71\n",
      "Epoch 4 step 1104: training loss: 1383.096384995542\n",
      "Epoch 4 step 1105: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1105: training loss: 1383.6938443545557\n",
      "Epoch 4 step 1106: training accuarcy: 0.6925\n",
      "Epoch 4 step 1106: training loss: 1382.9890182850017\n",
      "Epoch 4 step 1107: training accuarcy: 0.7055\n",
      "Epoch 4 step 1107: training loss: 1382.5561620337169\n",
      "Epoch 4 step 1108: training accuarcy: 0.7115\n",
      "Epoch 4 step 1108: training loss: 1381.8575838431948\n",
      "Epoch 4 step 1109: training accuarcy: 0.7095\n",
      "Epoch 4 step 1109: training loss: 1383.3660477108515\n",
      "Epoch 4 step 1110: training accuarcy: 0.6975\n",
      "Epoch 4 step 1110: training loss: 1381.8435447717409\n",
      "Epoch 4 step 1111: training accuarcy: 0.7105\n",
      "Epoch 4 step 1111: training loss: 1382.9712178132029\n",
      "Epoch 4 step 1112: training accuarcy: 0.6965\n",
      "Epoch 4 step 1112: training loss: 1381.7814957576643\n",
      "Epoch 4 step 1113: training accuarcy: 0.7145\n",
      "Epoch 4 step 1113: training loss: 1382.4054846645988\n",
      "Epoch 4 step 1114: training accuarcy: 0.717\n",
      "Epoch 4 step 1114: training loss: 1382.6677745977152\n",
      "Epoch 4 step 1115: training accuarcy: 0.6965\n",
      "Epoch 4 step 1115: training loss: 1382.2913078135691\n",
      "Epoch 4 step 1116: training accuarcy: 0.7165\n",
      "Epoch 4 step 1116: training loss: 1382.2806523578379\n",
      "Epoch 4 step 1117: training accuarcy: 0.711\n",
      "Epoch 4 step 1117: training loss: 1383.021032986505\n",
      "Epoch 4 step 1118: training accuarcy: 0.7095\n",
      "Epoch 4 step 1118: training loss: 1382.442373797845\n",
      "Epoch 4 step 1119: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1119: training loss: 1383.7716768063751\n",
      "Epoch 4 step 1120: training accuarcy: 0.6855\n",
      "Epoch 4 step 1120: training loss: 1383.2161369727069\n",
      "Epoch 4 step 1121: training accuarcy: 0.712\n",
      "Epoch 4 step 1121: training loss: 1383.5018757701373\n",
      "Epoch 4 step 1122: training accuarcy: 0.7000000000000001\n",
      "Epoch 4 step 1122: training loss: 1382.5853124867097\n",
      "Epoch 4 step 1123: training accuarcy: 0.6955\n",
      "Epoch 4 step 1123: training loss: 1382.4945164570886\n",
      "Epoch 4 step 1124: training accuarcy: 0.7125\n",
      "Epoch 4 step 1124: training loss: 1381.75168389833\n",
      "Epoch 4 step 1125: training accuarcy: 0.7125\n",
      "Epoch 4 step 1125: training loss: 1383.2234116520476\n",
      "Epoch 4 step 1126: training accuarcy: 0.6985\n",
      "Epoch 4 step 1126: training loss: 1383.0332918829\n",
      "Epoch 4 step 1127: training accuarcy: 0.7095\n",
      "Epoch 4 step 1127: training loss: 1383.6438040309013\n",
      "Epoch 4 step 1128: training accuarcy: 0.6905\n",
      "Epoch 4 step 1128: training loss: 1382.2627212057143\n",
      "Epoch 4 step 1129: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1129: training loss: 1381.2620396875639\n",
      "Epoch 4 step 1130: training accuarcy: 0.7075\n",
      "Epoch 4 step 1130: training loss: 1383.246616963283\n",
      "Epoch 4 step 1131: training accuarcy: 0.7085\n",
      "Epoch 4 step 1131: training loss: 1383.189207585979\n",
      "Epoch 4 step 1132: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1132: training loss: 1382.9823398961673\n",
      "Epoch 4 step 1133: training accuarcy: 0.7005\n",
      "Epoch 4 step 1133: training loss: 1382.6765201630299\n",
      "Epoch 4 step 1134: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1134: training loss: 1382.825506868949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1135: training accuarcy: 0.6965\n",
      "Epoch 4 step 1135: training loss: 1382.5509399401856\n",
      "Epoch 4 step 1136: training accuarcy: 0.6955\n",
      "Epoch 4 step 1136: training loss: 1382.303881698267\n",
      "Epoch 4 step 1137: training accuarcy: 0.6965\n",
      "Epoch 4 step 1137: training loss: 1382.4545808159153\n",
      "Epoch 4 step 1138: training accuarcy: 0.7185\n",
      "Epoch 4 step 1138: training loss: 1381.7943073951442\n",
      "Epoch 4 step 1139: training accuarcy: 0.7025\n",
      "Epoch 4 step 1139: training loss: 1381.9264066065105\n",
      "Epoch 4 step 1140: training accuarcy: 0.712\n",
      "Epoch 4 step 1140: training loss: 1383.2061433088438\n",
      "Epoch 4 step 1141: training accuarcy: 0.704\n",
      "Epoch 4 step 1141: training loss: 1382.2743686269353\n",
      "Epoch 4 step 1142: training accuarcy: 0.7185\n",
      "Epoch 4 step 1142: training loss: 1383.247236501556\n",
      "Epoch 4 step 1143: training accuarcy: 0.686\n",
      "Epoch 4 step 1143: training loss: 1382.0403790674904\n",
      "Epoch 4 step 1144: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1144: training loss: 1383.4319116190868\n",
      "Epoch 4 step 1145: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 1145: training loss: 1383.1307511989519\n",
      "Epoch 4 step 1146: training accuarcy: 0.707\n",
      "Epoch 4 step 1146: training loss: 1383.0046433858554\n",
      "Epoch 4 step 1147: training accuarcy: 0.6805\n",
      "Epoch 4 step 1147: training loss: 1382.2645718788558\n",
      "Epoch 4 step 1148: training accuarcy: 0.7125\n",
      "Epoch 4 step 1148: training loss: 1383.8387737674457\n",
      "Epoch 4 step 1149: training accuarcy: 0.68\n",
      "Epoch 4 step 1149: training loss: 1382.37047267295\n",
      "Epoch 4 step 1150: training accuarcy: 0.7065\n",
      "Epoch 4 step 1150: training loss: 1383.3386577972622\n",
      "Epoch 4 step 1151: training accuarcy: 0.6945\n",
      "Epoch 4 step 1151: training loss: 1382.6290832758627\n",
      "Epoch 4 step 1152: training accuarcy: 0.7075\n",
      "Epoch 4 step 1152: training loss: 1382.6313336732458\n",
      "Epoch 4 step 1153: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1153: training loss: 1382.9459854862805\n",
      "Epoch 4 step 1154: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1154: training loss: 1383.0895001518127\n",
      "Epoch 4 step 1155: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1155: training loss: 1383.6926458206294\n",
      "Epoch 4 step 1156: training accuarcy: 0.6855\n",
      "Epoch 4 step 1156: training loss: 1383.4612352155866\n",
      "Epoch 4 step 1157: training accuarcy: 0.677\n",
      "Epoch 4 step 1157: training loss: 1381.8953555923385\n",
      "Epoch 4 step 1158: training accuarcy: 0.6965\n",
      "Epoch 4 step 1158: training loss: 1383.788787983831\n",
      "Epoch 4 step 1159: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1159: training loss: 1383.344137439292\n",
      "Epoch 4 step 1160: training accuarcy: 0.6975\n",
      "Epoch 4 step 1160: training loss: 1382.9243662862054\n",
      "Epoch 4 step 1161: training accuarcy: 0.709\n",
      "Epoch 4 step 1161: training loss: 1384.4103723992132\n",
      "Epoch 4 step 1162: training accuarcy: 0.684\n",
      "Epoch 4 step 1162: training loss: 1383.5218182578444\n",
      "Epoch 4 step 1163: training accuarcy: 0.6945\n",
      "Epoch 4 step 1163: training loss: 1383.2485617404427\n",
      "Epoch 4 step 1164: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1164: training loss: 1383.3981601780833\n",
      "Epoch 4 step 1165: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1165: training loss: 1382.3650302416445\n",
      "Epoch 4 step 1166: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1166: training loss: 1382.916603673417\n",
      "Epoch 4 step 1167: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1167: training loss: 1383.072070642687\n",
      "Epoch 4 step 1168: training accuarcy: 0.6915\n",
      "Epoch 4 step 1168: training loss: 1381.919229977157\n",
      "Epoch 4 step 1169: training accuarcy: 0.7165\n",
      "Epoch 4 step 1169: training loss: 1383.3382279743926\n",
      "Epoch 4 step 1170: training accuarcy: 0.6815\n",
      "Epoch 4 step 1170: training loss: 1382.6410799981625\n",
      "Epoch 4 step 1171: training accuarcy: 0.6975\n",
      "Epoch 4 step 1171: training loss: 1382.8436437967616\n",
      "Epoch 4 step 1172: training accuarcy: 0.7025\n",
      "Epoch 4 step 1172: training loss: 1383.7522428174339\n",
      "Epoch 4 step 1173: training accuarcy: 0.6835\n",
      "Epoch 4 step 1173: training loss: 1384.042432229929\n",
      "Epoch 4 step 1174: training accuarcy: 0.6815\n",
      "Epoch 4 step 1174: training loss: 1382.869508478283\n",
      "Epoch 4 step 1175: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1175: training loss: 1382.6417747535938\n",
      "Epoch 4 step 1176: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1176: training loss: 1384.261961077231\n",
      "Epoch 4 step 1177: training accuarcy: 0.684\n",
      "Epoch 4 step 1177: training loss: 1382.1217952025047\n",
      "Epoch 4 step 1178: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1178: training loss: 1382.5246678850779\n",
      "Epoch 4 step 1179: training accuarcy: 0.706\n",
      "Epoch 4 step 1179: training loss: 1383.8668644401716\n",
      "Epoch 4 step 1180: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1180: training loss: 1383.3004002590997\n",
      "Epoch 4 step 1181: training accuarcy: 0.707\n",
      "Epoch 4 step 1181: training loss: 1382.241572589099\n",
      "Epoch 4 step 1182: training accuarcy: 0.6945\n",
      "Epoch 4 step 1182: training loss: 1382.880752328525\n",
      "Epoch 4 step 1183: training accuarcy: 0.6895\n",
      "Epoch 4 step 1183: training loss: 1383.9278020534464\n",
      "Epoch 4 step 1184: training accuarcy: 0.675\n",
      "Epoch 4 step 1184: training loss: 1382.2284323846307\n",
      "Epoch 4 step 1185: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1185: training loss: 1383.9415608748766\n",
      "Epoch 4 step 1186: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1186: training loss: 1382.0479656800176\n",
      "Epoch 4 step 1187: training accuarcy: 0.6975\n",
      "Epoch 4 step 1187: training loss: 1382.2734984413585\n",
      "Epoch 4 step 1188: training accuarcy: 0.6925\n",
      "Epoch 4 step 1188: training loss: 1383.9165761890993\n",
      "Epoch 4 step 1189: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1189: training loss: 1381.7339154220429\n",
      "Epoch 4 step 1190: training accuarcy: 0.7065\n",
      "Epoch 4 step 1190: training loss: 1382.1036324278941\n",
      "Epoch 4 step 1191: training accuarcy: 0.7045\n",
      "Epoch 4 step 1191: training loss: 1382.5862177096012\n",
      "Epoch 4 step 1192: training accuarcy: 0.675\n",
      "Epoch 4 step 1192: training loss: 1383.04676570879\n",
      "Epoch 4 step 1193: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1193: training loss: 1382.5874836638316\n",
      "Epoch 4 step 1194: training accuarcy: 0.7005\n",
      "Epoch 4 step 1194: training loss: 1383.4971179787988\n",
      "Epoch 4 step 1195: training accuarcy: 0.7115\n",
      "Epoch 4 step 1195: training loss: 1382.8972291401467\n",
      "Epoch 4 step 1196: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1196: training loss: 1384.0762414908645\n",
      "Epoch 4 step 1197: training accuarcy: 0.6900000000000001\n",
      "Epoch 4 step 1197: training loss: 1382.77558446805\n",
      "Epoch 4 step 1198: training accuarcy: 0.708\n",
      "Epoch 4 step 1198: training loss: 1383.118629185266\n",
      "Epoch 4 step 1199: training accuarcy: 0.6965\n",
      "Epoch 4 step 1199: training loss: 1383.546364669714\n",
      "Epoch 4 step 1200: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1200: training loss: 1383.2083546899235\n",
      "Epoch 4 step 1201: training accuarcy: 0.7035\n",
      "Epoch 4 step 1201: training loss: 1383.0448026892855\n",
      "Epoch 4 step 1202: training accuarcy: 0.6875\n",
      "Epoch 4 step 1202: training loss: 1382.8708919652863\n",
      "Epoch 4 step 1203: training accuarcy: 0.7075\n",
      "Epoch 4 step 1203: training loss: 1382.6040956392742\n",
      "Epoch 4 step 1204: training accuarcy: 0.6975\n",
      "Epoch 4 step 1204: training loss: 1382.7879502250148\n",
      "Epoch 4 step 1205: training accuarcy: 0.7105\n",
      "Epoch 4 step 1205: training loss: 1382.4433817104618\n",
      "Epoch 4 step 1206: training accuarcy: 0.7075\n",
      "Epoch 4 step 1206: training loss: 1383.4640053260082\n",
      "Epoch 4 step 1207: training accuarcy: 0.6795\n",
      "Epoch 4 step 1207: training loss: 1383.1397159755359\n",
      "Epoch 4 step 1208: training accuarcy: 0.685\n",
      "Epoch 4 step 1208: training loss: 1382.6508130689338\n",
      "Epoch 4 step 1209: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1209: training loss: 1383.486899209177\n",
      "Epoch 4 step 1210: training accuarcy: 0.6945\n",
      "Epoch 4 step 1210: training loss: 1384.100116611686\n",
      "Epoch 4 step 1211: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1211: training loss: 1381.9898043623737\n",
      "Epoch 4 step 1212: training accuarcy: 0.7015\n",
      "Epoch 4 step 1212: training loss: 1383.0348029668078\n",
      "Epoch 4 step 1213: training accuarcy: 0.6935\n",
      "Epoch 4 step 1213: training loss: 1383.6729505398985\n",
      "Epoch 4 step 1214: training accuarcy: 0.6975\n",
      "Epoch 4 step 1214: training loss: 1383.3638833539712\n",
      "Epoch 4 step 1215: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1215: training loss: 1384.238067820306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1216: training accuarcy: 0.668\n",
      "Epoch 4 step 1216: training loss: 1382.9617814476774\n",
      "Epoch 4 step 1217: training accuarcy: 0.712\n",
      "Epoch 4 step 1217: training loss: 1383.166450067319\n",
      "Epoch 4 step 1218: training accuarcy: 0.7025\n",
      "Epoch 4 step 1218: training loss: 1382.0339700890731\n",
      "Epoch 4 step 1219: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1219: training loss: 1382.6973158426463\n",
      "Epoch 4 step 1220: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1220: training loss: 1383.7232188561632\n",
      "Epoch 4 step 1221: training accuarcy: 0.6920000000000001\n",
      "Epoch 4 step 1221: training loss: 1381.9970947061893\n",
      "Epoch 4 step 1222: training accuarcy: 0.714\n",
      "Epoch 4 step 1222: training loss: 1383.2876036518117\n",
      "Epoch 4 step 1223: training accuarcy: 0.6935\n",
      "Epoch 4 step 1223: training loss: 1383.3480438914078\n",
      "Epoch 4 step 1224: training accuarcy: 0.7055\n",
      "Epoch 4 step 1224: training loss: 1383.440626057764\n",
      "Epoch 4 step 1225: training accuarcy: 0.7015\n",
      "Epoch 4 step 1225: training loss: 1383.3946085388015\n",
      "Epoch 4 step 1226: training accuarcy: 0.685\n",
      "Epoch 4 step 1226: training loss: 1382.717356856836\n",
      "Epoch 4 step 1227: training accuarcy: 0.6965\n",
      "Epoch 4 step 1227: training loss: 1383.4382776824116\n",
      "Epoch 4 step 1228: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1228: training loss: 1382.7030864046417\n",
      "Epoch 4 step 1229: training accuarcy: 0.6955\n",
      "Epoch 4 step 1229: training loss: 1383.006849717402\n",
      "Epoch 4 step 1230: training accuarcy: 0.6985\n",
      "Epoch 4 step 1230: training loss: 1383.9816545947356\n",
      "Epoch 4 step 1231: training accuarcy: 0.6875\n",
      "Epoch 4 step 1231: training loss: 1382.823549586863\n",
      "Epoch 4 step 1232: training accuarcy: 0.6985\n",
      "Epoch 4 step 1232: training loss: 1382.755883064133\n",
      "Epoch 4 step 1233: training accuarcy: 0.7075\n",
      "Epoch 4 step 1233: training loss: 1383.2413387739784\n",
      "Epoch 4 step 1234: training accuarcy: 0.6945\n",
      "Epoch 4 step 1234: training loss: 1383.0245072481518\n",
      "Epoch 4 step 1235: training accuarcy: 0.7055\n",
      "Epoch 4 step 1235: training loss: 1383.1797095724594\n",
      "Epoch 4 step 1236: training accuarcy: 0.674\n",
      "Epoch 4 step 1236: training loss: 1383.4791848545592\n",
      "Epoch 4 step 1237: training accuarcy: 0.6855\n",
      "Epoch 4 step 1237: training loss: 1382.1925008556443\n",
      "Epoch 4 step 1238: training accuarcy: 0.7105\n",
      "Epoch 4 step 1238: training loss: 1382.6521441473365\n",
      "Epoch 4 step 1239: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1239: training loss: 1383.1395992255177\n",
      "Epoch 4 step 1240: training accuarcy: 0.6995\n",
      "Epoch 4 step 1240: training loss: 1382.5292200196518\n",
      "Epoch 4 step 1241: training accuarcy: 0.6975\n",
      "Epoch 4 step 1241: training loss: 1383.3511921343918\n",
      "Epoch 4 step 1242: training accuarcy: 0.6995\n",
      "Epoch 4 step 1242: training loss: 1382.7325362366219\n",
      "Epoch 4 step 1243: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1243: training loss: 1383.2878524607652\n",
      "Epoch 4 step 1244: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1244: training loss: 1381.6197321980949\n",
      "Epoch 4 step 1245: training accuarcy: 0.719\n",
      "Epoch 4 step 1245: training loss: 1382.8896129435238\n",
      "Epoch 4 step 1246: training accuarcy: 0.6905\n",
      "Epoch 4 step 1246: training loss: 1382.3481170239413\n",
      "Epoch 4 step 1247: training accuarcy: 0.6985\n",
      "Epoch 4 step 1247: training loss: 1383.468906385795\n",
      "Epoch 4 step 1248: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1248: training loss: 1381.978874514369\n",
      "Epoch 4 step 1249: training accuarcy: 0.7065\n",
      "Epoch 4 step 1249: training loss: 1382.5720331458745\n",
      "Epoch 4 step 1250: training accuarcy: 0.686\n",
      "Epoch 4 step 1250: training loss: 1382.9807012292247\n",
      "Epoch 4 step 1251: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 1251: training loss: 1382.993673430494\n",
      "Epoch 4 step 1252: training accuarcy: 0.6955\n",
      "Epoch 4 step 1252: training loss: 1382.4462378110247\n",
      "Epoch 4 step 1253: training accuarcy: 0.712\n",
      "Epoch 4 step 1253: training loss: 1383.4802309569711\n",
      "Epoch 4 step 1254: training accuarcy: 0.683\n",
      "Epoch 4 step 1254: training loss: 1383.175083665975\n",
      "Epoch 4 step 1255: training accuarcy: 0.7075\n",
      "Epoch 4 step 1255: training loss: 1383.8110049738789\n",
      "Epoch 4 step 1256: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1256: training loss: 1383.2503777161285\n",
      "Epoch 4 step 1257: training accuarcy: 0.6865\n",
      "Epoch 4 step 1257: training loss: 1381.5793561016035\n",
      "Epoch 4 step 1258: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1258: training loss: 1382.4094091935306\n",
      "Epoch 4 step 1259: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1259: training loss: 1382.673922040631\n",
      "Epoch 4 step 1260: training accuarcy: 0.7175\n",
      "Epoch 4 step 1260: training loss: 1382.4097728734318\n",
      "Epoch 4 step 1261: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1261: training loss: 1382.4604772454732\n",
      "Epoch 4 step 1262: training accuarcy: 0.7025\n",
      "Epoch 4 step 1262: training loss: 1383.44474084762\n",
      "Epoch 4 step 1263: training accuarcy: 0.6945\n",
      "Epoch 4 step 1263: training loss: 1383.3975814100627\n",
      "Epoch 4 step 1264: training accuarcy: 0.6975\n",
      "Epoch 4 step 1264: training loss: 1381.7607107467854\n",
      "Epoch 4 step 1265: training accuarcy: 0.6995\n",
      "Epoch 4 step 1265: training loss: 1382.782517158179\n",
      "Epoch 4 step 1266: training accuarcy: 0.685\n",
      "Epoch 4 step 1266: training loss: 1383.450170518286\n",
      "Epoch 4 step 1267: training accuarcy: 0.6875\n",
      "Epoch 4 step 1267: training loss: 1381.8907065151027\n",
      "Epoch 4 step 1268: training accuarcy: 0.7025\n",
      "Epoch 4 step 1268: training loss: 1382.0445823701782\n",
      "Epoch 4 step 1269: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1269: training loss: 1383.891815304113\n",
      "Epoch 4 step 1270: training accuarcy: 0.6895\n",
      "Epoch 4 step 1270: training loss: 1382.4373296038023\n",
      "Epoch 4 step 1271: training accuarcy: 0.7065\n",
      "Epoch 4 step 1271: training loss: 1382.240662712702\n",
      "Epoch 4 step 1272: training accuarcy: 0.7005\n",
      "Epoch 4 step 1272: training loss: 1382.1451568522082\n",
      "Epoch 4 step 1273: training accuarcy: 0.7145\n",
      "Epoch 4 step 1273: training loss: 1382.6366186587736\n",
      "Epoch 4 step 1274: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1274: training loss: 1382.182725291315\n",
      "Epoch 4 step 1275: training accuarcy: 0.711\n",
      "Epoch 4 step 1275: training loss: 1382.4531663728558\n",
      "Epoch 4 step 1276: training accuarcy: 0.6905\n",
      "Epoch 4 step 1276: training loss: 1382.7712450212914\n",
      "Epoch 4 step 1277: training accuarcy: 0.6955\n",
      "Epoch 4 step 1277: training loss: 1383.2699798719925\n",
      "Epoch 4 step 1278: training accuarcy: 0.687\n",
      "Epoch 4 step 1278: training loss: 1383.2092100893792\n",
      "Epoch 4 step 1279: training accuarcy: 0.706\n",
      "Epoch 4 step 1279: training loss: 1382.725958753093\n",
      "Epoch 4 step 1280: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1280: training loss: 1382.2294678343167\n",
      "Epoch 4 step 1281: training accuarcy: 0.7025\n",
      "Epoch 4 step 1281: training loss: 1382.6584267516528\n",
      "Epoch 4 step 1282: training accuarcy: 0.6935\n",
      "Epoch 4 step 1282: training loss: 1382.33012032181\n",
      "Epoch 4 step 1283: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1283: training loss: 1382.2151073071705\n",
      "Epoch 4 step 1284: training accuarcy: 0.7035\n",
      "Epoch 4 step 1284: training loss: 1383.386834074184\n",
      "Epoch 4 step 1285: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1285: training loss: 1383.2260256704121\n",
      "Epoch 4 step 1286: training accuarcy: 0.682\n",
      "Epoch 4 step 1286: training loss: 1382.7713598859305\n",
      "Epoch 4 step 1287: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1287: training loss: 1382.9161862711344\n",
      "Epoch 4 step 1288: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1288: training loss: 1382.7465660033558\n",
      "Epoch 4 step 1289: training accuarcy: 0.7015\n",
      "Epoch 4 step 1289: training loss: 1383.1521192143537\n",
      "Epoch 4 step 1290: training accuarcy: 0.6825\n",
      "Epoch 4 step 1290: training loss: 1382.919535725292\n",
      "Epoch 4 step 1291: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1291: training loss: 1383.417946632399\n",
      "Epoch 4 step 1292: training accuarcy: 0.6795\n",
      "Epoch 4 step 1292: training loss: 1384.2938694509587\n",
      "Epoch 4 step 1293: training accuarcy: 0.681\n",
      "Epoch 4 step 1293: training loss: 1381.89921531084\n",
      "Epoch 4 step 1294: training accuarcy: 0.6985\n",
      "Epoch 4 step 1294: training loss: 1383.2747393477102\n",
      "Epoch 4 step 1295: training accuarcy: 0.7035\n",
      "Epoch 4 step 1295: training loss: 1383.8229885932556\n",
      "Epoch 4 step 1296: training accuarcy: 0.687\n",
      "Epoch 4 step 1296: training loss: 1382.7725136029621\n",
      "Epoch 4 step 1297: training accuarcy: 0.708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1297: training loss: 1383.4334950204632\n",
      "Epoch 4 step 1298: training accuarcy: 0.679\n",
      "Epoch 4 step 1298: training loss: 1382.6049478996752\n",
      "Epoch 4 step 1299: training accuarcy: 0.6985\n",
      "Epoch 4 step 1299: training loss: 1383.1202348315921\n",
      "Epoch 4 step 1300: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1300: training loss: 1383.5203515307292\n",
      "Epoch 4 step 1301: training accuarcy: 0.681\n",
      "Epoch 4 step 1301: training loss: 1383.1862973337197\n",
      "Epoch 4 step 1302: training accuarcy: 0.708\n",
      "Epoch 4 step 1302: training loss: 1382.9444791556332\n",
      "Epoch 4 step 1303: training accuarcy: 0.684\n",
      "Epoch 4 step 1303: training loss: 1382.371725197218\n",
      "Epoch 4 step 1304: training accuarcy: 0.6975\n",
      "Epoch 4 step 1304: training loss: 1382.606559640893\n",
      "Epoch 4 step 1305: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1305: training loss: 1382.8515706737464\n",
      "Epoch 4 step 1306: training accuarcy: 0.7035\n",
      "Epoch 4 step 1306: training loss: 1383.454628888206\n",
      "Epoch 4 step 1307: training accuarcy: 0.6875\n",
      "Epoch 4 step 1307: training loss: 1383.7621143081903\n",
      "Epoch 4 step 1308: training accuarcy: 0.6795\n",
      "Epoch 4 step 1308: training loss: 1382.3130457341572\n",
      "Epoch 4 step 1309: training accuarcy: 0.6935\n",
      "Epoch 4 step 1309: training loss: 1383.1834271176986\n",
      "Epoch 4 step 1310: training accuarcy: 0.683\n",
      "Epoch 4 step 1310: training loss: 1383.412112273902\n",
      "Epoch 4 step 1311: training accuarcy: 0.6880000000000001\n",
      "Epoch 4 step 1311: training loss: 1383.4289710979676\n",
      "Epoch 4 step 1312: training accuarcy: 0.686\n",
      "Epoch 4 step 1312: training loss: 1382.7629612603437\n",
      "Epoch 4 step 1313: training accuarcy: 0.684\n",
      "Epoch 4 step 1313: training loss: 1383.1199585790146\n",
      "Epoch 4 step 1314: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1314: training loss: 543.1382792785935\n",
      "Epoch 4 step 1315: training accuarcy: 0.6923076923076923\n",
      "Epoch 4: train loss 1379.587826988156, train accuarcy 0.7011027932167053\n",
      "Epoch 4: valid loss 1363.6866410149555, valid accuarcy 0.7122498750686646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                         | 5/8 [09:51<06:00, 120.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 1315: training loss: 1381.7430139295107\n",
      "Epoch 5 step 1316: training accuarcy: 0.7095\n",
      "Epoch 5 step 1316: training loss: 1382.0481881308806\n",
      "Epoch 5 step 1317: training accuarcy: 0.7095\n",
      "Epoch 5 step 1317: training loss: 1380.6654792173902\n",
      "Epoch 5 step 1318: training accuarcy: 0.7185\n",
      "Epoch 5 step 1318: training loss: 1382.1917287253682\n",
      "Epoch 5 step 1319: training accuarcy: 0.6925\n",
      "Epoch 5 step 1319: training loss: 1382.9747114238955\n",
      "Epoch 5 step 1320: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1320: training loss: 1381.605863538038\n",
      "Epoch 5 step 1321: training accuarcy: 0.7265\n",
      "Epoch 5 step 1321: training loss: 1382.7886336902134\n",
      "Epoch 5 step 1322: training accuarcy: 0.717\n",
      "Epoch 5 step 1322: training loss: 1382.2918614195526\n",
      "Epoch 5 step 1323: training accuarcy: 0.7075\n",
      "Epoch 5 step 1323: training loss: 1382.7890369344545\n",
      "Epoch 5 step 1324: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1324: training loss: 1381.0536386608583\n",
      "Epoch 5 step 1325: training accuarcy: 0.7355\n",
      "Epoch 5 step 1325: training loss: 1381.4990234430454\n",
      "Epoch 5 step 1326: training accuarcy: 0.7255\n",
      "Epoch 5 step 1326: training loss: 1382.1771793684345\n",
      "Epoch 5 step 1327: training accuarcy: 0.713\n",
      "Epoch 5 step 1327: training loss: 1381.7147141787048\n",
      "Epoch 5 step 1328: training accuarcy: 0.7235\n",
      "Epoch 5 step 1328: training loss: 1381.3933862506485\n",
      "Epoch 5 step 1329: training accuarcy: 0.716\n",
      "Epoch 5 step 1329: training loss: 1382.168421562897\n",
      "Epoch 5 step 1330: training accuarcy: 0.723\n",
      "Epoch 5 step 1330: training loss: 1381.9565367555529\n",
      "Epoch 5 step 1331: training accuarcy: 0.7205\n",
      "Epoch 5 step 1331: training loss: 1382.7577670226833\n",
      "Epoch 5 step 1332: training accuarcy: 0.7020000000000001\n",
      "Epoch 5 step 1332: training loss: 1382.0274896683422\n",
      "Epoch 5 step 1333: training accuarcy: 0.7165\n",
      "Epoch 5 step 1333: training loss: 1382.6823489305975\n",
      "Epoch 5 step 1334: training accuarcy: 0.716\n",
      "Epoch 5 step 1334: training loss: 1382.1648263686907\n",
      "Epoch 5 step 1335: training accuarcy: 0.723\n",
      "Epoch 5 step 1335: training loss: 1382.1522106787947\n",
      "Epoch 5 step 1336: training accuarcy: 0.719\n",
      "Epoch 5 step 1336: training loss: 1382.7630124287527\n",
      "Epoch 5 step 1337: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1337: training loss: 1382.728895442681\n",
      "Epoch 5 step 1338: training accuarcy: 0.6995\n",
      "Epoch 5 step 1338: training loss: 1382.594956906891\n",
      "Epoch 5 step 1339: training accuarcy: 0.715\n",
      "Epoch 5 step 1339: training loss: 1382.7014103027057\n",
      "Epoch 5 step 1340: training accuarcy: 0.7295\n",
      "Epoch 5 step 1340: training loss: 1382.6285203720759\n",
      "Epoch 5 step 1341: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1341: training loss: 1384.07666763788\n",
      "Epoch 5 step 1342: training accuarcy: 0.7085\n",
      "Epoch 5 step 1342: training loss: 1382.525481902144\n",
      "Epoch 5 step 1343: training accuarcy: 0.715\n",
      "Epoch 5 step 1343: training loss: 1382.3238878442755\n",
      "Epoch 5 step 1344: training accuarcy: 0.7085\n",
      "Epoch 5 step 1344: training loss: 1382.599617037363\n",
      "Epoch 5 step 1345: training accuarcy: 0.709\n",
      "Epoch 5 step 1345: training loss: 1383.3073867372227\n",
      "Epoch 5 step 1346: training accuarcy: 0.6945\n",
      "Epoch 5 step 1346: training loss: 1382.9058881337453\n",
      "Epoch 5 step 1347: training accuarcy: 0.7035\n",
      "Epoch 5 step 1347: training loss: 1381.2480632766358\n",
      "Epoch 5 step 1348: training accuarcy: 0.7235\n",
      "Epoch 5 step 1348: training loss: 1382.561288975631\n",
      "Epoch 5 step 1349: training accuarcy: 0.712\n",
      "Epoch 5 step 1349: training loss: 1382.3670718407484\n",
      "Epoch 5 step 1350: training accuarcy: 0.709\n",
      "Epoch 5 step 1350: training loss: 1382.6971105417317\n",
      "Epoch 5 step 1351: training accuarcy: 0.6955\n",
      "Epoch 5 step 1351: training loss: 1381.9597317058754\n",
      "Epoch 5 step 1352: training accuarcy: 0.72\n",
      "Epoch 5 step 1352: training loss: 1383.3086941367958\n",
      "Epoch 5 step 1353: training accuarcy: 0.6975\n",
      "Epoch 5 step 1353: training loss: 1382.2819382304835\n",
      "Epoch 5 step 1354: training accuarcy: 0.711\n",
      "Epoch 5 step 1354: training loss: 1383.1743567174783\n",
      "Epoch 5 step 1355: training accuarcy: 0.6965\n",
      "Epoch 5 step 1355: training loss: 1382.1885106138848\n",
      "Epoch 5 step 1356: training accuarcy: 0.7045\n",
      "Epoch 5 step 1356: training loss: 1382.6595766707314\n",
      "Epoch 5 step 1357: training accuarcy: 0.7075\n",
      "Epoch 5 step 1357: training loss: 1382.8952865557783\n",
      "Epoch 5 step 1358: training accuarcy: 0.7000000000000001\n",
      "Epoch 5 step 1358: training loss: 1382.1975232020372\n",
      "Epoch 5 step 1359: training accuarcy: 0.715\n",
      "Epoch 5 step 1359: training loss: 1381.8930015839205\n",
      "Epoch 5 step 1360: training accuarcy: 0.7005\n",
      "Epoch 5 step 1360: training loss: 1382.340972063037\n",
      "Epoch 5 step 1361: training accuarcy: 0.7055\n",
      "Epoch 5 step 1361: training loss: 1383.0985831860758\n",
      "Epoch 5 step 1362: training accuarcy: 0.7135\n",
      "Epoch 5 step 1362: training loss: 1382.0056637603739\n",
      "Epoch 5 step 1363: training accuarcy: 0.6945\n",
      "Epoch 5 step 1363: training loss: 1382.4215144264044\n",
      "Epoch 5 step 1364: training accuarcy: 0.6975\n",
      "Epoch 5 step 1364: training loss: 1382.5113139176847\n",
      "Epoch 5 step 1365: training accuarcy: 0.6865\n",
      "Epoch 5 step 1365: training loss: 1381.642814729337\n",
      "Epoch 5 step 1366: training accuarcy: 0.7055\n",
      "Epoch 5 step 1366: training loss: 1382.5917927293306\n",
      "Epoch 5 step 1367: training accuarcy: 0.687\n",
      "Epoch 5 step 1367: training loss: 1382.2917283118215\n",
      "Epoch 5 step 1368: training accuarcy: 0.726\n",
      "Epoch 5 step 1368: training loss: 1382.3499836845936\n",
      "Epoch 5 step 1369: training accuarcy: 0.7025\n",
      "Epoch 5 step 1369: training loss: 1383.2470042162984\n",
      "Epoch 5 step 1370: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1370: training loss: 1382.5862368666772\n",
      "Epoch 5 step 1371: training accuarcy: 0.7145\n",
      "Epoch 5 step 1371: training loss: 1383.2817897537343\n",
      "Epoch 5 step 1372: training accuarcy: 0.6905\n",
      "Epoch 5 step 1372: training loss: 1383.0545011114632\n",
      "Epoch 5 step 1373: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1373: training loss: 1383.7393124020582\n",
      "Epoch 5 step 1374: training accuarcy: 0.6910000000000001\n",
      "Epoch 5 step 1374: training loss: 1382.5157015495215\n",
      "Epoch 5 step 1375: training accuarcy: 0.716\n",
      "Epoch 5 step 1375: training loss: 1381.7623287302397\n",
      "Epoch 5 step 1376: training accuarcy: 0.7115\n",
      "Epoch 5 step 1376: training loss: 1383.5676495273417\n",
      "Epoch 5 step 1377: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1377: training loss: 1382.9085356244964\n",
      "Epoch 5 step 1378: training accuarcy: 0.6995\n",
      "Epoch 5 step 1378: training loss: 1382.4580594385711\n",
      "Epoch 5 step 1379: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1379: training loss: 1382.3471386079045\n",
      "Epoch 5 step 1380: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1380: training loss: 1382.367609724157\n",
      "Epoch 5 step 1381: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1381: training loss: 1383.5819791992028\n",
      "Epoch 5 step 1382: training accuarcy: 0.6895\n",
      "Epoch 5 step 1382: training loss: 1382.014700899339\n",
      "Epoch 5 step 1383: training accuarcy: 0.7135\n",
      "Epoch 5 step 1383: training loss: 1384.0662074544318\n",
      "Epoch 5 step 1384: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1384: training loss: 1382.4797349159705\n",
      "Epoch 5 step 1385: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1385: training loss: 1382.276192638554\n",
      "Epoch 5 step 1386: training accuarcy: 0.7155\n",
      "Epoch 5 step 1386: training loss: 1382.3502760250592\n",
      "Epoch 5 step 1387: training accuarcy: 0.7045\n",
      "Epoch 5 step 1387: training loss: 1381.967949058758\n",
      "Epoch 5 step 1388: training accuarcy: 0.7185\n",
      "Epoch 5 step 1388: training loss: 1382.9239596730295\n",
      "Epoch 5 step 1389: training accuarcy: 0.7065\n",
      "Epoch 5 step 1389: training loss: 1382.9070524510403\n",
      "Epoch 5 step 1390: training accuarcy: 0.7035\n",
      "Epoch 5 step 1390: training loss: 1382.4031485377284\n",
      "Epoch 5 step 1391: training accuarcy: 0.7095\n",
      "Epoch 5 step 1391: training loss: 1382.9887058363029\n",
      "Epoch 5 step 1392: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1392: training loss: 1382.8549500473064\n",
      "Epoch 5 step 1393: training accuarcy: 0.687\n",
      "Epoch 5 step 1393: training loss: 1382.2470238201543\n",
      "Epoch 5 step 1394: training accuarcy: 0.6995\n",
      "Epoch 5 step 1394: training loss: 1382.8606774795512\n",
      "Epoch 5 step 1395: training accuarcy: 0.7020000000000001\n",
      "Epoch 5 step 1395: training loss: 1383.4043662530019\n",
      "Epoch 5 step 1396: training accuarcy: 0.6965\n",
      "Epoch 5 step 1396: training loss: 1382.5012266479446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1397: training accuarcy: 0.708\n",
      "Epoch 5 step 1397: training loss: 1382.8126362361938\n",
      "Epoch 5 step 1398: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1398: training loss: 1382.391627564742\n",
      "Epoch 5 step 1399: training accuarcy: 0.705\n",
      "Epoch 5 step 1399: training loss: 1383.3915142571245\n",
      "Epoch 5 step 1400: training accuarcy: 0.7045\n",
      "Epoch 5 step 1400: training loss: 1382.441820889064\n",
      "Epoch 5 step 1401: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1401: training loss: 1383.5258328182952\n",
      "Epoch 5 step 1402: training accuarcy: 0.7005\n",
      "Epoch 5 step 1402: training loss: 1382.9377967390399\n",
      "Epoch 5 step 1403: training accuarcy: 0.6935\n",
      "Epoch 5 step 1403: training loss: 1381.9188440805449\n",
      "Epoch 5 step 1404: training accuarcy: 0.712\n",
      "Epoch 5 step 1404: training loss: 1382.8926918317238\n",
      "Epoch 5 step 1405: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1405: training loss: 1383.3602291657726\n",
      "Epoch 5 step 1406: training accuarcy: 0.6945\n",
      "Epoch 5 step 1406: training loss: 1383.2008238190926\n",
      "Epoch 5 step 1407: training accuarcy: 0.7015\n",
      "Epoch 5 step 1407: training loss: 1382.7844434447159\n",
      "Epoch 5 step 1408: training accuarcy: 0.711\n",
      "Epoch 5 step 1408: training loss: 1382.6051478332458\n",
      "Epoch 5 step 1409: training accuarcy: 0.7065\n",
      "Epoch 5 step 1409: training loss: 1381.1059622534685\n",
      "Epoch 5 step 1410: training accuarcy: 0.716\n",
      "Epoch 5 step 1410: training loss: 1381.8871400209216\n",
      "Epoch 5 step 1411: training accuarcy: 0.7075\n",
      "Epoch 5 step 1411: training loss: 1382.9164714117474\n",
      "Epoch 5 step 1412: training accuarcy: 0.7045\n",
      "Epoch 5 step 1412: training loss: 1383.2101231663491\n",
      "Epoch 5 step 1413: training accuarcy: 0.6915\n",
      "Epoch 5 step 1413: training loss: 1383.0582316933987\n",
      "Epoch 5 step 1414: training accuarcy: 0.6955\n",
      "Epoch 5 step 1414: training loss: 1382.4925299457077\n",
      "Epoch 5 step 1415: training accuarcy: 0.7105\n",
      "Epoch 5 step 1415: training loss: 1382.7377140208496\n",
      "Epoch 5 step 1416: training accuarcy: 0.713\n",
      "Epoch 5 step 1416: training loss: 1383.526802301007\n",
      "Epoch 5 step 1417: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1417: training loss: 1382.7760600124411\n",
      "Epoch 5 step 1418: training accuarcy: 0.6855\n",
      "Epoch 5 step 1418: training loss: 1383.5280697862177\n",
      "Epoch 5 step 1419: training accuarcy: 0.6915\n",
      "Epoch 5 step 1419: training loss: 1383.5512790298924\n",
      "Epoch 5 step 1420: training accuarcy: 0.6950000000000001\n",
      "Epoch 5 step 1420: training loss: 1383.0258164036018\n",
      "Epoch 5 step 1421: training accuarcy: 0.6975\n",
      "Epoch 5 step 1421: training loss: 1383.3649494544936\n",
      "Epoch 5 step 1422: training accuarcy: 0.6965\n",
      "Epoch 5 step 1422: training loss: 1382.2027039083382\n",
      "Epoch 5 step 1423: training accuarcy: 0.7175\n",
      "Epoch 5 step 1423: training loss: 1383.507029707887\n",
      "Epoch 5 step 1424: training accuarcy: 0.6795\n",
      "Epoch 5 step 1424: training loss: 1383.6915459363454\n",
      "Epoch 5 step 1425: training accuarcy: 0.6885\n",
      "Epoch 5 step 1425: training loss: 1382.7983415428798\n",
      "Epoch 5 step 1426: training accuarcy: 0.704\n",
      "Epoch 5 step 1426: training loss: 1382.5784888202438\n",
      "Epoch 5 step 1427: training accuarcy: 0.7115\n",
      "Epoch 5 step 1427: training loss: 1382.6735001927896\n",
      "Epoch 5 step 1428: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1428: training loss: 1383.143476851463\n",
      "Epoch 5 step 1429: training accuarcy: 0.6915\n",
      "Epoch 5 step 1429: training loss: 1382.0723947590159\n",
      "Epoch 5 step 1430: training accuarcy: 0.727\n",
      "Epoch 5 step 1430: training loss: 1384.139090667186\n",
      "Epoch 5 step 1431: training accuarcy: 0.6825\n",
      "Epoch 5 step 1431: training loss: 1382.6835527360822\n",
      "Epoch 5 step 1432: training accuarcy: 0.686\n",
      "Epoch 5 step 1432: training loss: 1382.687121134065\n",
      "Epoch 5 step 1433: training accuarcy: 0.7005\n",
      "Epoch 5 step 1433: training loss: 1383.2058319533837\n",
      "Epoch 5 step 1434: training accuarcy: 0.6985\n",
      "Epoch 5 step 1434: training loss: 1381.6574095736173\n",
      "Epoch 5 step 1435: training accuarcy: 0.7125\n",
      "Epoch 5 step 1435: training loss: 1383.5830643310906\n",
      "Epoch 5 step 1436: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1436: training loss: 1383.2206082098876\n",
      "Epoch 5 step 1437: training accuarcy: 0.6945\n",
      "Epoch 5 step 1437: training loss: 1382.9644829193421\n",
      "Epoch 5 step 1438: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1438: training loss: 1383.1544585015354\n",
      "Epoch 5 step 1439: training accuarcy: 0.6865\n",
      "Epoch 5 step 1439: training loss: 1382.0913852594256\n",
      "Epoch 5 step 1440: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1440: training loss: 1383.1198256350408\n",
      "Epoch 5 step 1441: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1441: training loss: 1382.5270106987757\n",
      "Epoch 5 step 1442: training accuarcy: 0.7115\n",
      "Epoch 5 step 1442: training loss: 1382.9920785529316\n",
      "Epoch 5 step 1443: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1443: training loss: 1383.002153325405\n",
      "Epoch 5 step 1444: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1444: training loss: 1382.502709100123\n",
      "Epoch 5 step 1445: training accuarcy: 0.687\n",
      "Epoch 5 step 1445: training loss: 1382.4973770295362\n",
      "Epoch 5 step 1446: training accuarcy: 0.6805\n",
      "Epoch 5 step 1446: training loss: 1382.7969827901807\n",
      "Epoch 5 step 1447: training accuarcy: 0.7035\n",
      "Epoch 5 step 1447: training loss: 1383.6517270108716\n",
      "Epoch 5 step 1448: training accuarcy: 0.6835\n",
      "Epoch 5 step 1448: training loss: 1382.1039567751625\n",
      "Epoch 5 step 1449: training accuarcy: 0.7085\n",
      "Epoch 5 step 1449: training loss: 1383.395107285104\n",
      "Epoch 5 step 1450: training accuarcy: 0.683\n",
      "Epoch 5 step 1450: training loss: 1383.0567009354386\n",
      "Epoch 5 step 1451: training accuarcy: 0.683\n",
      "Epoch 5 step 1451: training loss: 1383.7683891798908\n",
      "Epoch 5 step 1452: training accuarcy: 0.6765\n",
      "Epoch 5 step 1452: training loss: 1382.4569574143316\n",
      "Epoch 5 step 1453: training accuarcy: 0.7035\n",
      "Epoch 5 step 1453: training loss: 1382.5800672410098\n",
      "Epoch 5 step 1454: training accuarcy: 0.7025\n",
      "Epoch 5 step 1454: training loss: 1382.2754698790895\n",
      "Epoch 5 step 1455: training accuarcy: 0.7035\n",
      "Epoch 5 step 1455: training loss: 1383.532889608138\n",
      "Epoch 5 step 1456: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1456: training loss: 1381.4004755581861\n",
      "Epoch 5 step 1457: training accuarcy: 0.712\n",
      "Epoch 5 step 1457: training loss: 1382.4081588589586\n",
      "Epoch 5 step 1458: training accuarcy: 0.71\n",
      "Epoch 5 step 1458: training loss: 1382.6435844331727\n",
      "Epoch 5 step 1459: training accuarcy: 0.7085\n",
      "Epoch 5 step 1459: training loss: 1382.4032846742507\n",
      "Epoch 5 step 1460: training accuarcy: 0.6955\n",
      "Epoch 5 step 1460: training loss: 1382.2496584988294\n",
      "Epoch 5 step 1461: training accuarcy: 0.7095\n",
      "Epoch 5 step 1461: training loss: 1383.2161649203197\n",
      "Epoch 5 step 1462: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1462: training loss: 1382.949123710178\n",
      "Epoch 5 step 1463: training accuarcy: 0.6925\n",
      "Epoch 5 step 1463: training loss: 1382.665128360635\n",
      "Epoch 5 step 1464: training accuarcy: 0.7045\n",
      "Epoch 5 step 1464: training loss: 1381.9745305281024\n",
      "Epoch 5 step 1465: training accuarcy: 0.709\n",
      "Epoch 5 step 1465: training loss: 1382.933640655328\n",
      "Epoch 5 step 1466: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1466: training loss: 1383.1969612013015\n",
      "Epoch 5 step 1467: training accuarcy: 0.6875\n",
      "Epoch 5 step 1467: training loss: 1382.5509886558416\n",
      "Epoch 5 step 1468: training accuarcy: 0.7035\n",
      "Epoch 5 step 1468: training loss: 1384.0078272877952\n",
      "Epoch 5 step 1469: training accuarcy: 0.6705\n",
      "Epoch 5 step 1469: training loss: 1382.8095877056628\n",
      "Epoch 5 step 1470: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1470: training loss: 1382.5209201799041\n",
      "Epoch 5 step 1471: training accuarcy: 0.7095\n",
      "Epoch 5 step 1471: training loss: 1383.3913102398662\n",
      "Epoch 5 step 1472: training accuarcy: 0.6985\n",
      "Epoch 5 step 1472: training loss: 1383.3066841297848\n",
      "Epoch 5 step 1473: training accuarcy: 0.6895\n",
      "Epoch 5 step 1473: training loss: 1382.9987023530723\n",
      "Epoch 5 step 1474: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1474: training loss: 1382.8309153225923\n",
      "Epoch 5 step 1475: training accuarcy: 0.707\n",
      "Epoch 5 step 1475: training loss: 1382.7883892306158\n",
      "Epoch 5 step 1476: training accuarcy: 0.6945\n",
      "Epoch 5 step 1476: training loss: 1383.712634597902\n",
      "Epoch 5 step 1477: training accuarcy: 0.6785\n",
      "Epoch 5 step 1477: training loss: 1382.1731872599025\n",
      "Epoch 5 step 1478: training accuarcy: 0.726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1478: training loss: 1382.9995514607767\n",
      "Epoch 5 step 1479: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1479: training loss: 1382.914911136799\n",
      "Epoch 5 step 1480: training accuarcy: 0.6815\n",
      "Epoch 5 step 1480: training loss: 1382.897355626293\n",
      "Epoch 5 step 1481: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1481: training loss: 1383.0658116648556\n",
      "Epoch 5 step 1482: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1482: training loss: 1382.016961750663\n",
      "Epoch 5 step 1483: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1483: training loss: 1383.6033826369344\n",
      "Epoch 5 step 1484: training accuarcy: 0.6865\n",
      "Epoch 5 step 1484: training loss: 1382.7465324346747\n",
      "Epoch 5 step 1485: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1485: training loss: 1382.7022507684408\n",
      "Epoch 5 step 1486: training accuarcy: 0.7055\n",
      "Epoch 5 step 1486: training loss: 1382.9449926222337\n",
      "Epoch 5 step 1487: training accuarcy: 0.6895\n",
      "Epoch 5 step 1487: training loss: 1383.5467038109782\n",
      "Epoch 5 step 1488: training accuarcy: 0.7065\n",
      "Epoch 5 step 1488: training loss: 1382.9110757047647\n",
      "Epoch 5 step 1489: training accuarcy: 0.7020000000000001\n",
      "Epoch 5 step 1489: training loss: 1383.0170378647206\n",
      "Epoch 5 step 1490: training accuarcy: 0.68\n",
      "Epoch 5 step 1490: training loss: 1383.6310011574108\n",
      "Epoch 5 step 1491: training accuarcy: 0.6845\n",
      "Epoch 5 step 1491: training loss: 1382.3506476263422\n",
      "Epoch 5 step 1492: training accuarcy: 0.6890000000000001\n",
      "Epoch 5 step 1492: training loss: 1383.1248238832789\n",
      "Epoch 5 step 1493: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1493: training loss: 1382.5681212410605\n",
      "Epoch 5 step 1494: training accuarcy: 0.705\n",
      "Epoch 5 step 1494: training loss: 1383.015297817909\n",
      "Epoch 5 step 1495: training accuarcy: 0.6910000000000001\n",
      "Epoch 5 step 1495: training loss: 1382.3674882453643\n",
      "Epoch 5 step 1496: training accuarcy: 0.7045\n",
      "Epoch 5 step 1496: training loss: 1382.5215627308878\n",
      "Epoch 5 step 1497: training accuarcy: 0.6905\n",
      "Epoch 5 step 1497: training loss: 1382.3113305135696\n",
      "Epoch 5 step 1498: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1498: training loss: 1383.4038016844056\n",
      "Epoch 5 step 1499: training accuarcy: 0.6925\n",
      "Epoch 5 step 1499: training loss: 1383.6470970211028\n",
      "Epoch 5 step 1500: training accuarcy: 0.6775\n",
      "Epoch 5 step 1500: training loss: 1382.3434216845606\n",
      "Epoch 5 step 1501: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1501: training loss: 1382.6353066682932\n",
      "Epoch 5 step 1502: training accuarcy: 0.713\n",
      "Epoch 5 step 1502: training loss: 1382.9335171595928\n",
      "Epoch 5 step 1503: training accuarcy: 0.6955\n",
      "Epoch 5 step 1503: training loss: 1383.2733187699803\n",
      "Epoch 5 step 1504: training accuarcy: 0.7025\n",
      "Epoch 5 step 1504: training loss: 1383.9975388219552\n",
      "Epoch 5 step 1505: training accuarcy: 0.676\n",
      "Epoch 5 step 1505: training loss: 1383.6715199901741\n",
      "Epoch 5 step 1506: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1506: training loss: 1381.8861172985758\n",
      "Epoch 5 step 1507: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1507: training loss: 1383.6867704362735\n",
      "Epoch 5 step 1508: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1508: training loss: 1382.5442233058764\n",
      "Epoch 5 step 1509: training accuarcy: 0.6995\n",
      "Epoch 5 step 1509: training loss: 1383.2393870401386\n",
      "Epoch 5 step 1510: training accuarcy: 0.684\n",
      "Epoch 5 step 1510: training loss: 1383.4421836518843\n",
      "Epoch 5 step 1511: training accuarcy: 0.7045\n",
      "Epoch 5 step 1511: training loss: 1383.049471679082\n",
      "Epoch 5 step 1512: training accuarcy: 0.706\n",
      "Epoch 5 step 1512: training loss: 1381.3009987709813\n",
      "Epoch 5 step 1513: training accuarcy: 0.7105\n",
      "Epoch 5 step 1513: training loss: 1384.4338233646415\n",
      "Epoch 5 step 1514: training accuarcy: 0.685\n",
      "Epoch 5 step 1514: training loss: 1382.4068682930651\n",
      "Epoch 5 step 1515: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1515: training loss: 1382.8388608407395\n",
      "Epoch 5 step 1516: training accuarcy: 0.708\n",
      "Epoch 5 step 1516: training loss: 1382.830058468603\n",
      "Epoch 5 step 1517: training accuarcy: 0.7000000000000001\n",
      "Epoch 5 step 1517: training loss: 1383.0562411497278\n",
      "Epoch 5 step 1518: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1518: training loss: 1382.7965428759517\n",
      "Epoch 5 step 1519: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1519: training loss: 1382.7786202014047\n",
      "Epoch 5 step 1520: training accuarcy: 0.686\n",
      "Epoch 5 step 1520: training loss: 1382.8167335564508\n",
      "Epoch 5 step 1521: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1521: training loss: 1382.7728069132713\n",
      "Epoch 5 step 1522: training accuarcy: 0.7045\n",
      "Epoch 5 step 1522: training loss: 1383.6685177501518\n",
      "Epoch 5 step 1523: training accuarcy: 0.6915\n",
      "Epoch 5 step 1523: training loss: 1383.2256417119263\n",
      "Epoch 5 step 1524: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1524: training loss: 1383.0736195663108\n",
      "Epoch 5 step 1525: training accuarcy: 0.6895\n",
      "Epoch 5 step 1525: training loss: 1383.2996919668806\n",
      "Epoch 5 step 1526: training accuarcy: 0.6910000000000001\n",
      "Epoch 5 step 1526: training loss: 1383.288430479522\n",
      "Epoch 5 step 1527: training accuarcy: 0.6895\n",
      "Epoch 5 step 1527: training loss: 1382.9997793883006\n",
      "Epoch 5 step 1528: training accuarcy: 0.6890000000000001\n",
      "Epoch 5 step 1528: training loss: 1382.003679603772\n",
      "Epoch 5 step 1529: training accuarcy: 0.7045\n",
      "Epoch 5 step 1529: training loss: 1383.0085267976274\n",
      "Epoch 5 step 1530: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1530: training loss: 1382.8151420265583\n",
      "Epoch 5 step 1531: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1531: training loss: 1383.6678459968593\n",
      "Epoch 5 step 1532: training accuarcy: 0.7155\n",
      "Epoch 5 step 1532: training loss: 1381.7529209499476\n",
      "Epoch 5 step 1533: training accuarcy: 0.7115\n",
      "Epoch 5 step 1533: training loss: 1383.862016891811\n",
      "Epoch 5 step 1534: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1534: training loss: 1383.6277466192676\n",
      "Epoch 5 step 1535: training accuarcy: 0.684\n",
      "Epoch 5 step 1535: training loss: 1383.0588501807706\n",
      "Epoch 5 step 1536: training accuarcy: 0.6885\n",
      "Epoch 5 step 1536: training loss: 1383.7543380499953\n",
      "Epoch 5 step 1537: training accuarcy: 0.681\n",
      "Epoch 5 step 1537: training loss: 1383.2111636137508\n",
      "Epoch 5 step 1538: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1538: training loss: 1382.6757958796263\n",
      "Epoch 5 step 1539: training accuarcy: 0.683\n",
      "Epoch 5 step 1539: training loss: 1383.0631808188296\n",
      "Epoch 5 step 1540: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1540: training loss: 1383.0813097048263\n",
      "Epoch 5 step 1541: training accuarcy: 0.6950000000000001\n",
      "Epoch 5 step 1541: training loss: 1383.248999479072\n",
      "Epoch 5 step 1542: training accuarcy: 0.6985\n",
      "Epoch 5 step 1542: training loss: 1382.9840239763691\n",
      "Epoch 5 step 1543: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1543: training loss: 1383.1787891304423\n",
      "Epoch 5 step 1544: training accuarcy: 0.6865\n",
      "Epoch 5 step 1544: training loss: 1383.4672694255155\n",
      "Epoch 5 step 1545: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1545: training loss: 1384.8951293287478\n",
      "Epoch 5 step 1546: training accuarcy: 0.6795\n",
      "Epoch 5 step 1546: training loss: 1381.5036152808295\n",
      "Epoch 5 step 1547: training accuarcy: 0.7165\n",
      "Epoch 5 step 1547: training loss: 1382.9546264105738\n",
      "Epoch 5 step 1548: training accuarcy: 0.7015\n",
      "Epoch 5 step 1548: training loss: 1383.1797442835527\n",
      "Epoch 5 step 1549: training accuarcy: 0.672\n",
      "Epoch 5 step 1549: training loss: 1382.2864824165054\n",
      "Epoch 5 step 1550: training accuarcy: 0.6925\n",
      "Epoch 5 step 1550: training loss: 1382.0808482960047\n",
      "Epoch 5 step 1551: training accuarcy: 0.7095\n",
      "Epoch 5 step 1551: training loss: 1383.0447850791447\n",
      "Epoch 5 step 1552: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1552: training loss: 1384.007826511675\n",
      "Epoch 5 step 1553: training accuarcy: 0.6845\n",
      "Epoch 5 step 1553: training loss: 1382.9157205449835\n",
      "Epoch 5 step 1554: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1554: training loss: 1383.038237839451\n",
      "Epoch 5 step 1555: training accuarcy: 0.7025\n",
      "Epoch 5 step 1555: training loss: 1382.311694300821\n",
      "Epoch 5 step 1556: training accuarcy: 0.6955\n",
      "Epoch 5 step 1556: training loss: 1382.9184849498251\n",
      "Epoch 5 step 1557: training accuarcy: 0.6950000000000001\n",
      "Epoch 5 step 1557: training loss: 1381.8293015258707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1558: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1558: training loss: 1383.0066986680454\n",
      "Epoch 5 step 1559: training accuarcy: 0.6915\n",
      "Epoch 5 step 1559: training loss: 1382.1709518592154\n",
      "Epoch 5 step 1560: training accuarcy: 0.717\n",
      "Epoch 5 step 1560: training loss: 1383.010156249655\n",
      "Epoch 5 step 1561: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1561: training loss: 1383.0715730022264\n",
      "Epoch 5 step 1562: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1562: training loss: 1382.89232141381\n",
      "Epoch 5 step 1563: training accuarcy: 0.6935\n",
      "Epoch 5 step 1563: training loss: 1382.9613326499905\n",
      "Epoch 5 step 1564: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1564: training loss: 1382.5014619569124\n",
      "Epoch 5 step 1565: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1565: training loss: 1382.7205394417886\n",
      "Epoch 5 step 1566: training accuarcy: 0.711\n",
      "Epoch 5 step 1566: training loss: 1382.7197085007454\n",
      "Epoch 5 step 1567: training accuarcy: 0.7135\n",
      "Epoch 5 step 1567: training loss: 1383.3869820178697\n",
      "Epoch 5 step 1568: training accuarcy: 0.6855\n",
      "Epoch 5 step 1568: training loss: 1383.3667229475143\n",
      "Epoch 5 step 1569: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1569: training loss: 1382.3542842302745\n",
      "Epoch 5 step 1570: training accuarcy: 0.7075\n",
      "Epoch 5 step 1570: training loss: 1384.0596020185553\n",
      "Epoch 5 step 1571: training accuarcy: 0.68\n",
      "Epoch 5 step 1571: training loss: 1382.7819025439826\n",
      "Epoch 5 step 1572: training accuarcy: 0.6865\n",
      "Epoch 5 step 1572: training loss: 1382.3503130588003\n",
      "Epoch 5 step 1573: training accuarcy: 0.6945\n",
      "Epoch 5 step 1573: training loss: 1383.5837746015932\n",
      "Epoch 5 step 1574: training accuarcy: 0.704\n",
      "Epoch 5 step 1574: training loss: 1381.7941270700296\n",
      "Epoch 5 step 1575: training accuarcy: 0.7105\n",
      "Epoch 5 step 1575: training loss: 1383.864787320412\n",
      "Epoch 5 step 1576: training accuarcy: 0.687\n",
      "Epoch 5 step 1576: training loss: 1382.5587172623423\n",
      "Epoch 5 step 1577: training accuarcy: 0.709\n",
      "Epoch 5 step 1577: training loss: 542.6906170146822\n",
      "Epoch 5 step 1578: training accuarcy: 0.6974358974358974\n",
      "Epoch 5: train loss 1379.5859887868876, train accuarcy 0.7022712826728821\n",
      "Epoch 5: valid loss 1363.8610506653429, valid accuarcy 0.7128562927246094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                      | 6/8 [11:58<04:04, 122.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 1578: training loss: 1381.9675770691683\n",
      "Epoch 6 step 1579: training accuarcy: 0.712\n",
      "Epoch 6 step 1579: training loss: 1382.0272766838964\n",
      "Epoch 6 step 1580: training accuarcy: 0.715\n",
      "Epoch 6 step 1580: training loss: 1381.6563032627062\n",
      "Epoch 6 step 1581: training accuarcy: 0.723\n",
      "Epoch 6 step 1581: training loss: 1381.134598980179\n",
      "Epoch 6 step 1582: training accuarcy: 0.724\n",
      "Epoch 6 step 1582: training loss: 1382.3083225159285\n",
      "Epoch 6 step 1583: training accuarcy: 0.7145\n",
      "Epoch 6 step 1583: training loss: 1380.7826923964485\n",
      "Epoch 6 step 1584: training accuarcy: 0.728\n",
      "Epoch 6 step 1584: training loss: 1381.892443708029\n",
      "Epoch 6 step 1585: training accuarcy: 0.7385\n",
      "Epoch 6 step 1585: training loss: 1381.9904319996722\n",
      "Epoch 6 step 1586: training accuarcy: 0.7115\n",
      "Epoch 6 step 1586: training loss: 1382.297759629386\n",
      "Epoch 6 step 1587: training accuarcy: 0.7255\n",
      "Epoch 6 step 1587: training loss: 1381.7433709642182\n",
      "Epoch 6 step 1588: training accuarcy: 0.713\n",
      "Epoch 6 step 1588: training loss: 1382.1604648000466\n",
      "Epoch 6 step 1589: training accuarcy: 0.713\n",
      "Epoch 6 step 1589: training loss: 1382.511790612164\n",
      "Epoch 6 step 1590: training accuarcy: 0.7075\n",
      "Epoch 6 step 1590: training loss: 1381.4905722388544\n",
      "Epoch 6 step 1591: training accuarcy: 0.7175\n",
      "Epoch 6 step 1591: training loss: 1382.275203931083\n",
      "Epoch 6 step 1592: training accuarcy: 0.7155\n",
      "Epoch 6 step 1592: training loss: 1383.546969341364\n",
      "Epoch 6 step 1593: training accuarcy: 0.7085\n",
      "Epoch 6 step 1593: training loss: 1382.5285730494704\n",
      "Epoch 6 step 1594: training accuarcy: 0.7045\n",
      "Epoch 6 step 1594: training loss: 1382.7431361004037\n",
      "Epoch 6 step 1595: training accuarcy: 0.6935\n",
      "Epoch 6 step 1595: training loss: 1382.0569092672126\n",
      "Epoch 6 step 1596: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1596: training loss: 1381.7993406862001\n",
      "Epoch 6 step 1597: training accuarcy: 0.716\n",
      "Epoch 6 step 1597: training loss: 1382.587739026875\n",
      "Epoch 6 step 1598: training accuarcy: 0.7165\n",
      "Epoch 6 step 1598: training loss: 1382.170754155124\n",
      "Epoch 6 step 1599: training accuarcy: 0.71\n",
      "Epoch 6 step 1599: training loss: 1382.9195702010593\n",
      "Epoch 6 step 1600: training accuarcy: 0.7085\n",
      "Epoch 6 step 1600: training loss: 1381.7025425376307\n",
      "Epoch 6 step 1601: training accuarcy: 0.7185\n",
      "Epoch 6 step 1601: training loss: 1382.591333151828\n",
      "Epoch 6 step 1602: training accuarcy: 0.72\n",
      "Epoch 6 step 1602: training loss: 1382.09542787208\n",
      "Epoch 6 step 1603: training accuarcy: 0.7225\n",
      "Epoch 6 step 1603: training loss: 1382.755982586662\n",
      "Epoch 6 step 1604: training accuarcy: 0.7215\n",
      "Epoch 6 step 1604: training loss: 1382.3125832773853\n",
      "Epoch 6 step 1605: training accuarcy: 0.704\n",
      "Epoch 6 step 1605: training loss: 1383.3175496430101\n",
      "Epoch 6 step 1606: training accuarcy: 0.7065\n",
      "Epoch 6 step 1606: training loss: 1382.6072957771564\n",
      "Epoch 6 step 1607: training accuarcy: 0.6975\n",
      "Epoch 6 step 1607: training loss: 1381.9863832403403\n",
      "Epoch 6 step 1608: training accuarcy: 0.6935\n",
      "Epoch 6 step 1608: training loss: 1382.564633362234\n",
      "Epoch 6 step 1609: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1609: training loss: 1382.6830397390936\n",
      "Epoch 6 step 1610: training accuarcy: 0.7015\n",
      "Epoch 6 step 1610: training loss: 1383.2709651962393\n",
      "Epoch 6 step 1611: training accuarcy: 0.7005\n",
      "Epoch 6 step 1611: training loss: 1384.6355372173055\n",
      "Epoch 6 step 1612: training accuarcy: 0.6755\n",
      "Epoch 6 step 1612: training loss: 1382.290494184569\n",
      "Epoch 6 step 1613: training accuarcy: 0.71\n",
      "Epoch 6 step 1613: training loss: 1383.3439664590176\n",
      "Epoch 6 step 1614: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1614: training loss: 1382.774308503921\n",
      "Epoch 6 step 1615: training accuarcy: 0.7015\n",
      "Epoch 6 step 1615: training loss: 1381.809342148173\n",
      "Epoch 6 step 1616: training accuarcy: 0.712\n",
      "Epoch 6 step 1616: training loss: 1382.7985144999143\n",
      "Epoch 6 step 1617: training accuarcy: 0.6965\n",
      "Epoch 6 step 1617: training loss: 1381.5232688939648\n",
      "Epoch 6 step 1618: training accuarcy: 0.7195\n",
      "Epoch 6 step 1618: training loss: 1383.0455208794474\n",
      "Epoch 6 step 1619: training accuarcy: 0.7025\n",
      "Epoch 6 step 1619: training loss: 1382.2003748852287\n",
      "Epoch 6 step 1620: training accuarcy: 0.7155\n",
      "Epoch 6 step 1620: training loss: 1382.494334265082\n",
      "Epoch 6 step 1621: training accuarcy: 0.707\n",
      "Epoch 6 step 1621: training loss: 1382.666792327653\n",
      "Epoch 6 step 1622: training accuarcy: 0.711\n",
      "Epoch 6 step 1622: training loss: 1382.6381638664566\n",
      "Epoch 6 step 1623: training accuarcy: 0.7025\n",
      "Epoch 6 step 1623: training loss: 1383.149133092724\n",
      "Epoch 6 step 1624: training accuarcy: 0.7005\n",
      "Epoch 6 step 1624: training loss: 1382.1897127764837\n",
      "Epoch 6 step 1625: training accuarcy: 0.6910000000000001\n",
      "Epoch 6 step 1625: training loss: 1382.3803415728055\n",
      "Epoch 6 step 1626: training accuarcy: 0.708\n",
      "Epoch 6 step 1626: training loss: 1381.6357289376892\n",
      "Epoch 6 step 1627: training accuarcy: 0.713\n",
      "Epoch 6 step 1627: training loss: 1382.916535001895\n",
      "Epoch 6 step 1628: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1628: training loss: 1381.8539534547945\n",
      "Epoch 6 step 1629: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1629: training loss: 1381.9342073855562\n",
      "Epoch 6 step 1630: training accuarcy: 0.708\n",
      "Epoch 6 step 1630: training loss: 1382.5489044210092\n",
      "Epoch 6 step 1631: training accuarcy: 0.6985\n",
      "Epoch 6 step 1631: training loss: 1382.7504483996593\n",
      "Epoch 6 step 1632: training accuarcy: 0.716\n",
      "Epoch 6 step 1632: training loss: 1382.6578169915647\n",
      "Epoch 6 step 1633: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1633: training loss: 1383.5417490390873\n",
      "Epoch 6 step 1634: training accuarcy: 0.6995\n",
      "Epoch 6 step 1634: training loss: 1382.509568207115\n",
      "Epoch 6 step 1635: training accuarcy: 0.6925\n",
      "Epoch 6 step 1635: training loss: 1383.9173597948115\n",
      "Epoch 6 step 1636: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1636: training loss: 1382.3536859067956\n",
      "Epoch 6 step 1637: training accuarcy: 0.6945\n",
      "Epoch 6 step 1637: training loss: 1383.6165499506076\n",
      "Epoch 6 step 1638: training accuarcy: 0.677\n",
      "Epoch 6 step 1638: training loss: 1380.8409978767436\n",
      "Epoch 6 step 1639: training accuarcy: 0.7115\n",
      "Epoch 6 step 1639: training loss: 1382.73524772723\n",
      "Epoch 6 step 1640: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1640: training loss: 1382.8993694354795\n",
      "Epoch 6 step 1641: training accuarcy: 0.7085\n",
      "Epoch 6 step 1641: training loss: 1382.3444044578519\n",
      "Epoch 6 step 1642: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1642: training loss: 1382.90295766026\n",
      "Epoch 6 step 1643: training accuarcy: 0.7065\n",
      "Epoch 6 step 1643: training loss: 1381.7096254305088\n",
      "Epoch 6 step 1644: training accuarcy: 0.7155\n",
      "Epoch 6 step 1644: training loss: 1383.3856893283037\n",
      "Epoch 6 step 1645: training accuarcy: 0.6855\n",
      "Epoch 6 step 1645: training loss: 1382.1270620968803\n",
      "Epoch 6 step 1646: training accuarcy: 0.714\n",
      "Epoch 6 step 1646: training loss: 1381.4240574491716\n",
      "Epoch 6 step 1647: training accuarcy: 0.7175\n",
      "Epoch 6 step 1647: training loss: 1382.5350147237275\n",
      "Epoch 6 step 1648: training accuarcy: 0.6975\n",
      "Epoch 6 step 1648: training loss: 1382.1326053092937\n",
      "Epoch 6 step 1649: training accuarcy: 0.714\n",
      "Epoch 6 step 1649: training loss: 1382.6752641107134\n",
      "Epoch 6 step 1650: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1650: training loss: 1382.758718567189\n",
      "Epoch 6 step 1651: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1651: training loss: 1383.2755646978499\n",
      "Epoch 6 step 1652: training accuarcy: 0.6975\n",
      "Epoch 6 step 1652: training loss: 1382.9898136191412\n",
      "Epoch 6 step 1653: training accuarcy: 0.6995\n",
      "Epoch 6 step 1653: training loss: 1382.0708966989598\n",
      "Epoch 6 step 1654: training accuarcy: 0.708\n",
      "Epoch 6 step 1654: training loss: 1383.5214567762675\n",
      "Epoch 6 step 1655: training accuarcy: 0.6935\n",
      "Epoch 6 step 1655: training loss: 1383.4971984403621\n",
      "Epoch 6 step 1656: training accuarcy: 0.6845\n",
      "Epoch 6 step 1656: training loss: 1382.3422455431794\n",
      "Epoch 6 step 1657: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1657: training loss: 1382.2894165083485\n",
      "Epoch 6 step 1658: training accuarcy: 0.7225\n",
      "Epoch 6 step 1658: training loss: 1383.1340629353924\n",
      "Epoch 6 step 1659: training accuarcy: 0.6925\n",
      "Epoch 6 step 1659: training loss: 1382.3521395997811\n",
      "Epoch 6 step 1660: training accuarcy: 0.6985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1660: training loss: 1382.513903374078\n",
      "Epoch 6 step 1661: training accuarcy: 0.706\n",
      "Epoch 6 step 1661: training loss: 1382.1241602869293\n",
      "Epoch 6 step 1662: training accuarcy: 0.7015\n",
      "Epoch 6 step 1662: training loss: 1382.8935729634572\n",
      "Epoch 6 step 1663: training accuarcy: 0.7075\n",
      "Epoch 6 step 1663: training loss: 1383.4001257190487\n",
      "Epoch 6 step 1664: training accuarcy: 0.7035\n",
      "Epoch 6 step 1664: training loss: 1383.573750430798\n",
      "Epoch 6 step 1665: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1665: training loss: 1383.2886481336964\n",
      "Epoch 6 step 1666: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1666: training loss: 1383.1548836876104\n",
      "Epoch 6 step 1667: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1667: training loss: 1381.9606028417168\n",
      "Epoch 6 step 1668: training accuarcy: 0.7045\n",
      "Epoch 6 step 1668: training loss: 1383.0989607509284\n",
      "Epoch 6 step 1669: training accuarcy: 0.6995\n",
      "Epoch 6 step 1669: training loss: 1382.4265137468155\n",
      "Epoch 6 step 1670: training accuarcy: 0.7025\n",
      "Epoch 6 step 1670: training loss: 1382.797340926649\n",
      "Epoch 6 step 1671: training accuarcy: 0.7025\n",
      "Epoch 6 step 1671: training loss: 1382.9063088904963\n",
      "Epoch 6 step 1672: training accuarcy: 0.7045\n",
      "Epoch 6 step 1672: training loss: 1382.534678753206\n",
      "Epoch 6 step 1673: training accuarcy: 0.7045\n",
      "Epoch 6 step 1673: training loss: 1383.1814082593503\n",
      "Epoch 6 step 1674: training accuarcy: 0.7075\n",
      "Epoch 6 step 1674: training loss: 1383.5722310814422\n",
      "Epoch 6 step 1675: training accuarcy: 0.6915\n",
      "Epoch 6 step 1675: training loss: 1383.3721403437046\n",
      "Epoch 6 step 1676: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1676: training loss: 1382.5232186198866\n",
      "Epoch 6 step 1677: training accuarcy: 0.705\n",
      "Epoch 6 step 1677: training loss: 1382.5562125582428\n",
      "Epoch 6 step 1678: training accuarcy: 0.707\n",
      "Epoch 6 step 1678: training loss: 1382.8631997653024\n",
      "Epoch 6 step 1679: training accuarcy: 0.7015\n",
      "Epoch 6 step 1679: training loss: 1382.8742263332613\n",
      "Epoch 6 step 1680: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1680: training loss: 1383.413527106638\n",
      "Epoch 6 step 1681: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1681: training loss: 1383.453876416113\n",
      "Epoch 6 step 1682: training accuarcy: 0.6865\n",
      "Epoch 6 step 1682: training loss: 1382.4975031667311\n",
      "Epoch 6 step 1683: training accuarcy: 0.706\n",
      "Epoch 6 step 1683: training loss: 1382.717652903344\n",
      "Epoch 6 step 1684: training accuarcy: 0.7135\n",
      "Epoch 6 step 1684: training loss: 1383.487031033267\n",
      "Epoch 6 step 1685: training accuarcy: 0.6890000000000001\n",
      "Epoch 6 step 1685: training loss: 1383.3624193203948\n",
      "Epoch 6 step 1686: training accuarcy: 0.684\n",
      "Epoch 6 step 1686: training loss: 1382.710407958306\n",
      "Epoch 6 step 1687: training accuarcy: 0.704\n",
      "Epoch 6 step 1687: training loss: 1383.9572697041933\n",
      "Epoch 6 step 1688: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1688: training loss: 1381.6633207946516\n",
      "Epoch 6 step 1689: training accuarcy: 0.71\n",
      "Epoch 6 step 1689: training loss: 1382.619039680641\n",
      "Epoch 6 step 1690: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1690: training loss: 1382.6465293343122\n",
      "Epoch 6 step 1691: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1691: training loss: 1382.4667138541988\n",
      "Epoch 6 step 1692: training accuarcy: 0.7065\n",
      "Epoch 6 step 1692: training loss: 1382.8178259174267\n",
      "Epoch 6 step 1693: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1693: training loss: 1382.541676782104\n",
      "Epoch 6 step 1694: training accuarcy: 0.714\n",
      "Epoch 6 step 1694: training loss: 1383.5483028907663\n",
      "Epoch 6 step 1695: training accuarcy: 0.6925\n",
      "Epoch 6 step 1695: training loss: 1382.9456047396104\n",
      "Epoch 6 step 1696: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1696: training loss: 1382.709889820627\n",
      "Epoch 6 step 1697: training accuarcy: 0.7155\n",
      "Epoch 6 step 1697: training loss: 1382.2885841296609\n",
      "Epoch 6 step 1698: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1698: training loss: 1383.5846912655975\n",
      "Epoch 6 step 1699: training accuarcy: 0.708\n",
      "Epoch 6 step 1699: training loss: 1382.6043613848537\n",
      "Epoch 6 step 1700: training accuarcy: 0.7085\n",
      "Epoch 6 step 1700: training loss: 1381.9617386987165\n",
      "Epoch 6 step 1701: training accuarcy: 0.705\n",
      "Epoch 6 step 1701: training loss: 1382.2391445050666\n",
      "Epoch 6 step 1702: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1702: training loss: 1382.7898408086833\n",
      "Epoch 6 step 1703: training accuarcy: 0.706\n",
      "Epoch 6 step 1703: training loss: 1383.0171864517733\n",
      "Epoch 6 step 1704: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1704: training loss: 1383.5056892474895\n",
      "Epoch 6 step 1705: training accuarcy: 0.7095\n",
      "Epoch 6 step 1705: training loss: 1383.5226120873851\n",
      "Epoch 6 step 1706: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1706: training loss: 1383.2614203809912\n",
      "Epoch 6 step 1707: training accuarcy: 0.7015\n",
      "Epoch 6 step 1707: training loss: 1383.0378681912596\n",
      "Epoch 6 step 1708: training accuarcy: 0.6985\n",
      "Epoch 6 step 1708: training loss: 1382.95414933248\n",
      "Epoch 6 step 1709: training accuarcy: 0.7025\n",
      "Epoch 6 step 1709: training loss: 1383.6187523086917\n",
      "Epoch 6 step 1710: training accuarcy: 0.6845\n",
      "Epoch 6 step 1710: training loss: 1383.5906566268543\n",
      "Epoch 6 step 1711: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1711: training loss: 1382.939851275537\n",
      "Epoch 6 step 1712: training accuarcy: 0.7005\n",
      "Epoch 6 step 1712: training loss: 1381.8337870581147\n",
      "Epoch 6 step 1713: training accuarcy: 0.708\n",
      "Epoch 6 step 1713: training loss: 1383.1155823138067\n",
      "Epoch 6 step 1714: training accuarcy: 0.6895\n",
      "Epoch 6 step 1714: training loss: 1381.6948399788967\n",
      "Epoch 6 step 1715: training accuarcy: 0.713\n",
      "Epoch 6 step 1715: training loss: 1383.1216591607174\n",
      "Epoch 6 step 1716: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1716: training loss: 1382.0895650157274\n",
      "Epoch 6 step 1717: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1717: training loss: 1382.805164204408\n",
      "Epoch 6 step 1718: training accuarcy: 0.711\n",
      "Epoch 6 step 1718: training loss: 1382.0969232852483\n",
      "Epoch 6 step 1719: training accuarcy: 0.711\n",
      "Epoch 6 step 1719: training loss: 1382.9066911229577\n",
      "Epoch 6 step 1720: training accuarcy: 0.6855\n",
      "Epoch 6 step 1720: training loss: 1381.390194589318\n",
      "Epoch 6 step 1721: training accuarcy: 0.707\n",
      "Epoch 6 step 1721: training loss: 1383.3631665788034\n",
      "Epoch 6 step 1722: training accuarcy: 0.6805\n",
      "Epoch 6 step 1722: training loss: 1381.8292994625208\n",
      "Epoch 6 step 1723: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1723: training loss: 1382.8312706356057\n",
      "Epoch 6 step 1724: training accuarcy: 0.708\n",
      "Epoch 6 step 1724: training loss: 1381.9144336205245\n",
      "Epoch 6 step 1725: training accuarcy: 0.708\n",
      "Epoch 6 step 1725: training loss: 1382.5919551755944\n",
      "Epoch 6 step 1726: training accuarcy: 0.6885\n",
      "Epoch 6 step 1726: training loss: 1383.634995996167\n",
      "Epoch 6 step 1727: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1727: training loss: 1382.2005613100512\n",
      "Epoch 6 step 1728: training accuarcy: 0.709\n",
      "Epoch 6 step 1728: training loss: 1381.66859189788\n",
      "Epoch 6 step 1729: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1729: training loss: 1383.7381491991239\n",
      "Epoch 6 step 1730: training accuarcy: 0.6945\n",
      "Epoch 6 step 1730: training loss: 1382.359733938593\n",
      "Epoch 6 step 1731: training accuarcy: 0.722\n",
      "Epoch 6 step 1731: training loss: 1383.5919702652664\n",
      "Epoch 6 step 1732: training accuarcy: 0.684\n",
      "Epoch 6 step 1732: training loss: 1383.488562227385\n",
      "Epoch 6 step 1733: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1733: training loss: 1382.864103666358\n",
      "Epoch 6 step 1734: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1734: training loss: 1383.2511250029715\n",
      "Epoch 6 step 1735: training accuarcy: 0.6975\n",
      "Epoch 6 step 1735: training loss: 1382.5647346326239\n",
      "Epoch 6 step 1736: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1736: training loss: 1383.1639511379005\n",
      "Epoch 6 step 1737: training accuarcy: 0.706\n",
      "Epoch 6 step 1737: training loss: 1381.845857446796\n",
      "Epoch 6 step 1738: training accuarcy: 0.7085\n",
      "Epoch 6 step 1738: training loss: 1383.8571630615072\n",
      "Epoch 6 step 1739: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1739: training loss: 1382.7050727324508\n",
      "Epoch 6 step 1740: training accuarcy: 0.6955\n",
      "Epoch 6 step 1740: training loss: 1382.9146916228913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1741: training accuarcy: 0.6955\n",
      "Epoch 6 step 1741: training loss: 1382.666794818975\n",
      "Epoch 6 step 1742: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1742: training loss: 1382.29850806044\n",
      "Epoch 6 step 1743: training accuarcy: 0.7045\n",
      "Epoch 6 step 1743: training loss: 1383.816248352409\n",
      "Epoch 6 step 1744: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1744: training loss: 1383.292257358835\n",
      "Epoch 6 step 1745: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1745: training loss: 1382.367831435093\n",
      "Epoch 6 step 1746: training accuarcy: 0.711\n",
      "Epoch 6 step 1746: training loss: 1383.8754677500765\n",
      "Epoch 6 step 1747: training accuarcy: 0.6845\n",
      "Epoch 6 step 1747: training loss: 1382.769616835386\n",
      "Epoch 6 step 1748: training accuarcy: 0.7030000000000001\n",
      "Epoch 6 step 1748: training loss: 1382.6848318598868\n",
      "Epoch 6 step 1749: training accuarcy: 0.6845\n",
      "Epoch 6 step 1749: training loss: 1383.3193289278927\n",
      "Epoch 6 step 1750: training accuarcy: 0.6890000000000001\n",
      "Epoch 6 step 1750: training loss: 1382.8285402801152\n",
      "Epoch 6 step 1751: training accuarcy: 0.7005\n",
      "Epoch 6 step 1751: training loss: 1383.6699847304399\n",
      "Epoch 6 step 1752: training accuarcy: 0.685\n",
      "Epoch 6 step 1752: training loss: 1383.3840737245641\n",
      "Epoch 6 step 1753: training accuarcy: 0.6955\n",
      "Epoch 6 step 1753: training loss: 1382.177050260439\n",
      "Epoch 6 step 1754: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1754: training loss: 1382.130112507747\n",
      "Epoch 6 step 1755: training accuarcy: 0.7025\n",
      "Epoch 6 step 1755: training loss: 1383.8764599508384\n",
      "Epoch 6 step 1756: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1756: training loss: 1382.470811107652\n",
      "Epoch 6 step 1757: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1757: training loss: 1382.5538925071357\n",
      "Epoch 6 step 1758: training accuarcy: 0.707\n",
      "Epoch 6 step 1758: training loss: 1382.4278056891008\n",
      "Epoch 6 step 1759: training accuarcy: 0.7035\n",
      "Epoch 6 step 1759: training loss: 1383.0420372711778\n",
      "Epoch 6 step 1760: training accuarcy: 0.686\n",
      "Epoch 6 step 1760: training loss: 1383.1775317906963\n",
      "Epoch 6 step 1761: training accuarcy: 0.687\n",
      "Epoch 6 step 1761: training loss: 1383.576735381077\n",
      "Epoch 6 step 1762: training accuarcy: 0.676\n",
      "Epoch 6 step 1762: training loss: 1383.4171166746041\n",
      "Epoch 6 step 1763: training accuarcy: 0.687\n",
      "Epoch 6 step 1763: training loss: 1382.7280811650119\n",
      "Epoch 6 step 1764: training accuarcy: 0.711\n",
      "Epoch 6 step 1764: training loss: 1383.3871876264002\n",
      "Epoch 6 step 1765: training accuarcy: 0.6905\n",
      "Epoch 6 step 1765: training loss: 1383.1165103570631\n",
      "Epoch 6 step 1766: training accuarcy: 0.6925\n",
      "Epoch 6 step 1766: training loss: 1383.136502256417\n",
      "Epoch 6 step 1767: training accuarcy: 0.6995\n",
      "Epoch 6 step 1767: training loss: 1383.5336711094942\n",
      "Epoch 6 step 1768: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1768: training loss: 1382.398765736321\n",
      "Epoch 6 step 1769: training accuarcy: 0.7065\n",
      "Epoch 6 step 1769: training loss: 1383.4535150928314\n",
      "Epoch 6 step 1770: training accuarcy: 0.6970000000000001\n",
      "Epoch 6 step 1770: training loss: 1383.766207150462\n",
      "Epoch 6 step 1771: training accuarcy: 0.6925\n",
      "Epoch 6 step 1771: training loss: 1382.4899964084616\n",
      "Epoch 6 step 1772: training accuarcy: 0.705\n",
      "Epoch 6 step 1772: training loss: 1382.2003744110511\n",
      "Epoch 6 step 1773: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1773: training loss: 1382.973767523179\n",
      "Epoch 6 step 1774: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1774: training loss: 1383.872338359432\n",
      "Epoch 6 step 1775: training accuarcy: 0.6785\n",
      "Epoch 6 step 1775: training loss: 1382.947365569849\n",
      "Epoch 6 step 1776: training accuarcy: 0.6965\n",
      "Epoch 6 step 1776: training loss: 1382.7607130690371\n",
      "Epoch 6 step 1777: training accuarcy: 0.712\n",
      "Epoch 6 step 1777: training loss: 1382.7787482759838\n",
      "Epoch 6 step 1778: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1778: training loss: 1382.8790991885758\n",
      "Epoch 6 step 1779: training accuarcy: 0.6945\n",
      "Epoch 6 step 1779: training loss: 1383.2227513380722\n",
      "Epoch 6 step 1780: training accuarcy: 0.7005\n",
      "Epoch 6 step 1780: training loss: 1382.3641958347814\n",
      "Epoch 6 step 1781: training accuarcy: 0.6805\n",
      "Epoch 6 step 1781: training loss: 1383.0780949219434\n",
      "Epoch 6 step 1782: training accuarcy: 0.6775\n",
      "Epoch 6 step 1782: training loss: 1382.8186963676703\n",
      "Epoch 6 step 1783: training accuarcy: 0.6905\n",
      "Epoch 6 step 1783: training loss: 1383.1011116281654\n",
      "Epoch 6 step 1784: training accuarcy: 0.6975\n",
      "Epoch 6 step 1784: training loss: 1383.8317017435809\n",
      "Epoch 6 step 1785: training accuarcy: 0.6880000000000001\n",
      "Epoch 6 step 1785: training loss: 1382.7276213666826\n",
      "Epoch 6 step 1786: training accuarcy: 0.712\n",
      "Epoch 6 step 1786: training loss: 1382.1356747428242\n",
      "Epoch 6 step 1787: training accuarcy: 0.7035\n",
      "Epoch 6 step 1787: training loss: 1383.1538892869453\n",
      "Epoch 6 step 1788: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1788: training loss: 1381.9626563551117\n",
      "Epoch 6 step 1789: training accuarcy: 0.7165\n",
      "Epoch 6 step 1789: training loss: 1383.5275602176116\n",
      "Epoch 6 step 1790: training accuarcy: 0.6910000000000001\n",
      "Epoch 6 step 1790: training loss: 1383.5504789285737\n",
      "Epoch 6 step 1791: training accuarcy: 0.6895\n",
      "Epoch 6 step 1791: training loss: 1382.7263334454017\n",
      "Epoch 6 step 1792: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1792: training loss: 1382.9165927623094\n",
      "Epoch 6 step 1793: training accuarcy: 0.6970000000000001\n",
      "Epoch 6 step 1793: training loss: 1382.8644334694923\n",
      "Epoch 6 step 1794: training accuarcy: 0.6935\n",
      "Epoch 6 step 1794: training loss: 1382.780748373219\n",
      "Epoch 6 step 1795: training accuarcy: 0.706\n",
      "Epoch 6 step 1795: training loss: 1382.8432722943203\n",
      "Epoch 6 step 1796: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1796: training loss: 1384.0041744633077\n",
      "Epoch 6 step 1797: training accuarcy: 0.6975\n",
      "Epoch 6 step 1797: training loss: 1382.5972917024342\n",
      "Epoch 6 step 1798: training accuarcy: 0.708\n",
      "Epoch 6 step 1798: training loss: 1383.1760603182884\n",
      "Epoch 6 step 1799: training accuarcy: 0.6910000000000001\n",
      "Epoch 6 step 1799: training loss: 1383.5621943157296\n",
      "Epoch 6 step 1800: training accuarcy: 0.678\n",
      "Epoch 6 step 1800: training loss: 1383.0738484390633\n",
      "Epoch 6 step 1801: training accuarcy: 0.6895\n",
      "Epoch 6 step 1801: training loss: 1383.9349121103453\n",
      "Epoch 6 step 1802: training accuarcy: 0.6745\n",
      "Epoch 6 step 1802: training loss: 1383.6223816802217\n",
      "Epoch 6 step 1803: training accuarcy: 0.6685\n",
      "Epoch 6 step 1803: training loss: 1382.4116776380147\n",
      "Epoch 6 step 1804: training accuarcy: 0.71\n",
      "Epoch 6 step 1804: training loss: 1382.9078309893148\n",
      "Epoch 6 step 1805: training accuarcy: 0.6875\n",
      "Epoch 6 step 1805: training loss: 1382.5159890417938\n",
      "Epoch 6 step 1806: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1806: training loss: 1383.4802845187223\n",
      "Epoch 6 step 1807: training accuarcy: 0.6925\n",
      "Epoch 6 step 1807: training loss: 1383.2959309836208\n",
      "Epoch 6 step 1808: training accuarcy: 0.6845\n",
      "Epoch 6 step 1808: training loss: 1383.5394679502438\n",
      "Epoch 6 step 1809: training accuarcy: 0.6875\n",
      "Epoch 6 step 1809: training loss: 1382.6545965766775\n",
      "Epoch 6 step 1810: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1810: training loss: 1383.1816439618717\n",
      "Epoch 6 step 1811: training accuarcy: 0.6925\n",
      "Epoch 6 step 1811: training loss: 1382.6983093292868\n",
      "Epoch 6 step 1812: training accuarcy: 0.6955\n",
      "Epoch 6 step 1812: training loss: 1382.9635225054776\n",
      "Epoch 6 step 1813: training accuarcy: 0.6895\n",
      "Epoch 6 step 1813: training loss: 1383.1469929699347\n",
      "Epoch 6 step 1814: training accuarcy: 0.6985\n",
      "Epoch 6 step 1814: training loss: 1382.7958451754064\n",
      "Epoch 6 step 1815: training accuarcy: 0.6925\n",
      "Epoch 6 step 1815: training loss: 1382.930372633199\n",
      "Epoch 6 step 1816: training accuarcy: 0.6835\n",
      "Epoch 6 step 1816: training loss: 1383.3474607125286\n",
      "Epoch 6 step 1817: training accuarcy: 0.6815\n",
      "Epoch 6 step 1817: training loss: 1382.7318563224892\n",
      "Epoch 6 step 1818: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1818: training loss: 1382.7912274289718\n",
      "Epoch 6 step 1819: training accuarcy: 0.6895\n",
      "Epoch 6 step 1819: training loss: 1383.7379910990055\n",
      "Epoch 6 step 1820: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1820: training loss: 1383.0445131883505\n",
      "Epoch 6 step 1821: training accuarcy: 0.6940000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1821: training loss: 1382.4626598323562\n",
      "Epoch 6 step 1822: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1822: training loss: 1383.444971459152\n",
      "Epoch 6 step 1823: training accuarcy: 0.6945\n",
      "Epoch 6 step 1823: training loss: 1383.5062680591793\n",
      "Epoch 6 step 1824: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1824: training loss: 1382.4972642726038\n",
      "Epoch 6 step 1825: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1825: training loss: 1382.6726609411392\n",
      "Epoch 6 step 1826: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1826: training loss: 1381.5563306478848\n",
      "Epoch 6 step 1827: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1827: training loss: 1381.7114711750353\n",
      "Epoch 6 step 1828: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1828: training loss: 1382.963432468188\n",
      "Epoch 6 step 1829: training accuarcy: 0.7085\n",
      "Epoch 6 step 1829: training loss: 1382.7810744554927\n",
      "Epoch 6 step 1830: training accuarcy: 0.7030000000000001\n",
      "Epoch 6 step 1830: training loss: 1383.7072831957094\n",
      "Epoch 6 step 1831: training accuarcy: 0.6935\n",
      "Epoch 6 step 1831: training loss: 1383.3338795125494\n",
      "Epoch 6 step 1832: training accuarcy: 0.681\n",
      "Epoch 6 step 1832: training loss: 1383.944339014659\n",
      "Epoch 6 step 1833: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1833: training loss: 1382.683440417752\n",
      "Epoch 6 step 1834: training accuarcy: 0.7005\n",
      "Epoch 6 step 1834: training loss: 1383.506656850785\n",
      "Epoch 6 step 1835: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1835: training loss: 1381.6502849363374\n",
      "Epoch 6 step 1836: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1836: training loss: 1382.2358967724965\n",
      "Epoch 6 step 1837: training accuarcy: 0.6925\n",
      "Epoch 6 step 1837: training loss: 1383.3485204497558\n",
      "Epoch 6 step 1838: training accuarcy: 0.685\n",
      "Epoch 6 step 1838: training loss: 1383.5846211462995\n",
      "Epoch 6 step 1839: training accuarcy: 0.687\n",
      "Epoch 6 step 1839: training loss: 1381.7283761228439\n",
      "Epoch 6 step 1840: training accuarcy: 0.712\n",
      "Epoch 6 step 1840: training loss: 543.4673908861304\n",
      "Epoch 6 step 1841: training accuarcy: 0.6858974358974359\n",
      "Epoch 6: train loss 1379.585207503064, train accuarcy 0.7014636397361755\n",
      "Epoch 6: valid loss 1364.0169861741695, valid accuarcy 0.7081059217453003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                   | 7/8 [14:03<02:03, 123.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 1841: training loss: 1382.0603360998941\n",
      "Epoch 7 step 1842: training accuarcy: 0.7085\n",
      "Epoch 7 step 1842: training loss: 1381.9663085083093\n",
      "Epoch 7 step 1843: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1843: training loss: 1382.155464600916\n",
      "Epoch 7 step 1844: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1844: training loss: 1381.8843937109814\n",
      "Epoch 7 step 1845: training accuarcy: 0.713\n",
      "Epoch 7 step 1845: training loss: 1383.6670187086052\n",
      "Epoch 7 step 1846: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1846: training loss: 1381.3106253079434\n",
      "Epoch 7 step 1847: training accuarcy: 0.715\n",
      "Epoch 7 step 1847: training loss: 1381.496890954251\n",
      "Epoch 7 step 1848: training accuarcy: 0.7135\n",
      "Epoch 7 step 1848: training loss: 1382.0257215977017\n",
      "Epoch 7 step 1849: training accuarcy: 0.7145\n",
      "Epoch 7 step 1849: training loss: 1382.4808050050588\n",
      "Epoch 7 step 1850: training accuarcy: 0.7125\n",
      "Epoch 7 step 1850: training loss: 1381.6245515092833\n",
      "Epoch 7 step 1851: training accuarcy: 0.72\n",
      "Epoch 7 step 1851: training loss: 1381.9438639168332\n",
      "Epoch 7 step 1852: training accuarcy: 0.7175\n",
      "Epoch 7 step 1852: training loss: 1382.4762624070863\n",
      "Epoch 7 step 1853: training accuarcy: 0.717\n",
      "Epoch 7 step 1853: training loss: 1382.7301643995213\n",
      "Epoch 7 step 1854: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 1854: training loss: 1382.0812947280397\n",
      "Epoch 7 step 1855: training accuarcy: 0.7055\n",
      "Epoch 7 step 1855: training loss: 1382.395209155815\n",
      "Epoch 7 step 1856: training accuarcy: 0.709\n",
      "Epoch 7 step 1856: training loss: 1381.303611004135\n",
      "Epoch 7 step 1857: training accuarcy: 0.718\n",
      "Epoch 7 step 1857: training loss: 1381.50301018183\n",
      "Epoch 7 step 1858: training accuarcy: 0.7235\n",
      "Epoch 7 step 1858: training loss: 1382.512272689947\n",
      "Epoch 7 step 1859: training accuarcy: 0.7055\n",
      "Epoch 7 step 1859: training loss: 1382.467369597758\n",
      "Epoch 7 step 1860: training accuarcy: 0.712\n",
      "Epoch 7 step 1860: training loss: 1381.5499270974503\n",
      "Epoch 7 step 1861: training accuarcy: 0.724\n",
      "Epoch 7 step 1861: training loss: 1382.1145669845066\n",
      "Epoch 7 step 1862: training accuarcy: 0.6995\n",
      "Epoch 7 step 1862: training loss: 1382.8821679840892\n",
      "Epoch 7 step 1863: training accuarcy: 0.7105\n",
      "Epoch 7 step 1863: training loss: 1381.41040928361\n",
      "Epoch 7 step 1864: training accuarcy: 0.718\n",
      "Epoch 7 step 1864: training loss: 1381.4357918044684\n",
      "Epoch 7 step 1865: training accuarcy: 0.7105\n",
      "Epoch 7 step 1865: training loss: 1382.4400369523273\n",
      "Epoch 7 step 1866: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 1866: training loss: 1382.2633364925748\n",
      "Epoch 7 step 1867: training accuarcy: 0.71\n",
      "Epoch 7 step 1867: training loss: 1381.7852884225215\n",
      "Epoch 7 step 1868: training accuarcy: 0.7085\n",
      "Epoch 7 step 1868: training loss: 1381.9455047066654\n",
      "Epoch 7 step 1869: training accuarcy: 0.7095\n",
      "Epoch 7 step 1869: training loss: 1382.9169412949614\n",
      "Epoch 7 step 1870: training accuarcy: 0.7045\n",
      "Epoch 7 step 1870: training loss: 1382.6654753559276\n",
      "Epoch 7 step 1871: training accuarcy: 0.6995\n",
      "Epoch 7 step 1871: training loss: 1383.522933049939\n",
      "Epoch 7 step 1872: training accuarcy: 0.6995\n",
      "Epoch 7 step 1872: training loss: 1382.4615214223359\n",
      "Epoch 7 step 1873: training accuarcy: 0.713\n",
      "Epoch 7 step 1873: training loss: 1382.609943580792\n",
      "Epoch 7 step 1874: training accuarcy: 0.722\n",
      "Epoch 7 step 1874: training loss: 1383.234736855301\n",
      "Epoch 7 step 1875: training accuarcy: 0.7025\n",
      "Epoch 7 step 1875: training loss: 1383.2545180920529\n",
      "Epoch 7 step 1876: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 1876: training loss: 1381.8730353684393\n",
      "Epoch 7 step 1877: training accuarcy: 0.718\n",
      "Epoch 7 step 1877: training loss: 1382.9877341426657\n",
      "Epoch 7 step 1878: training accuarcy: 0.6915\n",
      "Epoch 7 step 1878: training loss: 1382.3982781363861\n",
      "Epoch 7 step 1879: training accuarcy: 0.7095\n",
      "Epoch 7 step 1879: training loss: 1382.5154961590217\n",
      "Epoch 7 step 1880: training accuarcy: 0.71\n",
      "Epoch 7 step 1880: training loss: 1382.463252377173\n",
      "Epoch 7 step 1881: training accuarcy: 0.7035\n",
      "Epoch 7 step 1881: training loss: 1382.4216335284254\n",
      "Epoch 7 step 1882: training accuarcy: 0.7135\n",
      "Epoch 7 step 1882: training loss: 1382.3137435523977\n",
      "Epoch 7 step 1883: training accuarcy: 0.7175\n",
      "Epoch 7 step 1883: training loss: 1382.6465847333689\n",
      "Epoch 7 step 1884: training accuarcy: 0.7125\n",
      "Epoch 7 step 1884: training loss: 1383.4704985793721\n",
      "Epoch 7 step 1885: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 1885: training loss: 1381.771683032104\n",
      "Epoch 7 step 1886: training accuarcy: 0.715\n",
      "Epoch 7 step 1886: training loss: 1383.1445992409276\n",
      "Epoch 7 step 1887: training accuarcy: 0.7005\n",
      "Epoch 7 step 1887: training loss: 1382.4350465620907\n",
      "Epoch 7 step 1888: training accuarcy: 0.7085\n",
      "Epoch 7 step 1888: training loss: 1382.670441240985\n",
      "Epoch 7 step 1889: training accuarcy: 0.704\n",
      "Epoch 7 step 1889: training loss: 1381.9690057041505\n",
      "Epoch 7 step 1890: training accuarcy: 0.7095\n",
      "Epoch 7 step 1890: training loss: 1381.8897881168418\n",
      "Epoch 7 step 1891: training accuarcy: 0.707\n",
      "Epoch 7 step 1891: training loss: 1382.0255423552035\n",
      "Epoch 7 step 1892: training accuarcy: 0.6995\n",
      "Epoch 7 step 1892: training loss: 1382.193955132465\n",
      "Epoch 7 step 1893: training accuarcy: 0.7065\n",
      "Epoch 7 step 1893: training loss: 1383.265974285161\n",
      "Epoch 7 step 1894: training accuarcy: 0.711\n",
      "Epoch 7 step 1894: training loss: 1381.7225246118442\n",
      "Epoch 7 step 1895: training accuarcy: 0.716\n",
      "Epoch 7 step 1895: training loss: 1383.916911724728\n",
      "Epoch 7 step 1896: training accuarcy: 0.683\n",
      "Epoch 7 step 1896: training loss: 1382.8207235514353\n",
      "Epoch 7 step 1897: training accuarcy: 0.7025\n",
      "Epoch 7 step 1897: training loss: 1383.0943508971443\n",
      "Epoch 7 step 1898: training accuarcy: 0.7125\n",
      "Epoch 7 step 1898: training loss: 1382.9600572166983\n",
      "Epoch 7 step 1899: training accuarcy: 0.71\n",
      "Epoch 7 step 1899: training loss: 1382.8062250287587\n",
      "Epoch 7 step 1900: training accuarcy: 0.7015\n",
      "Epoch 7 step 1900: training loss: 1383.232296927972\n",
      "Epoch 7 step 1901: training accuarcy: 0.6895\n",
      "Epoch 7 step 1901: training loss: 1383.1619776536031\n",
      "Epoch 7 step 1902: training accuarcy: 0.687\n",
      "Epoch 7 step 1902: training loss: 1382.926480348751\n",
      "Epoch 7 step 1903: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 1903: training loss: 1383.3602114382265\n",
      "Epoch 7 step 1904: training accuarcy: 0.7085\n",
      "Epoch 7 step 1904: training loss: 1381.6211606506222\n",
      "Epoch 7 step 1905: training accuarcy: 0.6950000000000001\n",
      "Epoch 7 step 1905: training loss: 1383.3446404023298\n",
      "Epoch 7 step 1906: training accuarcy: 0.7005\n",
      "Epoch 7 step 1906: training loss: 1382.7273940021119\n",
      "Epoch 7 step 1907: training accuarcy: 0.7065\n",
      "Epoch 7 step 1907: training loss: 1382.646120403467\n",
      "Epoch 7 step 1908: training accuarcy: 0.6960000000000001\n",
      "Epoch 7 step 1908: training loss: 1382.8194861123766\n",
      "Epoch 7 step 1909: training accuarcy: 0.6995\n",
      "Epoch 7 step 1909: training loss: 1381.786212984366\n",
      "Epoch 7 step 1910: training accuarcy: 0.7175\n",
      "Epoch 7 step 1910: training loss: 1382.5412420918915\n",
      "Epoch 7 step 1911: training accuarcy: 0.7045\n",
      "Epoch 7 step 1911: training loss: 1382.653296698967\n",
      "Epoch 7 step 1912: training accuarcy: 0.7095\n",
      "Epoch 7 step 1912: training loss: 1380.9802450187117\n",
      "Epoch 7 step 1913: training accuarcy: 0.71\n",
      "Epoch 7 step 1913: training loss: 1383.3964697277922\n",
      "Epoch 7 step 1914: training accuarcy: 0.6950000000000001\n",
      "Epoch 7 step 1914: training loss: 1383.6943471669742\n",
      "Epoch 7 step 1915: training accuarcy: 0.6965\n",
      "Epoch 7 step 1915: training loss: 1383.3175242691643\n",
      "Epoch 7 step 1916: training accuarcy: 0.708\n",
      "Epoch 7 step 1916: training loss: 1382.6933976806874\n",
      "Epoch 7 step 1917: training accuarcy: 0.704\n",
      "Epoch 7 step 1917: training loss: 1382.049911762732\n",
      "Epoch 7 step 1918: training accuarcy: 0.706\n",
      "Epoch 7 step 1918: training loss: 1382.122684561887\n",
      "Epoch 7 step 1919: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1919: training loss: 1383.41519911101\n",
      "Epoch 7 step 1920: training accuarcy: 0.6945\n",
      "Epoch 7 step 1920: training loss: 1383.566347015459\n",
      "Epoch 7 step 1921: training accuarcy: 0.6985\n",
      "Epoch 7 step 1921: training loss: 1382.6503568430396\n",
      "Epoch 7 step 1922: training accuarcy: 0.7115\n",
      "Epoch 7 step 1922: training loss: 1383.6203613187745\n",
      "Epoch 7 step 1923: training accuarcy: 0.6970000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 1923: training loss: 1383.0912891277162\n",
      "Epoch 7 step 1924: training accuarcy: 0.706\n",
      "Epoch 7 step 1924: training loss: 1382.6613293629712\n",
      "Epoch 7 step 1925: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 1925: training loss: 1383.015189897159\n",
      "Epoch 7 step 1926: training accuarcy: 0.6915\n",
      "Epoch 7 step 1926: training loss: 1383.0345461815352\n",
      "Epoch 7 step 1927: training accuarcy: 0.682\n",
      "Epoch 7 step 1927: training loss: 1382.1299860127394\n",
      "Epoch 7 step 1928: training accuarcy: 0.704\n",
      "Epoch 7 step 1928: training loss: 1382.3489006138675\n",
      "Epoch 7 step 1929: training accuarcy: 0.7065\n",
      "Epoch 7 step 1929: training loss: 1381.8290284077368\n",
      "Epoch 7 step 1930: training accuarcy: 0.713\n",
      "Epoch 7 step 1930: training loss: 1382.6977309232643\n",
      "Epoch 7 step 1931: training accuarcy: 0.7015\n",
      "Epoch 7 step 1931: training loss: 1383.862271691608\n",
      "Epoch 7 step 1932: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1932: training loss: 1383.1418919672076\n",
      "Epoch 7 step 1933: training accuarcy: 0.6960000000000001\n",
      "Epoch 7 step 1933: training loss: 1382.4142663364478\n",
      "Epoch 7 step 1934: training accuarcy: 0.7025\n",
      "Epoch 7 step 1934: training loss: 1382.38403526993\n",
      "Epoch 7 step 1935: training accuarcy: 0.7105\n",
      "Epoch 7 step 1935: training loss: 1383.6570604728372\n",
      "Epoch 7 step 1936: training accuarcy: 0.687\n",
      "Epoch 7 step 1936: training loss: 1382.109301513553\n",
      "Epoch 7 step 1937: training accuarcy: 0.713\n",
      "Epoch 7 step 1937: training loss: 1382.9437740201593\n",
      "Epoch 7 step 1938: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1938: training loss: 1382.4747998880964\n",
      "Epoch 7 step 1939: training accuarcy: 0.7145\n",
      "Epoch 7 step 1939: training loss: 1382.7663886336431\n",
      "Epoch 7 step 1940: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 1940: training loss: 1383.0607511141714\n",
      "Epoch 7 step 1941: training accuarcy: 0.6805\n",
      "Epoch 7 step 1941: training loss: 1382.9087208400847\n",
      "Epoch 7 step 1942: training accuarcy: 0.6910000000000001\n",
      "Epoch 7 step 1942: training loss: 1382.7603611847487\n",
      "Epoch 7 step 1943: training accuarcy: 0.6815\n",
      "Epoch 7 step 1943: training loss: 1382.5468228030602\n",
      "Epoch 7 step 1944: training accuarcy: 0.6825\n",
      "Epoch 7 step 1944: training loss: 1382.309472033533\n",
      "Epoch 7 step 1945: training accuarcy: 0.708\n",
      "Epoch 7 step 1945: training loss: 1383.056189212135\n",
      "Epoch 7 step 1946: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 1946: training loss: 1383.3934256242542\n",
      "Epoch 7 step 1947: training accuarcy: 0.687\n",
      "Epoch 7 step 1947: training loss: 1382.3195306266744\n",
      "Epoch 7 step 1948: training accuarcy: 0.71\n",
      "Epoch 7 step 1948: training loss: 1382.9997608676808\n",
      "Epoch 7 step 1949: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1949: training loss: 1382.5805194957136\n",
      "Epoch 7 step 1950: training accuarcy: 0.71\n",
      "Epoch 7 step 1950: training loss: 1382.9742599089416\n",
      "Epoch 7 step 1951: training accuarcy: 0.6845\n",
      "Epoch 7 step 1951: training loss: 1383.1883284036714\n",
      "Epoch 7 step 1952: training accuarcy: 0.685\n",
      "Epoch 7 step 1952: training loss: 1382.375398776669\n",
      "Epoch 7 step 1953: training accuarcy: 0.712\n",
      "Epoch 7 step 1953: training loss: 1383.8692390482674\n",
      "Epoch 7 step 1954: training accuarcy: 0.682\n",
      "Epoch 7 step 1954: training loss: 1382.7509743373632\n",
      "Epoch 7 step 1955: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 1955: training loss: 1382.668599438273\n",
      "Epoch 7 step 1956: training accuarcy: 0.6865\n",
      "Epoch 7 step 1956: training loss: 1384.1517825044145\n",
      "Epoch 7 step 1957: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1957: training loss: 1383.1545781870177\n",
      "Epoch 7 step 1958: training accuarcy: 0.7055\n",
      "Epoch 7 step 1958: training loss: 1383.943635417747\n",
      "Epoch 7 step 1959: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 1959: training loss: 1383.6097305447345\n",
      "Epoch 7 step 1960: training accuarcy: 0.683\n",
      "Epoch 7 step 1960: training loss: 1382.9062469031326\n",
      "Epoch 7 step 1961: training accuarcy: 0.6915\n",
      "Epoch 7 step 1961: training loss: 1382.06782703944\n",
      "Epoch 7 step 1962: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 1962: training loss: 1382.2447278049788\n",
      "Epoch 7 step 1963: training accuarcy: 0.6895\n",
      "Epoch 7 step 1963: training loss: 1383.0677643702365\n",
      "Epoch 7 step 1964: training accuarcy: 0.6745\n",
      "Epoch 7 step 1964: training loss: 1383.360088850567\n",
      "Epoch 7 step 1965: training accuarcy: 0.707\n",
      "Epoch 7 step 1965: training loss: 1383.8877804429521\n",
      "Epoch 7 step 1966: training accuarcy: 0.6815\n",
      "Epoch 7 step 1966: training loss: 1383.4579115174834\n",
      "Epoch 7 step 1967: training accuarcy: 0.676\n",
      "Epoch 7 step 1967: training loss: 1382.675751058588\n",
      "Epoch 7 step 1968: training accuarcy: 0.708\n",
      "Epoch 7 step 1968: training loss: 1382.2699148427669\n",
      "Epoch 7 step 1969: training accuarcy: 0.711\n",
      "Epoch 7 step 1969: training loss: 1383.0710203140754\n",
      "Epoch 7 step 1970: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1970: training loss: 1382.3037581045623\n",
      "Epoch 7 step 1971: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 1971: training loss: 1382.2059671022469\n",
      "Epoch 7 step 1972: training accuarcy: 0.7135\n",
      "Epoch 7 step 1972: training loss: 1382.2604718850587\n",
      "Epoch 7 step 1973: training accuarcy: 0.721\n",
      "Epoch 7 step 1973: training loss: 1383.0036519461592\n",
      "Epoch 7 step 1974: training accuarcy: 0.6960000000000001\n",
      "Epoch 7 step 1974: training loss: 1382.1671415093783\n",
      "Epoch 7 step 1975: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 1975: training loss: 1382.9670422122367\n",
      "Epoch 7 step 1976: training accuarcy: 0.6945\n",
      "Epoch 7 step 1976: training loss: 1383.4971224898622\n",
      "Epoch 7 step 1977: training accuarcy: 0.6955\n",
      "Epoch 7 step 1977: training loss: 1383.3525837879215\n",
      "Epoch 7 step 1978: training accuarcy: 0.687\n",
      "Epoch 7 step 1978: training loss: 1381.8530450222763\n",
      "Epoch 7 step 1979: training accuarcy: 0.706\n",
      "Epoch 7 step 1979: training loss: 1383.7667891031435\n",
      "Epoch 7 step 1980: training accuarcy: 0.687\n",
      "Epoch 7 step 1980: training loss: 1382.3199007640633\n",
      "Epoch 7 step 1981: training accuarcy: 0.6975\n",
      "Epoch 7 step 1981: training loss: 1383.0844098888858\n",
      "Epoch 7 step 1982: training accuarcy: 0.6890000000000001\n",
      "Epoch 7 step 1982: training loss: 1382.5709306010276\n",
      "Epoch 7 step 1983: training accuarcy: 0.7085\n",
      "Epoch 7 step 1983: training loss: 1381.997343259998\n",
      "Epoch 7 step 1984: training accuarcy: 0.71\n",
      "Epoch 7 step 1984: training loss: 1383.634625736233\n",
      "Epoch 7 step 1985: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 1985: training loss: 1383.1872719981238\n",
      "Epoch 7 step 1986: training accuarcy: 0.6845\n",
      "Epoch 7 step 1986: training loss: 1381.8646021029138\n",
      "Epoch 7 step 1987: training accuarcy: 0.7165\n",
      "Epoch 7 step 1987: training loss: 1383.5459810045272\n",
      "Epoch 7 step 1988: training accuarcy: 0.7025\n",
      "Epoch 7 step 1988: training loss: 1383.0309385660457\n",
      "Epoch 7 step 1989: training accuarcy: 0.6995\n",
      "Epoch 7 step 1989: training loss: 1382.8277724383781\n",
      "Epoch 7 step 1990: training accuarcy: 0.7085\n",
      "Epoch 7 step 1990: training loss: 1382.6038722386859\n",
      "Epoch 7 step 1991: training accuarcy: 0.7065\n",
      "Epoch 7 step 1991: training loss: 1384.0395375407847\n",
      "Epoch 7 step 1992: training accuarcy: 0.6705\n",
      "Epoch 7 step 1992: training loss: 1383.1960398643203\n",
      "Epoch 7 step 1993: training accuarcy: 0.7025\n",
      "Epoch 7 step 1993: training loss: 1382.8137285993187\n",
      "Epoch 7 step 1994: training accuarcy: 0.7025\n",
      "Epoch 7 step 1994: training loss: 1383.4785105767578\n",
      "Epoch 7 step 1995: training accuarcy: 0.6805\n",
      "Epoch 7 step 1995: training loss: 1382.7106518278881\n",
      "Epoch 7 step 1996: training accuarcy: 0.6865\n",
      "Epoch 7 step 1996: training loss: 1383.9801436972891\n",
      "Epoch 7 step 1997: training accuarcy: 0.6765\n",
      "Epoch 7 step 1997: training loss: 1383.0360996882653\n",
      "Epoch 7 step 1998: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 1998: training loss: 1383.1800720793244\n",
      "Epoch 7 step 1999: training accuarcy: 0.684\n",
      "Epoch 7 step 1999: training loss: 1383.6784106163109\n",
      "Epoch 7 step 2000: training accuarcy: 0.6845\n",
      "Epoch 7 step 2000: training loss: 1383.154967569966\n",
      "Epoch 7 step 2001: training accuarcy: 0.705\n",
      "Epoch 7 step 2001: training loss: 1383.0302225666048\n",
      "Epoch 7 step 2002: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 2002: training loss: 1382.8736945950473\n",
      "Epoch 7 step 2003: training accuarcy: 0.6915\n",
      "Epoch 7 step 2003: training loss: 1383.1499878263\n",
      "Epoch 7 step 2004: training accuarcy: 0.6985\n",
      "Epoch 7 step 2004: training loss: 1383.7902294568473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2005: training accuarcy: 0.677\n",
      "Epoch 7 step 2005: training loss: 1382.4426488788642\n",
      "Epoch 7 step 2006: training accuarcy: 0.7115\n",
      "Epoch 7 step 2006: training loss: 1384.0143155934102\n",
      "Epoch 7 step 2007: training accuarcy: 0.6895\n",
      "Epoch 7 step 2007: training loss: 1381.786588647689\n",
      "Epoch 7 step 2008: training accuarcy: 0.7020000000000001\n",
      "Epoch 7 step 2008: training loss: 1382.724263304761\n",
      "Epoch 7 step 2009: training accuarcy: 0.6945\n",
      "Epoch 7 step 2009: training loss: 1383.3853444019803\n",
      "Epoch 7 step 2010: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2010: training loss: 1382.1493924876793\n",
      "Epoch 7 step 2011: training accuarcy: 0.7115\n",
      "Epoch 7 step 2011: training loss: 1382.429153467883\n",
      "Epoch 7 step 2012: training accuarcy: 0.711\n",
      "Epoch 7 step 2012: training loss: 1381.6580251088676\n",
      "Epoch 7 step 2013: training accuarcy: 0.707\n",
      "Epoch 7 step 2013: training loss: 1382.0272257235463\n",
      "Epoch 7 step 2014: training accuarcy: 0.715\n",
      "Epoch 7 step 2014: training loss: 1383.463280344703\n",
      "Epoch 7 step 2015: training accuarcy: 0.68\n",
      "Epoch 7 step 2015: training loss: 1383.3854367992765\n",
      "Epoch 7 step 2016: training accuarcy: 0.6995\n",
      "Epoch 7 step 2016: training loss: 1383.1893180102234\n",
      "Epoch 7 step 2017: training accuarcy: 0.6805\n",
      "Epoch 7 step 2017: training loss: 1382.2574699271618\n",
      "Epoch 7 step 2018: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 2018: training loss: 1381.7713659772248\n",
      "Epoch 7 step 2019: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 2019: training loss: 1382.6251003740852\n",
      "Epoch 7 step 2020: training accuarcy: 0.7225\n",
      "Epoch 7 step 2020: training loss: 1382.6989417714576\n",
      "Epoch 7 step 2021: training accuarcy: 0.685\n",
      "Epoch 7 step 2021: training loss: 1383.3184181671552\n",
      "Epoch 7 step 2022: training accuarcy: 0.6895\n",
      "Epoch 7 step 2022: training loss: 1383.3959091111437\n",
      "Epoch 7 step 2023: training accuarcy: 0.6975\n",
      "Epoch 7 step 2023: training loss: 1383.4034246631988\n",
      "Epoch 7 step 2024: training accuarcy: 0.6955\n",
      "Epoch 7 step 2024: training loss: 1381.8268139009629\n",
      "Epoch 7 step 2025: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 2025: training loss: 1383.3749052270855\n",
      "Epoch 7 step 2026: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 2026: training loss: 1384.0016848437513\n",
      "Epoch 7 step 2027: training accuarcy: 0.6835\n",
      "Epoch 7 step 2027: training loss: 1382.148509634583\n",
      "Epoch 7 step 2028: training accuarcy: 0.7015\n",
      "Epoch 7 step 2028: training loss: 1383.2195894752633\n",
      "Epoch 7 step 2029: training accuarcy: 0.6715\n",
      "Epoch 7 step 2029: training loss: 1382.2087802492035\n",
      "Epoch 7 step 2030: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 2030: training loss: 1381.568950656667\n",
      "Epoch 7 step 2031: training accuarcy: 0.7225\n",
      "Epoch 7 step 2031: training loss: 1383.1441528252167\n",
      "Epoch 7 step 2032: training accuarcy: 0.6950000000000001\n",
      "Epoch 7 step 2032: training loss: 1381.7738328935784\n",
      "Epoch 7 step 2033: training accuarcy: 0.7005\n",
      "Epoch 7 step 2033: training loss: 1382.5142156132465\n",
      "Epoch 7 step 2034: training accuarcy: 0.7045\n",
      "Epoch 7 step 2034: training loss: 1383.3433812358376\n",
      "Epoch 7 step 2035: training accuarcy: 0.7035\n",
      "Epoch 7 step 2035: training loss: 1381.6758552725903\n",
      "Epoch 7 step 2036: training accuarcy: 0.6985\n",
      "Epoch 7 step 2036: training loss: 1382.9492043371129\n",
      "Epoch 7 step 2037: training accuarcy: 0.6985\n",
      "Epoch 7 step 2037: training loss: 1383.191589789876\n",
      "Epoch 7 step 2038: training accuarcy: 0.6895\n",
      "Epoch 7 step 2038: training loss: 1383.0451275457776\n",
      "Epoch 7 step 2039: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 2039: training loss: 1384.8761668834325\n",
      "Epoch 7 step 2040: training accuarcy: 0.684\n",
      "Epoch 7 step 2040: training loss: 1382.315535318513\n",
      "Epoch 7 step 2041: training accuarcy: 0.6965\n",
      "Epoch 7 step 2041: training loss: 1382.9946755194778\n",
      "Epoch 7 step 2042: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 2042: training loss: 1382.347136125648\n",
      "Epoch 7 step 2043: training accuarcy: 0.7185\n",
      "Epoch 7 step 2043: training loss: 1383.390883793246\n",
      "Epoch 7 step 2044: training accuarcy: 0.704\n",
      "Epoch 7 step 2044: training loss: 1381.9063198891429\n",
      "Epoch 7 step 2045: training accuarcy: 0.7005\n",
      "Epoch 7 step 2045: training loss: 1382.4886581789438\n",
      "Epoch 7 step 2046: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 2046: training loss: 1382.5702973043956\n",
      "Epoch 7 step 2047: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2047: training loss: 1383.4694350407758\n",
      "Epoch 7 step 2048: training accuarcy: 0.6955\n",
      "Epoch 7 step 2048: training loss: 1382.4956106742504\n",
      "Epoch 7 step 2049: training accuarcy: 0.685\n",
      "Epoch 7 step 2049: training loss: 1382.5135140079985\n",
      "Epoch 7 step 2050: training accuarcy: 0.678\n",
      "Epoch 7 step 2050: training loss: 1383.558775972797\n",
      "Epoch 7 step 2051: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 2051: training loss: 1382.4660468600568\n",
      "Epoch 7 step 2052: training accuarcy: 0.7055\n",
      "Epoch 7 step 2052: training loss: 1382.7933164365163\n",
      "Epoch 7 step 2053: training accuarcy: 0.6955\n",
      "Epoch 7 step 2053: training loss: 1382.9793721051658\n",
      "Epoch 7 step 2054: training accuarcy: 0.6955\n",
      "Epoch 7 step 2054: training loss: 1383.0868765929588\n",
      "Epoch 7 step 2055: training accuarcy: 0.6910000000000001\n",
      "Epoch 7 step 2055: training loss: 1383.2520644337146\n",
      "Epoch 7 step 2056: training accuarcy: 0.7005\n",
      "Epoch 7 step 2056: training loss: 1382.6117511996072\n",
      "Epoch 7 step 2057: training accuarcy: 0.7055\n",
      "Epoch 7 step 2057: training loss: 1383.1640257911342\n",
      "Epoch 7 step 2058: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2058: training loss: 1382.9740542258644\n",
      "Epoch 7 step 2059: training accuarcy: 0.6890000000000001\n",
      "Epoch 7 step 2059: training loss: 1383.297283502518\n",
      "Epoch 7 step 2060: training accuarcy: 0.6895\n",
      "Epoch 7 step 2060: training loss: 1382.7589975163887\n",
      "Epoch 7 step 2061: training accuarcy: 0.687\n",
      "Epoch 7 step 2061: training loss: 1383.4250146023487\n",
      "Epoch 7 step 2062: training accuarcy: 0.705\n",
      "Epoch 7 step 2062: training loss: 1383.1919483399793\n",
      "Epoch 7 step 2063: training accuarcy: 0.6935\n",
      "Epoch 7 step 2063: training loss: 1382.301573322696\n",
      "Epoch 7 step 2064: training accuarcy: 0.7045\n",
      "Epoch 7 step 2064: training loss: 1382.3481636013687\n",
      "Epoch 7 step 2065: training accuarcy: 0.6825\n",
      "Epoch 7 step 2065: training loss: 1382.721566870602\n",
      "Epoch 7 step 2066: training accuarcy: 0.6825\n",
      "Epoch 7 step 2066: training loss: 1382.5898181583511\n",
      "Epoch 7 step 2067: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 2067: training loss: 1382.5633876823506\n",
      "Epoch 7 step 2068: training accuarcy: 0.71\n",
      "Epoch 7 step 2068: training loss: 1383.546104719856\n",
      "Epoch 7 step 2069: training accuarcy: 0.678\n",
      "Epoch 7 step 2069: training loss: 1382.5308906948135\n",
      "Epoch 7 step 2070: training accuarcy: 0.6805\n",
      "Epoch 7 step 2070: training loss: 1383.3669578686822\n",
      "Epoch 7 step 2071: training accuarcy: 0.687\n",
      "Epoch 7 step 2071: training loss: 1383.3987621237263\n",
      "Epoch 7 step 2072: training accuarcy: 0.6895\n",
      "Epoch 7 step 2072: training loss: 1382.7972369641752\n",
      "Epoch 7 step 2073: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 2073: training loss: 1383.9657718759267\n",
      "Epoch 7 step 2074: training accuarcy: 0.6975\n",
      "Epoch 7 step 2074: training loss: 1383.2739988994238\n",
      "Epoch 7 step 2075: training accuarcy: 0.7035\n",
      "Epoch 7 step 2075: training loss: 1383.6034375621978\n",
      "Epoch 7 step 2076: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2076: training loss: 1381.8546343935025\n",
      "Epoch 7 step 2077: training accuarcy: 0.709\n",
      "Epoch 7 step 2077: training loss: 1382.5917915487757\n",
      "Epoch 7 step 2078: training accuarcy: 0.716\n",
      "Epoch 7 step 2078: training loss: 1381.7020321113912\n",
      "Epoch 7 step 2079: training accuarcy: 0.7115\n",
      "Epoch 7 step 2079: training loss: 1384.2011504735474\n",
      "Epoch 7 step 2080: training accuarcy: 0.6885\n",
      "Epoch 7 step 2080: training loss: 1382.384190891219\n",
      "Epoch 7 step 2081: training accuarcy: 0.7075\n",
      "Epoch 7 step 2081: training loss: 1383.374364854654\n",
      "Epoch 7 step 2082: training accuarcy: 0.6945\n",
      "Epoch 7 step 2082: training loss: 1382.954434501625\n",
      "Epoch 7 step 2083: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2083: training loss: 1383.8024422043643\n",
      "Epoch 7 step 2084: training accuarcy: 0.6880000000000001\n",
      "Epoch 7 step 2084: training loss: 1382.6606739560916\n",
      "Epoch 7 step 2085: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 2085: training loss: 1383.9491932681844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2086: training accuarcy: 0.683\n",
      "Epoch 7 step 2086: training loss: 1383.475166157004\n",
      "Epoch 7 step 2087: training accuarcy: 0.6910000000000001\n",
      "Epoch 7 step 2087: training loss: 1382.605241274103\n",
      "Epoch 7 step 2088: training accuarcy: 0.7065\n",
      "Epoch 7 step 2088: training loss: 1382.853922574848\n",
      "Epoch 7 step 2089: training accuarcy: 0.7135\n",
      "Epoch 7 step 2089: training loss: 1382.7739855277407\n",
      "Epoch 7 step 2090: training accuarcy: 0.7035\n",
      "Epoch 7 step 2090: training loss: 1381.6857171824581\n",
      "Epoch 7 step 2091: training accuarcy: 0.7175\n",
      "Epoch 7 step 2091: training loss: 1382.2089479325002\n",
      "Epoch 7 step 2092: training accuarcy: 0.6975\n",
      "Epoch 7 step 2092: training loss: 1384.6542699408378\n",
      "Epoch 7 step 2093: training accuarcy: 0.679\n",
      "Epoch 7 step 2093: training loss: 1382.7846881925848\n",
      "Epoch 7 step 2094: training accuarcy: 0.7015\n",
      "Epoch 7 step 2094: training loss: 1382.872421434367\n",
      "Epoch 7 step 2095: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 2095: training loss: 1383.661988762389\n",
      "Epoch 7 step 2096: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 2096: training loss: 1382.8828193362726\n",
      "Epoch 7 step 2097: training accuarcy: 0.706\n",
      "Epoch 7 step 2097: training loss: 1382.8054833610279\n",
      "Epoch 7 step 2098: training accuarcy: 0.6955\n",
      "Epoch 7 step 2098: training loss: 1383.1855861176098\n",
      "Epoch 7 step 2099: training accuarcy: 0.6895\n",
      "Epoch 7 step 2099: training loss: 1382.9760957318902\n",
      "Epoch 7 step 2100: training accuarcy: 0.7005\n",
      "Epoch 7 step 2100: training loss: 1383.4437447501978\n",
      "Epoch 7 step 2101: training accuarcy: 0.6925\n",
      "Epoch 7 step 2101: training loss: 1383.1937816222232\n",
      "Epoch 7 step 2102: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 2102: training loss: 1383.5634458112738\n",
      "Epoch 7 step 2103: training accuarcy: 0.6935\n",
      "Epoch 7 step 2103: training loss: 543.6967183521883\n",
      "Epoch 7 step 2104: training accuarcy: 0.6807692307692308\n",
      "Epoch 7: train loss 1379.5888962898016, train accuarcy 0.702133297920227\n",
      "Epoch 7: valid loss 1363.7015398173155, valid accuarcy 0.7098241448402405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [16:07<00:00, 123.26s/it]\n"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=8,\n",
    "                 loss_callback=simple_loss_callback,\n",
    "                 log_dir=get_log_dir('topcoder', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:40:50.761855Z",
     "start_time": "2019-09-25T13:40:50.753854Z"
    }
   },
   "outputs": [],
   "source": [
    "del prme_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Trans FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:41:20.750866Z",
     "start_time": "2019-09-25T13:41:20.279842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTransFM()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model = TorchTransFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "trans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:41:25.540686Z",
     "start_time": "2019-09-25T13:41:25.535685Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(trans_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:01:44.545467Z",
     "start_time": "2019-09-25T14:01:44.416483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1f2000ef860>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_learner = FMLearner(trans_model, adam_opt, schedular, db)\n",
    "trans_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:59:33.576534Z",
     "start_time": "2019-09-25T13:41:49.976665Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 37807.26401193977\n",
      "Epoch 0 step 1: training accuarcy: 0.4955\n",
      "Epoch 0 step 1: training loss: 36749.778786841816\n",
      "Epoch 0 step 2: training accuarcy: 0.504\n",
      "Epoch 0 step 2: training loss: 35706.60817088014\n",
      "Epoch 0 step 3: training accuarcy: 0.5\n",
      "Epoch 0 step 3: training loss: 34674.50367937326\n",
      "Epoch 0 step 4: training accuarcy: 0.5275\n",
      "Epoch 0 step 4: training loss: 33686.3987004469\n",
      "Epoch 0 step 5: training accuarcy: 0.5125\n",
      "Epoch 0 step 5: training loss: 32706.96820491349\n",
      "Epoch 0 step 6: training accuarcy: 0.5165\n",
      "Epoch 0 step 6: training loss: 31751.5109393562\n",
      "Epoch 0 step 7: training accuarcy: 0.505\n",
      "Epoch 0 step 7: training loss: 30820.65553823673\n",
      "Epoch 0 step 8: training accuarcy: 0.524\n",
      "Epoch 0 step 8: training loss: 29908.863742371064\n",
      "Epoch 0 step 9: training accuarcy: 0.522\n",
      "Epoch 0 step 9: training loss: 29012.36624609563\n",
      "Epoch 0 step 10: training accuarcy: 0.529\n",
      "Epoch 0 step 10: training loss: 28151.494881656392\n",
      "Epoch 0 step 11: training accuarcy: 0.531\n",
      "Epoch 0 step 11: training loss: 27296.73065409167\n",
      "Epoch 0 step 12: training accuarcy: 0.532\n",
      "Epoch 0 step 12: training loss: 26465.293305611896\n",
      "Epoch 0 step 13: training accuarcy: 0.5525\n",
      "Epoch 0 step 13: training loss: 25663.33159232767\n",
      "Epoch 0 step 14: training accuarcy: 0.5465\n",
      "Epoch 0 step 14: training loss: 24874.398401647133\n",
      "Epoch 0 step 15: training accuarcy: 0.5735\n",
      "Epoch 0 step 15: training loss: 24108.81702105178\n",
      "Epoch 0 step 16: training accuarcy: 0.5575\n",
      "Epoch 0 step 16: training loss: 23364.372776034852\n",
      "Epoch 0 step 17: training accuarcy: 0.5635\n",
      "Epoch 0 step 17: training loss: 22632.536561241533\n",
      "Epoch 0 step 18: training accuarcy: 0.581\n",
      "Epoch 0 step 18: training loss: 21932.3174302705\n",
      "Epoch 0 step 19: training accuarcy: 0.5675\n",
      "Epoch 0 step 19: training loss: 21241.2289315382\n",
      "Epoch 0 step 20: training accuarcy: 0.5755\n",
      "Epoch 0 step 20: training loss: 20565.63908159412\n",
      "Epoch 0 step 21: training accuarcy: 0.5855\n",
      "Epoch 0 step 21: training loss: 19914.348633807673\n",
      "Epoch 0 step 22: training accuarcy: 0.5760000000000001\n",
      "Epoch 0 step 22: training loss: 19290.43030440168\n",
      "Epoch 0 step 23: training accuarcy: 0.5760000000000001\n",
      "Epoch 0 step 23: training loss: 18673.217514074706\n",
      "Epoch 0 step 24: training accuarcy: 0.5675\n",
      "Epoch 0 step 24: training loss: 18074.13889747675\n",
      "Epoch 0 step 25: training accuarcy: 0.5765\n",
      "Epoch 0 step 25: training loss: 17515.124039011876\n",
      "Epoch 0 step 26: training accuarcy: 0.5675\n",
      "Epoch 0 step 26: training loss: 16928.95624648848\n",
      "Epoch 0 step 27: training accuarcy: 0.5995\n",
      "Epoch 0 step 27: training loss: 16376.251210331942\n",
      "Epoch 0 step 28: training accuarcy: 0.612\n",
      "Epoch 0 step 28: training loss: 15849.988447244388\n",
      "Epoch 0 step 29: training accuarcy: 0.61\n",
      "Epoch 0 step 29: training loss: 15336.521834996098\n",
      "Epoch 0 step 30: training accuarcy: 0.6\n",
      "Epoch 0 step 30: training loss: 14847.306555761625\n",
      "Epoch 0 step 31: training accuarcy: 0.6\n",
      "Epoch 0 step 31: training loss: 14366.568087915792\n",
      "Epoch 0 step 32: training accuarcy: 0.5855\n",
      "Epoch 0 step 32: training loss: 13884.270569369459\n",
      "Epoch 0 step 33: training accuarcy: 0.6135\n",
      "Epoch 0 step 33: training loss: 13429.200777405837\n",
      "Epoch 0 step 34: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 34: training loss: 12993.172393196342\n",
      "Epoch 0 step 35: training accuarcy: 0.61\n",
      "Epoch 0 step 35: training loss: 12561.847787314573\n",
      "Epoch 0 step 36: training accuarcy: 0.6335000000000001\n",
      "Epoch 0 step 36: training loss: 12151.778393121518\n",
      "Epoch 0 step 37: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 37: training loss: 11747.452603597381\n",
      "Epoch 0 step 38: training accuarcy: 0.6355000000000001\n",
      "Epoch 0 step 38: training loss: 11370.18956021107\n",
      "Epoch 0 step 39: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 39: training loss: 11003.954083230707\n",
      "Epoch 0 step 40: training accuarcy: 0.6145\n",
      "Epoch 0 step 40: training loss: 10646.632036427458\n",
      "Epoch 0 step 41: training accuarcy: 0.6095\n",
      "Epoch 0 step 41: training loss: 10277.94445478633\n",
      "Epoch 0 step 42: training accuarcy: 0.6415\n",
      "Epoch 0 step 42: training loss: 9944.35775023208\n",
      "Epoch 0 step 43: training accuarcy: 0.6415\n",
      "Epoch 0 step 43: training loss: 9620.457316117445\n",
      "Epoch 0 step 44: training accuarcy: 0.6335000000000001\n",
      "Epoch 0 step 44: training loss: 9291.776773222145\n",
      "Epoch 0 step 45: training accuarcy: 0.6625\n",
      "Epoch 0 step 45: training loss: 8999.448261826157\n",
      "Epoch 0 step 46: training accuarcy: 0.631\n",
      "Epoch 0 step 46: training loss: 8705.566687939012\n",
      "Epoch 0 step 47: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 47: training loss: 8411.011325638465\n",
      "Epoch 0 step 48: training accuarcy: 0.6395000000000001\n",
      "Epoch 0 step 48: training loss: 8125.302741168825\n",
      "Epoch 0 step 49: training accuarcy: 0.674\n",
      "Epoch 0 step 49: training loss: 7870.697359500158\n",
      "Epoch 0 step 50: training accuarcy: 0.655\n",
      "Epoch 0 step 50: training loss: 7620.628442819659\n",
      "Epoch 0 step 51: training accuarcy: 0.6445\n",
      "Epoch 0 step 51: training loss: 7369.017516477856\n",
      "Epoch 0 step 52: training accuarcy: 0.6515\n",
      "Epoch 0 step 52: training loss: 7134.474059178719\n",
      "Epoch 0 step 53: training accuarcy: 0.65\n",
      "Epoch 0 step 53: training loss: 6897.915740659644\n",
      "Epoch 0 step 54: training accuarcy: 0.663\n",
      "Epoch 0 step 54: training loss: 6674.080289255842\n",
      "Epoch 0 step 55: training accuarcy: 0.67\n",
      "Epoch 0 step 55: training loss: 6472.874866710912\n",
      "Epoch 0 step 56: training accuarcy: 0.6455\n",
      "Epoch 0 step 56: training loss: 6238.577805738282\n",
      "Epoch 0 step 57: training accuarcy: 0.6785\n",
      "Epoch 0 step 57: training loss: 6068.08365176819\n",
      "Epoch 0 step 58: training accuarcy: 0.6485\n",
      "Epoch 0 step 58: training loss: 5872.644907243513\n",
      "Epoch 0 step 59: training accuarcy: 0.6495\n",
      "Epoch 0 step 59: training loss: 5687.595115027241\n",
      "Epoch 0 step 60: training accuarcy: 0.6525\n",
      "Epoch 0 step 60: training loss: 5486.259298334925\n",
      "Epoch 0 step 61: training accuarcy: 0.677\n",
      "Epoch 0 step 61: training loss: 5334.817678426318\n",
      "Epoch 0 step 62: training accuarcy: 0.6645\n",
      "Epoch 0 step 62: training loss: 5144.1925596263545\n",
      "Epoch 0 step 63: training accuarcy: 0.6935\n",
      "Epoch 0 step 63: training loss: 4980.7562944827405\n",
      "Epoch 0 step 64: training accuarcy: 0.7010000000000001\n",
      "Epoch 0 step 64: training loss: 4847.351369488293\n",
      "Epoch 0 step 65: training accuarcy: 0.675\n",
      "Epoch 0 step 65: training loss: 4680.587684682028\n",
      "Epoch 0 step 66: training accuarcy: 0.682\n",
      "Epoch 0 step 66: training loss: 4554.403222704429\n",
      "Epoch 0 step 67: training accuarcy: 0.677\n",
      "Epoch 0 step 67: training loss: 4424.322252587183\n",
      "Epoch 0 step 68: training accuarcy: 0.6655\n",
      "Epoch 0 step 68: training loss: 4284.222575264397\n",
      "Epoch 0 step 69: training accuarcy: 0.6845\n",
      "Epoch 0 step 69: training loss: 4136.81348425163\n",
      "Epoch 0 step 70: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 70: training loss: 4034.7659112444258\n",
      "Epoch 0 step 71: training accuarcy: 0.6805\n",
      "Epoch 0 step 71: training loss: 3910.9619909563066\n",
      "Epoch 0 step 72: training accuarcy: 0.6895\n",
      "Epoch 0 step 72: training loss: 3797.9764625649073\n",
      "Epoch 0 step 73: training accuarcy: 0.674\n",
      "Epoch 0 step 73: training loss: 3686.4941960656242\n",
      "Epoch 0 step 74: training accuarcy: 0.6815\n",
      "Epoch 0 step 74: training loss: 3583.539933729381\n",
      "Epoch 0 step 75: training accuarcy: 0.683\n",
      "Epoch 0 step 75: training loss: 3476.531925526665\n",
      "Epoch 0 step 76: training accuarcy: 0.705\n",
      "Epoch 0 step 76: training loss: 3382.8494498929367\n",
      "Epoch 0 step 77: training accuarcy: 0.7030000000000001\n",
      "Epoch 0 step 77: training loss: 3291.016697247184\n",
      "Epoch 0 step 78: training accuarcy: 0.6900000000000001\n",
      "Epoch 0 step 78: training loss: 3207.264472623237\n",
      "Epoch 0 step 79: training accuarcy: 0.6915\n",
      "Epoch 0 step 79: training loss: 3103.8994136020506\n",
      "Epoch 0 step 80: training accuarcy: 0.708\n",
      "Epoch 0 step 80: training loss: 3043.7345031251625\n",
      "Epoch 0 step 81: training accuarcy: 0.673\n",
      "Epoch 0 step 81: training loss: 2958.5010603649885\n",
      "Epoch 0 step 82: training accuarcy: 0.6885\n",
      "Epoch 0 step 82: training loss: 2874.053798820428\n",
      "Epoch 0 step 83: training accuarcy: 0.712\n",
      "Epoch 0 step 83: training loss: 2811.552514865482\n",
      "Epoch 0 step 84: training accuarcy: 0.68\n",
      "Epoch 0 step 84: training loss: 2729.337244245081\n",
      "Epoch 0 step 85: training accuarcy: 0.6875\n",
      "Epoch 0 step 85: training loss: 2666.2523024403354\n",
      "Epoch 0 step 86: training accuarcy: 0.6910000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 86: training loss: 2608.6593532551665\n",
      "Epoch 0 step 87: training accuarcy: 0.7025\n",
      "Epoch 0 step 87: training loss: 2544.652764805356\n",
      "Epoch 0 step 88: training accuarcy: 0.6880000000000001\n",
      "Epoch 0 step 88: training loss: 2468.6590752649295\n",
      "Epoch 0 step 89: training accuarcy: 0.71\n",
      "Epoch 0 step 89: training loss: 2423.153420795797\n",
      "Epoch 0 step 90: training accuarcy: 0.6975\n",
      "Epoch 0 step 90: training loss: 2361.7523445788593\n",
      "Epoch 0 step 91: training accuarcy: 0.713\n",
      "Epoch 0 step 91: training loss: 2310.594598037431\n",
      "Epoch 0 step 92: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 92: training loss: 2254.93353079578\n",
      "Epoch 0 step 93: training accuarcy: 0.712\n",
      "Epoch 0 step 93: training loss: 2206.2737715561316\n",
      "Epoch 0 step 94: training accuarcy: 0.7135\n",
      "Epoch 0 step 94: training loss: 2179.5454209468944\n",
      "Epoch 0 step 95: training accuarcy: 0.684\n",
      "Epoch 0 step 95: training loss: 2129.032255605367\n",
      "Epoch 0 step 96: training accuarcy: 0.6985\n",
      "Epoch 0 step 96: training loss: 2075.8736270671952\n",
      "Epoch 0 step 97: training accuarcy: 0.7085\n",
      "Epoch 0 step 97: training loss: 2035.9709134901077\n",
      "Epoch 0 step 98: training accuarcy: 0.7085\n",
      "Epoch 0 step 98: training loss: 1993.3797435417275\n",
      "Epoch 0 step 99: training accuarcy: 0.7085\n",
      "Epoch 0 step 99: training loss: 1954.3972386910968\n",
      "Epoch 0 step 100: training accuarcy: 0.7105\n",
      "Epoch 0 step 100: training loss: 1927.6730427420675\n",
      "Epoch 0 step 101: training accuarcy: 0.7035\n",
      "Epoch 0 step 101: training loss: 1886.1198058622754\n",
      "Epoch 0 step 102: training accuarcy: 0.7005\n",
      "Epoch 0 step 102: training loss: 1850.0905513083387\n",
      "Epoch 0 step 103: training accuarcy: 0.711\n",
      "Epoch 0 step 103: training loss: 1841.0903347695662\n",
      "Epoch 0 step 104: training accuarcy: 0.6915\n",
      "Epoch 0 step 104: training loss: 1798.8394565356461\n",
      "Epoch 0 step 105: training accuarcy: 0.6995\n",
      "Epoch 0 step 105: training loss: 1762.9487344945678\n",
      "Epoch 0 step 106: training accuarcy: 0.7185\n",
      "Epoch 0 step 106: training loss: 1733.0626431074666\n",
      "Epoch 0 step 107: training accuarcy: 0.715\n",
      "Epoch 0 step 107: training loss: 1710.380037864818\n",
      "Epoch 0 step 108: training accuarcy: 0.721\n",
      "Epoch 0 step 108: training loss: 1675.265609888339\n",
      "Epoch 0 step 109: training accuarcy: 0.711\n",
      "Epoch 0 step 109: training loss: 1648.5779112702803\n",
      "Epoch 0 step 110: training accuarcy: 0.7245\n",
      "Epoch 0 step 110: training loss: 1647.3809420328232\n",
      "Epoch 0 step 111: training accuarcy: 0.7095\n",
      "Epoch 0 step 111: training loss: 1633.3702005702396\n",
      "Epoch 0 step 112: training accuarcy: 0.6905\n",
      "Epoch 0 step 112: training loss: 1597.4880324723638\n",
      "Epoch 0 step 113: training accuarcy: 0.705\n",
      "Epoch 0 step 113: training loss: 1579.078647840429\n",
      "Epoch 0 step 114: training accuarcy: 0.6990000000000001\n",
      "Epoch 0 step 114: training loss: 1566.2107715791728\n",
      "Epoch 0 step 115: training accuarcy: 0.714\n",
      "Epoch 0 step 115: training loss: 1536.0951812720323\n",
      "Epoch 0 step 116: training accuarcy: 0.716\n",
      "Epoch 0 step 116: training loss: 1501.7811420943224\n",
      "Epoch 0 step 117: training accuarcy: 0.741\n",
      "Epoch 0 step 117: training loss: 1517.2548790899996\n",
      "Epoch 0 step 118: training accuarcy: 0.7145\n",
      "Epoch 0 step 118: training loss: 1501.2234539052015\n",
      "Epoch 0 step 119: training accuarcy: 0.7010000000000001\n",
      "Epoch 0 step 119: training loss: 1466.312368547694\n",
      "Epoch 0 step 120: training accuarcy: 0.723\n",
      "Epoch 0 step 120: training loss: 1450.4940142050527\n",
      "Epoch 0 step 121: training accuarcy: 0.7185\n",
      "Epoch 0 step 121: training loss: 1432.6769432313476\n",
      "Epoch 0 step 122: training accuarcy: 0.732\n",
      "Epoch 0 step 122: training loss: 1429.1484376279031\n",
      "Epoch 0 step 123: training accuarcy: 0.724\n",
      "Epoch 0 step 123: training loss: 1420.510733923596\n",
      "Epoch 0 step 124: training accuarcy: 0.7105\n",
      "Epoch 0 step 124: training loss: 1402.4185365007986\n",
      "Epoch 0 step 125: training accuarcy: 0.7195\n",
      "Epoch 0 step 125: training loss: 1397.7160325818302\n",
      "Epoch 0 step 126: training accuarcy: 0.711\n",
      "Epoch 0 step 126: training loss: 1357.4998526525949\n",
      "Epoch 0 step 127: training accuarcy: 0.731\n",
      "Epoch 0 step 127: training loss: 1361.6563353731374\n",
      "Epoch 0 step 128: training accuarcy: 0.7145\n",
      "Epoch 0 step 128: training loss: 1361.5360167508948\n",
      "Epoch 0 step 129: training accuarcy: 0.724\n",
      "Epoch 0 step 129: training loss: 1372.4237233278684\n",
      "Epoch 0 step 130: training accuarcy: 0.7105\n",
      "Epoch 0 step 130: training loss: 1349.5937886560278\n",
      "Epoch 0 step 131: training accuarcy: 0.7005\n",
      "Epoch 0 step 131: training loss: 1326.600667653221\n",
      "Epoch 0 step 132: training accuarcy: 0.729\n",
      "Epoch 0 step 132: training loss: 1315.50424680294\n",
      "Epoch 0 step 133: training accuarcy: 0.7315\n",
      "Epoch 0 step 133: training loss: 1312.919796029183\n",
      "Epoch 0 step 134: training accuarcy: 0.7235\n",
      "Epoch 0 step 134: training loss: 1282.4709758879862\n",
      "Epoch 0 step 135: training accuarcy: 0.738\n",
      "Epoch 0 step 135: training loss: 1284.5860610171092\n",
      "Epoch 0 step 136: training accuarcy: 0.7315\n",
      "Epoch 0 step 136: training loss: 1283.8376772029837\n",
      "Epoch 0 step 137: training accuarcy: 0.7375\n",
      "Epoch 0 step 137: training loss: 1295.186555747755\n",
      "Epoch 0 step 138: training accuarcy: 0.7165\n",
      "Epoch 0 step 138: training loss: 1280.9211586904385\n",
      "Epoch 0 step 139: training accuarcy: 0.712\n",
      "Epoch 0 step 139: training loss: 1282.1534772507393\n",
      "Epoch 0 step 140: training accuarcy: 0.7125\n",
      "Epoch 0 step 140: training loss: 1269.354818383104\n",
      "Epoch 0 step 141: training accuarcy: 0.718\n",
      "Epoch 0 step 141: training loss: 1256.3801779009002\n",
      "Epoch 0 step 142: training accuarcy: 0.7335\n",
      "Epoch 0 step 142: training loss: 1260.5525024095418\n",
      "Epoch 0 step 143: training accuarcy: 0.7165\n",
      "Epoch 0 step 143: training loss: 1222.5887550519124\n",
      "Epoch 0 step 144: training accuarcy: 0.7425\n",
      "Epoch 0 step 144: training loss: 1228.7804044552704\n",
      "Epoch 0 step 145: training accuarcy: 0.7315\n",
      "Epoch 0 step 145: training loss: 1244.1737650023947\n",
      "Epoch 0 step 146: training accuarcy: 0.724\n",
      "Epoch 0 step 146: training loss: 1236.5794005125633\n",
      "Epoch 0 step 147: training accuarcy: 0.7365\n",
      "Epoch 0 step 147: training loss: 1226.266435221166\n",
      "Epoch 0 step 148: training accuarcy: 0.7325\n",
      "Epoch 0 step 148: training loss: 1213.10731553315\n",
      "Epoch 0 step 149: training accuarcy: 0.742\n",
      "Epoch 0 step 149: training loss: 1225.0384420131065\n",
      "Epoch 0 step 150: training accuarcy: 0.714\n",
      "Epoch 0 step 150: training loss: 1217.2996274897494\n",
      "Epoch 0 step 151: training accuarcy: 0.722\n",
      "Epoch 0 step 151: training loss: 1227.84155184617\n",
      "Epoch 0 step 152: training accuarcy: 0.7145\n",
      "Epoch 0 step 152: training loss: 1202.5860792428082\n",
      "Epoch 0 step 153: training accuarcy: 0.7385\n",
      "Epoch 0 step 153: training loss: 1207.9447280215845\n",
      "Epoch 0 step 154: training accuarcy: 0.7185\n",
      "Epoch 0 step 154: training loss: 1195.2877449141567\n",
      "Epoch 0 step 155: training accuarcy: 0.732\n",
      "Epoch 0 step 155: training loss: 1196.2440751727117\n",
      "Epoch 0 step 156: training accuarcy: 0.7295\n",
      "Epoch 0 step 156: training loss: 1194.6329608790734\n",
      "Epoch 0 step 157: training accuarcy: 0.729\n",
      "Epoch 0 step 157: training loss: 1184.7649059468536\n",
      "Epoch 0 step 158: training accuarcy: 0.7395\n",
      "Epoch 0 step 158: training loss: 1193.032734805529\n",
      "Epoch 0 step 159: training accuarcy: 0.7305\n",
      "Epoch 0 step 159: training loss: 1165.6210284946724\n",
      "Epoch 0 step 160: training accuarcy: 0.737\n",
      "Epoch 0 step 160: training loss: 1193.6654953404163\n",
      "Epoch 0 step 161: training accuarcy: 0.7155\n",
      "Epoch 0 step 161: training loss: 1180.39103466858\n",
      "Epoch 0 step 162: training accuarcy: 0.736\n",
      "Epoch 0 step 162: training loss: 1178.7229866295888\n",
      "Epoch 0 step 163: training accuarcy: 0.737\n",
      "Epoch 0 step 163: training loss: 1179.7303721261203\n",
      "Epoch 0 step 164: training accuarcy: 0.7225\n",
      "Epoch 0 step 164: training loss: 1161.5225663087497\n",
      "Epoch 0 step 165: training accuarcy: 0.7405\n",
      "Epoch 0 step 165: training loss: 1167.1448120644477\n",
      "Epoch 0 step 166: training accuarcy: 0.7465\n",
      "Epoch 0 step 166: training loss: 1189.4974800599095\n",
      "Epoch 0 step 167: training accuarcy: 0.7195\n",
      "Epoch 0 step 167: training loss: 1151.9930560984008\n",
      "Epoch 0 step 168: training accuarcy: 0.7475\n",
      "Epoch 0 step 168: training loss: 1168.1985040603033\n",
      "Epoch 0 step 169: training accuarcy: 0.739\n",
      "Epoch 0 step 169: training loss: 1158.230081184211\n",
      "Epoch 0 step 170: training accuarcy: 0.752\n",
      "Epoch 0 step 170: training loss: 1163.2051637503212\n",
      "Epoch 0 step 171: training accuarcy: 0.733\n",
      "Epoch 0 step 171: training loss: 1163.0653006023238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 172: training accuarcy: 0.7435\n",
      "Epoch 0 step 172: training loss: 1142.4687222205114\n",
      "Epoch 0 step 173: training accuarcy: 0.7425\n",
      "Epoch 0 step 173: training loss: 1148.4498302713594\n",
      "Epoch 0 step 174: training accuarcy: 0.748\n",
      "Epoch 0 step 174: training loss: 1126.3964042349307\n",
      "Epoch 0 step 175: training accuarcy: 0.751\n",
      "Epoch 0 step 175: training loss: 1175.33577255694\n",
      "Epoch 0 step 176: training accuarcy: 0.7195\n",
      "Epoch 0 step 176: training loss: 1150.6492550502567\n",
      "Epoch 0 step 177: training accuarcy: 0.738\n",
      "Epoch 0 step 177: training loss: 1154.2970654569183\n",
      "Epoch 0 step 178: training accuarcy: 0.751\n",
      "Epoch 0 step 178: training loss: 1132.282888711028\n",
      "Epoch 0 step 179: training accuarcy: 0.7485\n",
      "Epoch 0 step 179: training loss: 1129.9585267991922\n",
      "Epoch 0 step 180: training accuarcy: 0.755\n",
      "Epoch 0 step 180: training loss: 1151.8311434767393\n",
      "Epoch 0 step 181: training accuarcy: 0.737\n",
      "Epoch 0 step 181: training loss: 1144.1199802066885\n",
      "Epoch 0 step 182: training accuarcy: 0.7335\n",
      "Epoch 0 step 182: training loss: 1149.5717620679309\n",
      "Epoch 0 step 183: training accuarcy: 0.736\n",
      "Epoch 0 step 183: training loss: 1145.04565135365\n",
      "Epoch 0 step 184: training accuarcy: 0.7315\n",
      "Epoch 0 step 184: training loss: 1139.879609137377\n",
      "Epoch 0 step 185: training accuarcy: 0.734\n",
      "Epoch 0 step 185: training loss: 1139.1484468014248\n",
      "Epoch 0 step 186: training accuarcy: 0.7315\n",
      "Epoch 0 step 186: training loss: 1134.104896027933\n",
      "Epoch 0 step 187: training accuarcy: 0.754\n",
      "Epoch 0 step 187: training loss: 1138.1229620292077\n",
      "Epoch 0 step 188: training accuarcy: 0.731\n",
      "Epoch 0 step 188: training loss: 1113.0948424942744\n",
      "Epoch 0 step 189: training accuarcy: 0.752\n",
      "Epoch 0 step 189: training loss: 1136.3698136300725\n",
      "Epoch 0 step 190: training accuarcy: 0.7355\n",
      "Epoch 0 step 190: training loss: 1159.7078459241334\n",
      "Epoch 0 step 191: training accuarcy: 0.7295\n",
      "Epoch 0 step 191: training loss: 1116.5934299755656\n",
      "Epoch 0 step 192: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 192: training loss: 1139.8301478179762\n",
      "Epoch 0 step 193: training accuarcy: 0.7275\n",
      "Epoch 0 step 193: training loss: 1118.052967666961\n",
      "Epoch 0 step 194: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 194: training loss: 1115.6900541361024\n",
      "Epoch 0 step 195: training accuarcy: 0.7515000000000001\n",
      "Epoch 0 step 195: training loss: 1126.165256556415\n",
      "Epoch 0 step 196: training accuarcy: 0.743\n",
      "Epoch 0 step 196: training loss: 1116.6201583998072\n",
      "Epoch 0 step 197: training accuarcy: 0.735\n",
      "Epoch 0 step 197: training loss: 1108.0160008193236\n",
      "Epoch 0 step 198: training accuarcy: 0.7445\n",
      "Epoch 0 step 198: training loss: 1124.7942906608514\n",
      "Epoch 0 step 199: training accuarcy: 0.7455\n",
      "Epoch 0 step 199: training loss: 1094.414690128558\n",
      "Epoch 0 step 200: training accuarcy: 0.756\n",
      "Epoch 0 step 200: training loss: 1136.8417697555096\n",
      "Epoch 0 step 201: training accuarcy: 0.7315\n",
      "Epoch 0 step 201: training loss: 1098.304564864787\n",
      "Epoch 0 step 202: training accuarcy: 0.766\n",
      "Epoch 0 step 202: training loss: 1118.7497192760854\n",
      "Epoch 0 step 203: training accuarcy: 0.7335\n",
      "Epoch 0 step 203: training loss: 1098.2509754300397\n",
      "Epoch 0 step 204: training accuarcy: 0.7505000000000001\n",
      "Epoch 0 step 204: training loss: 1084.7903029102504\n",
      "Epoch 0 step 205: training accuarcy: 0.7685\n",
      "Epoch 0 step 205: training loss: 1114.3538648747558\n",
      "Epoch 0 step 206: training accuarcy: 0.7375\n",
      "Epoch 0 step 206: training loss: 1109.1628794110952\n",
      "Epoch 0 step 207: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 207: training loss: 1101.90181965778\n",
      "Epoch 0 step 208: training accuarcy: 0.7495\n",
      "Epoch 0 step 208: training loss: 1098.353421050054\n",
      "Epoch 0 step 209: training accuarcy: 0.757\n",
      "Epoch 0 step 209: training loss: 1112.8900540336288\n",
      "Epoch 0 step 210: training accuarcy: 0.737\n",
      "Epoch 0 step 210: training loss: 1114.7670777665633\n",
      "Epoch 0 step 211: training accuarcy: 0.7325\n",
      "Epoch 0 step 211: training loss: 1122.8590159727923\n",
      "Epoch 0 step 212: training accuarcy: 0.736\n",
      "Epoch 0 step 212: training loss: 1123.302358444635\n",
      "Epoch 0 step 213: training accuarcy: 0.744\n",
      "Epoch 0 step 213: training loss: 1104.201056691348\n",
      "Epoch 0 step 214: training accuarcy: 0.747\n",
      "Epoch 0 step 214: training loss: 1096.7656611658979\n",
      "Epoch 0 step 215: training accuarcy: 0.761\n",
      "Epoch 0 step 215: training loss: 1108.66013939918\n",
      "Epoch 0 step 216: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 216: training loss: 1092.3197274873003\n",
      "Epoch 0 step 217: training accuarcy: 0.749\n",
      "Epoch 0 step 217: training loss: 1080.2445909111714\n",
      "Epoch 0 step 218: training accuarcy: 0.753\n",
      "Epoch 0 step 218: training loss: 1105.93240397533\n",
      "Epoch 0 step 219: training accuarcy: 0.7415\n",
      "Epoch 0 step 219: training loss: 1088.1897985383157\n",
      "Epoch 0 step 220: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 220: training loss: 1094.5257326100902\n",
      "Epoch 0 step 221: training accuarcy: 0.754\n",
      "Epoch 0 step 221: training loss: 1096.1290052438412\n",
      "Epoch 0 step 222: training accuarcy: 0.7695\n",
      "Epoch 0 step 222: training loss: 1084.7712796017945\n",
      "Epoch 0 step 223: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 223: training loss: 1097.080853931582\n",
      "Epoch 0 step 224: training accuarcy: 0.752\n",
      "Epoch 0 step 224: training loss: 1124.7750507094092\n",
      "Epoch 0 step 225: training accuarcy: 0.7275\n",
      "Epoch 0 step 225: training loss: 1078.0326860853854\n",
      "Epoch 0 step 226: training accuarcy: 0.755\n",
      "Epoch 0 step 226: training loss: 1090.6661707782384\n",
      "Epoch 0 step 227: training accuarcy: 0.7485\n",
      "Epoch 0 step 227: training loss: 1103.1397659234194\n",
      "Epoch 0 step 228: training accuarcy: 0.7495\n",
      "Epoch 0 step 228: training loss: 1101.838882137178\n",
      "Epoch 0 step 229: training accuarcy: 0.738\n",
      "Epoch 0 step 229: training loss: 1085.552859717642\n",
      "Epoch 0 step 230: training accuarcy: 0.751\n",
      "Epoch 0 step 230: training loss: 1077.2558804575128\n",
      "Epoch 0 step 231: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 231: training loss: 1093.9134904191671\n",
      "Epoch 0 step 232: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 232: training loss: 1096.421543036433\n",
      "Epoch 0 step 233: training accuarcy: 0.759\n",
      "Epoch 0 step 233: training loss: 1079.8327969811635\n",
      "Epoch 0 step 234: training accuarcy: 0.7505000000000001\n",
      "Epoch 0 step 234: training loss: 1118.7340116938014\n",
      "Epoch 0 step 235: training accuarcy: 0.7385\n",
      "Epoch 0 step 235: training loss: 1103.9720289932052\n",
      "Epoch 0 step 236: training accuarcy: 0.7465\n",
      "Epoch 0 step 236: training loss: 1095.5555931653569\n",
      "Epoch 0 step 237: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 237: training loss: 1076.9513968918732\n",
      "Epoch 0 step 238: training accuarcy: 0.7685\n",
      "Epoch 0 step 238: training loss: 1072.517621425396\n",
      "Epoch 0 step 239: training accuarcy: 0.7585000000000001\n",
      "Epoch 0 step 239: training loss: 1054.017455433905\n",
      "Epoch 0 step 240: training accuarcy: 0.7705\n",
      "Epoch 0 step 240: training loss: 1081.4099765563258\n",
      "Epoch 0 step 241: training accuarcy: 0.7525000000000001\n",
      "Epoch 0 step 241: training loss: 1078.7900167183884\n",
      "Epoch 0 step 242: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 242: training loss: 1084.7120022892088\n",
      "Epoch 0 step 243: training accuarcy: 0.7505000000000001\n",
      "Epoch 0 step 243: training loss: 1048.059120968565\n",
      "Epoch 0 step 244: training accuarcy: 0.776\n",
      "Epoch 0 step 244: training loss: 1057.6020296892452\n",
      "Epoch 0 step 245: training accuarcy: 0.767\n",
      "Epoch 0 step 245: training loss: 1070.747337109404\n",
      "Epoch 0 step 246: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 246: training loss: 1083.7991155223049\n",
      "Epoch 0 step 247: training accuarcy: 0.7455\n",
      "Epoch 0 step 247: training loss: 1093.2811430275992\n",
      "Epoch 0 step 248: training accuarcy: 0.751\n",
      "Epoch 0 step 248: training loss: 1107.259535008458\n",
      "Epoch 0 step 249: training accuarcy: 0.739\n",
      "Epoch 0 step 249: training loss: 1050.6813750660008\n",
      "Epoch 0 step 250: training accuarcy: 0.7755\n",
      "Epoch 0 step 250: training loss: 1083.5120033443106\n",
      "Epoch 0 step 251: training accuarcy: 0.759\n",
      "Epoch 0 step 251: training loss: 1055.6936243028965\n",
      "Epoch 0 step 252: training accuarcy: 0.7635000000000001\n",
      "Epoch 0 step 252: training loss: 1069.2158951827535\n",
      "Epoch 0 step 253: training accuarcy: 0.755\n",
      "Epoch 0 step 253: training loss: 1075.0355515731055\n",
      "Epoch 0 step 254: training accuarcy: 0.7455\n",
      "Epoch 0 step 254: training loss: 1074.3076205464038\n",
      "Epoch 0 step 255: training accuarcy: 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 255: training loss: 1084.4392297650447\n",
      "Epoch 0 step 256: training accuarcy: 0.751\n",
      "Epoch 0 step 256: training loss: 1064.7772086371895\n",
      "Epoch 0 step 257: training accuarcy: 0.7495\n",
      "Epoch 0 step 257: training loss: 1078.0242551238125\n",
      "Epoch 0 step 258: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 258: training loss: 1080.5240449647833\n",
      "Epoch 0 step 259: training accuarcy: 0.7445\n",
      "Epoch 0 step 259: training loss: 1070.4557052705234\n",
      "Epoch 0 step 260: training accuarcy: 0.76\n",
      "Epoch 0 step 260: training loss: 1051.8578109743326\n",
      "Epoch 0 step 261: training accuarcy: 0.7795\n",
      "Epoch 0 step 261: training loss: 1052.9890355081523\n",
      "Epoch 0 step 262: training accuarcy: 0.7675000000000001\n",
      "Epoch 0 step 262: training loss: 419.62126561278643\n",
      "Epoch 0 step 263: training accuarcy: 0.7769230769230769\n",
      "Epoch 0: train loss 5223.848272861555, train accuarcy 0.6741687059402466\n",
      "Epoch 0: valid loss 1091.9839639880483, valid accuarcy 0.7284212708473206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████                                                                                                                                     | 1/8 [02:13<15:37, 133.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 968.1247141387189\n",
      "Epoch 1 step 264: training accuarcy: 0.8155\n",
      "Epoch 1 step 264: training loss: 974.9155207398045\n",
      "Epoch 1 step 265: training accuarcy: 0.799\n",
      "Epoch 1 step 265: training loss: 968.9799277918569\n",
      "Epoch 1 step 266: training accuarcy: 0.8115\n",
      "Epoch 1 step 266: training loss: 969.3743419102765\n",
      "Epoch 1 step 267: training accuarcy: 0.8095\n",
      "Epoch 1 step 267: training loss: 975.4692887293817\n",
      "Epoch 1 step 268: training accuarcy: 0.802\n",
      "Epoch 1 step 268: training loss: 986.443471727153\n",
      "Epoch 1 step 269: training accuarcy: 0.801\n",
      "Epoch 1 step 269: training loss: 965.1668782718472\n",
      "Epoch 1 step 270: training accuarcy: 0.809\n",
      "Epoch 1 step 270: training loss: 978.3685398036984\n",
      "Epoch 1 step 271: training accuarcy: 0.7915\n",
      "Epoch 1 step 271: training loss: 977.2293235825683\n",
      "Epoch 1 step 272: training accuarcy: 0.799\n",
      "Epoch 1 step 272: training loss: 976.9488489259388\n",
      "Epoch 1 step 273: training accuarcy: 0.798\n",
      "Epoch 1 step 273: training loss: 968.6765519553475\n",
      "Epoch 1 step 274: training accuarcy: 0.811\n",
      "Epoch 1 step 274: training loss: 967.3988338807497\n",
      "Epoch 1 step 275: training accuarcy: 0.808\n",
      "Epoch 1 step 275: training loss: 990.0202723941416\n",
      "Epoch 1 step 276: training accuarcy: 0.798\n",
      "Epoch 1 step 276: training loss: 991.5557820134724\n",
      "Epoch 1 step 277: training accuarcy: 0.799\n",
      "Epoch 1 step 277: training loss: 980.1655755840899\n",
      "Epoch 1 step 278: training accuarcy: 0.8005\n",
      "Epoch 1 step 278: training loss: 959.4941410540938\n",
      "Epoch 1 step 279: training accuarcy: 0.8140000000000001\n",
      "Epoch 1 step 279: training loss: 955.9214431318514\n",
      "Epoch 1 step 280: training accuarcy: 0.8180000000000001\n",
      "Epoch 1 step 280: training loss: 958.0553189057822\n",
      "Epoch 1 step 281: training accuarcy: 0.8095\n",
      "Epoch 1 step 281: training loss: 961.2588368668607\n",
      "Epoch 1 step 282: training accuarcy: 0.8025\n",
      "Epoch 1 step 282: training loss: 963.8433784536728\n",
      "Epoch 1 step 283: training accuarcy: 0.8025\n",
      "Epoch 1 step 283: training loss: 979.3783438952615\n",
      "Epoch 1 step 284: training accuarcy: 0.806\n",
      "Epoch 1 step 284: training loss: 963.0782442812564\n",
      "Epoch 1 step 285: training accuarcy: 0.81\n",
      "Epoch 1 step 285: training loss: 948.8312387673778\n",
      "Epoch 1 step 286: training accuarcy: 0.8185\n",
      "Epoch 1 step 286: training loss: 979.6909189725178\n",
      "Epoch 1 step 287: training accuarcy: 0.7965\n",
      "Epoch 1 step 287: training loss: 946.9660894498473\n",
      "Epoch 1 step 288: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 288: training loss: 974.6851141170539\n",
      "Epoch 1 step 289: training accuarcy: 0.802\n",
      "Epoch 1 step 289: training loss: 975.0458939221426\n",
      "Epoch 1 step 290: training accuarcy: 0.7945\n",
      "Epoch 1 step 290: training loss: 958.4915186084331\n",
      "Epoch 1 step 291: training accuarcy: 0.806\n",
      "Epoch 1 step 291: training loss: 969.8158073687748\n",
      "Epoch 1 step 292: training accuarcy: 0.8075\n",
      "Epoch 1 step 292: training loss: 953.9938644044348\n",
      "Epoch 1 step 293: training accuarcy: 0.8095\n",
      "Epoch 1 step 293: training loss: 978.3582524453193\n",
      "Epoch 1 step 294: training accuarcy: 0.7995\n",
      "Epoch 1 step 294: training loss: 947.5335551926571\n",
      "Epoch 1 step 295: training accuarcy: 0.8095\n",
      "Epoch 1 step 295: training loss: 961.4748081959683\n",
      "Epoch 1 step 296: training accuarcy: 0.802\n",
      "Epoch 1 step 296: training loss: 948.4041588850102\n",
      "Epoch 1 step 297: training accuarcy: 0.8085\n",
      "Epoch 1 step 297: training loss: 963.366037473171\n",
      "Epoch 1 step 298: training accuarcy: 0.8155\n",
      "Epoch 1 step 298: training loss: 949.2099318898507\n",
      "Epoch 1 step 299: training accuarcy: 0.792\n",
      "Epoch 1 step 299: training loss: 960.7395407782798\n",
      "Epoch 1 step 300: training accuarcy: 0.7945\n",
      "Epoch 1 step 300: training loss: 967.0012332772461\n",
      "Epoch 1 step 301: training accuarcy: 0.8085\n",
      "Epoch 1 step 301: training loss: 962.5263537158971\n",
      "Epoch 1 step 302: training accuarcy: 0.801\n",
      "Epoch 1 step 302: training loss: 950.7168434309668\n",
      "Epoch 1 step 303: training accuarcy: 0.808\n",
      "Epoch 1 step 303: training loss: 956.1449412421084\n",
      "Epoch 1 step 304: training accuarcy: 0.8015\n",
      "Epoch 1 step 304: training loss: 939.7307271607176\n",
      "Epoch 1 step 305: training accuarcy: 0.8135\n",
      "Epoch 1 step 305: training loss: 952.7095173690778\n",
      "Epoch 1 step 306: training accuarcy: 0.808\n",
      "Epoch 1 step 306: training loss: 965.0924585090116\n",
      "Epoch 1 step 307: training accuarcy: 0.8025\n",
      "Epoch 1 step 307: training loss: 947.6073854411834\n",
      "Epoch 1 step 308: training accuarcy: 0.811\n",
      "Epoch 1 step 308: training loss: 957.2226910915473\n",
      "Epoch 1 step 309: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 309: training loss: 955.7768564621261\n",
      "Epoch 1 step 310: training accuarcy: 0.8025\n",
      "Epoch 1 step 310: training loss: 937.6446772560804\n",
      "Epoch 1 step 311: training accuarcy: 0.8150000000000001\n",
      "Epoch 1 step 311: training loss: 970.2076418352667\n",
      "Epoch 1 step 312: training accuarcy: 0.8\n",
      "Epoch 1 step 312: training loss: 969.6621306618093\n",
      "Epoch 1 step 313: training accuarcy: 0.7995\n",
      "Epoch 1 step 313: training loss: 968.5107792350655\n",
      "Epoch 1 step 314: training accuarcy: 0.804\n",
      "Epoch 1 step 314: training loss: 926.1903379542814\n",
      "Epoch 1 step 315: training accuarcy: 0.8135\n",
      "Epoch 1 step 315: training loss: 944.6543444484688\n",
      "Epoch 1 step 316: training accuarcy: 0.8125\n",
      "Epoch 1 step 316: training loss: 979.5799436237542\n",
      "Epoch 1 step 317: training accuarcy: 0.7865\n",
      "Epoch 1 step 317: training loss: 943.1074887301318\n",
      "Epoch 1 step 318: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 318: training loss: 953.1796370105519\n",
      "Epoch 1 step 319: training accuarcy: 0.803\n",
      "Epoch 1 step 319: training loss: 967.042661787043\n",
      "Epoch 1 step 320: training accuarcy: 0.7995\n",
      "Epoch 1 step 320: training loss: 951.2773874169641\n",
      "Epoch 1 step 321: training accuarcy: 0.806\n",
      "Epoch 1 step 321: training loss: 968.2184369891289\n",
      "Epoch 1 step 322: training accuarcy: 0.7955\n",
      "Epoch 1 step 322: training loss: 968.7574056842118\n",
      "Epoch 1 step 323: training accuarcy: 0.793\n",
      "Epoch 1 step 323: training loss: 958.4653967528717\n",
      "Epoch 1 step 324: training accuarcy: 0.7955\n",
      "Epoch 1 step 324: training loss: 937.6073201946\n",
      "Epoch 1 step 325: training accuarcy: 0.812\n",
      "Epoch 1 step 325: training loss: 956.7467555866848\n",
      "Epoch 1 step 326: training accuarcy: 0.8015\n",
      "Epoch 1 step 326: training loss: 947.6129490351016\n",
      "Epoch 1 step 327: training accuarcy: 0.804\n",
      "Epoch 1 step 327: training loss: 944.2791409380202\n",
      "Epoch 1 step 328: training accuarcy: 0.798\n",
      "Epoch 1 step 328: training loss: 978.3855271004462\n",
      "Epoch 1 step 329: training accuarcy: 0.7955\n",
      "Epoch 1 step 329: training loss: 969.299055822711\n",
      "Epoch 1 step 330: training accuarcy: 0.789\n",
      "Epoch 1 step 330: training loss: 968.1583027739466\n",
      "Epoch 1 step 331: training accuarcy: 0.7945\n",
      "Epoch 1 step 331: training loss: 951.3468477448588\n",
      "Epoch 1 step 332: training accuarcy: 0.8055\n",
      "Epoch 1 step 332: training loss: 941.2527685464956\n",
      "Epoch 1 step 333: training accuarcy: 0.8175\n",
      "Epoch 1 step 333: training loss: 970.2815934874245\n",
      "Epoch 1 step 334: training accuarcy: 0.7925\n",
      "Epoch 1 step 334: training loss: 969.297863384904\n",
      "Epoch 1 step 335: training accuarcy: 0.797\n",
      "Epoch 1 step 335: training loss: 970.3446747411999\n",
      "Epoch 1 step 336: training accuarcy: 0.7935\n",
      "Epoch 1 step 336: training loss: 964.8972781144962\n",
      "Epoch 1 step 337: training accuarcy: 0.7955\n",
      "Epoch 1 step 337: training loss: 969.9226577456288\n",
      "Epoch 1 step 338: training accuarcy: 0.7835\n",
      "Epoch 1 step 338: training loss: 944.8713206901162\n",
      "Epoch 1 step 339: training accuarcy: 0.802\n",
      "Epoch 1 step 339: training loss: 942.6613706548015\n",
      "Epoch 1 step 340: training accuarcy: 0.804\n",
      "Epoch 1 step 340: training loss: 943.8016912033653\n",
      "Epoch 1 step 341: training accuarcy: 0.8015\n",
      "Epoch 1 step 341: training loss: 967.8787080837488\n",
      "Epoch 1 step 342: training accuarcy: 0.7985\n",
      "Epoch 1 step 342: training loss: 990.9678462621906\n",
      "Epoch 1 step 343: training accuarcy: 0.7755\n",
      "Epoch 1 step 343: training loss: 952.0363119666803\n",
      "Epoch 1 step 344: training accuarcy: 0.8\n",
      "Epoch 1 step 344: training loss: 916.7883767792899\n",
      "Epoch 1 step 345: training accuarcy: 0.8290000000000001\n",
      "Epoch 1 step 345: training loss: 947.4478747122247\n",
      "Epoch 1 step 346: training accuarcy: 0.807\n",
      "Epoch 1 step 346: training loss: 941.0675009885224\n",
      "Epoch 1 step 347: training accuarcy: 0.8055\n",
      "Epoch 1 step 347: training loss: 958.88465518413\n",
      "Epoch 1 step 348: training accuarcy: 0.797\n",
      "Epoch 1 step 348: training loss: 939.266019383642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 349: training accuarcy: 0.7975\n",
      "Epoch 1 step 349: training loss: 950.8717818009897\n",
      "Epoch 1 step 350: training accuarcy: 0.798\n",
      "Epoch 1 step 350: training loss: 959.7410874457715\n",
      "Epoch 1 step 351: training accuarcy: 0.8015\n",
      "Epoch 1 step 351: training loss: 939.7930656540661\n",
      "Epoch 1 step 352: training accuarcy: 0.7995\n",
      "Epoch 1 step 352: training loss: 928.928823646394\n",
      "Epoch 1 step 353: training accuarcy: 0.809\n",
      "Epoch 1 step 353: training loss: 940.3219438879934\n",
      "Epoch 1 step 354: training accuarcy: 0.809\n",
      "Epoch 1 step 354: training loss: 940.3015358129531\n",
      "Epoch 1 step 355: training accuarcy: 0.8025\n",
      "Epoch 1 step 355: training loss: 968.1069796320263\n",
      "Epoch 1 step 356: training accuarcy: 0.788\n",
      "Epoch 1 step 356: training loss: 937.4825802975965\n",
      "Epoch 1 step 357: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 357: training loss: 955.3890597764367\n",
      "Epoch 1 step 358: training accuarcy: 0.7975\n",
      "Epoch 1 step 358: training loss: 944.1715121254935\n",
      "Epoch 1 step 359: training accuarcy: 0.808\n",
      "Epoch 1 step 359: training loss: 933.8347758888716\n",
      "Epoch 1 step 360: training accuarcy: 0.804\n",
      "Epoch 1 step 360: training loss: 939.6722865493296\n",
      "Epoch 1 step 361: training accuarcy: 0.8185\n",
      "Epoch 1 step 361: training loss: 951.0383936829347\n",
      "Epoch 1 step 362: training accuarcy: 0.7955\n",
      "Epoch 1 step 362: training loss: 956.2790038183771\n",
      "Epoch 1 step 363: training accuarcy: 0.798\n",
      "Epoch 1 step 363: training loss: 937.7298053035425\n",
      "Epoch 1 step 364: training accuarcy: 0.8\n",
      "Epoch 1 step 364: training loss: 950.7118929488483\n",
      "Epoch 1 step 365: training accuarcy: 0.794\n",
      "Epoch 1 step 365: training loss: 941.7862002485521\n",
      "Epoch 1 step 366: training accuarcy: 0.805\n",
      "Epoch 1 step 366: training loss: 951.347708758121\n",
      "Epoch 1 step 367: training accuarcy: 0.7925\n",
      "Epoch 1 step 367: training loss: 935.434140683991\n",
      "Epoch 1 step 368: training accuarcy: 0.8035\n",
      "Epoch 1 step 368: training loss: 951.0877090468025\n",
      "Epoch 1 step 369: training accuarcy: 0.8015\n",
      "Epoch 1 step 369: training loss: 921.8178164790243\n",
      "Epoch 1 step 370: training accuarcy: 0.8095\n",
      "Epoch 1 step 370: training loss: 970.6076282343855\n",
      "Epoch 1 step 371: training accuarcy: 0.7875\n",
      "Epoch 1 step 371: training loss: 953.8891183334576\n",
      "Epoch 1 step 372: training accuarcy: 0.7945\n",
      "Epoch 1 step 372: training loss: 942.9652281278939\n",
      "Epoch 1 step 373: training accuarcy: 0.804\n",
      "Epoch 1 step 373: training loss: 936.6591442353787\n",
      "Epoch 1 step 374: training accuarcy: 0.8025\n",
      "Epoch 1 step 374: training loss: 936.2162562731104\n",
      "Epoch 1 step 375: training accuarcy: 0.8065\n",
      "Epoch 1 step 375: training loss: 960.5325249280536\n",
      "Epoch 1 step 376: training accuarcy: 0.7905\n",
      "Epoch 1 step 376: training loss: 914.1658470667687\n",
      "Epoch 1 step 377: training accuarcy: 0.8210000000000001\n",
      "Epoch 1 step 377: training loss: 952.869497642497\n",
      "Epoch 1 step 378: training accuarcy: 0.788\n",
      "Epoch 1 step 378: training loss: 936.3909806273913\n",
      "Epoch 1 step 379: training accuarcy: 0.8\n",
      "Epoch 1 step 379: training loss: 935.9300595345738\n",
      "Epoch 1 step 380: training accuarcy: 0.8140000000000001\n",
      "Epoch 1 step 380: training loss: 912.5004421218201\n",
      "Epoch 1 step 381: training accuarcy: 0.8065\n",
      "Epoch 1 step 381: training loss: 925.4782541803469\n",
      "Epoch 1 step 382: training accuarcy: 0.8025\n",
      "Epoch 1 step 382: training loss: 942.2457101190175\n",
      "Epoch 1 step 383: training accuarcy: 0.7965\n",
      "Epoch 1 step 383: training loss: 932.8718085021969\n",
      "Epoch 1 step 384: training accuarcy: 0.808\n",
      "Epoch 1 step 384: training loss: 942.30310038748\n",
      "Epoch 1 step 385: training accuarcy: 0.8035\n",
      "Epoch 1 step 385: training loss: 951.3529421956471\n",
      "Epoch 1 step 386: training accuarcy: 0.8045\n",
      "Epoch 1 step 386: training loss: 937.6604256704239\n",
      "Epoch 1 step 387: training accuarcy: 0.803\n",
      "Epoch 1 step 387: training loss: 925.1137362112374\n",
      "Epoch 1 step 388: training accuarcy: 0.809\n",
      "Epoch 1 step 388: training loss: 912.6404746063693\n",
      "Epoch 1 step 389: training accuarcy: 0.8035\n",
      "Epoch 1 step 389: training loss: 937.9285439075761\n",
      "Epoch 1 step 390: training accuarcy: 0.807\n",
      "Epoch 1 step 390: training loss: 921.35391197255\n",
      "Epoch 1 step 391: training accuarcy: 0.8135\n",
      "Epoch 1 step 391: training loss: 949.9008014535598\n",
      "Epoch 1 step 392: training accuarcy: 0.7945\n",
      "Epoch 1 step 392: training loss: 917.2238450169878\n",
      "Epoch 1 step 393: training accuarcy: 0.8015\n",
      "Epoch 1 step 393: training loss: 916.2184092415521\n",
      "Epoch 1 step 394: training accuarcy: 0.8085\n",
      "Epoch 1 step 394: training loss: 937.8858734511876\n",
      "Epoch 1 step 395: training accuarcy: 0.8005\n",
      "Epoch 1 step 395: training loss: 941.6326896315127\n",
      "Epoch 1 step 396: training accuarcy: 0.799\n",
      "Epoch 1 step 396: training loss: 934.7199448412409\n",
      "Epoch 1 step 397: training accuarcy: 0.7995\n",
      "Epoch 1 step 397: training loss: 896.5518311030104\n",
      "Epoch 1 step 398: training accuarcy: 0.8205\n",
      "Epoch 1 step 398: training loss: 945.6226002953235\n",
      "Epoch 1 step 399: training accuarcy: 0.7935\n",
      "Epoch 1 step 399: training loss: 920.882861414484\n",
      "Epoch 1 step 400: training accuarcy: 0.8085\n",
      "Epoch 1 step 400: training loss: 914.9200032784006\n",
      "Epoch 1 step 401: training accuarcy: 0.811\n",
      "Epoch 1 step 401: training loss: 910.2487511471386\n",
      "Epoch 1 step 402: training accuarcy: 0.8095\n",
      "Epoch 1 step 402: training loss: 919.1600305589118\n",
      "Epoch 1 step 403: training accuarcy: 0.8175\n",
      "Epoch 1 step 403: training loss: 937.5576225344529\n",
      "Epoch 1 step 404: training accuarcy: 0.7955\n",
      "Epoch 1 step 404: training loss: 919.2465895280275\n",
      "Epoch 1 step 405: training accuarcy: 0.8125\n",
      "Epoch 1 step 405: training loss: 921.6093277159958\n",
      "Epoch 1 step 406: training accuarcy: 0.8045\n",
      "Epoch 1 step 406: training loss: 937.8979919583327\n",
      "Epoch 1 step 407: training accuarcy: 0.807\n",
      "Epoch 1 step 407: training loss: 921.9845183353134\n",
      "Epoch 1 step 408: training accuarcy: 0.804\n",
      "Epoch 1 step 408: training loss: 947.904140442438\n",
      "Epoch 1 step 409: training accuarcy: 0.7955\n",
      "Epoch 1 step 409: training loss: 912.9436093734262\n",
      "Epoch 1 step 410: training accuarcy: 0.8115\n",
      "Epoch 1 step 410: training loss: 911.3383235941433\n",
      "Epoch 1 step 411: training accuarcy: 0.8125\n",
      "Epoch 1 step 411: training loss: 936.4861990949207\n",
      "Epoch 1 step 412: training accuarcy: 0.7975\n",
      "Epoch 1 step 412: training loss: 933.0653903369282\n",
      "Epoch 1 step 413: training accuarcy: 0.809\n",
      "Epoch 1 step 413: training loss: 915.9646951154637\n",
      "Epoch 1 step 414: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 414: training loss: 924.8916070634398\n",
      "Epoch 1 step 415: training accuarcy: 0.8150000000000001\n",
      "Epoch 1 step 415: training loss: 908.9119101488735\n",
      "Epoch 1 step 416: training accuarcy: 0.8145\n",
      "Epoch 1 step 416: training loss: 922.4731873765932\n",
      "Epoch 1 step 417: training accuarcy: 0.8005\n",
      "Epoch 1 step 417: training loss: 913.8617550076947\n",
      "Epoch 1 step 418: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 418: training loss: 906.8866662111049\n",
      "Epoch 1 step 419: training accuarcy: 0.8165\n",
      "Epoch 1 step 419: training loss: 922.1091455786659\n",
      "Epoch 1 step 420: training accuarcy: 0.799\n",
      "Epoch 1 step 420: training loss: 945.959534165528\n",
      "Epoch 1 step 421: training accuarcy: 0.804\n",
      "Epoch 1 step 421: training loss: 888.5537389367439\n",
      "Epoch 1 step 422: training accuarcy: 0.8150000000000001\n",
      "Epoch 1 step 422: training loss: 894.5591547413176\n",
      "Epoch 1 step 423: training accuarcy: 0.8220000000000001\n",
      "Epoch 1 step 423: training loss: 918.6947448245932\n",
      "Epoch 1 step 424: training accuarcy: 0.809\n",
      "Epoch 1 step 424: training loss: 872.4844575083742\n",
      "Epoch 1 step 425: training accuarcy: 0.8365\n",
      "Epoch 1 step 425: training loss: 924.3589328152015\n",
      "Epoch 1 step 426: training accuarcy: 0.802\n",
      "Epoch 1 step 426: training loss: 939.2774772256321\n",
      "Epoch 1 step 427: training accuarcy: 0.7945\n",
      "Epoch 1 step 427: training loss: 915.3645851679022\n",
      "Epoch 1 step 428: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 428: training loss: 904.3285171995814\n",
      "Epoch 1 step 429: training accuarcy: 0.8175\n",
      "Epoch 1 step 429: training loss: 907.0549746289603\n",
      "Epoch 1 step 430: training accuarcy: 0.8185\n",
      "Epoch 1 step 430: training loss: 926.1696315488589\n",
      "Epoch 1 step 431: training accuarcy: 0.804\n",
      "Epoch 1 step 431: training loss: 921.9289293093663\n",
      "Epoch 1 step 432: training accuarcy: 0.7965\n",
      "Epoch 1 step 432: training loss: 940.7365518159972\n",
      "Epoch 1 step 433: training accuarcy: 0.801\n",
      "Epoch 1 step 433: training loss: 894.5801148440013\n",
      "Epoch 1 step 434: training accuarcy: 0.8255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 434: training loss: 915.6342369499238\n",
      "Epoch 1 step 435: training accuarcy: 0.805\n",
      "Epoch 1 step 435: training loss: 885.4818930078619\n",
      "Epoch 1 step 436: training accuarcy: 0.8295\n",
      "Epoch 1 step 436: training loss: 921.8968032519256\n",
      "Epoch 1 step 437: training accuarcy: 0.8\n",
      "Epoch 1 step 437: training loss: 926.9152894747033\n",
      "Epoch 1 step 438: training accuarcy: 0.8055\n",
      "Epoch 1 step 438: training loss: 880.693478972882\n",
      "Epoch 1 step 439: training accuarcy: 0.8240000000000001\n",
      "Epoch 1 step 439: training loss: 870.7242582027243\n",
      "Epoch 1 step 440: training accuarcy: 0.8300000000000001\n",
      "Epoch 1 step 440: training loss: 932.6479720911238\n",
      "Epoch 1 step 441: training accuarcy: 0.805\n",
      "Epoch 1 step 441: training loss: 930.8609398827883\n",
      "Epoch 1 step 442: training accuarcy: 0.79\n",
      "Epoch 1 step 442: training loss: 902.011950630336\n",
      "Epoch 1 step 443: training accuarcy: 0.8145\n",
      "Epoch 1 step 443: training loss: 910.8092236093823\n",
      "Epoch 1 step 444: training accuarcy: 0.8115\n",
      "Epoch 1 step 444: training loss: 907.5459113044874\n",
      "Epoch 1 step 445: training accuarcy: 0.806\n",
      "Epoch 1 step 445: training loss: 899.9013172340759\n",
      "Epoch 1 step 446: training accuarcy: 0.809\n",
      "Epoch 1 step 446: training loss: 935.4835202867666\n",
      "Epoch 1 step 447: training accuarcy: 0.781\n",
      "Epoch 1 step 447: training loss: 932.2388146065932\n",
      "Epoch 1 step 448: training accuarcy: 0.8015\n",
      "Epoch 1 step 448: training loss: 888.9967642474976\n",
      "Epoch 1 step 449: training accuarcy: 0.8215\n",
      "Epoch 1 step 449: training loss: 896.3712045654295\n",
      "Epoch 1 step 450: training accuarcy: 0.8115\n",
      "Epoch 1 step 450: training loss: 906.9516502291317\n",
      "Epoch 1 step 451: training accuarcy: 0.8085\n",
      "Epoch 1 step 451: training loss: 926.518875936674\n",
      "Epoch 1 step 452: training accuarcy: 0.8055\n",
      "Epoch 1 step 452: training loss: 896.1471669386129\n",
      "Epoch 1 step 453: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 453: training loss: 903.0236803883255\n",
      "Epoch 1 step 454: training accuarcy: 0.8135\n",
      "Epoch 1 step 454: training loss: 910.4570528890227\n",
      "Epoch 1 step 455: training accuarcy: 0.7985\n",
      "Epoch 1 step 455: training loss: 916.6905392721122\n",
      "Epoch 1 step 456: training accuarcy: 0.8\n",
      "Epoch 1 step 456: training loss: 888.644256799868\n",
      "Epoch 1 step 457: training accuarcy: 0.8230000000000001\n",
      "Epoch 1 step 457: training loss: 910.9373919543998\n",
      "Epoch 1 step 458: training accuarcy: 0.8095\n",
      "Epoch 1 step 458: training loss: 876.2955754677255\n",
      "Epoch 1 step 459: training accuarcy: 0.8215\n",
      "Epoch 1 step 459: training loss: 909.8011121376614\n",
      "Epoch 1 step 460: training accuarcy: 0.812\n",
      "Epoch 1 step 460: training loss: 923.3029049028219\n",
      "Epoch 1 step 461: training accuarcy: 0.8055\n",
      "Epoch 1 step 461: training loss: 901.0071309076233\n",
      "Epoch 1 step 462: training accuarcy: 0.8075\n",
      "Epoch 1 step 462: training loss: 891.658978157895\n",
      "Epoch 1 step 463: training accuarcy: 0.811\n",
      "Epoch 1 step 463: training loss: 902.6462428213697\n",
      "Epoch 1 step 464: training accuarcy: 0.8055\n",
      "Epoch 1 step 464: training loss: 918.2684055501819\n",
      "Epoch 1 step 465: training accuarcy: 0.799\n",
      "Epoch 1 step 465: training loss: 918.857511604727\n",
      "Epoch 1 step 466: training accuarcy: 0.805\n",
      "Epoch 1 step 466: training loss: 898.6284284122821\n",
      "Epoch 1 step 467: training accuarcy: 0.8215\n",
      "Epoch 1 step 467: training loss: 906.5723257740599\n",
      "Epoch 1 step 468: training accuarcy: 0.8045\n",
      "Epoch 1 step 468: training loss: 894.3112882000252\n",
      "Epoch 1 step 469: training accuarcy: 0.7995\n",
      "Epoch 1 step 469: training loss: 919.9069982971466\n",
      "Epoch 1 step 470: training accuarcy: 0.8025\n",
      "Epoch 1 step 470: training loss: 916.4603871298983\n",
      "Epoch 1 step 471: training accuarcy: 0.8035\n",
      "Epoch 1 step 471: training loss: 933.108476170444\n",
      "Epoch 1 step 472: training accuarcy: 0.795\n",
      "Epoch 1 step 472: training loss: 913.8912252410336\n",
      "Epoch 1 step 473: training accuarcy: 0.806\n",
      "Epoch 1 step 473: training loss: 874.8664263867635\n",
      "Epoch 1 step 474: training accuarcy: 0.8240000000000001\n",
      "Epoch 1 step 474: training loss: 885.7294628253601\n",
      "Epoch 1 step 475: training accuarcy: 0.8230000000000001\n",
      "Epoch 1 step 475: training loss: 909.1329010094801\n",
      "Epoch 1 step 476: training accuarcy: 0.8065\n",
      "Epoch 1 step 476: training loss: 857.2437984675829\n",
      "Epoch 1 step 477: training accuarcy: 0.8305\n",
      "Epoch 1 step 477: training loss: 880.8734379434\n",
      "Epoch 1 step 478: training accuarcy: 0.8145\n",
      "Epoch 1 step 478: training loss: 878.9144141103945\n",
      "Epoch 1 step 479: training accuarcy: 0.8190000000000001\n",
      "Epoch 1 step 479: training loss: 918.7785675059289\n",
      "Epoch 1 step 480: training accuarcy: 0.8025\n",
      "Epoch 1 step 480: training loss: 930.7052117829851\n",
      "Epoch 1 step 481: training accuarcy: 0.7985\n",
      "Epoch 1 step 481: training loss: 885.4559885385233\n",
      "Epoch 1 step 482: training accuarcy: 0.811\n",
      "Epoch 1 step 482: training loss: 889.7213023655793\n",
      "Epoch 1 step 483: training accuarcy: 0.8175\n",
      "Epoch 1 step 483: training loss: 885.7981052666208\n",
      "Epoch 1 step 484: training accuarcy: 0.8190000000000001\n",
      "Epoch 1 step 484: training loss: 922.0987496064316\n",
      "Epoch 1 step 485: training accuarcy: 0.803\n",
      "Epoch 1 step 485: training loss: 891.8345118585848\n",
      "Epoch 1 step 486: training accuarcy: 0.807\n",
      "Epoch 1 step 486: training loss: 897.2903981632684\n",
      "Epoch 1 step 487: training accuarcy: 0.8175\n",
      "Epoch 1 step 487: training loss: 892.8032927756367\n",
      "Epoch 1 step 488: training accuarcy: 0.8200000000000001\n",
      "Epoch 1 step 488: training loss: 866.3113012517231\n",
      "Epoch 1 step 489: training accuarcy: 0.8280000000000001\n",
      "Epoch 1 step 489: training loss: 902.5375705083869\n",
      "Epoch 1 step 490: training accuarcy: 0.8035\n",
      "Epoch 1 step 490: training loss: 872.6728417489616\n",
      "Epoch 1 step 491: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 491: training loss: 871.2843748879059\n",
      "Epoch 1 step 492: training accuarcy: 0.833\n",
      "Epoch 1 step 492: training loss: 901.0084990249843\n",
      "Epoch 1 step 493: training accuarcy: 0.8145\n",
      "Epoch 1 step 493: training loss: 849.7258926275658\n",
      "Epoch 1 step 494: training accuarcy: 0.8265\n",
      "Epoch 1 step 494: training loss: 895.5202434097649\n",
      "Epoch 1 step 495: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 495: training loss: 884.1703010475971\n",
      "Epoch 1 step 496: training accuarcy: 0.8220000000000001\n",
      "Epoch 1 step 496: training loss: 907.1518772986575\n",
      "Epoch 1 step 497: training accuarcy: 0.812\n",
      "Epoch 1 step 497: training loss: 872.2367188638381\n",
      "Epoch 1 step 498: training accuarcy: 0.8135\n",
      "Epoch 1 step 498: training loss: 889.5158741102001\n",
      "Epoch 1 step 499: training accuarcy: 0.8195\n",
      "Epoch 1 step 499: training loss: 909.3100718058804\n",
      "Epoch 1 step 500: training accuarcy: 0.793\n",
      "Epoch 1 step 500: training loss: 875.0622097471374\n",
      "Epoch 1 step 501: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 501: training loss: 870.1154296071131\n",
      "Epoch 1 step 502: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 502: training loss: 893.1824907903302\n",
      "Epoch 1 step 503: training accuarcy: 0.8140000000000001\n",
      "Epoch 1 step 503: training loss: 873.2346954779141\n",
      "Epoch 1 step 504: training accuarcy: 0.8235\n",
      "Epoch 1 step 504: training loss: 908.8416711714375\n",
      "Epoch 1 step 505: training accuarcy: 0.796\n",
      "Epoch 1 step 505: training loss: 888.7945036225116\n",
      "Epoch 1 step 506: training accuarcy: 0.8180000000000001\n",
      "Epoch 1 step 506: training loss: 888.8565525700604\n",
      "Epoch 1 step 507: training accuarcy: 0.8155\n",
      "Epoch 1 step 507: training loss: 896.7533669772008\n",
      "Epoch 1 step 508: training accuarcy: 0.808\n",
      "Epoch 1 step 508: training loss: 878.5022133234602\n",
      "Epoch 1 step 509: training accuarcy: 0.8240000000000001\n",
      "Epoch 1 step 509: training loss: 877.989763332877\n",
      "Epoch 1 step 510: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 510: training loss: 881.1878902106114\n",
      "Epoch 1 step 511: training accuarcy: 0.8190000000000001\n",
      "Epoch 1 step 511: training loss: 869.3232602165872\n",
      "Epoch 1 step 512: training accuarcy: 0.8165\n",
      "Epoch 1 step 512: training loss: 882.5952988343997\n",
      "Epoch 1 step 513: training accuarcy: 0.8180000000000001\n",
      "Epoch 1 step 513: training loss: 902.1990519491507\n",
      "Epoch 1 step 514: training accuarcy: 0.805\n",
      "Epoch 1 step 514: training loss: 883.1765129798016\n",
      "Epoch 1 step 515: training accuarcy: 0.8190000000000001\n",
      "Epoch 1 step 515: training loss: 897.7491943638788\n",
      "Epoch 1 step 516: training accuarcy: 0.812\n",
      "Epoch 1 step 516: training loss: 907.0049547776216\n",
      "Epoch 1 step 517: training accuarcy: 0.795\n",
      "Epoch 1 step 517: training loss: 885.0305855787253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 518: training accuarcy: 0.809\n",
      "Epoch 1 step 518: training loss: 847.6455863895205\n",
      "Epoch 1 step 519: training accuarcy: 0.834\n",
      "Epoch 1 step 519: training loss: 879.2882706613302\n",
      "Epoch 1 step 520: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 520: training loss: 881.0519361443079\n",
      "Epoch 1 step 521: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 521: training loss: 882.7376274444861\n",
      "Epoch 1 step 522: training accuarcy: 0.8290000000000001\n",
      "Epoch 1 step 522: training loss: 884.1038942775203\n",
      "Epoch 1 step 523: training accuarcy: 0.8095\n",
      "Epoch 1 step 523: training loss: 859.9500563503879\n",
      "Epoch 1 step 524: training accuarcy: 0.8230000000000001\n",
      "Epoch 1 step 524: training loss: 921.6668859603786\n",
      "Epoch 1 step 525: training accuarcy: 0.7945\n",
      "Epoch 1 step 525: training loss: 366.212572380409\n",
      "Epoch 1 step 526: training accuarcy: 0.808974358974359\n",
      "Epoch 1: train loss 926.5457459866907, train accuarcy 0.780495822429657\n",
      "Epoch 1: valid loss 1030.1578735028932, valid accuarcy 0.7507580518722534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████                                                                                                                  | 2/8 [04:27<13:23, 133.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 796.1800074425452\n",
      "Epoch 2 step 527: training accuarcy: 0.85\n",
      "Epoch 2 step 527: training loss: 812.4522747391758\n",
      "Epoch 2 step 528: training accuarcy: 0.847\n",
      "Epoch 2 step 528: training loss: 780.7816190859206\n",
      "Epoch 2 step 529: training accuarcy: 0.854\n",
      "Epoch 2 step 529: training loss: 776.5738854869907\n",
      "Epoch 2 step 530: training accuarcy: 0.8525\n",
      "Epoch 2 step 530: training loss: 820.261246135934\n",
      "Epoch 2 step 531: training accuarcy: 0.8415\n",
      "Epoch 2 step 531: training loss: 796.089065489707\n",
      "Epoch 2 step 532: training accuarcy: 0.8455\n",
      "Epoch 2 step 532: training loss: 807.46911121999\n",
      "Epoch 2 step 533: training accuarcy: 0.85\n",
      "Epoch 2 step 533: training loss: 804.490586968923\n",
      "Epoch 2 step 534: training accuarcy: 0.8535\n",
      "Epoch 2 step 534: training loss: 783.8217581882924\n",
      "Epoch 2 step 535: training accuarcy: 0.8655\n",
      "Epoch 2 step 535: training loss: 796.2572558360356\n",
      "Epoch 2 step 536: training accuarcy: 0.846\n",
      "Epoch 2 step 536: training loss: 797.5429647445313\n",
      "Epoch 2 step 537: training accuarcy: 0.852\n",
      "Epoch 2 step 537: training loss: 785.9269450117723\n",
      "Epoch 2 step 538: training accuarcy: 0.855\n",
      "Epoch 2 step 538: training loss: 777.1120275225402\n",
      "Epoch 2 step 539: training accuarcy: 0.8535\n",
      "Epoch 2 step 539: training loss: 765.9641825566367\n",
      "Epoch 2 step 540: training accuarcy: 0.868\n",
      "Epoch 2 step 540: training loss: 792.5543227951563\n",
      "Epoch 2 step 541: training accuarcy: 0.8575\n",
      "Epoch 2 step 541: training loss: 771.7797940969472\n",
      "Epoch 2 step 542: training accuarcy: 0.854\n",
      "Epoch 2 step 542: training loss: 779.7850392772558\n",
      "Epoch 2 step 543: training accuarcy: 0.8625\n",
      "Epoch 2 step 543: training loss: 807.4489979849913\n",
      "Epoch 2 step 544: training accuarcy: 0.8405\n",
      "Epoch 2 step 544: training loss: 781.2263413252878\n",
      "Epoch 2 step 545: training accuarcy: 0.863\n",
      "Epoch 2 step 545: training loss: 785.0842019010652\n",
      "Epoch 2 step 546: training accuarcy: 0.8525\n",
      "Epoch 2 step 546: training loss: 806.8972463462004\n",
      "Epoch 2 step 547: training accuarcy: 0.85\n",
      "Epoch 2 step 547: training loss: 789.6397449029377\n",
      "Epoch 2 step 548: training accuarcy: 0.847\n",
      "Epoch 2 step 548: training loss: 817.4891479424759\n",
      "Epoch 2 step 549: training accuarcy: 0.849\n",
      "Epoch 2 step 549: training loss: 806.1026027410845\n",
      "Epoch 2 step 550: training accuarcy: 0.8415\n",
      "Epoch 2 step 550: training loss: 824.2791145610278\n",
      "Epoch 2 step 551: training accuarcy: 0.836\n",
      "Epoch 2 step 551: training loss: 796.2750680470148\n",
      "Epoch 2 step 552: training accuarcy: 0.8505\n",
      "Epoch 2 step 552: training loss: 784.3591450339752\n",
      "Epoch 2 step 553: training accuarcy: 0.847\n",
      "Epoch 2 step 553: training loss: 778.5189321969697\n",
      "Epoch 2 step 554: training accuarcy: 0.869\n",
      "Epoch 2 step 554: training loss: 780.5581326098114\n",
      "Epoch 2 step 555: training accuarcy: 0.8595\n",
      "Epoch 2 step 555: training loss: 773.2501561867882\n",
      "Epoch 2 step 556: training accuarcy: 0.8615\n",
      "Epoch 2 step 556: training loss: 796.966390189815\n",
      "Epoch 2 step 557: training accuarcy: 0.85\n",
      "Epoch 2 step 557: training loss: 795.8470273280439\n",
      "Epoch 2 step 558: training accuarcy: 0.8335\n",
      "Epoch 2 step 558: training loss: 779.8341563612703\n",
      "Epoch 2 step 559: training accuarcy: 0.867\n",
      "Epoch 2 step 559: training loss: 814.8248293079744\n",
      "Epoch 2 step 560: training accuarcy: 0.8335\n",
      "Epoch 2 step 560: training loss: 798.6936999691505\n",
      "Epoch 2 step 561: training accuarcy: 0.8435\n",
      "Epoch 2 step 561: training loss: 803.0200896508421\n",
      "Epoch 2 step 562: training accuarcy: 0.8485\n",
      "Epoch 2 step 562: training loss: 776.9975872058865\n",
      "Epoch 2 step 563: training accuarcy: 0.859\n",
      "Epoch 2 step 563: training loss: 794.2429341944941\n",
      "Epoch 2 step 564: training accuarcy: 0.85\n",
      "Epoch 2 step 564: training loss: 816.5554855231147\n",
      "Epoch 2 step 565: training accuarcy: 0.842\n",
      "Epoch 2 step 565: training loss: 792.3792507684763\n",
      "Epoch 2 step 566: training accuarcy: 0.8485\n",
      "Epoch 2 step 566: training loss: 802.8030237370067\n",
      "Epoch 2 step 567: training accuarcy: 0.8435\n",
      "Epoch 2 step 567: training loss: 765.1355112632274\n",
      "Epoch 2 step 568: training accuarcy: 0.865\n",
      "Epoch 2 step 568: training loss: 795.8728498697774\n",
      "Epoch 2 step 569: training accuarcy: 0.847\n",
      "Epoch 2 step 569: training loss: 786.4804795969313\n",
      "Epoch 2 step 570: training accuarcy: 0.8525\n",
      "Epoch 2 step 570: training loss: 783.0475673099291\n",
      "Epoch 2 step 571: training accuarcy: 0.851\n",
      "Epoch 2 step 571: training loss: 777.4583192272395\n",
      "Epoch 2 step 572: training accuarcy: 0.8535\n",
      "Epoch 2 step 572: training loss: 804.7296217485965\n",
      "Epoch 2 step 573: training accuarcy: 0.839\n",
      "Epoch 2 step 573: training loss: 759.3682774132584\n",
      "Epoch 2 step 574: training accuarcy: 0.8605\n",
      "Epoch 2 step 574: training loss: 775.3414939414196\n",
      "Epoch 2 step 575: training accuarcy: 0.849\n",
      "Epoch 2 step 575: training loss: 798.8087537101989\n",
      "Epoch 2 step 576: training accuarcy: 0.841\n",
      "Epoch 2 step 576: training loss: 792.3110181076376\n",
      "Epoch 2 step 577: training accuarcy: 0.844\n",
      "Epoch 2 step 577: training loss: 781.6918829587345\n",
      "Epoch 2 step 578: training accuarcy: 0.856\n",
      "Epoch 2 step 578: training loss: 809.259773019416\n",
      "Epoch 2 step 579: training accuarcy: 0.8355\n",
      "Epoch 2 step 579: training loss: 782.229389121675\n",
      "Epoch 2 step 580: training accuarcy: 0.854\n",
      "Epoch 2 step 580: training loss: 803.3952065318404\n",
      "Epoch 2 step 581: training accuarcy: 0.846\n",
      "Epoch 2 step 581: training loss: 783.9860159672567\n",
      "Epoch 2 step 582: training accuarcy: 0.844\n",
      "Epoch 2 step 582: training loss: 802.3450089465293\n",
      "Epoch 2 step 583: training accuarcy: 0.8505\n",
      "Epoch 2 step 583: training loss: 777.0472496151967\n",
      "Epoch 2 step 584: training accuarcy: 0.858\n",
      "Epoch 2 step 584: training loss: 779.0114278335346\n",
      "Epoch 2 step 585: training accuarcy: 0.8495\n",
      "Epoch 2 step 585: training loss: 810.0261401462268\n",
      "Epoch 2 step 586: training accuarcy: 0.8425\n",
      "Epoch 2 step 586: training loss: 788.1766293224549\n",
      "Epoch 2 step 587: training accuarcy: 0.8475\n",
      "Epoch 2 step 587: training loss: 785.4503132102133\n",
      "Epoch 2 step 588: training accuarcy: 0.8485\n",
      "Epoch 2 step 588: training loss: 753.5111042541085\n",
      "Epoch 2 step 589: training accuarcy: 0.8645\n",
      "Epoch 2 step 589: training loss: 773.5931075631677\n",
      "Epoch 2 step 590: training accuarcy: 0.8515\n",
      "Epoch 2 step 590: training loss: 778.829069920378\n",
      "Epoch 2 step 591: training accuarcy: 0.848\n",
      "Epoch 2 step 591: training loss: 798.359446113896\n",
      "Epoch 2 step 592: training accuarcy: 0.8475\n",
      "Epoch 2 step 592: training loss: 775.0327734993836\n",
      "Epoch 2 step 593: training accuarcy: 0.8505\n",
      "Epoch 2 step 593: training loss: 777.4259914029711\n",
      "Epoch 2 step 594: training accuarcy: 0.8435\n",
      "Epoch 2 step 594: training loss: 786.5295927071585\n",
      "Epoch 2 step 595: training accuarcy: 0.847\n",
      "Epoch 2 step 595: training loss: 788.057773779904\n",
      "Epoch 2 step 596: training accuarcy: 0.854\n",
      "Epoch 2 step 596: training loss: 800.1674212347297\n",
      "Epoch 2 step 597: training accuarcy: 0.8415\n",
      "Epoch 2 step 597: training loss: 776.8951370046502\n",
      "Epoch 2 step 598: training accuarcy: 0.859\n",
      "Epoch 2 step 598: training loss: 777.4886086123083\n",
      "Epoch 2 step 599: training accuarcy: 0.8585\n",
      "Epoch 2 step 599: training loss: 810.5682967054661\n",
      "Epoch 2 step 600: training accuarcy: 0.8355\n",
      "Epoch 2 step 600: training loss: 774.3854857451448\n",
      "Epoch 2 step 601: training accuarcy: 0.856\n",
      "Epoch 2 step 601: training loss: 767.5171351513667\n",
      "Epoch 2 step 602: training accuarcy: 0.8645\n",
      "Epoch 2 step 602: training loss: 760.8901683672561\n",
      "Epoch 2 step 603: training accuarcy: 0.8595\n",
      "Epoch 2 step 603: training loss: 785.8663314723315\n",
      "Epoch 2 step 604: training accuarcy: 0.8535\n",
      "Epoch 2 step 604: training loss: 768.8507269270973\n",
      "Epoch 2 step 605: training accuarcy: 0.854\n",
      "Epoch 2 step 605: training loss: 771.812861281134\n",
      "Epoch 2 step 606: training accuarcy: 0.8495\n",
      "Epoch 2 step 606: training loss: 769.831779972039\n",
      "Epoch 2 step 607: training accuarcy: 0.857\n",
      "Epoch 2 step 607: training loss: 794.7295236064163\n",
      "Epoch 2 step 608: training accuarcy: 0.845\n",
      "Epoch 2 step 608: training loss: 772.40084148998\n",
      "Epoch 2 step 609: training accuarcy: 0.841\n",
      "Epoch 2 step 609: training loss: 787.4582924583556\n",
      "Epoch 2 step 610: training accuarcy: 0.847\n",
      "Epoch 2 step 610: training loss: 777.5521872534661\n",
      "Epoch 2 step 611: training accuarcy: 0.845\n",
      "Epoch 2 step 611: training loss: 755.0290466317182\n",
      "Epoch 2 step 612: training accuarcy: 0.8575\n",
      "Epoch 2 step 612: training loss: 784.1157688799981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 613: training accuarcy: 0.852\n",
      "Epoch 2 step 613: training loss: 774.1487747745001\n",
      "Epoch 2 step 614: training accuarcy: 0.859\n",
      "Epoch 2 step 614: training loss: 749.2056961632467\n",
      "Epoch 2 step 615: training accuarcy: 0.8595\n",
      "Epoch 2 step 615: training loss: 782.1607213728528\n",
      "Epoch 2 step 616: training accuarcy: 0.8575\n",
      "Epoch 2 step 616: training loss: 792.8569721290753\n",
      "Epoch 2 step 617: training accuarcy: 0.8465\n",
      "Epoch 2 step 617: training loss: 777.4729972415197\n",
      "Epoch 2 step 618: training accuarcy: 0.851\n",
      "Epoch 2 step 618: training loss: 760.3674300183445\n",
      "Epoch 2 step 619: training accuarcy: 0.859\n",
      "Epoch 2 step 619: training loss: 787.8744260398984\n",
      "Epoch 2 step 620: training accuarcy: 0.845\n",
      "Epoch 2 step 620: training loss: 774.7002469387361\n",
      "Epoch 2 step 621: training accuarcy: 0.8615\n",
      "Epoch 2 step 621: training loss: 779.4076274004615\n",
      "Epoch 2 step 622: training accuarcy: 0.8545\n",
      "Epoch 2 step 622: training loss: 776.9889290073825\n",
      "Epoch 2 step 623: training accuarcy: 0.8525\n",
      "Epoch 2 step 623: training loss: 776.0932673511003\n",
      "Epoch 2 step 624: training accuarcy: 0.854\n",
      "Epoch 2 step 624: training loss: 785.0107807753343\n",
      "Epoch 2 step 625: training accuarcy: 0.851\n",
      "Epoch 2 step 625: training loss: 780.975754353276\n",
      "Epoch 2 step 626: training accuarcy: 0.848\n",
      "Epoch 2 step 626: training loss: 775.8098861252827\n",
      "Epoch 2 step 627: training accuarcy: 0.8525\n",
      "Epoch 2 step 627: training loss: 761.3655655248332\n",
      "Epoch 2 step 628: training accuarcy: 0.862\n",
      "Epoch 2 step 628: training loss: 770.0572732243741\n",
      "Epoch 2 step 629: training accuarcy: 0.853\n",
      "Epoch 2 step 629: training loss: 778.2999160918622\n",
      "Epoch 2 step 630: training accuarcy: 0.854\n",
      "Epoch 2 step 630: training loss: 772.7535712361791\n",
      "Epoch 2 step 631: training accuarcy: 0.8525\n",
      "Epoch 2 step 631: training loss: 769.3868537490658\n",
      "Epoch 2 step 632: training accuarcy: 0.855\n",
      "Epoch 2 step 632: training loss: 771.6138539556387\n",
      "Epoch 2 step 633: training accuarcy: 0.856\n",
      "Epoch 2 step 633: training loss: 765.0664224499462\n",
      "Epoch 2 step 634: training accuarcy: 0.8615\n",
      "Epoch 2 step 634: training loss: 780.3875188627078\n",
      "Epoch 2 step 635: training accuarcy: 0.8495\n",
      "Epoch 2 step 635: training loss: 780.7319629310111\n",
      "Epoch 2 step 636: training accuarcy: 0.8555\n",
      "Epoch 2 step 636: training loss: 782.7540875841175\n",
      "Epoch 2 step 637: training accuarcy: 0.8525\n",
      "Epoch 2 step 637: training loss: 787.976686306184\n",
      "Epoch 2 step 638: training accuarcy: 0.849\n",
      "Epoch 2 step 638: training loss: 759.6975334118107\n",
      "Epoch 2 step 639: training accuarcy: 0.854\n",
      "Epoch 2 step 639: training loss: 761.6121343558058\n",
      "Epoch 2 step 640: training accuarcy: 0.865\n",
      "Epoch 2 step 640: training loss: 760.9241653678525\n",
      "Epoch 2 step 641: training accuarcy: 0.858\n",
      "Epoch 2 step 641: training loss: 763.568208564113\n",
      "Epoch 2 step 642: training accuarcy: 0.8475\n",
      "Epoch 2 step 642: training loss: 739.5957405528545\n",
      "Epoch 2 step 643: training accuarcy: 0.869\n",
      "Epoch 2 step 643: training loss: 789.1622131039193\n",
      "Epoch 2 step 644: training accuarcy: 0.85\n",
      "Epoch 2 step 644: training loss: 764.8013934267304\n",
      "Epoch 2 step 645: training accuarcy: 0.8505\n",
      "Epoch 2 step 645: training loss: 779.3581441849065\n",
      "Epoch 2 step 646: training accuarcy: 0.8465\n",
      "Epoch 2 step 646: training loss: 795.4815743276704\n",
      "Epoch 2 step 647: training accuarcy: 0.8495\n",
      "Epoch 2 step 647: training loss: 743.9794967730041\n",
      "Epoch 2 step 648: training accuarcy: 0.872\n",
      "Epoch 2 step 648: training loss: 774.6925177390859\n",
      "Epoch 2 step 649: training accuarcy: 0.8445\n",
      "Epoch 2 step 649: training loss: 771.5479087456979\n",
      "Epoch 2 step 650: training accuarcy: 0.857\n",
      "Epoch 2 step 650: training loss: 764.1144426749715\n",
      "Epoch 2 step 651: training accuarcy: 0.8565\n",
      "Epoch 2 step 651: training loss: 781.0216742555757\n",
      "Epoch 2 step 652: training accuarcy: 0.856\n",
      "Epoch 2 step 652: training loss: 786.676372529573\n",
      "Epoch 2 step 653: training accuarcy: 0.8475\n",
      "Epoch 2 step 653: training loss: 769.0049395151542\n",
      "Epoch 2 step 654: training accuarcy: 0.854\n",
      "Epoch 2 step 654: training loss: 783.6552562304842\n",
      "Epoch 2 step 655: training accuarcy: 0.856\n",
      "Epoch 2 step 655: training loss: 753.9610330910269\n",
      "Epoch 2 step 656: training accuarcy: 0.8645\n",
      "Epoch 2 step 656: training loss: 761.3682643799087\n",
      "Epoch 2 step 657: training accuarcy: 0.8575\n",
      "Epoch 2 step 657: training loss: 743.9377425817737\n",
      "Epoch 2 step 658: training accuarcy: 0.866\n",
      "Epoch 2 step 658: training loss: 745.9318550898979\n",
      "Epoch 2 step 659: training accuarcy: 0.8695\n",
      "Epoch 2 step 659: training loss: 768.9552388290748\n",
      "Epoch 2 step 660: training accuarcy: 0.858\n",
      "Epoch 2 step 660: training loss: 759.2576830308204\n",
      "Epoch 2 step 661: training accuarcy: 0.8595\n",
      "Epoch 2 step 661: training loss: 751.0162538979622\n",
      "Epoch 2 step 662: training accuarcy: 0.8615\n",
      "Epoch 2 step 662: training loss: 762.1051088166209\n",
      "Epoch 2 step 663: training accuarcy: 0.8525\n",
      "Epoch 2 step 663: training loss: 786.0133588413612\n",
      "Epoch 2 step 664: training accuarcy: 0.854\n",
      "Epoch 2 step 664: training loss: 756.8745579189905\n",
      "Epoch 2 step 665: training accuarcy: 0.855\n",
      "Epoch 2 step 665: training loss: 770.9939005549421\n",
      "Epoch 2 step 666: training accuarcy: 0.856\n",
      "Epoch 2 step 666: training loss: 746.0445930610174\n",
      "Epoch 2 step 667: training accuarcy: 0.8695\n",
      "Epoch 2 step 667: training loss: 734.9015563415734\n",
      "Epoch 2 step 668: training accuarcy: 0.8585\n",
      "Epoch 2 step 668: training loss: 768.0104518835254\n",
      "Epoch 2 step 669: training accuarcy: 0.8495\n",
      "Epoch 2 step 669: training loss: 768.7262361666657\n",
      "Epoch 2 step 670: training accuarcy: 0.845\n",
      "Epoch 2 step 670: training loss: 759.9097026431725\n",
      "Epoch 2 step 671: training accuarcy: 0.8605\n",
      "Epoch 2 step 671: training loss: 771.7165326183127\n",
      "Epoch 2 step 672: training accuarcy: 0.854\n",
      "Epoch 2 step 672: training loss: 767.9320838988161\n",
      "Epoch 2 step 673: training accuarcy: 0.8535\n",
      "Epoch 2 step 673: training loss: 754.1070105628121\n",
      "Epoch 2 step 674: training accuarcy: 0.8575\n",
      "Epoch 2 step 674: training loss: 746.0823090282474\n",
      "Epoch 2 step 675: training accuarcy: 0.8665\n",
      "Epoch 2 step 675: training loss: 776.3511752785063\n",
      "Epoch 2 step 676: training accuarcy: 0.851\n",
      "Epoch 2 step 676: training loss: 755.0862092999873\n",
      "Epoch 2 step 677: training accuarcy: 0.86\n",
      "Epoch 2 step 677: training loss: 761.239720387776\n",
      "Epoch 2 step 678: training accuarcy: 0.847\n",
      "Epoch 2 step 678: training loss: 728.6638007863052\n",
      "Epoch 2 step 679: training accuarcy: 0.865\n",
      "Epoch 2 step 679: training loss: 769.9685451715546\n",
      "Epoch 2 step 680: training accuarcy: 0.8515\n",
      "Epoch 2 step 680: training loss: 773.2159675184118\n",
      "Epoch 2 step 681: training accuarcy: 0.8505\n",
      "Epoch 2 step 681: training loss: 757.4394793083762\n",
      "Epoch 2 step 682: training accuarcy: 0.863\n",
      "Epoch 2 step 682: training loss: 726.3250747838565\n",
      "Epoch 2 step 683: training accuarcy: 0.867\n",
      "Epoch 2 step 683: training loss: 730.7210961761962\n",
      "Epoch 2 step 684: training accuarcy: 0.8665\n",
      "Epoch 2 step 684: training loss: 745.9295965504441\n",
      "Epoch 2 step 685: training accuarcy: 0.865\n",
      "Epoch 2 step 685: training loss: 780.9295685746807\n",
      "Epoch 2 step 686: training accuarcy: 0.844\n",
      "Epoch 2 step 686: training loss: 742.7374691922289\n",
      "Epoch 2 step 687: training accuarcy: 0.865\n",
      "Epoch 2 step 687: training loss: 762.2696966759561\n",
      "Epoch 2 step 688: training accuarcy: 0.863\n",
      "Epoch 2 step 688: training loss: 769.6379545550515\n",
      "Epoch 2 step 689: training accuarcy: 0.8505\n",
      "Epoch 2 step 689: training loss: 755.3367875041054\n",
      "Epoch 2 step 690: training accuarcy: 0.8565\n",
      "Epoch 2 step 690: training loss: 769.0627680952451\n",
      "Epoch 2 step 691: training accuarcy: 0.845\n",
      "Epoch 2 step 691: training loss: 776.2723836262605\n",
      "Epoch 2 step 692: training accuarcy: 0.8405\n",
      "Epoch 2 step 692: training loss: 754.9158066905314\n",
      "Epoch 2 step 693: training accuarcy: 0.8665\n",
      "Epoch 2 step 693: training loss: 736.5276547988788\n",
      "Epoch 2 step 694: training accuarcy: 0.8675\n",
      "Epoch 2 step 694: training loss: 752.15397998294\n",
      "Epoch 2 step 695: training accuarcy: 0.8595\n",
      "Epoch 2 step 695: training loss: 776.0422431623896\n",
      "Epoch 2 step 696: training accuarcy: 0.8525\n",
      "Epoch 2 step 696: training loss: 731.1373060755727\n",
      "Epoch 2 step 697: training accuarcy: 0.8675\n",
      "Epoch 2 step 697: training loss: 755.672278102905\n",
      "Epoch 2 step 698: training accuarcy: 0.8555\n",
      "Epoch 2 step 698: training loss: 738.0780874973877\n",
      "Epoch 2 step 699: training accuarcy: 0.8635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 699: training loss: 743.8939994949307\n",
      "Epoch 2 step 700: training accuarcy: 0.859\n",
      "Epoch 2 step 700: training loss: 767.8181413563457\n",
      "Epoch 2 step 701: training accuarcy: 0.845\n",
      "Epoch 2 step 701: training loss: 756.4298216116127\n",
      "Epoch 2 step 702: training accuarcy: 0.853\n",
      "Epoch 2 step 702: training loss: 770.4160470194092\n",
      "Epoch 2 step 703: training accuarcy: 0.8555\n",
      "Epoch 2 step 703: training loss: 743.8601747379605\n",
      "Epoch 2 step 704: training accuarcy: 0.8615\n",
      "Epoch 2 step 704: training loss: 738.7535914079012\n",
      "Epoch 2 step 705: training accuarcy: 0.8685\n",
      "Epoch 2 step 705: training loss: 744.9842776048594\n",
      "Epoch 2 step 706: training accuarcy: 0.8625\n",
      "Epoch 2 step 706: training loss: 739.8717323576851\n",
      "Epoch 2 step 707: training accuarcy: 0.8685\n",
      "Epoch 2 step 707: training loss: 731.1224802882629\n",
      "Epoch 2 step 708: training accuarcy: 0.8585\n",
      "Epoch 2 step 708: training loss: 743.7190371733068\n",
      "Epoch 2 step 709: training accuarcy: 0.852\n",
      "Epoch 2 step 709: training loss: 747.0002384990096\n",
      "Epoch 2 step 710: training accuarcy: 0.859\n",
      "Epoch 2 step 710: training loss: 748.294675320831\n",
      "Epoch 2 step 711: training accuarcy: 0.858\n",
      "Epoch 2 step 711: training loss: 744.4939607846619\n",
      "Epoch 2 step 712: training accuarcy: 0.856\n",
      "Epoch 2 step 712: training loss: 778.8816227379613\n",
      "Epoch 2 step 713: training accuarcy: 0.845\n",
      "Epoch 2 step 713: training loss: 772.1955057683982\n",
      "Epoch 2 step 714: training accuarcy: 0.851\n",
      "Epoch 2 step 714: training loss: 720.3522673125993\n",
      "Epoch 2 step 715: training accuarcy: 0.874\n",
      "Epoch 2 step 715: training loss: 766.7374839044995\n",
      "Epoch 2 step 716: training accuarcy: 0.8505\n",
      "Epoch 2 step 716: training loss: 781.6398505607975\n",
      "Epoch 2 step 717: training accuarcy: 0.839\n",
      "Epoch 2 step 717: training loss: 753.8787228258934\n",
      "Epoch 2 step 718: training accuarcy: 0.856\n",
      "Epoch 2 step 718: training loss: 756.5417712310484\n",
      "Epoch 2 step 719: training accuarcy: 0.8625\n",
      "Epoch 2 step 719: training loss: 743.6211990968493\n",
      "Epoch 2 step 720: training accuarcy: 0.8645\n",
      "Epoch 2 step 720: training loss: 766.0375005627575\n",
      "Epoch 2 step 721: training accuarcy: 0.858\n",
      "Epoch 2 step 721: training loss: 727.17240834098\n",
      "Epoch 2 step 722: training accuarcy: 0.8745\n",
      "Epoch 2 step 722: training loss: 739.6107849459924\n",
      "Epoch 2 step 723: training accuarcy: 0.8635\n",
      "Epoch 2 step 723: training loss: 743.2353859847403\n",
      "Epoch 2 step 724: training accuarcy: 0.8635\n",
      "Epoch 2 step 724: training loss: 741.0599474673826\n",
      "Epoch 2 step 725: training accuarcy: 0.8695\n",
      "Epoch 2 step 725: training loss: 755.7625036536975\n",
      "Epoch 2 step 726: training accuarcy: 0.8605\n",
      "Epoch 2 step 726: training loss: 743.795561573342\n",
      "Epoch 2 step 727: training accuarcy: 0.8585\n",
      "Epoch 2 step 727: training loss: 703.9519240225426\n",
      "Epoch 2 step 728: training accuarcy: 0.877\n",
      "Epoch 2 step 728: training loss: 763.6234866267303\n",
      "Epoch 2 step 729: training accuarcy: 0.857\n",
      "Epoch 2 step 729: training loss: 747.2374650007444\n",
      "Epoch 2 step 730: training accuarcy: 0.8595\n",
      "Epoch 2 step 730: training loss: 721.1882187360585\n",
      "Epoch 2 step 731: training accuarcy: 0.873\n",
      "Epoch 2 step 731: training loss: 714.3015299095359\n",
      "Epoch 2 step 732: training accuarcy: 0.881\n",
      "Epoch 2 step 732: training loss: 758.2395626960665\n",
      "Epoch 2 step 733: training accuarcy: 0.849\n",
      "Epoch 2 step 733: training loss: 732.9036383942816\n",
      "Epoch 2 step 734: training accuarcy: 0.8695\n",
      "Epoch 2 step 734: training loss: 749.9274516733692\n",
      "Epoch 2 step 735: training accuarcy: 0.857\n",
      "Epoch 2 step 735: training loss: 726.6082130647709\n",
      "Epoch 2 step 736: training accuarcy: 0.8705\n",
      "Epoch 2 step 736: training loss: 764.442533394659\n",
      "Epoch 2 step 737: training accuarcy: 0.85\n",
      "Epoch 2 step 737: training loss: 725.0888468231249\n",
      "Epoch 2 step 738: training accuarcy: 0.868\n",
      "Epoch 2 step 738: training loss: 759.2056667610639\n",
      "Epoch 2 step 739: training accuarcy: 0.8585\n",
      "Epoch 2 step 739: training loss: 750.6478810998636\n",
      "Epoch 2 step 740: training accuarcy: 0.855\n",
      "Epoch 2 step 740: training loss: 751.1540865993941\n",
      "Epoch 2 step 741: training accuarcy: 0.8615\n",
      "Epoch 2 step 741: training loss: 752.9432339050387\n",
      "Epoch 2 step 742: training accuarcy: 0.8695\n",
      "Epoch 2 step 742: training loss: 729.5136364054541\n",
      "Epoch 2 step 743: training accuarcy: 0.8675\n",
      "Epoch 2 step 743: training loss: 743.3728313632253\n",
      "Epoch 2 step 744: training accuarcy: 0.854\n",
      "Epoch 2 step 744: training loss: 734.5572913104745\n",
      "Epoch 2 step 745: training accuarcy: 0.8625\n",
      "Epoch 2 step 745: training loss: 765.740091376465\n",
      "Epoch 2 step 746: training accuarcy: 0.8535\n",
      "Epoch 2 step 746: training loss: 750.351437346983\n",
      "Epoch 2 step 747: training accuarcy: 0.861\n",
      "Epoch 2 step 747: training loss: 724.0932876294044\n",
      "Epoch 2 step 748: training accuarcy: 0.8675\n",
      "Epoch 2 step 748: training loss: 749.4197439398228\n",
      "Epoch 2 step 749: training accuarcy: 0.853\n",
      "Epoch 2 step 749: training loss: 779.1412835889441\n",
      "Epoch 2 step 750: training accuarcy: 0.8455\n",
      "Epoch 2 step 750: training loss: 761.9957352863408\n",
      "Epoch 2 step 751: training accuarcy: 0.8525\n",
      "Epoch 2 step 751: training loss: 734.0107120510773\n",
      "Epoch 2 step 752: training accuarcy: 0.8625\n",
      "Epoch 2 step 752: training loss: 720.8442807286928\n",
      "Epoch 2 step 753: training accuarcy: 0.8705\n",
      "Epoch 2 step 753: training loss: 761.5228387718412\n",
      "Epoch 2 step 754: training accuarcy: 0.854\n",
      "Epoch 2 step 754: training loss: 749.6025699534661\n",
      "Epoch 2 step 755: training accuarcy: 0.852\n",
      "Epoch 2 step 755: training loss: 751.5843721464942\n",
      "Epoch 2 step 756: training accuarcy: 0.856\n",
      "Epoch 2 step 756: training loss: 721.2138039301998\n",
      "Epoch 2 step 757: training accuarcy: 0.8705\n",
      "Epoch 2 step 757: training loss: 735.3401843551148\n",
      "Epoch 2 step 758: training accuarcy: 0.859\n",
      "Epoch 2 step 758: training loss: 738.8078575248521\n",
      "Epoch 2 step 759: training accuarcy: 0.8535\n",
      "Epoch 2 step 759: training loss: 706.5600499310553\n",
      "Epoch 2 step 760: training accuarcy: 0.874\n",
      "Epoch 2 step 760: training loss: 729.2526697820941\n",
      "Epoch 2 step 761: training accuarcy: 0.866\n",
      "Epoch 2 step 761: training loss: 767.9226867603619\n",
      "Epoch 2 step 762: training accuarcy: 0.8545\n",
      "Epoch 2 step 762: training loss: 716.342706516616\n",
      "Epoch 2 step 763: training accuarcy: 0.8705\n",
      "Epoch 2 step 763: training loss: 744.613310420205\n",
      "Epoch 2 step 764: training accuarcy: 0.8625\n",
      "Epoch 2 step 764: training loss: 737.2714397729812\n",
      "Epoch 2 step 765: training accuarcy: 0.8605\n",
      "Epoch 2 step 765: training loss: 741.2447091633207\n",
      "Epoch 2 step 766: training accuarcy: 0.8675\n",
      "Epoch 2 step 766: training loss: 741.7135623904335\n",
      "Epoch 2 step 767: training accuarcy: 0.8615\n",
      "Epoch 2 step 767: training loss: 735.1288018654847\n",
      "Epoch 2 step 768: training accuarcy: 0.8625\n",
      "Epoch 2 step 768: training loss: 739.5984370721707\n",
      "Epoch 2 step 769: training accuarcy: 0.863\n",
      "Epoch 2 step 769: training loss: 724.6353805252223\n",
      "Epoch 2 step 770: training accuarcy: 0.863\n",
      "Epoch 2 step 770: training loss: 719.2229209049711\n",
      "Epoch 2 step 771: training accuarcy: 0.8695\n",
      "Epoch 2 step 771: training loss: 732.4137822190867\n",
      "Epoch 2 step 772: training accuarcy: 0.8625\n",
      "Epoch 2 step 772: training loss: 740.1639552903606\n",
      "Epoch 2 step 773: training accuarcy: 0.86\n",
      "Epoch 2 step 773: training loss: 699.8642412052849\n",
      "Epoch 2 step 774: training accuarcy: 0.8725\n",
      "Epoch 2 step 774: training loss: 728.3033732659502\n",
      "Epoch 2 step 775: training accuarcy: 0.86\n",
      "Epoch 2 step 775: training loss: 710.4161075760886\n",
      "Epoch 2 step 776: training accuarcy: 0.883\n",
      "Epoch 2 step 776: training loss: 709.2309252109319\n",
      "Epoch 2 step 777: training accuarcy: 0.8705\n",
      "Epoch 2 step 777: training loss: 716.419184550129\n",
      "Epoch 2 step 778: training accuarcy: 0.872\n",
      "Epoch 2 step 778: training loss: 716.2584458908352\n",
      "Epoch 2 step 779: training accuarcy: 0.871\n",
      "Epoch 2 step 779: training loss: 719.777804180573\n",
      "Epoch 2 step 780: training accuarcy: 0.867\n",
      "Epoch 2 step 780: training loss: 736.2886871131082\n",
      "Epoch 2 step 781: training accuarcy: 0.853\n",
      "Epoch 2 step 781: training loss: 751.1175293195354\n",
      "Epoch 2 step 782: training accuarcy: 0.8585\n",
      "Epoch 2 step 782: training loss: 701.1516736168925\n",
      "Epoch 2 step 783: training accuarcy: 0.885\n",
      "Epoch 2 step 783: training loss: 706.7191819808181\n",
      "Epoch 2 step 784: training accuarcy: 0.8765000000000001\n",
      "Epoch 2 step 784: training loss: 730.0881508810436\n",
      "Epoch 2 step 785: training accuarcy: 0.869\n",
      "Epoch 2 step 785: training loss: 725.6723783543562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 786: training accuarcy: 0.8635\n",
      "Epoch 2 step 786: training loss: 705.8044547701887\n",
      "Epoch 2 step 787: training accuarcy: 0.875\n",
      "Epoch 2 step 787: training loss: 745.0177152427982\n",
      "Epoch 2 step 788: training accuarcy: 0.8595\n",
      "Epoch 2 step 788: training loss: 294.58378716480166\n",
      "Epoch 2 step 789: training accuarcy: 0.8692307692307693\n",
      "Epoch 2: train loss 762.3531646269417, train accuarcy 0.8305734395980835\n",
      "Epoch 2: valid loss 989.3576875305267, valid accuarcy 0.761976957321167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████                                                                                               | 3/8 [06:41<11:09, 133.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 662.8766847026562\n",
      "Epoch 3 step 790: training accuarcy: 0.8915000000000001\n",
      "Epoch 3 step 790: training loss: 641.7494012508415\n",
      "Epoch 3 step 791: training accuarcy: 0.897\n",
      "Epoch 3 step 791: training loss: 653.0795543581189\n",
      "Epoch 3 step 792: training accuarcy: 0.89\n",
      "Epoch 3 step 792: training loss: 641.8297138849573\n",
      "Epoch 3 step 793: training accuarcy: 0.8995\n",
      "Epoch 3 step 793: training loss: 652.5418764175977\n",
      "Epoch 3 step 794: training accuarcy: 0.8835000000000001\n",
      "Epoch 3 step 794: training loss: 615.5849772180482\n",
      "Epoch 3 step 795: training accuarcy: 0.909\n",
      "Epoch 3 step 795: training loss: 636.9408397253416\n",
      "Epoch 3 step 796: training accuarcy: 0.9035\n",
      "Epoch 3 step 796: training loss: 657.5432947642714\n",
      "Epoch 3 step 797: training accuarcy: 0.893\n",
      "Epoch 3 step 797: training loss: 655.2370914042366\n",
      "Epoch 3 step 798: training accuarcy: 0.8915000000000001\n",
      "Epoch 3 step 798: training loss: 652.8417217789472\n",
      "Epoch 3 step 799: training accuarcy: 0.886\n",
      "Epoch 3 step 799: training loss: 627.4010480464716\n",
      "Epoch 3 step 800: training accuarcy: 0.8965\n",
      "Epoch 3 step 800: training loss: 644.6436265513939\n",
      "Epoch 3 step 801: training accuarcy: 0.8905000000000001\n",
      "Epoch 3 step 801: training loss: 654.4724262355905\n",
      "Epoch 3 step 802: training accuarcy: 0.892\n",
      "Epoch 3 step 802: training loss: 615.619207011632\n",
      "Epoch 3 step 803: training accuarcy: 0.9115\n",
      "Epoch 3 step 803: training loss: 662.776048578862\n",
      "Epoch 3 step 804: training accuarcy: 0.884\n",
      "Epoch 3 step 804: training loss: 643.2340863580932\n",
      "Epoch 3 step 805: training accuarcy: 0.893\n",
      "Epoch 3 step 805: training loss: 634.5783797609706\n",
      "Epoch 3 step 806: training accuarcy: 0.9045\n",
      "Epoch 3 step 806: training loss: 633.7534603023989\n",
      "Epoch 3 step 807: training accuarcy: 0.892\n",
      "Epoch 3 step 807: training loss: 642.0109064680227\n",
      "Epoch 3 step 808: training accuarcy: 0.901\n",
      "Epoch 3 step 808: training loss: 618.8613450016787\n",
      "Epoch 3 step 809: training accuarcy: 0.908\n",
      "Epoch 3 step 809: training loss: 620.8374095157193\n",
      "Epoch 3 step 810: training accuarcy: 0.904\n",
      "Epoch 3 step 810: training loss: 635.7238149166745\n",
      "Epoch 3 step 811: training accuarcy: 0.8955000000000001\n",
      "Epoch 3 step 811: training loss: 670.1702278287014\n",
      "Epoch 3 step 812: training accuarcy: 0.8825000000000001\n",
      "Epoch 3 step 812: training loss: 635.3453621058246\n",
      "Epoch 3 step 813: training accuarcy: 0.8895000000000001\n",
      "Epoch 3 step 813: training loss: 653.9157852564011\n",
      "Epoch 3 step 814: training accuarcy: 0.886\n",
      "Epoch 3 step 814: training loss: 642.0495867139908\n",
      "Epoch 3 step 815: training accuarcy: 0.8935000000000001\n",
      "Epoch 3 step 815: training loss: 624.9100671763329\n",
      "Epoch 3 step 816: training accuarcy: 0.899\n",
      "Epoch 3 step 816: training loss: 632.3728065308904\n",
      "Epoch 3 step 817: training accuarcy: 0.903\n",
      "Epoch 3 step 817: training loss: 639.8965589240551\n",
      "Epoch 3 step 818: training accuarcy: 0.8945000000000001\n",
      "Epoch 3 step 818: training loss: 648.764901893936\n",
      "Epoch 3 step 819: training accuarcy: 0.8845000000000001\n",
      "Epoch 3 step 819: training loss: 637.3282402966926\n",
      "Epoch 3 step 820: training accuarcy: 0.8965\n",
      "Epoch 3 step 820: training loss: 636.7309131173433\n",
      "Epoch 3 step 821: training accuarcy: 0.8955000000000001\n",
      "Epoch 3 step 821: training loss: 652.701254566443\n",
      "Epoch 3 step 822: training accuarcy: 0.8925000000000001\n",
      "Epoch 3 step 822: training loss: 634.2687739246242\n",
      "Epoch 3 step 823: training accuarcy: 0.904\n",
      "Epoch 3 step 823: training loss: 642.7637444252917\n",
      "Epoch 3 step 824: training accuarcy: 0.893\n",
      "Epoch 3 step 824: training loss: 635.4527437008979\n",
      "Epoch 3 step 825: training accuarcy: 0.897\n",
      "Epoch 3 step 825: training loss: 622.779915959506\n",
      "Epoch 3 step 826: training accuarcy: 0.901\n",
      "Epoch 3 step 826: training loss: 637.6645223526884\n",
      "Epoch 3 step 827: training accuarcy: 0.892\n",
      "Epoch 3 step 827: training loss: 641.5748015652212\n",
      "Epoch 3 step 828: training accuarcy: 0.8895000000000001\n",
      "Epoch 3 step 828: training loss: 630.739388554526\n",
      "Epoch 3 step 829: training accuarcy: 0.9\n",
      "Epoch 3 step 829: training loss: 662.1406690844304\n",
      "Epoch 3 step 830: training accuarcy: 0.8835000000000001\n",
      "Epoch 3 step 830: training loss: 630.6148683153795\n",
      "Epoch 3 step 831: training accuarcy: 0.899\n",
      "Epoch 3 step 831: training loss: 636.8501924057136\n",
      "Epoch 3 step 832: training accuarcy: 0.8975\n",
      "Epoch 3 step 832: training loss: 657.821666868991\n",
      "Epoch 3 step 833: training accuarcy: 0.8875000000000001\n",
      "Epoch 3 step 833: training loss: 637.391573587845\n",
      "Epoch 3 step 834: training accuarcy: 0.902\n",
      "Epoch 3 step 834: training loss: 652.586760623071\n",
      "Epoch 3 step 835: training accuarcy: 0.8785000000000001\n",
      "Epoch 3 step 835: training loss: 640.5356468709936\n",
      "Epoch 3 step 836: training accuarcy: 0.895\n",
      "Epoch 3 step 836: training loss: 650.6529660146996\n",
      "Epoch 3 step 837: training accuarcy: 0.893\n",
      "Epoch 3 step 837: training loss: 613.1759842192798\n",
      "Epoch 3 step 838: training accuarcy: 0.908\n",
      "Epoch 3 step 838: training loss: 627.7530330898999\n",
      "Epoch 3 step 839: training accuarcy: 0.897\n",
      "Epoch 3 step 839: training loss: 641.6298346800905\n",
      "Epoch 3 step 840: training accuarcy: 0.887\n",
      "Epoch 3 step 840: training loss: 653.5036135503079\n",
      "Epoch 3 step 841: training accuarcy: 0.894\n",
      "Epoch 3 step 841: training loss: 609.3570538941638\n",
      "Epoch 3 step 842: training accuarcy: 0.9045\n",
      "Epoch 3 step 842: training loss: 623.1652240505256\n",
      "Epoch 3 step 843: training accuarcy: 0.9015\n",
      "Epoch 3 step 843: training loss: 641.5794107573162\n",
      "Epoch 3 step 844: training accuarcy: 0.8975\n",
      "Epoch 3 step 844: training loss: 639.5110796306161\n",
      "Epoch 3 step 845: training accuarcy: 0.8965\n",
      "Epoch 3 step 845: training loss: 627.0107720029714\n",
      "Epoch 3 step 846: training accuarcy: 0.9015\n",
      "Epoch 3 step 846: training loss: 635.887399723688\n",
      "Epoch 3 step 847: training accuarcy: 0.904\n",
      "Epoch 3 step 847: training loss: 624.4079866901488\n",
      "Epoch 3 step 848: training accuarcy: 0.898\n",
      "Epoch 3 step 848: training loss: 623.7424182976496\n",
      "Epoch 3 step 849: training accuarcy: 0.902\n",
      "Epoch 3 step 849: training loss: 638.9948531402808\n",
      "Epoch 3 step 850: training accuarcy: 0.8975\n",
      "Epoch 3 step 850: training loss: 631.7945887660054\n",
      "Epoch 3 step 851: training accuarcy: 0.8975\n",
      "Epoch 3 step 851: training loss: 663.1124195523128\n",
      "Epoch 3 step 852: training accuarcy: 0.887\n",
      "Epoch 3 step 852: training loss: 635.3366540489152\n",
      "Epoch 3 step 853: training accuarcy: 0.898\n",
      "Epoch 3 step 853: training loss: 636.3894985701598\n",
      "Epoch 3 step 854: training accuarcy: 0.9015\n",
      "Epoch 3 step 854: training loss: 642.2503416066736\n",
      "Epoch 3 step 855: training accuarcy: 0.8925000000000001\n",
      "Epoch 3 step 855: training loss: 646.3583732158486\n",
      "Epoch 3 step 856: training accuarcy: 0.893\n",
      "Epoch 3 step 856: training loss: 630.2192890823669\n",
      "Epoch 3 step 857: training accuarcy: 0.896\n",
      "Epoch 3 step 857: training loss: 619.4339587843289\n",
      "Epoch 3 step 858: training accuarcy: 0.904\n",
      "Epoch 3 step 858: training loss: 620.9405762706504\n",
      "Epoch 3 step 859: training accuarcy: 0.903\n",
      "Epoch 3 step 859: training loss: 607.2325271556595\n",
      "Epoch 3 step 860: training accuarcy: 0.908\n",
      "Epoch 3 step 860: training loss: 608.9860031469888\n",
      "Epoch 3 step 861: training accuarcy: 0.9065\n",
      "Epoch 3 step 861: training loss: 612.571657958063\n",
      "Epoch 3 step 862: training accuarcy: 0.902\n",
      "Epoch 3 step 862: training loss: 610.6489481511056\n",
      "Epoch 3 step 863: training accuarcy: 0.9055\n",
      "Epoch 3 step 863: training loss: 637.5982586721499\n",
      "Epoch 3 step 864: training accuarcy: 0.8945000000000001\n",
      "Epoch 3 step 864: training loss: 629.5807653761664\n",
      "Epoch 3 step 865: training accuarcy: 0.8955000000000001\n",
      "Epoch 3 step 865: training loss: 616.400982617663\n",
      "Epoch 3 step 866: training accuarcy: 0.8955000000000001\n",
      "Epoch 3 step 866: training loss: 641.4233517488124\n",
      "Epoch 3 step 867: training accuarcy: 0.896\n",
      "Epoch 3 step 867: training loss: 616.8728021028965\n",
      "Epoch 3 step 868: training accuarcy: 0.906\n",
      "Epoch 3 step 868: training loss: 629.5249357094846\n",
      "Epoch 3 step 869: training accuarcy: 0.8995\n",
      "Epoch 3 step 869: training loss: 617.3762964110273\n",
      "Epoch 3 step 870: training accuarcy: 0.907\n",
      "Epoch 3 step 870: training loss: 619.1636913060552\n",
      "Epoch 3 step 871: training accuarcy: 0.901\n",
      "Epoch 3 step 871: training loss: 638.9212699006945\n",
      "Epoch 3 step 872: training accuarcy: 0.901\n",
      "Epoch 3 step 872: training loss: 625.2038069954167\n",
      "Epoch 3 step 873: training accuarcy: 0.8995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 873: training loss: 631.7020458312045\n",
      "Epoch 3 step 874: training accuarcy: 0.898\n",
      "Epoch 3 step 874: training loss: 614.8507439320256\n",
      "Epoch 3 step 875: training accuarcy: 0.904\n",
      "Epoch 3 step 875: training loss: 610.3429885348747\n",
      "Epoch 3 step 876: training accuarcy: 0.9055\n",
      "Epoch 3 step 876: training loss: 640.9670313974943\n",
      "Epoch 3 step 877: training accuarcy: 0.899\n",
      "Epoch 3 step 877: training loss: 594.5607772476968\n",
      "Epoch 3 step 878: training accuarcy: 0.9145\n",
      "Epoch 3 step 878: training loss: 633.6067317804761\n",
      "Epoch 3 step 879: training accuarcy: 0.886\n",
      "Epoch 3 step 879: training loss: 617.5884767642145\n",
      "Epoch 3 step 880: training accuarcy: 0.9015\n",
      "Epoch 3 step 880: training loss: 617.2515007800687\n",
      "Epoch 3 step 881: training accuarcy: 0.903\n",
      "Epoch 3 step 881: training loss: 625.2582513765694\n",
      "Epoch 3 step 882: training accuarcy: 0.899\n",
      "Epoch 3 step 882: training loss: 632.0769984484552\n",
      "Epoch 3 step 883: training accuarcy: 0.889\n",
      "Epoch 3 step 883: training loss: 629.4311883065981\n",
      "Epoch 3 step 884: training accuarcy: 0.89\n",
      "Epoch 3 step 884: training loss: 636.0218044660703\n",
      "Epoch 3 step 885: training accuarcy: 0.893\n",
      "Epoch 3 step 885: training loss: 640.2831274834982\n",
      "Epoch 3 step 886: training accuarcy: 0.89\n",
      "Epoch 3 step 886: training loss: 621.3258795178285\n",
      "Epoch 3 step 887: training accuarcy: 0.889\n",
      "Epoch 3 step 887: training loss: 594.6628768576041\n",
      "Epoch 3 step 888: training accuarcy: 0.9065\n",
      "Epoch 3 step 888: training loss: 644.3059301173025\n",
      "Epoch 3 step 889: training accuarcy: 0.894\n",
      "Epoch 3 step 889: training loss: 614.5492085787586\n",
      "Epoch 3 step 890: training accuarcy: 0.8975\n",
      "Epoch 3 step 890: training loss: 605.3429985323482\n",
      "Epoch 3 step 891: training accuarcy: 0.9005\n",
      "Epoch 3 step 891: training loss: 635.6359236988071\n",
      "Epoch 3 step 892: training accuarcy: 0.9005\n",
      "Epoch 3 step 892: training loss: 628.2588484496522\n",
      "Epoch 3 step 893: training accuarcy: 0.897\n",
      "Epoch 3 step 893: training loss: 585.1763676270598\n",
      "Epoch 3 step 894: training accuarcy: 0.9115\n",
      "Epoch 3 step 894: training loss: 616.1109799078569\n",
      "Epoch 3 step 895: training accuarcy: 0.902\n",
      "Epoch 3 step 895: training loss: 628.6738027367205\n",
      "Epoch 3 step 896: training accuarcy: 0.895\n",
      "Epoch 3 step 896: training loss: 617.8218047725396\n",
      "Epoch 3 step 897: training accuarcy: 0.905\n",
      "Epoch 3 step 897: training loss: 620.7613919482644\n",
      "Epoch 3 step 898: training accuarcy: 0.897\n",
      "Epoch 3 step 898: training loss: 597.048298233279\n",
      "Epoch 3 step 899: training accuarcy: 0.913\n",
      "Epoch 3 step 899: training loss: 612.4749080160636\n",
      "Epoch 3 step 900: training accuarcy: 0.8995\n",
      "Epoch 3 step 900: training loss: 648.2250130191632\n",
      "Epoch 3 step 901: training accuarcy: 0.887\n",
      "Epoch 3 step 901: training loss: 608.0781588871788\n",
      "Epoch 3 step 902: training accuarcy: 0.909\n",
      "Epoch 3 step 902: training loss: 603.9472195689664\n",
      "Epoch 3 step 903: training accuarcy: 0.9065\n",
      "Epoch 3 step 903: training loss: 611.7732319156844\n",
      "Epoch 3 step 904: training accuarcy: 0.9085\n",
      "Epoch 3 step 904: training loss: 627.8959345378587\n",
      "Epoch 3 step 905: training accuarcy: 0.896\n",
      "Epoch 3 step 905: training loss: 615.1567905406131\n",
      "Epoch 3 step 906: training accuarcy: 0.905\n",
      "Epoch 3 step 906: training loss: 604.9696658957182\n",
      "Epoch 3 step 907: training accuarcy: 0.9005\n",
      "Epoch 3 step 907: training loss: 590.6502954665166\n",
      "Epoch 3 step 908: training accuarcy: 0.9135\n",
      "Epoch 3 step 908: training loss: 628.8610017313854\n",
      "Epoch 3 step 909: training accuarcy: 0.889\n",
      "Epoch 3 step 909: training loss: 622.6320440787288\n",
      "Epoch 3 step 910: training accuarcy: 0.899\n",
      "Epoch 3 step 910: training loss: 617.4641835563815\n",
      "Epoch 3 step 911: training accuarcy: 0.904\n",
      "Epoch 3 step 911: training loss: 612.2195536809227\n",
      "Epoch 3 step 912: training accuarcy: 0.9055\n",
      "Epoch 3 step 912: training loss: 614.1183731608432\n",
      "Epoch 3 step 913: training accuarcy: 0.902\n",
      "Epoch 3 step 913: training loss: 614.3386314150512\n",
      "Epoch 3 step 914: training accuarcy: 0.9\n",
      "Epoch 3 step 914: training loss: 623.413209953815\n",
      "Epoch 3 step 915: training accuarcy: 0.8915000000000001\n",
      "Epoch 3 step 915: training loss: 619.0747931942964\n",
      "Epoch 3 step 916: training accuarcy: 0.8995\n",
      "Epoch 3 step 916: training loss: 614.1736417222934\n",
      "Epoch 3 step 917: training accuarcy: 0.906\n",
      "Epoch 3 step 917: training loss: 617.6238456617184\n",
      "Epoch 3 step 918: training accuarcy: 0.897\n",
      "Epoch 3 step 918: training loss: 617.2282031995603\n",
      "Epoch 3 step 919: training accuarcy: 0.9015\n",
      "Epoch 3 step 919: training loss: 599.7559328714614\n",
      "Epoch 3 step 920: training accuarcy: 0.9035\n",
      "Epoch 3 step 920: training loss: 612.7890372561017\n",
      "Epoch 3 step 921: training accuarcy: 0.904\n",
      "Epoch 3 step 921: training loss: 648.5319068276281\n",
      "Epoch 3 step 922: training accuarcy: 0.887\n",
      "Epoch 3 step 922: training loss: 603.0151002633504\n",
      "Epoch 3 step 923: training accuarcy: 0.908\n",
      "Epoch 3 step 923: training loss: 603.5081856751358\n",
      "Epoch 3 step 924: training accuarcy: 0.908\n",
      "Epoch 3 step 924: training loss: 610.7756038096791\n",
      "Epoch 3 step 925: training accuarcy: 0.898\n",
      "Epoch 3 step 925: training loss: 621.0711065515324\n",
      "Epoch 3 step 926: training accuarcy: 0.9045\n",
      "Epoch 3 step 926: training loss: 614.0582243652938\n",
      "Epoch 3 step 927: training accuarcy: 0.9055\n",
      "Epoch 3 step 927: training loss: 638.4707466947801\n",
      "Epoch 3 step 928: training accuarcy: 0.8915000000000001\n",
      "Epoch 3 step 928: training loss: 614.2656577449316\n",
      "Epoch 3 step 929: training accuarcy: 0.904\n",
      "Epoch 3 step 929: training loss: 592.059218701082\n",
      "Epoch 3 step 930: training accuarcy: 0.907\n",
      "Epoch 3 step 930: training loss: 619.2149902850215\n",
      "Epoch 3 step 931: training accuarcy: 0.9005\n",
      "Epoch 3 step 931: training loss: 622.8691594482268\n",
      "Epoch 3 step 932: training accuarcy: 0.8925000000000001\n",
      "Epoch 3 step 932: training loss: 594.1848622625462\n",
      "Epoch 3 step 933: training accuarcy: 0.91\n",
      "Epoch 3 step 933: training loss: 622.6910832774295\n",
      "Epoch 3 step 934: training accuarcy: 0.894\n",
      "Epoch 3 step 934: training loss: 596.7669238265926\n",
      "Epoch 3 step 935: training accuarcy: 0.896\n",
      "Epoch 3 step 935: training loss: 594.4813641754157\n",
      "Epoch 3 step 936: training accuarcy: 0.9075\n",
      "Epoch 3 step 936: training loss: 620.6535328338666\n",
      "Epoch 3 step 937: training accuarcy: 0.91\n",
      "Epoch 3 step 937: training loss: 611.7301696594615\n",
      "Epoch 3 step 938: training accuarcy: 0.901\n",
      "Epoch 3 step 938: training loss: 602.257455085793\n",
      "Epoch 3 step 939: training accuarcy: 0.912\n",
      "Epoch 3 step 939: training loss: 612.5936784569078\n",
      "Epoch 3 step 940: training accuarcy: 0.9035\n",
      "Epoch 3 step 940: training loss: 605.0225981395077\n",
      "Epoch 3 step 941: training accuarcy: 0.905\n",
      "Epoch 3 step 941: training loss: 569.9780435622803\n",
      "Epoch 3 step 942: training accuarcy: 0.927\n",
      "Epoch 3 step 942: training loss: 606.5895663160769\n",
      "Epoch 3 step 943: training accuarcy: 0.905\n",
      "Epoch 3 step 943: training loss: 603.6268600203846\n",
      "Epoch 3 step 944: training accuarcy: 0.908\n",
      "Epoch 3 step 944: training loss: 603.7864444099603\n",
      "Epoch 3 step 945: training accuarcy: 0.9\n",
      "Epoch 3 step 945: training loss: 598.2732330204936\n",
      "Epoch 3 step 946: training accuarcy: 0.9015\n",
      "Epoch 3 step 946: training loss: 584.4926720781169\n",
      "Epoch 3 step 947: training accuarcy: 0.9095\n",
      "Epoch 3 step 947: training loss: 610.847036771957\n",
      "Epoch 3 step 948: training accuarcy: 0.9005\n",
      "Epoch 3 step 948: training loss: 600.1153895177847\n",
      "Epoch 3 step 949: training accuarcy: 0.904\n",
      "Epoch 3 step 949: training loss: 580.9029844032609\n",
      "Epoch 3 step 950: training accuarcy: 0.906\n",
      "Epoch 3 step 950: training loss: 599.8507765035732\n",
      "Epoch 3 step 951: training accuarcy: 0.913\n",
      "Epoch 3 step 951: training loss: 609.6629516988905\n",
      "Epoch 3 step 952: training accuarcy: 0.913\n",
      "Epoch 3 step 952: training loss: 596.8273247410076\n",
      "Epoch 3 step 953: training accuarcy: 0.9125\n",
      "Epoch 3 step 953: training loss: 608.2346354564096\n",
      "Epoch 3 step 954: training accuarcy: 0.905\n",
      "Epoch 3 step 954: training loss: 639.0171106163374\n",
      "Epoch 3 step 955: training accuarcy: 0.89\n",
      "Epoch 3 step 955: training loss: 590.4817245137676\n",
      "Epoch 3 step 956: training accuarcy: 0.9055\n",
      "Epoch 3 step 956: training loss: 597.6731801951302\n",
      "Epoch 3 step 957: training accuarcy: 0.911\n",
      "Epoch 3 step 957: training loss: 609.5462539238164\n",
      "Epoch 3 step 958: training accuarcy: 0.8995\n",
      "Epoch 3 step 958: training loss: 566.6245692437208\n",
      "Epoch 3 step 959: training accuarcy: 0.9175\n",
      "Epoch 3 step 959: training loss: 635.1996001739645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 960: training accuarcy: 0.887\n",
      "Epoch 3 step 960: training loss: 613.5972854995517\n",
      "Epoch 3 step 961: training accuarcy: 0.903\n",
      "Epoch 3 step 961: training loss: 592.151937238631\n",
      "Epoch 3 step 962: training accuarcy: 0.914\n",
      "Epoch 3 step 962: training loss: 597.8531012286089\n",
      "Epoch 3 step 963: training accuarcy: 0.91\n",
      "Epoch 3 step 963: training loss: 586.7394798985896\n",
      "Epoch 3 step 964: training accuarcy: 0.909\n",
      "Epoch 3 step 964: training loss: 597.3925989849918\n",
      "Epoch 3 step 965: training accuarcy: 0.906\n",
      "Epoch 3 step 965: training loss: 626.650744639278\n",
      "Epoch 3 step 966: training accuarcy: 0.89\n",
      "Epoch 3 step 966: training loss: 602.5752597330887\n",
      "Epoch 3 step 967: training accuarcy: 0.909\n",
      "Epoch 3 step 967: training loss: 587.6273881538823\n",
      "Epoch 3 step 968: training accuarcy: 0.9115\n",
      "Epoch 3 step 968: training loss: 581.9788915852397\n",
      "Epoch 3 step 969: training accuarcy: 0.9095\n",
      "Epoch 3 step 969: training loss: 597.8279681108854\n",
      "Epoch 3 step 970: training accuarcy: 0.905\n",
      "Epoch 3 step 970: training loss: 595.887427655117\n",
      "Epoch 3 step 971: training accuarcy: 0.9115\n",
      "Epoch 3 step 971: training loss: 579.918642455101\n",
      "Epoch 3 step 972: training accuarcy: 0.91\n",
      "Epoch 3 step 972: training loss: 608.8978900420308\n",
      "Epoch 3 step 973: training accuarcy: 0.9025\n",
      "Epoch 3 step 973: training loss: 583.2930142824478\n",
      "Epoch 3 step 974: training accuarcy: 0.9065\n",
      "Epoch 3 step 974: training loss: 601.7538885494221\n",
      "Epoch 3 step 975: training accuarcy: 0.903\n",
      "Epoch 3 step 975: training loss: 608.1859029249816\n",
      "Epoch 3 step 976: training accuarcy: 0.9035\n",
      "Epoch 3 step 976: training loss: 618.0520413662524\n",
      "Epoch 3 step 977: training accuarcy: 0.903\n",
      "Epoch 3 step 977: training loss: 610.1439650780485\n",
      "Epoch 3 step 978: training accuarcy: 0.899\n",
      "Epoch 3 step 978: training loss: 618.6864538712755\n",
      "Epoch 3 step 979: training accuarcy: 0.899\n",
      "Epoch 3 step 979: training loss: 618.1358507561155\n",
      "Epoch 3 step 980: training accuarcy: 0.902\n",
      "Epoch 3 step 980: training loss: 589.0634884712142\n",
      "Epoch 3 step 981: training accuarcy: 0.9105\n",
      "Epoch 3 step 981: training loss: 595.695770176505\n",
      "Epoch 3 step 982: training accuarcy: 0.8995\n",
      "Epoch 3 step 982: training loss: 589.9886863075566\n",
      "Epoch 3 step 983: training accuarcy: 0.906\n",
      "Epoch 3 step 983: training loss: 587.8143278876553\n",
      "Epoch 3 step 984: training accuarcy: 0.903\n",
      "Epoch 3 step 984: training loss: 586.4992072311305\n",
      "Epoch 3 step 985: training accuarcy: 0.9145\n",
      "Epoch 3 step 985: training loss: 586.5661179458039\n",
      "Epoch 3 step 986: training accuarcy: 0.917\n",
      "Epoch 3 step 986: training loss: 584.271630244891\n",
      "Epoch 3 step 987: training accuarcy: 0.9055\n",
      "Epoch 3 step 987: training loss: 615.4598080302783\n",
      "Epoch 3 step 988: training accuarcy: 0.8975\n",
      "Epoch 3 step 988: training loss: 579.2274740945895\n",
      "Epoch 3 step 989: training accuarcy: 0.9115\n",
      "Epoch 3 step 989: training loss: 607.9767214870938\n",
      "Epoch 3 step 990: training accuarcy: 0.904\n",
      "Epoch 3 step 990: training loss: 566.0900550127619\n",
      "Epoch 3 step 991: training accuarcy: 0.911\n",
      "Epoch 3 step 991: training loss: 587.6654273753486\n",
      "Epoch 3 step 992: training accuarcy: 0.906\n",
      "Epoch 3 step 992: training loss: 601.8593939464142\n",
      "Epoch 3 step 993: training accuarcy: 0.9055\n",
      "Epoch 3 step 993: training loss: 578.2546215959427\n",
      "Epoch 3 step 994: training accuarcy: 0.912\n",
      "Epoch 3 step 994: training loss: 610.2583221453964\n",
      "Epoch 3 step 995: training accuarcy: 0.9015\n",
      "Epoch 3 step 995: training loss: 591.7000481996579\n",
      "Epoch 3 step 996: training accuarcy: 0.906\n",
      "Epoch 3 step 996: training loss: 629.1199699085199\n",
      "Epoch 3 step 997: training accuarcy: 0.8885000000000001\n",
      "Epoch 3 step 997: training loss: 600.1461449870511\n",
      "Epoch 3 step 998: training accuarcy: 0.899\n",
      "Epoch 3 step 998: training loss: 591.6737528501423\n",
      "Epoch 3 step 999: training accuarcy: 0.9125\n",
      "Epoch 3 step 999: training loss: 571.809202489584\n",
      "Epoch 3 step 1000: training accuarcy: 0.9215\n",
      "Epoch 3 step 1000: training loss: 602.8771018867285\n",
      "Epoch 3 step 1001: training accuarcy: 0.908\n",
      "Epoch 3 step 1001: training loss: 594.4310280688755\n",
      "Epoch 3 step 1002: training accuarcy: 0.905\n",
      "Epoch 3 step 1002: training loss: 583.3323177083583\n",
      "Epoch 3 step 1003: training accuarcy: 0.914\n",
      "Epoch 3 step 1003: training loss: 595.5102940159478\n",
      "Epoch 3 step 1004: training accuarcy: 0.9035\n",
      "Epoch 3 step 1004: training loss: 593.4943575136024\n",
      "Epoch 3 step 1005: training accuarcy: 0.913\n",
      "Epoch 3 step 1005: training loss: 599.5851873559865\n",
      "Epoch 3 step 1006: training accuarcy: 0.9065\n",
      "Epoch 3 step 1006: training loss: 582.565812502121\n",
      "Epoch 3 step 1007: training accuarcy: 0.907\n",
      "Epoch 3 step 1007: training loss: 592.3066686181711\n",
      "Epoch 3 step 1008: training accuarcy: 0.911\n",
      "Epoch 3 step 1008: training loss: 575.8012874660891\n",
      "Epoch 3 step 1009: training accuarcy: 0.9095\n",
      "Epoch 3 step 1009: training loss: 568.7500459496277\n",
      "Epoch 3 step 1010: training accuarcy: 0.9135\n",
      "Epoch 3 step 1010: training loss: 579.7113263629296\n",
      "Epoch 3 step 1011: training accuarcy: 0.9125\n",
      "Epoch 3 step 1011: training loss: 581.9738401189942\n",
      "Epoch 3 step 1012: training accuarcy: 0.9005\n",
      "Epoch 3 step 1012: training loss: 604.2483045890458\n",
      "Epoch 3 step 1013: training accuarcy: 0.9015\n",
      "Epoch 3 step 1013: training loss: 586.0197896233167\n",
      "Epoch 3 step 1014: training accuarcy: 0.9035\n",
      "Epoch 3 step 1014: training loss: 567.9399079337837\n",
      "Epoch 3 step 1015: training accuarcy: 0.9155\n",
      "Epoch 3 step 1015: training loss: 561.760935050534\n",
      "Epoch 3 step 1016: training accuarcy: 0.9135\n",
      "Epoch 3 step 1016: training loss: 559.7460975103452\n",
      "Epoch 3 step 1017: training accuarcy: 0.923\n",
      "Epoch 3 step 1017: training loss: 614.7643770411747\n",
      "Epoch 3 step 1018: training accuarcy: 0.9\n",
      "Epoch 3 step 1018: training loss: 586.7376176881052\n",
      "Epoch 3 step 1019: training accuarcy: 0.91\n",
      "Epoch 3 step 1019: training loss: 570.6301050376721\n",
      "Epoch 3 step 1020: training accuarcy: 0.9175\n",
      "Epoch 3 step 1020: training loss: 567.3682272274789\n",
      "Epoch 3 step 1021: training accuarcy: 0.9185\n",
      "Epoch 3 step 1021: training loss: 586.5477087304015\n",
      "Epoch 3 step 1022: training accuarcy: 0.907\n",
      "Epoch 3 step 1022: training loss: 593.8765051759469\n",
      "Epoch 3 step 1023: training accuarcy: 0.905\n",
      "Epoch 3 step 1023: training loss: 584.2963790698755\n",
      "Epoch 3 step 1024: training accuarcy: 0.917\n",
      "Epoch 3 step 1024: training loss: 572.5872099572371\n",
      "Epoch 3 step 1025: training accuarcy: 0.9195\n",
      "Epoch 3 step 1025: training loss: 551.0740768034016\n",
      "Epoch 3 step 1026: training accuarcy: 0.9185\n",
      "Epoch 3 step 1026: training loss: 589.3804780269711\n",
      "Epoch 3 step 1027: training accuarcy: 0.905\n",
      "Epoch 3 step 1027: training loss: 591.0624367783429\n",
      "Epoch 3 step 1028: training accuarcy: 0.9025\n",
      "Epoch 3 step 1028: training loss: 591.5213233537523\n",
      "Epoch 3 step 1029: training accuarcy: 0.902\n",
      "Epoch 3 step 1029: training loss: 599.1010180709221\n",
      "Epoch 3 step 1030: training accuarcy: 0.8985\n",
      "Epoch 3 step 1030: training loss: 573.0326172507318\n",
      "Epoch 3 step 1031: training accuarcy: 0.919\n",
      "Epoch 3 step 1031: training loss: 571.043234586487\n",
      "Epoch 3 step 1032: training accuarcy: 0.9165\n",
      "Epoch 3 step 1032: training loss: 582.5309370494087\n",
      "Epoch 3 step 1033: training accuarcy: 0.904\n",
      "Epoch 3 step 1033: training loss: 579.8306543992453\n",
      "Epoch 3 step 1034: training accuarcy: 0.9\n",
      "Epoch 3 step 1034: training loss: 593.4149993822388\n",
      "Epoch 3 step 1035: training accuarcy: 0.9045\n",
      "Epoch 3 step 1035: training loss: 574.6428448593431\n",
      "Epoch 3 step 1036: training accuarcy: 0.904\n",
      "Epoch 3 step 1036: training loss: 591.7875230046909\n",
      "Epoch 3 step 1037: training accuarcy: 0.908\n",
      "Epoch 3 step 1037: training loss: 588.9954904042659\n",
      "Epoch 3 step 1038: training accuarcy: 0.9005\n",
      "Epoch 3 step 1038: training loss: 585.3212945524281\n",
      "Epoch 3 step 1039: training accuarcy: 0.9075\n",
      "Epoch 3 step 1039: training loss: 578.9755308816832\n",
      "Epoch 3 step 1040: training accuarcy: 0.9075\n",
      "Epoch 3 step 1040: training loss: 583.4360486548418\n",
      "Epoch 3 step 1041: training accuarcy: 0.899\n",
      "Epoch 3 step 1041: training loss: 594.2413550434547\n",
      "Epoch 3 step 1042: training accuarcy: 0.8985\n",
      "Epoch 3 step 1042: training loss: 583.3787980281887\n",
      "Epoch 3 step 1043: training accuarcy: 0.901\n",
      "Epoch 3 step 1043: training loss: 539.8795088263919\n",
      "Epoch 3 step 1044: training accuarcy: 0.92\n",
      "Epoch 3 step 1044: training loss: 560.8154531365071\n",
      "Epoch 3 step 1045: training accuarcy: 0.9065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 1045: training loss: 563.8318709796129\n",
      "Epoch 3 step 1046: training accuarcy: 0.915\n",
      "Epoch 3 step 1046: training loss: 605.913150342893\n",
      "Epoch 3 step 1047: training accuarcy: 0.901\n",
      "Epoch 3 step 1047: training loss: 572.8340791039749\n",
      "Epoch 3 step 1048: training accuarcy: 0.913\n",
      "Epoch 3 step 1048: training loss: 565.5807341276164\n",
      "Epoch 3 step 1049: training accuarcy: 0.915\n",
      "Epoch 3 step 1049: training loss: 585.5508826188516\n",
      "Epoch 3 step 1050: training accuarcy: 0.9105\n",
      "Epoch 3 step 1050: training loss: 570.3915620121885\n",
      "Epoch 3 step 1051: training accuarcy: 0.9155\n",
      "Epoch 3 step 1051: training loss: 233.66633421860715\n",
      "Epoch 3 step 1052: training accuarcy: 0.9166666666666666\n",
      "Epoch 3: train loss 610.4419720792491, train accuarcy 0.8780782222747803\n",
      "Epoch 3: valid loss 924.4322790288536, valid accuarcy 0.7848190665245056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 4/8 [08:55<08:55, 133.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 492.93881175929664\n",
      "Epoch 4 step 1053: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1053: training loss: 490.8071296319089\n",
      "Epoch 4 step 1054: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1054: training loss: 487.3139157325017\n",
      "Epoch 4 step 1055: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1055: training loss: 492.41937599976586\n",
      "Epoch 4 step 1056: training accuarcy: 0.9415\n",
      "Epoch 4 step 1056: training loss: 507.1195418844647\n",
      "Epoch 4 step 1057: training accuarcy: 0.9315\n",
      "Epoch 4 step 1057: training loss: 502.2970678688273\n",
      "Epoch 4 step 1058: training accuarcy: 0.9345\n",
      "Epoch 4 step 1058: training loss: 483.9729111697408\n",
      "Epoch 4 step 1059: training accuarcy: 0.9455\n",
      "Epoch 4 step 1059: training loss: 495.1825056675923\n",
      "Epoch 4 step 1060: training accuarcy: 0.933\n",
      "Epoch 4 step 1060: training loss: 519.3698487515812\n",
      "Epoch 4 step 1061: training accuarcy: 0.9325\n",
      "Epoch 4 step 1061: training loss: 486.655807742015\n",
      "Epoch 4 step 1062: training accuarcy: 0.9365\n",
      "Epoch 4 step 1062: training loss: 481.0401568187343\n",
      "Epoch 4 step 1063: training accuarcy: 0.9465\n",
      "Epoch 4 step 1063: training loss: 505.9441301532514\n",
      "Epoch 4 step 1064: training accuarcy: 0.928\n",
      "Epoch 4 step 1064: training loss: 489.44840689268597\n",
      "Epoch 4 step 1065: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1065: training loss: 491.55856039938817\n",
      "Epoch 4 step 1066: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1066: training loss: 480.79706390623045\n",
      "Epoch 4 step 1067: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1067: training loss: 506.741285033615\n",
      "Epoch 4 step 1068: training accuarcy: 0.9295\n",
      "Epoch 4 step 1068: training loss: 503.0876351403099\n",
      "Epoch 4 step 1069: training accuarcy: 0.932\n",
      "Epoch 4 step 1069: training loss: 492.41876734886347\n",
      "Epoch 4 step 1070: training accuarcy: 0.9395\n",
      "Epoch 4 step 1070: training loss: 504.1055805284022\n",
      "Epoch 4 step 1071: training accuarcy: 0.9405\n",
      "Epoch 4 step 1071: training loss: 487.36557995994457\n",
      "Epoch 4 step 1072: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1072: training loss: 515.982663592892\n",
      "Epoch 4 step 1073: training accuarcy: 0.9325\n",
      "Epoch 4 step 1073: training loss: 477.5710713678779\n",
      "Epoch 4 step 1074: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1074: training loss: 482.57301084212986\n",
      "Epoch 4 step 1075: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1075: training loss: 513.8809230516515\n",
      "Epoch 4 step 1076: training accuarcy: 0.931\n",
      "Epoch 4 step 1076: training loss: 485.1578830071843\n",
      "Epoch 4 step 1077: training accuarcy: 0.9395\n",
      "Epoch 4 step 1077: training loss: 506.35101969949835\n",
      "Epoch 4 step 1078: training accuarcy: 0.9305\n",
      "Epoch 4 step 1078: training loss: 494.26679100933313\n",
      "Epoch 4 step 1079: training accuarcy: 0.9375\n",
      "Epoch 4 step 1079: training loss: 483.9070337005747\n",
      "Epoch 4 step 1080: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1080: training loss: 482.7931725748046\n",
      "Epoch 4 step 1081: training accuarcy: 0.9385\n",
      "Epoch 4 step 1081: training loss: 487.1626764366959\n",
      "Epoch 4 step 1082: training accuarcy: 0.9375\n",
      "Epoch 4 step 1082: training loss: 513.5229301915459\n",
      "Epoch 4 step 1083: training accuarcy: 0.9305\n",
      "Epoch 4 step 1083: training loss: 484.08584994614466\n",
      "Epoch 4 step 1084: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1084: training loss: 486.36489731257694\n",
      "Epoch 4 step 1085: training accuarcy: 0.9355\n",
      "Epoch 4 step 1085: training loss: 489.2453181632451\n",
      "Epoch 4 step 1086: training accuarcy: 0.9355\n",
      "Epoch 4 step 1086: training loss: 498.11195862098805\n",
      "Epoch 4 step 1087: training accuarcy: 0.927\n",
      "Epoch 4 step 1087: training loss: 482.5883601082131\n",
      "Epoch 4 step 1088: training accuarcy: 0.9385\n",
      "Epoch 4 step 1088: training loss: 489.52532987384154\n",
      "Epoch 4 step 1089: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1089: training loss: 508.4867784143641\n",
      "Epoch 4 step 1090: training accuarcy: 0.9305\n",
      "Epoch 4 step 1090: training loss: 476.8947951409796\n",
      "Epoch 4 step 1091: training accuarcy: 0.937\n",
      "Epoch 4 step 1091: training loss: 497.1274088058961\n",
      "Epoch 4 step 1092: training accuarcy: 0.9305\n",
      "Epoch 4 step 1092: training loss: 491.8131978327222\n",
      "Epoch 4 step 1093: training accuarcy: 0.935\n",
      "Epoch 4 step 1093: training loss: 520.9107600547014\n",
      "Epoch 4 step 1094: training accuarcy: 0.9275\n",
      "Epoch 4 step 1094: training loss: 479.1923649458463\n",
      "Epoch 4 step 1095: training accuarcy: 0.9475\n",
      "Epoch 4 step 1095: training loss: 510.47856395089065\n",
      "Epoch 4 step 1096: training accuarcy: 0.929\n",
      "Epoch 4 step 1096: training loss: 503.07840183641565\n",
      "Epoch 4 step 1097: training accuarcy: 0.9275\n",
      "Epoch 4 step 1097: training loss: 477.2045303024169\n",
      "Epoch 4 step 1098: training accuarcy: 0.9380000000000001\n",
      "Epoch 4 step 1098: training loss: 479.20470315299673\n",
      "Epoch 4 step 1099: training accuarcy: 0.9335\n",
      "Epoch 4 step 1099: training loss: 486.9747050771465\n",
      "Epoch 4 step 1100: training accuarcy: 0.9395\n",
      "Epoch 4 step 1100: training loss: 501.724341159249\n",
      "Epoch 4 step 1101: training accuarcy: 0.936\n",
      "Epoch 4 step 1101: training loss: 495.8574947040828\n",
      "Epoch 4 step 1102: training accuarcy: 0.934\n",
      "Epoch 4 step 1102: training loss: 483.7454094179629\n",
      "Epoch 4 step 1103: training accuarcy: 0.9355\n",
      "Epoch 4 step 1103: training loss: 490.216556473647\n",
      "Epoch 4 step 1104: training accuarcy: 0.9395\n",
      "Epoch 4 step 1104: training loss: 488.4609502242444\n",
      "Epoch 4 step 1105: training accuarcy: 0.9415\n",
      "Epoch 4 step 1105: training loss: 485.21077979399814\n",
      "Epoch 4 step 1106: training accuarcy: 0.9395\n",
      "Epoch 4 step 1106: training loss: 491.2287754338998\n",
      "Epoch 4 step 1107: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1107: training loss: 495.7187902805075\n",
      "Epoch 4 step 1108: training accuarcy: 0.9355\n",
      "Epoch 4 step 1108: training loss: 480.2735847685666\n",
      "Epoch 4 step 1109: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1109: training loss: 459.21444232547753\n",
      "Epoch 4 step 1110: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1110: training loss: 486.1683843591104\n",
      "Epoch 4 step 1111: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1111: training loss: 510.21431344191546\n",
      "Epoch 4 step 1112: training accuarcy: 0.9255\n",
      "Epoch 4 step 1112: training loss: 480.76790989211804\n",
      "Epoch 4 step 1113: training accuarcy: 0.937\n",
      "Epoch 4 step 1113: training loss: 495.4401222501357\n",
      "Epoch 4 step 1114: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1114: training loss: 475.0190035854394\n",
      "Epoch 4 step 1115: training accuarcy: 0.9485\n",
      "Epoch 4 step 1115: training loss: 475.75277958459367\n",
      "Epoch 4 step 1116: training accuarcy: 0.9355\n",
      "Epoch 4 step 1116: training loss: 458.08456818668515\n",
      "Epoch 4 step 1117: training accuarcy: 0.9505\n",
      "Epoch 4 step 1117: training loss: 494.6280662408893\n",
      "Epoch 4 step 1118: training accuarcy: 0.9285\n",
      "Epoch 4 step 1118: training loss: 508.36455522891526\n",
      "Epoch 4 step 1119: training accuarcy: 0.928\n",
      "Epoch 4 step 1119: training loss: 480.0170450703848\n",
      "Epoch 4 step 1120: training accuarcy: 0.937\n",
      "Epoch 4 step 1120: training loss: 477.7233031430887\n",
      "Epoch 4 step 1121: training accuarcy: 0.934\n",
      "Epoch 4 step 1121: training loss: 502.1187306881064\n",
      "Epoch 4 step 1122: training accuarcy: 0.932\n",
      "Epoch 4 step 1122: training loss: 488.4507824736268\n",
      "Epoch 4 step 1123: training accuarcy: 0.933\n",
      "Epoch 4 step 1123: training loss: 492.67796324185014\n",
      "Epoch 4 step 1124: training accuarcy: 0.9365\n",
      "Epoch 4 step 1124: training loss: 479.01401273603693\n",
      "Epoch 4 step 1125: training accuarcy: 0.9345\n",
      "Epoch 4 step 1125: training loss: 484.15462673381273\n",
      "Epoch 4 step 1126: training accuarcy: 0.9380000000000001\n",
      "Epoch 4 step 1126: training loss: 507.6524436980206\n",
      "Epoch 4 step 1127: training accuarcy: 0.929\n",
      "Epoch 4 step 1127: training loss: 489.8342151837762\n",
      "Epoch 4 step 1128: training accuarcy: 0.935\n",
      "Epoch 4 step 1128: training loss: 495.87133408520333\n",
      "Epoch 4 step 1129: training accuarcy: 0.9365\n",
      "Epoch 4 step 1129: training loss: 466.04770171084294\n",
      "Epoch 4 step 1130: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1130: training loss: 502.6790779578909\n",
      "Epoch 4 step 1131: training accuarcy: 0.93\n",
      "Epoch 4 step 1131: training loss: 499.9209573032777\n",
      "Epoch 4 step 1132: training accuarcy: 0.9265\n",
      "Epoch 4 step 1132: training loss: 471.4471025388642\n",
      "Epoch 4 step 1133: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1133: training loss: 505.28503635829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1134: training accuarcy: 0.933\n",
      "Epoch 4 step 1134: training loss: 500.6057543846951\n",
      "Epoch 4 step 1135: training accuarcy: 0.931\n",
      "Epoch 4 step 1135: training loss: 476.81830133964263\n",
      "Epoch 4 step 1136: training accuarcy: 0.9355\n",
      "Epoch 4 step 1136: training loss: 500.23706419609647\n",
      "Epoch 4 step 1137: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1137: training loss: 492.7494687494114\n",
      "Epoch 4 step 1138: training accuarcy: 0.933\n",
      "Epoch 4 step 1138: training loss: 472.97427583483204\n",
      "Epoch 4 step 1139: training accuarcy: 0.9385\n",
      "Epoch 4 step 1139: training loss: 484.9685377202417\n",
      "Epoch 4 step 1140: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1140: training loss: 483.04747723974407\n",
      "Epoch 4 step 1141: training accuarcy: 0.9355\n",
      "Epoch 4 step 1141: training loss: 478.6710343315687\n",
      "Epoch 4 step 1142: training accuarcy: 0.937\n",
      "Epoch 4 step 1142: training loss: 490.88491428158267\n",
      "Epoch 4 step 1143: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1143: training loss: 494.25228382533703\n",
      "Epoch 4 step 1144: training accuarcy: 0.9295\n",
      "Epoch 4 step 1144: training loss: 509.45868677935204\n",
      "Epoch 4 step 1145: training accuarcy: 0.9235\n",
      "Epoch 4 step 1145: training loss: 492.22455720255795\n",
      "Epoch 4 step 1146: training accuarcy: 0.9395\n",
      "Epoch 4 step 1146: training loss: 471.8001596620112\n",
      "Epoch 4 step 1147: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1147: training loss: 494.846420510376\n",
      "Epoch 4 step 1148: training accuarcy: 0.9395\n",
      "Epoch 4 step 1148: training loss: 468.49638585173216\n",
      "Epoch 4 step 1149: training accuarcy: 0.9375\n",
      "Epoch 4 step 1149: training loss: 483.4378160788602\n",
      "Epoch 4 step 1150: training accuarcy: 0.9365\n",
      "Epoch 4 step 1150: training loss: 462.3801238177543\n",
      "Epoch 4 step 1151: training accuarcy: 0.9505\n",
      "Epoch 4 step 1151: training loss: 461.494410407486\n",
      "Epoch 4 step 1152: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1152: training loss: 469.84480631447485\n",
      "Epoch 4 step 1153: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1153: training loss: 464.4749573275054\n",
      "Epoch 4 step 1154: training accuarcy: 0.9435\n",
      "Epoch 4 step 1154: training loss: 479.8925389967162\n",
      "Epoch 4 step 1155: training accuarcy: 0.9365\n",
      "Epoch 4 step 1155: training loss: 469.9890287249264\n",
      "Epoch 4 step 1156: training accuarcy: 0.9435\n",
      "Epoch 4 step 1156: training loss: 486.3382226347667\n",
      "Epoch 4 step 1157: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1157: training loss: 468.401664988185\n",
      "Epoch 4 step 1158: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 1158: training loss: 469.5267514052423\n",
      "Epoch 4 step 1159: training accuarcy: 0.9425\n",
      "Epoch 4 step 1159: training loss: 481.23053195669655\n",
      "Epoch 4 step 1160: training accuarcy: 0.9335\n",
      "Epoch 4 step 1160: training loss: 489.709140763926\n",
      "Epoch 4 step 1161: training accuarcy: 0.9385\n",
      "Epoch 4 step 1161: training loss: 499.8584116968265\n",
      "Epoch 4 step 1162: training accuarcy: 0.9305\n",
      "Epoch 4 step 1162: training loss: 465.1122260206523\n",
      "Epoch 4 step 1163: training accuarcy: 0.9465\n",
      "Epoch 4 step 1163: training loss: 457.73204845113776\n",
      "Epoch 4 step 1164: training accuarcy: 0.9445\n",
      "Epoch 4 step 1164: training loss: 476.8544240600423\n",
      "Epoch 4 step 1165: training accuarcy: 0.9445\n",
      "Epoch 4 step 1165: training loss: 455.3588765323231\n",
      "Epoch 4 step 1166: training accuarcy: 0.9455\n",
      "Epoch 4 step 1166: training loss: 487.3999335480377\n",
      "Epoch 4 step 1167: training accuarcy: 0.934\n",
      "Epoch 4 step 1167: training loss: 461.71594309010914\n",
      "Epoch 4 step 1168: training accuarcy: 0.9445\n",
      "Epoch 4 step 1168: training loss: 470.50076215117264\n",
      "Epoch 4 step 1169: training accuarcy: 0.9395\n",
      "Epoch 4 step 1169: training loss: 468.4199258333835\n",
      "Epoch 4 step 1170: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1170: training loss: 465.5133105714327\n",
      "Epoch 4 step 1171: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1171: training loss: 484.5346706754307\n",
      "Epoch 4 step 1172: training accuarcy: 0.935\n",
      "Epoch 4 step 1172: training loss: 488.02206773670906\n",
      "Epoch 4 step 1173: training accuarcy: 0.926\n",
      "Epoch 4 step 1173: training loss: 471.76880001373735\n",
      "Epoch 4 step 1174: training accuarcy: 0.9365\n",
      "Epoch 4 step 1174: training loss: 472.7614209171197\n",
      "Epoch 4 step 1175: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1175: training loss: 494.3852302811872\n",
      "Epoch 4 step 1176: training accuarcy: 0.93\n",
      "Epoch 4 step 1176: training loss: 483.71007606125175\n",
      "Epoch 4 step 1177: training accuarcy: 0.9375\n",
      "Epoch 4 step 1177: training loss: 479.5029007183392\n",
      "Epoch 4 step 1178: training accuarcy: 0.932\n",
      "Epoch 4 step 1178: training loss: 473.381348554976\n",
      "Epoch 4 step 1179: training accuarcy: 0.9335\n",
      "Epoch 4 step 1179: training loss: 465.11073405430943\n",
      "Epoch 4 step 1180: training accuarcy: 0.9405\n",
      "Epoch 4 step 1180: training loss: 465.328812839948\n",
      "Epoch 4 step 1181: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1181: training loss: 476.50874380373205\n",
      "Epoch 4 step 1182: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1182: training loss: 481.43641422895735\n",
      "Epoch 4 step 1183: training accuarcy: 0.934\n",
      "Epoch 4 step 1183: training loss: 469.7141408969687\n",
      "Epoch 4 step 1184: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1184: training loss: 460.6425719121952\n",
      "Epoch 4 step 1185: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1185: training loss: 462.2993968596558\n",
      "Epoch 4 step 1186: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1186: training loss: 467.80833151042725\n",
      "Epoch 4 step 1187: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1187: training loss: 455.6187874795244\n",
      "Epoch 4 step 1188: training accuarcy: 0.9395\n",
      "Epoch 4 step 1188: training loss: 462.42309359294165\n",
      "Epoch 4 step 1189: training accuarcy: 0.9405\n",
      "Epoch 4 step 1189: training loss: 479.5481842955949\n",
      "Epoch 4 step 1190: training accuarcy: 0.9385\n",
      "Epoch 4 step 1190: training loss: 471.7810265002425\n",
      "Epoch 4 step 1191: training accuarcy: 0.9375\n",
      "Epoch 4 step 1191: training loss: 484.1462238803597\n",
      "Epoch 4 step 1192: training accuarcy: 0.9345\n",
      "Epoch 4 step 1192: training loss: 469.73108678763117\n",
      "Epoch 4 step 1193: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1193: training loss: 466.0396115755104\n",
      "Epoch 4 step 1194: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1194: training loss: 473.7569949764664\n",
      "Epoch 4 step 1195: training accuarcy: 0.9365\n",
      "Epoch 4 step 1195: training loss: 482.44462185981394\n",
      "Epoch 4 step 1196: training accuarcy: 0.936\n",
      "Epoch 4 step 1196: training loss: 484.8918547637384\n",
      "Epoch 4 step 1197: training accuarcy: 0.9395\n",
      "Epoch 4 step 1197: training loss: 481.58321099182996\n",
      "Epoch 4 step 1198: training accuarcy: 0.9425\n",
      "Epoch 4 step 1198: training loss: 482.76267814694137\n",
      "Epoch 4 step 1199: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1199: training loss: 455.09709150075616\n",
      "Epoch 4 step 1200: training accuarcy: 0.9465\n",
      "Epoch 4 step 1200: training loss: 472.7010102197713\n",
      "Epoch 4 step 1201: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1201: training loss: 491.4278390384545\n",
      "Epoch 4 step 1202: training accuarcy: 0.933\n",
      "Epoch 4 step 1202: training loss: 462.1368040257156\n",
      "Epoch 4 step 1203: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1203: training loss: 480.19431698784024\n",
      "Epoch 4 step 1204: training accuarcy: 0.9395\n",
      "Epoch 4 step 1204: training loss: 483.5022266024404\n",
      "Epoch 4 step 1205: training accuarcy: 0.9405\n",
      "Epoch 4 step 1205: training loss: 481.1574636665825\n",
      "Epoch 4 step 1206: training accuarcy: 0.9395\n",
      "Epoch 4 step 1206: training loss: 456.54305548946695\n",
      "Epoch 4 step 1207: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1207: training loss: 455.3013469556173\n",
      "Epoch 4 step 1208: training accuarcy: 0.9525\n",
      "Epoch 4 step 1208: training loss: 506.1382926635429\n",
      "Epoch 4 step 1209: training accuarcy: 0.9275\n",
      "Epoch 4 step 1209: training loss: 490.71657855168803\n",
      "Epoch 4 step 1210: training accuarcy: 0.9375\n",
      "Epoch 4 step 1210: training loss: 474.10651690065816\n",
      "Epoch 4 step 1211: training accuarcy: 0.936\n",
      "Epoch 4 step 1211: training loss: 445.5364072256615\n",
      "Epoch 4 step 1212: training accuarcy: 0.9445\n",
      "Epoch 4 step 1212: training loss: 455.56855665962684\n",
      "Epoch 4 step 1213: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1213: training loss: 460.92138550567165\n",
      "Epoch 4 step 1214: training accuarcy: 0.9445\n",
      "Epoch 4 step 1214: training loss: 447.17951914716593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1215: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 1215: training loss: 469.529197750791\n",
      "Epoch 4 step 1216: training accuarcy: 0.9385\n",
      "Epoch 4 step 1216: training loss: 464.0903094449788\n",
      "Epoch 4 step 1217: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1217: training loss: 444.28034476058025\n",
      "Epoch 4 step 1218: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 1218: training loss: 460.46076550649326\n",
      "Epoch 4 step 1219: training accuarcy: 0.9415\n",
      "Epoch 4 step 1219: training loss: 450.6857025809599\n",
      "Epoch 4 step 1220: training accuarcy: 0.9455\n",
      "Epoch 4 step 1220: training loss: 470.5263183588476\n",
      "Epoch 4 step 1221: training accuarcy: 0.937\n",
      "Epoch 4 step 1221: training loss: 476.2984204497551\n",
      "Epoch 4 step 1222: training accuarcy: 0.9380000000000001\n",
      "Epoch 4 step 1222: training loss: 475.4403628080806\n",
      "Epoch 4 step 1223: training accuarcy: 0.9380000000000001\n",
      "Epoch 4 step 1223: training loss: 457.44684615694365\n",
      "Epoch 4 step 1224: training accuarcy: 0.9495\n",
      "Epoch 4 step 1224: training loss: 471.9558452999809\n",
      "Epoch 4 step 1225: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1225: training loss: 457.7986752798578\n",
      "Epoch 4 step 1226: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1226: training loss: 443.8056574290286\n",
      "Epoch 4 step 1227: training accuarcy: 0.9415\n",
      "Epoch 4 step 1227: training loss: 432.8606430643479\n",
      "Epoch 4 step 1228: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 1228: training loss: 455.7023687379576\n",
      "Epoch 4 step 1229: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1229: training loss: 467.4789656488462\n",
      "Epoch 4 step 1230: training accuarcy: 0.9385\n",
      "Epoch 4 step 1230: training loss: 460.12522795187175\n",
      "Epoch 4 step 1231: training accuarcy: 0.9435\n",
      "Epoch 4 step 1231: training loss: 477.201759595757\n",
      "Epoch 4 step 1232: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1232: training loss: 480.1777416894801\n",
      "Epoch 4 step 1233: training accuarcy: 0.935\n",
      "Epoch 4 step 1233: training loss: 430.9951029202602\n",
      "Epoch 4 step 1234: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 1234: training loss: 474.62740433157296\n",
      "Epoch 4 step 1235: training accuarcy: 0.9375\n",
      "Epoch 4 step 1235: training loss: 455.5872366932029\n",
      "Epoch 4 step 1236: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 1236: training loss: 445.9681893524446\n",
      "Epoch 4 step 1237: training accuarcy: 0.9375\n",
      "Epoch 4 step 1237: training loss: 447.076585326452\n",
      "Epoch 4 step 1238: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 1238: training loss: 461.65606362375235\n",
      "Epoch 4 step 1239: training accuarcy: 0.9395\n",
      "Epoch 4 step 1239: training loss: 442.28539422217716\n",
      "Epoch 4 step 1240: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1240: training loss: 442.1063857898716\n",
      "Epoch 4 step 1241: training accuarcy: 0.9530000000000001\n",
      "Epoch 4 step 1241: training loss: 445.9413995177998\n",
      "Epoch 4 step 1242: training accuarcy: 0.9475\n",
      "Epoch 4 step 1242: training loss: 456.19445187494534\n",
      "Epoch 4 step 1243: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1243: training loss: 476.9939325801484\n",
      "Epoch 4 step 1244: training accuarcy: 0.9405\n",
      "Epoch 4 step 1244: training loss: 457.069751998839\n",
      "Epoch 4 step 1245: training accuarcy: 0.9385\n",
      "Epoch 4 step 1245: training loss: 479.7018867543972\n",
      "Epoch 4 step 1246: training accuarcy: 0.9405\n",
      "Epoch 4 step 1246: training loss: 445.647366195505\n",
      "Epoch 4 step 1247: training accuarcy: 0.9495\n",
      "Epoch 4 step 1247: training loss: 444.0147201748429\n",
      "Epoch 4 step 1248: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1248: training loss: 469.330674067352\n",
      "Epoch 4 step 1249: training accuarcy: 0.9425\n",
      "Epoch 4 step 1249: training loss: 438.6148025956452\n",
      "Epoch 4 step 1250: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1250: training loss: 447.0933075827592\n",
      "Epoch 4 step 1251: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 1251: training loss: 438.42254482038516\n",
      "Epoch 4 step 1252: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1252: training loss: 457.4336590970104\n",
      "Epoch 4 step 1253: training accuarcy: 0.9490000000000001\n",
      "Epoch 4 step 1253: training loss: 438.7206899838776\n",
      "Epoch 4 step 1254: training accuarcy: 0.9445\n",
      "Epoch 4 step 1254: training loss: 468.395995774145\n",
      "Epoch 4 step 1255: training accuarcy: 0.935\n",
      "Epoch 4 step 1255: training loss: 446.9731290611329\n",
      "Epoch 4 step 1256: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1256: training loss: 447.1388504772879\n",
      "Epoch 4 step 1257: training accuarcy: 0.9445\n",
      "Epoch 4 step 1257: training loss: 471.4811762022541\n",
      "Epoch 4 step 1258: training accuarcy: 0.934\n",
      "Epoch 4 step 1258: training loss: 455.10926135566973\n",
      "Epoch 4 step 1259: training accuarcy: 0.9465\n",
      "Epoch 4 step 1259: training loss: 464.69079424067945\n",
      "Epoch 4 step 1260: training accuarcy: 0.9355\n",
      "Epoch 4 step 1260: training loss: 464.67178485267834\n",
      "Epoch 4 step 1261: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1261: training loss: 454.55164745757247\n",
      "Epoch 4 step 1262: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1262: training loss: 451.9843377201054\n",
      "Epoch 4 step 1263: training accuarcy: 0.9395\n",
      "Epoch 4 step 1263: training loss: 443.76614287443607\n",
      "Epoch 4 step 1264: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 1264: training loss: 454.01820049405086\n",
      "Epoch 4 step 1265: training accuarcy: 0.9445\n",
      "Epoch 4 step 1265: training loss: 450.94618384846314\n",
      "Epoch 4 step 1266: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1266: training loss: 450.36609009856767\n",
      "Epoch 4 step 1267: training accuarcy: 0.9445\n",
      "Epoch 4 step 1267: training loss: 465.2033796765868\n",
      "Epoch 4 step 1268: training accuarcy: 0.9405\n",
      "Epoch 4 step 1268: training loss: 444.2031341873577\n",
      "Epoch 4 step 1269: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1269: training loss: 455.9952694442733\n",
      "Epoch 4 step 1270: training accuarcy: 0.9425\n",
      "Epoch 4 step 1270: training loss: 460.0478680698582\n",
      "Epoch 4 step 1271: training accuarcy: 0.937\n",
      "Epoch 4 step 1271: training loss: 440.04450557936565\n",
      "Epoch 4 step 1272: training accuarcy: 0.9425\n",
      "Epoch 4 step 1272: training loss: 434.4513021411015\n",
      "Epoch 4 step 1273: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 1273: training loss: 450.98038773258975\n",
      "Epoch 4 step 1274: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1274: training loss: 437.3272533811769\n",
      "Epoch 4 step 1275: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1275: training loss: 437.2229026668565\n",
      "Epoch 4 step 1276: training accuarcy: 0.9455\n",
      "Epoch 4 step 1276: training loss: 461.2048961424606\n",
      "Epoch 4 step 1277: training accuarcy: 0.935\n",
      "Epoch 4 step 1277: training loss: 428.41042582559135\n",
      "Epoch 4 step 1278: training accuarcy: 0.9455\n",
      "Epoch 4 step 1278: training loss: 444.3160709339044\n",
      "Epoch 4 step 1279: training accuarcy: 0.9395\n",
      "Epoch 4 step 1279: training loss: 455.168858874797\n",
      "Epoch 4 step 1280: training accuarcy: 0.937\n",
      "Epoch 4 step 1280: training loss: 448.90782903199494\n",
      "Epoch 4 step 1281: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1281: training loss: 452.9701473633489\n",
      "Epoch 4 step 1282: training accuarcy: 0.9415\n",
      "Epoch 4 step 1282: training loss: 449.18324693380157\n",
      "Epoch 4 step 1283: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1283: training loss: 435.06067261938495\n",
      "Epoch 4 step 1284: training accuarcy: 0.9445\n",
      "Epoch 4 step 1284: training loss: 441.24530764822333\n",
      "Epoch 4 step 1285: training accuarcy: 0.9510000000000001\n",
      "Epoch 4 step 1285: training loss: 423.47247450575264\n",
      "Epoch 4 step 1286: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 1286: training loss: 430.73200725643744\n",
      "Epoch 4 step 1287: training accuarcy: 0.9540000000000001\n",
      "Epoch 4 step 1287: training loss: 452.13582095990586\n",
      "Epoch 4 step 1288: training accuarcy: 0.9385\n",
      "Epoch 4 step 1288: training loss: 450.76895380224596\n",
      "Epoch 4 step 1289: training accuarcy: 0.9425\n",
      "Epoch 4 step 1289: training loss: 443.1329476891979\n",
      "Epoch 4 step 1290: training accuarcy: 0.9475\n",
      "Epoch 4 step 1290: training loss: 456.5185784517656\n",
      "Epoch 4 step 1291: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1291: training loss: 440.2267482490948\n",
      "Epoch 4 step 1292: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1292: training loss: 442.84806355752863\n",
      "Epoch 4 step 1293: training accuarcy: 0.9530000000000001\n",
      "Epoch 4 step 1293: training loss: 425.1855362044043\n",
      "Epoch 4 step 1294: training accuarcy: 0.9520000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1294: training loss: 437.1273217584302\n",
      "Epoch 4 step 1295: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1295: training loss: 444.1141464316622\n",
      "Epoch 4 step 1296: training accuarcy: 0.9385\n",
      "Epoch 4 step 1296: training loss: 437.65787754953857\n",
      "Epoch 4 step 1297: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1297: training loss: 460.94384804461305\n",
      "Epoch 4 step 1298: training accuarcy: 0.936\n",
      "Epoch 4 step 1298: training loss: 440.6641427777167\n",
      "Epoch 4 step 1299: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1299: training loss: 432.6464711463914\n",
      "Epoch 4 step 1300: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1300: training loss: 443.52887251346385\n",
      "Epoch 4 step 1301: training accuarcy: 0.9435\n",
      "Epoch 4 step 1301: training loss: 435.30653725169236\n",
      "Epoch 4 step 1302: training accuarcy: 0.9505\n",
      "Epoch 4 step 1302: training loss: 427.79114290583607\n",
      "Epoch 4 step 1303: training accuarcy: 0.9465\n",
      "Epoch 4 step 1303: training loss: 429.83220819100416\n",
      "Epoch 4 step 1304: training accuarcy: 0.9435\n",
      "Epoch 4 step 1304: training loss: 472.9038115941386\n",
      "Epoch 4 step 1305: training accuarcy: 0.927\n",
      "Epoch 4 step 1305: training loss: 452.0153038690386\n",
      "Epoch 4 step 1306: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1306: training loss: 438.03376195206914\n",
      "Epoch 4 step 1307: training accuarcy: 0.9495\n",
      "Epoch 4 step 1307: training loss: 466.2403743986378\n",
      "Epoch 4 step 1308: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1308: training loss: 450.1246393626815\n",
      "Epoch 4 step 1309: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1309: training loss: 445.32260632101094\n",
      "Epoch 4 step 1310: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1310: training loss: 437.6925308676833\n",
      "Epoch 4 step 1311: training accuarcy: 0.9500000000000001\n",
      "Epoch 4 step 1311: training loss: 434.6035986935275\n",
      "Epoch 4 step 1312: training accuarcy: 0.9505\n",
      "Epoch 4 step 1312: training loss: 450.9117122959288\n",
      "Epoch 4 step 1313: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1313: training loss: 417.12124505241593\n",
      "Epoch 4 step 1314: training accuarcy: 0.9530000000000001\n",
      "Epoch 4 step 1314: training loss: 201.572068130793\n",
      "Epoch 4 step 1315: training accuarcy: 0.9320512820512821\n",
      "Epoch 4: train loss 470.036798754853, train accuarcy 0.9173548221588135\n",
      "Epoch 4: valid loss 854.5999364682344, valid accuarcy 0.8117040395736694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                         | 5/8 [11:08<06:40, 133.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 1315: training loss: 382.69158540254244\n",
      "Epoch 5 step 1316: training accuarcy: 0.9635\n",
      "Epoch 5 step 1316: training loss: 378.9544995619815\n",
      "Epoch 5 step 1317: training accuarcy: 0.9675\n",
      "Epoch 5 step 1317: training loss: 363.4918250079975\n",
      "Epoch 5 step 1318: training accuarcy: 0.9655\n",
      "Epoch 5 step 1318: training loss: 377.13418737692473\n",
      "Epoch 5 step 1319: training accuarcy: 0.9645\n",
      "Epoch 5 step 1319: training loss: 369.4395039650103\n",
      "Epoch 5 step 1320: training accuarcy: 0.968\n",
      "Epoch 5 step 1320: training loss: 368.33549376382643\n",
      "Epoch 5 step 1321: training accuarcy: 0.967\n",
      "Epoch 5 step 1321: training loss: 376.05285522612644\n",
      "Epoch 5 step 1322: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 1322: training loss: 383.3455247175375\n",
      "Epoch 5 step 1323: training accuarcy: 0.9560000000000001\n",
      "Epoch 5 step 1323: training loss: 394.03600890055907\n",
      "Epoch 5 step 1324: training accuarcy: 0.96\n",
      "Epoch 5 step 1324: training loss: 368.10827830408823\n",
      "Epoch 5 step 1325: training accuarcy: 0.9605\n",
      "Epoch 5 step 1325: training loss: 357.2667778923073\n",
      "Epoch 5 step 1326: training accuarcy: 0.9665\n",
      "Epoch 5 step 1326: training loss: 385.37176866324285\n",
      "Epoch 5 step 1327: training accuarcy: 0.9555\n",
      "Epoch 5 step 1327: training loss: 389.00166681687546\n",
      "Epoch 5 step 1328: training accuarcy: 0.9525\n",
      "Epoch 5 step 1328: training loss: 376.73723843418446\n",
      "Epoch 5 step 1329: training accuarcy: 0.9685\n",
      "Epoch 5 step 1329: training loss: 373.0386316723108\n",
      "Epoch 5 step 1330: training accuarcy: 0.9615\n",
      "Epoch 5 step 1330: training loss: 384.1369031983517\n",
      "Epoch 5 step 1331: training accuarcy: 0.963\n",
      "Epoch 5 step 1331: training loss: 389.1043689828713\n",
      "Epoch 5 step 1332: training accuarcy: 0.964\n",
      "Epoch 5 step 1332: training loss: 380.9112900758134\n",
      "Epoch 5 step 1333: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1333: training loss: 391.9172942477719\n",
      "Epoch 5 step 1334: training accuarcy: 0.9575\n",
      "Epoch 5 step 1334: training loss: 388.8439137647177\n",
      "Epoch 5 step 1335: training accuarcy: 0.9585\n",
      "Epoch 5 step 1335: training loss: 374.87974920685036\n",
      "Epoch 5 step 1336: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1336: training loss: 368.54480448553215\n",
      "Epoch 5 step 1337: training accuarcy: 0.9625\n",
      "Epoch 5 step 1337: training loss: 360.83882997419045\n",
      "Epoch 5 step 1338: training accuarcy: 0.9685\n",
      "Epoch 5 step 1338: training loss: 362.49080658433564\n",
      "Epoch 5 step 1339: training accuarcy: 0.9655\n",
      "Epoch 5 step 1339: training loss: 369.37269297567155\n",
      "Epoch 5 step 1340: training accuarcy: 0.9675\n",
      "Epoch 5 step 1340: training loss: 354.9684758337568\n",
      "Epoch 5 step 1341: training accuarcy: 0.9675\n",
      "Epoch 5 step 1341: training loss: 356.2151078093665\n",
      "Epoch 5 step 1342: training accuarcy: 0.97\n",
      "Epoch 5 step 1342: training loss: 369.45502012874124\n",
      "Epoch 5 step 1343: training accuarcy: 0.965\n",
      "Epoch 5 step 1343: training loss: 385.3505082581804\n",
      "Epoch 5 step 1344: training accuarcy: 0.965\n",
      "Epoch 5 step 1344: training loss: 373.0715085828656\n",
      "Epoch 5 step 1345: training accuarcy: 0.9605\n",
      "Epoch 5 step 1345: training loss: 368.9012939472809\n",
      "Epoch 5 step 1346: training accuarcy: 0.9645\n",
      "Epoch 5 step 1346: training loss: 360.8844567659201\n",
      "Epoch 5 step 1347: training accuarcy: 0.966\n",
      "Epoch 5 step 1347: training loss: 379.11061297692413\n",
      "Epoch 5 step 1348: training accuarcy: 0.962\n",
      "Epoch 5 step 1348: training loss: 342.44666994510885\n",
      "Epoch 5 step 1349: training accuarcy: 0.967\n",
      "Epoch 5 step 1349: training loss: 372.8796569881777\n",
      "Epoch 5 step 1350: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1350: training loss: 368.33157205627987\n",
      "Epoch 5 step 1351: training accuarcy: 0.9635\n",
      "Epoch 5 step 1351: training loss: 371.73619924450577\n",
      "Epoch 5 step 1352: training accuarcy: 0.962\n",
      "Epoch 5 step 1352: training loss: 374.5977572445264\n",
      "Epoch 5 step 1353: training accuarcy: 0.961\n",
      "Epoch 5 step 1353: training loss: 361.19481168317566\n",
      "Epoch 5 step 1354: training accuarcy: 0.9655\n",
      "Epoch 5 step 1354: training loss: 382.75568958326437\n",
      "Epoch 5 step 1355: training accuarcy: 0.9575\n",
      "Epoch 5 step 1355: training loss: 371.43406137498215\n",
      "Epoch 5 step 1356: training accuarcy: 0.9605\n",
      "Epoch 5 step 1356: training loss: 366.161139341541\n",
      "Epoch 5 step 1357: training accuarcy: 0.9625\n",
      "Epoch 5 step 1357: training loss: 365.4045230382417\n",
      "Epoch 5 step 1358: training accuarcy: 0.9605\n",
      "Epoch 5 step 1358: training loss: 376.73902495310557\n",
      "Epoch 5 step 1359: training accuarcy: 0.9585\n",
      "Epoch 5 step 1359: training loss: 343.105716176177\n",
      "Epoch 5 step 1360: training accuarcy: 0.9705\n",
      "Epoch 5 step 1360: training loss: 383.26556953612214\n",
      "Epoch 5 step 1361: training accuarcy: 0.9595\n",
      "Epoch 5 step 1361: training loss: 367.3349134494623\n",
      "Epoch 5 step 1362: training accuarcy: 0.9605\n",
      "Epoch 5 step 1362: training loss: 372.37156609854026\n",
      "Epoch 5 step 1363: training accuarcy: 0.96\n",
      "Epoch 5 step 1363: training loss: 369.0460028449929\n",
      "Epoch 5 step 1364: training accuarcy: 0.962\n",
      "Epoch 5 step 1364: training loss: 372.3194152366676\n",
      "Epoch 5 step 1365: training accuarcy: 0.964\n",
      "Epoch 5 step 1365: training loss: 388.2204354919293\n",
      "Epoch 5 step 1366: training accuarcy: 0.9555\n",
      "Epoch 5 step 1366: training loss: 368.5187463442473\n",
      "Epoch 5 step 1367: training accuarcy: 0.962\n",
      "Epoch 5 step 1367: training loss: 371.0222410766313\n",
      "Epoch 5 step 1368: training accuarcy: 0.9585\n",
      "Epoch 5 step 1368: training loss: 360.6768512767868\n",
      "Epoch 5 step 1369: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 1369: training loss: 360.01559048845655\n",
      "Epoch 5 step 1370: training accuarcy: 0.964\n",
      "Epoch 5 step 1370: training loss: 354.3760515096719\n",
      "Epoch 5 step 1371: training accuarcy: 0.968\n",
      "Epoch 5 step 1371: training loss: 368.5888367295678\n",
      "Epoch 5 step 1372: training accuarcy: 0.963\n",
      "Epoch 5 step 1372: training loss: 361.2251911302937\n",
      "Epoch 5 step 1373: training accuarcy: 0.9625\n",
      "Epoch 5 step 1373: training loss: 389.5425313718113\n",
      "Epoch 5 step 1374: training accuarcy: 0.9555\n",
      "Epoch 5 step 1374: training loss: 359.0013039747652\n",
      "Epoch 5 step 1375: training accuarcy: 0.9675\n",
      "Epoch 5 step 1375: training loss: 376.09999198336124\n",
      "Epoch 5 step 1376: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 1376: training loss: 353.60269003695134\n",
      "Epoch 5 step 1377: training accuarcy: 0.967\n",
      "Epoch 5 step 1377: training loss: 356.32404841826167\n",
      "Epoch 5 step 1378: training accuarcy: 0.965\n",
      "Epoch 5 step 1378: training loss: 353.91622839794195\n",
      "Epoch 5 step 1379: training accuarcy: 0.963\n",
      "Epoch 5 step 1379: training loss: 357.72491450618446\n",
      "Epoch 5 step 1380: training accuarcy: 0.967\n",
      "Epoch 5 step 1380: training loss: 369.8785130250578\n",
      "Epoch 5 step 1381: training accuarcy: 0.964\n",
      "Epoch 5 step 1381: training loss: 367.81600941468423\n",
      "Epoch 5 step 1382: training accuarcy: 0.9575\n",
      "Epoch 5 step 1382: training loss: 356.5015154263381\n",
      "Epoch 5 step 1383: training accuarcy: 0.9695\n",
      "Epoch 5 step 1383: training loss: 369.286776017886\n",
      "Epoch 5 step 1384: training accuarcy: 0.9615\n",
      "Epoch 5 step 1384: training loss: 380.5699752941365\n",
      "Epoch 5 step 1385: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 1385: training loss: 358.44908121216434\n",
      "Epoch 5 step 1386: training accuarcy: 0.965\n",
      "Epoch 5 step 1386: training loss: 355.022224200475\n",
      "Epoch 5 step 1387: training accuarcy: 0.966\n",
      "Epoch 5 step 1387: training loss: 357.78514459163796\n",
      "Epoch 5 step 1388: training accuarcy: 0.969\n",
      "Epoch 5 step 1388: training loss: 349.62268363137645\n",
      "Epoch 5 step 1389: training accuarcy: 0.968\n",
      "Epoch 5 step 1389: training loss: 361.9305219410088\n",
      "Epoch 5 step 1390: training accuarcy: 0.9615\n",
      "Epoch 5 step 1390: training loss: 362.4485247186824\n",
      "Epoch 5 step 1391: training accuarcy: 0.9665\n",
      "Epoch 5 step 1391: training loss: 385.7788082755617\n",
      "Epoch 5 step 1392: training accuarcy: 0.9480000000000001\n",
      "Epoch 5 step 1392: training loss: 364.5551727300041\n",
      "Epoch 5 step 1393: training accuarcy: 0.9615\n",
      "Epoch 5 step 1393: training loss: 354.2507025471991\n",
      "Epoch 5 step 1394: training accuarcy: 0.967\n",
      "Epoch 5 step 1394: training loss: 353.0645352520993\n",
      "Epoch 5 step 1395: training accuarcy: 0.971\n",
      "Epoch 5 step 1395: training loss: 358.2446742843462\n",
      "Epoch 5 step 1396: training accuarcy: 0.9655\n",
      "Epoch 5 step 1396: training loss: 360.2377696111586\n",
      "Epoch 5 step 1397: training accuarcy: 0.963\n",
      "Epoch 5 step 1397: training loss: 376.3593117604212\n",
      "Epoch 5 step 1398: training accuarcy: 0.9580000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1398: training loss: 370.61162255082525\n",
      "Epoch 5 step 1399: training accuarcy: 0.965\n",
      "Epoch 5 step 1399: training loss: 352.5954774327864\n",
      "Epoch 5 step 1400: training accuarcy: 0.9655\n",
      "Epoch 5 step 1400: training loss: 372.20125989883155\n",
      "Epoch 5 step 1401: training accuarcy: 0.9605\n",
      "Epoch 5 step 1401: training loss: 367.08795141030123\n",
      "Epoch 5 step 1402: training accuarcy: 0.96\n",
      "Epoch 5 step 1402: training loss: 354.8000940037484\n",
      "Epoch 5 step 1403: training accuarcy: 0.968\n",
      "Epoch 5 step 1403: training loss: 367.3864738985663\n",
      "Epoch 5 step 1404: training accuarcy: 0.9585\n",
      "Epoch 5 step 1404: training loss: 358.9317877069067\n",
      "Epoch 5 step 1405: training accuarcy: 0.967\n",
      "Epoch 5 step 1405: training loss: 366.36194844782074\n",
      "Epoch 5 step 1406: training accuarcy: 0.9605\n",
      "Epoch 5 step 1406: training loss: 371.16212999958697\n",
      "Epoch 5 step 1407: training accuarcy: 0.9575\n",
      "Epoch 5 step 1407: training loss: 349.032322132999\n",
      "Epoch 5 step 1408: training accuarcy: 0.965\n",
      "Epoch 5 step 1408: training loss: 358.2526190063952\n",
      "Epoch 5 step 1409: training accuarcy: 0.966\n",
      "Epoch 5 step 1409: training loss: 364.61490573103475\n",
      "Epoch 5 step 1410: training accuarcy: 0.963\n",
      "Epoch 5 step 1410: training loss: 367.0863498712427\n",
      "Epoch 5 step 1411: training accuarcy: 0.9605\n",
      "Epoch 5 step 1411: training loss: 343.38363423700736\n",
      "Epoch 5 step 1412: training accuarcy: 0.97\n",
      "Epoch 5 step 1412: training loss: 357.2337516810921\n",
      "Epoch 5 step 1413: training accuarcy: 0.963\n",
      "Epoch 5 step 1413: training loss: 366.47524018676086\n",
      "Epoch 5 step 1414: training accuarcy: 0.9575\n",
      "Epoch 5 step 1414: training loss: 350.403840332308\n",
      "Epoch 5 step 1415: training accuarcy: 0.967\n",
      "Epoch 5 step 1415: training loss: 363.8162357412618\n",
      "Epoch 5 step 1416: training accuarcy: 0.963\n",
      "Epoch 5 step 1416: training loss: 364.63344048445055\n",
      "Epoch 5 step 1417: training accuarcy: 0.9625\n",
      "Epoch 5 step 1417: training loss: 361.16233400629164\n",
      "Epoch 5 step 1418: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1418: training loss: 388.7908599634804\n",
      "Epoch 5 step 1419: training accuarcy: 0.9500000000000001\n",
      "Epoch 5 step 1419: training loss: 346.6927080073898\n",
      "Epoch 5 step 1420: training accuarcy: 0.972\n",
      "Epoch 5 step 1420: training loss: 358.27901023607683\n",
      "Epoch 5 step 1421: training accuarcy: 0.9625\n",
      "Epoch 5 step 1421: training loss: 361.0686723852575\n",
      "Epoch 5 step 1422: training accuarcy: 0.963\n",
      "Epoch 5 step 1422: training loss: 352.0415604391552\n",
      "Epoch 5 step 1423: training accuarcy: 0.966\n",
      "Epoch 5 step 1423: training loss: 361.1486457862621\n",
      "Epoch 5 step 1424: training accuarcy: 0.966\n",
      "Epoch 5 step 1424: training loss: 347.2244688368544\n",
      "Epoch 5 step 1425: training accuarcy: 0.966\n",
      "Epoch 5 step 1425: training loss: 376.1165733419272\n",
      "Epoch 5 step 1426: training accuarcy: 0.96\n",
      "Epoch 5 step 1426: training loss: 377.4489074725743\n",
      "Epoch 5 step 1427: training accuarcy: 0.9605\n",
      "Epoch 5 step 1427: training loss: 346.4394765900143\n",
      "Epoch 5 step 1428: training accuarcy: 0.966\n",
      "Epoch 5 step 1428: training loss: 354.1851974650243\n",
      "Epoch 5 step 1429: training accuarcy: 0.969\n",
      "Epoch 5 step 1429: training loss: 360.65270195116494\n",
      "Epoch 5 step 1430: training accuarcy: 0.965\n",
      "Epoch 5 step 1430: training loss: 370.83477884705684\n",
      "Epoch 5 step 1431: training accuarcy: 0.9565\n",
      "Epoch 5 step 1431: training loss: 365.1891086383447\n",
      "Epoch 5 step 1432: training accuarcy: 0.965\n",
      "Epoch 5 step 1432: training loss: 357.43189153602316\n",
      "Epoch 5 step 1433: training accuarcy: 0.964\n",
      "Epoch 5 step 1433: training loss: 350.93545405002556\n",
      "Epoch 5 step 1434: training accuarcy: 0.964\n",
      "Epoch 5 step 1434: training loss: 347.36028082263385\n",
      "Epoch 5 step 1435: training accuarcy: 0.9655\n",
      "Epoch 5 step 1435: training loss: 351.72863533377955\n",
      "Epoch 5 step 1436: training accuarcy: 0.9665\n",
      "Epoch 5 step 1436: training loss: 362.7678980001666\n",
      "Epoch 5 step 1437: training accuarcy: 0.963\n",
      "Epoch 5 step 1437: training loss: 348.38045479655256\n",
      "Epoch 5 step 1438: training accuarcy: 0.963\n",
      "Epoch 5 step 1438: training loss: 351.14206135710157\n",
      "Epoch 5 step 1439: training accuarcy: 0.969\n",
      "Epoch 5 step 1439: training loss: 347.14807425850336\n",
      "Epoch 5 step 1440: training accuarcy: 0.97\n",
      "Epoch 5 step 1440: training loss: 359.9599815866347\n",
      "Epoch 5 step 1441: training accuarcy: 0.96\n",
      "Epoch 5 step 1441: training loss: 351.43981631212944\n",
      "Epoch 5 step 1442: training accuarcy: 0.965\n",
      "Epoch 5 step 1442: training loss: 343.63756342483043\n",
      "Epoch 5 step 1443: training accuarcy: 0.968\n",
      "Epoch 5 step 1443: training loss: 350.4423985600214\n",
      "Epoch 5 step 1444: training accuarcy: 0.9675\n",
      "Epoch 5 step 1444: training loss: 351.9021356778893\n",
      "Epoch 5 step 1445: training accuarcy: 0.9655\n",
      "Epoch 5 step 1445: training loss: 363.8479465401379\n",
      "Epoch 5 step 1446: training accuarcy: 0.9595\n",
      "Epoch 5 step 1446: training loss: 357.37071484099357\n",
      "Epoch 5 step 1447: training accuarcy: 0.962\n",
      "Epoch 5 step 1447: training loss: 350.4840675754159\n",
      "Epoch 5 step 1448: training accuarcy: 0.964\n",
      "Epoch 5 step 1448: training loss: 372.1700948606445\n",
      "Epoch 5 step 1449: training accuarcy: 0.9535\n",
      "Epoch 5 step 1449: training loss: 352.5597559063592\n",
      "Epoch 5 step 1450: training accuarcy: 0.966\n",
      "Epoch 5 step 1450: training loss: 357.821348152241\n",
      "Epoch 5 step 1451: training accuarcy: 0.9595\n",
      "Epoch 5 step 1451: training loss: 358.10026354836805\n",
      "Epoch 5 step 1452: training accuarcy: 0.9625\n",
      "Epoch 5 step 1452: training loss: 338.15404411957724\n",
      "Epoch 5 step 1453: training accuarcy: 0.967\n",
      "Epoch 5 step 1453: training loss: 365.36176451532344\n",
      "Epoch 5 step 1454: training accuarcy: 0.9595\n",
      "Epoch 5 step 1454: training loss: 352.92065893081696\n",
      "Epoch 5 step 1455: training accuarcy: 0.9635\n",
      "Epoch 5 step 1455: training loss: 362.06760328642105\n",
      "Epoch 5 step 1456: training accuarcy: 0.9585\n",
      "Epoch 5 step 1456: training loss: 364.94957818809644\n",
      "Epoch 5 step 1457: training accuarcy: 0.9585\n",
      "Epoch 5 step 1457: training loss: 373.93476899939367\n",
      "Epoch 5 step 1458: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1458: training loss: 344.8152491998192\n",
      "Epoch 5 step 1459: training accuarcy: 0.9655\n",
      "Epoch 5 step 1459: training loss: 339.2026520154003\n",
      "Epoch 5 step 1460: training accuarcy: 0.9695\n",
      "Epoch 5 step 1460: training loss: 357.27123623249673\n",
      "Epoch 5 step 1461: training accuarcy: 0.9635\n",
      "Epoch 5 step 1461: training loss: 350.86821191564087\n",
      "Epoch 5 step 1462: training accuarcy: 0.9635\n",
      "Epoch 5 step 1462: training loss: 346.0553484415009\n",
      "Epoch 5 step 1463: training accuarcy: 0.968\n",
      "Epoch 5 step 1463: training loss: 364.28233047747574\n",
      "Epoch 5 step 1464: training accuarcy: 0.9595\n",
      "Epoch 5 step 1464: training loss: 349.1902274971392\n",
      "Epoch 5 step 1465: training accuarcy: 0.963\n",
      "Epoch 5 step 1465: training loss: 344.95552818493854\n",
      "Epoch 5 step 1466: training accuarcy: 0.9695\n",
      "Epoch 5 step 1466: training loss: 356.8022948868434\n",
      "Epoch 5 step 1467: training accuarcy: 0.9685\n",
      "Epoch 5 step 1467: training loss: 350.7642699287267\n",
      "Epoch 5 step 1468: training accuarcy: 0.97\n",
      "Epoch 5 step 1468: training loss: 358.64813680716736\n",
      "Epoch 5 step 1469: training accuarcy: 0.961\n",
      "Epoch 5 step 1469: training loss: 341.3574511392756\n",
      "Epoch 5 step 1470: training accuarcy: 0.9705\n",
      "Epoch 5 step 1470: training loss: 336.09941768972317\n",
      "Epoch 5 step 1471: training accuarcy: 0.967\n",
      "Epoch 5 step 1471: training loss: 324.04923193690394\n",
      "Epoch 5 step 1472: training accuarcy: 0.9685\n",
      "Epoch 5 step 1472: training loss: 357.57086938870316\n",
      "Epoch 5 step 1473: training accuarcy: 0.9625\n",
      "Epoch 5 step 1473: training loss: 334.6831816276475\n",
      "Epoch 5 step 1474: training accuarcy: 0.9685\n",
      "Epoch 5 step 1474: training loss: 339.56200030400186\n",
      "Epoch 5 step 1475: training accuarcy: 0.968\n",
      "Epoch 5 step 1475: training loss: 344.5076804043756\n",
      "Epoch 5 step 1476: training accuarcy: 0.9645\n",
      "Epoch 5 step 1476: training loss: 341.9376039661485\n",
      "Epoch 5 step 1477: training accuarcy: 0.968\n",
      "Epoch 5 step 1477: training loss: 342.7846843922527\n",
      "Epoch 5 step 1478: training accuarcy: 0.971\n",
      "Epoch 5 step 1478: training loss: 353.97350463684853\n",
      "Epoch 5 step 1479: training accuarcy: 0.9605\n",
      "Epoch 5 step 1479: training loss: 338.2625325923947\n",
      "Epoch 5 step 1480: training accuarcy: 0.9645\n",
      "Epoch 5 step 1480: training loss: 329.70009437969753\n",
      "Epoch 5 step 1481: training accuarcy: 0.9685\n",
      "Epoch 5 step 1481: training loss: 350.97892265134107\n",
      "Epoch 5 step 1482: training accuarcy: 0.9635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1482: training loss: 350.86489094437417\n",
      "Epoch 5 step 1483: training accuarcy: 0.9585\n",
      "Epoch 5 step 1483: training loss: 347.93031699317316\n",
      "Epoch 5 step 1484: training accuarcy: 0.969\n",
      "Epoch 5 step 1484: training loss: 342.0816574469401\n",
      "Epoch 5 step 1485: training accuarcy: 0.9645\n",
      "Epoch 5 step 1485: training loss: 350.72908298711667\n",
      "Epoch 5 step 1486: training accuarcy: 0.9595\n",
      "Epoch 5 step 1486: training loss: 360.46153950730786\n",
      "Epoch 5 step 1487: training accuarcy: 0.9635\n",
      "Epoch 5 step 1487: training loss: 341.6690652410034\n",
      "Epoch 5 step 1488: training accuarcy: 0.964\n",
      "Epoch 5 step 1488: training loss: 324.1891203548594\n",
      "Epoch 5 step 1489: training accuarcy: 0.965\n",
      "Epoch 5 step 1489: training loss: 355.0459323375949\n",
      "Epoch 5 step 1490: training accuarcy: 0.9625\n",
      "Epoch 5 step 1490: training loss: 344.19741632898473\n",
      "Epoch 5 step 1491: training accuarcy: 0.965\n",
      "Epoch 5 step 1491: training loss: 335.5746837353208\n",
      "Epoch 5 step 1492: training accuarcy: 0.9625\n",
      "Epoch 5 step 1492: training loss: 338.1034167076275\n",
      "Epoch 5 step 1493: training accuarcy: 0.9685\n",
      "Epoch 5 step 1493: training loss: 335.0188969287787\n",
      "Epoch 5 step 1494: training accuarcy: 0.9645\n",
      "Epoch 5 step 1494: training loss: 346.4105404026589\n",
      "Epoch 5 step 1495: training accuarcy: 0.9655\n",
      "Epoch 5 step 1495: training loss: 332.464994823811\n",
      "Epoch 5 step 1496: training accuarcy: 0.968\n",
      "Epoch 5 step 1496: training loss: 346.2958350368761\n",
      "Epoch 5 step 1497: training accuarcy: 0.9605\n",
      "Epoch 5 step 1497: training loss: 333.2902438443681\n",
      "Epoch 5 step 1498: training accuarcy: 0.969\n",
      "Epoch 5 step 1498: training loss: 342.4445639849189\n",
      "Epoch 5 step 1499: training accuarcy: 0.9635\n",
      "Epoch 5 step 1499: training loss: 349.3626434533507\n",
      "Epoch 5 step 1500: training accuarcy: 0.964\n",
      "Epoch 5 step 1500: training loss: 336.15056097876726\n",
      "Epoch 5 step 1501: training accuarcy: 0.9645\n",
      "Epoch 5 step 1501: training loss: 343.86075355880587\n",
      "Epoch 5 step 1502: training accuarcy: 0.963\n",
      "Epoch 5 step 1502: training loss: 342.7205490871597\n",
      "Epoch 5 step 1503: training accuarcy: 0.9635\n",
      "Epoch 5 step 1503: training loss: 340.7326435804172\n",
      "Epoch 5 step 1504: training accuarcy: 0.9665\n",
      "Epoch 5 step 1504: training loss: 333.1061582602347\n",
      "Epoch 5 step 1505: training accuarcy: 0.9655\n",
      "Epoch 5 step 1505: training loss: 329.88571735466076\n",
      "Epoch 5 step 1506: training accuarcy: 0.9685\n",
      "Epoch 5 step 1506: training loss: 335.7823331980342\n",
      "Epoch 5 step 1507: training accuarcy: 0.9675\n",
      "Epoch 5 step 1507: training loss: 351.8194668421435\n",
      "Epoch 5 step 1508: training accuarcy: 0.965\n",
      "Epoch 5 step 1508: training loss: 348.0018739878736\n",
      "Epoch 5 step 1509: training accuarcy: 0.9625\n",
      "Epoch 5 step 1509: training loss: 355.3727269803479\n",
      "Epoch 5 step 1510: training accuarcy: 0.9605\n",
      "Epoch 5 step 1510: training loss: 335.32163987213687\n",
      "Epoch 5 step 1511: training accuarcy: 0.964\n",
      "Epoch 5 step 1511: training loss: 340.05386802073724\n",
      "Epoch 5 step 1512: training accuarcy: 0.9645\n",
      "Epoch 5 step 1512: training loss: 338.560891280071\n",
      "Epoch 5 step 1513: training accuarcy: 0.965\n",
      "Epoch 5 step 1513: training loss: 362.6283887938932\n",
      "Epoch 5 step 1514: training accuarcy: 0.9575\n",
      "Epoch 5 step 1514: training loss: 343.53027367427313\n",
      "Epoch 5 step 1515: training accuarcy: 0.966\n",
      "Epoch 5 step 1515: training loss: 329.74007239110614\n",
      "Epoch 5 step 1516: training accuarcy: 0.967\n",
      "Epoch 5 step 1516: training loss: 343.9430919658257\n",
      "Epoch 5 step 1517: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1517: training loss: 322.3671456669002\n",
      "Epoch 5 step 1518: training accuarcy: 0.971\n",
      "Epoch 5 step 1518: training loss: 341.6984182377673\n",
      "Epoch 5 step 1519: training accuarcy: 0.9615\n",
      "Epoch 5 step 1519: training loss: 331.26108093457293\n",
      "Epoch 5 step 1520: training accuarcy: 0.9655\n",
      "Epoch 5 step 1520: training loss: 350.99262080280755\n",
      "Epoch 5 step 1521: training accuarcy: 0.9625\n",
      "Epoch 5 step 1521: training loss: 336.95114224270924\n",
      "Epoch 5 step 1522: training accuarcy: 0.9655\n",
      "Epoch 5 step 1522: training loss: 350.53343372401747\n",
      "Epoch 5 step 1523: training accuarcy: 0.9645\n",
      "Epoch 5 step 1523: training loss: 330.5800437299222\n",
      "Epoch 5 step 1524: training accuarcy: 0.971\n",
      "Epoch 5 step 1524: training loss: 352.305155191768\n",
      "Epoch 5 step 1525: training accuarcy: 0.9595\n",
      "Epoch 5 step 1525: training loss: 328.62914772991826\n",
      "Epoch 5 step 1526: training accuarcy: 0.966\n",
      "Epoch 5 step 1526: training loss: 344.1184476856398\n",
      "Epoch 5 step 1527: training accuarcy: 0.964\n",
      "Epoch 5 step 1527: training loss: 354.53197745353674\n",
      "Epoch 5 step 1528: training accuarcy: 0.963\n",
      "Epoch 5 step 1528: training loss: 335.6298122138025\n",
      "Epoch 5 step 1529: training accuarcy: 0.9685\n",
      "Epoch 5 step 1529: training loss: 332.8333117666503\n",
      "Epoch 5 step 1530: training accuarcy: 0.965\n",
      "Epoch 5 step 1530: training loss: 337.43335420933244\n",
      "Epoch 5 step 1531: training accuarcy: 0.971\n",
      "Epoch 5 step 1531: training loss: 325.0814883780845\n",
      "Epoch 5 step 1532: training accuarcy: 0.9645\n",
      "Epoch 5 step 1532: training loss: 346.50785808904993\n",
      "Epoch 5 step 1533: training accuarcy: 0.9665\n",
      "Epoch 5 step 1533: training loss: 342.53403252141715\n",
      "Epoch 5 step 1534: training accuarcy: 0.9655\n",
      "Epoch 5 step 1534: training loss: 348.2689945506066\n",
      "Epoch 5 step 1535: training accuarcy: 0.964\n",
      "Epoch 5 step 1535: training loss: 334.74345573684013\n",
      "Epoch 5 step 1536: training accuarcy: 0.9655\n",
      "Epoch 5 step 1536: training loss: 328.0241075823676\n",
      "Epoch 5 step 1537: training accuarcy: 0.9655\n",
      "Epoch 5 step 1537: training loss: 337.72831625379587\n",
      "Epoch 5 step 1538: training accuarcy: 0.968\n",
      "Epoch 5 step 1538: training loss: 332.51093122247914\n",
      "Epoch 5 step 1539: training accuarcy: 0.9685\n",
      "Epoch 5 step 1539: training loss: 337.27816914084326\n",
      "Epoch 5 step 1540: training accuarcy: 0.965\n",
      "Epoch 5 step 1540: training loss: 338.32348950910455\n",
      "Epoch 5 step 1541: training accuarcy: 0.966\n",
      "Epoch 5 step 1541: training loss: 349.0463752040977\n",
      "Epoch 5 step 1542: training accuarcy: 0.9625\n",
      "Epoch 5 step 1542: training loss: 359.5964860427521\n",
      "Epoch 5 step 1543: training accuarcy: 0.9585\n",
      "Epoch 5 step 1543: training loss: 349.0503275363311\n",
      "Epoch 5 step 1544: training accuarcy: 0.9635\n",
      "Epoch 5 step 1544: training loss: 341.37754142712276\n",
      "Epoch 5 step 1545: training accuarcy: 0.9635\n",
      "Epoch 5 step 1545: training loss: 309.3950904778985\n",
      "Epoch 5 step 1546: training accuarcy: 0.9695\n",
      "Epoch 5 step 1546: training loss: 329.30930155212536\n",
      "Epoch 5 step 1547: training accuarcy: 0.9655\n",
      "Epoch 5 step 1547: training loss: 325.33541023890893\n",
      "Epoch 5 step 1548: training accuarcy: 0.971\n",
      "Epoch 5 step 1548: training loss: 344.0563164962989\n",
      "Epoch 5 step 1549: training accuarcy: 0.9570000000000001\n",
      "Epoch 5 step 1549: training loss: 332.2532272002021\n",
      "Epoch 5 step 1550: training accuarcy: 0.9605\n",
      "Epoch 5 step 1550: training loss: 338.233560893496\n",
      "Epoch 5 step 1551: training accuarcy: 0.9655\n",
      "Epoch 5 step 1551: training loss: 327.5752345031427\n",
      "Epoch 5 step 1552: training accuarcy: 0.9685\n",
      "Epoch 5 step 1552: training loss: 333.3265750704217\n",
      "Epoch 5 step 1553: training accuarcy: 0.97\n",
      "Epoch 5 step 1553: training loss: 318.0734818788854\n",
      "Epoch 5 step 1554: training accuarcy: 0.9685\n",
      "Epoch 5 step 1554: training loss: 353.13880498379143\n",
      "Epoch 5 step 1555: training accuarcy: 0.96\n",
      "Epoch 5 step 1555: training loss: 327.01647990660365\n",
      "Epoch 5 step 1556: training accuarcy: 0.9695\n",
      "Epoch 5 step 1556: training loss: 346.44596437130934\n",
      "Epoch 5 step 1557: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1557: training loss: 321.1964847613762\n",
      "Epoch 5 step 1558: training accuarcy: 0.9665\n",
      "Epoch 5 step 1558: training loss: 336.53292805264346\n",
      "Epoch 5 step 1559: training accuarcy: 0.9675\n",
      "Epoch 5 step 1559: training loss: 329.35187399134145\n",
      "Epoch 5 step 1560: training accuarcy: 0.9645\n",
      "Epoch 5 step 1560: training loss: 330.88553322705815\n",
      "Epoch 5 step 1561: training accuarcy: 0.9725\n",
      "Epoch 5 step 1561: training loss: 327.8326955934555\n",
      "Epoch 5 step 1562: training accuarcy: 0.971\n",
      "Epoch 5 step 1562: training loss: 330.4411393714402\n",
      "Epoch 5 step 1563: training accuarcy: 0.9665\n",
      "Epoch 5 step 1563: training loss: 309.48608623280785\n",
      "Epoch 5 step 1564: training accuarcy: 0.975\n",
      "Epoch 5 step 1564: training loss: 326.4398962341057\n",
      "Epoch 5 step 1565: training accuarcy: 0.9705\n",
      "Epoch 5 step 1565: training loss: 325.726141946895\n",
      "Epoch 5 step 1566: training accuarcy: 0.9685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1566: training loss: 319.46091384713014\n",
      "Epoch 5 step 1567: training accuarcy: 0.9705\n",
      "Epoch 5 step 1567: training loss: 336.4508112412377\n",
      "Epoch 5 step 1568: training accuarcy: 0.9665\n",
      "Epoch 5 step 1568: training loss: 323.92785401255634\n",
      "Epoch 5 step 1569: training accuarcy: 0.9715\n",
      "Epoch 5 step 1569: training loss: 339.8402999860718\n",
      "Epoch 5 step 1570: training accuarcy: 0.963\n",
      "Epoch 5 step 1570: training loss: 341.8542358580971\n",
      "Epoch 5 step 1571: training accuarcy: 0.961\n",
      "Epoch 5 step 1571: training loss: 339.8648487810889\n",
      "Epoch 5 step 1572: training accuarcy: 0.9625\n",
      "Epoch 5 step 1572: training loss: 331.38994079883923\n",
      "Epoch 5 step 1573: training accuarcy: 0.969\n",
      "Epoch 5 step 1573: training loss: 320.7305441607089\n",
      "Epoch 5 step 1574: training accuarcy: 0.969\n",
      "Epoch 5 step 1574: training loss: 324.4055315365067\n",
      "Epoch 5 step 1575: training accuarcy: 0.969\n",
      "Epoch 5 step 1575: training loss: 311.67969606415227\n",
      "Epoch 5 step 1576: training accuarcy: 0.9695\n",
      "Epoch 5 step 1576: training loss: 324.06062842315373\n",
      "Epoch 5 step 1577: training accuarcy: 0.966\n",
      "Epoch 5 step 1577: training loss: 140.02689109158143\n",
      "Epoch 5 step 1578: training accuarcy: 0.9692307692307692\n",
      "Epoch 5: train loss 352.3288985248026, train accuarcy 0.9495946168899536\n",
      "Epoch 5: valid loss 802.4176493329032, valid accuarcy 0.8263593912124634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                      | 6/8 [13:22<04:27, 133.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 1578: training loss: 270.0170837335595\n",
      "Epoch 6 step 1579: training accuarcy: 0.9805\n",
      "Epoch 6 step 1579: training loss: 281.39022914885066\n",
      "Epoch 6 step 1580: training accuarcy: 0.9825\n",
      "Epoch 6 step 1580: training loss: 287.0902149166919\n",
      "Epoch 6 step 1581: training accuarcy: 0.9765\n",
      "Epoch 6 step 1581: training loss: 276.83760292112345\n",
      "Epoch 6 step 1582: training accuarcy: 0.979\n",
      "Epoch 6 step 1582: training loss: 282.16422260623045\n",
      "Epoch 6 step 1583: training accuarcy: 0.977\n",
      "Epoch 6 step 1583: training loss: 294.42673460693646\n",
      "Epoch 6 step 1584: training accuarcy: 0.976\n",
      "Epoch 6 step 1584: training loss: 268.28140452610694\n",
      "Epoch 6 step 1585: training accuarcy: 0.984\n",
      "Epoch 6 step 1585: training loss: 280.8093418639373\n",
      "Epoch 6 step 1586: training accuarcy: 0.9765\n",
      "Epoch 6 step 1586: training loss: 280.86573416291475\n",
      "Epoch 6 step 1587: training accuarcy: 0.9765\n",
      "Epoch 6 step 1587: training loss: 280.5462040540863\n",
      "Epoch 6 step 1588: training accuarcy: 0.9765\n",
      "Epoch 6 step 1588: training loss: 285.58117621949214\n",
      "Epoch 6 step 1589: training accuarcy: 0.976\n",
      "Epoch 6 step 1589: training loss: 272.7977748299742\n",
      "Epoch 6 step 1590: training accuarcy: 0.9805\n",
      "Epoch 6 step 1590: training loss: 278.9236678338517\n",
      "Epoch 6 step 1591: training accuarcy: 0.9835\n",
      "Epoch 6 step 1591: training loss: 281.63060565653115\n",
      "Epoch 6 step 1592: training accuarcy: 0.974\n",
      "Epoch 6 step 1592: training loss: 280.938081211283\n",
      "Epoch 6 step 1593: training accuarcy: 0.977\n",
      "Epoch 6 step 1593: training loss: 276.081406316355\n",
      "Epoch 6 step 1594: training accuarcy: 0.9785\n",
      "Epoch 6 step 1594: training loss: 280.160700200939\n",
      "Epoch 6 step 1595: training accuarcy: 0.974\n",
      "Epoch 6 step 1595: training loss: 286.25082377251096\n",
      "Epoch 6 step 1596: training accuarcy: 0.9795\n",
      "Epoch 6 step 1596: training loss: 278.172627516598\n",
      "Epoch 6 step 1597: training accuarcy: 0.9765\n",
      "Epoch 6 step 1597: training loss: 270.34142155204603\n",
      "Epoch 6 step 1598: training accuarcy: 0.981\n",
      "Epoch 6 step 1598: training loss: 268.6250639141191\n",
      "Epoch 6 step 1599: training accuarcy: 0.98\n",
      "Epoch 6 step 1599: training loss: 271.5184000115381\n",
      "Epoch 6 step 1600: training accuarcy: 0.979\n",
      "Epoch 6 step 1600: training loss: 278.26316874313073\n",
      "Epoch 6 step 1601: training accuarcy: 0.976\n",
      "Epoch 6 step 1601: training loss: 272.1294241754911\n",
      "Epoch 6 step 1602: training accuarcy: 0.983\n",
      "Epoch 6 step 1602: training loss: 264.8159012468623\n",
      "Epoch 6 step 1603: training accuarcy: 0.984\n",
      "Epoch 6 step 1603: training loss: 266.9353363377707\n",
      "Epoch 6 step 1604: training accuarcy: 0.9775\n",
      "Epoch 6 step 1604: training loss: 281.7272305184764\n",
      "Epoch 6 step 1605: training accuarcy: 0.98\n",
      "Epoch 6 step 1605: training loss: 273.9934651110955\n",
      "Epoch 6 step 1606: training accuarcy: 0.9775\n",
      "Epoch 6 step 1606: training loss: 267.32707268208463\n",
      "Epoch 6 step 1607: training accuarcy: 0.981\n",
      "Epoch 6 step 1607: training loss: 271.48881802503956\n",
      "Epoch 6 step 1608: training accuarcy: 0.977\n",
      "Epoch 6 step 1608: training loss: 276.995267815898\n",
      "Epoch 6 step 1609: training accuarcy: 0.977\n",
      "Epoch 6 step 1609: training loss: 278.45708787611403\n",
      "Epoch 6 step 1610: training accuarcy: 0.978\n",
      "Epoch 6 step 1610: training loss: 276.74235749435167\n",
      "Epoch 6 step 1611: training accuarcy: 0.9745\n",
      "Epoch 6 step 1611: training loss: 283.9929408560065\n",
      "Epoch 6 step 1612: training accuarcy: 0.9745\n",
      "Epoch 6 step 1612: training loss: 256.25900264692007\n",
      "Epoch 6 step 1613: training accuarcy: 0.9815\n",
      "Epoch 6 step 1613: training loss: 280.75294194195027\n",
      "Epoch 6 step 1614: training accuarcy: 0.9795\n",
      "Epoch 6 step 1614: training loss: 258.2171078895964\n",
      "Epoch 6 step 1615: training accuarcy: 0.983\n",
      "Epoch 6 step 1615: training loss: 259.6775768147659\n",
      "Epoch 6 step 1616: training accuarcy: 0.986\n",
      "Epoch 6 step 1616: training loss: 273.8932175755521\n",
      "Epoch 6 step 1617: training accuarcy: 0.9775\n",
      "Epoch 6 step 1617: training loss: 263.3175353296605\n",
      "Epoch 6 step 1618: training accuarcy: 0.981\n",
      "Epoch 6 step 1618: training loss: 274.1495953280165\n",
      "Epoch 6 step 1619: training accuarcy: 0.9805\n",
      "Epoch 6 step 1619: training loss: 276.1791956182646\n",
      "Epoch 6 step 1620: training accuarcy: 0.9765\n",
      "Epoch 6 step 1620: training loss: 271.2131629556698\n",
      "Epoch 6 step 1621: training accuarcy: 0.9765\n",
      "Epoch 6 step 1621: training loss: 285.3706027248065\n",
      "Epoch 6 step 1622: training accuarcy: 0.974\n",
      "Epoch 6 step 1622: training loss: 255.44989844867953\n",
      "Epoch 6 step 1623: training accuarcy: 0.9805\n",
      "Epoch 6 step 1623: training loss: 263.8870007709947\n",
      "Epoch 6 step 1624: training accuarcy: 0.982\n",
      "Epoch 6 step 1624: training loss: 291.00834384074847\n",
      "Epoch 6 step 1625: training accuarcy: 0.9695\n",
      "Epoch 6 step 1625: training loss: 262.89219324377416\n",
      "Epoch 6 step 1626: training accuarcy: 0.9815\n",
      "Epoch 6 step 1626: training loss: 268.46857077812706\n",
      "Epoch 6 step 1627: training accuarcy: 0.9815\n",
      "Epoch 6 step 1627: training loss: 273.89211195650813\n",
      "Epoch 6 step 1628: training accuarcy: 0.979\n",
      "Epoch 6 step 1628: training loss: 273.69120054914515\n",
      "Epoch 6 step 1629: training accuarcy: 0.98\n",
      "Epoch 6 step 1629: training loss: 279.17479742511904\n",
      "Epoch 6 step 1630: training accuarcy: 0.977\n",
      "Epoch 6 step 1630: training loss: 271.3598788190899\n",
      "Epoch 6 step 1631: training accuarcy: 0.981\n",
      "Epoch 6 step 1631: training loss: 267.33602078991305\n",
      "Epoch 6 step 1632: training accuarcy: 0.979\n",
      "Epoch 6 step 1632: training loss: 267.5690667862284\n",
      "Epoch 6 step 1633: training accuarcy: 0.98\n",
      "Epoch 6 step 1633: training loss: 270.0686805463954\n",
      "Epoch 6 step 1634: training accuarcy: 0.978\n",
      "Epoch 6 step 1634: training loss: 276.1315232823255\n",
      "Epoch 6 step 1635: training accuarcy: 0.979\n",
      "Epoch 6 step 1635: training loss: 283.1505619402868\n",
      "Epoch 6 step 1636: training accuarcy: 0.972\n",
      "Epoch 6 step 1636: training loss: 286.6300186849585\n",
      "Epoch 6 step 1637: training accuarcy: 0.974\n",
      "Epoch 6 step 1637: training loss: 269.96489131216714\n",
      "Epoch 6 step 1638: training accuarcy: 0.9775\n",
      "Epoch 6 step 1638: training loss: 266.0297890019558\n",
      "Epoch 6 step 1639: training accuarcy: 0.9805\n",
      "Epoch 6 step 1639: training loss: 277.5604597419505\n",
      "Epoch 6 step 1640: training accuarcy: 0.9715\n",
      "Epoch 6 step 1640: training loss: 296.30689998635546\n",
      "Epoch 6 step 1641: training accuarcy: 0.977\n",
      "Epoch 6 step 1641: training loss: 273.2654402249805\n",
      "Epoch 6 step 1642: training accuarcy: 0.9775\n",
      "Epoch 6 step 1642: training loss: 268.8891634720089\n",
      "Epoch 6 step 1643: training accuarcy: 0.977\n",
      "Epoch 6 step 1643: training loss: 263.5368056481297\n",
      "Epoch 6 step 1644: training accuarcy: 0.984\n",
      "Epoch 6 step 1644: training loss: 282.02631637096886\n",
      "Epoch 6 step 1645: training accuarcy: 0.9745\n",
      "Epoch 6 step 1645: training loss: 262.1185434065985\n",
      "Epoch 6 step 1646: training accuarcy: 0.9785\n",
      "Epoch 6 step 1646: training loss: 263.4193749415298\n",
      "Epoch 6 step 1647: training accuarcy: 0.981\n",
      "Epoch 6 step 1647: training loss: 259.12879855415247\n",
      "Epoch 6 step 1648: training accuarcy: 0.9805\n",
      "Epoch 6 step 1648: training loss: 276.1462945636002\n",
      "Epoch 6 step 1649: training accuarcy: 0.979\n",
      "Epoch 6 step 1649: training loss: 277.74620032257246\n",
      "Epoch 6 step 1650: training accuarcy: 0.977\n",
      "Epoch 6 step 1650: training loss: 276.2624753843141\n",
      "Epoch 6 step 1651: training accuarcy: 0.981\n",
      "Epoch 6 step 1651: training loss: 259.6986749349745\n",
      "Epoch 6 step 1652: training accuarcy: 0.9785\n",
      "Epoch 6 step 1652: training loss: 274.5135957873615\n",
      "Epoch 6 step 1653: training accuarcy: 0.9795\n",
      "Epoch 6 step 1653: training loss: 262.11393875107075\n",
      "Epoch 6 step 1654: training accuarcy: 0.9815\n",
      "Epoch 6 step 1654: training loss: 264.95611272392455\n",
      "Epoch 6 step 1655: training accuarcy: 0.9775\n",
      "Epoch 6 step 1655: training loss: 283.2503979631502\n",
      "Epoch 6 step 1656: training accuarcy: 0.976\n",
      "Epoch 6 step 1656: training loss: 274.7658798832755\n",
      "Epoch 6 step 1657: training accuarcy: 0.9735\n",
      "Epoch 6 step 1657: training loss: 255.633827454966\n",
      "Epoch 6 step 1658: training accuarcy: 0.985\n",
      "Epoch 6 step 1658: training loss: 274.24146135335906\n",
      "Epoch 6 step 1659: training accuarcy: 0.9785\n",
      "Epoch 6 step 1659: training loss: 258.7500039195834\n",
      "Epoch 6 step 1660: training accuarcy: 0.9795\n",
      "Epoch 6 step 1660: training loss: 257.94103732665144\n",
      "Epoch 6 step 1661: training accuarcy: 0.9835\n",
      "Epoch 6 step 1661: training loss: 275.3171947919614\n",
      "Epoch 6 step 1662: training accuarcy: 0.979\n",
      "Epoch 6 step 1662: training loss: 255.36261909054792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1663: training accuarcy: 0.982\n",
      "Epoch 6 step 1663: training loss: 272.74372363398527\n",
      "Epoch 6 step 1664: training accuarcy: 0.974\n",
      "Epoch 6 step 1664: training loss: 260.3027794584119\n",
      "Epoch 6 step 1665: training accuarcy: 0.982\n",
      "Epoch 6 step 1665: training loss: 260.45961682298406\n",
      "Epoch 6 step 1666: training accuarcy: 0.9825\n",
      "Epoch 6 step 1666: training loss: 264.2485271111472\n",
      "Epoch 6 step 1667: training accuarcy: 0.976\n",
      "Epoch 6 step 1667: training loss: 250.7108814927727\n",
      "Epoch 6 step 1668: training accuarcy: 0.985\n",
      "Epoch 6 step 1668: training loss: 277.4262102871315\n",
      "Epoch 6 step 1669: training accuarcy: 0.9725\n",
      "Epoch 6 step 1669: training loss: 277.7818438752277\n",
      "Epoch 6 step 1670: training accuarcy: 0.977\n",
      "Epoch 6 step 1670: training loss: 247.6617572399866\n",
      "Epoch 6 step 1671: training accuarcy: 0.9835\n",
      "Epoch 6 step 1671: training loss: 263.3573428730975\n",
      "Epoch 6 step 1672: training accuarcy: 0.9785\n",
      "Epoch 6 step 1672: training loss: 290.92088549129676\n",
      "Epoch 6 step 1673: training accuarcy: 0.972\n",
      "Epoch 6 step 1673: training loss: 263.8942009695222\n",
      "Epoch 6 step 1674: training accuarcy: 0.981\n",
      "Epoch 6 step 1674: training loss: 260.26832539324676\n",
      "Epoch 6 step 1675: training accuarcy: 0.9785\n",
      "Epoch 6 step 1675: training loss: 274.1068358144867\n",
      "Epoch 6 step 1676: training accuarcy: 0.975\n",
      "Epoch 6 step 1676: training loss: 273.40048832839597\n",
      "Epoch 6 step 1677: training accuarcy: 0.9805\n",
      "Epoch 6 step 1677: training loss: 261.22257199836184\n",
      "Epoch 6 step 1678: training accuarcy: 0.983\n",
      "Epoch 6 step 1678: training loss: 264.8551496309749\n",
      "Epoch 6 step 1679: training accuarcy: 0.9825\n",
      "Epoch 6 step 1679: training loss: 260.88947797648035\n",
      "Epoch 6 step 1680: training accuarcy: 0.981\n",
      "Epoch 6 step 1680: training loss: 272.7302599766638\n",
      "Epoch 6 step 1681: training accuarcy: 0.9775\n",
      "Epoch 6 step 1681: training loss: 269.37535821800003\n",
      "Epoch 6 step 1682: training accuarcy: 0.976\n",
      "Epoch 6 step 1682: training loss: 267.5768951528557\n",
      "Epoch 6 step 1683: training accuarcy: 0.9805\n",
      "Epoch 6 step 1683: training loss: 261.033311630623\n",
      "Epoch 6 step 1684: training accuarcy: 0.977\n",
      "Epoch 6 step 1684: training loss: 280.0650897941534\n",
      "Epoch 6 step 1685: training accuarcy: 0.9755\n",
      "Epoch 6 step 1685: training loss: 253.430523252086\n",
      "Epoch 6 step 1686: training accuarcy: 0.981\n",
      "Epoch 6 step 1686: training loss: 264.0397321383741\n",
      "Epoch 6 step 1687: training accuarcy: 0.982\n",
      "Epoch 6 step 1687: training loss: 267.04877498004294\n",
      "Epoch 6 step 1688: training accuarcy: 0.976\n",
      "Epoch 6 step 1688: training loss: 253.46289256459806\n",
      "Epoch 6 step 1689: training accuarcy: 0.9835\n",
      "Epoch 6 step 1689: training loss: 262.70769869049934\n",
      "Epoch 6 step 1690: training accuarcy: 0.9795\n",
      "Epoch 6 step 1690: training loss: 282.1544558237107\n",
      "Epoch 6 step 1691: training accuarcy: 0.974\n",
      "Epoch 6 step 1691: training loss: 248.11256470757337\n",
      "Epoch 6 step 1692: training accuarcy: 0.983\n",
      "Epoch 6 step 1692: training loss: 260.00558014111436\n",
      "Epoch 6 step 1693: training accuarcy: 0.9815\n",
      "Epoch 6 step 1693: training loss: 278.8272784282808\n",
      "Epoch 6 step 1694: training accuarcy: 0.9765\n",
      "Epoch 6 step 1694: training loss: 258.723044936743\n",
      "Epoch 6 step 1695: training accuarcy: 0.9825\n",
      "Epoch 6 step 1695: training loss: 258.7900460749452\n",
      "Epoch 6 step 1696: training accuarcy: 0.9835\n",
      "Epoch 6 step 1696: training loss: 267.5639968107928\n",
      "Epoch 6 step 1697: training accuarcy: 0.978\n",
      "Epoch 6 step 1697: training loss: 267.98892516446745\n",
      "Epoch 6 step 1698: training accuarcy: 0.981\n",
      "Epoch 6 step 1698: training loss: 265.2252341291449\n",
      "Epoch 6 step 1699: training accuarcy: 0.982\n",
      "Epoch 6 step 1699: training loss: 254.4797245190806\n",
      "Epoch 6 step 1700: training accuarcy: 0.9795\n",
      "Epoch 6 step 1700: training loss: 272.9674928224901\n",
      "Epoch 6 step 1701: training accuarcy: 0.9765\n",
      "Epoch 6 step 1701: training loss: 267.5638114163887\n",
      "Epoch 6 step 1702: training accuarcy: 0.974\n",
      "Epoch 6 step 1702: training loss: 257.9938498687566\n",
      "Epoch 6 step 1703: training accuarcy: 0.9795\n",
      "Epoch 6 step 1703: training loss: 270.5840436889749\n",
      "Epoch 6 step 1704: training accuarcy: 0.9755\n",
      "Epoch 6 step 1704: training loss: 263.56928697058333\n",
      "Epoch 6 step 1705: training accuarcy: 0.976\n",
      "Epoch 6 step 1705: training loss: 270.67272749691386\n",
      "Epoch 6 step 1706: training accuarcy: 0.9775\n",
      "Epoch 6 step 1706: training loss: 266.79376417105846\n",
      "Epoch 6 step 1707: training accuarcy: 0.977\n",
      "Epoch 6 step 1707: training loss: 266.449997459825\n",
      "Epoch 6 step 1708: training accuarcy: 0.9745\n",
      "Epoch 6 step 1708: training loss: 262.48309674944153\n",
      "Epoch 6 step 1709: training accuarcy: 0.9785\n",
      "Epoch 6 step 1709: training loss: 254.40875097226095\n",
      "Epoch 6 step 1710: training accuarcy: 0.9835\n",
      "Epoch 6 step 1710: training loss: 259.6766761125917\n",
      "Epoch 6 step 1711: training accuarcy: 0.98\n",
      "Epoch 6 step 1711: training loss: 277.6837879867001\n",
      "Epoch 6 step 1712: training accuarcy: 0.9775\n",
      "Epoch 6 step 1712: training loss: 267.60614324742664\n",
      "Epoch 6 step 1713: training accuarcy: 0.9775\n",
      "Epoch 6 step 1713: training loss: 269.65131635386683\n",
      "Epoch 6 step 1714: training accuarcy: 0.979\n",
      "Epoch 6 step 1714: training loss: 283.71627128291203\n",
      "Epoch 6 step 1715: training accuarcy: 0.97\n",
      "Epoch 6 step 1715: training loss: 262.46827721002416\n",
      "Epoch 6 step 1716: training accuarcy: 0.9775\n",
      "Epoch 6 step 1716: training loss: 261.41895614905206\n",
      "Epoch 6 step 1717: training accuarcy: 0.9845\n",
      "Epoch 6 step 1717: training loss: 263.50584353876474\n",
      "Epoch 6 step 1718: training accuarcy: 0.979\n",
      "Epoch 6 step 1718: training loss: 270.33753434990507\n",
      "Epoch 6 step 1719: training accuarcy: 0.979\n",
      "Epoch 6 step 1719: training loss: 261.5131764280716\n",
      "Epoch 6 step 1720: training accuarcy: 0.9785\n",
      "Epoch 6 step 1720: training loss: 259.08333473442775\n",
      "Epoch 6 step 1721: training accuarcy: 0.981\n",
      "Epoch 6 step 1721: training loss: 257.8313532986698\n",
      "Epoch 6 step 1722: training accuarcy: 0.979\n",
      "Epoch 6 step 1722: training loss: 256.53667030129657\n",
      "Epoch 6 step 1723: training accuarcy: 0.984\n",
      "Epoch 6 step 1723: training loss: 267.8412529214317\n",
      "Epoch 6 step 1724: training accuarcy: 0.9765\n",
      "Epoch 6 step 1724: training loss: 264.23730245031396\n",
      "Epoch 6 step 1725: training accuarcy: 0.9725\n",
      "Epoch 6 step 1725: training loss: 256.4297984488082\n",
      "Epoch 6 step 1726: training accuarcy: 0.981\n",
      "Epoch 6 step 1726: training loss: 277.77485795732775\n",
      "Epoch 6 step 1727: training accuarcy: 0.9775\n",
      "Epoch 6 step 1727: training loss: 266.36563506879077\n",
      "Epoch 6 step 1728: training accuarcy: 0.977\n",
      "Epoch 6 step 1728: training loss: 266.14001929856664\n",
      "Epoch 6 step 1729: training accuarcy: 0.977\n",
      "Epoch 6 step 1729: training loss: 270.2616248868637\n",
      "Epoch 6 step 1730: training accuarcy: 0.9775\n",
      "Epoch 6 step 1730: training loss: 274.7949002442893\n",
      "Epoch 6 step 1731: training accuarcy: 0.9765\n",
      "Epoch 6 step 1731: training loss: 263.1661097774419\n",
      "Epoch 6 step 1732: training accuarcy: 0.9835\n",
      "Epoch 6 step 1732: training loss: 256.40085727717667\n",
      "Epoch 6 step 1733: training accuarcy: 0.983\n",
      "Epoch 6 step 1733: training loss: 244.02128464220442\n",
      "Epoch 6 step 1734: training accuarcy: 0.9835\n",
      "Epoch 6 step 1734: training loss: 273.50001981095727\n",
      "Epoch 6 step 1735: training accuarcy: 0.976\n",
      "Epoch 6 step 1735: training loss: 263.3828392106512\n",
      "Epoch 6 step 1736: training accuarcy: 0.9785\n",
      "Epoch 6 step 1736: training loss: 259.23715724651123\n",
      "Epoch 6 step 1737: training accuarcy: 0.975\n",
      "Epoch 6 step 1737: training loss: 243.57481883021515\n",
      "Epoch 6 step 1738: training accuarcy: 0.9795\n",
      "Epoch 6 step 1738: training loss: 252.65132228346465\n",
      "Epoch 6 step 1739: training accuarcy: 0.984\n",
      "Epoch 6 step 1739: training loss: 255.01007347903067\n",
      "Epoch 6 step 1740: training accuarcy: 0.979\n",
      "Epoch 6 step 1740: training loss: 250.25535417451542\n",
      "Epoch 6 step 1741: training accuarcy: 0.985\n",
      "Epoch 6 step 1741: training loss: 251.8631334836184\n",
      "Epoch 6 step 1742: training accuarcy: 0.983\n",
      "Epoch 6 step 1742: training loss: 270.3495310734739\n",
      "Epoch 6 step 1743: training accuarcy: 0.9775\n",
      "Epoch 6 step 1743: training loss: 260.07093732603585\n",
      "Epoch 6 step 1744: training accuarcy: 0.975\n",
      "Epoch 6 step 1744: training loss: 260.40606978259046\n",
      "Epoch 6 step 1745: training accuarcy: 0.9795\n",
      "Epoch 6 step 1745: training loss: 259.51936510967727\n",
      "Epoch 6 step 1746: training accuarcy: 0.976\n",
      "Epoch 6 step 1746: training loss: 260.32376106762484\n",
      "Epoch 6 step 1747: training accuarcy: 0.9835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1747: training loss: 239.25604234662714\n",
      "Epoch 6 step 1748: training accuarcy: 0.984\n",
      "Epoch 6 step 1748: training loss: 250.5090920832568\n",
      "Epoch 6 step 1749: training accuarcy: 0.9795\n",
      "Epoch 6 step 1749: training loss: 270.89029412113\n",
      "Epoch 6 step 1750: training accuarcy: 0.978\n",
      "Epoch 6 step 1750: training loss: 252.03609499469488\n",
      "Epoch 6 step 1751: training accuarcy: 0.983\n",
      "Epoch 6 step 1751: training loss: 260.92424193545634\n",
      "Epoch 6 step 1752: training accuarcy: 0.978\n",
      "Epoch 6 step 1752: training loss: 250.57131871772398\n",
      "Epoch 6 step 1753: training accuarcy: 0.979\n",
      "Epoch 6 step 1753: training loss: 260.8822594586854\n",
      "Epoch 6 step 1754: training accuarcy: 0.981\n",
      "Epoch 6 step 1754: training loss: 277.7113655219941\n",
      "Epoch 6 step 1755: training accuarcy: 0.97\n",
      "Epoch 6 step 1755: training loss: 244.93968000144952\n",
      "Epoch 6 step 1756: training accuarcy: 0.985\n",
      "Epoch 6 step 1756: training loss: 244.50587167179933\n",
      "Epoch 6 step 1757: training accuarcy: 0.9825\n",
      "Epoch 6 step 1757: training loss: 275.94315535762985\n",
      "Epoch 6 step 1758: training accuarcy: 0.973\n",
      "Epoch 6 step 1758: training loss: 241.4897174038524\n",
      "Epoch 6 step 1759: training accuarcy: 0.9845\n",
      "Epoch 6 step 1759: training loss: 256.035754816181\n",
      "Epoch 6 step 1760: training accuarcy: 0.9795\n",
      "Epoch 6 step 1760: training loss: 257.71811619637015\n",
      "Epoch 6 step 1761: training accuarcy: 0.9795\n",
      "Epoch 6 step 1761: training loss: 262.7257399169824\n",
      "Epoch 6 step 1762: training accuarcy: 0.979\n",
      "Epoch 6 step 1762: training loss: 268.3139347165613\n",
      "Epoch 6 step 1763: training accuarcy: 0.972\n",
      "Epoch 6 step 1763: training loss: 257.28933729297785\n",
      "Epoch 6 step 1764: training accuarcy: 0.9785\n",
      "Epoch 6 step 1764: training loss: 278.81043417448296\n",
      "Epoch 6 step 1765: training accuarcy: 0.9725\n",
      "Epoch 6 step 1765: training loss: 249.2661061042268\n",
      "Epoch 6 step 1766: training accuarcy: 0.9805\n",
      "Epoch 6 step 1766: training loss: 254.4940428517244\n",
      "Epoch 6 step 1767: training accuarcy: 0.979\n",
      "Epoch 6 step 1767: training loss: 247.2847766267866\n",
      "Epoch 6 step 1768: training accuarcy: 0.9825\n",
      "Epoch 6 step 1768: training loss: 249.0398636475485\n",
      "Epoch 6 step 1769: training accuarcy: 0.974\n",
      "Epoch 6 step 1769: training loss: 247.62503253982274\n",
      "Epoch 6 step 1770: training accuarcy: 0.981\n",
      "Epoch 6 step 1770: training loss: 259.30532953480395\n",
      "Epoch 6 step 1771: training accuarcy: 0.979\n",
      "Epoch 6 step 1771: training loss: 263.3945955329253\n",
      "Epoch 6 step 1772: training accuarcy: 0.9745\n",
      "Epoch 6 step 1772: training loss: 253.93944075874427\n",
      "Epoch 6 step 1773: training accuarcy: 0.977\n",
      "Epoch 6 step 1773: training loss: 257.6256590548361\n",
      "Epoch 6 step 1774: training accuarcy: 0.9795\n",
      "Epoch 6 step 1774: training loss: 246.07140032177628\n",
      "Epoch 6 step 1775: training accuarcy: 0.9815\n",
      "Epoch 6 step 1775: training loss: 252.24862196208565\n",
      "Epoch 6 step 1776: training accuarcy: 0.979\n",
      "Epoch 6 step 1776: training loss: 251.16566958192956\n",
      "Epoch 6 step 1777: training accuarcy: 0.98\n",
      "Epoch 6 step 1777: training loss: 257.8733214970596\n",
      "Epoch 6 step 1778: training accuarcy: 0.9785\n",
      "Epoch 6 step 1778: training loss: 244.8770815054065\n",
      "Epoch 6 step 1779: training accuarcy: 0.982\n",
      "Epoch 6 step 1779: training loss: 268.6301245721637\n",
      "Epoch 6 step 1780: training accuarcy: 0.9785\n",
      "Epoch 6 step 1780: training loss: 249.06234070962668\n",
      "Epoch 6 step 1781: training accuarcy: 0.98\n",
      "Epoch 6 step 1781: training loss: 251.64418287603237\n",
      "Epoch 6 step 1782: training accuarcy: 0.9805\n",
      "Epoch 6 step 1782: training loss: 239.35787069012997\n",
      "Epoch 6 step 1783: training accuarcy: 0.984\n",
      "Epoch 6 step 1783: training loss: 261.14754244698145\n",
      "Epoch 6 step 1784: training accuarcy: 0.981\n",
      "Epoch 6 step 1784: training loss: 232.35752799502748\n",
      "Epoch 6 step 1785: training accuarcy: 0.9865\n",
      "Epoch 6 step 1785: training loss: 246.66336676395443\n",
      "Epoch 6 step 1786: training accuarcy: 0.9795\n",
      "Epoch 6 step 1786: training loss: 245.01233835187574\n",
      "Epoch 6 step 1787: training accuarcy: 0.9815\n",
      "Epoch 6 step 1787: training loss: 254.03923881851526\n",
      "Epoch 6 step 1788: training accuarcy: 0.9735\n",
      "Epoch 6 step 1788: training loss: 263.1899697604548\n",
      "Epoch 6 step 1789: training accuarcy: 0.977\n",
      "Epoch 6 step 1789: training loss: 252.62166756519872\n",
      "Epoch 6 step 1790: training accuarcy: 0.984\n",
      "Epoch 6 step 1790: training loss: 257.2471443455303\n",
      "Epoch 6 step 1791: training accuarcy: 0.979\n",
      "Epoch 6 step 1791: training loss: 258.45453480353706\n",
      "Epoch 6 step 1792: training accuarcy: 0.9745\n",
      "Epoch 6 step 1792: training loss: 248.08072209227913\n",
      "Epoch 6 step 1793: training accuarcy: 0.9815\n",
      "Epoch 6 step 1793: training loss: 252.33819238968738\n",
      "Epoch 6 step 1794: training accuarcy: 0.977\n",
      "Epoch 6 step 1794: training loss: 251.4093181149011\n",
      "Epoch 6 step 1795: training accuarcy: 0.9815\n",
      "Epoch 6 step 1795: training loss: 244.8244139970017\n",
      "Epoch 6 step 1796: training accuarcy: 0.98\n",
      "Epoch 6 step 1796: training loss: 238.64620648992477\n",
      "Epoch 6 step 1797: training accuarcy: 0.983\n",
      "Epoch 6 step 1797: training loss: 269.8079739804908\n",
      "Epoch 6 step 1798: training accuarcy: 0.9735\n",
      "Epoch 6 step 1798: training loss: 254.19337154366997\n",
      "Epoch 6 step 1799: training accuarcy: 0.9805\n",
      "Epoch 6 step 1799: training loss: 237.73741590248446\n",
      "Epoch 6 step 1800: training accuarcy: 0.984\n",
      "Epoch 6 step 1800: training loss: 245.3562263867922\n",
      "Epoch 6 step 1801: training accuarcy: 0.9805\n",
      "Epoch 6 step 1801: training loss: 256.47953881585954\n",
      "Epoch 6 step 1802: training accuarcy: 0.977\n",
      "Epoch 6 step 1802: training loss: 237.28103721011405\n",
      "Epoch 6 step 1803: training accuarcy: 0.981\n",
      "Epoch 6 step 1803: training loss: 259.5292243578821\n",
      "Epoch 6 step 1804: training accuarcy: 0.977\n",
      "Epoch 6 step 1804: training loss: 270.9612594496008\n",
      "Epoch 6 step 1805: training accuarcy: 0.975\n",
      "Epoch 6 step 1805: training loss: 242.45224056329275\n",
      "Epoch 6 step 1806: training accuarcy: 0.9835\n",
      "Epoch 6 step 1806: training loss: 258.81291629957127\n",
      "Epoch 6 step 1807: training accuarcy: 0.977\n",
      "Epoch 6 step 1807: training loss: 245.82492768834203\n",
      "Epoch 6 step 1808: training accuarcy: 0.9765\n",
      "Epoch 6 step 1808: training loss: 251.51543386053677\n",
      "Epoch 6 step 1809: training accuarcy: 0.9805\n",
      "Epoch 6 step 1809: training loss: 236.2341073234748\n",
      "Epoch 6 step 1810: training accuarcy: 0.982\n",
      "Epoch 6 step 1810: training loss: 254.45739199584182\n",
      "Epoch 6 step 1811: training accuarcy: 0.98\n",
      "Epoch 6 step 1811: training loss: 255.52121004901437\n",
      "Epoch 6 step 1812: training accuarcy: 0.9735\n",
      "Epoch 6 step 1812: training loss: 255.12113387743403\n",
      "Epoch 6 step 1813: training accuarcy: 0.9765\n",
      "Epoch 6 step 1813: training loss: 260.44650347940666\n",
      "Epoch 6 step 1814: training accuarcy: 0.9785\n",
      "Epoch 6 step 1814: training loss: 251.47734441443754\n",
      "Epoch 6 step 1815: training accuarcy: 0.979\n",
      "Epoch 6 step 1815: training loss: 248.42193518552247\n",
      "Epoch 6 step 1816: training accuarcy: 0.9825\n",
      "Epoch 6 step 1816: training loss: 250.47870144084405\n",
      "Epoch 6 step 1817: training accuarcy: 0.9765\n",
      "Epoch 6 step 1817: training loss: 268.0226121957191\n",
      "Epoch 6 step 1818: training accuarcy: 0.9765\n",
      "Epoch 6 step 1818: training loss: 244.1862447047771\n",
      "Epoch 6 step 1819: training accuarcy: 0.981\n",
      "Epoch 6 step 1819: training loss: 254.5950402200072\n",
      "Epoch 6 step 1820: training accuarcy: 0.977\n",
      "Epoch 6 step 1820: training loss: 242.71185699834217\n",
      "Epoch 6 step 1821: training accuarcy: 0.986\n",
      "Epoch 6 step 1821: training loss: 257.9550995131125\n",
      "Epoch 6 step 1822: training accuarcy: 0.982\n",
      "Epoch 6 step 1822: training loss: 237.90502269109322\n",
      "Epoch 6 step 1823: training accuarcy: 0.982\n",
      "Epoch 6 step 1823: training loss: 235.21757969490199\n",
      "Epoch 6 step 1824: training accuarcy: 0.9805\n",
      "Epoch 6 step 1824: training loss: 259.4864811785262\n",
      "Epoch 6 step 1825: training accuarcy: 0.975\n",
      "Epoch 6 step 1825: training loss: 242.27586397904543\n",
      "Epoch 6 step 1826: training accuarcy: 0.9835\n",
      "Epoch 6 step 1826: training loss: 250.5727823694232\n",
      "Epoch 6 step 1827: training accuarcy: 0.98\n",
      "Epoch 6 step 1827: training loss: 256.0310314560261\n",
      "Epoch 6 step 1828: training accuarcy: 0.979\n",
      "Epoch 6 step 1828: training loss: 260.66424387995374\n",
      "Epoch 6 step 1829: training accuarcy: 0.974\n",
      "Epoch 6 step 1829: training loss: 249.02252095722676\n",
      "Epoch 6 step 1830: training accuarcy: 0.982\n",
      "Epoch 6 step 1830: training loss: 243.6317316114198\n",
      "Epoch 6 step 1831: training accuarcy: 0.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1831: training loss: 256.0917421326599\n",
      "Epoch 6 step 1832: training accuarcy: 0.977\n",
      "Epoch 6 step 1832: training loss: 242.16521695225123\n",
      "Epoch 6 step 1833: training accuarcy: 0.98\n",
      "Epoch 6 step 1833: training loss: 245.67361457117\n",
      "Epoch 6 step 1834: training accuarcy: 0.9795\n",
      "Epoch 6 step 1834: training loss: 240.71289769909637\n",
      "Epoch 6 step 1835: training accuarcy: 0.979\n",
      "Epoch 6 step 1835: training loss: 245.31565645645395\n",
      "Epoch 6 step 1836: training accuarcy: 0.979\n",
      "Epoch 6 step 1836: training loss: 256.6094074278575\n",
      "Epoch 6 step 1837: training accuarcy: 0.979\n",
      "Epoch 6 step 1837: training loss: 233.37632116617718\n",
      "Epoch 6 step 1838: training accuarcy: 0.986\n",
      "Epoch 6 step 1838: training loss: 234.0348094966601\n",
      "Epoch 6 step 1839: training accuarcy: 0.9845\n",
      "Epoch 6 step 1839: training loss: 244.20991161044654\n",
      "Epoch 6 step 1840: training accuarcy: 0.981\n",
      "Epoch 6 step 1840: training loss: 100.71726507129797\n",
      "Epoch 6 step 1841: training accuarcy: 0.985897435897436\n",
      "Epoch 6: train loss 262.19572522030217, train accuarcy 0.9682431817054749\n",
      "Epoch 6: valid loss 734.2455726166758, valid accuarcy 0.8498079776763916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                   | 7/8 [15:33<02:12, 132.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 1841: training loss: 207.70664988120637\n",
      "Epoch 7 step 1842: training accuarcy: 0.987\n",
      "Epoch 7 step 1842: training loss: 201.9350882288199\n",
      "Epoch 7 step 1843: training accuarcy: 0.9905\n",
      "Epoch 7 step 1843: training loss: 200.32081485993274\n",
      "Epoch 7 step 1844: training accuarcy: 0.9895\n",
      "Epoch 7 step 1844: training loss: 211.9872521946416\n",
      "Epoch 7 step 1845: training accuarcy: 0.9875\n",
      "Epoch 7 step 1845: training loss: 211.92228309816176\n",
      "Epoch 7 step 1846: training accuarcy: 0.9875\n",
      "Epoch 7 step 1846: training loss: 201.46405688122888\n",
      "Epoch 7 step 1847: training accuarcy: 0.99\n",
      "Epoch 7 step 1847: training loss: 209.99295785035767\n",
      "Epoch 7 step 1848: training accuarcy: 0.99\n",
      "Epoch 7 step 1848: training loss: 212.418770928042\n",
      "Epoch 7 step 1849: training accuarcy: 0.9915\n",
      "Epoch 7 step 1849: training loss: 198.96235930242898\n",
      "Epoch 7 step 1850: training accuarcy: 0.989\n",
      "Epoch 7 step 1850: training loss: 210.20298751367457\n",
      "Epoch 7 step 1851: training accuarcy: 0.9865\n",
      "Epoch 7 step 1851: training loss: 206.28328152527143\n",
      "Epoch 7 step 1852: training accuarcy: 0.987\n",
      "Epoch 7 step 1852: training loss: 212.21004746160577\n",
      "Epoch 7 step 1853: training accuarcy: 0.989\n",
      "Epoch 7 step 1853: training loss: 205.25245702461083\n",
      "Epoch 7 step 1854: training accuarcy: 0.99\n",
      "Epoch 7 step 1854: training loss: 207.03804603198563\n",
      "Epoch 7 step 1855: training accuarcy: 0.987\n",
      "Epoch 7 step 1855: training loss: 216.46610950002713\n",
      "Epoch 7 step 1856: training accuarcy: 0.9835\n",
      "Epoch 7 step 1856: training loss: 203.73785600015194\n",
      "Epoch 7 step 1857: training accuarcy: 0.992\n",
      "Epoch 7 step 1857: training loss: 199.68019644183815\n",
      "Epoch 7 step 1858: training accuarcy: 0.9925\n",
      "Epoch 7 step 1858: training loss: 203.8936169723514\n",
      "Epoch 7 step 1859: training accuarcy: 0.9865\n",
      "Epoch 7 step 1859: training loss: 206.8329228607105\n",
      "Epoch 7 step 1860: training accuarcy: 0.988\n",
      "Epoch 7 step 1860: training loss: 209.40738050590758\n",
      "Epoch 7 step 1861: training accuarcy: 0.986\n",
      "Epoch 7 step 1861: training loss: 216.34653107898498\n",
      "Epoch 7 step 1862: training accuarcy: 0.9865\n",
      "Epoch 7 step 1862: training loss: 196.20570418592675\n",
      "Epoch 7 step 1863: training accuarcy: 0.9925\n",
      "Epoch 7 step 1863: training loss: 218.90210548357902\n",
      "Epoch 7 step 1864: training accuarcy: 0.984\n",
      "Epoch 7 step 1864: training loss: 220.80608181557324\n",
      "Epoch 7 step 1865: training accuarcy: 0.9845\n",
      "Epoch 7 step 1865: training loss: 199.7191261786881\n",
      "Epoch 7 step 1866: training accuarcy: 0.988\n",
      "Epoch 7 step 1866: training loss: 209.15978368340532\n",
      "Epoch 7 step 1867: training accuarcy: 0.987\n",
      "Epoch 7 step 1867: training loss: 197.66564552967057\n",
      "Epoch 7 step 1868: training accuarcy: 0.991\n",
      "Epoch 7 step 1868: training loss: 194.39877216583437\n",
      "Epoch 7 step 1869: training accuarcy: 0.992\n",
      "Epoch 7 step 1869: training loss: 206.53974390207526\n",
      "Epoch 7 step 1870: training accuarcy: 0.9885\n",
      "Epoch 7 step 1870: training loss: 205.64486093595596\n",
      "Epoch 7 step 1871: training accuarcy: 0.987\n",
      "Epoch 7 step 1871: training loss: 208.27479352607037\n",
      "Epoch 7 step 1872: training accuarcy: 0.9855\n",
      "Epoch 7 step 1872: training loss: 210.34860504273257\n",
      "Epoch 7 step 1873: training accuarcy: 0.986\n",
      "Epoch 7 step 1873: training loss: 206.3676189988021\n",
      "Epoch 7 step 1874: training accuarcy: 0.985\n",
      "Epoch 7 step 1874: training loss: 196.79478599101955\n",
      "Epoch 7 step 1875: training accuarcy: 0.99\n",
      "Epoch 7 step 1875: training loss: 210.77153170845563\n",
      "Epoch 7 step 1876: training accuarcy: 0.9825\n",
      "Epoch 7 step 1876: training loss: 201.0320534732664\n",
      "Epoch 7 step 1877: training accuarcy: 0.9885\n",
      "Epoch 7 step 1877: training loss: 223.19123227634645\n",
      "Epoch 7 step 1878: training accuarcy: 0.9835\n",
      "Epoch 7 step 1878: training loss: 207.84727550643893\n",
      "Epoch 7 step 1879: training accuarcy: 0.9915\n",
      "Epoch 7 step 1879: training loss: 212.26499436710253\n",
      "Epoch 7 step 1880: training accuarcy: 0.987\n",
      "Epoch 7 step 1880: training loss: 205.5824762407209\n",
      "Epoch 7 step 1881: training accuarcy: 0.9835\n",
      "Epoch 7 step 1881: training loss: 212.916578981712\n",
      "Epoch 7 step 1882: training accuarcy: 0.9845\n",
      "Epoch 7 step 1882: training loss: 199.19871843771577\n",
      "Epoch 7 step 1883: training accuarcy: 0.992\n",
      "Epoch 7 step 1883: training loss: 208.20969171851533\n",
      "Epoch 7 step 1884: training accuarcy: 0.9885\n",
      "Epoch 7 step 1884: training loss: 207.0822084322706\n",
      "Epoch 7 step 1885: training accuarcy: 0.9915\n",
      "Epoch 7 step 1885: training loss: 215.89584840198268\n",
      "Epoch 7 step 1886: training accuarcy: 0.987\n",
      "Epoch 7 step 1886: training loss: 201.756712916904\n",
      "Epoch 7 step 1887: training accuarcy: 0.993\n",
      "Epoch 7 step 1887: training loss: 198.17100482913378\n",
      "Epoch 7 step 1888: training accuarcy: 0.9905\n",
      "Epoch 7 step 1888: training loss: 186.12629840702314\n",
      "Epoch 7 step 1889: training accuarcy: 0.993\n",
      "Epoch 7 step 1889: training loss: 196.13258398652175\n",
      "Epoch 7 step 1890: training accuarcy: 0.988\n",
      "Epoch 7 step 1890: training loss: 199.34010475357076\n",
      "Epoch 7 step 1891: training accuarcy: 0.988\n",
      "Epoch 7 step 1891: training loss: 208.90641850264302\n",
      "Epoch 7 step 1892: training accuarcy: 0.9875\n",
      "Epoch 7 step 1892: training loss: 201.05307091780773\n",
      "Epoch 7 step 1893: training accuarcy: 0.9895\n",
      "Epoch 7 step 1893: training loss: 198.40222576840728\n",
      "Epoch 7 step 1894: training accuarcy: 0.989\n",
      "Epoch 7 step 1894: training loss: 200.41799014830212\n",
      "Epoch 7 step 1895: training accuarcy: 0.9865\n",
      "Epoch 7 step 1895: training loss: 206.0597351567057\n",
      "Epoch 7 step 1896: training accuarcy: 0.987\n",
      "Epoch 7 step 1896: training loss: 201.04561977728264\n",
      "Epoch 7 step 1897: training accuarcy: 0.991\n",
      "Epoch 7 step 1897: training loss: 208.93453316002797\n",
      "Epoch 7 step 1898: training accuarcy: 0.9835\n",
      "Epoch 7 step 1898: training loss: 197.4842003085442\n",
      "Epoch 7 step 1899: training accuarcy: 0.985\n",
      "Epoch 7 step 1899: training loss: 196.65385662236508\n",
      "Epoch 7 step 1900: training accuarcy: 0.9885\n",
      "Epoch 7 step 1900: training loss: 201.8013241509314\n",
      "Epoch 7 step 1901: training accuarcy: 0.9895\n",
      "Epoch 7 step 1901: training loss: 204.01851140246262\n",
      "Epoch 7 step 1902: training accuarcy: 0.9885\n",
      "Epoch 7 step 1902: training loss: 210.53709441447893\n",
      "Epoch 7 step 1903: training accuarcy: 0.9855\n",
      "Epoch 7 step 1903: training loss: 196.83829263895285\n",
      "Epoch 7 step 1904: training accuarcy: 0.9865\n",
      "Epoch 7 step 1904: training loss: 210.4778962181575\n",
      "Epoch 7 step 1905: training accuarcy: 0.9875\n",
      "Epoch 7 step 1905: training loss: 204.5471636297518\n",
      "Epoch 7 step 1906: training accuarcy: 0.985\n",
      "Epoch 7 step 1906: training loss: 206.06966534713692\n",
      "Epoch 7 step 1907: training accuarcy: 0.9875\n",
      "Epoch 7 step 1907: training loss: 197.9910080986268\n",
      "Epoch 7 step 1908: training accuarcy: 0.9915\n",
      "Epoch 7 step 1908: training loss: 201.06860680874303\n",
      "Epoch 7 step 1909: training accuarcy: 0.9875\n",
      "Epoch 7 step 1909: training loss: 198.2796926241906\n",
      "Epoch 7 step 1910: training accuarcy: 0.9875\n",
      "Epoch 7 step 1910: training loss: 204.05249855637157\n",
      "Epoch 7 step 1911: training accuarcy: 0.9875\n",
      "Epoch 7 step 1911: training loss: 215.64568078583463\n",
      "Epoch 7 step 1912: training accuarcy: 0.9815\n",
      "Epoch 7 step 1912: training loss: 201.51768013759047\n",
      "Epoch 7 step 1913: training accuarcy: 0.988\n",
      "Epoch 7 step 1913: training loss: 198.85645156867508\n",
      "Epoch 7 step 1914: training accuarcy: 0.9895\n",
      "Epoch 7 step 1914: training loss: 200.6896932764546\n",
      "Epoch 7 step 1915: training accuarcy: 0.988\n",
      "Epoch 7 step 1915: training loss: 200.52126072762235\n",
      "Epoch 7 step 1916: training accuarcy: 0.986\n",
      "Epoch 7 step 1916: training loss: 211.13272561373245\n",
      "Epoch 7 step 1917: training accuarcy: 0.9875\n",
      "Epoch 7 step 1917: training loss: 201.4299673616234\n",
      "Epoch 7 step 1918: training accuarcy: 0.984\n",
      "Epoch 7 step 1918: training loss: 210.011861055806\n",
      "Epoch 7 step 1919: training accuarcy: 0.984\n",
      "Epoch 7 step 1919: training loss: 209.27923374761755\n",
      "Epoch 7 step 1920: training accuarcy: 0.9865\n",
      "Epoch 7 step 1920: training loss: 213.8370305881519\n",
      "Epoch 7 step 1921: training accuarcy: 0.985\n",
      "Epoch 7 step 1921: training loss: 203.47938612126683\n",
      "Epoch 7 step 1922: training accuarcy: 0.9855\n",
      "Epoch 7 step 1922: training loss: 182.47372060774933\n",
      "Epoch 7 step 1923: training accuarcy: 0.991\n",
      "Epoch 7 step 1923: training loss: 204.3209083862531\n",
      "Epoch 7 step 1924: training accuarcy: 0.9905\n",
      "Epoch 7 step 1924: training loss: 210.20890107110307\n",
      "Epoch 7 step 1925: training accuarcy: 0.985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 1925: training loss: 194.71575382818938\n",
      "Epoch 7 step 1926: training accuarcy: 0.989\n",
      "Epoch 7 step 1926: training loss: 202.36474075876674\n",
      "Epoch 7 step 1927: training accuarcy: 0.9845\n",
      "Epoch 7 step 1927: training loss: 195.91515723538737\n",
      "Epoch 7 step 1928: training accuarcy: 0.9875\n",
      "Epoch 7 step 1928: training loss: 191.8330441657476\n",
      "Epoch 7 step 1929: training accuarcy: 0.9895\n",
      "Epoch 7 step 1929: training loss: 201.41853329777916\n",
      "Epoch 7 step 1930: training accuarcy: 0.987\n",
      "Epoch 7 step 1930: training loss: 209.73906444579933\n",
      "Epoch 7 step 1931: training accuarcy: 0.986\n",
      "Epoch 7 step 1931: training loss: 200.4897903799368\n",
      "Epoch 7 step 1932: training accuarcy: 0.99\n",
      "Epoch 7 step 1932: training loss: 201.1701422003739\n",
      "Epoch 7 step 1933: training accuarcy: 0.99\n",
      "Epoch 7 step 1933: training loss: 191.6006720319378\n",
      "Epoch 7 step 1934: training accuarcy: 0.99\n",
      "Epoch 7 step 1934: training loss: 204.16885072806537\n",
      "Epoch 7 step 1935: training accuarcy: 0.988\n",
      "Epoch 7 step 1935: training loss: 198.74664771077443\n",
      "Epoch 7 step 1936: training accuarcy: 0.9865\n",
      "Epoch 7 step 1936: training loss: 213.47148690419525\n",
      "Epoch 7 step 1937: training accuarcy: 0.982\n",
      "Epoch 7 step 1937: training loss: 207.84459242494327\n",
      "Epoch 7 step 1938: training accuarcy: 0.985\n",
      "Epoch 7 step 1938: training loss: 201.60023047268984\n",
      "Epoch 7 step 1939: training accuarcy: 0.9885\n",
      "Epoch 7 step 1939: training loss: 209.7801363203075\n",
      "Epoch 7 step 1940: training accuarcy: 0.9845\n",
      "Epoch 7 step 1940: training loss: 197.54670242346373\n",
      "Epoch 7 step 1941: training accuarcy: 0.987\n",
      "Epoch 7 step 1941: training loss: 198.21055272053238\n",
      "Epoch 7 step 1942: training accuarcy: 0.9855\n",
      "Epoch 7 step 1942: training loss: 195.50967000738547\n",
      "Epoch 7 step 1943: training accuarcy: 0.9895\n",
      "Epoch 7 step 1943: training loss: 199.33828677231728\n",
      "Epoch 7 step 1944: training accuarcy: 0.9865\n",
      "Epoch 7 step 1944: training loss: 209.98100839365097\n",
      "Epoch 7 step 1945: training accuarcy: 0.9845\n",
      "Epoch 7 step 1945: training loss: 194.4666235470349\n",
      "Epoch 7 step 1946: training accuarcy: 0.989\n",
      "Epoch 7 step 1946: training loss: 206.44200386616953\n",
      "Epoch 7 step 1947: training accuarcy: 0.9865\n",
      "Epoch 7 step 1947: training loss: 197.67111978269708\n",
      "Epoch 7 step 1948: training accuarcy: 0.9885\n",
      "Epoch 7 step 1948: training loss: 201.37410786076435\n",
      "Epoch 7 step 1949: training accuarcy: 0.988\n",
      "Epoch 7 step 1949: training loss: 196.47088668835818\n",
      "Epoch 7 step 1950: training accuarcy: 0.986\n",
      "Epoch 7 step 1950: training loss: 191.04163304970857\n",
      "Epoch 7 step 1951: training accuarcy: 0.9885\n",
      "Epoch 7 step 1951: training loss: 197.27841809450135\n",
      "Epoch 7 step 1952: training accuarcy: 0.988\n",
      "Epoch 7 step 1952: training loss: 205.82140466503495\n",
      "Epoch 7 step 1953: training accuarcy: 0.986\n",
      "Epoch 7 step 1953: training loss: 203.10865649024007\n",
      "Epoch 7 step 1954: training accuarcy: 0.9865\n",
      "Epoch 7 step 1954: training loss: 207.035770225772\n",
      "Epoch 7 step 1955: training accuarcy: 0.985\n",
      "Epoch 7 step 1955: training loss: 205.97336136898127\n",
      "Epoch 7 step 1956: training accuarcy: 0.991\n",
      "Epoch 7 step 1956: training loss: 197.2440452127474\n",
      "Epoch 7 step 1957: training accuarcy: 0.9855\n",
      "Epoch 7 step 1957: training loss: 200.02210792097776\n",
      "Epoch 7 step 1958: training accuarcy: 0.988\n",
      "Epoch 7 step 1958: training loss: 180.46491338651143\n",
      "Epoch 7 step 1959: training accuarcy: 0.9935\n",
      "Epoch 7 step 1959: training loss: 213.08817781437548\n",
      "Epoch 7 step 1960: training accuarcy: 0.984\n",
      "Epoch 7 step 1960: training loss: 200.85004781878473\n",
      "Epoch 7 step 1961: training accuarcy: 0.99\n",
      "Epoch 7 step 1961: training loss: 211.23806002233056\n",
      "Epoch 7 step 1962: training accuarcy: 0.9845\n",
      "Epoch 7 step 1962: training loss: 191.12599953704284\n",
      "Epoch 7 step 1963: training accuarcy: 0.9905\n",
      "Epoch 7 step 1963: training loss: 199.8681200860777\n",
      "Epoch 7 step 1964: training accuarcy: 0.99\n",
      "Epoch 7 step 1964: training loss: 197.5919619902008\n",
      "Epoch 7 step 1965: training accuarcy: 0.9905\n",
      "Epoch 7 step 1965: training loss: 191.8435230537722\n",
      "Epoch 7 step 1966: training accuarcy: 0.99\n",
      "Epoch 7 step 1966: training loss: 192.63991299493273\n",
      "Epoch 7 step 1967: training accuarcy: 0.9885\n",
      "Epoch 7 step 1967: training loss: 196.63968493693608\n",
      "Epoch 7 step 1968: training accuarcy: 0.9865\n",
      "Epoch 7 step 1968: training loss: 203.30959703656043\n",
      "Epoch 7 step 1969: training accuarcy: 0.9835\n",
      "Epoch 7 step 1969: training loss: 212.65313173429706\n",
      "Epoch 7 step 1970: training accuarcy: 0.983\n",
      "Epoch 7 step 1970: training loss: 191.11323596396767\n",
      "Epoch 7 step 1971: training accuarcy: 0.991\n",
      "Epoch 7 step 1971: training loss: 202.2275146407526\n",
      "Epoch 7 step 1972: training accuarcy: 0.984\n",
      "Epoch 7 step 1972: training loss: 202.94064708674244\n",
      "Epoch 7 step 1973: training accuarcy: 0.986\n",
      "Epoch 7 step 1973: training loss: 192.56344414030292\n",
      "Epoch 7 step 1974: training accuarcy: 0.989\n",
      "Epoch 7 step 1974: training loss: 204.03016873888907\n",
      "Epoch 7 step 1975: training accuarcy: 0.9855\n",
      "Epoch 7 step 1975: training loss: 205.22476052198516\n",
      "Epoch 7 step 1976: training accuarcy: 0.985\n",
      "Epoch 7 step 1976: training loss: 198.82506497053208\n",
      "Epoch 7 step 1977: training accuarcy: 0.9875\n",
      "Epoch 7 step 1977: training loss: 193.90618673103452\n",
      "Epoch 7 step 1978: training accuarcy: 0.9865\n",
      "Epoch 7 step 1978: training loss: 186.19781962227609\n",
      "Epoch 7 step 1979: training accuarcy: 0.9915\n",
      "Epoch 7 step 1979: training loss: 201.53266485911212\n",
      "Epoch 7 step 1980: training accuarcy: 0.985\n",
      "Epoch 7 step 1980: training loss: 203.1395460522974\n",
      "Epoch 7 step 1981: training accuarcy: 0.987\n",
      "Epoch 7 step 1981: training loss: 201.94741133514486\n",
      "Epoch 7 step 1982: training accuarcy: 0.986\n",
      "Epoch 7 step 1982: training loss: 185.27685996095758\n",
      "Epoch 7 step 1983: training accuarcy: 0.9905\n",
      "Epoch 7 step 1983: training loss: 193.9234018274758\n",
      "Epoch 7 step 1984: training accuarcy: 0.9905\n",
      "Epoch 7 step 1984: training loss: 195.5698908991922\n",
      "Epoch 7 step 1985: training accuarcy: 0.99\n",
      "Epoch 7 step 1985: training loss: 188.28098755807721\n",
      "Epoch 7 step 1986: training accuarcy: 0.9905\n",
      "Epoch 7 step 1986: training loss: 206.3608874661516\n",
      "Epoch 7 step 1987: training accuarcy: 0.985\n",
      "Epoch 7 step 1987: training loss: 198.62140155505048\n",
      "Epoch 7 step 1988: training accuarcy: 0.987\n",
      "Epoch 7 step 1988: training loss: 198.78647780080698\n",
      "Epoch 7 step 1989: training accuarcy: 0.9895\n",
      "Epoch 7 step 1989: training loss: 200.68356044309226\n",
      "Epoch 7 step 1990: training accuarcy: 0.989\n",
      "Epoch 7 step 1990: training loss: 197.96384078905317\n",
      "Epoch 7 step 1991: training accuarcy: 0.9895\n",
      "Epoch 7 step 1991: training loss: 202.32091299680184\n",
      "Epoch 7 step 1992: training accuarcy: 0.988\n",
      "Epoch 7 step 1992: training loss: 207.1027944162582\n",
      "Epoch 7 step 1993: training accuarcy: 0.983\n",
      "Epoch 7 step 1993: training loss: 198.69508829122765\n",
      "Epoch 7 step 1994: training accuarcy: 0.985\n",
      "Epoch 7 step 1994: training loss: 200.68728667292635\n",
      "Epoch 7 step 1995: training accuarcy: 0.988\n",
      "Epoch 7 step 1995: training loss: 201.70874841066\n",
      "Epoch 7 step 1996: training accuarcy: 0.988\n",
      "Epoch 7 step 1996: training loss: 204.1426079823822\n",
      "Epoch 7 step 1997: training accuarcy: 0.9845\n",
      "Epoch 7 step 1997: training loss: 200.30664951380268\n",
      "Epoch 7 step 1998: training accuarcy: 0.9885\n",
      "Epoch 7 step 1998: training loss: 193.848050018941\n",
      "Epoch 7 step 1999: training accuarcy: 0.9905\n",
      "Epoch 7 step 1999: training loss: 197.39485712266907\n",
      "Epoch 7 step 2000: training accuarcy: 0.9855\n",
      "Epoch 7 step 2000: training loss: 207.72221698265668\n",
      "Epoch 7 step 2001: training accuarcy: 0.9855\n",
      "Epoch 7 step 2001: training loss: 179.10700673977752\n",
      "Epoch 7 step 2002: training accuarcy: 0.994\n",
      "Epoch 7 step 2002: training loss: 201.19844097024207\n",
      "Epoch 7 step 2003: training accuarcy: 0.9875\n",
      "Epoch 7 step 2003: training loss: 185.09509158644244\n",
      "Epoch 7 step 2004: training accuarcy: 0.9885\n",
      "Epoch 7 step 2004: training loss: 191.6743682502882\n",
      "Epoch 7 step 2005: training accuarcy: 0.991\n",
      "Epoch 7 step 2005: training loss: 195.402850712939\n",
      "Epoch 7 step 2006: training accuarcy: 0.9845\n",
      "Epoch 7 step 2006: training loss: 187.46562219729825\n",
      "Epoch 7 step 2007: training accuarcy: 0.989\n",
      "Epoch 7 step 2007: training loss: 193.03300272817557\n",
      "Epoch 7 step 2008: training accuarcy: 0.9885\n",
      "Epoch 7 step 2008: training loss: 199.88918413295312\n",
      "Epoch 7 step 2009: training accuarcy: 0.9845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2009: training loss: 189.85619358721257\n",
      "Epoch 7 step 2010: training accuarcy: 0.989\n",
      "Epoch 7 step 2010: training loss: 190.03284790052487\n",
      "Epoch 7 step 2011: training accuarcy: 0.9855\n",
      "Epoch 7 step 2011: training loss: 188.68278290596368\n",
      "Epoch 7 step 2012: training accuarcy: 0.985\n",
      "Epoch 7 step 2012: training loss: 192.35876979405282\n",
      "Epoch 7 step 2013: training accuarcy: 0.99\n",
      "Epoch 7 step 2013: training loss: 194.51239370097494\n",
      "Epoch 7 step 2014: training accuarcy: 0.989\n",
      "Epoch 7 step 2014: training loss: 198.96255752038684\n",
      "Epoch 7 step 2015: training accuarcy: 0.991\n",
      "Epoch 7 step 2015: training loss: 195.3538354390031\n",
      "Epoch 7 step 2016: training accuarcy: 0.9875\n",
      "Epoch 7 step 2016: training loss: 205.26478259370407\n",
      "Epoch 7 step 2017: training accuarcy: 0.9865\n",
      "Epoch 7 step 2017: training loss: 198.26959940048633\n",
      "Epoch 7 step 2018: training accuarcy: 0.986\n",
      "Epoch 7 step 2018: training loss: 199.2528931640351\n",
      "Epoch 7 step 2019: training accuarcy: 0.9865\n",
      "Epoch 7 step 2019: training loss: 199.7590547529936\n",
      "Epoch 7 step 2020: training accuarcy: 0.9875\n",
      "Epoch 7 step 2020: training loss: 188.27624475792013\n",
      "Epoch 7 step 2021: training accuarcy: 0.989\n",
      "Epoch 7 step 2021: training loss: 181.34892453168501\n",
      "Epoch 7 step 2022: training accuarcy: 0.9935\n",
      "Epoch 7 step 2022: training loss: 197.83602276026335\n",
      "Epoch 7 step 2023: training accuarcy: 0.9895\n",
      "Epoch 7 step 2023: training loss: 195.7729010874447\n",
      "Epoch 7 step 2024: training accuarcy: 0.99\n",
      "Epoch 7 step 2024: training loss: 190.42880594596636\n",
      "Epoch 7 step 2025: training accuarcy: 0.99\n",
      "Epoch 7 step 2025: training loss: 185.79472392948955\n",
      "Epoch 7 step 2026: training accuarcy: 0.9895\n",
      "Epoch 7 step 2026: training loss: 209.58440086630597\n",
      "Epoch 7 step 2027: training accuarcy: 0.9825\n",
      "Epoch 7 step 2027: training loss: 198.38913875452613\n",
      "Epoch 7 step 2028: training accuarcy: 0.9865\n",
      "Epoch 7 step 2028: training loss: 198.44707735483303\n",
      "Epoch 7 step 2029: training accuarcy: 0.9875\n",
      "Epoch 7 step 2029: training loss: 196.30683988173192\n",
      "Epoch 7 step 2030: training accuarcy: 0.9835\n",
      "Epoch 7 step 2030: training loss: 199.60591834061208\n",
      "Epoch 7 step 2031: training accuarcy: 0.9845\n",
      "Epoch 7 step 2031: training loss: 173.67882995352292\n",
      "Epoch 7 step 2032: training accuarcy: 0.993\n",
      "Epoch 7 step 2032: training loss: 194.71329452790442\n",
      "Epoch 7 step 2033: training accuarcy: 0.986\n",
      "Epoch 7 step 2033: training loss: 172.83044820915825\n",
      "Epoch 7 step 2034: training accuarcy: 0.9945\n",
      "Epoch 7 step 2034: training loss: 189.52351565214897\n",
      "Epoch 7 step 2035: training accuarcy: 0.987\n",
      "Epoch 7 step 2035: training loss: 173.64356547589992\n",
      "Epoch 7 step 2036: training accuarcy: 0.993\n",
      "Epoch 7 step 2036: training loss: 197.30656723116135\n",
      "Epoch 7 step 2037: training accuarcy: 0.987\n",
      "Epoch 7 step 2037: training loss: 192.29480510301516\n",
      "Epoch 7 step 2038: training accuarcy: 0.987\n",
      "Epoch 7 step 2038: training loss: 194.53694280264844\n",
      "Epoch 7 step 2039: training accuarcy: 0.987\n",
      "Epoch 7 step 2039: training loss: 187.69421798533608\n",
      "Epoch 7 step 2040: training accuarcy: 0.9905\n",
      "Epoch 7 step 2040: training loss: 198.555644407369\n",
      "Epoch 7 step 2041: training accuarcy: 0.983\n",
      "Epoch 7 step 2041: training loss: 191.93193296575194\n",
      "Epoch 7 step 2042: training accuarcy: 0.9875\n",
      "Epoch 7 step 2042: training loss: 192.45203923080987\n",
      "Epoch 7 step 2043: training accuarcy: 0.9905\n",
      "Epoch 7 step 2043: training loss: 189.39820519648387\n",
      "Epoch 7 step 2044: training accuarcy: 0.992\n",
      "Epoch 7 step 2044: training loss: 190.69307676882633\n",
      "Epoch 7 step 2045: training accuarcy: 0.991\n",
      "Epoch 7 step 2045: training loss: 196.52907024321266\n",
      "Epoch 7 step 2046: training accuarcy: 0.988\n",
      "Epoch 7 step 2046: training loss: 191.86125264137468\n",
      "Epoch 7 step 2047: training accuarcy: 0.9825\n",
      "Epoch 7 step 2047: training loss: 206.69035759094822\n",
      "Epoch 7 step 2048: training accuarcy: 0.985\n",
      "Epoch 7 step 2048: training loss: 185.41801711236423\n",
      "Epoch 7 step 2049: training accuarcy: 0.9855\n",
      "Epoch 7 step 2049: training loss: 182.71136341669524\n",
      "Epoch 7 step 2050: training accuarcy: 0.9935\n",
      "Epoch 7 step 2050: training loss: 187.49942972915073\n",
      "Epoch 7 step 2051: training accuarcy: 0.989\n",
      "Epoch 7 step 2051: training loss: 189.62607332813013\n",
      "Epoch 7 step 2052: training accuarcy: 0.988\n",
      "Epoch 7 step 2052: training loss: 194.80141304551734\n",
      "Epoch 7 step 2053: training accuarcy: 0.9875\n",
      "Epoch 7 step 2053: training loss: 205.79302416930517\n",
      "Epoch 7 step 2054: training accuarcy: 0.983\n",
      "Epoch 7 step 2054: training loss: 184.89396442867303\n",
      "Epoch 7 step 2055: training accuarcy: 0.9855\n",
      "Epoch 7 step 2055: training loss: 183.95724765061362\n",
      "Epoch 7 step 2056: training accuarcy: 0.9895\n",
      "Epoch 7 step 2056: training loss: 192.08901278275178\n",
      "Epoch 7 step 2057: training accuarcy: 0.988\n",
      "Epoch 7 step 2057: training loss: 187.84568237999935\n",
      "Epoch 7 step 2058: training accuarcy: 0.987\n",
      "Epoch 7 step 2058: training loss: 184.0788129070872\n",
      "Epoch 7 step 2059: training accuarcy: 0.9885\n",
      "Epoch 7 step 2059: training loss: 193.69681684047129\n",
      "Epoch 7 step 2060: training accuarcy: 0.9865\n",
      "Epoch 7 step 2060: training loss: 201.23998206451972\n",
      "Epoch 7 step 2061: training accuarcy: 0.982\n",
      "Epoch 7 step 2061: training loss: 192.2542157447082\n",
      "Epoch 7 step 2062: training accuarcy: 0.9865\n",
      "Epoch 7 step 2062: training loss: 180.66511268384042\n",
      "Epoch 7 step 2063: training accuarcy: 0.991\n",
      "Epoch 7 step 2063: training loss: 195.98270056589848\n",
      "Epoch 7 step 2064: training accuarcy: 0.9845\n",
      "Epoch 7 step 2064: training loss: 192.9654278037764\n",
      "Epoch 7 step 2065: training accuarcy: 0.9895\n",
      "Epoch 7 step 2065: training loss: 195.50076917454024\n",
      "Epoch 7 step 2066: training accuarcy: 0.984\n",
      "Epoch 7 step 2066: training loss: 205.94770168418367\n",
      "Epoch 7 step 2067: training accuarcy: 0.9845\n",
      "Epoch 7 step 2067: training loss: 192.32612773351752\n",
      "Epoch 7 step 2068: training accuarcy: 0.987\n",
      "Epoch 7 step 2068: training loss: 178.38090083410864\n",
      "Epoch 7 step 2069: training accuarcy: 0.992\n",
      "Epoch 7 step 2069: training loss: 190.74094878630427\n",
      "Epoch 7 step 2070: training accuarcy: 0.991\n",
      "Epoch 7 step 2070: training loss: 185.96453725993788\n",
      "Epoch 7 step 2071: training accuarcy: 0.99\n",
      "Epoch 7 step 2071: training loss: 182.22145757716066\n",
      "Epoch 7 step 2072: training accuarcy: 0.9895\n",
      "Epoch 7 step 2072: training loss: 192.0989869058445\n",
      "Epoch 7 step 2073: training accuarcy: 0.9875\n",
      "Epoch 7 step 2073: training loss: 185.49345528783212\n",
      "Epoch 7 step 2074: training accuarcy: 0.9885\n",
      "Epoch 7 step 2074: training loss: 184.29696136444073\n",
      "Epoch 7 step 2075: training accuarcy: 0.9885\n",
      "Epoch 7 step 2075: training loss: 192.48282668414708\n",
      "Epoch 7 step 2076: training accuarcy: 0.988\n",
      "Epoch 7 step 2076: training loss: 191.0164560232529\n",
      "Epoch 7 step 2077: training accuarcy: 0.9855\n",
      "Epoch 7 step 2077: training loss: 183.9878441542758\n",
      "Epoch 7 step 2078: training accuarcy: 0.9905\n",
      "Epoch 7 step 2078: training loss: 200.77817819576717\n",
      "Epoch 7 step 2079: training accuarcy: 0.983\n",
      "Epoch 7 step 2079: training loss: 194.41916817337096\n",
      "Epoch 7 step 2080: training accuarcy: 0.985\n",
      "Epoch 7 step 2080: training loss: 190.40002338679088\n",
      "Epoch 7 step 2081: training accuarcy: 0.984\n",
      "Epoch 7 step 2081: training loss: 191.12751448896162\n",
      "Epoch 7 step 2082: training accuarcy: 0.9845\n",
      "Epoch 7 step 2082: training loss: 192.18796050301884\n",
      "Epoch 7 step 2083: training accuarcy: 0.9905\n",
      "Epoch 7 step 2083: training loss: 192.56957501876468\n",
      "Epoch 7 step 2084: training accuarcy: 0.9825\n",
      "Epoch 7 step 2084: training loss: 186.33359235461464\n",
      "Epoch 7 step 2085: training accuarcy: 0.989\n",
      "Epoch 7 step 2085: training loss: 186.03518050064932\n",
      "Epoch 7 step 2086: training accuarcy: 0.9885\n",
      "Epoch 7 step 2086: training loss: 192.14680697194305\n",
      "Epoch 7 step 2087: training accuarcy: 0.9835\n",
      "Epoch 7 step 2087: training loss: 186.64037300681636\n",
      "Epoch 7 step 2088: training accuarcy: 0.9895\n",
      "Epoch 7 step 2088: training loss: 171.380399580007\n",
      "Epoch 7 step 2089: training accuarcy: 0.9895\n",
      "Epoch 7 step 2089: training loss: 186.04106786328725\n",
      "Epoch 7 step 2090: training accuarcy: 0.9885\n",
      "Epoch 7 step 2090: training loss: 186.28147805399107\n",
      "Epoch 7 step 2091: training accuarcy: 0.989\n",
      "Epoch 7 step 2091: training loss: 212.10677828226704\n",
      "Epoch 7 step 2092: training accuarcy: 0.9795\n",
      "Epoch 7 step 2092: training loss: 185.1045714290892\n",
      "Epoch 7 step 2093: training accuarcy: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2093: training loss: 179.8702585323107\n",
      "Epoch 7 step 2094: training accuarcy: 0.9905\n",
      "Epoch 7 step 2094: training loss: 174.41150387827287\n",
      "Epoch 7 step 2095: training accuarcy: 0.9925\n",
      "Epoch 7 step 2095: training loss: 198.10309414644811\n",
      "Epoch 7 step 2096: training accuarcy: 0.985\n",
      "Epoch 7 step 2096: training loss: 180.61786192221035\n",
      "Epoch 7 step 2097: training accuarcy: 0.99\n",
      "Epoch 7 step 2097: training loss: 189.88928679214618\n",
      "Epoch 7 step 2098: training accuarcy: 0.99\n",
      "Epoch 7 step 2098: training loss: 189.52055193954126\n",
      "Epoch 7 step 2099: training accuarcy: 0.985\n",
      "Epoch 7 step 2099: training loss: 185.10759346590456\n",
      "Epoch 7 step 2100: training accuarcy: 0.9885\n",
      "Epoch 7 step 2100: training loss: 186.21774012971227\n",
      "Epoch 7 step 2101: training accuarcy: 0.987\n",
      "Epoch 7 step 2101: training loss: 188.7504374050971\n",
      "Epoch 7 step 2102: training accuarcy: 0.987\n",
      "Epoch 7 step 2102: training loss: 177.66305252278585\n",
      "Epoch 7 step 2103: training accuarcy: 0.9925\n",
      "Epoch 7 step 2103: training loss: 87.69625613753814\n",
      "Epoch 7 step 2104: training accuarcy: 0.9782051282051282\n",
      "Epoch 7: train loss 197.56988383337168, train accuarcy 0.9818708300590515\n",
      "Epoch 7: valid loss 689.107076028713, valid accuarcy 0.8642611503601074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [17:43<00:00, 132.13s/it]\n"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=8,\n",
    "                  loss_callback=trans_loss_callback,\n",
    "                  log_dir=get_log_dir('topcoder', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:02:29.409573Z",
     "start_time": "2019-09-25T14:02:29.280561Z"
    }
   },
   "outputs": [],
   "source": [
    "del trans_model\n",
    "T.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
