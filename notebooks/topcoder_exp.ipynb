{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:01:30.581717Z",
     "start_time": "2019-09-25T14:01:30.456659Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:45:50.828532Z",
     "start_time": "2019-10-07T13:45:50.821530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\python\\recommender\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:45:52.322439Z",
     "start_time": "2019-10-07T13:45:51.797442Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import get_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:46:06.272811Z",
     "start_time": "2019-10-07T13:46:06.246782Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import SeqTopcoder\n",
    "from models import FMLearner, TorchFM, TorchHrmFM, TorchPrmeFM, TorchTransFM\n",
    "from models.fm_learner import simple_loss, trans_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:47:03.385757Z",
     "start_time": "2019-10-07T13:47:03.382755Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = T.cuda.current_device()\n",
    "BATCH = 2000\n",
    "SHUFFLE = True\n",
    "WORKERS = 0\n",
    "NEG_SAMPLE = 5\n",
    "REGS_PATH = Path(\"./inputs/topcoder/regs.csv\")\n",
    "CHAG_PATH = Path(\"./inputs/topcoder/challenge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:47:20.834884Z",
     "start_time": "2019-10-07T13:47:18.789884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset in inputs\\topcoder\\regs.csv\n",
      "Original regs shape: (610025, 3)\n",
      "Original registants size: 60017\n",
      "Original challenges size: 39916\n",
      "Filter dataframe shape: (544568, 3)\n",
      "Index(['challengeId', 'period', 'date', 'prizes', 'technologies', 'platforms'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datasets.torch_topcoder.SeqTopcoder at 0x29988593ba8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = SeqTopcoder(regs_path=REGS_PATH, chag_path=CHAG_PATH)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:00.555397Z",
     "start_time": "2019-10-07T13:48:00.551397Z"
    }
   },
   "outputs": [],
   "source": [
    "db.config_db(batch_size=BATCH,\n",
    "             shuffle=SHUFFLE,\n",
    "             num_workers=WORKERS,\n",
    "             device=DEVICE,\n",
    "             neg_sample=NEG_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:01.209575Z",
     "start_time": "2019-10-07T13:48:01.206573Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:03.712165Z",
     "start_time": "2019-10-07T13:48:03.709161Z"
    }
   },
   "outputs": [],
   "source": [
    "# regst setting\n",
    "LINEAR_REG = 1\n",
    "EMB_REG = 1\n",
    "TRANS_REG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:04.114533Z",
     "start_time": "2019-10-07T13:48:04.109559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_loss at 0x00000299882EC7B8>, 1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_loss_callback = partial(simple_loss, LINEAR_REG, EMB_REG)\n",
    "simple_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:04.592906Z",
     "start_time": "2019-10-07T13:48:04.588877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_loss at 0x00000299882CB950>, 1, 1, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_loss_callback = partial(trans_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_loss_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "#### Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:08.280870Z",
     "start_time": "2019-10-07T13:48:08.277865Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:09.417726Z",
     "start_time": "2019-10-07T13:48:09.413726Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DECAY_FREQ = 1000\n",
    "DECAY_GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:11.257785Z",
     "start_time": "2019-10-07T13:48:11.006785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchFM()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model = TorchFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:15.241827Z",
     "start_time": "2019-10-07T13:48:15.237838Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(fm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:19.052146Z",
     "start_time": "2019-10-07T13:48:16.627983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x29992036438>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_learner = FMLearner(fm_model, adam_opt, schedular, db)\n",
    "fm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:25.856807Z",
     "start_time": "2019-10-07T13:48:25.853806Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:16:03.054318Z",
     "start_time": "2019-10-07T13:48:46.441352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 38491.63848501149\n",
      "Epoch 0 step 1: training accuarcy: 0.322\n",
      "Epoch 0 step 1: training loss: 37296.29721029236\n",
      "Epoch 0 step 2: training accuarcy: 0.3655\n",
      "Epoch 0 step 2: training loss: 36060.13888788689\n",
      "Epoch 0 step 3: training accuarcy: 0.4345\n",
      "Epoch 0 step 3: training loss: 34891.4932175206\n",
      "Epoch 0 step 4: training accuarcy: 0.5075000000000001\n",
      "Epoch 0 step 4: training loss: 33787.61291161411\n",
      "Epoch 0 step 5: training accuarcy: 0.5615\n",
      "Epoch 0 step 5: training loss: 32763.335192424827\n",
      "Epoch 0 step 6: training accuarcy: 0.6035\n",
      "Epoch 0 step 6: training loss: 31799.103568248593\n",
      "Epoch 0 step 7: training accuarcy: 0.6445\n",
      "Epoch 0 step 7: training loss: 30902.477741931438\n",
      "Epoch 0 step 8: training accuarcy: 0.655\n",
      "Epoch 0 step 8: training loss: 29939.08243198299\n",
      "Epoch 0 step 9: training accuarcy: 0.6995\n",
      "Epoch 0 step 9: training loss: 29042.39969591284\n",
      "Epoch 0 step 10: training accuarcy: 0.735\n",
      "Epoch 0 step 10: training loss: 28232.15831329322\n",
      "Epoch 0 step 11: training accuarcy: 0.745\n",
      "Epoch 0 step 11: training loss: 27420.22466787341\n",
      "Epoch 0 step 12: training accuarcy: 0.753\n",
      "Epoch 0 step 12: training loss: 26599.44764356879\n",
      "Epoch 0 step 13: training accuarcy: 0.7825\n",
      "Epoch 0 step 13: training loss: 25836.831869752612\n",
      "Epoch 0 step 14: training accuarcy: 0.8005\n",
      "Epoch 0 step 14: training loss: 25091.038546776508\n",
      "Epoch 0 step 15: training accuarcy: 0.811\n",
      "Epoch 0 step 15: training loss: 24412.185510526546\n",
      "Epoch 0 step 16: training accuarcy: 0.809\n",
      "Epoch 0 step 16: training loss: 23696.403784269645\n",
      "Epoch 0 step 17: training accuarcy: 0.8200000000000001\n",
      "Epoch 0 step 17: training loss: 22992.703931223296\n",
      "Epoch 0 step 18: training accuarcy: 0.847\n",
      "Epoch 0 step 18: training loss: 22353.081377723374\n",
      "Epoch 0 step 19: training accuarcy: 0.84\n",
      "Epoch 0 step 19: training loss: 21675.594449579065\n",
      "Epoch 0 step 20: training accuarcy: 0.8535\n",
      "Epoch 0 step 20: training loss: 21068.6656800561\n",
      "Epoch 0 step 21: training accuarcy: 0.853\n",
      "Epoch 0 step 21: training loss: 20439.825773376644\n",
      "Epoch 0 step 22: training accuarcy: 0.866\n",
      "Epoch 0 step 22: training loss: 19855.546843951903\n",
      "Epoch 0 step 23: training accuarcy: 0.8655\n",
      "Epoch 0 step 23: training loss: 19267.40439581441\n",
      "Epoch 0 step 24: training accuarcy: 0.8745\n",
      "Epoch 0 step 24: training loss: 18711.896051614196\n",
      "Epoch 0 step 25: training accuarcy: 0.8755000000000001\n",
      "Epoch 0 step 25: training loss: 18160.770045173995\n",
      "Epoch 0 step 26: training accuarcy: 0.8825000000000001\n",
      "Epoch 0 step 26: training loss: 17602.763566712394\n",
      "Epoch 0 step 27: training accuarcy: 0.8875000000000001\n",
      "Epoch 0 step 27: training loss: 17123.64893710899\n",
      "Epoch 0 step 28: training accuarcy: 0.876\n",
      "Epoch 0 step 28: training loss: 16588.092502401145\n",
      "Epoch 0 step 29: training accuarcy: 0.89\n",
      "Epoch 0 step 29: training loss: 16089.417395838722\n",
      "Epoch 0 step 30: training accuarcy: 0.895\n",
      "Epoch 0 step 30: training loss: 15621.737500368174\n",
      "Epoch 0 step 31: training accuarcy: 0.885\n",
      "Epoch 0 step 31: training loss: 15132.021348650382\n",
      "Epoch 0 step 32: training accuarcy: 0.895\n",
      "Epoch 0 step 32: training loss: 14675.958204688386\n",
      "Epoch 0 step 33: training accuarcy: 0.901\n",
      "Epoch 0 step 33: training loss: 14244.265519501458\n",
      "Epoch 0 step 34: training accuarcy: 0.8975\n",
      "Epoch 0 step 34: training loss: 13802.10560327962\n",
      "Epoch 0 step 35: training accuarcy: 0.9095\n",
      "Epoch 0 step 35: training loss: 13391.467300249005\n",
      "Epoch 0 step 36: training accuarcy: 0.9105\n",
      "Epoch 0 step 36: training loss: 12988.219688652234\n",
      "Epoch 0 step 37: training accuarcy: 0.9075\n",
      "Epoch 0 step 37: training loss: 12592.63761790463\n",
      "Epoch 0 step 38: training accuarcy: 0.9145\n",
      "Epoch 0 step 38: training loss: 12221.746040932972\n",
      "Epoch 0 step 39: training accuarcy: 0.907\n",
      "Epoch 0 step 39: training loss: 11851.100199564473\n",
      "Epoch 0 step 40: training accuarcy: 0.9065\n",
      "Epoch 0 step 40: training loss: 11457.460313458674\n",
      "Epoch 0 step 41: training accuarcy: 0.9175\n",
      "Epoch 0 step 41: training loss: 11144.584127435515\n",
      "Epoch 0 step 42: training accuarcy: 0.9105\n",
      "Epoch 0 step 42: training loss: 10764.284034836273\n",
      "Epoch 0 step 43: training accuarcy: 0.922\n",
      "Epoch 0 step 43: training loss: 10457.633004806783\n",
      "Epoch 0 step 44: training accuarcy: 0.9215\n",
      "Epoch 0 step 44: training loss: 10121.70448201118\n",
      "Epoch 0 step 45: training accuarcy: 0.919\n",
      "Epoch 0 step 45: training loss: 9775.280558144634\n",
      "Epoch 0 step 46: training accuarcy: 0.9345\n",
      "Epoch 0 step 46: training loss: 9506.733449012654\n",
      "Epoch 0 step 47: training accuarcy: 0.929\n",
      "Epoch 0 step 47: training loss: 9201.66573922814\n",
      "Epoch 0 step 48: training accuarcy: 0.925\n",
      "Epoch 0 step 48: training loss: 8918.366206453653\n",
      "Epoch 0 step 49: training accuarcy: 0.925\n",
      "Epoch 0 step 49: training loss: 8631.818920047172\n",
      "Epoch 0 step 50: training accuarcy: 0.935\n",
      "Epoch 0 step 50: training loss: 8366.043985889974\n",
      "Epoch 0 step 51: training accuarcy: 0.925\n",
      "Epoch 0 step 51: training loss: 8108.959375962602\n",
      "Epoch 0 step 52: training accuarcy: 0.933\n",
      "Epoch 0 step 52: training loss: 7833.58547294475\n",
      "Epoch 0 step 53: training accuarcy: 0.936\n",
      "Epoch 0 step 53: training loss: 7589.53026745598\n",
      "Epoch 0 step 54: training accuarcy: 0.9380000000000001\n",
      "Epoch 0 step 54: training loss: 7350.057652566216\n",
      "Epoch 0 step 55: training accuarcy: 0.9365\n",
      "Epoch 0 step 55: training loss: 7104.5174982045055\n",
      "Epoch 0 step 56: training accuarcy: 0.9425\n",
      "Epoch 0 step 56: training loss: 6899.835507684425\n",
      "Epoch 0 step 57: training accuarcy: 0.9380000000000001\n",
      "Epoch 0 step 57: training loss: 6690.3790457369705\n",
      "Epoch 0 step 58: training accuarcy: 0.9345\n",
      "Epoch 0 step 58: training loss: 6468.875830008624\n",
      "Epoch 0 step 59: training accuarcy: 0.9400000000000001\n",
      "Epoch 0 step 59: training loss: 6245.692179115162\n",
      "Epoch 0 step 60: training accuarcy: 0.9450000000000001\n",
      "Epoch 0 step 60: training loss: 6041.743609537742\n",
      "Epoch 0 step 61: training accuarcy: 0.9475\n",
      "Epoch 0 step 61: training loss: 5844.010925368999\n",
      "Epoch 0 step 62: training accuarcy: 0.9530000000000001\n",
      "Epoch 0 step 62: training loss: 5659.670080803115\n",
      "Epoch 0 step 63: training accuarcy: 0.9530000000000001\n",
      "Epoch 0 step 63: training loss: 5487.320905987905\n",
      "Epoch 0 step 64: training accuarcy: 0.9475\n",
      "Epoch 0 step 64: training loss: 5322.0229886627685\n",
      "Epoch 0 step 65: training accuarcy: 0.9450000000000001\n",
      "Epoch 0 step 65: training loss: 5148.619280295559\n",
      "Epoch 0 step 66: training accuarcy: 0.9505\n",
      "Epoch 0 step 66: training loss: 4982.3422068692935\n",
      "Epoch 0 step 67: training accuarcy: 0.9510000000000001\n",
      "Epoch 0 step 67: training loss: 4830.23759319728\n",
      "Epoch 0 step 68: training accuarcy: 0.9465\n",
      "Epoch 0 step 68: training loss: 4654.845139074605\n",
      "Epoch 0 step 69: training accuarcy: 0.9585\n",
      "Epoch 0 step 69: training loss: 4501.442291140006\n",
      "Epoch 0 step 70: training accuarcy: 0.9575\n",
      "Epoch 0 step 70: training loss: 4360.069561033619\n",
      "Epoch 0 step 71: training accuarcy: 0.9615\n",
      "Epoch 0 step 71: training loss: 4205.039173847774\n",
      "Epoch 0 step 72: training accuarcy: 0.9655\n",
      "Epoch 0 step 72: training loss: 4090.553212397454\n",
      "Epoch 0 step 73: training accuarcy: 0.9580000000000001\n",
      "Epoch 0 step 73: training loss: 3955.096856559863\n",
      "Epoch 0 step 74: training accuarcy: 0.9585\n",
      "Epoch 0 step 74: training loss: 3829.711203282753\n",
      "Epoch 0 step 75: training accuarcy: 0.961\n",
      "Epoch 0 step 75: training loss: 3714.4850032444438\n",
      "Epoch 0 step 76: training accuarcy: 0.9550000000000001\n",
      "Epoch 0 step 76: training loss: 3582.207548662187\n",
      "Epoch 0 step 77: training accuarcy: 0.9575\n",
      "Epoch 0 step 77: training loss: 3471.643760382932\n",
      "Epoch 0 step 78: training accuarcy: 0.96\n",
      "Epoch 0 step 78: training loss: 3349.905415363554\n",
      "Epoch 0 step 79: training accuarcy: 0.9625\n",
      "Epoch 0 step 79: training loss: 3245.311495587963\n",
      "Epoch 0 step 80: training accuarcy: 0.9675\n",
      "Epoch 0 step 80: training loss: 3145.9133281619374\n",
      "Epoch 0 step 81: training accuarcy: 0.962\n",
      "Epoch 0 step 81: training loss: 3043.860927993528\n",
      "Epoch 0 step 82: training accuarcy: 0.964\n",
      "Epoch 0 step 82: training loss: 2973.8506328356316\n",
      "Epoch 0 step 83: training accuarcy: 0.9525\n",
      "Epoch 0 step 83: training loss: 2853.928279108004\n",
      "Epoch 0 step 84: training accuarcy: 0.964\n",
      "Epoch 0 step 84: training loss: 2747.7360921541954\n",
      "Epoch 0 step 85: training accuarcy: 0.968\n",
      "Epoch 0 step 85: training loss: 2676.952565051386\n",
      "Epoch 0 step 86: training accuarcy: 0.9635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 86: training loss: 2583.2870419622363\n",
      "Epoch 0 step 87: training accuarcy: 0.9675\n",
      "Epoch 0 step 87: training loss: 2510.1658766706582\n",
      "Epoch 0 step 88: training accuarcy: 0.966\n",
      "Epoch 0 step 88: training loss: 2429.3595244899984\n",
      "Epoch 0 step 89: training accuarcy: 0.967\n",
      "Epoch 0 step 89: training loss: 2335.163170841124\n",
      "Epoch 0 step 90: training accuarcy: 0.972\n",
      "Epoch 0 step 90: training loss: 2266.4117187967418\n",
      "Epoch 0 step 91: training accuarcy: 0.9705\n",
      "Epoch 0 step 91: training loss: 2210.5986547968396\n",
      "Epoch 0 step 92: training accuarcy: 0.9655\n",
      "Epoch 0 step 92: training loss: 2135.9960769289733\n",
      "Epoch 0 step 93: training accuarcy: 0.964\n",
      "Epoch 0 step 93: training loss: 2064.2956218820577\n",
      "Epoch 0 step 94: training accuarcy: 0.971\n",
      "Epoch 0 step 94: training loss: 2013.3649624560735\n",
      "Epoch 0 step 95: training accuarcy: 0.968\n",
      "Epoch 0 step 95: training loss: 1942.529633240988\n",
      "Epoch 0 step 96: training accuarcy: 0.968\n",
      "Epoch 0 step 96: training loss: 1877.0055457768146\n",
      "Epoch 0 step 97: training accuarcy: 0.969\n",
      "Epoch 0 step 97: training loss: 1816.8614230742423\n",
      "Epoch 0 step 98: training accuarcy: 0.974\n",
      "Epoch 0 step 98: training loss: 1764.9083965094067\n",
      "Epoch 0 step 99: training accuarcy: 0.9725\n",
      "Epoch 0 step 99: training loss: 1702.932582801594\n",
      "Epoch 0 step 100: training accuarcy: 0.9725\n",
      "Epoch 0 step 100: training loss: 1672.550876144226\n",
      "Epoch 0 step 101: training accuarcy: 0.969\n",
      "Epoch 0 step 101: training loss: 1592.3881072082104\n",
      "Epoch 0 step 102: training accuarcy: 0.9735\n",
      "Epoch 0 step 102: training loss: 1543.0901662072267\n",
      "Epoch 0 step 103: training accuarcy: 0.975\n",
      "Epoch 0 step 103: training loss: 1507.121459857699\n",
      "Epoch 0 step 104: training accuarcy: 0.9715\n",
      "Epoch 0 step 104: training loss: 1463.8000107670382\n",
      "Epoch 0 step 105: training accuarcy: 0.972\n",
      "Epoch 0 step 105: training loss: 1423.126390267546\n",
      "Epoch 0 step 106: training accuarcy: 0.9765\n",
      "Epoch 0 step 106: training loss: 1379.9381195342398\n",
      "Epoch 0 step 107: training accuarcy: 0.9715\n",
      "Epoch 0 step 107: training loss: 1326.3005380762438\n",
      "Epoch 0 step 108: training accuarcy: 0.9785\n",
      "Epoch 0 step 108: training loss: 1297.5728196289583\n",
      "Epoch 0 step 109: training accuarcy: 0.974\n",
      "Epoch 0 step 109: training loss: 1244.391799505859\n",
      "Epoch 0 step 110: training accuarcy: 0.9805\n",
      "Epoch 0 step 110: training loss: 1221.6595584792049\n",
      "Epoch 0 step 111: training accuarcy: 0.9755\n",
      "Epoch 0 step 111: training loss: 1184.3906622526608\n",
      "Epoch 0 step 112: training accuarcy: 0.9755\n",
      "Epoch 0 step 112: training loss: 1150.2931805217681\n",
      "Epoch 0 step 113: training accuarcy: 0.973\n",
      "Epoch 0 step 113: training loss: 1117.1630736719817\n",
      "Epoch 0 step 114: training accuarcy: 0.9765\n",
      "Epoch 0 step 114: training loss: 1073.8375745918424\n",
      "Epoch 0 step 115: training accuarcy: 0.981\n",
      "Epoch 0 step 115: training loss: 1050.9268312610266\n",
      "Epoch 0 step 116: training accuarcy: 0.979\n",
      "Epoch 0 step 116: training loss: 1039.4806683660029\n",
      "Epoch 0 step 117: training accuarcy: 0.974\n",
      "Epoch 0 step 117: training loss: 998.3235103089594\n",
      "Epoch 0 step 118: training accuarcy: 0.9775\n",
      "Epoch 0 step 118: training loss: 968.2772098926362\n",
      "Epoch 0 step 119: training accuarcy: 0.9785\n",
      "Epoch 0 step 119: training loss: 935.5210849885625\n",
      "Epoch 0 step 120: training accuarcy: 0.981\n",
      "Epoch 0 step 120: training loss: 910.1186829485497\n",
      "Epoch 0 step 121: training accuarcy: 0.977\n",
      "Epoch 0 step 121: training loss: 901.2313159532218\n",
      "Epoch 0 step 122: training accuarcy: 0.9755\n",
      "Epoch 0 step 122: training loss: 864.3035813606518\n",
      "Epoch 0 step 123: training accuarcy: 0.9815\n",
      "Epoch 0 step 123: training loss: 842.8397655863598\n",
      "Epoch 0 step 124: training accuarcy: 0.978\n",
      "Epoch 0 step 124: training loss: 831.0117344691166\n",
      "Epoch 0 step 125: training accuarcy: 0.976\n",
      "Epoch 0 step 125: training loss: 795.6207902899271\n",
      "Epoch 0 step 126: training accuarcy: 0.981\n",
      "Epoch 0 step 126: training loss: 775.0706508050649\n",
      "Epoch 0 step 127: training accuarcy: 0.981\n",
      "Epoch 0 step 127: training loss: 763.9601602421146\n",
      "Epoch 0 step 128: training accuarcy: 0.979\n",
      "Epoch 0 step 128: training loss: 732.7174926344609\n",
      "Epoch 0 step 129: training accuarcy: 0.982\n",
      "Epoch 0 step 129: training loss: 732.9689347647211\n",
      "Epoch 0 step 130: training accuarcy: 0.972\n",
      "Epoch 0 step 130: training loss: 703.5404910788868\n",
      "Epoch 0 step 131: training accuarcy: 0.98\n",
      "Epoch 0 step 131: training loss: 690.7025290855861\n",
      "Epoch 0 step 132: training accuarcy: 0.982\n",
      "Epoch 0 step 132: training loss: 679.9284350901034\n",
      "Epoch 0 step 133: training accuarcy: 0.973\n",
      "Epoch 0 step 133: training loss: 662.2147259641151\n",
      "Epoch 0 step 134: training accuarcy: 0.975\n",
      "Epoch 0 step 134: training loss: 637.6537778336086\n",
      "Epoch 0 step 135: training accuarcy: 0.9855\n",
      "Epoch 0 step 135: training loss: 613.4298792193714\n",
      "Epoch 0 step 136: training accuarcy: 0.985\n",
      "Epoch 0 step 136: training loss: 593.556297748961\n",
      "Epoch 0 step 137: training accuarcy: 0.9865\n",
      "Epoch 0 step 137: training loss: 577.5733085410516\n",
      "Epoch 0 step 138: training accuarcy: 0.9865\n",
      "Epoch 0 step 138: training loss: 567.103495094784\n",
      "Epoch 0 step 139: training accuarcy: 0.9845\n",
      "Epoch 0 step 139: training loss: 559.4757832593832\n",
      "Epoch 0 step 140: training accuarcy: 0.9875\n",
      "Epoch 0 step 140: training loss: 555.8801977871586\n",
      "Epoch 0 step 141: training accuarcy: 0.982\n",
      "Epoch 0 step 141: training loss: 531.4907666058996\n",
      "Epoch 0 step 142: training accuarcy: 0.9835\n",
      "Epoch 0 step 142: training loss: 517.6249908984768\n",
      "Epoch 0 step 143: training accuarcy: 0.9815\n",
      "Epoch 0 step 143: training loss: 494.4304309221748\n",
      "Epoch 0 step 144: training accuarcy: 0.9895\n",
      "Epoch 0 step 144: training loss: 518.3061400815453\n",
      "Epoch 0 step 145: training accuarcy: 0.9815\n",
      "Epoch 0 step 145: training loss: 499.45178919904686\n",
      "Epoch 0 step 146: training accuarcy: 0.9835\n",
      "Epoch 0 step 146: training loss: 476.6750675648389\n",
      "Epoch 0 step 147: training accuarcy: 0.984\n",
      "Epoch 0 step 147: training loss: 487.47651764764674\n",
      "Epoch 0 step 148: training accuarcy: 0.98\n",
      "Epoch 0 step 148: training loss: 471.15844233118935\n",
      "Epoch 0 step 149: training accuarcy: 0.9855\n",
      "Epoch 0 step 149: training loss: 451.12044079951716\n",
      "Epoch 0 step 150: training accuarcy: 0.9855\n",
      "Epoch 0 step 150: training loss: 452.12306783376044\n",
      "Epoch 0 step 151: training accuarcy: 0.983\n",
      "Epoch 0 step 151: training loss: 442.85694673622595\n",
      "Epoch 0 step 152: training accuarcy: 0.9835\n",
      "Epoch 0 step 152: training loss: 425.4126457064955\n",
      "Epoch 0 step 153: training accuarcy: 0.9825\n",
      "Epoch 0 step 153: training loss: 414.4253110557456\n",
      "Epoch 0 step 154: training accuarcy: 0.9865\n",
      "Epoch 0 step 154: training loss: 409.60491207726807\n",
      "Epoch 0 step 155: training accuarcy: 0.983\n",
      "Epoch 0 step 155: training loss: 408.31044895466323\n",
      "Epoch 0 step 156: training accuarcy: 0.9825\n",
      "Epoch 0 step 156: training loss: 409.1616702959921\n",
      "Epoch 0 step 157: training accuarcy: 0.9855\n",
      "Epoch 0 step 157: training loss: 396.1373099961279\n",
      "Epoch 0 step 158: training accuarcy: 0.9835\n",
      "Epoch 0 step 158: training loss: 376.4799109447185\n",
      "Epoch 0 step 159: training accuarcy: 0.9875\n",
      "Epoch 0 step 159: training loss: 383.74771068493317\n",
      "Epoch 0 step 160: training accuarcy: 0.9865\n",
      "Epoch 0 step 160: training loss: 368.6366767116862\n",
      "Epoch 0 step 161: training accuarcy: 0.9875\n",
      "Epoch 0 step 161: training loss: 366.6284803823073\n",
      "Epoch 0 step 162: training accuarcy: 0.9835\n",
      "Epoch 0 step 162: training loss: 356.59815180473817\n",
      "Epoch 0 step 163: training accuarcy: 0.986\n",
      "Epoch 0 step 163: training loss: 350.0584246270457\n",
      "Epoch 0 step 164: training accuarcy: 0.9855\n",
      "Epoch 0 step 164: training loss: 339.2852592917062\n",
      "Epoch 0 step 165: training accuarcy: 0.9875\n",
      "Epoch 0 step 165: training loss: 349.65635816068044\n",
      "Epoch 0 step 166: training accuarcy: 0.987\n",
      "Epoch 0 step 166: training loss: 344.24478742239825\n",
      "Epoch 0 step 167: training accuarcy: 0.9835\n",
      "Epoch 0 step 167: training loss: 328.3033923005176\n",
      "Epoch 0 step 168: training accuarcy: 0.9895\n",
      "Epoch 0 step 168: training loss: 315.65277362653165\n",
      "Epoch 0 step 169: training accuarcy: 0.992\n",
      "Epoch 0 step 169: training loss: 326.11261096479\n",
      "Epoch 0 step 170: training accuarcy: 0.9835\n",
      "Epoch 0 step 170: training loss: 311.6225090959041\n",
      "Epoch 0 step 171: training accuarcy: 0.989\n",
      "Epoch 0 step 171: training loss: 307.7609697673733\n",
      "Epoch 0 step 172: training accuarcy: 0.987\n",
      "Epoch 0 step 172: training loss: 323.0518432446557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 173: training accuarcy: 0.9805\n",
      "Epoch 0 step 173: training loss: 303.8813086886423\n",
      "Epoch 0 step 174: training accuarcy: 0.992\n",
      "Epoch 0 step 174: training loss: 310.3144314127424\n",
      "Epoch 0 step 175: training accuarcy: 0.986\n",
      "Epoch 0 step 175: training loss: 301.6742089193331\n",
      "Epoch 0 step 176: training accuarcy: 0.986\n",
      "Epoch 0 step 176: training loss: 289.32933664338225\n",
      "Epoch 0 step 177: training accuarcy: 0.9875\n",
      "Epoch 0 step 177: training loss: 289.3618106355673\n",
      "Epoch 0 step 178: training accuarcy: 0.9865\n",
      "Epoch 0 step 178: training loss: 279.4964899817554\n",
      "Epoch 0 step 179: training accuarcy: 0.989\n",
      "Epoch 0 step 179: training loss: 289.20747936951943\n",
      "Epoch 0 step 180: training accuarcy: 0.9855\n",
      "Epoch 0 step 180: training loss: 278.86956698372063\n",
      "Epoch 0 step 181: training accuarcy: 0.9855\n",
      "Epoch 0 step 181: training loss: 276.4663721659801\n",
      "Epoch 0 step 182: training accuarcy: 0.9915\n",
      "Epoch 0 step 182: training loss: 272.2153539873596\n",
      "Epoch 0 step 183: training accuarcy: 0.984\n",
      "Epoch 0 step 183: training loss: 266.1709702396542\n",
      "Epoch 0 step 184: training accuarcy: 0.987\n",
      "Epoch 0 step 184: training loss: 267.45876020494273\n",
      "Epoch 0 step 185: training accuarcy: 0.987\n",
      "Epoch 0 step 185: training loss: 274.5101765048868\n",
      "Epoch 0 step 186: training accuarcy: 0.9805\n",
      "Epoch 0 step 186: training loss: 254.45087499729874\n",
      "Epoch 0 step 187: training accuarcy: 0.994\n",
      "Epoch 0 step 187: training loss: 269.599469313168\n",
      "Epoch 0 step 188: training accuarcy: 0.9865\n",
      "Epoch 0 step 188: training loss: 255.9989550185628\n",
      "Epoch 0 step 189: training accuarcy: 0.991\n",
      "Epoch 0 step 189: training loss: 260.356090655349\n",
      "Epoch 0 step 190: training accuarcy: 0.9885\n",
      "Epoch 0 step 190: training loss: 242.98390490652105\n",
      "Epoch 0 step 191: training accuarcy: 0.9905\n",
      "Epoch 0 step 191: training loss: 247.70167171521635\n",
      "Epoch 0 step 192: training accuarcy: 0.992\n",
      "Epoch 0 step 192: training loss: 241.92037186777182\n",
      "Epoch 0 step 193: training accuarcy: 0.989\n",
      "Epoch 0 step 193: training loss: 243.07006377745998\n",
      "Epoch 0 step 194: training accuarcy: 0.9895\n",
      "Epoch 0 step 194: training loss: 242.98732333777636\n",
      "Epoch 0 step 195: training accuarcy: 0.9865\n",
      "Epoch 0 step 195: training loss: 225.15155781171268\n",
      "Epoch 0 step 196: training accuarcy: 0.991\n",
      "Epoch 0 step 196: training loss: 226.54294004075064\n",
      "Epoch 0 step 197: training accuarcy: 0.9925\n",
      "Epoch 0 step 197: training loss: 237.5950946030831\n",
      "Epoch 0 step 198: training accuarcy: 0.9825\n",
      "Epoch 0 step 198: training loss: 234.3659390754256\n",
      "Epoch 0 step 199: training accuarcy: 0.988\n",
      "Epoch 0 step 199: training loss: 234.766598366601\n",
      "Epoch 0 step 200: training accuarcy: 0.9895\n",
      "Epoch 0 step 200: training loss: 239.06714044340384\n",
      "Epoch 0 step 201: training accuarcy: 0.985\n",
      "Epoch 0 step 201: training loss: 227.2866277914552\n",
      "Epoch 0 step 202: training accuarcy: 0.9875\n",
      "Epoch 0 step 202: training loss: 218.59083917137997\n",
      "Epoch 0 step 203: training accuarcy: 0.991\n",
      "Epoch 0 step 203: training loss: 213.01182323289996\n",
      "Epoch 0 step 204: training accuarcy: 0.99\n",
      "Epoch 0 step 204: training loss: 220.6977533345514\n",
      "Epoch 0 step 205: training accuarcy: 0.9925\n",
      "Epoch 0 step 205: training loss: 226.9684567228027\n",
      "Epoch 0 step 206: training accuarcy: 0.992\n",
      "Epoch 0 step 206: training loss: 220.14648324117326\n",
      "Epoch 0 step 207: training accuarcy: 0.9915\n",
      "Epoch 0 step 207: training loss: 217.96433383062646\n",
      "Epoch 0 step 208: training accuarcy: 0.989\n",
      "Epoch 0 step 208: training loss: 206.16306804227156\n",
      "Epoch 0 step 209: training accuarcy: 0.9885\n",
      "Epoch 0 step 209: training loss: 202.73199059467714\n",
      "Epoch 0 step 210: training accuarcy: 0.993\n",
      "Epoch 0 step 210: training loss: 207.70095243866987\n",
      "Epoch 0 step 211: training accuarcy: 0.9915\n",
      "Epoch 0 step 211: training loss: 201.63611521333723\n",
      "Epoch 0 step 212: training accuarcy: 0.994\n",
      "Epoch 0 step 212: training loss: 212.22794205326656\n",
      "Epoch 0 step 213: training accuarcy: 0.989\n",
      "Epoch 0 step 213: training loss: 212.48645876847127\n",
      "Epoch 0 step 214: training accuarcy: 0.9875\n",
      "Epoch 0 step 214: training loss: 207.76292018296863\n",
      "Epoch 0 step 215: training accuarcy: 0.989\n",
      "Epoch 0 step 215: training loss: 216.4585232127174\n",
      "Epoch 0 step 216: training accuarcy: 0.989\n",
      "Epoch 0 step 216: training loss: 204.72474909767578\n",
      "Epoch 0 step 217: training accuarcy: 0.9915\n",
      "Epoch 0 step 217: training loss: 212.3819998027821\n",
      "Epoch 0 step 218: training accuarcy: 0.988\n",
      "Epoch 0 step 218: training loss: 194.56394370216483\n",
      "Epoch 0 step 219: training accuarcy: 0.9945\n",
      "Epoch 0 step 219: training loss: 201.04969542405541\n",
      "Epoch 0 step 220: training accuarcy: 0.991\n",
      "Epoch 0 step 220: training loss: 190.83377239303837\n",
      "Epoch 0 step 221: training accuarcy: 0.9925\n",
      "Epoch 0 step 221: training loss: 193.99368089244427\n",
      "Epoch 0 step 222: training accuarcy: 0.99\n",
      "Epoch 0 step 222: training loss: 196.2511564097999\n",
      "Epoch 0 step 223: training accuarcy: 0.99\n",
      "Epoch 0 step 223: training loss: 187.98441740977415\n",
      "Epoch 0 step 224: training accuarcy: 0.9935\n",
      "Epoch 0 step 224: training loss: 197.41297175174088\n",
      "Epoch 0 step 225: training accuarcy: 0.989\n",
      "Epoch 0 step 225: training loss: 192.40757360018944\n",
      "Epoch 0 step 226: training accuarcy: 0.9935\n",
      "Epoch 0 step 226: training loss: 198.4298420797665\n",
      "Epoch 0 step 227: training accuarcy: 0.987\n",
      "Epoch 0 step 227: training loss: 183.20922020636425\n",
      "Epoch 0 step 228: training accuarcy: 0.993\n",
      "Epoch 0 step 228: training loss: 186.46970168436187\n",
      "Epoch 0 step 229: training accuarcy: 0.9925\n",
      "Epoch 0 step 229: training loss: 188.04319033189492\n",
      "Epoch 0 step 230: training accuarcy: 0.9955\n",
      "Epoch 0 step 230: training loss: 195.49789746869033\n",
      "Epoch 0 step 231: training accuarcy: 0.989\n",
      "Epoch 0 step 231: training loss: 194.65851064958122\n",
      "Epoch 0 step 232: training accuarcy: 0.989\n",
      "Epoch 0 step 232: training loss: 185.918048424181\n",
      "Epoch 0 step 233: training accuarcy: 0.9915\n",
      "Epoch 0 step 233: training loss: 186.6643954332469\n",
      "Epoch 0 step 234: training accuarcy: 0.99\n",
      "Epoch 0 step 234: training loss: 190.8141133980908\n",
      "Epoch 0 step 235: training accuarcy: 0.9925\n",
      "Epoch 0 step 235: training loss: 170.6678792585346\n",
      "Epoch 0 step 236: training accuarcy: 0.997\n",
      "Epoch 0 step 236: training loss: 180.47621167307184\n",
      "Epoch 0 step 237: training accuarcy: 0.993\n",
      "Epoch 0 step 237: training loss: 179.8835025991915\n",
      "Epoch 0 step 238: training accuarcy: 0.995\n",
      "Epoch 0 step 238: training loss: 173.66763624541767\n",
      "Epoch 0 step 239: training accuarcy: 0.994\n",
      "Epoch 0 step 239: training loss: 166.5119721729031\n",
      "Epoch 0 step 240: training accuarcy: 0.9955\n",
      "Epoch 0 step 240: training loss: 188.33458739467162\n",
      "Epoch 0 step 241: training accuarcy: 0.9905\n",
      "Epoch 0 step 241: training loss: 179.37172732915027\n",
      "Epoch 0 step 242: training accuarcy: 0.989\n",
      "Epoch 0 step 242: training loss: 169.05827808068315\n",
      "Epoch 0 step 243: training accuarcy: 0.9965\n",
      "Epoch 0 step 243: training loss: 177.3364726860981\n",
      "Epoch 0 step 244: training accuarcy: 0.9915\n",
      "Epoch 0 step 244: training loss: 172.43699117619116\n",
      "Epoch 0 step 245: training accuarcy: 0.993\n",
      "Epoch 0 step 245: training loss: 176.48942814515905\n",
      "Epoch 0 step 246: training accuarcy: 0.9935\n",
      "Epoch 0 step 246: training loss: 180.15601502501545\n",
      "Epoch 0 step 247: training accuarcy: 0.9915\n",
      "Epoch 0 step 247: training loss: 175.389794224921\n",
      "Epoch 0 step 248: training accuarcy: 0.993\n",
      "Epoch 0 step 248: training loss: 181.8337794990521\n",
      "Epoch 0 step 249: training accuarcy: 0.9935\n",
      "Epoch 0 step 249: training loss: 166.0744024586863\n",
      "Epoch 0 step 250: training accuarcy: 0.9915\n",
      "Epoch 0 step 250: training loss: 169.83770630645049\n",
      "Epoch 0 step 251: training accuarcy: 0.9935\n",
      "Epoch 0 step 251: training loss: 170.67795198794926\n",
      "Epoch 0 step 252: training accuarcy: 0.9935\n",
      "Epoch 0 step 252: training loss: 170.27021195883432\n",
      "Epoch 0 step 253: training accuarcy: 0.9945\n",
      "Epoch 0 step 253: training loss: 174.14974199204005\n",
      "Epoch 0 step 254: training accuarcy: 0.993\n",
      "Epoch 0 step 254: training loss: 169.2318699592928\n",
      "Epoch 0 step 255: training accuarcy: 0.991\n",
      "Epoch 0 step 255: training loss: 175.96908254489801\n",
      "Epoch 0 step 256: training accuarcy: 0.991\n",
      "Epoch 0 step 256: training loss: 157.18149639779256\n",
      "Epoch 0 step 257: training accuarcy: 0.995\n",
      "Epoch 0 step 257: training loss: 152.7615675140778\n",
      "Epoch 0 step 258: training accuarcy: 0.996\n",
      "Epoch 0 step 258: training loss: 174.47896018130177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 259: training accuarcy: 0.9915\n",
      "Epoch 0 step 259: training loss: 168.1945615915896\n",
      "Epoch 0 step 260: training accuarcy: 0.9935\n",
      "Epoch 0 step 260: training loss: 163.89993818122878\n",
      "Epoch 0 step 261: training accuarcy: 0.994\n",
      "Epoch 0 step 261: training loss: 157.5699326275507\n",
      "Epoch 0 step 262: training accuarcy: 0.996\n",
      "Epoch 0 step 262: training loss: 122.55542614316795\n",
      "Epoch 0 step 263: training accuarcy: 0.9897435897435898\n",
      "Epoch 0: train loss 4893.037013669688, train accuarcy 0.9411860704421997\n",
      "Epoch 0: valid loss 605.7904897973084, valid accuarcy 0.9905195236206055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▍                                                                                                                                         | 1/5 [05:21<21:25, 321.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 166.45061092507194\n",
      "Epoch 1 step 264: training accuarcy: 0.995\n",
      "Epoch 1 step 264: training loss: 164.17322528542175\n",
      "Epoch 1 step 265: training accuarcy: 0.991\n",
      "Epoch 1 step 265: training loss: 163.43406317931476\n",
      "Epoch 1 step 266: training accuarcy: 0.992\n",
      "Epoch 1 step 266: training loss: 154.35362837278421\n",
      "Epoch 1 step 267: training accuarcy: 0.994\n",
      "Epoch 1 step 267: training loss: 160.97968850708222\n",
      "Epoch 1 step 268: training accuarcy: 0.994\n",
      "Epoch 1 step 268: training loss: 160.13874640570538\n",
      "Epoch 1 step 269: training accuarcy: 0.9935\n",
      "Epoch 1 step 269: training loss: 152.5956226119392\n",
      "Epoch 1 step 270: training accuarcy: 0.996\n",
      "Epoch 1 step 270: training loss: 160.58439028197623\n",
      "Epoch 1 step 271: training accuarcy: 0.993\n",
      "Epoch 1 step 271: training loss: 158.8554035635884\n",
      "Epoch 1 step 272: training accuarcy: 0.992\n",
      "Epoch 1 step 272: training loss: 154.02959716765886\n",
      "Epoch 1 step 273: training accuarcy: 0.9945\n",
      "Epoch 1 step 273: training loss: 158.62376050604246\n",
      "Epoch 1 step 274: training accuarcy: 0.994\n",
      "Epoch 1 step 274: training loss: 155.64147935587062\n",
      "Epoch 1 step 275: training accuarcy: 0.994\n",
      "Epoch 1 step 275: training loss: 154.22864553051073\n",
      "Epoch 1 step 276: training accuarcy: 0.995\n",
      "Epoch 1 step 276: training loss: 153.20108552694643\n",
      "Epoch 1 step 277: training accuarcy: 0.996\n",
      "Epoch 1 step 277: training loss: 152.87420662109528\n",
      "Epoch 1 step 278: training accuarcy: 0.995\n",
      "Epoch 1 step 278: training loss: 159.67751391676148\n",
      "Epoch 1 step 279: training accuarcy: 0.9935\n",
      "Epoch 1 step 279: training loss: 160.31166926522545\n",
      "Epoch 1 step 280: training accuarcy: 0.9935\n",
      "Epoch 1 step 280: training loss: 156.67764314622443\n",
      "Epoch 1 step 281: training accuarcy: 0.993\n",
      "Epoch 1 step 281: training loss: 158.19515008611745\n",
      "Epoch 1 step 282: training accuarcy: 0.9935\n",
      "Epoch 1 step 282: training loss: 143.26866531072974\n",
      "Epoch 1 step 283: training accuarcy: 0.9945\n",
      "Epoch 1 step 283: training loss: 145.7294729375318\n",
      "Epoch 1 step 284: training accuarcy: 0.9965\n",
      "Epoch 1 step 284: training loss: 147.13250205429688\n",
      "Epoch 1 step 285: training accuarcy: 0.996\n",
      "Epoch 1 step 285: training loss: 150.7643841499273\n",
      "Epoch 1 step 286: training accuarcy: 0.996\n",
      "Epoch 1 step 286: training loss: 135.3235785669965\n",
      "Epoch 1 step 287: training accuarcy: 0.996\n",
      "Epoch 1 step 287: training loss: 150.83972183206797\n",
      "Epoch 1 step 288: training accuarcy: 0.995\n",
      "Epoch 1 step 288: training loss: 140.58395537829222\n",
      "Epoch 1 step 289: training accuarcy: 0.9975\n",
      "Epoch 1 step 289: training loss: 141.82546291361595\n",
      "Epoch 1 step 290: training accuarcy: 0.9955\n",
      "Epoch 1 step 290: training loss: 149.2327610095274\n",
      "Epoch 1 step 291: training accuarcy: 0.996\n",
      "Epoch 1 step 291: training loss: 135.3475882528002\n",
      "Epoch 1 step 292: training accuarcy: 0.9955\n",
      "Epoch 1 step 292: training loss: 145.71453378267557\n",
      "Epoch 1 step 293: training accuarcy: 0.994\n",
      "Epoch 1 step 293: training loss: 152.5601304975092\n",
      "Epoch 1 step 294: training accuarcy: 0.994\n",
      "Epoch 1 step 294: training loss: 142.47121031514973\n",
      "Epoch 1 step 295: training accuarcy: 0.996\n",
      "Epoch 1 step 295: training loss: 153.91041703314755\n",
      "Epoch 1 step 296: training accuarcy: 0.992\n",
      "Epoch 1 step 296: training loss: 151.6380130996933\n",
      "Epoch 1 step 297: training accuarcy: 0.991\n",
      "Epoch 1 step 297: training loss: 155.41354694381874\n",
      "Epoch 1 step 298: training accuarcy: 0.996\n",
      "Epoch 1 step 298: training loss: 137.86695793821758\n",
      "Epoch 1 step 299: training accuarcy: 0.996\n",
      "Epoch 1 step 299: training loss: 146.83922353669226\n",
      "Epoch 1 step 300: training accuarcy: 0.996\n",
      "Epoch 1 step 300: training loss: 148.83883607233483\n",
      "Epoch 1 step 301: training accuarcy: 0.995\n",
      "Epoch 1 step 301: training loss: 149.14418030153786\n",
      "Epoch 1 step 302: training accuarcy: 0.9965\n",
      "Epoch 1 step 302: training loss: 137.22258030239928\n",
      "Epoch 1 step 303: training accuarcy: 0.995\n",
      "Epoch 1 step 303: training loss: 133.9759508185584\n",
      "Epoch 1 step 304: training accuarcy: 0.9965\n",
      "Epoch 1 step 304: training loss: 144.40477993315338\n",
      "Epoch 1 step 305: training accuarcy: 0.9955\n",
      "Epoch 1 step 305: training loss: 140.4135274774864\n",
      "Epoch 1 step 306: training accuarcy: 0.9945\n",
      "Epoch 1 step 306: training loss: 146.2107010708906\n",
      "Epoch 1 step 307: training accuarcy: 0.994\n",
      "Epoch 1 step 307: training loss: 142.77967207207155\n",
      "Epoch 1 step 308: training accuarcy: 0.994\n",
      "Epoch 1 step 308: training loss: 139.25192128597945\n",
      "Epoch 1 step 309: training accuarcy: 0.996\n",
      "Epoch 1 step 309: training loss: 139.78672138416601\n",
      "Epoch 1 step 310: training accuarcy: 0.9935\n",
      "Epoch 1 step 310: training loss: 131.03061403511407\n",
      "Epoch 1 step 311: training accuarcy: 0.9955\n",
      "Epoch 1 step 311: training loss: 136.96767597762548\n",
      "Epoch 1 step 312: training accuarcy: 0.9965\n",
      "Epoch 1 step 312: training loss: 138.25104111067066\n",
      "Epoch 1 step 313: training accuarcy: 0.997\n",
      "Epoch 1 step 313: training loss: 144.81884958490645\n",
      "Epoch 1 step 314: training accuarcy: 0.995\n",
      "Epoch 1 step 314: training loss: 134.39336798997343\n",
      "Epoch 1 step 315: training accuarcy: 0.996\n",
      "Epoch 1 step 315: training loss: 147.45646807092356\n",
      "Epoch 1 step 316: training accuarcy: 0.996\n",
      "Epoch 1 step 316: training loss: 134.908479660988\n",
      "Epoch 1 step 317: training accuarcy: 0.996\n",
      "Epoch 1 step 317: training loss: 143.54610890281822\n",
      "Epoch 1 step 318: training accuarcy: 0.9955\n",
      "Epoch 1 step 318: training loss: 140.78061110657274\n",
      "Epoch 1 step 319: training accuarcy: 0.9945\n",
      "Epoch 1 step 319: training loss: 128.6726022169351\n",
      "Epoch 1 step 320: training accuarcy: 0.9975\n",
      "Epoch 1 step 320: training loss: 145.39350153287643\n",
      "Epoch 1 step 321: training accuarcy: 0.9955\n",
      "Epoch 1 step 321: training loss: 136.72091996887988\n",
      "Epoch 1 step 322: training accuarcy: 0.9965\n",
      "Epoch 1 step 322: training loss: 137.0142014881315\n",
      "Epoch 1 step 323: training accuarcy: 0.9955\n",
      "Epoch 1 step 323: training loss: 146.0881934662877\n",
      "Epoch 1 step 324: training accuarcy: 0.995\n",
      "Epoch 1 step 324: training loss: 134.42891119761737\n",
      "Epoch 1 step 325: training accuarcy: 0.9965\n",
      "Epoch 1 step 325: training loss: 141.98515060983226\n",
      "Epoch 1 step 326: training accuarcy: 0.9965\n",
      "Epoch 1 step 326: training loss: 134.4935692615303\n",
      "Epoch 1 step 327: training accuarcy: 0.9985\n",
      "Epoch 1 step 327: training loss: 134.50227685847113\n",
      "Epoch 1 step 328: training accuarcy: 0.996\n",
      "Epoch 1 step 328: training loss: 134.02184432250158\n",
      "Epoch 1 step 329: training accuarcy: 0.9965\n",
      "Epoch 1 step 329: training loss: 136.77401830895639\n",
      "Epoch 1 step 330: training accuarcy: 0.9945\n",
      "Epoch 1 step 330: training loss: 140.49962488360217\n",
      "Epoch 1 step 331: training accuarcy: 0.9985\n",
      "Epoch 1 step 331: training loss: 129.2032100986375\n",
      "Epoch 1 step 332: training accuarcy: 0.9965\n",
      "Epoch 1 step 332: training loss: 131.55380868885698\n",
      "Epoch 1 step 333: training accuarcy: 0.997\n",
      "Epoch 1 step 333: training loss: 121.02522382743432\n",
      "Epoch 1 step 334: training accuarcy: 0.9955\n",
      "Epoch 1 step 334: training loss: 128.5453173181334\n",
      "Epoch 1 step 335: training accuarcy: 0.995\n",
      "Epoch 1 step 335: training loss: 129.46943691602257\n",
      "Epoch 1 step 336: training accuarcy: 0.9965\n",
      "Epoch 1 step 336: training loss: 127.62621866709263\n",
      "Epoch 1 step 337: training accuarcy: 0.997\n",
      "Epoch 1 step 337: training loss: 130.5169499013452\n",
      "Epoch 1 step 338: training accuarcy: 0.997\n",
      "Epoch 1 step 338: training loss: 128.13684844250625\n",
      "Epoch 1 step 339: training accuarcy: 0.996\n",
      "Epoch 1 step 339: training loss: 129.86865092067637\n",
      "Epoch 1 step 340: training accuarcy: 0.9945\n",
      "Epoch 1 step 340: training loss: 129.16207644885162\n",
      "Epoch 1 step 341: training accuarcy: 0.996\n",
      "Epoch 1 step 341: training loss: 129.8723808650599\n",
      "Epoch 1 step 342: training accuarcy: 0.997\n",
      "Epoch 1 step 342: training loss: 131.39398747878585\n",
      "Epoch 1 step 343: training accuarcy: 0.9945\n",
      "Epoch 1 step 343: training loss: 141.698649010759\n",
      "Epoch 1 step 344: training accuarcy: 0.9975\n",
      "Epoch 1 step 344: training loss: 130.7391975575901\n",
      "Epoch 1 step 345: training accuarcy: 0.9965\n",
      "Epoch 1 step 345: training loss: 130.67082073982075\n",
      "Epoch 1 step 346: training accuarcy: 0.998\n",
      "Epoch 1 step 346: training loss: 123.21993895885329\n",
      "Epoch 1 step 347: training accuarcy: 0.9985\n",
      "Epoch 1 step 347: training loss: 130.65199858823604\n",
      "Epoch 1 step 348: training accuarcy: 0.996\n",
      "Epoch 1 step 348: training loss: 126.47505164954832\n",
      "Epoch 1 step 349: training accuarcy: 0.9965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 349: training loss: 135.50704939435502\n",
      "Epoch 1 step 350: training accuarcy: 0.9945\n",
      "Epoch 1 step 350: training loss: 116.46290512286197\n",
      "Epoch 1 step 351: training accuarcy: 0.996\n",
      "Epoch 1 step 351: training loss: 126.70377739351447\n",
      "Epoch 1 step 352: training accuarcy: 0.9985\n",
      "Epoch 1 step 352: training loss: 130.3485048170117\n",
      "Epoch 1 step 353: training accuarcy: 0.998\n",
      "Epoch 1 step 353: training loss: 133.21660476678252\n",
      "Epoch 1 step 354: training accuarcy: 0.995\n",
      "Epoch 1 step 354: training loss: 137.23963334902908\n",
      "Epoch 1 step 355: training accuarcy: 0.995\n",
      "Epoch 1 step 355: training loss: 124.70967808092774\n",
      "Epoch 1 step 356: training accuarcy: 0.997\n",
      "Epoch 1 step 356: training loss: 127.5959669529157\n",
      "Epoch 1 step 357: training accuarcy: 0.9955\n",
      "Epoch 1 step 357: training loss: 128.6775070654335\n",
      "Epoch 1 step 358: training accuarcy: 0.997\n",
      "Epoch 1 step 358: training loss: 132.07856803947666\n",
      "Epoch 1 step 359: training accuarcy: 0.996\n",
      "Epoch 1 step 359: training loss: 130.8413463858962\n",
      "Epoch 1 step 360: training accuarcy: 0.996\n",
      "Epoch 1 step 360: training loss: 122.3152121666068\n",
      "Epoch 1 step 361: training accuarcy: 0.9955\n",
      "Epoch 1 step 361: training loss: 123.61227526132592\n",
      "Epoch 1 step 362: training accuarcy: 0.9955\n",
      "Epoch 1 step 362: training loss: 126.22411796462822\n",
      "Epoch 1 step 363: training accuarcy: 0.9955\n",
      "Epoch 1 step 363: training loss: 118.13578083609636\n",
      "Epoch 1 step 364: training accuarcy: 0.9975\n",
      "Epoch 1 step 364: training loss: 116.98125480584604\n",
      "Epoch 1 step 365: training accuarcy: 0.9985\n",
      "Epoch 1 step 365: training loss: 123.42653040064954\n",
      "Epoch 1 step 366: training accuarcy: 0.9945\n",
      "Epoch 1 step 366: training loss: 117.06873947118578\n",
      "Epoch 1 step 367: training accuarcy: 0.996\n",
      "Epoch 1 step 367: training loss: 133.42692492829246\n",
      "Epoch 1 step 368: training accuarcy: 0.9985\n",
      "Epoch 1 step 368: training loss: 118.63346172231641\n",
      "Epoch 1 step 369: training accuarcy: 0.9955\n",
      "Epoch 1 step 369: training loss: 131.74246901043958\n",
      "Epoch 1 step 370: training accuarcy: 0.9955\n",
      "Epoch 1 step 370: training loss: 124.3575514030511\n",
      "Epoch 1 step 371: training accuarcy: 0.998\n",
      "Epoch 1 step 371: training loss: 123.5799447879354\n",
      "Epoch 1 step 372: training accuarcy: 0.999\n",
      "Epoch 1 step 372: training loss: 127.87802005172543\n",
      "Epoch 1 step 373: training accuarcy: 0.9965\n",
      "Epoch 1 step 373: training loss: 121.06148124142476\n",
      "Epoch 1 step 374: training accuarcy: 0.9955\n",
      "Epoch 1 step 374: training loss: 120.84407677925398\n",
      "Epoch 1 step 375: training accuarcy: 0.9965\n",
      "Epoch 1 step 375: training loss: 109.53874328773477\n",
      "Epoch 1 step 376: training accuarcy: 0.9975\n",
      "Epoch 1 step 376: training loss: 120.44835193557962\n",
      "Epoch 1 step 377: training accuarcy: 0.994\n",
      "Epoch 1 step 377: training loss: 115.08550487514171\n",
      "Epoch 1 step 378: training accuarcy: 0.997\n",
      "Epoch 1 step 378: training loss: 117.06750580306354\n",
      "Epoch 1 step 379: training accuarcy: 0.9995\n",
      "Epoch 1 step 379: training loss: 119.05230718901329\n",
      "Epoch 1 step 380: training accuarcy: 0.9955\n",
      "Epoch 1 step 380: training loss: 113.60688677776812\n",
      "Epoch 1 step 381: training accuarcy: 0.9965\n",
      "Epoch 1 step 381: training loss: 129.06602278347233\n",
      "Epoch 1 step 382: training accuarcy: 0.997\n",
      "Epoch 1 step 382: training loss: 126.27520892083683\n",
      "Epoch 1 step 383: training accuarcy: 0.997\n",
      "Epoch 1 step 383: training loss: 122.07608753701376\n",
      "Epoch 1 step 384: training accuarcy: 0.994\n",
      "Epoch 1 step 384: training loss: 117.66558202911486\n",
      "Epoch 1 step 385: training accuarcy: 0.997\n",
      "Epoch 1 step 385: training loss: 121.54112292153067\n",
      "Epoch 1 step 386: training accuarcy: 0.998\n",
      "Epoch 1 step 386: training loss: 122.78315534398385\n",
      "Epoch 1 step 387: training accuarcy: 0.9975\n",
      "Epoch 1 step 387: training loss: 119.18648280406094\n",
      "Epoch 1 step 388: training accuarcy: 0.997\n",
      "Epoch 1 step 388: training loss: 120.0197585468995\n",
      "Epoch 1 step 389: training accuarcy: 0.9975\n",
      "Epoch 1 step 389: training loss: 123.83191841028444\n",
      "Epoch 1 step 390: training accuarcy: 0.9975\n",
      "Epoch 1 step 390: training loss: 128.7393010068579\n",
      "Epoch 1 step 391: training accuarcy: 0.9945\n",
      "Epoch 1 step 391: training loss: 112.45881829967252\n",
      "Epoch 1 step 392: training accuarcy: 0.997\n",
      "Epoch 1 step 392: training loss: 114.97691490818926\n",
      "Epoch 1 step 393: training accuarcy: 0.996\n",
      "Epoch 1 step 393: training loss: 124.13525375375353\n",
      "Epoch 1 step 394: training accuarcy: 0.9925\n",
      "Epoch 1 step 394: training loss: 124.39583718493184\n",
      "Epoch 1 step 395: training accuarcy: 0.9965\n",
      "Epoch 1 step 395: training loss: 120.09321487478782\n",
      "Epoch 1 step 396: training accuarcy: 0.9955\n",
      "Epoch 1 step 396: training loss: 114.10006518956556\n",
      "Epoch 1 step 397: training accuarcy: 0.997\n",
      "Epoch 1 step 397: training loss: 123.23962153089131\n",
      "Epoch 1 step 398: training accuarcy: 0.995\n",
      "Epoch 1 step 398: training loss: 124.65911471478003\n",
      "Epoch 1 step 399: training accuarcy: 0.9945\n",
      "Epoch 1 step 399: training loss: 125.97595479334286\n",
      "Epoch 1 step 400: training accuarcy: 0.995\n",
      "Epoch 1 step 400: training loss: 125.52826295063446\n",
      "Epoch 1 step 401: training accuarcy: 0.9955\n",
      "Epoch 1 step 401: training loss: 118.30237081521943\n",
      "Epoch 1 step 402: training accuarcy: 0.996\n",
      "Epoch 1 step 402: training loss: 124.25240267793723\n",
      "Epoch 1 step 403: training accuarcy: 0.996\n",
      "Epoch 1 step 403: training loss: 122.37613541593372\n",
      "Epoch 1 step 404: training accuarcy: 0.9955\n",
      "Epoch 1 step 404: training loss: 115.14040070366894\n",
      "Epoch 1 step 405: training accuarcy: 0.998\n",
      "Epoch 1 step 405: training loss: 120.85500825334671\n",
      "Epoch 1 step 406: training accuarcy: 0.997\n",
      "Epoch 1 step 406: training loss: 115.62786657274808\n",
      "Epoch 1 step 407: training accuarcy: 0.9985\n",
      "Epoch 1 step 407: training loss: 123.18125263051932\n",
      "Epoch 1 step 408: training accuarcy: 0.997\n",
      "Epoch 1 step 408: training loss: 123.83211529574294\n",
      "Epoch 1 step 409: training accuarcy: 0.996\n",
      "Epoch 1 step 409: training loss: 112.40993744909007\n",
      "Epoch 1 step 410: training accuarcy: 0.996\n",
      "Epoch 1 step 410: training loss: 119.32827240568582\n",
      "Epoch 1 step 411: training accuarcy: 0.998\n",
      "Epoch 1 step 411: training loss: 114.00300927399107\n",
      "Epoch 1 step 412: training accuarcy: 0.998\n",
      "Epoch 1 step 412: training loss: 115.23486175185955\n",
      "Epoch 1 step 413: training accuarcy: 0.996\n",
      "Epoch 1 step 413: training loss: 120.32734199971416\n",
      "Epoch 1 step 414: training accuarcy: 0.999\n",
      "Epoch 1 step 414: training loss: 119.34144616137117\n",
      "Epoch 1 step 415: training accuarcy: 0.9975\n",
      "Epoch 1 step 415: training loss: 123.17529756296199\n",
      "Epoch 1 step 416: training accuarcy: 0.996\n",
      "Epoch 1 step 416: training loss: 125.12837158251907\n",
      "Epoch 1 step 417: training accuarcy: 0.9965\n",
      "Epoch 1 step 417: training loss: 111.05429929481673\n",
      "Epoch 1 step 418: training accuarcy: 0.994\n",
      "Epoch 1 step 418: training loss: 115.32607556646235\n",
      "Epoch 1 step 419: training accuarcy: 0.996\n",
      "Epoch 1 step 419: training loss: 113.28365907954168\n",
      "Epoch 1 step 420: training accuarcy: 0.9975\n",
      "Epoch 1 step 420: training loss: 121.06516002518029\n",
      "Epoch 1 step 421: training accuarcy: 0.9965\n",
      "Epoch 1 step 421: training loss: 114.84539750210755\n",
      "Epoch 1 step 422: training accuarcy: 0.998\n",
      "Epoch 1 step 422: training loss: 120.95463858517661\n",
      "Epoch 1 step 423: training accuarcy: 0.9965\n",
      "Epoch 1 step 423: training loss: 118.4416455703473\n",
      "Epoch 1 step 424: training accuarcy: 0.9955\n",
      "Epoch 1 step 424: training loss: 121.83146408755196\n",
      "Epoch 1 step 425: training accuarcy: 0.997\n",
      "Epoch 1 step 425: training loss: 117.49192017193195\n",
      "Epoch 1 step 426: training accuarcy: 0.9975\n",
      "Epoch 1 step 426: training loss: 101.67998322724128\n",
      "Epoch 1 step 427: training accuarcy: 0.9995\n",
      "Epoch 1 step 427: training loss: 114.21697874161265\n",
      "Epoch 1 step 428: training accuarcy: 0.997\n",
      "Epoch 1 step 428: training loss: 112.87233550678637\n",
      "Epoch 1 step 429: training accuarcy: 0.9965\n",
      "Epoch 1 step 429: training loss: 113.58705784558009\n",
      "Epoch 1 step 430: training accuarcy: 0.996\n",
      "Epoch 1 step 430: training loss: 121.25357487894144\n",
      "Epoch 1 step 431: training accuarcy: 0.9945\n",
      "Epoch 1 step 431: training loss: 114.35118720043594\n",
      "Epoch 1 step 432: training accuarcy: 0.998\n",
      "Epoch 1 step 432: training loss: 110.6913312312908\n",
      "Epoch 1 step 433: training accuarcy: 0.998\n",
      "Epoch 1 step 433: training loss: 110.3448724639782\n",
      "Epoch 1 step 434: training accuarcy: 0.997\n",
      "Epoch 1 step 434: training loss: 113.47779750065118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 435: training accuarcy: 0.9975\n",
      "Epoch 1 step 435: training loss: 112.49009702031108\n",
      "Epoch 1 step 436: training accuarcy: 0.998\n",
      "Epoch 1 step 436: training loss: 119.56916172301013\n",
      "Epoch 1 step 437: training accuarcy: 0.9955\n",
      "Epoch 1 step 437: training loss: 110.96336825663379\n",
      "Epoch 1 step 438: training accuarcy: 0.9985\n",
      "Epoch 1 step 438: training loss: 121.65719167229362\n",
      "Epoch 1 step 439: training accuarcy: 0.996\n",
      "Epoch 1 step 439: training loss: 124.68620914173573\n",
      "Epoch 1 step 440: training accuarcy: 0.996\n",
      "Epoch 1 step 440: training loss: 123.29986116405284\n",
      "Epoch 1 step 441: training accuarcy: 0.9955\n",
      "Epoch 1 step 441: training loss: 114.54776348600578\n",
      "Epoch 1 step 442: training accuarcy: 0.997\n",
      "Epoch 1 step 442: training loss: 116.22916761433754\n",
      "Epoch 1 step 443: training accuarcy: 0.9975\n",
      "Epoch 1 step 443: training loss: 109.10152770772186\n",
      "Epoch 1 step 444: training accuarcy: 0.9945\n",
      "Epoch 1 step 444: training loss: 112.64984230699433\n",
      "Epoch 1 step 445: training accuarcy: 0.9975\n",
      "Epoch 1 step 445: training loss: 125.5456202827155\n",
      "Epoch 1 step 446: training accuarcy: 0.9945\n",
      "Epoch 1 step 446: training loss: 110.31122474291473\n",
      "Epoch 1 step 447: training accuarcy: 0.9975\n",
      "Epoch 1 step 447: training loss: 112.39179061605876\n",
      "Epoch 1 step 448: training accuarcy: 0.997\n",
      "Epoch 1 step 448: training loss: 119.72557152235414\n",
      "Epoch 1 step 449: training accuarcy: 0.9945\n",
      "Epoch 1 step 449: training loss: 116.8666442538976\n",
      "Epoch 1 step 450: training accuarcy: 0.996\n",
      "Epoch 1 step 450: training loss: 112.01823871216675\n",
      "Epoch 1 step 451: training accuarcy: 0.998\n",
      "Epoch 1 step 451: training loss: 108.71145551061639\n",
      "Epoch 1 step 452: training accuarcy: 0.9975\n",
      "Epoch 1 step 452: training loss: 114.03054085669504\n",
      "Epoch 1 step 453: training accuarcy: 0.9955\n",
      "Epoch 1 step 453: training loss: 107.31078712980582\n",
      "Epoch 1 step 454: training accuarcy: 0.998\n",
      "Epoch 1 step 454: training loss: 113.5570258684632\n",
      "Epoch 1 step 455: training accuarcy: 0.997\n",
      "Epoch 1 step 455: training loss: 112.98659003816681\n",
      "Epoch 1 step 456: training accuarcy: 0.996\n",
      "Epoch 1 step 456: training loss: 112.65797639699518\n",
      "Epoch 1 step 457: training accuarcy: 0.9975\n",
      "Epoch 1 step 457: training loss: 114.77709536056939\n",
      "Epoch 1 step 458: training accuarcy: 0.995\n",
      "Epoch 1 step 458: training loss: 102.61941113527587\n",
      "Epoch 1 step 459: training accuarcy: 1.0\n",
      "Epoch 1 step 459: training loss: 110.274691978971\n",
      "Epoch 1 step 460: training accuarcy: 0.997\n",
      "Epoch 1 step 460: training loss: 111.15872478538253\n",
      "Epoch 1 step 461: training accuarcy: 0.995\n",
      "Epoch 1 step 461: training loss: 113.82894477714707\n",
      "Epoch 1 step 462: training accuarcy: 0.997\n",
      "Epoch 1 step 462: training loss: 115.78723877341872\n",
      "Epoch 1 step 463: training accuarcy: 0.998\n",
      "Epoch 1 step 463: training loss: 107.25154472696588\n",
      "Epoch 1 step 464: training accuarcy: 0.998\n",
      "Epoch 1 step 464: training loss: 110.71261333745733\n",
      "Epoch 1 step 465: training accuarcy: 0.9985\n",
      "Epoch 1 step 465: training loss: 99.51899202708734\n",
      "Epoch 1 step 466: training accuarcy: 0.9985\n",
      "Epoch 1 step 466: training loss: 99.08945131492257\n",
      "Epoch 1 step 467: training accuarcy: 0.999\n",
      "Epoch 1 step 467: training loss: 108.85062477861504\n",
      "Epoch 1 step 468: training accuarcy: 0.9975\n",
      "Epoch 1 step 468: training loss: 108.57743338819071\n",
      "Epoch 1 step 469: training accuarcy: 0.995\n",
      "Epoch 1 step 469: training loss: 107.24026562194224\n",
      "Epoch 1 step 470: training accuarcy: 0.9985\n",
      "Epoch 1 step 470: training loss: 109.83948151509102\n",
      "Epoch 1 step 471: training accuarcy: 0.9965\n",
      "Epoch 1 step 471: training loss: 104.36188152607657\n",
      "Epoch 1 step 472: training accuarcy: 0.999\n",
      "Epoch 1 step 472: training loss: 111.94420014348547\n",
      "Epoch 1 step 473: training accuarcy: 0.997\n",
      "Epoch 1 step 473: training loss: 103.87149610173368\n",
      "Epoch 1 step 474: training accuarcy: 0.9975\n",
      "Epoch 1 step 474: training loss: 106.98611068691363\n",
      "Epoch 1 step 475: training accuarcy: 0.997\n",
      "Epoch 1 step 475: training loss: 105.65070685446534\n",
      "Epoch 1 step 476: training accuarcy: 0.9965\n",
      "Epoch 1 step 476: training loss: 104.00158246106832\n",
      "Epoch 1 step 477: training accuarcy: 0.9995\n",
      "Epoch 1 step 477: training loss: 106.11761242141613\n",
      "Epoch 1 step 478: training accuarcy: 0.9965\n",
      "Epoch 1 step 478: training loss: 101.85946131301979\n",
      "Epoch 1 step 479: training accuarcy: 0.998\n",
      "Epoch 1 step 479: training loss: 103.35433298078601\n",
      "Epoch 1 step 480: training accuarcy: 0.9975\n",
      "Epoch 1 step 480: training loss: 104.47333691213998\n",
      "Epoch 1 step 481: training accuarcy: 0.997\n",
      "Epoch 1 step 481: training loss: 111.13537116516429\n",
      "Epoch 1 step 482: training accuarcy: 0.9945\n",
      "Epoch 1 step 482: training loss: 105.9632683666502\n",
      "Epoch 1 step 483: training accuarcy: 0.9985\n",
      "Epoch 1 step 483: training loss: 101.34725686921723\n",
      "Epoch 1 step 484: training accuarcy: 0.9975\n",
      "Epoch 1 step 484: training loss: 99.43345285245852\n",
      "Epoch 1 step 485: training accuarcy: 0.999\n",
      "Epoch 1 step 485: training loss: 104.807823477504\n",
      "Epoch 1 step 486: training accuarcy: 0.997\n",
      "Epoch 1 step 486: training loss: 104.170377895866\n",
      "Epoch 1 step 487: training accuarcy: 0.998\n",
      "Epoch 1 step 487: training loss: 101.00416842665977\n",
      "Epoch 1 step 488: training accuarcy: 0.997\n",
      "Epoch 1 step 488: training loss: 101.76428475908423\n",
      "Epoch 1 step 489: training accuarcy: 0.9965\n",
      "Epoch 1 step 489: training loss: 104.40526578829854\n",
      "Epoch 1 step 490: training accuarcy: 0.998\n",
      "Epoch 1 step 490: training loss: 114.26490135820242\n",
      "Epoch 1 step 491: training accuarcy: 0.9955\n",
      "Epoch 1 step 491: training loss: 105.67157917512014\n",
      "Epoch 1 step 492: training accuarcy: 0.9965\n",
      "Epoch 1 step 492: training loss: 102.2964405412667\n",
      "Epoch 1 step 493: training accuarcy: 0.9975\n",
      "Epoch 1 step 493: training loss: 104.6837224001381\n",
      "Epoch 1 step 494: training accuarcy: 0.997\n",
      "Epoch 1 step 494: training loss: 106.0460189941122\n",
      "Epoch 1 step 495: training accuarcy: 0.9975\n",
      "Epoch 1 step 495: training loss: 103.25142526864809\n",
      "Epoch 1 step 496: training accuarcy: 0.996\n",
      "Epoch 1 step 496: training loss: 113.067795990098\n",
      "Epoch 1 step 497: training accuarcy: 0.997\n",
      "Epoch 1 step 497: training loss: 101.45975457311775\n",
      "Epoch 1 step 498: training accuarcy: 0.998\n",
      "Epoch 1 step 498: training loss: 100.39913956318713\n",
      "Epoch 1 step 499: training accuarcy: 0.998\n",
      "Epoch 1 step 499: training loss: 103.99723527125481\n",
      "Epoch 1 step 500: training accuarcy: 0.9985\n",
      "Epoch 1 step 500: training loss: 105.52363811999348\n",
      "Epoch 1 step 501: training accuarcy: 0.9965\n",
      "Epoch 1 step 501: training loss: 109.89328329897509\n",
      "Epoch 1 step 502: training accuarcy: 0.999\n",
      "Epoch 1 step 502: training loss: 96.73532468932709\n",
      "Epoch 1 step 503: training accuarcy: 0.998\n",
      "Epoch 1 step 503: training loss: 100.75854802766474\n",
      "Epoch 1 step 504: training accuarcy: 0.996\n",
      "Epoch 1 step 504: training loss: 93.56736180956574\n",
      "Epoch 1 step 505: training accuarcy: 0.9975\n",
      "Epoch 1 step 505: training loss: 90.3058172496217\n",
      "Epoch 1 step 506: training accuarcy: 0.9975\n",
      "Epoch 1 step 506: training loss: 98.10757632696695\n",
      "Epoch 1 step 507: training accuarcy: 0.9985\n",
      "Epoch 1 step 507: training loss: 96.92037947142413\n",
      "Epoch 1 step 508: training accuarcy: 0.9975\n",
      "Epoch 1 step 508: training loss: 100.11450521627465\n",
      "Epoch 1 step 509: training accuarcy: 0.9985\n",
      "Epoch 1 step 509: training loss: 101.40907234318729\n",
      "Epoch 1 step 510: training accuarcy: 0.998\n",
      "Epoch 1 step 510: training loss: 90.68450152912546\n",
      "Epoch 1 step 511: training accuarcy: 0.9985\n",
      "Epoch 1 step 511: training loss: 95.03277052026979\n",
      "Epoch 1 step 512: training accuarcy: 0.998\n",
      "Epoch 1 step 512: training loss: 104.64665103827664\n",
      "Epoch 1 step 513: training accuarcy: 0.997\n",
      "Epoch 1 step 513: training loss: 95.25161360663034\n",
      "Epoch 1 step 514: training accuarcy: 0.997\n",
      "Epoch 1 step 514: training loss: 95.83360985121874\n",
      "Epoch 1 step 515: training accuarcy: 0.999\n",
      "Epoch 1 step 515: training loss: 101.33308821132061\n",
      "Epoch 1 step 516: training accuarcy: 0.9985\n",
      "Epoch 1 step 516: training loss: 106.01889262438479\n",
      "Epoch 1 step 517: training accuarcy: 0.9965\n",
      "Epoch 1 step 517: training loss: 93.09529554906783\n",
      "Epoch 1 step 518: training accuarcy: 0.998\n",
      "Epoch 1 step 518: training loss: 112.94816163400743\n",
      "Epoch 1 step 519: training accuarcy: 0.994\n",
      "Epoch 1 step 519: training loss: 96.09197493076616\n",
      "Epoch 1 step 520: training accuarcy: 0.9985\n",
      "Epoch 1 step 520: training loss: 100.61754031239431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 521: training accuarcy: 0.9965\n",
      "Epoch 1 step 521: training loss: 96.93767231722407\n",
      "Epoch 1 step 522: training accuarcy: 0.9985\n",
      "Epoch 1 step 522: training loss: 91.9787822499195\n",
      "Epoch 1 step 523: training accuarcy: 0.9985\n",
      "Epoch 1 step 523: training loss: 99.50599382463761\n",
      "Epoch 1 step 524: training accuarcy: 0.998\n",
      "Epoch 1 step 524: training loss: 105.52852715672671\n",
      "Epoch 1 step 525: training accuarcy: 0.996\n",
      "Epoch 1 step 525: training loss: 66.68580641387067\n",
      "Epoch 1 step 526: training accuarcy: 0.9987179487179487\n",
      "Epoch 1: train loss 122.85835488505256, train accuarcy 0.9913392663002014\n",
      "Epoch 1: valid loss 493.6364807763769, valid accuarcy 0.9928440451622009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████████████████████████████▊                                                                                                       | 2/5 [10:50<16:11, 323.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 101.43861902523096\n",
      "Epoch 2 step 527: training accuarcy: 0.997\n",
      "Epoch 2 step 527: training loss: 98.12519836007206\n",
      "Epoch 2 step 528: training accuarcy: 0.9985\n",
      "Epoch 2 step 528: training loss: 100.08480651839184\n",
      "Epoch 2 step 529: training accuarcy: 0.998\n",
      "Epoch 2 step 529: training loss: 94.41992006424238\n",
      "Epoch 2 step 530: training accuarcy: 0.9975\n",
      "Epoch 2 step 530: training loss: 93.59335687173876\n",
      "Epoch 2 step 531: training accuarcy: 0.9985\n",
      "Epoch 2 step 531: training loss: 98.69908418056505\n",
      "Epoch 2 step 532: training accuarcy: 0.997\n",
      "Epoch 2 step 532: training loss: 98.73295389109896\n",
      "Epoch 2 step 533: training accuarcy: 0.997\n",
      "Epoch 2 step 533: training loss: 102.66478820543115\n",
      "Epoch 2 step 534: training accuarcy: 0.999\n",
      "Epoch 2 step 534: training loss: 103.41619363995642\n",
      "Epoch 2 step 535: training accuarcy: 0.997\n",
      "Epoch 2 step 535: training loss: 105.08133857130198\n",
      "Epoch 2 step 536: training accuarcy: 0.9965\n",
      "Epoch 2 step 536: training loss: 99.4214891898074\n",
      "Epoch 2 step 537: training accuarcy: 0.998\n",
      "Epoch 2 step 537: training loss: 101.0890380971835\n",
      "Epoch 2 step 538: training accuarcy: 0.9985\n",
      "Epoch 2 step 538: training loss: 93.61549841242217\n",
      "Epoch 2 step 539: training accuarcy: 0.999\n",
      "Epoch 2 step 539: training loss: 92.58094909400918\n",
      "Epoch 2 step 540: training accuarcy: 0.9985\n",
      "Epoch 2 step 540: training loss: 94.92943143094504\n",
      "Epoch 2 step 541: training accuarcy: 0.998\n",
      "Epoch 2 step 541: training loss: 103.76773418796509\n",
      "Epoch 2 step 542: training accuarcy: 0.997\n",
      "Epoch 2 step 542: training loss: 96.32973599830339\n",
      "Epoch 2 step 543: training accuarcy: 0.9985\n",
      "Epoch 2 step 543: training loss: 96.25279026689773\n",
      "Epoch 2 step 544: training accuarcy: 0.9975\n",
      "Epoch 2 step 544: training loss: 96.73520518025316\n",
      "Epoch 2 step 545: training accuarcy: 0.998\n",
      "Epoch 2 step 545: training loss: 96.99572941580954\n",
      "Epoch 2 step 546: training accuarcy: 0.9975\n",
      "Epoch 2 step 546: training loss: 106.24617268747011\n",
      "Epoch 2 step 547: training accuarcy: 0.997\n",
      "Epoch 2 step 547: training loss: 100.02526206185999\n",
      "Epoch 2 step 548: training accuarcy: 0.9985\n",
      "Epoch 2 step 548: training loss: 101.75065209229874\n",
      "Epoch 2 step 549: training accuarcy: 0.9985\n",
      "Epoch 2 step 549: training loss: 104.9143070873894\n",
      "Epoch 2 step 550: training accuarcy: 0.9975\n",
      "Epoch 2 step 550: training loss: 94.70259396148028\n",
      "Epoch 2 step 551: training accuarcy: 0.9975\n",
      "Epoch 2 step 551: training loss: 100.50579537933703\n",
      "Epoch 2 step 552: training accuarcy: 0.997\n",
      "Epoch 2 step 552: training loss: 99.6830119847135\n",
      "Epoch 2 step 553: training accuarcy: 0.9975\n",
      "Epoch 2 step 553: training loss: 98.1881118546725\n",
      "Epoch 2 step 554: training accuarcy: 0.998\n",
      "Epoch 2 step 554: training loss: 100.36414925720493\n",
      "Epoch 2 step 555: training accuarcy: 0.9975\n",
      "Epoch 2 step 555: training loss: 98.7780271203214\n",
      "Epoch 2 step 556: training accuarcy: 0.998\n",
      "Epoch 2 step 556: training loss: 107.55545659443442\n",
      "Epoch 2 step 557: training accuarcy: 0.996\n",
      "Epoch 2 step 557: training loss: 94.97295908916746\n",
      "Epoch 2 step 558: training accuarcy: 0.9995\n",
      "Epoch 2 step 558: training loss: 97.83268675427976\n",
      "Epoch 2 step 559: training accuarcy: 0.998\n",
      "Epoch 2 step 559: training loss: 95.27432773265384\n",
      "Epoch 2 step 560: training accuarcy: 0.998\n",
      "Epoch 2 step 560: training loss: 102.47040687926854\n",
      "Epoch 2 step 561: training accuarcy: 0.997\n",
      "Epoch 2 step 561: training loss: 102.54676347632395\n",
      "Epoch 2 step 562: training accuarcy: 0.9975\n",
      "Epoch 2 step 562: training loss: 90.86922494568117\n",
      "Epoch 2 step 563: training accuarcy: 0.9975\n",
      "Epoch 2 step 563: training loss: 98.36692027448112\n",
      "Epoch 2 step 564: training accuarcy: 0.996\n",
      "Epoch 2 step 564: training loss: 104.52501508485565\n",
      "Epoch 2 step 565: training accuarcy: 0.9985\n",
      "Epoch 2 step 565: training loss: 98.74454419018386\n",
      "Epoch 2 step 566: training accuarcy: 0.998\n",
      "Epoch 2 step 566: training loss: 88.6507687267312\n",
      "Epoch 2 step 567: training accuarcy: 0.9985\n",
      "Epoch 2 step 567: training loss: 94.62486673245226\n",
      "Epoch 2 step 568: training accuarcy: 0.9965\n",
      "Epoch 2 step 568: training loss: 93.21978037110998\n",
      "Epoch 2 step 569: training accuarcy: 0.9975\n",
      "Epoch 2 step 569: training loss: 92.78693842620191\n",
      "Epoch 2 step 570: training accuarcy: 0.998\n",
      "Epoch 2 step 570: training loss: 103.99194224069652\n",
      "Epoch 2 step 571: training accuarcy: 0.9965\n",
      "Epoch 2 step 571: training loss: 103.87707258405455\n",
      "Epoch 2 step 572: training accuarcy: 0.996\n",
      "Epoch 2 step 572: training loss: 92.62821579035878\n",
      "Epoch 2 step 573: training accuarcy: 0.9985\n",
      "Epoch 2 step 573: training loss: 96.29989459995352\n",
      "Epoch 2 step 574: training accuarcy: 0.9965\n",
      "Epoch 2 step 574: training loss: 95.73769442224255\n",
      "Epoch 2 step 575: training accuarcy: 0.9985\n",
      "Epoch 2 step 575: training loss: 93.16517827508878\n",
      "Epoch 2 step 576: training accuarcy: 0.9975\n",
      "Epoch 2 step 576: training loss: 97.0461110871075\n",
      "Epoch 2 step 577: training accuarcy: 0.9975\n",
      "Epoch 2 step 577: training loss: 94.98686479767048\n",
      "Epoch 2 step 578: training accuarcy: 0.9975\n",
      "Epoch 2 step 578: training loss: 94.4728748706134\n",
      "Epoch 2 step 579: training accuarcy: 0.999\n",
      "Epoch 2 step 579: training loss: 97.06831946668939\n",
      "Epoch 2 step 580: training accuarcy: 0.998\n",
      "Epoch 2 step 580: training loss: 99.66944314486966\n",
      "Epoch 2 step 581: training accuarcy: 0.995\n",
      "Epoch 2 step 581: training loss: 95.76978738415355\n",
      "Epoch 2 step 582: training accuarcy: 0.9975\n",
      "Epoch 2 step 582: training loss: 100.11784674897615\n",
      "Epoch 2 step 583: training accuarcy: 0.9985\n",
      "Epoch 2 step 583: training loss: 97.843573681825\n",
      "Epoch 2 step 584: training accuarcy: 0.998\n",
      "Epoch 2 step 584: training loss: 97.04560536904756\n",
      "Epoch 2 step 585: training accuarcy: 0.9955\n",
      "Epoch 2 step 585: training loss: 98.81226737302373\n",
      "Epoch 2 step 586: training accuarcy: 0.998\n",
      "Epoch 2 step 586: training loss: 105.30304722328538\n",
      "Epoch 2 step 587: training accuarcy: 0.9965\n",
      "Epoch 2 step 587: training loss: 92.64454079565958\n",
      "Epoch 2 step 588: training accuarcy: 0.9985\n",
      "Epoch 2 step 588: training loss: 95.34439766254474\n",
      "Epoch 2 step 589: training accuarcy: 0.997\n",
      "Epoch 2 step 589: training loss: 86.33748526386478\n",
      "Epoch 2 step 590: training accuarcy: 0.998\n",
      "Epoch 2 step 590: training loss: 104.22903368307473\n",
      "Epoch 2 step 591: training accuarcy: 0.9965\n",
      "Epoch 2 step 591: training loss: 92.71406222212241\n",
      "Epoch 2 step 592: training accuarcy: 0.9975\n",
      "Epoch 2 step 592: training loss: 94.86341962951028\n",
      "Epoch 2 step 593: training accuarcy: 0.9985\n",
      "Epoch 2 step 593: training loss: 102.62641059809084\n",
      "Epoch 2 step 594: training accuarcy: 0.999\n",
      "Epoch 2 step 594: training loss: 95.64259075793964\n",
      "Epoch 2 step 595: training accuarcy: 0.997\n",
      "Epoch 2 step 595: training loss: 87.44892142843045\n",
      "Epoch 2 step 596: training accuarcy: 0.998\n",
      "Epoch 2 step 596: training loss: 92.3018951331302\n",
      "Epoch 2 step 597: training accuarcy: 0.9975\n",
      "Epoch 2 step 597: training loss: 99.32091804392874\n",
      "Epoch 2 step 598: training accuarcy: 0.9975\n",
      "Epoch 2 step 598: training loss: 94.0786038780087\n",
      "Epoch 2 step 599: training accuarcy: 0.9985\n",
      "Epoch 2 step 599: training loss: 107.8782419264414\n",
      "Epoch 2 step 600: training accuarcy: 0.9965\n",
      "Epoch 2 step 600: training loss: 106.50868200263788\n",
      "Epoch 2 step 601: training accuarcy: 0.9965\n",
      "Epoch 2 step 601: training loss: 97.86755597573747\n",
      "Epoch 2 step 602: training accuarcy: 0.997\n",
      "Epoch 2 step 602: training loss: 97.819109171922\n",
      "Epoch 2 step 603: training accuarcy: 0.997\n",
      "Epoch 2 step 603: training loss: 92.32276618768692\n",
      "Epoch 2 step 604: training accuarcy: 0.998\n",
      "Epoch 2 step 604: training loss: 99.05903694275698\n",
      "Epoch 2 step 605: training accuarcy: 0.9965\n",
      "Epoch 2 step 605: training loss: 92.69703057678086\n",
      "Epoch 2 step 606: training accuarcy: 0.9995\n",
      "Epoch 2 step 606: training loss: 97.83654880742061\n",
      "Epoch 2 step 607: training accuarcy: 0.999\n",
      "Epoch 2 step 607: training loss: 93.79319198909732\n",
      "Epoch 2 step 608: training accuarcy: 0.9975\n",
      "Epoch 2 step 608: training loss: 96.00570057694586\n",
      "Epoch 2 step 609: training accuarcy: 0.9955\n",
      "Epoch 2 step 609: training loss: 87.03994333899689\n",
      "Epoch 2 step 610: training accuarcy: 0.9975\n",
      "Epoch 2 step 610: training loss: 99.15897666017466\n",
      "Epoch 2 step 611: training accuarcy: 0.997\n",
      "Epoch 2 step 611: training loss: 98.20312999191964\n",
      "Epoch 2 step 612: training accuarcy: 0.9975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 612: training loss: 88.0239922086632\n",
      "Epoch 2 step 613: training accuarcy: 0.998\n",
      "Epoch 2 step 613: training loss: 99.59241436324064\n",
      "Epoch 2 step 614: training accuarcy: 0.997\n",
      "Epoch 2 step 614: training loss: 98.35345662969932\n",
      "Epoch 2 step 615: training accuarcy: 0.9965\n",
      "Epoch 2 step 615: training loss: 97.12087719309253\n",
      "Epoch 2 step 616: training accuarcy: 0.9975\n",
      "Epoch 2 step 616: training loss: 97.25359518414396\n",
      "Epoch 2 step 617: training accuarcy: 0.9965\n",
      "Epoch 2 step 617: training loss: 97.9420853363641\n",
      "Epoch 2 step 618: training accuarcy: 0.996\n",
      "Epoch 2 step 618: training loss: 97.45004058112477\n",
      "Epoch 2 step 619: training accuarcy: 0.9965\n",
      "Epoch 2 step 619: training loss: 91.72737432227034\n",
      "Epoch 2 step 620: training accuarcy: 0.998\n",
      "Epoch 2 step 620: training loss: 96.01672531023945\n",
      "Epoch 2 step 621: training accuarcy: 0.9975\n",
      "Epoch 2 step 621: training loss: 88.61321146524595\n",
      "Epoch 2 step 622: training accuarcy: 0.998\n",
      "Epoch 2 step 622: training loss: 94.96622299355444\n",
      "Epoch 2 step 623: training accuarcy: 0.9995\n",
      "Epoch 2 step 623: training loss: 94.6374018941753\n",
      "Epoch 2 step 624: training accuarcy: 0.998\n",
      "Epoch 2 step 624: training loss: 90.44794697662405\n",
      "Epoch 2 step 625: training accuarcy: 0.9995\n",
      "Epoch 2 step 625: training loss: 103.63986384894051\n",
      "Epoch 2 step 626: training accuarcy: 0.997\n",
      "Epoch 2 step 626: training loss: 85.19277878452237\n",
      "Epoch 2 step 627: training accuarcy: 0.9985\n",
      "Epoch 2 step 627: training loss: 88.34350143715082\n",
      "Epoch 2 step 628: training accuarcy: 0.999\n",
      "Epoch 2 step 628: training loss: 86.66479326933307\n",
      "Epoch 2 step 629: training accuarcy: 0.9975\n",
      "Epoch 2 step 629: training loss: 88.66283759895813\n",
      "Epoch 2 step 630: training accuarcy: 0.9995\n",
      "Epoch 2 step 630: training loss: 96.94939133266763\n",
      "Epoch 2 step 631: training accuarcy: 0.998\n",
      "Epoch 2 step 631: training loss: 98.21794873181605\n",
      "Epoch 2 step 632: training accuarcy: 0.997\n",
      "Epoch 2 step 632: training loss: 93.9554736141546\n",
      "Epoch 2 step 633: training accuarcy: 0.9965\n",
      "Epoch 2 step 633: training loss: 96.07488010585999\n",
      "Epoch 2 step 634: training accuarcy: 0.9975\n",
      "Epoch 2 step 634: training loss: 94.41119932794928\n",
      "Epoch 2 step 635: training accuarcy: 0.9975\n",
      "Epoch 2 step 635: training loss: 91.3190580274896\n",
      "Epoch 2 step 636: training accuarcy: 0.9995\n",
      "Epoch 2 step 636: training loss: 98.49983808924654\n",
      "Epoch 2 step 637: training accuarcy: 0.996\n",
      "Epoch 2 step 637: training loss: 93.10639571918496\n",
      "Epoch 2 step 638: training accuarcy: 0.9965\n",
      "Epoch 2 step 638: training loss: 93.53678777087575\n",
      "Epoch 2 step 639: training accuarcy: 0.998\n",
      "Epoch 2 step 639: training loss: 90.8727866258148\n",
      "Epoch 2 step 640: training accuarcy: 0.9995\n",
      "Epoch 2 step 640: training loss: 98.9625924428\n",
      "Epoch 2 step 641: training accuarcy: 0.9975\n",
      "Epoch 2 step 641: training loss: 93.61959584971711\n",
      "Epoch 2 step 642: training accuarcy: 0.9975\n",
      "Epoch 2 step 642: training loss: 94.0524621614854\n",
      "Epoch 2 step 643: training accuarcy: 0.9975\n",
      "Epoch 2 step 643: training loss: 92.11552769663649\n",
      "Epoch 2 step 644: training accuarcy: 0.998\n",
      "Epoch 2 step 644: training loss: 99.70493670041348\n",
      "Epoch 2 step 645: training accuarcy: 0.9945\n",
      "Epoch 2 step 645: training loss: 101.47001023057405\n",
      "Epoch 2 step 646: training accuarcy: 0.9975\n",
      "Epoch 2 step 646: training loss: 90.28637520635317\n",
      "Epoch 2 step 647: training accuarcy: 0.9995\n",
      "Epoch 2 step 647: training loss: 96.96302076434961\n",
      "Epoch 2 step 648: training accuarcy: 0.997\n",
      "Epoch 2 step 648: training loss: 92.36218451913933\n",
      "Epoch 2 step 649: training accuarcy: 0.9975\n",
      "Epoch 2 step 649: training loss: 93.5140418012331\n",
      "Epoch 2 step 650: training accuarcy: 0.9985\n",
      "Epoch 2 step 650: training loss: 86.50727824869541\n",
      "Epoch 2 step 651: training accuarcy: 0.9995\n",
      "Epoch 2 step 651: training loss: 92.73268616638961\n",
      "Epoch 2 step 652: training accuarcy: 0.999\n",
      "Epoch 2 step 652: training loss: 86.46655167254008\n",
      "Epoch 2 step 653: training accuarcy: 1.0\n",
      "Epoch 2 step 653: training loss: 91.64519544774943\n",
      "Epoch 2 step 654: training accuarcy: 0.9975\n",
      "Epoch 2 step 654: training loss: 92.57463729338052\n",
      "Epoch 2 step 655: training accuarcy: 0.9975\n",
      "Epoch 2 step 655: training loss: 89.35115523236394\n",
      "Epoch 2 step 656: training accuarcy: 0.998\n",
      "Epoch 2 step 656: training loss: 84.83932342419453\n",
      "Epoch 2 step 657: training accuarcy: 0.998\n",
      "Epoch 2 step 657: training loss: 85.25737560075098\n",
      "Epoch 2 step 658: training accuarcy: 0.999\n",
      "Epoch 2 step 658: training loss: 89.63609625738827\n",
      "Epoch 2 step 659: training accuarcy: 0.9985\n",
      "Epoch 2 step 659: training loss: 89.6802628714153\n",
      "Epoch 2 step 660: training accuarcy: 0.998\n",
      "Epoch 2 step 660: training loss: 94.29873484746611\n",
      "Epoch 2 step 661: training accuarcy: 0.997\n",
      "Epoch 2 step 661: training loss: 89.19731756278186\n",
      "Epoch 2 step 662: training accuarcy: 0.999\n",
      "Epoch 2 step 662: training loss: 94.05147561639899\n",
      "Epoch 2 step 663: training accuarcy: 0.998\n",
      "Epoch 2 step 663: training loss: 82.04581455356544\n",
      "Epoch 2 step 664: training accuarcy: 0.9995\n",
      "Epoch 2 step 664: training loss: 88.6435178073758\n",
      "Epoch 2 step 665: training accuarcy: 0.998\n",
      "Epoch 2 step 665: training loss: 95.71293286683708\n",
      "Epoch 2 step 666: training accuarcy: 0.999\n",
      "Epoch 2 step 666: training loss: 92.64721939178632\n",
      "Epoch 2 step 667: training accuarcy: 0.997\n",
      "Epoch 2 step 667: training loss: 85.14323009290672\n",
      "Epoch 2 step 668: training accuarcy: 0.999\n",
      "Epoch 2 step 668: training loss: 91.32372253505427\n",
      "Epoch 2 step 669: training accuarcy: 0.9975\n",
      "Epoch 2 step 669: training loss: 84.14659536345506\n",
      "Epoch 2 step 670: training accuarcy: 0.9985\n",
      "Epoch 2 step 670: training loss: 95.15117019556291\n",
      "Epoch 2 step 671: training accuarcy: 0.996\n",
      "Epoch 2 step 671: training loss: 99.18613682740246\n",
      "Epoch 2 step 672: training accuarcy: 0.997\n",
      "Epoch 2 step 672: training loss: 90.15270103077415\n",
      "Epoch 2 step 673: training accuarcy: 1.0\n",
      "Epoch 2 step 673: training loss: 81.70625813227127\n",
      "Epoch 2 step 674: training accuarcy: 0.9985\n",
      "Epoch 2 step 674: training loss: 89.54462906485115\n",
      "Epoch 2 step 675: training accuarcy: 0.9985\n",
      "Epoch 2 step 675: training loss: 94.9353908155731\n",
      "Epoch 2 step 676: training accuarcy: 0.998\n",
      "Epoch 2 step 676: training loss: 84.9461037214351\n",
      "Epoch 2 step 677: training accuarcy: 0.9985\n",
      "Epoch 2 step 677: training loss: 86.83997905728006\n",
      "Epoch 2 step 678: training accuarcy: 0.9985\n",
      "Epoch 2 step 678: training loss: 93.68923534654684\n",
      "Epoch 2 step 679: training accuarcy: 0.999\n",
      "Epoch 2 step 679: training loss: 90.28379991623375\n",
      "Epoch 2 step 680: training accuarcy: 0.998\n",
      "Epoch 2 step 680: training loss: 98.82077009529264\n",
      "Epoch 2 step 681: training accuarcy: 0.996\n",
      "Epoch 2 step 681: training loss: 90.38734894965603\n",
      "Epoch 2 step 682: training accuarcy: 0.999\n",
      "Epoch 2 step 682: training loss: 90.7642098385621\n",
      "Epoch 2 step 683: training accuarcy: 0.9995\n",
      "Epoch 2 step 683: training loss: 89.69589209779001\n",
      "Epoch 2 step 684: training accuarcy: 0.9965\n",
      "Epoch 2 step 684: training loss: 80.7492207137111\n",
      "Epoch 2 step 685: training accuarcy: 0.998\n",
      "Epoch 2 step 685: training loss: 96.4697462763939\n",
      "Epoch 2 step 686: training accuarcy: 0.9995\n",
      "Epoch 2 step 686: training loss: 86.94047012758935\n",
      "Epoch 2 step 687: training accuarcy: 1.0\n",
      "Epoch 2 step 687: training loss: 89.17809771449305\n",
      "Epoch 2 step 688: training accuarcy: 0.997\n",
      "Epoch 2 step 688: training loss: 92.76909539181824\n",
      "Epoch 2 step 689: training accuarcy: 0.998\n",
      "Epoch 2 step 689: training loss: 85.37256455980986\n",
      "Epoch 2 step 690: training accuarcy: 0.9995\n",
      "Epoch 2 step 690: training loss: 94.22606094766934\n",
      "Epoch 2 step 691: training accuarcy: 0.995\n",
      "Epoch 2 step 691: training loss: 88.9307561924769\n",
      "Epoch 2 step 692: training accuarcy: 0.998\n",
      "Epoch 2 step 692: training loss: 89.1048193783147\n",
      "Epoch 2 step 693: training accuarcy: 0.9985\n",
      "Epoch 2 step 693: training loss: 88.71914663215028\n",
      "Epoch 2 step 694: training accuarcy: 0.997\n",
      "Epoch 2 step 694: training loss: 86.89875366203648\n",
      "Epoch 2 step 695: training accuarcy: 0.999\n",
      "Epoch 2 step 695: training loss: 87.15858154613653\n",
      "Epoch 2 step 696: training accuarcy: 0.9975\n",
      "Epoch 2 step 696: training loss: 86.35033063405348\n",
      "Epoch 2 step 697: training accuarcy: 0.9985\n",
      "Epoch 2 step 697: training loss: 96.61677891368817\n",
      "Epoch 2 step 698: training accuarcy: 0.9975\n",
      "Epoch 2 step 698: training loss: 100.89275415527521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 699: training accuarcy: 0.999\n",
      "Epoch 2 step 699: training loss: 89.94454110920289\n",
      "Epoch 2 step 700: training accuarcy: 0.9975\n",
      "Epoch 2 step 700: training loss: 93.39324463408919\n",
      "Epoch 2 step 701: training accuarcy: 0.9975\n",
      "Epoch 2 step 701: training loss: 81.37584796347107\n",
      "Epoch 2 step 702: training accuarcy: 0.9965\n",
      "Epoch 2 step 702: training loss: 96.24924640624602\n",
      "Epoch 2 step 703: training accuarcy: 0.998\n",
      "Epoch 2 step 703: training loss: 96.54910484218979\n",
      "Epoch 2 step 704: training accuarcy: 0.997\n",
      "Epoch 2 step 704: training loss: 94.70215488821518\n",
      "Epoch 2 step 705: training accuarcy: 0.997\n",
      "Epoch 2 step 705: training loss: 94.55321092476878\n",
      "Epoch 2 step 706: training accuarcy: 0.999\n",
      "Epoch 2 step 706: training loss: 91.03590086328015\n",
      "Epoch 2 step 707: training accuarcy: 0.999\n",
      "Epoch 2 step 707: training loss: 89.24351150783198\n",
      "Epoch 2 step 708: training accuarcy: 0.999\n",
      "Epoch 2 step 708: training loss: 97.63448040190605\n",
      "Epoch 2 step 709: training accuarcy: 0.9965\n",
      "Epoch 2 step 709: training loss: 88.30522496678748\n",
      "Epoch 2 step 710: training accuarcy: 0.998\n",
      "Epoch 2 step 710: training loss: 92.47876905868269\n",
      "Epoch 2 step 711: training accuarcy: 0.9965\n",
      "Epoch 2 step 711: training loss: 84.68717387145873\n",
      "Epoch 2 step 712: training accuarcy: 0.9985\n",
      "Epoch 2 step 712: training loss: 85.0407369977118\n",
      "Epoch 2 step 713: training accuarcy: 0.9975\n",
      "Epoch 2 step 713: training loss: 86.5528983954224\n",
      "Epoch 2 step 714: training accuarcy: 0.9995\n",
      "Epoch 2 step 714: training loss: 86.5444818216053\n",
      "Epoch 2 step 715: training accuarcy: 0.9975\n",
      "Epoch 2 step 715: training loss: 87.78558796182728\n",
      "Epoch 2 step 716: training accuarcy: 0.996\n",
      "Epoch 2 step 716: training loss: 82.74888389424308\n",
      "Epoch 2 step 717: training accuarcy: 0.999\n",
      "Epoch 2 step 717: training loss: 83.04454524353378\n",
      "Epoch 2 step 718: training accuarcy: 0.9985\n",
      "Epoch 2 step 718: training loss: 93.12903281133109\n",
      "Epoch 2 step 719: training accuarcy: 0.9975\n",
      "Epoch 2 step 719: training loss: 84.2058799295562\n",
      "Epoch 2 step 720: training accuarcy: 0.999\n",
      "Epoch 2 step 720: training loss: 90.49054018105475\n",
      "Epoch 2 step 721: training accuarcy: 0.999\n",
      "Epoch 2 step 721: training loss: 89.09399436458217\n",
      "Epoch 2 step 722: training accuarcy: 0.999\n",
      "Epoch 2 step 722: training loss: 89.72856905932866\n",
      "Epoch 2 step 723: training accuarcy: 0.998\n",
      "Epoch 2 step 723: training loss: 87.72006619151679\n",
      "Epoch 2 step 724: training accuarcy: 0.999\n",
      "Epoch 2 step 724: training loss: 83.78419215661464\n",
      "Epoch 2 step 725: training accuarcy: 0.998\n",
      "Epoch 2 step 725: training loss: 78.05322784949453\n",
      "Epoch 2 step 726: training accuarcy: 0.9985\n",
      "Epoch 2 step 726: training loss: 83.08849255568992\n",
      "Epoch 2 step 727: training accuarcy: 0.999\n",
      "Epoch 2 step 727: training loss: 88.66168291017073\n",
      "Epoch 2 step 728: training accuarcy: 0.9975\n",
      "Epoch 2 step 728: training loss: 81.27517078942836\n",
      "Epoch 2 step 729: training accuarcy: 0.9995\n",
      "Epoch 2 step 729: training loss: 87.94325060333378\n",
      "Epoch 2 step 730: training accuarcy: 0.997\n",
      "Epoch 2 step 730: training loss: 88.2875652827935\n",
      "Epoch 2 step 731: training accuarcy: 0.9975\n",
      "Epoch 2 step 731: training loss: 81.65447828157124\n",
      "Epoch 2 step 732: training accuarcy: 0.9995\n",
      "Epoch 2 step 732: training loss: 87.91293320976988\n",
      "Epoch 2 step 733: training accuarcy: 0.997\n",
      "Epoch 2 step 733: training loss: 89.46876137859465\n",
      "Epoch 2 step 734: training accuarcy: 0.998\n",
      "Epoch 2 step 734: training loss: 79.27953095380897\n",
      "Epoch 2 step 735: training accuarcy: 0.997\n",
      "Epoch 2 step 735: training loss: 80.83905674993828\n",
      "Epoch 2 step 736: training accuarcy: 0.9975\n",
      "Epoch 2 step 736: training loss: 83.09401786487278\n",
      "Epoch 2 step 737: training accuarcy: 0.9985\n",
      "Epoch 2 step 737: training loss: 82.01741407473241\n",
      "Epoch 2 step 738: training accuarcy: 0.9985\n",
      "Epoch 2 step 738: training loss: 82.80774031667929\n",
      "Epoch 2 step 739: training accuarcy: 0.9985\n",
      "Epoch 2 step 739: training loss: 87.718895113666\n",
      "Epoch 2 step 740: training accuarcy: 0.998\n",
      "Epoch 2 step 740: training loss: 76.18133245418983\n",
      "Epoch 2 step 741: training accuarcy: 0.9985\n",
      "Epoch 2 step 741: training loss: 84.10009261539204\n",
      "Epoch 2 step 742: training accuarcy: 0.9975\n",
      "Epoch 2 step 742: training loss: 88.9860663249911\n",
      "Epoch 2 step 743: training accuarcy: 0.997\n",
      "Epoch 2 step 743: training loss: 85.51753644105418\n",
      "Epoch 2 step 744: training accuarcy: 0.999\n",
      "Epoch 2 step 744: training loss: 90.24063004302423\n",
      "Epoch 2 step 745: training accuarcy: 0.999\n",
      "Epoch 2 step 745: training loss: 82.32209119684902\n",
      "Epoch 2 step 746: training accuarcy: 0.9985\n",
      "Epoch 2 step 746: training loss: 88.66706030329419\n",
      "Epoch 2 step 747: training accuarcy: 0.997\n",
      "Epoch 2 step 747: training loss: 84.768961374246\n",
      "Epoch 2 step 748: training accuarcy: 0.999\n",
      "Epoch 2 step 748: training loss: 77.77251937046181\n",
      "Epoch 2 step 749: training accuarcy: 0.9995\n",
      "Epoch 2 step 749: training loss: 83.97002863324445\n",
      "Epoch 2 step 750: training accuarcy: 0.9995\n",
      "Epoch 2 step 750: training loss: 92.72933196763532\n",
      "Epoch 2 step 751: training accuarcy: 0.997\n",
      "Epoch 2 step 751: training loss: 91.27412684631778\n",
      "Epoch 2 step 752: training accuarcy: 0.995\n",
      "Epoch 2 step 752: training loss: 82.41753081472626\n",
      "Epoch 2 step 753: training accuarcy: 0.997\n",
      "Epoch 2 step 753: training loss: 85.43068336038594\n",
      "Epoch 2 step 754: training accuarcy: 0.998\n",
      "Epoch 2 step 754: training loss: 81.0512651043481\n",
      "Epoch 2 step 755: training accuarcy: 0.9995\n",
      "Epoch 2 step 755: training loss: 84.96545792554542\n",
      "Epoch 2 step 756: training accuarcy: 0.998\n",
      "Epoch 2 step 756: training loss: 82.24881122618514\n",
      "Epoch 2 step 757: training accuarcy: 0.999\n",
      "Epoch 2 step 757: training loss: 81.88639924637738\n",
      "Epoch 2 step 758: training accuarcy: 0.9985\n",
      "Epoch 2 step 758: training loss: 85.95620058484882\n",
      "Epoch 2 step 759: training accuarcy: 0.9965\n",
      "Epoch 2 step 759: training loss: 80.16357032792878\n",
      "Epoch 2 step 760: training accuarcy: 0.9995\n",
      "Epoch 2 step 760: training loss: 90.79089312524296\n",
      "Epoch 2 step 761: training accuarcy: 0.9975\n",
      "Epoch 2 step 761: training loss: 82.94605092776366\n",
      "Epoch 2 step 762: training accuarcy: 0.997\n",
      "Epoch 2 step 762: training loss: 85.9259963325669\n",
      "Epoch 2 step 763: training accuarcy: 0.9995\n",
      "Epoch 2 step 763: training loss: 96.17148750237443\n",
      "Epoch 2 step 764: training accuarcy: 0.9975\n",
      "Epoch 2 step 764: training loss: 76.99866744386485\n",
      "Epoch 2 step 765: training accuarcy: 0.998\n",
      "Epoch 2 step 765: training loss: 93.85916143405353\n",
      "Epoch 2 step 766: training accuarcy: 0.9965\n",
      "Epoch 2 step 766: training loss: 85.99006661469039\n",
      "Epoch 2 step 767: training accuarcy: 0.998\n",
      "Epoch 2 step 767: training loss: 92.33736492437771\n",
      "Epoch 2 step 768: training accuarcy: 0.997\n",
      "Epoch 2 step 768: training loss: 79.70955491157127\n",
      "Epoch 2 step 769: training accuarcy: 0.9985\n",
      "Epoch 2 step 769: training loss: 87.11579134154155\n",
      "Epoch 2 step 770: training accuarcy: 0.998\n",
      "Epoch 2 step 770: training loss: 81.38848485338488\n",
      "Epoch 2 step 771: training accuarcy: 0.999\n",
      "Epoch 2 step 771: training loss: 89.31390698440516\n",
      "Epoch 2 step 772: training accuarcy: 0.997\n",
      "Epoch 2 step 772: training loss: 85.96177857610928\n",
      "Epoch 2 step 773: training accuarcy: 0.999\n",
      "Epoch 2 step 773: training loss: 86.9228519671887\n",
      "Epoch 2 step 774: training accuarcy: 0.998\n",
      "Epoch 2 step 774: training loss: 81.63389660615549\n",
      "Epoch 2 step 775: training accuarcy: 0.999\n",
      "Epoch 2 step 775: training loss: 81.80480985732024\n",
      "Epoch 2 step 776: training accuarcy: 0.9985\n",
      "Epoch 2 step 776: training loss: 80.27194897618645\n",
      "Epoch 2 step 777: training accuarcy: 1.0\n",
      "Epoch 2 step 777: training loss: 85.97229655750911\n",
      "Epoch 2 step 778: training accuarcy: 0.997\n",
      "Epoch 2 step 778: training loss: 78.23411431625001\n",
      "Epoch 2 step 779: training accuarcy: 0.998\n",
      "Epoch 2 step 779: training loss: 79.42881752018212\n",
      "Epoch 2 step 780: training accuarcy: 0.9985\n",
      "Epoch 2 step 780: training loss: 80.938777250346\n",
      "Epoch 2 step 781: training accuarcy: 0.999\n",
      "Epoch 2 step 781: training loss: 77.1255313674228\n",
      "Epoch 2 step 782: training accuarcy: 0.999\n",
      "Epoch 2 step 782: training loss: 94.52456067462047\n",
      "Epoch 2 step 783: training accuarcy: 0.9975\n",
      "Epoch 2 step 783: training loss: 84.766172408389\n",
      "Epoch 2 step 784: training accuarcy: 0.999\n",
      "Epoch 2 step 784: training loss: 83.40179290643655\n",
      "Epoch 2 step 785: training accuarcy: 0.9975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 785: training loss: 90.46943986814654\n",
      "Epoch 2 step 786: training accuarcy: 0.998\n",
      "Epoch 2 step 786: training loss: 82.80781465229008\n",
      "Epoch 2 step 787: training accuarcy: 0.9995\n",
      "Epoch 2 step 787: training loss: 84.22402104579967\n",
      "Epoch 2 step 788: training accuarcy: 0.9985\n",
      "Epoch 2 step 788: training loss: 67.69088402022675\n",
      "Epoch 2 step 789: training accuarcy: 0.9974358974358974\n",
      "Epoch 2: train loss 91.71740211437867, train accuarcy 0.9936988353729248\n",
      "Epoch 2: valid loss 395.8574833169728, valid accuarcy 0.9953102469444275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 3/5 [16:11<10:45, 322.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 84.31587940653682\n",
      "Epoch 3 step 790: training accuarcy: 0.9995\n",
      "Epoch 3 step 790: training loss: 91.08497624143925\n",
      "Epoch 3 step 791: training accuarcy: 0.998\n",
      "Epoch 3 step 791: training loss: 83.29018092444335\n",
      "Epoch 3 step 792: training accuarcy: 0.9965\n",
      "Epoch 3 step 792: training loss: 84.81785292669059\n",
      "Epoch 3 step 793: training accuarcy: 0.997\n",
      "Epoch 3 step 793: training loss: 81.05530565084699\n",
      "Epoch 3 step 794: training accuarcy: 0.9985\n",
      "Epoch 3 step 794: training loss: 89.35545414463454\n",
      "Epoch 3 step 795: training accuarcy: 0.9965\n",
      "Epoch 3 step 795: training loss: 91.1396885802651\n",
      "Epoch 3 step 796: training accuarcy: 0.9975\n",
      "Epoch 3 step 796: training loss: 90.41944701087513\n",
      "Epoch 3 step 797: training accuarcy: 0.9975\n",
      "Epoch 3 step 797: training loss: 84.42028542316743\n",
      "Epoch 3 step 798: training accuarcy: 0.9975\n",
      "Epoch 3 step 798: training loss: 86.56936948164973\n",
      "Epoch 3 step 799: training accuarcy: 0.998\n",
      "Epoch 3 step 799: training loss: 82.94465287080146\n",
      "Epoch 3 step 800: training accuarcy: 0.998\n",
      "Epoch 3 step 800: training loss: 83.6642091624737\n",
      "Epoch 3 step 801: training accuarcy: 0.9975\n",
      "Epoch 3 step 801: training loss: 92.61685297187286\n",
      "Epoch 3 step 802: training accuarcy: 0.9955\n",
      "Epoch 3 step 802: training loss: 86.81232328970785\n",
      "Epoch 3 step 803: training accuarcy: 0.9965\n",
      "Epoch 3 step 803: training loss: 79.84977954244316\n",
      "Epoch 3 step 804: training accuarcy: 0.9985\n",
      "Epoch 3 step 804: training loss: 84.91369626610981\n",
      "Epoch 3 step 805: training accuarcy: 0.9975\n",
      "Epoch 3 step 805: training loss: 81.79947252325488\n",
      "Epoch 3 step 806: training accuarcy: 0.9975\n",
      "Epoch 3 step 806: training loss: 91.54012123207761\n",
      "Epoch 3 step 807: training accuarcy: 0.997\n",
      "Epoch 3 step 807: training loss: 81.64700101778935\n",
      "Epoch 3 step 808: training accuarcy: 0.999\n",
      "Epoch 3 step 808: training loss: 83.55816724603191\n",
      "Epoch 3 step 809: training accuarcy: 0.9975\n",
      "Epoch 3 step 809: training loss: 79.21207352535507\n",
      "Epoch 3 step 810: training accuarcy: 0.998\n",
      "Epoch 3 step 810: training loss: 88.57251998822323\n",
      "Epoch 3 step 811: training accuarcy: 0.998\n",
      "Epoch 3 step 811: training loss: 81.43403331402504\n",
      "Epoch 3 step 812: training accuarcy: 0.9985\n",
      "Epoch 3 step 812: training loss: 84.52270356759323\n",
      "Epoch 3 step 813: training accuarcy: 0.999\n",
      "Epoch 3 step 813: training loss: 80.36436746771676\n",
      "Epoch 3 step 814: training accuarcy: 0.999\n",
      "Epoch 3 step 814: training loss: 76.20008895373675\n",
      "Epoch 3 step 815: training accuarcy: 0.9995\n",
      "Epoch 3 step 815: training loss: 84.90043239292507\n",
      "Epoch 3 step 816: training accuarcy: 0.998\n",
      "Epoch 3 step 816: training loss: 77.27355361822913\n",
      "Epoch 3 step 817: training accuarcy: 0.9975\n",
      "Epoch 3 step 817: training loss: 84.48334343655762\n",
      "Epoch 3 step 818: training accuarcy: 0.998\n",
      "Epoch 3 step 818: training loss: 88.23511685505005\n",
      "Epoch 3 step 819: training accuarcy: 0.998\n",
      "Epoch 3 step 819: training loss: 82.21186169298699\n",
      "Epoch 3 step 820: training accuarcy: 0.998\n",
      "Epoch 3 step 820: training loss: 85.96599489498553\n",
      "Epoch 3 step 821: training accuarcy: 0.9985\n",
      "Epoch 3 step 821: training loss: 82.64056033128463\n",
      "Epoch 3 step 822: training accuarcy: 0.998\n",
      "Epoch 3 step 822: training loss: 77.123936482028\n",
      "Epoch 3 step 823: training accuarcy: 0.9985\n",
      "Epoch 3 step 823: training loss: 89.75914677953841\n",
      "Epoch 3 step 824: training accuarcy: 0.999\n",
      "Epoch 3 step 824: training loss: 80.0743483907353\n",
      "Epoch 3 step 825: training accuarcy: 0.9985\n",
      "Epoch 3 step 825: training loss: 83.64919044035435\n",
      "Epoch 3 step 826: training accuarcy: 0.998\n",
      "Epoch 3 step 826: training loss: 84.72462472845999\n",
      "Epoch 3 step 827: training accuarcy: 0.9985\n",
      "Epoch 3 step 827: training loss: 80.62074247892515\n",
      "Epoch 3 step 828: training accuarcy: 0.9965\n",
      "Epoch 3 step 828: training loss: 80.81406849261852\n",
      "Epoch 3 step 829: training accuarcy: 0.999\n",
      "Epoch 3 step 829: training loss: 82.03154464168786\n",
      "Epoch 3 step 830: training accuarcy: 0.9985\n",
      "Epoch 3 step 830: training loss: 75.56731638879906\n",
      "Epoch 3 step 831: training accuarcy: 0.9995\n",
      "Epoch 3 step 831: training loss: 86.19037143816048\n",
      "Epoch 3 step 832: training accuarcy: 0.9995\n",
      "Epoch 3 step 832: training loss: 78.48663052246944\n",
      "Epoch 3 step 833: training accuarcy: 0.998\n",
      "Epoch 3 step 833: training loss: 70.97248458658233\n",
      "Epoch 3 step 834: training accuarcy: 0.999\n",
      "Epoch 3 step 834: training loss: 78.66862635312214\n",
      "Epoch 3 step 835: training accuarcy: 0.997\n",
      "Epoch 3 step 835: training loss: 78.55341362265754\n",
      "Epoch 3 step 836: training accuarcy: 0.999\n",
      "Epoch 3 step 836: training loss: 86.0392791129361\n",
      "Epoch 3 step 837: training accuarcy: 0.996\n",
      "Epoch 3 step 837: training loss: 82.0378317194788\n",
      "Epoch 3 step 838: training accuarcy: 0.998\n",
      "Epoch 3 step 838: training loss: 81.40711683739403\n",
      "Epoch 3 step 839: training accuarcy: 0.9985\n",
      "Epoch 3 step 839: training loss: 77.13147387177713\n",
      "Epoch 3 step 840: training accuarcy: 0.998\n",
      "Epoch 3 step 840: training loss: 78.75109203581061\n",
      "Epoch 3 step 841: training accuarcy: 0.998\n",
      "Epoch 3 step 841: training loss: 82.35390286957274\n",
      "Epoch 3 step 842: training accuarcy: 0.997\n",
      "Epoch 3 step 842: training loss: 79.18004208019146\n",
      "Epoch 3 step 843: training accuarcy: 0.998\n",
      "Epoch 3 step 843: training loss: 83.6965618651048\n",
      "Epoch 3 step 844: training accuarcy: 0.998\n",
      "Epoch 3 step 844: training loss: 78.69537617379635\n",
      "Epoch 3 step 845: training accuarcy: 0.9975\n",
      "Epoch 3 step 845: training loss: 76.26690739645365\n",
      "Epoch 3 step 846: training accuarcy: 0.997\n",
      "Epoch 3 step 846: training loss: 78.07194735293177\n",
      "Epoch 3 step 847: training accuarcy: 0.998\n",
      "Epoch 3 step 847: training loss: 81.3720256958022\n",
      "Epoch 3 step 848: training accuarcy: 0.997\n",
      "Epoch 3 step 848: training loss: 77.7640962939494\n",
      "Epoch 3 step 849: training accuarcy: 0.9975\n",
      "Epoch 3 step 849: training loss: 85.0038781951435\n",
      "Epoch 3 step 850: training accuarcy: 0.9985\n",
      "Epoch 3 step 850: training loss: 76.38329845223367\n",
      "Epoch 3 step 851: training accuarcy: 0.999\n",
      "Epoch 3 step 851: training loss: 73.34358834492589\n",
      "Epoch 3 step 852: training accuarcy: 0.999\n",
      "Epoch 3 step 852: training loss: 82.23505167726725\n",
      "Epoch 3 step 853: training accuarcy: 0.998\n",
      "Epoch 3 step 853: training loss: 86.81052122808197\n",
      "Epoch 3 step 854: training accuarcy: 0.997\n",
      "Epoch 3 step 854: training loss: 81.03619127167326\n",
      "Epoch 3 step 855: training accuarcy: 0.9985\n",
      "Epoch 3 step 855: training loss: 88.13276666524648\n",
      "Epoch 3 step 856: training accuarcy: 0.997\n",
      "Epoch 3 step 856: training loss: 80.30207419161715\n",
      "Epoch 3 step 857: training accuarcy: 0.999\n",
      "Epoch 3 step 857: training loss: 82.04961627132029\n",
      "Epoch 3 step 858: training accuarcy: 0.9975\n",
      "Epoch 3 step 858: training loss: 81.77281086458844\n",
      "Epoch 3 step 859: training accuarcy: 0.998\n",
      "Epoch 3 step 859: training loss: 80.12371718634807\n",
      "Epoch 3 step 860: training accuarcy: 0.9985\n",
      "Epoch 3 step 860: training loss: 80.79532507092893\n",
      "Epoch 3 step 861: training accuarcy: 0.9965\n",
      "Epoch 3 step 861: training loss: 92.8265462923543\n",
      "Epoch 3 step 862: training accuarcy: 0.998\n",
      "Epoch 3 step 862: training loss: 84.07319383066925\n",
      "Epoch 3 step 863: training accuarcy: 0.997\n",
      "Epoch 3 step 863: training loss: 81.6646305652909\n",
      "Epoch 3 step 864: training accuarcy: 0.997\n",
      "Epoch 3 step 864: training loss: 79.34189817565905\n",
      "Epoch 3 step 865: training accuarcy: 1.0\n",
      "Epoch 3 step 865: training loss: 79.63479048540549\n",
      "Epoch 3 step 866: training accuarcy: 0.9975\n",
      "Epoch 3 step 866: training loss: 81.0291894773773\n",
      "Epoch 3 step 867: training accuarcy: 0.9995\n",
      "Epoch 3 step 867: training loss: 76.14534858510103\n",
      "Epoch 3 step 868: training accuarcy: 0.9985\n",
      "Epoch 3 step 868: training loss: 82.56964774043414\n",
      "Epoch 3 step 869: training accuarcy: 0.999\n",
      "Epoch 3 step 869: training loss: 78.28520494584865\n",
      "Epoch 3 step 870: training accuarcy: 0.9985\n",
      "Epoch 3 step 870: training loss: 83.90516069630493\n",
      "Epoch 3 step 871: training accuarcy: 0.999\n",
      "Epoch 3 step 871: training loss: 79.52863583781885\n",
      "Epoch 3 step 872: training accuarcy: 0.998\n",
      "Epoch 3 step 872: training loss: 80.09349613260414\n",
      "Epoch 3 step 873: training accuarcy: 0.9985\n",
      "Epoch 3 step 873: training loss: 83.27256486906421\n",
      "Epoch 3 step 874: training accuarcy: 0.9995\n",
      "Epoch 3 step 874: training loss: 88.90475120200938\n",
      "Epoch 3 step 875: training accuarcy: 0.9975\n",
      "Epoch 3 step 875: training loss: 79.25480991487115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 876: training accuarcy: 0.9985\n",
      "Epoch 3 step 876: training loss: 83.1523623054694\n",
      "Epoch 3 step 877: training accuarcy: 0.999\n",
      "Epoch 3 step 877: training loss: 85.29317765580154\n",
      "Epoch 3 step 878: training accuarcy: 0.998\n",
      "Epoch 3 step 878: training loss: 80.9200787041535\n",
      "Epoch 3 step 879: training accuarcy: 0.9995\n",
      "Epoch 3 step 879: training loss: 75.73755804440607\n",
      "Epoch 3 step 880: training accuarcy: 1.0\n",
      "Epoch 3 step 880: training loss: 86.1828677918307\n",
      "Epoch 3 step 881: training accuarcy: 0.9975\n",
      "Epoch 3 step 881: training loss: 78.29192165646836\n",
      "Epoch 3 step 882: training accuarcy: 1.0\n",
      "Epoch 3 step 882: training loss: 88.05559798146749\n",
      "Epoch 3 step 883: training accuarcy: 0.9965\n",
      "Epoch 3 step 883: training loss: 78.70566437138788\n",
      "Epoch 3 step 884: training accuarcy: 0.999\n",
      "Epoch 3 step 884: training loss: 82.82901820682002\n",
      "Epoch 3 step 885: training accuarcy: 0.997\n",
      "Epoch 3 step 885: training loss: 81.0699892632295\n",
      "Epoch 3 step 886: training accuarcy: 0.999\n",
      "Epoch 3 step 886: training loss: 78.75261379684125\n",
      "Epoch 3 step 887: training accuarcy: 0.9985\n",
      "Epoch 3 step 887: training loss: 88.99051616421926\n",
      "Epoch 3 step 888: training accuarcy: 0.9995\n",
      "Epoch 3 step 888: training loss: 86.91762715498898\n",
      "Epoch 3 step 889: training accuarcy: 0.998\n",
      "Epoch 3 step 889: training loss: 82.26278331485841\n",
      "Epoch 3 step 890: training accuarcy: 0.998\n",
      "Epoch 3 step 890: training loss: 85.58849974304752\n",
      "Epoch 3 step 891: training accuarcy: 0.9965\n",
      "Epoch 3 step 891: training loss: 85.66456258015691\n",
      "Epoch 3 step 892: training accuarcy: 1.0\n",
      "Epoch 3 step 892: training loss: 74.47786414078104\n",
      "Epoch 3 step 893: training accuarcy: 0.9995\n",
      "Epoch 3 step 893: training loss: 79.46462431775078\n",
      "Epoch 3 step 894: training accuarcy: 0.999\n",
      "Epoch 3 step 894: training loss: 83.49006073105451\n",
      "Epoch 3 step 895: training accuarcy: 0.9975\n",
      "Epoch 3 step 895: training loss: 84.28146603556043\n",
      "Epoch 3 step 896: training accuarcy: 1.0\n",
      "Epoch 3 step 896: training loss: 85.3167492186042\n",
      "Epoch 3 step 897: training accuarcy: 0.998\n",
      "Epoch 3 step 897: training loss: 73.6736109554212\n",
      "Epoch 3 step 898: training accuarcy: 0.9995\n",
      "Epoch 3 step 898: training loss: 70.98490597875399\n",
      "Epoch 3 step 899: training accuarcy: 0.998\n",
      "Epoch 3 step 899: training loss: 83.31167999185158\n",
      "Epoch 3 step 900: training accuarcy: 0.999\n",
      "Epoch 3 step 900: training loss: 78.48395027804011\n",
      "Epoch 3 step 901: training accuarcy: 0.9995\n",
      "Epoch 3 step 901: training loss: 79.8583648591385\n",
      "Epoch 3 step 902: training accuarcy: 0.998\n",
      "Epoch 3 step 902: training loss: 87.40700857907669\n",
      "Epoch 3 step 903: training accuarcy: 0.998\n",
      "Epoch 3 step 903: training loss: 90.60856256680415\n",
      "Epoch 3 step 904: training accuarcy: 0.9985\n",
      "Epoch 3 step 904: training loss: 85.04939718549448\n",
      "Epoch 3 step 905: training accuarcy: 0.9975\n",
      "Epoch 3 step 905: training loss: 73.41692286161245\n",
      "Epoch 3 step 906: training accuarcy: 0.9995\n",
      "Epoch 3 step 906: training loss: 76.49275153174727\n",
      "Epoch 3 step 907: training accuarcy: 0.9965\n",
      "Epoch 3 step 907: training loss: 71.900102168091\n",
      "Epoch 3 step 908: training accuarcy: 0.999\n",
      "Epoch 3 step 908: training loss: 76.70035301059863\n",
      "Epoch 3 step 909: training accuarcy: 0.999\n",
      "Epoch 3 step 909: training loss: 74.08492239809578\n",
      "Epoch 3 step 910: training accuarcy: 0.9975\n",
      "Epoch 3 step 910: training loss: 87.71820058349066\n",
      "Epoch 3 step 911: training accuarcy: 0.9975\n",
      "Epoch 3 step 911: training loss: 72.28187415601212\n",
      "Epoch 3 step 912: training accuarcy: 1.0\n",
      "Epoch 3 step 912: training loss: 75.19376042396672\n",
      "Epoch 3 step 913: training accuarcy: 1.0\n",
      "Epoch 3 step 913: training loss: 77.42169657964061\n",
      "Epoch 3 step 914: training accuarcy: 0.999\n",
      "Epoch 3 step 914: training loss: 74.64033901236611\n",
      "Epoch 3 step 915: training accuarcy: 0.9995\n",
      "Epoch 3 step 915: training loss: 82.78571335143613\n",
      "Epoch 3 step 916: training accuarcy: 0.9975\n",
      "Epoch 3 step 916: training loss: 77.18331926943151\n",
      "Epoch 3 step 917: training accuarcy: 0.9985\n",
      "Epoch 3 step 917: training loss: 74.07234676269186\n",
      "Epoch 3 step 918: training accuarcy: 0.9995\n",
      "Epoch 3 step 918: training loss: 83.70754071695836\n",
      "Epoch 3 step 919: training accuarcy: 0.999\n",
      "Epoch 3 step 919: training loss: 80.78564101475291\n",
      "Epoch 3 step 920: training accuarcy: 0.998\n",
      "Epoch 3 step 920: training loss: 74.69020038631238\n",
      "Epoch 3 step 921: training accuarcy: 0.9995\n",
      "Epoch 3 step 921: training loss: 89.0858059662527\n",
      "Epoch 3 step 922: training accuarcy: 0.9985\n",
      "Epoch 3 step 922: training loss: 77.27411854659883\n",
      "Epoch 3 step 923: training accuarcy: 0.9975\n",
      "Epoch 3 step 923: training loss: 78.53687409853136\n",
      "Epoch 3 step 924: training accuarcy: 0.9985\n",
      "Epoch 3 step 924: training loss: 72.2138639587144\n",
      "Epoch 3 step 925: training accuarcy: 0.9975\n",
      "Epoch 3 step 925: training loss: 78.37925791626506\n",
      "Epoch 3 step 926: training accuarcy: 0.9975\n",
      "Epoch 3 step 926: training loss: 82.4514000140127\n",
      "Epoch 3 step 927: training accuarcy: 0.997\n",
      "Epoch 3 step 927: training loss: 79.04995419068561\n",
      "Epoch 3 step 928: training accuarcy: 0.999\n",
      "Epoch 3 step 928: training loss: 82.7184335283982\n",
      "Epoch 3 step 929: training accuarcy: 0.9995\n",
      "Epoch 3 step 929: training loss: 79.67255535208056\n",
      "Epoch 3 step 930: training accuarcy: 0.9985\n",
      "Epoch 3 step 930: training loss: 82.40572863922614\n",
      "Epoch 3 step 931: training accuarcy: 0.999\n",
      "Epoch 3 step 931: training loss: 78.25028608159471\n",
      "Epoch 3 step 932: training accuarcy: 0.998\n",
      "Epoch 3 step 932: training loss: 80.20851732787673\n",
      "Epoch 3 step 933: training accuarcy: 0.999\n",
      "Epoch 3 step 933: training loss: 78.6691837981631\n",
      "Epoch 3 step 934: training accuarcy: 0.9995\n",
      "Epoch 3 step 934: training loss: 81.77620453246564\n",
      "Epoch 3 step 935: training accuarcy: 0.999\n",
      "Epoch 3 step 935: training loss: 82.44713802606617\n",
      "Epoch 3 step 936: training accuarcy: 0.9985\n",
      "Epoch 3 step 936: training loss: 73.6770133502923\n",
      "Epoch 3 step 937: training accuarcy: 0.999\n",
      "Epoch 3 step 937: training loss: 74.81137816881517\n",
      "Epoch 3 step 938: training accuarcy: 0.998\n",
      "Epoch 3 step 938: training loss: 82.23721785568019\n",
      "Epoch 3 step 939: training accuarcy: 0.9965\n",
      "Epoch 3 step 939: training loss: 76.77005571388963\n",
      "Epoch 3 step 940: training accuarcy: 0.999\n",
      "Epoch 3 step 940: training loss: 84.93984627590042\n",
      "Epoch 3 step 941: training accuarcy: 0.9995\n",
      "Epoch 3 step 941: training loss: 79.71175286038945\n",
      "Epoch 3 step 942: training accuarcy: 0.999\n",
      "Epoch 3 step 942: training loss: 76.41010448449425\n",
      "Epoch 3 step 943: training accuarcy: 0.999\n",
      "Epoch 3 step 943: training loss: 80.0446313059972\n",
      "Epoch 3 step 944: training accuarcy: 0.9995\n",
      "Epoch 3 step 944: training loss: 83.76648103365173\n",
      "Epoch 3 step 945: training accuarcy: 0.9985\n",
      "Epoch 3 step 945: training loss: 78.60750985220034\n",
      "Epoch 3 step 946: training accuarcy: 0.999\n",
      "Epoch 3 step 946: training loss: 80.93468093421524\n",
      "Epoch 3 step 947: training accuarcy: 0.998\n",
      "Epoch 3 step 947: training loss: 84.39021517655316\n",
      "Epoch 3 step 948: training accuarcy: 0.997\n",
      "Epoch 3 step 948: training loss: 80.72333753483193\n",
      "Epoch 3 step 949: training accuarcy: 0.9995\n",
      "Epoch 3 step 949: training loss: 81.26069650546869\n",
      "Epoch 3 step 950: training accuarcy: 0.999\n",
      "Epoch 3 step 950: training loss: 79.68443186976052\n",
      "Epoch 3 step 951: training accuarcy: 0.9995\n",
      "Epoch 3 step 951: training loss: 80.45931927306407\n",
      "Epoch 3 step 952: training accuarcy: 0.9985\n",
      "Epoch 3 step 952: training loss: 79.98134629393266\n",
      "Epoch 3 step 953: training accuarcy: 0.998\n",
      "Epoch 3 step 953: training loss: 68.08089433465146\n",
      "Epoch 3 step 954: training accuarcy: 1.0\n",
      "Epoch 3 step 954: training loss: 82.50946677761111\n",
      "Epoch 3 step 955: training accuarcy: 0.9985\n",
      "Epoch 3 step 955: training loss: 79.64079623110948\n",
      "Epoch 3 step 956: training accuarcy: 0.9985\n",
      "Epoch 3 step 956: training loss: 75.70309562090509\n",
      "Epoch 3 step 957: training accuarcy: 0.9995\n",
      "Epoch 3 step 957: training loss: 82.04096970694201\n",
      "Epoch 3 step 958: training accuarcy: 0.9985\n",
      "Epoch 3 step 958: training loss: 70.54069098006545\n",
      "Epoch 3 step 959: training accuarcy: 1.0\n",
      "Epoch 3 step 959: training loss: 83.1899372340799\n",
      "Epoch 3 step 960: training accuarcy: 0.999\n",
      "Epoch 3 step 960: training loss: 71.91004129004436\n",
      "Epoch 3 step 961: training accuarcy: 0.9975\n",
      "Epoch 3 step 961: training loss: 74.34402620303652\n",
      "Epoch 3 step 962: training accuarcy: 0.998\n",
      "Epoch 3 step 962: training loss: 78.07031111571489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 963: training accuarcy: 1.0\n",
      "Epoch 3 step 963: training loss: 82.56523291273922\n",
      "Epoch 3 step 964: training accuarcy: 0.9985\n",
      "Epoch 3 step 964: training loss: 80.3918251524544\n",
      "Epoch 3 step 965: training accuarcy: 0.999\n",
      "Epoch 3 step 965: training loss: 79.70816613166092\n",
      "Epoch 3 step 966: training accuarcy: 0.999\n",
      "Epoch 3 step 966: training loss: 83.70085702925675\n",
      "Epoch 3 step 967: training accuarcy: 0.9975\n",
      "Epoch 3 step 967: training loss: 93.83716332104228\n",
      "Epoch 3 step 968: training accuarcy: 0.999\n",
      "Epoch 3 step 968: training loss: 75.48472323147712\n",
      "Epoch 3 step 969: training accuarcy: 1.0\n",
      "Epoch 3 step 969: training loss: 73.1298923305736\n",
      "Epoch 3 step 970: training accuarcy: 0.999\n",
      "Epoch 3 step 970: training loss: 80.83726074056045\n",
      "Epoch 3 step 971: training accuarcy: 0.999\n",
      "Epoch 3 step 971: training loss: 76.97168962014158\n",
      "Epoch 3 step 972: training accuarcy: 0.999\n",
      "Epoch 3 step 972: training loss: 75.55547606460891\n",
      "Epoch 3 step 973: training accuarcy: 0.9985\n",
      "Epoch 3 step 973: training loss: 82.65073530761717\n",
      "Epoch 3 step 974: training accuarcy: 0.9975\n",
      "Epoch 3 step 974: training loss: 79.41973214650156\n",
      "Epoch 3 step 975: training accuarcy: 0.9985\n",
      "Epoch 3 step 975: training loss: 78.72342743077002\n",
      "Epoch 3 step 976: training accuarcy: 0.9985\n",
      "Epoch 3 step 976: training loss: 84.88320722391533\n",
      "Epoch 3 step 977: training accuarcy: 0.999\n",
      "Epoch 3 step 977: training loss: 80.94903654163195\n",
      "Epoch 3 step 978: training accuarcy: 0.998\n",
      "Epoch 3 step 978: training loss: 76.84721275153993\n",
      "Epoch 3 step 979: training accuarcy: 0.9985\n",
      "Epoch 3 step 979: training loss: 79.61159778057115\n",
      "Epoch 3 step 980: training accuarcy: 0.999\n",
      "Epoch 3 step 980: training loss: 74.27004122839283\n",
      "Epoch 3 step 981: training accuarcy: 0.998\n",
      "Epoch 3 step 981: training loss: 77.73554978574735\n",
      "Epoch 3 step 982: training accuarcy: 0.9985\n",
      "Epoch 3 step 982: training loss: 74.15107992612738\n",
      "Epoch 3 step 983: training accuarcy: 0.9975\n",
      "Epoch 3 step 983: training loss: 82.52720285956457\n",
      "Epoch 3 step 984: training accuarcy: 0.9975\n",
      "Epoch 3 step 984: training loss: 78.32304050781352\n",
      "Epoch 3 step 985: training accuarcy: 0.9985\n",
      "Epoch 3 step 985: training loss: 74.47401598098978\n",
      "Epoch 3 step 986: training accuarcy: 0.9995\n",
      "Epoch 3 step 986: training loss: 83.94236931964554\n",
      "Epoch 3 step 987: training accuarcy: 0.997\n",
      "Epoch 3 step 987: training loss: 76.46042446615525\n",
      "Epoch 3 step 988: training accuarcy: 0.9985\n",
      "Epoch 3 step 988: training loss: 78.57211190686387\n",
      "Epoch 3 step 989: training accuarcy: 0.9995\n",
      "Epoch 3 step 989: training loss: 74.22196379001609\n",
      "Epoch 3 step 990: training accuarcy: 0.9985\n",
      "Epoch 3 step 990: training loss: 85.35955246183494\n",
      "Epoch 3 step 991: training accuarcy: 0.998\n",
      "Epoch 3 step 991: training loss: 84.03179176857029\n",
      "Epoch 3 step 992: training accuarcy: 0.997\n",
      "Epoch 3 step 992: training loss: 70.99008550560946\n",
      "Epoch 3 step 993: training accuarcy: 0.998\n",
      "Epoch 3 step 993: training loss: 78.95204082225695\n",
      "Epoch 3 step 994: training accuarcy: 0.999\n",
      "Epoch 3 step 994: training loss: 74.59352471535563\n",
      "Epoch 3 step 995: training accuarcy: 0.9985\n",
      "Epoch 3 step 995: training loss: 77.34825703344211\n",
      "Epoch 3 step 996: training accuarcy: 0.9995\n",
      "Epoch 3 step 996: training loss: 79.60948001617922\n",
      "Epoch 3 step 997: training accuarcy: 0.9995\n",
      "Epoch 3 step 997: training loss: 80.95099848752113\n",
      "Epoch 3 step 998: training accuarcy: 0.9985\n",
      "Epoch 3 step 998: training loss: 77.12054764206309\n",
      "Epoch 3 step 999: training accuarcy: 0.999\n",
      "Epoch 3 step 999: training loss: 81.92369162788427\n",
      "Epoch 3 step 1000: training accuarcy: 0.9995\n",
      "Epoch 3 step 1000: training loss: 80.9939513160407\n",
      "Epoch 3 step 1001: training accuarcy: 0.999\n",
      "Epoch 3 step 1001: training loss: 78.40686637644632\n",
      "Epoch 3 step 1002: training accuarcy: 0.999\n",
      "Epoch 3 step 1002: training loss: 75.95374706186918\n",
      "Epoch 3 step 1003: training accuarcy: 0.9985\n",
      "Epoch 3 step 1003: training loss: 78.86779307138184\n",
      "Epoch 3 step 1004: training accuarcy: 0.9985\n",
      "Epoch 3 step 1004: training loss: 88.51263345471165\n",
      "Epoch 3 step 1005: training accuarcy: 0.9975\n",
      "Epoch 3 step 1005: training loss: 74.6652190692585\n",
      "Epoch 3 step 1006: training accuarcy: 0.999\n",
      "Epoch 3 step 1006: training loss: 70.7523185918856\n",
      "Epoch 3 step 1007: training accuarcy: 0.999\n",
      "Epoch 3 step 1007: training loss: 75.99001234825553\n",
      "Epoch 3 step 1008: training accuarcy: 0.998\n",
      "Epoch 3 step 1008: training loss: 82.1842500717545\n",
      "Epoch 3 step 1009: training accuarcy: 0.999\n",
      "Epoch 3 step 1009: training loss: 75.89787231597688\n",
      "Epoch 3 step 1010: training accuarcy: 1.0\n",
      "Epoch 3 step 1010: training loss: 79.17561899034557\n",
      "Epoch 3 step 1011: training accuarcy: 0.9995\n",
      "Epoch 3 step 1011: training loss: 79.07653937552524\n",
      "Epoch 3 step 1012: training accuarcy: 0.9995\n",
      "Epoch 3 step 1012: training loss: 77.85843653356733\n",
      "Epoch 3 step 1013: training accuarcy: 0.999\n",
      "Epoch 3 step 1013: training loss: 87.76773744537184\n",
      "Epoch 3 step 1014: training accuarcy: 0.9965\n",
      "Epoch 3 step 1014: training loss: 81.16751575600415\n",
      "Epoch 3 step 1015: training accuarcy: 0.9995\n",
      "Epoch 3 step 1015: training loss: 80.60354240127955\n",
      "Epoch 3 step 1016: training accuarcy: 0.998\n",
      "Epoch 3 step 1016: training loss: 78.95006286498256\n",
      "Epoch 3 step 1017: training accuarcy: 0.999\n",
      "Epoch 3 step 1017: training loss: 73.50426215408399\n",
      "Epoch 3 step 1018: training accuarcy: 0.9995\n",
      "Epoch 3 step 1018: training loss: 79.03550390591408\n",
      "Epoch 3 step 1019: training accuarcy: 0.9985\n",
      "Epoch 3 step 1019: training loss: 73.44538481338148\n",
      "Epoch 3 step 1020: training accuarcy: 0.999\n",
      "Epoch 3 step 1020: training loss: 77.05057365829842\n",
      "Epoch 3 step 1021: training accuarcy: 0.9985\n",
      "Epoch 3 step 1021: training loss: 76.37270223267957\n",
      "Epoch 3 step 1022: training accuarcy: 0.9985\n",
      "Epoch 3 step 1022: training loss: 76.43306478500543\n",
      "Epoch 3 step 1023: training accuarcy: 0.9985\n",
      "Epoch 3 step 1023: training loss: 78.36287878175679\n",
      "Epoch 3 step 1024: training accuarcy: 0.999\n",
      "Epoch 3 step 1024: training loss: 81.27037316583969\n",
      "Epoch 3 step 1025: training accuarcy: 0.9975\n",
      "Epoch 3 step 1025: training loss: 83.35117795811357\n",
      "Epoch 3 step 1026: training accuarcy: 0.999\n",
      "Epoch 3 step 1026: training loss: 79.612620999715\n",
      "Epoch 3 step 1027: training accuarcy: 0.9975\n",
      "Epoch 3 step 1027: training loss: 81.34467348614038\n",
      "Epoch 3 step 1028: training accuarcy: 0.998\n",
      "Epoch 3 step 1028: training loss: 76.96405589462151\n",
      "Epoch 3 step 1029: training accuarcy: 0.998\n",
      "Epoch 3 step 1029: training loss: 74.95370577577678\n",
      "Epoch 3 step 1030: training accuarcy: 0.9995\n",
      "Epoch 3 step 1030: training loss: 81.96427523606634\n",
      "Epoch 3 step 1031: training accuarcy: 0.9985\n",
      "Epoch 3 step 1031: training loss: 79.56777900912593\n",
      "Epoch 3 step 1032: training accuarcy: 0.998\n",
      "Epoch 3 step 1032: training loss: 77.57398640145843\n",
      "Epoch 3 step 1033: training accuarcy: 0.9975\n",
      "Epoch 3 step 1033: training loss: 87.07843845716707\n",
      "Epoch 3 step 1034: training accuarcy: 0.9975\n",
      "Epoch 3 step 1034: training loss: 73.37903021366444\n",
      "Epoch 3 step 1035: training accuarcy: 0.998\n",
      "Epoch 3 step 1035: training loss: 75.21044582573242\n",
      "Epoch 3 step 1036: training accuarcy: 0.998\n",
      "Epoch 3 step 1036: training loss: 81.49260616579974\n",
      "Epoch 3 step 1037: training accuarcy: 0.999\n",
      "Epoch 3 step 1037: training loss: 80.00007622518757\n",
      "Epoch 3 step 1038: training accuarcy: 0.9985\n",
      "Epoch 3 step 1038: training loss: 83.8623655978012\n",
      "Epoch 3 step 1039: training accuarcy: 0.9985\n",
      "Epoch 3 step 1039: training loss: 74.68371806327613\n",
      "Epoch 3 step 1040: training accuarcy: 0.9985\n",
      "Epoch 3 step 1040: training loss: 79.383844355827\n",
      "Epoch 3 step 1041: training accuarcy: 0.998\n",
      "Epoch 3 step 1041: training loss: 76.3716464341289\n",
      "Epoch 3 step 1042: training accuarcy: 0.998\n",
      "Epoch 3 step 1042: training loss: 86.50732743206453\n",
      "Epoch 3 step 1043: training accuarcy: 0.999\n",
      "Epoch 3 step 1043: training loss: 70.72015137175744\n",
      "Epoch 3 step 1044: training accuarcy: 0.999\n",
      "Epoch 3 step 1044: training loss: 82.20054214941928\n",
      "Epoch 3 step 1045: training accuarcy: 0.9985\n",
      "Epoch 3 step 1045: training loss: 84.65820741025527\n",
      "Epoch 3 step 1046: training accuarcy: 0.999\n",
      "Epoch 3 step 1046: training loss: 81.12037456673899\n",
      "Epoch 3 step 1047: training accuarcy: 0.9985\n",
      "Epoch 3 step 1047: training loss: 83.41386059812871\n",
      "Epoch 3 step 1048: training accuarcy: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 1048: training loss: 78.39962968533172\n",
      "Epoch 3 step 1049: training accuarcy: 0.9985\n",
      "Epoch 3 step 1049: training loss: 83.83146368424673\n",
      "Epoch 3 step 1050: training accuarcy: 0.998\n",
      "Epoch 3 step 1050: training loss: 87.91054644324207\n",
      "Epoch 3 step 1051: training accuarcy: 0.998\n",
      "Epoch 3 step 1051: training loss: 57.7186269280099\n",
      "Epoch 3 step 1052: training accuarcy: 0.9987179487179487\n",
      "Epoch 3: train loss 80.42673594301799, train accuarcy 0.9947951436042786\n",
      "Epoch 3: valid loss 341.6815974517183, valid accuarcy 0.9968870282173157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 4/5 [21:48<05:26, 326.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 72.72105277164918\n",
      "Epoch 4 step 1053: training accuarcy: 0.999\n",
      "Epoch 4 step 1053: training loss: 77.93215464233515\n",
      "Epoch 4 step 1054: training accuarcy: 0.999\n",
      "Epoch 4 step 1054: training loss: 77.76225047042777\n",
      "Epoch 4 step 1055: training accuarcy: 0.9985\n",
      "Epoch 4 step 1055: training loss: 77.32250261089878\n",
      "Epoch 4 step 1056: training accuarcy: 0.9965\n",
      "Epoch 4 step 1056: training loss: 83.90414791490176\n",
      "Epoch 4 step 1057: training accuarcy: 0.9975\n",
      "Epoch 4 step 1057: training loss: 86.10148702151633\n",
      "Epoch 4 step 1058: training accuarcy: 0.997\n",
      "Epoch 4 step 1058: training loss: 82.44206099960529\n",
      "Epoch 4 step 1059: training accuarcy: 0.9975\n",
      "Epoch 4 step 1059: training loss: 78.1009582064336\n",
      "Epoch 4 step 1060: training accuarcy: 0.9995\n",
      "Epoch 4 step 1060: training loss: 73.19186944221843\n",
      "Epoch 4 step 1061: training accuarcy: 0.999\n",
      "Epoch 4 step 1061: training loss: 76.25496077408869\n",
      "Epoch 4 step 1062: training accuarcy: 0.9985\n",
      "Epoch 4 step 1062: training loss: 77.80773515742825\n",
      "Epoch 4 step 1063: training accuarcy: 0.999\n",
      "Epoch 4 step 1063: training loss: 81.74789532525372\n",
      "Epoch 4 step 1064: training accuarcy: 0.9985\n",
      "Epoch 4 step 1064: training loss: 78.40290123227167\n",
      "Epoch 4 step 1065: training accuarcy: 0.998\n",
      "Epoch 4 step 1065: training loss: 81.13827150825333\n",
      "Epoch 4 step 1066: training accuarcy: 0.9985\n",
      "Epoch 4 step 1066: training loss: 78.90884496100955\n",
      "Epoch 4 step 1067: training accuarcy: 0.9985\n",
      "Epoch 4 step 1067: training loss: 71.65251085584612\n",
      "Epoch 4 step 1068: training accuarcy: 0.9995\n",
      "Epoch 4 step 1068: training loss: 80.88307147479995\n",
      "Epoch 4 step 1069: training accuarcy: 0.9995\n",
      "Epoch 4 step 1069: training loss: 81.82294094248199\n",
      "Epoch 4 step 1070: training accuarcy: 0.999\n",
      "Epoch 4 step 1070: training loss: 81.58897875837897\n",
      "Epoch 4 step 1071: training accuarcy: 0.998\n",
      "Epoch 4 step 1071: training loss: 70.8400299621272\n",
      "Epoch 4 step 1072: training accuarcy: 0.999\n",
      "Epoch 4 step 1072: training loss: 75.58205118682224\n",
      "Epoch 4 step 1073: training accuarcy: 0.999\n",
      "Epoch 4 step 1073: training loss: 79.22246448879895\n",
      "Epoch 4 step 1074: training accuarcy: 0.999\n",
      "Epoch 4 step 1074: training loss: 79.65081380729319\n",
      "Epoch 4 step 1075: training accuarcy: 0.9985\n",
      "Epoch 4 step 1075: training loss: 75.1431407436734\n",
      "Epoch 4 step 1076: training accuarcy: 0.9985\n",
      "Epoch 4 step 1076: training loss: 72.35178243819433\n",
      "Epoch 4 step 1077: training accuarcy: 0.999\n",
      "Epoch 4 step 1077: training loss: 79.70852980652683\n",
      "Epoch 4 step 1078: training accuarcy: 0.997\n",
      "Epoch 4 step 1078: training loss: 77.4305758412828\n",
      "Epoch 4 step 1079: training accuarcy: 0.997\n",
      "Epoch 4 step 1079: training loss: 78.36535720652837\n",
      "Epoch 4 step 1080: training accuarcy: 0.998\n",
      "Epoch 4 step 1080: training loss: 85.20931077142018\n",
      "Epoch 4 step 1081: training accuarcy: 0.997\n",
      "Epoch 4 step 1081: training loss: 78.981006155106\n",
      "Epoch 4 step 1082: training accuarcy: 0.9985\n",
      "Epoch 4 step 1082: training loss: 72.99931134218909\n",
      "Epoch 4 step 1083: training accuarcy: 0.999\n",
      "Epoch 4 step 1083: training loss: 83.09406150153418\n",
      "Epoch 4 step 1084: training accuarcy: 0.9985\n",
      "Epoch 4 step 1084: training loss: 85.42489472087894\n",
      "Epoch 4 step 1085: training accuarcy: 0.9985\n",
      "Epoch 4 step 1085: training loss: 78.0721242786666\n",
      "Epoch 4 step 1086: training accuarcy: 0.999\n",
      "Epoch 4 step 1086: training loss: 80.27476467846154\n",
      "Epoch 4 step 1087: training accuarcy: 0.9995\n",
      "Epoch 4 step 1087: training loss: 80.12612315651799\n",
      "Epoch 4 step 1088: training accuarcy: 0.9995\n",
      "Epoch 4 step 1088: training loss: 76.88425730318403\n",
      "Epoch 4 step 1089: training accuarcy: 0.998\n",
      "Epoch 4 step 1089: training loss: 80.16199450146365\n",
      "Epoch 4 step 1090: training accuarcy: 0.999\n",
      "Epoch 4 step 1090: training loss: 75.72222537527621\n",
      "Epoch 4 step 1091: training accuarcy: 0.999\n",
      "Epoch 4 step 1091: training loss: 76.40883452469163\n",
      "Epoch 4 step 1092: training accuarcy: 0.9985\n",
      "Epoch 4 step 1092: training loss: 75.4570650835478\n",
      "Epoch 4 step 1093: training accuarcy: 0.999\n",
      "Epoch 4 step 1093: training loss: 76.69639323602584\n",
      "Epoch 4 step 1094: training accuarcy: 0.9985\n",
      "Epoch 4 step 1094: training loss: 74.28575278544307\n",
      "Epoch 4 step 1095: training accuarcy: 0.9995\n",
      "Epoch 4 step 1095: training loss: 76.22062173689676\n",
      "Epoch 4 step 1096: training accuarcy: 0.999\n",
      "Epoch 4 step 1096: training loss: 77.48373170605132\n",
      "Epoch 4 step 1097: training accuarcy: 0.998\n",
      "Epoch 4 step 1097: training loss: 75.86038034431792\n",
      "Epoch 4 step 1098: training accuarcy: 1.0\n",
      "Epoch 4 step 1098: training loss: 73.40325976393981\n",
      "Epoch 4 step 1099: training accuarcy: 0.999\n",
      "Epoch 4 step 1099: training loss: 73.29597761872469\n",
      "Epoch 4 step 1100: training accuarcy: 0.999\n",
      "Epoch 4 step 1100: training loss: 74.65931338294854\n",
      "Epoch 4 step 1101: training accuarcy: 0.999\n",
      "Epoch 4 step 1101: training loss: 79.19718733932132\n",
      "Epoch 4 step 1102: training accuarcy: 0.9985\n",
      "Epoch 4 step 1102: training loss: 81.63323501531983\n",
      "Epoch 4 step 1103: training accuarcy: 0.9985\n",
      "Epoch 4 step 1103: training loss: 77.18205498450075\n",
      "Epoch 4 step 1104: training accuarcy: 0.9985\n",
      "Epoch 4 step 1104: training loss: 75.41745767019303\n",
      "Epoch 4 step 1105: training accuarcy: 0.999\n",
      "Epoch 4 step 1105: training loss: 80.88067658303926\n",
      "Epoch 4 step 1106: training accuarcy: 0.9975\n",
      "Epoch 4 step 1106: training loss: 74.68933598513992\n",
      "Epoch 4 step 1107: training accuarcy: 0.998\n",
      "Epoch 4 step 1107: training loss: 75.42839138438396\n",
      "Epoch 4 step 1108: training accuarcy: 0.9985\n",
      "Epoch 4 step 1108: training loss: 77.90265254381065\n",
      "Epoch 4 step 1109: training accuarcy: 0.998\n",
      "Epoch 4 step 1109: training loss: 86.43656605204868\n",
      "Epoch 4 step 1110: training accuarcy: 0.9985\n",
      "Epoch 4 step 1110: training loss: 74.88820340855224\n",
      "Epoch 4 step 1111: training accuarcy: 0.9995\n",
      "Epoch 4 step 1111: training loss: 87.06223585803846\n",
      "Epoch 4 step 1112: training accuarcy: 0.997\n",
      "Epoch 4 step 1112: training loss: 76.8849028771881\n",
      "Epoch 4 step 1113: training accuarcy: 0.9995\n",
      "Epoch 4 step 1113: training loss: 72.36346236448739\n",
      "Epoch 4 step 1114: training accuarcy: 0.9995\n",
      "Epoch 4 step 1114: training loss: 80.62269006058051\n",
      "Epoch 4 step 1115: training accuarcy: 0.998\n",
      "Epoch 4 step 1115: training loss: 70.25168861758571\n",
      "Epoch 4 step 1116: training accuarcy: 0.9995\n",
      "Epoch 4 step 1116: training loss: 74.1714648097099\n",
      "Epoch 4 step 1117: training accuarcy: 0.9975\n",
      "Epoch 4 step 1117: training loss: 75.86086702230659\n",
      "Epoch 4 step 1118: training accuarcy: 0.9985\n",
      "Epoch 4 step 1118: training loss: 86.71409460659861\n",
      "Epoch 4 step 1119: training accuarcy: 0.9995\n",
      "Epoch 4 step 1119: training loss: 76.42769574539199\n",
      "Epoch 4 step 1120: training accuarcy: 0.999\n",
      "Epoch 4 step 1120: training loss: 76.84131998861464\n",
      "Epoch 4 step 1121: training accuarcy: 0.9995\n",
      "Epoch 4 step 1121: training loss: 77.60256474565645\n",
      "Epoch 4 step 1122: training accuarcy: 0.9985\n",
      "Epoch 4 step 1122: training loss: 77.26064551141025\n",
      "Epoch 4 step 1123: training accuarcy: 0.997\n",
      "Epoch 4 step 1123: training loss: 76.39179515290724\n",
      "Epoch 4 step 1124: training accuarcy: 0.9985\n",
      "Epoch 4 step 1124: training loss: 76.54173377478756\n",
      "Epoch 4 step 1125: training accuarcy: 0.997\n",
      "Epoch 4 step 1125: training loss: 75.49416305843161\n",
      "Epoch 4 step 1126: training accuarcy: 0.9995\n",
      "Epoch 4 step 1126: training loss: 78.65499496375233\n",
      "Epoch 4 step 1127: training accuarcy: 0.9995\n",
      "Epoch 4 step 1127: training loss: 84.12822385411572\n",
      "Epoch 4 step 1128: training accuarcy: 0.9975\n",
      "Epoch 4 step 1128: training loss: 77.0168765197185\n",
      "Epoch 4 step 1129: training accuarcy: 0.999\n",
      "Epoch 4 step 1129: training loss: 73.98567157248104\n",
      "Epoch 4 step 1130: training accuarcy: 0.999\n",
      "Epoch 4 step 1130: training loss: 81.3651001509065\n",
      "Epoch 4 step 1131: training accuarcy: 0.999\n",
      "Epoch 4 step 1131: training loss: 76.6877487235561\n",
      "Epoch 4 step 1132: training accuarcy: 0.9995\n",
      "Epoch 4 step 1132: training loss: 67.40959813611363\n",
      "Epoch 4 step 1133: training accuarcy: 0.9995\n",
      "Epoch 4 step 1133: training loss: 73.55299011220816\n",
      "Epoch 4 step 1134: training accuarcy: 0.997\n",
      "Epoch 4 step 1134: training loss: 77.34482435362139\n",
      "Epoch 4 step 1135: training accuarcy: 0.996\n",
      "Epoch 4 step 1135: training loss: 78.11728498905478\n",
      "Epoch 4 step 1136: training accuarcy: 0.999\n",
      "Epoch 4 step 1136: training loss: 79.66438596296271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1137: training accuarcy: 0.998\n",
      "Epoch 4 step 1137: training loss: 71.87503486655606\n",
      "Epoch 4 step 1138: training accuarcy: 0.998\n",
      "Epoch 4 step 1138: training loss: 78.7090981171257\n",
      "Epoch 4 step 1139: training accuarcy: 0.998\n",
      "Epoch 4 step 1139: training loss: 73.35889559097228\n",
      "Epoch 4 step 1140: training accuarcy: 0.9995\n",
      "Epoch 4 step 1140: training loss: 79.72226704577253\n",
      "Epoch 4 step 1141: training accuarcy: 0.999\n",
      "Epoch 4 step 1141: training loss: 79.29290496878298\n",
      "Epoch 4 step 1142: training accuarcy: 0.998\n",
      "Epoch 4 step 1142: training loss: 80.26239862485535\n",
      "Epoch 4 step 1143: training accuarcy: 0.997\n",
      "Epoch 4 step 1143: training loss: 74.29393184031116\n",
      "Epoch 4 step 1144: training accuarcy: 0.9985\n",
      "Epoch 4 step 1144: training loss: 73.01828405875892\n",
      "Epoch 4 step 1145: training accuarcy: 0.998\n",
      "Epoch 4 step 1145: training loss: 74.93337061686027\n",
      "Epoch 4 step 1146: training accuarcy: 0.9975\n",
      "Epoch 4 step 1146: training loss: 80.15301326594172\n",
      "Epoch 4 step 1147: training accuarcy: 0.9985\n",
      "Epoch 4 step 1147: training loss: 74.71024662776254\n",
      "Epoch 4 step 1148: training accuarcy: 0.9985\n",
      "Epoch 4 step 1148: training loss: 71.67865802759923\n",
      "Epoch 4 step 1149: training accuarcy: 0.999\n",
      "Epoch 4 step 1149: training loss: 80.4805218899441\n",
      "Epoch 4 step 1150: training accuarcy: 0.999\n",
      "Epoch 4 step 1150: training loss: 75.60726071062219\n",
      "Epoch 4 step 1151: training accuarcy: 0.998\n",
      "Epoch 4 step 1151: training loss: 76.73622823621749\n",
      "Epoch 4 step 1152: training accuarcy: 0.9985\n",
      "Epoch 4 step 1152: training loss: 73.98639986676295\n",
      "Epoch 4 step 1153: training accuarcy: 0.999\n",
      "Epoch 4 step 1153: training loss: 79.57777616292009\n",
      "Epoch 4 step 1154: training accuarcy: 0.998\n",
      "Epoch 4 step 1154: training loss: 83.11629064376359\n",
      "Epoch 4 step 1155: training accuarcy: 0.999\n",
      "Epoch 4 step 1155: training loss: 77.03596377489984\n",
      "Epoch 4 step 1156: training accuarcy: 0.999\n",
      "Epoch 4 step 1156: training loss: 76.22148038621789\n",
      "Epoch 4 step 1157: training accuarcy: 0.998\n",
      "Epoch 4 step 1157: training loss: 75.93928640914422\n",
      "Epoch 4 step 1158: training accuarcy: 0.998\n",
      "Epoch 4 step 1158: training loss: 80.65793945395406\n",
      "Epoch 4 step 1159: training accuarcy: 0.997\n",
      "Epoch 4 step 1159: training loss: 80.42242201853111\n",
      "Epoch 4 step 1160: training accuarcy: 0.997\n",
      "Epoch 4 step 1160: training loss: 85.87517284759656\n",
      "Epoch 4 step 1161: training accuarcy: 0.9965\n",
      "Epoch 4 step 1161: training loss: 80.56040512888913\n",
      "Epoch 4 step 1162: training accuarcy: 0.998\n",
      "Epoch 4 step 1162: training loss: 80.14631848001677\n",
      "Epoch 4 step 1163: training accuarcy: 0.998\n",
      "Epoch 4 step 1163: training loss: 80.91203620897066\n",
      "Epoch 4 step 1164: training accuarcy: 0.998\n",
      "Epoch 4 step 1164: training loss: 76.20710090158137\n",
      "Epoch 4 step 1165: training accuarcy: 0.999\n",
      "Epoch 4 step 1165: training loss: 77.89247391436378\n",
      "Epoch 4 step 1166: training accuarcy: 0.9985\n",
      "Epoch 4 step 1166: training loss: 78.94892490123016\n",
      "Epoch 4 step 1167: training accuarcy: 0.998\n",
      "Epoch 4 step 1167: training loss: 75.01964777278351\n",
      "Epoch 4 step 1168: training accuarcy: 0.9975\n",
      "Epoch 4 step 1168: training loss: 76.8791034008829\n",
      "Epoch 4 step 1169: training accuarcy: 0.999\n",
      "Epoch 4 step 1169: training loss: 76.28134736211408\n",
      "Epoch 4 step 1170: training accuarcy: 0.999\n",
      "Epoch 4 step 1170: training loss: 73.2025511711528\n",
      "Epoch 4 step 1171: training accuarcy: 0.9985\n",
      "Epoch 4 step 1171: training loss: 73.00523246724472\n",
      "Epoch 4 step 1172: training accuarcy: 0.999\n",
      "Epoch 4 step 1172: training loss: 77.00540884256043\n",
      "Epoch 4 step 1173: training accuarcy: 0.9985\n",
      "Epoch 4 step 1173: training loss: 76.16013116501506\n",
      "Epoch 4 step 1174: training accuarcy: 0.998\n",
      "Epoch 4 step 1174: training loss: 81.44536949511131\n",
      "Epoch 4 step 1175: training accuarcy: 0.9985\n",
      "Epoch 4 step 1175: training loss: 78.23212111555986\n",
      "Epoch 4 step 1176: training accuarcy: 0.999\n",
      "Epoch 4 step 1176: training loss: 74.26028373661939\n",
      "Epoch 4 step 1177: training accuarcy: 0.999\n",
      "Epoch 4 step 1177: training loss: 81.092244910531\n",
      "Epoch 4 step 1178: training accuarcy: 0.998\n",
      "Epoch 4 step 1178: training loss: 78.28787633394865\n",
      "Epoch 4 step 1179: training accuarcy: 0.999\n",
      "Epoch 4 step 1179: training loss: 74.96572648172014\n",
      "Epoch 4 step 1180: training accuarcy: 0.9995\n",
      "Epoch 4 step 1180: training loss: 78.91410447277066\n",
      "Epoch 4 step 1181: training accuarcy: 0.998\n",
      "Epoch 4 step 1181: training loss: 71.3501275253765\n",
      "Epoch 4 step 1182: training accuarcy: 0.9995\n",
      "Epoch 4 step 1182: training loss: 76.85783991175131\n",
      "Epoch 4 step 1183: training accuarcy: 0.999\n",
      "Epoch 4 step 1183: training loss: 72.69488570099271\n",
      "Epoch 4 step 1184: training accuarcy: 0.998\n",
      "Epoch 4 step 1184: training loss: 77.56717259933049\n",
      "Epoch 4 step 1185: training accuarcy: 1.0\n",
      "Epoch 4 step 1185: training loss: 82.07262266945213\n",
      "Epoch 4 step 1186: training accuarcy: 0.9975\n",
      "Epoch 4 step 1186: training loss: 70.54344817568463\n",
      "Epoch 4 step 1187: training accuarcy: 0.9975\n",
      "Epoch 4 step 1187: training loss: 84.93691981865776\n",
      "Epoch 4 step 1188: training accuarcy: 0.999\n",
      "Epoch 4 step 1188: training loss: 77.76179368325461\n",
      "Epoch 4 step 1189: training accuarcy: 0.9975\n",
      "Epoch 4 step 1189: training loss: 71.76134864276938\n",
      "Epoch 4 step 1190: training accuarcy: 0.999\n",
      "Epoch 4 step 1190: training loss: 77.4236833423686\n",
      "Epoch 4 step 1191: training accuarcy: 0.9965\n",
      "Epoch 4 step 1191: training loss: 72.13258206397522\n",
      "Epoch 4 step 1192: training accuarcy: 0.998\n",
      "Epoch 4 step 1192: training loss: 75.17013665922913\n",
      "Epoch 4 step 1193: training accuarcy: 0.9985\n",
      "Epoch 4 step 1193: training loss: 78.90125828407409\n",
      "Epoch 4 step 1194: training accuarcy: 0.998\n",
      "Epoch 4 step 1194: training loss: 76.85656115286014\n",
      "Epoch 4 step 1195: training accuarcy: 0.998\n",
      "Epoch 4 step 1195: training loss: 79.37080771111378\n",
      "Epoch 4 step 1196: training accuarcy: 0.9985\n",
      "Epoch 4 step 1196: training loss: 77.6772072716771\n",
      "Epoch 4 step 1197: training accuarcy: 0.998\n",
      "Epoch 4 step 1197: training loss: 73.91433505424392\n",
      "Epoch 4 step 1198: training accuarcy: 0.999\n",
      "Epoch 4 step 1198: training loss: 66.24220842655458\n",
      "Epoch 4 step 1199: training accuarcy: 0.9985\n",
      "Epoch 4 step 1199: training loss: 80.22831956053427\n",
      "Epoch 4 step 1200: training accuarcy: 0.9975\n",
      "Epoch 4 step 1200: training loss: 84.11728598787298\n",
      "Epoch 4 step 1201: training accuarcy: 0.9985\n",
      "Epoch 4 step 1201: training loss: 87.47501741264608\n",
      "Epoch 4 step 1202: training accuarcy: 0.998\n",
      "Epoch 4 step 1202: training loss: 72.32990045784358\n",
      "Epoch 4 step 1203: training accuarcy: 0.9985\n",
      "Epoch 4 step 1203: training loss: 79.32106143218448\n",
      "Epoch 4 step 1204: training accuarcy: 0.997\n",
      "Epoch 4 step 1204: training loss: 79.45164293843794\n",
      "Epoch 4 step 1205: training accuarcy: 0.9985\n",
      "Epoch 4 step 1205: training loss: 79.18342470119116\n",
      "Epoch 4 step 1206: training accuarcy: 0.9995\n",
      "Epoch 4 step 1206: training loss: 77.95214532361393\n",
      "Epoch 4 step 1207: training accuarcy: 0.9985\n",
      "Epoch 4 step 1207: training loss: 83.39799298372566\n",
      "Epoch 4 step 1208: training accuarcy: 0.997\n",
      "Epoch 4 step 1208: training loss: 79.68783706468312\n",
      "Epoch 4 step 1209: training accuarcy: 0.998\n",
      "Epoch 4 step 1209: training loss: 83.96945264197107\n",
      "Epoch 4 step 1210: training accuarcy: 0.9985\n",
      "Epoch 4 step 1210: training loss: 76.3075050560816\n",
      "Epoch 4 step 1211: training accuarcy: 0.998\n",
      "Epoch 4 step 1211: training loss: 83.88347627103997\n",
      "Epoch 4 step 1212: training accuarcy: 0.9975\n",
      "Epoch 4 step 1212: training loss: 85.21940355775206\n",
      "Epoch 4 step 1213: training accuarcy: 0.9985\n",
      "Epoch 4 step 1213: training loss: 78.4122610318314\n",
      "Epoch 4 step 1214: training accuarcy: 0.9985\n",
      "Epoch 4 step 1214: training loss: 73.71197364864588\n",
      "Epoch 4 step 1215: training accuarcy: 0.9995\n",
      "Epoch 4 step 1215: training loss: 80.90568783179782\n",
      "Epoch 4 step 1216: training accuarcy: 0.9995\n",
      "Epoch 4 step 1216: training loss: 80.35529784945348\n",
      "Epoch 4 step 1217: training accuarcy: 0.9975\n",
      "Epoch 4 step 1217: training loss: 80.00020591481649\n",
      "Epoch 4 step 1218: training accuarcy: 0.999\n",
      "Epoch 4 step 1218: training loss: 79.64757584600449\n",
      "Epoch 4 step 1219: training accuarcy: 0.997\n",
      "Epoch 4 step 1219: training loss: 75.6909642950738\n",
      "Epoch 4 step 1220: training accuarcy: 0.9985\n",
      "Epoch 4 step 1220: training loss: 74.36586554722807\n",
      "Epoch 4 step 1221: training accuarcy: 0.9995\n",
      "Epoch 4 step 1221: training loss: 81.00806901288632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1222: training accuarcy: 0.9995\n",
      "Epoch 4 step 1222: training loss: 74.92359162762563\n",
      "Epoch 4 step 1223: training accuarcy: 0.998\n",
      "Epoch 4 step 1223: training loss: 84.31135476574738\n",
      "Epoch 4 step 1224: training accuarcy: 0.999\n",
      "Epoch 4 step 1224: training loss: 79.9002021190812\n",
      "Epoch 4 step 1225: training accuarcy: 0.9995\n",
      "Epoch 4 step 1225: training loss: 70.33057597386022\n",
      "Epoch 4 step 1226: training accuarcy: 0.998\n",
      "Epoch 4 step 1226: training loss: 78.13646766789287\n",
      "Epoch 4 step 1227: training accuarcy: 0.998\n",
      "Epoch 4 step 1227: training loss: 78.85248120253226\n",
      "Epoch 4 step 1228: training accuarcy: 0.998\n",
      "Epoch 4 step 1228: training loss: 76.18430906708866\n",
      "Epoch 4 step 1229: training accuarcy: 0.9985\n",
      "Epoch 4 step 1229: training loss: 75.4696267718135\n",
      "Epoch 4 step 1230: training accuarcy: 0.9985\n",
      "Epoch 4 step 1230: training loss: 75.86598780391948\n",
      "Epoch 4 step 1231: training accuarcy: 0.999\n",
      "Epoch 4 step 1231: training loss: 86.69235213158778\n",
      "Epoch 4 step 1232: training accuarcy: 0.999\n",
      "Epoch 4 step 1232: training loss: 69.95901816846259\n",
      "Epoch 4 step 1233: training accuarcy: 0.9985\n",
      "Epoch 4 step 1233: training loss: 74.60854312830178\n",
      "Epoch 4 step 1234: training accuarcy: 0.9985\n",
      "Epoch 4 step 1234: training loss: 70.69131393292699\n",
      "Epoch 4 step 1235: training accuarcy: 0.999\n",
      "Epoch 4 step 1235: training loss: 81.97342249743392\n",
      "Epoch 4 step 1236: training accuarcy: 0.9955\n",
      "Epoch 4 step 1236: training loss: 69.1076950837187\n",
      "Epoch 4 step 1237: training accuarcy: 0.999\n",
      "Epoch 4 step 1237: training loss: 73.37438628962894\n",
      "Epoch 4 step 1238: training accuarcy: 0.9985\n",
      "Epoch 4 step 1238: training loss: 76.10790092600229\n",
      "Epoch 4 step 1239: training accuarcy: 0.9975\n",
      "Epoch 4 step 1239: training loss: 79.42532476789756\n",
      "Epoch 4 step 1240: training accuarcy: 0.997\n",
      "Epoch 4 step 1240: training loss: 72.18932038064486\n",
      "Epoch 4 step 1241: training accuarcy: 0.998\n",
      "Epoch 4 step 1241: training loss: 84.72550182528916\n",
      "Epoch 4 step 1242: training accuarcy: 0.9975\n",
      "Epoch 4 step 1242: training loss: 80.23096724985959\n",
      "Epoch 4 step 1243: training accuarcy: 0.9985\n",
      "Epoch 4 step 1243: training loss: 76.26409730941927\n",
      "Epoch 4 step 1244: training accuarcy: 0.9975\n",
      "Epoch 4 step 1244: training loss: 79.50209192558766\n",
      "Epoch 4 step 1245: training accuarcy: 0.997\n",
      "Epoch 4 step 1245: training loss: 89.49180374835032\n",
      "Epoch 4 step 1246: training accuarcy: 0.996\n",
      "Epoch 4 step 1246: training loss: 75.90238981990015\n",
      "Epoch 4 step 1247: training accuarcy: 0.9985\n",
      "Epoch 4 step 1247: training loss: 79.7099171276206\n",
      "Epoch 4 step 1248: training accuarcy: 0.998\n",
      "Epoch 4 step 1248: training loss: 78.26623726724291\n",
      "Epoch 4 step 1249: training accuarcy: 0.999\n",
      "Epoch 4 step 1249: training loss: 82.80710885638172\n",
      "Epoch 4 step 1250: training accuarcy: 0.999\n",
      "Epoch 4 step 1250: training loss: 76.71937226112462\n",
      "Epoch 4 step 1251: training accuarcy: 0.999\n",
      "Epoch 4 step 1251: training loss: 72.17635899297872\n",
      "Epoch 4 step 1252: training accuarcy: 0.999\n",
      "Epoch 4 step 1252: training loss: 82.4264554749617\n",
      "Epoch 4 step 1253: training accuarcy: 0.9985\n",
      "Epoch 4 step 1253: training loss: 77.58622606667427\n",
      "Epoch 4 step 1254: training accuarcy: 1.0\n",
      "Epoch 4 step 1254: training loss: 80.68054641067172\n",
      "Epoch 4 step 1255: training accuarcy: 0.999\n",
      "Epoch 4 step 1255: training loss: 74.87599734816095\n",
      "Epoch 4 step 1256: training accuarcy: 0.997\n",
      "Epoch 4 step 1256: training loss: 86.33655731039676\n",
      "Epoch 4 step 1257: training accuarcy: 0.9975\n",
      "Epoch 4 step 1257: training loss: 77.66719825959348\n",
      "Epoch 4 step 1258: training accuarcy: 0.9995\n",
      "Epoch 4 step 1258: training loss: 76.24053932828524\n",
      "Epoch 4 step 1259: training accuarcy: 0.9995\n",
      "Epoch 4 step 1259: training loss: 89.41955563467454\n",
      "Epoch 4 step 1260: training accuarcy: 0.9955\n",
      "Epoch 4 step 1260: training loss: 75.55444109657645\n",
      "Epoch 4 step 1261: training accuarcy: 0.9995\n",
      "Epoch 4 step 1261: training loss: 77.88835971005376\n",
      "Epoch 4 step 1262: training accuarcy: 1.0\n",
      "Epoch 4 step 1262: training loss: 71.07400416471233\n",
      "Epoch 4 step 1263: training accuarcy: 0.9995\n",
      "Epoch 4 step 1263: training loss: 78.3243974631762\n",
      "Epoch 4 step 1264: training accuarcy: 0.999\n",
      "Epoch 4 step 1264: training loss: 66.10190701259093\n",
      "Epoch 4 step 1265: training accuarcy: 0.9995\n",
      "Epoch 4 step 1265: training loss: 81.80548105589975\n",
      "Epoch 4 step 1266: training accuarcy: 0.9985\n",
      "Epoch 4 step 1266: training loss: 73.38390215001021\n",
      "Epoch 4 step 1267: training accuarcy: 0.999\n",
      "Epoch 4 step 1267: training loss: 74.29979255624694\n",
      "Epoch 4 step 1268: training accuarcy: 0.999\n",
      "Epoch 4 step 1268: training loss: 73.45929292223558\n",
      "Epoch 4 step 1269: training accuarcy: 0.999\n",
      "Epoch 4 step 1269: training loss: 78.63128096970344\n",
      "Epoch 4 step 1270: training accuarcy: 0.998\n",
      "Epoch 4 step 1270: training loss: 72.65620228317393\n",
      "Epoch 4 step 1271: training accuarcy: 1.0\n",
      "Epoch 4 step 1271: training loss: 78.33526580065038\n",
      "Epoch 4 step 1272: training accuarcy: 0.998\n",
      "Epoch 4 step 1272: training loss: 67.87578007504085\n",
      "Epoch 4 step 1273: training accuarcy: 0.9985\n",
      "Epoch 4 step 1273: training loss: 74.81348754816833\n",
      "Epoch 4 step 1274: training accuarcy: 0.9985\n",
      "Epoch 4 step 1274: training loss: 79.16751846170695\n",
      "Epoch 4 step 1275: training accuarcy: 0.999\n",
      "Epoch 4 step 1275: training loss: 70.48488912705068\n",
      "Epoch 4 step 1276: training accuarcy: 0.998\n",
      "Epoch 4 step 1276: training loss: 79.56818911730383\n",
      "Epoch 4 step 1277: training accuarcy: 0.997\n",
      "Epoch 4 step 1277: training loss: 77.49545376149624\n",
      "Epoch 4 step 1278: training accuarcy: 0.999\n",
      "Epoch 4 step 1278: training loss: 76.81811828088172\n",
      "Epoch 4 step 1279: training accuarcy: 0.998\n",
      "Epoch 4 step 1279: training loss: 89.27427253102627\n",
      "Epoch 4 step 1280: training accuarcy: 0.9955\n",
      "Epoch 4 step 1280: training loss: 82.16561529090718\n",
      "Epoch 4 step 1281: training accuarcy: 0.9995\n",
      "Epoch 4 step 1281: training loss: 79.29108610732565\n",
      "Epoch 4 step 1282: training accuarcy: 0.999\n",
      "Epoch 4 step 1282: training loss: 72.19404288644114\n",
      "Epoch 4 step 1283: training accuarcy: 0.998\n",
      "Epoch 4 step 1283: training loss: 80.55957045625613\n",
      "Epoch 4 step 1284: training accuarcy: 0.997\n",
      "Epoch 4 step 1284: training loss: 78.66806432740759\n",
      "Epoch 4 step 1285: training accuarcy: 0.999\n",
      "Epoch 4 step 1285: training loss: 73.76381994343643\n",
      "Epoch 4 step 1286: training accuarcy: 0.9995\n",
      "Epoch 4 step 1286: training loss: 70.52787112167236\n",
      "Epoch 4 step 1287: training accuarcy: 0.9995\n",
      "Epoch 4 step 1287: training loss: 75.72717645614034\n",
      "Epoch 4 step 1288: training accuarcy: 0.9975\n",
      "Epoch 4 step 1288: training loss: 78.91922297799852\n",
      "Epoch 4 step 1289: training accuarcy: 0.999\n",
      "Epoch 4 step 1289: training loss: 70.49080166289531\n",
      "Epoch 4 step 1290: training accuarcy: 0.9995\n",
      "Epoch 4 step 1290: training loss: 81.47135246520835\n",
      "Epoch 4 step 1291: training accuarcy: 0.9975\n",
      "Epoch 4 step 1291: training loss: 83.39132777610024\n",
      "Epoch 4 step 1292: training accuarcy: 0.998\n",
      "Epoch 4 step 1292: training loss: 79.4286295393494\n",
      "Epoch 4 step 1293: training accuarcy: 0.999\n",
      "Epoch 4 step 1293: training loss: 78.07321355378622\n",
      "Epoch 4 step 1294: training accuarcy: 0.999\n",
      "Epoch 4 step 1294: training loss: 77.20882386946731\n",
      "Epoch 4 step 1295: training accuarcy: 0.9985\n",
      "Epoch 4 step 1295: training loss: 73.59423890825443\n",
      "Epoch 4 step 1296: training accuarcy: 0.999\n",
      "Epoch 4 step 1296: training loss: 76.81262774040985\n",
      "Epoch 4 step 1297: training accuarcy: 0.9995\n",
      "Epoch 4 step 1297: training loss: 81.73246604522552\n",
      "Epoch 4 step 1298: training accuarcy: 0.999\n",
      "Epoch 4 step 1298: training loss: 79.35898713047808\n",
      "Epoch 4 step 1299: training accuarcy: 0.9985\n",
      "Epoch 4 step 1299: training loss: 77.91123635344968\n",
      "Epoch 4 step 1300: training accuarcy: 0.998\n",
      "Epoch 4 step 1300: training loss: 76.71224747516636\n",
      "Epoch 4 step 1301: training accuarcy: 0.9995\n",
      "Epoch 4 step 1301: training loss: 78.18054796192098\n",
      "Epoch 4 step 1302: training accuarcy: 0.9995\n",
      "Epoch 4 step 1302: training loss: 76.67539032478837\n",
      "Epoch 4 step 1303: training accuarcy: 0.999\n",
      "Epoch 4 step 1303: training loss: 85.97953801401735\n",
      "Epoch 4 step 1304: training accuarcy: 0.9975\n",
      "Epoch 4 step 1304: training loss: 70.41446688513707\n",
      "Epoch 4 step 1305: training accuarcy: 0.999\n",
      "Epoch 4 step 1305: training loss: 75.14939589669017\n",
      "Epoch 4 step 1306: training accuarcy: 0.9995\n",
      "Epoch 4 step 1306: training loss: 77.65937029239967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1307: training accuarcy: 0.998\n",
      "Epoch 4 step 1307: training loss: 71.39991421353481\n",
      "Epoch 4 step 1308: training accuarcy: 0.9985\n",
      "Epoch 4 step 1308: training loss: 82.12035057453046\n",
      "Epoch 4 step 1309: training accuarcy: 1.0\n",
      "Epoch 4 step 1309: training loss: 74.98332669669469\n",
      "Epoch 4 step 1310: training accuarcy: 0.9975\n",
      "Epoch 4 step 1310: training loss: 75.28519357555795\n",
      "Epoch 4 step 1311: training accuarcy: 0.9995\n",
      "Epoch 4 step 1311: training loss: 76.36441178879241\n",
      "Epoch 4 step 1312: training accuarcy: 0.9975\n",
      "Epoch 4 step 1312: training loss: 76.68488594109112\n",
      "Epoch 4 step 1313: training accuarcy: 0.999\n",
      "Epoch 4 step 1313: training loss: 75.4159289031391\n",
      "Epoch 4 step 1314: training accuarcy: 0.9975\n",
      "Epoch 4 step 1314: training loss: 57.42034484899322\n",
      "Epoch 4 step 1315: training accuarcy: 0.9974358974358974\n",
      "Epoch 4: train loss 77.51178102284467, train accuarcy 0.9949812889099121\n",
      "Epoch 4: valid loss 352.3283909370639, valid accuarcy 0.9964019060134888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [27:16<00:00, 327.43s/it]\n"
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=5,\n",
    "               log_dir=get_log_dir('seq_topcoder', 'fm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:17:00.484179Z",
     "start_time": "2019-10-07T14:17:00.468206Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ed9dfae1365c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mfm_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fm_model' is not defined"
     ]
    }
   ],
   "source": [
    "del fm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HRM FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:16:27.748374Z",
     "start_time": "2019-10-07T14:16:27.503373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchHrmFM()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_model = TorchHrmFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "hrm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:16:28.170033Z",
     "start_time": "2019-10-07T14:16:28.166046Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(hrm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:16:30.233677Z",
     "start_time": "2019-10-07T14:16:30.191701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x2999048fb70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_learner = FMLearner(hrm_model, adam_opt, schedular, db)\n",
    "hrm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:17:13.066245Z",
     "start_time": "2019-10-07T14:17:13.063215Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-07T14:17:14.109Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 42819.92472716793\n",
      "Epoch 0 step 1: training accuarcy: 0.5235\n",
      "Epoch 0 step 1: training loss: 41781.00844523763\n",
      "Epoch 0 step 2: training accuarcy: 0.5135\n",
      "Epoch 0 step 2: training loss: 40377.03383102127\n",
      "Epoch 0 step 3: training accuarcy: 0.5275\n",
      "Epoch 0 step 3: training loss: 39087.03981484522\n",
      "Epoch 0 step 4: training accuarcy: 0.5285\n",
      "Epoch 0 step 4: training loss: 38546.408552017965\n",
      "Epoch 0 step 5: training accuarcy: 0.514\n",
      "Epoch 0 step 5: training loss: 37357.36722897061\n",
      "Epoch 0 step 6: training accuarcy: 0.5195\n",
      "Epoch 0 step 6: training loss: 35973.60338429364\n",
      "Epoch 0 step 7: training accuarcy: 0.522\n",
      "Epoch 0 step 7: training loss: 34682.32533080768\n",
      "Epoch 0 step 8: training accuarcy: 0.5485\n",
      "Epoch 0 step 8: training loss: 34184.93159070983\n",
      "Epoch 0 step 9: training accuarcy: 0.526\n",
      "Epoch 0 step 9: training loss: 33033.03608529885\n",
      "Epoch 0 step 10: training accuarcy: 0.5425\n",
      "Epoch 0 step 10: training loss: 31927.703055961145\n",
      "Epoch 0 step 11: training accuarcy: 0.5355\n",
      "Epoch 0 step 11: training loss: 30707.953479796415\n",
      "Epoch 0 step 12: training accuarcy: 0.55\n",
      "Epoch 0 step 12: training loss: 30275.50379619774\n",
      "Epoch 0 step 13: training accuarcy: 0.5285\n",
      "Epoch 0 step 13: training loss: 29469.12641635641\n",
      "Epoch 0 step 14: training accuarcy: 0.5365\n",
      "Epoch 0 step 14: training loss: 28309.9726656483\n",
      "Epoch 0 step 15: training accuarcy: 0.5225\n",
      "Epoch 0 step 15: training loss: 27703.159828816235\n",
      "Epoch 0 step 16: training accuarcy: 0.5385\n",
      "Epoch 0 step 16: training loss: 26693.17676709884\n",
      "Epoch 0 step 17: training accuarcy: 0.535\n",
      "Epoch 0 step 17: training loss: 25872.633638910396\n",
      "Epoch 0 step 18: training accuarcy: 0.547\n",
      "Epoch 0 step 18: training loss: 25284.202607066552\n",
      "Epoch 0 step 19: training accuarcy: 0.513\n",
      "Epoch 0 step 19: training loss: 24189.162136559586\n",
      "Epoch 0 step 20: training accuarcy: 0.5395\n",
      "Epoch 0 step 20: training loss: 23478.20472823497\n",
      "Epoch 0 step 21: training accuarcy: 0.529\n",
      "Epoch 0 step 21: training loss: 23062.12158954146\n",
      "Epoch 0 step 22: training accuarcy: 0.5245\n",
      "Epoch 0 step 22: training loss: 22146.401019713623\n",
      "Epoch 0 step 23: training accuarcy: 0.5295\n",
      "Epoch 0 step 23: training loss: 21457.155690837128\n",
      "Epoch 0 step 24: training accuarcy: 0.5315\n",
      "Epoch 0 step 24: training loss: 20660.60080937981\n",
      "Epoch 0 step 25: training accuarcy: 0.5525\n",
      "Epoch 0 step 25: training loss: 20046.693406104838\n",
      "Epoch 0 step 26: training accuarcy: 0.5335\n",
      "Epoch 0 step 26: training loss: 19347.189142818435\n",
      "Epoch 0 step 27: training accuarcy: 0.5515\n",
      "Epoch 0 step 27: training loss: 18636.562831236948\n",
      "Epoch 0 step 28: training accuarcy: 0.549\n",
      "Epoch 0 step 28: training loss: 18340.422454482385\n",
      "Epoch 0 step 29: training accuarcy: 0.525\n",
      "Epoch 0 step 29: training loss: 17657.16375660908\n",
      "Epoch 0 step 30: training accuarcy: 0.5575\n",
      "Epoch 0 step 30: training loss: 17152.079382298016\n",
      "Epoch 0 step 31: training accuarcy: 0.5405\n",
      "Epoch 0 step 31: training loss: 16576.736708337812\n",
      "Epoch 0 step 32: training accuarcy: 0.5355\n",
      "Epoch 0 step 32: training loss: 16257.091596750808\n",
      "Epoch 0 step 33: training accuarcy: 0.5225\n",
      "Epoch 0 step 33: training loss: 15521.320122971183\n",
      "Epoch 0 step 34: training accuarcy: 0.5700000000000001\n",
      "Epoch 0 step 34: training loss: 15154.470661849606\n",
      "Epoch 0 step 35: training accuarcy: 0.536\n",
      "Epoch 0 step 35: training loss: 14607.728831212682\n",
      "Epoch 0 step 36: training accuarcy: 0.534\n",
      "Epoch 0 step 36: training loss: 14358.833056694537\n",
      "Epoch 0 step 37: training accuarcy: 0.53\n",
      "Epoch 0 step 37: training loss: 13666.411656732957\n",
      "Epoch 0 step 38: training accuarcy: 0.529\n",
      "Epoch 0 step 38: training loss: 13135.42668571689\n",
      "Epoch 0 step 39: training accuarcy: 0.542\n",
      "Epoch 0 step 39: training loss: 12715.828148321756\n",
      "Epoch 0 step 40: training accuarcy: 0.5630000000000001\n",
      "Epoch 0 step 40: training loss: 12514.34385054798\n",
      "Epoch 0 step 41: training accuarcy: 0.534\n",
      "Epoch 0 step 41: training loss: 12068.5141782822\n",
      "Epoch 0 step 42: training accuarcy: 0.529\n",
      "Epoch 0 step 42: training loss: 11640.635893682127\n",
      "Epoch 0 step 43: training accuarcy: 0.5515\n",
      "Epoch 0 step 43: training loss: 11225.025668433133\n",
      "Epoch 0 step 44: training accuarcy: 0.5465\n",
      "Epoch 0 step 44: training loss: 10925.330465479909\n",
      "Epoch 0 step 45: training accuarcy: 0.545\n",
      "Epoch 0 step 45: training loss: 10555.337586942387\n",
      "Epoch 0 step 46: training accuarcy: 0.5235\n",
      "Epoch 0 step 46: training loss: 10293.335866232208\n",
      "Epoch 0 step 47: training accuarcy: 0.5495\n",
      "Epoch 0 step 47: training loss: 9918.443017683612\n",
      "Epoch 0 step 48: training accuarcy: 0.54\n",
      "Epoch 0 step 48: training loss: 9624.44026028649\n",
      "Epoch 0 step 49: training accuarcy: 0.542\n",
      "Epoch 0 step 49: training loss: 9323.344801420037\n",
      "Epoch 0 step 50: training accuarcy: 0.537\n",
      "Epoch 0 step 50: training loss: 9066.721836276893\n",
      "Epoch 0 step 51: training accuarcy: 0.535\n",
      "Epoch 0 step 51: training loss: 8750.727112410661\n",
      "Epoch 0 step 52: training accuarcy: 0.5465\n",
      "Epoch 0 step 52: training loss: 8391.328572834012\n",
      "Epoch 0 step 53: training accuarcy: 0.5435\n",
      "Epoch 0 step 53: training loss: 8141.795891567686\n",
      "Epoch 0 step 54: training accuarcy: 0.5635\n",
      "Epoch 0 step 54: training loss: 7901.044112642197\n",
      "Epoch 0 step 55: training accuarcy: 0.556\n",
      "Epoch 0 step 55: training loss: 7689.463979909122\n",
      "Epoch 0 step 56: training accuarcy: 0.5755\n",
      "Epoch 0 step 56: training loss: 7480.008285906117\n",
      "Epoch 0 step 57: training accuarcy: 0.55\n",
      "Epoch 0 step 57: training loss: 7252.517532566334\n",
      "Epoch 0 step 58: training accuarcy: 0.545\n",
      "Epoch 0 step 58: training loss: 7043.9270737351435\n",
      "Epoch 0 step 59: training accuarcy: 0.5595\n",
      "Epoch 0 step 59: training loss: 6816.854897414677\n",
      "Epoch 0 step 60: training accuarcy: 0.5495\n",
      "Epoch 0 step 60: training loss: 6573.677158578592\n",
      "Epoch 0 step 61: training accuarcy: 0.5405\n",
      "Epoch 0 step 61: training loss: 6423.033719802928\n",
      "Epoch 0 step 62: training accuarcy: 0.561\n",
      "Epoch 0 step 62: training loss: 6251.303450314897\n",
      "Epoch 0 step 63: training accuarcy: 0.534\n",
      "Epoch 0 step 63: training loss: 6018.553498023742\n",
      "Epoch 0 step 64: training accuarcy: 0.5700000000000001\n",
      "Epoch 0 step 64: training loss: 5854.75142625521\n",
      "Epoch 0 step 65: training accuarcy: 0.5595\n",
      "Epoch 0 step 65: training loss: 5707.709232636929\n",
      "Epoch 0 step 66: training accuarcy: 0.557\n",
      "Epoch 0 step 66: training loss: 5606.386966646557\n",
      "Epoch 0 step 67: training accuarcy: 0.541\n",
      "Epoch 0 step 67: training loss: 5399.956480408718\n",
      "Epoch 0 step 68: training accuarcy: 0.5485\n",
      "Epoch 0 step 68: training loss: 5242.486305270679\n",
      "Epoch 0 step 69: training accuarcy: 0.5505\n",
      "Epoch 0 step 69: training loss: 5101.4156628384335\n",
      "Epoch 0 step 70: training accuarcy: 0.552\n",
      "Epoch 0 step 70: training loss: 4987.807919675456\n",
      "Epoch 0 step 71: training accuarcy: 0.552\n",
      "Epoch 0 step 71: training loss: 4855.717661448178\n",
      "Epoch 0 step 72: training accuarcy: 0.5545\n",
      "Epoch 0 step 72: training loss: 4675.3886802006555\n",
      "Epoch 0 step 73: training accuarcy: 0.5755\n",
      "Epoch 0 step 73: training loss: 4568.171062043028\n",
      "Epoch 0 step 74: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 74: training loss: 4432.609727628548\n",
      "Epoch 0 step 75: training accuarcy: 0.5715\n",
      "Epoch 0 step 75: training loss: 4367.542369394859\n",
      "Epoch 0 step 76: training accuarcy: 0.547\n",
      "Epoch 0 step 76: training loss: 4227.354094179587\n",
      "Epoch 0 step 77: training accuarcy: 0.5740000000000001\n",
      "Epoch 0 step 77: training loss: 4118.208454990849\n",
      "Epoch 0 step 78: training accuarcy: 0.5710000000000001\n",
      "Epoch 0 step 78: training loss: 3990.0084089693696\n",
      "Epoch 0 step 79: training accuarcy: 0.579\n",
      "Epoch 0 step 79: training loss: 3920.059867622048\n",
      "Epoch 0 step 80: training accuarcy: 0.5565\n",
      "Epoch 0 step 80: training loss: 3834.9264173268684\n",
      "Epoch 0 step 81: training accuarcy: 0.547\n",
      "Epoch 0 step 81: training loss: 3755.3038992756397\n",
      "Epoch 0 step 82: training accuarcy: 0.545\n",
      "Epoch 0 step 82: training loss: 3617.315046398444\n",
      "Epoch 0 step 83: training accuarcy: 0.5745\n",
      "Epoch 0 step 83: training loss: 3573.136930666361\n",
      "Epoch 0 step 84: training accuarcy: 0.562\n",
      "Epoch 0 step 84: training loss: 3460.818895300476\n",
      "Epoch 0 step 85: training accuarcy: 0.5730000000000001\n",
      "Epoch 0 step 85: training loss: 3376.938082857633\n",
      "Epoch 0 step 86: training accuarcy: 0.577\n",
      "Epoch 0 step 86: training loss: 3286.2639214453548\n",
      "Epoch 0 step 87: training accuarcy: 0.5865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 87: training loss: 3231.609322376661\n",
      "Epoch 0 step 88: training accuarcy: 0.5775\n",
      "Epoch 0 step 88: training loss: 3177.266716641235\n",
      "Epoch 0 step 89: training accuarcy: 0.5755\n",
      "Epoch 0 step 89: training loss: 3096.6982001874267\n",
      "Epoch 0 step 90: training accuarcy: 0.5745\n",
      "Epoch 0 step 90: training loss: 3027.00633871001\n",
      "Epoch 0 step 91: training accuarcy: 0.581\n",
      "Epoch 0 step 91: training loss: 2989.0730920266324\n",
      "Epoch 0 step 92: training accuarcy: 0.5505\n",
      "Epoch 0 step 92: training loss: 2910.784283710388\n",
      "Epoch 0 step 93: training accuarcy: 0.584\n",
      "Epoch 0 step 93: training loss: 2863.0436492025647\n",
      "Epoch 0 step 94: training accuarcy: 0.5700000000000001\n",
      "Epoch 0 step 94: training loss: 2778.609765932709\n",
      "Epoch 0 step 95: training accuarcy: 0.5945\n",
      "Epoch 0 step 95: training loss: 2734.366319276499\n",
      "Epoch 0 step 96: training accuarcy: 0.5925\n",
      "Epoch 0 step 96: training loss: 2693.315033934541\n",
      "Epoch 0 step 97: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 97: training loss: 2631.619137117235\n",
      "Epoch 0 step 98: training accuarcy: 0.5885\n",
      "Epoch 0 step 98: training loss: 2587.7151117368253\n",
      "Epoch 0 step 99: training accuarcy: 0.579\n",
      "Epoch 0 step 99: training loss: 2544.514426906918\n",
      "Epoch 0 step 100: training accuarcy: 0.5720000000000001\n",
      "Epoch 0 step 100: training loss: 2493.308996599022\n",
      "Epoch 0 step 101: training accuarcy: 0.5895\n",
      "Epoch 0 step 101: training loss: 2442.960873265255\n",
      "Epoch 0 step 102: training accuarcy: 0.5915\n",
      "Epoch 0 step 102: training loss: 2411.0783780671973\n",
      "Epoch 0 step 103: training accuarcy: 0.581\n",
      "Epoch 0 step 103: training loss: 2369.8509351627204\n",
      "Epoch 0 step 104: training accuarcy: 0.5875\n",
      "Epoch 0 step 104: training loss: 2320.0353572307376\n",
      "Epoch 0 step 105: training accuarcy: 0.5985\n",
      "Epoch 0 step 105: training loss: 2303.443535810332\n",
      "Epoch 0 step 106: training accuarcy: 0.5765\n",
      "Epoch 0 step 106: training loss: 2250.5079073939964\n",
      "Epoch 0 step 107: training accuarcy: 0.5895\n",
      "Epoch 0 step 107: training loss: 2216.011925849828\n",
      "Epoch 0 step 108: training accuarcy: 0.592\n",
      "Epoch 0 step 108: training loss: 2181.5437171110443\n",
      "Epoch 0 step 109: training accuarcy: 0.5865\n",
      "Epoch 0 step 109: training loss: 2146.000571243675\n",
      "Epoch 0 step 110: training accuarcy: 0.601\n",
      "Epoch 0 step 110: training loss: 2139.2687655390055\n",
      "Epoch 0 step 111: training accuarcy: 0.587\n",
      "Epoch 0 step 111: training loss: 2114.402177749118\n",
      "Epoch 0 step 112: training accuarcy: 0.59\n",
      "Epoch 0 step 112: training loss: 2070.311766260923\n",
      "Epoch 0 step 113: training accuarcy: 0.599\n",
      "Epoch 0 step 113: training loss: 2052.884727639058\n",
      "Epoch 0 step 114: training accuarcy: 0.592\n",
      "Epoch 0 step 114: training loss: 2015.5659195557469\n",
      "Epoch 0 step 115: training accuarcy: 0.593\n",
      "Epoch 0 step 115: training loss: 1989.0294980758006\n",
      "Epoch 0 step 116: training accuarcy: 0.609\n",
      "Epoch 0 step 116: training loss: 1959.0945942168253\n",
      "Epoch 0 step 117: training accuarcy: 0.593\n",
      "Epoch 0 step 117: training loss: 1949.3539338495007\n",
      "Epoch 0 step 118: training accuarcy: 0.589\n",
      "Epoch 0 step 118: training loss: 1941.4721462248094\n",
      "Epoch 0 step 119: training accuarcy: 0.6085\n",
      "Epoch 0 step 119: training loss: 1890.0959365479143\n",
      "Epoch 0 step 120: training accuarcy: 0.61\n",
      "Epoch 0 step 120: training loss: 1897.7463163650427\n",
      "Epoch 0 step 121: training accuarcy: 0.612\n",
      "Epoch 0 step 121: training loss: 1854.3492648664965\n",
      "Epoch 0 step 122: training accuarcy: 0.608\n",
      "Epoch 0 step 122: training loss: 1851.696395627432\n",
      "Epoch 0 step 123: training accuarcy: 0.595\n",
      "Epoch 0 step 123: training loss: 1817.3417723666412\n",
      "Epoch 0 step 124: training accuarcy: 0.617\n",
      "Epoch 0 step 124: training loss: 1809.3307443118279\n",
      "Epoch 0 step 125: training accuarcy: 0.5855\n",
      "Epoch 0 step 125: training loss: 1783.400294145897\n",
      "Epoch 0 step 126: training accuarcy: 0.604\n",
      "Epoch 0 step 126: training loss: 1767.0878640707467\n",
      "Epoch 0 step 127: training accuarcy: 0.5885\n",
      "Epoch 0 step 127: training loss: 1751.180242106044\n",
      "Epoch 0 step 128: training accuarcy: 0.593\n",
      "Epoch 0 step 128: training loss: 1731.1759102717501\n",
      "Epoch 0 step 129: training accuarcy: 0.6035\n",
      "Epoch 0 step 129: training loss: 1735.384892236195\n",
      "Epoch 0 step 130: training accuarcy: 0.584\n",
      "Epoch 0 step 130: training loss: 1705.8529055364302\n",
      "Epoch 0 step 131: training accuarcy: 0.6135\n",
      "Epoch 0 step 131: training loss: 1708.3006906694527\n",
      "Epoch 0 step 132: training accuarcy: 0.586\n",
      "Epoch 0 step 132: training loss: 1687.3752487321094\n",
      "Epoch 0 step 133: training accuarcy: 0.6005\n",
      "Epoch 0 step 133: training loss: 1671.1125319648752\n",
      "Epoch 0 step 134: training accuarcy: 0.6135\n",
      "Epoch 0 step 134: training loss: 1654.9510039059046\n",
      "Epoch 0 step 135: training accuarcy: 0.6005\n",
      "Epoch 0 step 135: training loss: 1640.3935592006503\n",
      "Epoch 0 step 136: training accuarcy: 0.606\n",
      "Epoch 0 step 136: training loss: 1642.7118293326191\n",
      "Epoch 0 step 137: training accuarcy: 0.6095\n",
      "Epoch 0 step 137: training loss: 1619.7659598419436\n",
      "Epoch 0 step 138: training accuarcy: 0.62\n",
      "Epoch 0 step 138: training loss: 1618.489763088326\n",
      "Epoch 0 step 139: training accuarcy: 0.613\n",
      "Epoch 0 step 139: training loss: 1631.7787278419898\n",
      "Epoch 0 step 140: training accuarcy: 0.6015\n",
      "Epoch 0 step 140: training loss: 1598.9984611881334\n",
      "Epoch 0 step 141: training accuarcy: 0.612\n",
      "Epoch 0 step 141: training loss: 1582.7394008030726\n",
      "Epoch 0 step 142: training accuarcy: 0.609\n",
      "Epoch 0 step 142: training loss: 1580.2836844881517\n",
      "Epoch 0 step 143: training accuarcy: 0.605\n",
      "Epoch 0 step 143: training loss: 1576.6971710076637\n",
      "Epoch 0 step 144: training accuarcy: 0.6015\n",
      "Epoch 0 step 144: training loss: 1570.3234787591284\n",
      "Epoch 0 step 145: training accuarcy: 0.598\n",
      "Epoch 0 step 145: training loss: 1545.3558449549387\n",
      "Epoch 0 step 146: training accuarcy: 0.6135\n",
      "Epoch 0 step 146: training loss: 1548.7109185790969\n",
      "Epoch 0 step 147: training accuarcy: 0.5985\n",
      "Epoch 0 step 147: training loss: 1549.155434493495\n",
      "Epoch 0 step 148: training accuarcy: 0.589\n",
      "Epoch 0 step 148: training loss: 1534.987273698996\n",
      "Epoch 0 step 149: training accuarcy: 0.606\n",
      "Epoch 0 step 149: training loss: 1544.4231185445933\n",
      "Epoch 0 step 150: training accuarcy: 0.589\n",
      "Epoch 0 step 150: training loss: 1525.2549169698852\n",
      "Epoch 0 step 151: training accuarcy: 0.61\n",
      "Epoch 0 step 151: training loss: 1513.0546585593236\n",
      "Epoch 0 step 152: training accuarcy: 0.6025\n",
      "Epoch 0 step 152: training loss: 1518.9982753080265\n",
      "Epoch 0 step 153: training accuarcy: 0.596\n",
      "Epoch 0 step 153: training loss: 1517.402222603925\n",
      "Epoch 0 step 154: training accuarcy: 0.5865\n",
      "Epoch 0 step 154: training loss: 1502.908488683254\n",
      "Epoch 0 step 155: training accuarcy: 0.5945\n",
      "Epoch 0 step 155: training loss: 1510.6858271062151\n",
      "Epoch 0 step 156: training accuarcy: 0.5985\n",
      "Epoch 0 step 156: training loss: 1482.4842430085705\n",
      "Epoch 0 step 157: training accuarcy: 0.612\n",
      "Epoch 0 step 157: training loss: 1514.3359474072854\n",
      "Epoch 0 step 158: training accuarcy: 0.5985\n",
      "Epoch 0 step 158: training loss: 1481.4515390901495\n",
      "Epoch 0 step 159: training accuarcy: 0.6095\n",
      "Epoch 0 step 159: training loss: 1470.5636950664405\n",
      "Epoch 0 step 160: training accuarcy: 0.5935\n",
      "Epoch 0 step 160: training loss: 1468.4837716597578\n",
      "Epoch 0 step 161: training accuarcy: 0.6035\n",
      "Epoch 0 step 161: training loss: 1469.776588394771\n",
      "Epoch 0 step 162: training accuarcy: 0.59\n",
      "Epoch 0 step 162: training loss: 1454.4951642780827\n",
      "Epoch 0 step 163: training accuarcy: 0.617\n",
      "Epoch 0 step 163: training loss: 1459.8499680873774\n",
      "Epoch 0 step 164: training accuarcy: 0.6295000000000001\n",
      "Epoch 0 step 164: training loss: 1449.0100156824212\n",
      "Epoch 0 step 165: training accuarcy: 0.6065\n",
      "Epoch 0 step 165: training loss: 1454.2150378942238\n",
      "Epoch 0 step 166: training accuarcy: 0.606\n",
      "Epoch 0 step 166: training loss: 1445.5166310443444\n",
      "Epoch 0 step 167: training accuarcy: 0.597\n",
      "Epoch 0 step 167: training loss: 1452.5432973202219\n",
      "Epoch 0 step 168: training accuarcy: 0.589\n",
      "Epoch 0 step 168: training loss: 1456.050261752207\n",
      "Epoch 0 step 169: training accuarcy: 0.591\n",
      "Epoch 0 step 169: training loss: 1439.8376883752678\n",
      "Epoch 0 step 170: training accuarcy: 0.6175\n",
      "Epoch 0 step 170: training loss: 1440.6595258100124\n",
      "Epoch 0 step 171: training accuarcy: 0.61\n",
      "Epoch 0 step 171: training loss: 1442.8501007493926\n",
      "Epoch 0 step 172: training accuarcy: 0.598\n",
      "Epoch 0 step 172: training loss: 1425.5438730243227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 173: training accuarcy: 0.622\n",
      "Epoch 0 step 173: training loss: 1414.9737667272586\n",
      "Epoch 0 step 174: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 174: training loss: 1429.33060624277\n",
      "Epoch 0 step 175: training accuarcy: 0.606\n",
      "Epoch 0 step 175: training loss: 1420.7935036779036\n",
      "Epoch 0 step 176: training accuarcy: 0.6115\n",
      "Epoch 0 step 176: training loss: 1415.4114227858433\n",
      "Epoch 0 step 177: training accuarcy: 0.6135\n",
      "Epoch 0 step 177: training loss: 1410.6567030656884\n",
      "Epoch 0 step 178: training accuarcy: 0.625\n",
      "Epoch 0 step 178: training loss: 1425.2908135379957\n",
      "Epoch 0 step 179: training accuarcy: 0.593\n",
      "Epoch 0 step 179: training loss: 1410.8200254771439\n",
      "Epoch 0 step 180: training accuarcy: 0.6095\n",
      "Epoch 0 step 180: training loss: 1412.2608280959723\n",
      "Epoch 0 step 181: training accuarcy: 0.609\n",
      "Epoch 0 step 181: training loss: 1401.468516024609\n",
      "Epoch 0 step 182: training accuarcy: 0.6245\n",
      "Epoch 0 step 182: training loss: 1403.8889403061905\n",
      "Epoch 0 step 183: training accuarcy: 0.6175\n",
      "Epoch 0 step 183: training loss: 1402.622178874247\n",
      "Epoch 0 step 184: training accuarcy: 0.6\n",
      "Epoch 0 step 184: training loss: 1410.8113636870612\n",
      "Epoch 0 step 185: training accuarcy: 0.6065\n",
      "Epoch 0 step 185: training loss: 1396.042846516161\n",
      "Epoch 0 step 186: training accuarcy: 0.622\n",
      "Epoch 0 step 186: training loss: 1394.287910497596\n",
      "Epoch 0 step 187: training accuarcy: 0.6245\n",
      "Epoch 0 step 187: training loss: 1405.5887400701645\n",
      "Epoch 0 step 188: training accuarcy: 0.609\n",
      "Epoch 0 step 188: training loss: 1403.5036644459951\n",
      "Epoch 0 step 189: training accuarcy: 0.6155\n",
      "Epoch 0 step 189: training loss: 1386.6487302162323\n",
      "Epoch 0 step 190: training accuarcy: 0.606\n",
      "Epoch 0 step 190: training loss: 1395.5482690474446\n",
      "Epoch 0 step 191: training accuarcy: 0.607\n",
      "Epoch 0 step 191: training loss: 1386.7850110032155\n",
      "Epoch 0 step 192: training accuarcy: 0.6155\n",
      "Epoch 0 step 192: training loss: 1384.374418487593\n",
      "Epoch 0 step 193: training accuarcy: 0.6185\n",
      "Epoch 0 step 193: training loss: 1387.5417382405867\n",
      "Epoch 0 step 194: training accuarcy: 0.5925\n",
      "Epoch 0 step 194: training loss: 1387.8807385113607\n",
      "Epoch 0 step 195: training accuarcy: 0.607\n",
      "Epoch 0 step 195: training loss: 1385.9016254004873\n",
      "Epoch 0 step 196: training accuarcy: 0.6025\n",
      "Epoch 0 step 196: training loss: 1386.2312965577926\n",
      "Epoch 0 step 197: training accuarcy: 0.604\n",
      "Epoch 0 step 197: training loss: 1374.9335459913768\n",
      "Epoch 0 step 198: training accuarcy: 0.598\n",
      "Epoch 0 step 198: training loss: 1373.096389849129\n",
      "Epoch 0 step 199: training accuarcy: 0.6125\n",
      "Epoch 0 step 199: training loss: 1407.5489009871744\n",
      "Epoch 0 step 200: training accuarcy: 0.6\n",
      "Epoch 0 step 200: training loss: 1382.6174025935923\n",
      "Epoch 0 step 201: training accuarcy: 0.593\n",
      "Epoch 0 step 201: training loss: 1374.4795797452405\n",
      "Epoch 0 step 202: training accuarcy: 0.61\n",
      "Epoch 0 step 202: training loss: 1378.679318187331\n",
      "Epoch 0 step 203: training accuarcy: 0.5935\n",
      "Epoch 0 step 203: training loss: 1382.2935050875599\n",
      "Epoch 0 step 204: training accuarcy: 0.5955\n",
      "Epoch 0 step 204: training loss: 1374.4874702269858\n",
      "Epoch 0 step 205: training accuarcy: 0.596\n",
      "Epoch 0 step 205: training loss: 1385.581603154303\n",
      "Epoch 0 step 206: training accuarcy: 0.5855\n",
      "Epoch 0 step 206: training loss: 1362.671298986375\n",
      "Epoch 0 step 207: training accuarcy: 0.616\n",
      "Epoch 0 step 207: training loss: 1379.9708149362787\n",
      "Epoch 0 step 208: training accuarcy: 0.606\n",
      "Epoch 0 step 208: training loss: 1389.2223588407314\n",
      "Epoch 0 step 209: training accuarcy: 0.5895\n",
      "Epoch 0 step 209: training loss: 1365.8135349992506\n",
      "Epoch 0 step 210: training accuarcy: 0.6065\n",
      "Epoch 0 step 210: training loss: 1377.3151610740558\n",
      "Epoch 0 step 211: training accuarcy: 0.6035\n",
      "Epoch 0 step 211: training loss: 1371.9345824742556\n",
      "Epoch 0 step 212: training accuarcy: 0.6205\n",
      "Epoch 0 step 212: training loss: 1372.901502236462\n",
      "Epoch 0 step 213: training accuarcy: 0.608\n",
      "Epoch 0 step 213: training loss: 1389.3851095425846\n",
      "Epoch 0 step 214: training accuarcy: 0.579\n",
      "Epoch 0 step 214: training loss: 1359.6091288772782\n",
      "Epoch 0 step 215: training accuarcy: 0.6135\n",
      "Epoch 0 step 215: training loss: 1373.2844165683707\n",
      "Epoch 0 step 216: training accuarcy: 0.6015\n",
      "Epoch 0 step 216: training loss: 1364.1441130242592\n",
      "Epoch 0 step 217: training accuarcy: 0.6035\n",
      "Epoch 0 step 217: training loss: 1369.0026610526297\n",
      "Epoch 0 step 218: training accuarcy: 0.6035\n",
      "Epoch 0 step 218: training loss: 1374.4618975826884\n",
      "Epoch 0 step 219: training accuarcy: 0.6175\n",
      "Epoch 0 step 219: training loss: 1360.5396492561997\n",
      "Epoch 0 step 220: training accuarcy: 0.614\n",
      "Epoch 0 step 220: training loss: 1363.5815131813551\n",
      "Epoch 0 step 221: training accuarcy: 0.622\n",
      "Epoch 0 step 221: training loss: 1363.8601540315951\n",
      "Epoch 0 step 222: training accuarcy: 0.631\n",
      "Epoch 0 step 222: training loss: 1371.2286838615078\n",
      "Epoch 0 step 223: training accuarcy: 0.6005\n",
      "Epoch 0 step 223: training loss: 1369.893687459268\n",
      "Epoch 0 step 224: training accuarcy: 0.602\n",
      "Epoch 0 step 224: training loss: 1373.864842617165\n",
      "Epoch 0 step 225: training accuarcy: 0.595\n",
      "Epoch 0 step 225: training loss: 1357.281638321411\n",
      "Epoch 0 step 226: training accuarcy: 0.621\n",
      "Epoch 0 step 226: training loss: 1368.5685058240815\n",
      "Epoch 0 step 227: training accuarcy: 0.6025\n",
      "Epoch 0 step 227: training loss: 1384.4842173584977\n",
      "Epoch 0 step 228: training accuarcy: 0.607\n",
      "Epoch 0 step 228: training loss: 1360.9434552960736\n",
      "Epoch 0 step 229: training accuarcy: 0.6035\n",
      "Epoch 0 step 229: training loss: 1377.9045243983414\n",
      "Epoch 0 step 230: training accuarcy: 0.595\n",
      "Epoch 0 step 230: training loss: 1351.953480661303\n",
      "Epoch 0 step 231: training accuarcy: 0.6255000000000001\n",
      "Epoch 0 step 231: training loss: 1357.5385423258099\n",
      "Epoch 0 step 232: training accuarcy: 0.63\n",
      "Epoch 0 step 232: training loss: 1378.2683273648634\n",
      "Epoch 0 step 233: training accuarcy: 0.5955\n",
      "Epoch 0 step 233: training loss: 1348.4873870785684\n",
      "Epoch 0 step 234: training accuarcy: 0.627\n",
      "Epoch 0 step 234: training loss: 1363.537178632112\n",
      "Epoch 0 step 235: training accuarcy: 0.6095\n",
      "Epoch 0 step 235: training loss: 1365.5194998413715\n",
      "Epoch 0 step 236: training accuarcy: 0.6065\n",
      "Epoch 0 step 236: training loss: 1358.0267987435764\n",
      "Epoch 0 step 237: training accuarcy: 0.6205\n",
      "Epoch 0 step 237: training loss: 1372.774116816407\n",
      "Epoch 0 step 238: training accuarcy: 0.6105\n",
      "Epoch 0 step 238: training loss: 1357.1482032200745\n",
      "Epoch 0 step 239: training accuarcy: 0.6185\n",
      "Epoch 0 step 239: training loss: 1345.723061225397\n",
      "Epoch 0 step 240: training accuarcy: 0.6415\n",
      "Epoch 0 step 240: training loss: 1354.7307084623244\n",
      "Epoch 0 step 241: training accuarcy: 0.613\n",
      "Epoch 0 step 241: training loss: 1362.4888901213326\n",
      "Epoch 0 step 242: training accuarcy: 0.607\n",
      "Epoch 0 step 242: training loss: 1365.4181715813083\n",
      "Epoch 0 step 243: training accuarcy: 0.596\n",
      "Epoch 0 step 243: training loss: 1358.6049906429896\n",
      "Epoch 0 step 244: training accuarcy: 0.621\n",
      "Epoch 0 step 244: training loss: 1365.8582831342492\n",
      "Epoch 0 step 245: training accuarcy: 0.599\n",
      "Epoch 0 step 245: training loss: 1351.2950718353236\n",
      "Epoch 0 step 246: training accuarcy: 0.6105\n",
      "Epoch 0 step 246: training loss: 1358.9826915436436\n",
      "Epoch 0 step 247: training accuarcy: 0.608\n",
      "Epoch 0 step 247: training loss: 1357.0352023735425\n",
      "Epoch 0 step 248: training accuarcy: 0.6175\n",
      "Epoch 0 step 248: training loss: 1356.7571214479715\n",
      "Epoch 0 step 249: training accuarcy: 0.61\n",
      "Epoch 0 step 249: training loss: 1358.1323470640698\n",
      "Epoch 0 step 250: training accuarcy: 0.6185\n",
      "Epoch 0 step 250: training loss: 1355.4255221484252\n",
      "Epoch 0 step 251: training accuarcy: 0.5995\n",
      "Epoch 0 step 251: training loss: 1364.7860844450388\n",
      "Epoch 0 step 252: training accuarcy: 0.604\n",
      "Epoch 0 step 252: training loss: 1348.0845310735856\n",
      "Epoch 0 step 253: training accuarcy: 0.6105\n",
      "Epoch 0 step 253: training loss: 1347.195033694953\n",
      "Epoch 0 step 254: training accuarcy: 0.6255000000000001\n",
      "Epoch 0 step 254: training loss: 1375.0552794086564\n",
      "Epoch 0 step 255: training accuarcy: 0.588\n",
      "Epoch 0 step 255: training loss: 1353.814406613389\n",
      "Epoch 0 step 256: training accuarcy: 0.6215\n",
      "Epoch 0 step 256: training loss: 1340.8924010919638\n",
      "Epoch 0 step 257: training accuarcy: 0.633\n",
      "Epoch 0 step 257: training loss: 1351.4569427198107\n",
      "Epoch 0 step 258: training accuarcy: 0.623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 258: training loss: 1356.371755687768\n",
      "Epoch 0 step 259: training accuarcy: 0.6225\n",
      "Epoch 0 step 259: training loss: 1361.9661363982937\n",
      "Epoch 0 step 260: training accuarcy: 0.604\n",
      "Epoch 0 step 260: training loss: 1357.778049080029\n",
      "Epoch 0 step 261: training accuarcy: 0.6\n",
      "Epoch 0 step 261: training loss: 1354.8313291723446\n",
      "Epoch 0 step 262: training accuarcy: 0.6035\n",
      "Epoch 0 step 262: training loss: 543.1271567781893\n",
      "Epoch 0 step 263: training accuarcy: 0.5935897435897436\n",
      "Epoch 0: train loss 6135.840041508194, train accuarcy 0.5847347378730774\n",
      "Epoch 0: valid loss 6706.5613106402725, valid accuarcy 0.5839095115661621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▍                                                                                                                                         | 1/5 [05:22<21:31, 322.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1350.5640083414346\n",
      "Epoch 1 step 264: training accuarcy: 0.6105\n",
      "Epoch 1 step 264: training loss: 1360.7130935065466\n",
      "Epoch 1 step 265: training accuarcy: 0.597\n",
      "Epoch 1 step 265: training loss: 1347.653572266799\n",
      "Epoch 1 step 266: training accuarcy: 0.625\n",
      "Epoch 1 step 266: training loss: 1356.300089808856\n",
      "Epoch 1 step 267: training accuarcy: 0.6065\n",
      "Epoch 1 step 267: training loss: 1345.5184495979202\n",
      "Epoch 1 step 268: training accuarcy: 0.624\n",
      "Epoch 1 step 268: training loss: 1351.0764297189323\n",
      "Epoch 1 step 269: training accuarcy: 0.6195\n",
      "Epoch 1 step 269: training loss: 1342.4884391028631\n",
      "Epoch 1 step 270: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 270: training loss: 1356.5929926053218\n",
      "Epoch 1 step 271: training accuarcy: 0.602\n",
      "Epoch 1 step 271: training loss: 1350.7376044860046\n",
      "Epoch 1 step 272: training accuarcy: 0.6065\n",
      "Epoch 1 step 272: training loss: 1351.1594359493672\n",
      "Epoch 1 step 273: training accuarcy: 0.6235\n",
      "Epoch 1 step 273: training loss: 1354.215592347017\n",
      "Epoch 1 step 274: training accuarcy: 0.607\n",
      "Epoch 1 step 274: training loss: 1361.2778525635297\n",
      "Epoch 1 step 275: training accuarcy: 0.602\n",
      "Epoch 1 step 275: training loss: 1343.6496523586948\n",
      "Epoch 1 step 276: training accuarcy: 0.6115\n",
      "Epoch 1 step 276: training loss: 1346.2907312764942\n",
      "Epoch 1 step 277: training accuarcy: 0.6225\n",
      "Epoch 1 step 277: training loss: 1344.790745296388\n",
      "Epoch 1 step 278: training accuarcy: 0.613\n",
      "Epoch 1 step 278: training loss: 1349.7769284219012\n",
      "Epoch 1 step 279: training accuarcy: 0.614\n",
      "Epoch 1 step 279: training loss: 1336.6850141409582\n",
      "Epoch 1 step 280: training accuarcy: 0.6215\n",
      "Epoch 1 step 280: training loss: 1332.3051027751412\n",
      "Epoch 1 step 281: training accuarcy: 0.6375000000000001\n",
      "Epoch 1 step 281: training loss: 1351.929342757235\n",
      "Epoch 1 step 282: training accuarcy: 0.618\n",
      "Epoch 1 step 282: training loss: 1364.8745522006939\n",
      "Epoch 1 step 283: training accuarcy: 0.6035\n",
      "Epoch 1 step 283: training loss: 1354.8002444561118\n",
      "Epoch 1 step 284: training accuarcy: 0.611\n",
      "Epoch 1 step 284: training loss: 1342.9571632199663\n",
      "Epoch 1 step 285: training accuarcy: 0.621\n",
      "Epoch 1 step 285: training loss: 1352.786947252234\n",
      "Epoch 1 step 286: training accuarcy: 0.6165\n",
      "Epoch 1 step 286: training loss: 1353.7483006574528\n",
      "Epoch 1 step 287: training accuarcy: 0.607\n",
      "Epoch 1 step 287: training loss: 1346.5171480002327\n",
      "Epoch 1 step 288: training accuarcy: 0.6125\n",
      "Epoch 1 step 288: training loss: 1347.4249071913214\n",
      "Epoch 1 step 289: training accuarcy: 0.62\n",
      "Epoch 1 step 289: training loss: 1346.493500738776\n",
      "Epoch 1 step 290: training accuarcy: 0.6115\n",
      "Epoch 1 step 290: training loss: 1349.0199978533979\n",
      "Epoch 1 step 291: training accuarcy: 0.6125\n",
      "Epoch 1 step 291: training loss: 1354.5265601467113\n",
      "Epoch 1 step 292: training accuarcy: 0.62\n",
      "Epoch 1 step 292: training loss: 1355.0340484352878\n",
      "Epoch 1 step 293: training accuarcy: 0.6185\n",
      "Epoch 1 step 293: training loss: 1351.5870750038086\n",
      "Epoch 1 step 294: training accuarcy: 0.6055\n",
      "Epoch 1 step 294: training loss: 1353.9020387422677\n",
      "Epoch 1 step 295: training accuarcy: 0.5975\n",
      "Epoch 1 step 295: training loss: 1350.983371206192\n",
      "Epoch 1 step 296: training accuarcy: 0.614\n",
      "Epoch 1 step 296: training loss: 1350.2002263960446\n",
      "Epoch 1 step 297: training accuarcy: 0.598\n",
      "Epoch 1 step 297: training loss: 1349.6983798783863\n",
      "Epoch 1 step 298: training accuarcy: 0.613\n",
      "Epoch 1 step 298: training loss: 1345.5529874089045\n",
      "Epoch 1 step 299: training accuarcy: 0.616\n",
      "Epoch 1 step 299: training loss: 1346.29455075383\n",
      "Epoch 1 step 300: training accuarcy: 0.599\n",
      "Epoch 1 step 300: training loss: 1347.3817269385604\n",
      "Epoch 1 step 301: training accuarcy: 0.624\n",
      "Epoch 1 step 301: training loss: 1344.2648821019004\n",
      "Epoch 1 step 302: training accuarcy: 0.6165\n",
      "Epoch 1 step 302: training loss: 1350.1759687945332\n",
      "Epoch 1 step 303: training accuarcy: 0.6055\n",
      "Epoch 1 step 303: training loss: 1343.2484877367156\n",
      "Epoch 1 step 304: training accuarcy: 0.6145\n",
      "Epoch 1 step 304: training loss: 1356.958027938697\n",
      "Epoch 1 step 305: training accuarcy: 0.6055\n",
      "Epoch 1 step 305: training loss: 1340.9556392051038\n",
      "Epoch 1 step 306: training accuarcy: 0.6235\n",
      "Epoch 1 step 306: training loss: 1357.441032748914\n",
      "Epoch 1 step 307: training accuarcy: 0.6075\n",
      "Epoch 1 step 307: training loss: 1352.0853833934734\n",
      "Epoch 1 step 308: training accuarcy: 0.612\n",
      "Epoch 1 step 308: training loss: 1346.1029093341574\n",
      "Epoch 1 step 309: training accuarcy: 0.607\n",
      "Epoch 1 step 309: training loss: 1346.802029739824\n",
      "Epoch 1 step 310: training accuarcy: 0.61\n",
      "Epoch 1 step 310: training loss: 1353.6979891435892\n",
      "Epoch 1 step 311: training accuarcy: 0.5985\n",
      "Epoch 1 step 311: training loss: 1342.7213682124245\n",
      "Epoch 1 step 312: training accuarcy: 0.605\n",
      "Epoch 1 step 312: training loss: 1335.8588405109292\n",
      "Epoch 1 step 313: training accuarcy: 0.6175\n",
      "Epoch 1 step 313: training loss: 1342.2679637995875\n",
      "Epoch 1 step 314: training accuarcy: 0.614\n",
      "Epoch 1 step 314: training loss: 1321.4797148266973\n",
      "Epoch 1 step 315: training accuarcy: 0.624\n",
      "Epoch 1 step 315: training loss: 1341.3518296268062\n",
      "Epoch 1 step 316: training accuarcy: 0.6235\n",
      "Epoch 1 step 316: training loss: 1341.8186916085247\n",
      "Epoch 1 step 317: training accuarcy: 0.629\n",
      "Epoch 1 step 317: training loss: 1360.1100601523435\n",
      "Epoch 1 step 318: training accuarcy: 0.5985\n",
      "Epoch 1 step 318: training loss: 1355.6690312406286\n",
      "Epoch 1 step 319: training accuarcy: 0.6065\n",
      "Epoch 1 step 319: training loss: 1347.4643620053691\n",
      "Epoch 1 step 320: training accuarcy: 0.6155\n",
      "Epoch 1 step 320: training loss: 1347.235687835245\n",
      "Epoch 1 step 321: training accuarcy: 0.6135\n",
      "Epoch 1 step 321: training loss: 1343.9397269116807\n",
      "Epoch 1 step 322: training accuarcy: 0.5955\n",
      "Epoch 1 step 322: training loss: 1346.6565196818244\n",
      "Epoch 1 step 323: training accuarcy: 0.609\n",
      "Epoch 1 step 323: training loss: 1354.0839579341045\n",
      "Epoch 1 step 324: training accuarcy: 0.6095\n",
      "Epoch 1 step 324: training loss: 1348.2670346632613\n",
      "Epoch 1 step 325: training accuarcy: 0.6065\n",
      "Epoch 1 step 325: training loss: 1348.8481046525437\n",
      "Epoch 1 step 326: training accuarcy: 0.6035\n",
      "Epoch 1 step 326: training loss: 1345.2518729554663\n",
      "Epoch 1 step 327: training accuarcy: 0.633\n",
      "Epoch 1 step 327: training loss: 1358.661897210252\n",
      "Epoch 1 step 328: training accuarcy: 0.5965\n",
      "Epoch 1 step 328: training loss: 1341.3980417356402\n",
      "Epoch 1 step 329: training accuarcy: 0.614\n",
      "Epoch 1 step 329: training loss: 1338.4863789285887\n",
      "Epoch 1 step 330: training accuarcy: 0.6185\n",
      "Epoch 1 step 330: training loss: 1347.6590181251959\n",
      "Epoch 1 step 331: training accuarcy: 0.615\n",
      "Epoch 1 step 331: training loss: 1344.0647012360705\n",
      "Epoch 1 step 332: training accuarcy: 0.63\n",
      "Epoch 1 step 332: training loss: 1350.0059609836655\n",
      "Epoch 1 step 333: training accuarcy: 0.602\n",
      "Epoch 1 step 333: training loss: 1357.0885088553168\n",
      "Epoch 1 step 334: training accuarcy: 0.6115\n",
      "Epoch 1 step 334: training loss: 1335.7092514074784\n",
      "Epoch 1 step 335: training accuarcy: 0.6315000000000001\n",
      "Epoch 1 step 335: training loss: 1351.8390399731445\n",
      "Epoch 1 step 336: training accuarcy: 0.603\n",
      "Epoch 1 step 336: training loss: 1344.4966000174645\n",
      "Epoch 1 step 337: training accuarcy: 0.5985\n",
      "Epoch 1 step 337: training loss: 1345.3485670117052\n",
      "Epoch 1 step 338: training accuarcy: 0.6105\n",
      "Epoch 1 step 338: training loss: 1354.0793126679307\n",
      "Epoch 1 step 339: training accuarcy: 0.6075\n",
      "Epoch 1 step 339: training loss: 1357.5866257249452\n",
      "Epoch 1 step 340: training accuarcy: 0.606\n",
      "Epoch 1 step 340: training loss: 1342.6826426003774\n",
      "Epoch 1 step 341: training accuarcy: 0.6065\n",
      "Epoch 1 step 341: training loss: 1342.5129974564868\n",
      "Epoch 1 step 342: training accuarcy: 0.611\n",
      "Epoch 1 step 342: training loss: 1337.5975759948808\n",
      "Epoch 1 step 343: training accuarcy: 0.618\n",
      "Epoch 1 step 343: training loss: 1348.6740697660987\n",
      "Epoch 1 step 344: training accuarcy: 0.592\n",
      "Epoch 1 step 344: training loss: 1357.1308912920263\n",
      "Epoch 1 step 345: training accuarcy: 0.6075\n",
      "Epoch 1 step 345: training loss: 1335.6315434024182\n",
      "Epoch 1 step 346: training accuarcy: 0.6235\n",
      "Epoch 1 step 346: training loss: 1343.5130878789303\n",
      "Epoch 1 step 347: training accuarcy: 0.6135\n",
      "Epoch 1 step 347: training loss: 1340.026691849795\n",
      "Epoch 1 step 348: training accuarcy: 0.6065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 348: training loss: 1337.2246532191577\n",
      "Epoch 1 step 349: training accuarcy: 0.621\n",
      "Epoch 1 step 349: training loss: 1345.0393020908332\n",
      "Epoch 1 step 350: training accuarcy: 0.617\n",
      "Epoch 1 step 350: training loss: 1340.169031368799\n",
      "Epoch 1 step 351: training accuarcy: 0.6125\n",
      "Epoch 1 step 351: training loss: 1359.6247804675613\n",
      "Epoch 1 step 352: training accuarcy: 0.602\n",
      "Epoch 1 step 352: training loss: 1340.7254080105401\n",
      "Epoch 1 step 353: training accuarcy: 0.6245\n",
      "Epoch 1 step 353: training loss: 1353.5470404479624\n",
      "Epoch 1 step 354: training accuarcy: 0.6045\n",
      "Epoch 1 step 354: training loss: 1345.1546179582806\n",
      "Epoch 1 step 355: training accuarcy: 0.62\n",
      "Epoch 1 step 355: training loss: 1350.0215189999258\n",
      "Epoch 1 step 356: training accuarcy: 0.6\n",
      "Epoch 1 step 356: training loss: 1351.0504338278745\n",
      "Epoch 1 step 357: training accuarcy: 0.6145\n",
      "Epoch 1 step 357: training loss: 1335.9143205643115\n",
      "Epoch 1 step 358: training accuarcy: 0.629\n",
      "Epoch 1 step 358: training loss: 1347.9339195011519\n",
      "Epoch 1 step 359: training accuarcy: 0.6095\n",
      "Epoch 1 step 359: training loss: 1346.164797339256\n",
      "Epoch 1 step 360: training accuarcy: 0.6105\n",
      "Epoch 1 step 360: training loss: 1344.3192805175001\n",
      "Epoch 1 step 361: training accuarcy: 0.627\n",
      "Epoch 1 step 361: training loss: 1349.424019750048\n",
      "Epoch 1 step 362: training accuarcy: 0.612\n",
      "Epoch 1 step 362: training loss: 1361.1082481357973\n",
      "Epoch 1 step 363: training accuarcy: 0.599\n",
      "Epoch 1 step 363: training loss: 1353.2788433681994\n",
      "Epoch 1 step 364: training accuarcy: 0.614\n",
      "Epoch 1 step 364: training loss: 1346.76092064186\n",
      "Epoch 1 step 365: training accuarcy: 0.606\n",
      "Epoch 1 step 365: training loss: 1352.964709460066\n",
      "Epoch 1 step 366: training accuarcy: 0.6115\n",
      "Epoch 1 step 366: training loss: 1347.746466206908\n",
      "Epoch 1 step 367: training accuarcy: 0.6\n",
      "Epoch 1 step 367: training loss: 1345.8753953516655\n",
      "Epoch 1 step 368: training accuarcy: 0.6135\n",
      "Epoch 1 step 368: training loss: 1338.8608617100729\n",
      "Epoch 1 step 369: training accuarcy: 0.6255000000000001\n",
      "Epoch 1 step 369: training loss: 1347.6129129652077\n",
      "Epoch 1 step 370: training accuarcy: 0.6045\n",
      "Epoch 1 step 370: training loss: 1353.155911096561\n",
      "Epoch 1 step 371: training accuarcy: 0.608\n",
      "Epoch 1 step 371: training loss: 1352.9558904830883\n",
      "Epoch 1 step 372: training accuarcy: 0.601\n",
      "Epoch 1 step 372: training loss: 1340.8267314704456\n",
      "Epoch 1 step 373: training accuarcy: 0.618\n",
      "Epoch 1 step 373: training loss: 1343.9134253151103\n",
      "Epoch 1 step 374: training accuarcy: 0.609\n",
      "Epoch 1 step 374: training loss: 1335.9340662550514\n",
      "Epoch 1 step 375: training accuarcy: 0.6155\n",
      "Epoch 1 step 375: training loss: 1348.1953932901133\n",
      "Epoch 1 step 376: training accuarcy: 0.595\n",
      "Epoch 1 step 376: training loss: 1345.3252287058367\n",
      "Epoch 1 step 377: training accuarcy: 0.6235\n",
      "Epoch 1 step 377: training loss: 1343.5060286459307\n",
      "Epoch 1 step 378: training accuarcy: 0.619\n",
      "Epoch 1 step 378: training loss: 1333.751624956569\n",
      "Epoch 1 step 379: training accuarcy: 0.617\n",
      "Epoch 1 step 379: training loss: 1339.2599853485146\n",
      "Epoch 1 step 380: training accuarcy: 0.623\n",
      "Epoch 1 step 380: training loss: 1342.4288073041794\n",
      "Epoch 1 step 381: training accuarcy: 0.6235\n",
      "Epoch 1 step 381: training loss: 1342.728654599403\n",
      "Epoch 1 step 382: training accuarcy: 0.6245\n",
      "Epoch 1 step 382: training loss: 1331.0651997200223\n",
      "Epoch 1 step 383: training accuarcy: 0.624\n",
      "Epoch 1 step 383: training loss: 1343.6218019737753\n",
      "Epoch 1 step 384: training accuarcy: 0.617\n",
      "Epoch 1 step 384: training loss: 1354.1891462295287\n",
      "Epoch 1 step 385: training accuarcy: 0.5975\n",
      "Epoch 1 step 385: training loss: 1345.8320891643184\n",
      "Epoch 1 step 386: training accuarcy: 0.6115\n",
      "Epoch 1 step 386: training loss: 1341.678377629401\n",
      "Epoch 1 step 387: training accuarcy: 0.609\n",
      "Epoch 1 step 387: training loss: 1344.3254527987315\n",
      "Epoch 1 step 388: training accuarcy: 0.609\n",
      "Epoch 1 step 388: training loss: 1340.188276643689\n",
      "Epoch 1 step 389: training accuarcy: 0.616\n",
      "Epoch 1 step 389: training loss: 1343.3695417971323\n",
      "Epoch 1 step 390: training accuarcy: 0.6085\n",
      "Epoch 1 step 390: training loss: 1339.6311009457422\n",
      "Epoch 1 step 391: training accuarcy: 0.6225\n",
      "Epoch 1 step 391: training loss: 1337.2749887887717\n",
      "Epoch 1 step 392: training accuarcy: 0.628\n",
      "Epoch 1 step 392: training loss: 1358.953683519302\n",
      "Epoch 1 step 393: training accuarcy: 0.59\n",
      "Epoch 1 step 393: training loss: 1337.2046110213241\n",
      "Epoch 1 step 394: training accuarcy: 0.6195\n",
      "Epoch 1 step 394: training loss: 1346.4675567524612\n",
      "Epoch 1 step 395: training accuarcy: 0.6235\n",
      "Epoch 1 step 395: training loss: 1346.5534980576576\n",
      "Epoch 1 step 396: training accuarcy: 0.615\n",
      "Epoch 1 step 396: training loss: 1346.3216327418945\n",
      "Epoch 1 step 397: training accuarcy: 0.616\n",
      "Epoch 1 step 397: training loss: 1352.7712884740024\n",
      "Epoch 1 step 398: training accuarcy: 0.589\n",
      "Epoch 1 step 398: training loss: 1334.9231453904922\n",
      "Epoch 1 step 399: training accuarcy: 0.6155\n",
      "Epoch 1 step 399: training loss: 1329.9656684535998\n",
      "Epoch 1 step 400: training accuarcy: 0.618\n",
      "Epoch 1 step 400: training loss: 1344.987287417928\n",
      "Epoch 1 step 401: training accuarcy: 0.629\n",
      "Epoch 1 step 401: training loss: 1347.2025188592252\n",
      "Epoch 1 step 402: training accuarcy: 0.621\n",
      "Epoch 1 step 402: training loss: 1336.2845644569193\n",
      "Epoch 1 step 403: training accuarcy: 0.6105\n",
      "Epoch 1 step 403: training loss: 1345.263835713591\n",
      "Epoch 1 step 404: training accuarcy: 0.6155\n",
      "Epoch 1 step 404: training loss: 1344.4985837469414\n",
      "Epoch 1 step 405: training accuarcy: 0.6135\n",
      "Epoch 1 step 405: training loss: 1337.1439299723131\n",
      "Epoch 1 step 406: training accuarcy: 0.6195\n",
      "Epoch 1 step 406: training loss: 1328.6495156457183\n",
      "Epoch 1 step 407: training accuarcy: 0.639\n",
      "Epoch 1 step 407: training loss: 1347.2242255844808\n",
      "Epoch 1 step 408: training accuarcy: 0.6175\n",
      "Epoch 1 step 408: training loss: 1320.6506824012663\n",
      "Epoch 1 step 409: training accuarcy: 0.635\n",
      "Epoch 1 step 409: training loss: 1344.764703119654\n",
      "Epoch 1 step 410: training accuarcy: 0.6115\n",
      "Epoch 1 step 410: training loss: 1345.5454550509048\n",
      "Epoch 1 step 411: training accuarcy: 0.605\n",
      "Epoch 1 step 411: training loss: 1358.7613462808706\n",
      "Epoch 1 step 412: training accuarcy: 0.6025\n",
      "Epoch 1 step 412: training loss: 1340.0979123756185\n",
      "Epoch 1 step 413: training accuarcy: 0.6185\n",
      "Epoch 1 step 413: training loss: 1335.790161070865\n",
      "Epoch 1 step 414: training accuarcy: 0.628\n",
      "Epoch 1 step 414: training loss: 1339.179195578465\n",
      "Epoch 1 step 415: training accuarcy: 0.612\n",
      "Epoch 1 step 415: training loss: 1349.7864549763615\n",
      "Epoch 1 step 416: training accuarcy: 0.5975\n",
      "Epoch 1 step 416: training loss: 1352.739077968825\n",
      "Epoch 1 step 417: training accuarcy: 0.616\n",
      "Epoch 1 step 417: training loss: 1348.6077810169338\n",
      "Epoch 1 step 418: training accuarcy: 0.6035\n",
      "Epoch 1 step 418: training loss: 1342.6785320994832\n",
      "Epoch 1 step 419: training accuarcy: 0.608\n",
      "Epoch 1 step 419: training loss: 1329.7187528411712\n",
      "Epoch 1 step 420: training accuarcy: 0.621\n",
      "Epoch 1 step 420: training loss: 1349.3219614159784\n",
      "Epoch 1 step 421: training accuarcy: 0.6115\n",
      "Epoch 1 step 421: training loss: 1335.6133484811564\n",
      "Epoch 1 step 422: training accuarcy: 0.6205\n",
      "Epoch 1 step 422: training loss: 1354.4133010324367\n",
      "Epoch 1 step 423: training accuarcy: 0.594\n",
      "Epoch 1 step 423: training loss: 1338.8967506407605\n",
      "Epoch 1 step 424: training accuarcy: 0.628\n",
      "Epoch 1 step 424: training loss: 1345.1427695502098\n",
      "Epoch 1 step 425: training accuarcy: 0.6065\n",
      "Epoch 1 step 425: training loss: 1338.5286495132696\n",
      "Epoch 1 step 426: training accuarcy: 0.6125\n",
      "Epoch 1 step 426: training loss: 1327.4259857562904\n",
      "Epoch 1 step 427: training accuarcy: 0.6205\n",
      "Epoch 1 step 427: training loss: 1336.5482673325773\n",
      "Epoch 1 step 428: training accuarcy: 0.6215\n",
      "Epoch 1 step 428: training loss: 1327.0805888606355\n",
      "Epoch 1 step 429: training accuarcy: 0.637\n",
      "Epoch 1 step 429: training loss: 1347.4875846951556\n",
      "Epoch 1 step 430: training accuarcy: 0.596\n",
      "Epoch 1 step 430: training loss: 1352.5395735582795\n",
      "Epoch 1 step 431: training accuarcy: 0.6065\n",
      "Epoch 1 step 431: training loss: 1342.3151160542534\n",
      "Epoch 1 step 432: training accuarcy: 0.619\n",
      "Epoch 1 step 432: training loss: 1330.9551101723941\n",
      "Epoch 1 step 433: training accuarcy: 0.6155\n",
      "Epoch 1 step 433: training loss: 1346.4398912525305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 434: training accuarcy: 0.6155\n",
      "Epoch 1 step 434: training loss: 1334.0719978767972\n",
      "Epoch 1 step 435: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 435: training loss: 1344.6222916703352\n",
      "Epoch 1 step 436: training accuarcy: 0.611\n",
      "Epoch 1 step 436: training loss: 1340.9855905642848\n",
      "Epoch 1 step 437: training accuarcy: 0.6125\n",
      "Epoch 1 step 437: training loss: 1328.2482716552602\n",
      "Epoch 1 step 438: training accuarcy: 0.624\n",
      "Epoch 1 step 438: training loss: 1342.3385849761016\n",
      "Epoch 1 step 439: training accuarcy: 0.608\n",
      "Epoch 1 step 439: training loss: 1333.0664643553919\n",
      "Epoch 1 step 440: training accuarcy: 0.6225\n",
      "Epoch 1 step 440: training loss: 1340.4190233265617\n",
      "Epoch 1 step 441: training accuarcy: 0.6185\n",
      "Epoch 1 step 441: training loss: 1346.9473516754113\n",
      "Epoch 1 step 442: training accuarcy: 0.605\n",
      "Epoch 1 step 442: training loss: 1332.0020040792551\n",
      "Epoch 1 step 443: training accuarcy: 0.6375000000000001\n",
      "Epoch 1 step 443: training loss: 1351.4632831870701\n",
      "Epoch 1 step 444: training accuarcy: 0.607\n",
      "Epoch 1 step 444: training loss: 1341.401274870714\n",
      "Epoch 1 step 445: training accuarcy: 0.622\n",
      "Epoch 1 step 445: training loss: 1349.6021678772743\n",
      "Epoch 1 step 446: training accuarcy: 0.612\n",
      "Epoch 1 step 446: training loss: 1340.2521356589134\n",
      "Epoch 1 step 447: training accuarcy: 0.603\n",
      "Epoch 1 step 447: training loss: 1351.078032528377\n",
      "Epoch 1 step 448: training accuarcy: 0.597\n",
      "Epoch 1 step 448: training loss: 1341.6423445710443\n",
      "Epoch 1 step 449: training accuarcy: 0.611\n",
      "Epoch 1 step 449: training loss: 1338.5495693194703\n",
      "Epoch 1 step 450: training accuarcy: 0.612\n",
      "Epoch 1 step 450: training loss: 1331.4399909858835\n",
      "Epoch 1 step 451: training accuarcy: 0.619\n",
      "Epoch 1 step 451: training loss: 1329.0878275332043\n",
      "Epoch 1 step 452: training accuarcy: 0.646\n",
      "Epoch 1 step 452: training loss: 1328.1616230940851\n",
      "Epoch 1 step 453: training accuarcy: 0.6235\n",
      "Epoch 1 step 453: training loss: 1346.884850364088\n",
      "Epoch 1 step 454: training accuarcy: 0.61\n",
      "Epoch 1 step 454: training loss: 1340.1725952284685\n",
      "Epoch 1 step 455: training accuarcy: 0.6225\n",
      "Epoch 1 step 455: training loss: 1331.2451877725193\n",
      "Epoch 1 step 456: training accuarcy: 0.628\n",
      "Epoch 1 step 456: training loss: 1348.4138724650168\n",
      "Epoch 1 step 457: training accuarcy: 0.6155\n",
      "Epoch 1 step 457: training loss: 1332.2599046909734\n",
      "Epoch 1 step 458: training accuarcy: 0.6115\n",
      "Epoch 1 step 458: training loss: 1336.4494822863758\n",
      "Epoch 1 step 459: training accuarcy: 0.612\n",
      "Epoch 1 step 459: training loss: 1343.9133007517344\n",
      "Epoch 1 step 460: training accuarcy: 0.6125\n",
      "Epoch 1 step 460: training loss: 1338.1211800090348\n",
      "Epoch 1 step 461: training accuarcy: 0.604\n",
      "Epoch 1 step 461: training loss: 1336.585079665069\n",
      "Epoch 1 step 462: training accuarcy: 0.614\n",
      "Epoch 1 step 462: training loss: 1330.8108146427878\n",
      "Epoch 1 step 463: training accuarcy: 0.611\n",
      "Epoch 1 step 463: training loss: 1346.6618081322542\n",
      "Epoch 1 step 464: training accuarcy: 0.6135\n",
      "Epoch 1 step 464: training loss: 1327.2115234590403\n",
      "Epoch 1 step 465: training accuarcy: 0.618\n",
      "Epoch 1 step 465: training loss: 1343.894216704683\n",
      "Epoch 1 step 466: training accuarcy: 0.613\n",
      "Epoch 1 step 466: training loss: 1323.668083677956\n",
      "Epoch 1 step 467: training accuarcy: 0.635\n",
      "Epoch 1 step 467: training loss: 1320.6118344966326\n",
      "Epoch 1 step 468: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 468: training loss: 1340.156958112067\n",
      "Epoch 1 step 469: training accuarcy: 0.613\n",
      "Epoch 1 step 469: training loss: 1337.10159332313\n",
      "Epoch 1 step 470: training accuarcy: 0.6185\n",
      "Epoch 1 step 470: training loss: 1328.7560112758335\n",
      "Epoch 1 step 471: training accuarcy: 0.628\n",
      "Epoch 1 step 471: training loss: 1325.1326110894809\n",
      "Epoch 1 step 472: training accuarcy: 0.6345000000000001\n",
      "Epoch 1 step 472: training loss: 1338.5562648584598\n",
      "Epoch 1 step 473: training accuarcy: 0.616\n",
      "Epoch 1 step 473: training loss: 1331.7325772673873\n",
      "Epoch 1 step 474: training accuarcy: 0.623\n",
      "Epoch 1 step 474: training loss: 1342.7980198757255\n",
      "Epoch 1 step 475: training accuarcy: 0.6085\n",
      "Epoch 1 step 475: training loss: 1335.3949050454219\n",
      "Epoch 1 step 476: training accuarcy: 0.6365000000000001\n",
      "Epoch 1 step 476: training loss: 1343.0215852005672\n",
      "Epoch 1 step 477: training accuarcy: 0.6165\n",
      "Epoch 1 step 477: training loss: 1325.8134421449588\n",
      "Epoch 1 step 478: training accuarcy: 0.623\n",
      "Epoch 1 step 478: training loss: 1340.0888200430913\n",
      "Epoch 1 step 479: training accuarcy: 0.6045\n",
      "Epoch 1 step 479: training loss: 1342.3872743127558\n",
      "Epoch 1 step 480: training accuarcy: 0.605\n",
      "Epoch 1 step 480: training loss: 1338.4550919399157\n",
      "Epoch 1 step 481: training accuarcy: 0.6235\n",
      "Epoch 1 step 481: training loss: 1344.094183964191\n",
      "Epoch 1 step 482: training accuarcy: 0.6025\n",
      "Epoch 1 step 482: training loss: 1346.2970913357783\n",
      "Epoch 1 step 483: training accuarcy: 0.626\n",
      "Epoch 1 step 483: training loss: 1356.2889989579926\n",
      "Epoch 1 step 484: training accuarcy: 0.594\n",
      "Epoch 1 step 484: training loss: 1342.2070576060878\n",
      "Epoch 1 step 485: training accuarcy: 0.608\n",
      "Epoch 1 step 485: training loss: 1343.3602218511787\n",
      "Epoch 1 step 486: training accuarcy: 0.5955\n",
      "Epoch 1 step 486: training loss: 1339.4777918220163\n",
      "Epoch 1 step 487: training accuarcy: 0.603\n",
      "Epoch 1 step 487: training loss: 1352.7033178044132\n",
      "Epoch 1 step 488: training accuarcy: 0.606\n",
      "Epoch 1 step 488: training loss: 1344.3730954366401\n",
      "Epoch 1 step 489: training accuarcy: 0.6095\n",
      "Epoch 1 step 489: training loss: 1330.6322376196422\n",
      "Epoch 1 step 490: training accuarcy: 0.6175\n",
      "Epoch 1 step 490: training loss: 1330.1260517484186\n",
      "Epoch 1 step 491: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 491: training loss: 1343.8283720078757\n",
      "Epoch 1 step 492: training accuarcy: 0.6155\n",
      "Epoch 1 step 492: training loss: 1337.2257192311251\n",
      "Epoch 1 step 493: training accuarcy: 0.6175\n",
      "Epoch 1 step 493: training loss: 1353.8229036746645\n",
      "Epoch 1 step 494: training accuarcy: 0.596\n",
      "Epoch 1 step 494: training loss: 1330.2538486425265\n",
      "Epoch 1 step 495: training accuarcy: 0.626\n",
      "Epoch 1 step 495: training loss: 1344.9044446615949\n",
      "Epoch 1 step 496: training accuarcy: 0.5975\n",
      "Epoch 1 step 496: training loss: 1331.0359884928896\n",
      "Epoch 1 step 497: training accuarcy: 0.633\n",
      "Epoch 1 step 497: training loss: 1338.0487814905848\n",
      "Epoch 1 step 498: training accuarcy: 0.6215\n",
      "Epoch 1 step 498: training loss: 1336.9700536443304\n",
      "Epoch 1 step 499: training accuarcy: 0.6095\n",
      "Epoch 1 step 499: training loss: 1335.3590536806505\n",
      "Epoch 1 step 500: training accuarcy: 0.609\n",
      "Epoch 1 step 500: training loss: 1358.1412010702936\n",
      "Epoch 1 step 501: training accuarcy: 0.5825\n",
      "Epoch 1 step 501: training loss: 1336.8999562519443\n",
      "Epoch 1 step 502: training accuarcy: 0.617\n",
      "Epoch 1 step 502: training loss: 1330.7142792019567\n",
      "Epoch 1 step 503: training accuarcy: 0.621\n",
      "Epoch 1 step 503: training loss: 1356.3401301040417\n",
      "Epoch 1 step 504: training accuarcy: 0.601\n",
      "Epoch 1 step 504: training loss: 1331.6374105216546\n",
      "Epoch 1 step 505: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 505: training loss: 1330.3100608573666\n",
      "Epoch 1 step 506: training accuarcy: 0.609\n",
      "Epoch 1 step 506: training loss: 1338.6411791717667\n",
      "Epoch 1 step 507: training accuarcy: 0.5985\n",
      "Epoch 1 step 507: training loss: 1336.5431713217006\n",
      "Epoch 1 step 508: training accuarcy: 0.6105\n",
      "Epoch 1 step 508: training loss: 1333.0860663462254\n",
      "Epoch 1 step 509: training accuarcy: 0.6195\n",
      "Epoch 1 step 509: training loss: 1326.1759956509711\n",
      "Epoch 1 step 510: training accuarcy: 0.6275000000000001\n",
      "Epoch 1 step 510: training loss: 1338.0012426531437\n",
      "Epoch 1 step 511: training accuarcy: 0.627\n",
      "Epoch 1 step 511: training loss: 1325.0174693161234\n",
      "Epoch 1 step 512: training accuarcy: 0.636\n",
      "Epoch 1 step 512: training loss: 1348.6188579404052\n",
      "Epoch 1 step 513: training accuarcy: 0.6115\n",
      "Epoch 1 step 513: training loss: 1343.2134673424737\n",
      "Epoch 1 step 514: training accuarcy: 0.6205\n",
      "Epoch 1 step 514: training loss: 1337.92178379799\n",
      "Epoch 1 step 515: training accuarcy: 0.611\n",
      "Epoch 1 step 515: training loss: 1359.239756941762\n",
      "Epoch 1 step 516: training accuarcy: 0.597\n",
      "Epoch 1 step 516: training loss: 1345.7344467165512\n",
      "Epoch 1 step 517: training accuarcy: 0.613\n",
      "Epoch 1 step 517: training loss: 1332.212550431596\n",
      "Epoch 1 step 518: training accuarcy: 0.623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 518: training loss: 1352.1339373766402\n",
      "Epoch 1 step 519: training accuarcy: 0.6125\n",
      "Epoch 1 step 519: training loss: 1349.1661336086472\n",
      "Epoch 1 step 520: training accuarcy: 0.597\n",
      "Epoch 1 step 520: training loss: 1342.5834784581064\n",
      "Epoch 1 step 521: training accuarcy: 0.6245\n",
      "Epoch 1 step 521: training loss: 1327.7983584490846\n",
      "Epoch 1 step 522: training accuarcy: 0.6255000000000001\n",
      "Epoch 1 step 522: training loss: 1338.3455036127793\n",
      "Epoch 1 step 523: training accuarcy: 0.6265000000000001\n",
      "Epoch 1 step 523: training loss: 1339.5079514718248\n",
      "Epoch 1 step 524: training accuarcy: 0.6255000000000001\n",
      "Epoch 1 step 524: training loss: 1329.446147251125\n",
      "Epoch 1 step 525: training accuarcy: 0.645\n",
      "Epoch 1 step 525: training loss: 531.5001299228525\n",
      "Epoch 1 step 526: training accuarcy: 0.6025641025641025\n",
      "Epoch 1: train loss 1340.293542307019, train accuarcy 0.6137834191322327\n",
      "Epoch 1: valid loss 6697.441975467826, valid accuarcy 0.5837680697441101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████████████████████████████▊                                                                                                       | 2/5 [10:47<16:10, 323.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 1348.7106739216574\n",
      "Epoch 2 step 527: training accuarcy: 0.611\n",
      "Epoch 2 step 527: training loss: 1348.3985891808638\n",
      "Epoch 2 step 528: training accuarcy: 0.611\n",
      "Epoch 2 step 528: training loss: 1348.6645753063267\n",
      "Epoch 2 step 529: training accuarcy: 0.6085\n",
      "Epoch 2 step 529: training loss: 1331.6184003089643\n",
      "Epoch 2 step 530: training accuarcy: 0.6185\n",
      "Epoch 2 step 530: training loss: 1343.4137042737266\n",
      "Epoch 2 step 531: training accuarcy: 0.6105\n",
      "Epoch 2 step 531: training loss: 1337.5092639910204\n",
      "Epoch 2 step 532: training accuarcy: 0.62\n",
      "Epoch 2 step 532: training loss: 1328.9896085835067\n",
      "Epoch 2 step 533: training accuarcy: 0.632\n",
      "Epoch 2 step 533: training loss: 1324.2098578989865\n",
      "Epoch 2 step 534: training accuarcy: 0.6375000000000001\n",
      "Epoch 2 step 534: training loss: 1346.4582061096435\n",
      "Epoch 2 step 535: training accuarcy: 0.6085\n",
      "Epoch 2 step 535: training loss: 1353.0802953885395\n",
      "Epoch 2 step 536: training accuarcy: 0.61\n",
      "Epoch 2 step 536: training loss: 1331.6204837871062\n",
      "Epoch 2 step 537: training accuarcy: 0.624\n",
      "Epoch 2 step 537: training loss: 1351.4746583024205\n",
      "Epoch 2 step 538: training accuarcy: 0.612\n",
      "Epoch 2 step 538: training loss: 1340.6718786159397\n",
      "Epoch 2 step 539: training accuarcy: 0.6155\n",
      "Epoch 2 step 539: training loss: 1337.908881565924\n",
      "Epoch 2 step 540: training accuarcy: 0.633\n",
      "Epoch 2 step 540: training loss: 1336.4988250353856\n",
      "Epoch 2 step 541: training accuarcy: 0.631\n",
      "Epoch 2 step 541: training loss: 1335.459324603331\n",
      "Epoch 2 step 542: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 542: training loss: 1335.0115330719418\n",
      "Epoch 2 step 543: training accuarcy: 0.643\n",
      "Epoch 2 step 543: training loss: 1332.3692146753483\n",
      "Epoch 2 step 544: training accuarcy: 0.6135\n",
      "Epoch 2 step 544: training loss: 1341.2126377419827\n",
      "Epoch 2 step 545: training accuarcy: 0.609\n",
      "Epoch 2 step 545: training loss: 1342.171540934082\n",
      "Epoch 2 step 546: training accuarcy: 0.6235\n",
      "Epoch 2 step 546: training loss: 1326.8459760014416\n",
      "Epoch 2 step 547: training accuarcy: 0.612\n",
      "Epoch 2 step 547: training loss: 1328.4780249564515\n",
      "Epoch 2 step 548: training accuarcy: 0.6205\n",
      "Epoch 2 step 548: training loss: 1321.6816430491083\n",
      "Epoch 2 step 549: training accuarcy: 0.634\n",
      "Epoch 2 step 549: training loss: 1331.9303185943018\n",
      "Epoch 2 step 550: training accuarcy: 0.613\n",
      "Epoch 2 step 550: training loss: 1326.152075583361\n",
      "Epoch 2 step 551: training accuarcy: 0.6125\n",
      "Epoch 2 step 551: training loss: 1338.7573031079974\n",
      "Epoch 2 step 552: training accuarcy: 0.613\n",
      "Epoch 2 step 552: training loss: 1356.8354754508398\n",
      "Epoch 2 step 553: training accuarcy: 0.6055\n",
      "Epoch 2 step 553: training loss: 1343.6906230429545\n",
      "Epoch 2 step 554: training accuarcy: 0.612\n",
      "Epoch 2 step 554: training loss: 1333.6399782206393\n",
      "Epoch 2 step 555: training accuarcy: 0.6155\n",
      "Epoch 2 step 555: training loss: 1342.5560563306758\n",
      "Epoch 2 step 556: training accuarcy: 0.6155\n",
      "Epoch 2 step 556: training loss: 1351.627063988001\n",
      "Epoch 2 step 557: training accuarcy: 0.5995\n",
      "Epoch 2 step 557: training loss: 1338.668903509936\n",
      "Epoch 2 step 558: training accuarcy: 0.611\n",
      "Epoch 2 step 558: training loss: 1331.9405526798066\n",
      "Epoch 2 step 559: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 559: training loss: 1340.6746612058885\n",
      "Epoch 2 step 560: training accuarcy: 0.6145\n",
      "Epoch 2 step 560: training loss: 1336.4701071070763\n",
      "Epoch 2 step 561: training accuarcy: 0.6145\n",
      "Epoch 2 step 561: training loss: 1335.9551641491273\n",
      "Epoch 2 step 562: training accuarcy: 0.6155\n",
      "Epoch 2 step 562: training loss: 1322.3516234523279\n",
      "Epoch 2 step 563: training accuarcy: 0.6345000000000001\n",
      "Epoch 2 step 563: training loss: 1334.3759210178525\n",
      "Epoch 2 step 564: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 564: training loss: 1354.852550065304\n",
      "Epoch 2 step 565: training accuarcy: 0.6035\n",
      "Epoch 2 step 565: training loss: 1348.5016985058892\n",
      "Epoch 2 step 566: training accuarcy: 0.593\n",
      "Epoch 2 step 566: training loss: 1339.2677433040574\n",
      "Epoch 2 step 567: training accuarcy: 0.616\n",
      "Epoch 2 step 567: training loss: 1342.1235657414484\n",
      "Epoch 2 step 568: training accuarcy: 0.62\n",
      "Epoch 2 step 568: training loss: 1353.9439888954391\n",
      "Epoch 2 step 569: training accuarcy: 0.5935\n",
      "Epoch 2 step 569: training loss: 1340.7351576368924\n",
      "Epoch 2 step 570: training accuarcy: 0.621\n",
      "Epoch 2 step 570: training loss: 1337.0346951571157\n",
      "Epoch 2 step 571: training accuarcy: 0.617\n",
      "Epoch 2 step 571: training loss: 1324.345700810226\n",
      "Epoch 2 step 572: training accuarcy: 0.6185\n",
      "Epoch 2 step 572: training loss: 1343.894896814467\n",
      "Epoch 2 step 573: training accuarcy: 0.5995\n",
      "Epoch 2 step 573: training loss: 1340.121325133423\n",
      "Epoch 2 step 574: training accuarcy: 0.631\n",
      "Epoch 2 step 574: training loss: 1343.0327844243166\n",
      "Epoch 2 step 575: training accuarcy: 0.611\n",
      "Epoch 2 step 575: training loss: 1339.052884306642\n",
      "Epoch 2 step 576: training accuarcy: 0.6015\n",
      "Epoch 2 step 576: training loss: 1346.5247275129545\n",
      "Epoch 2 step 577: training accuarcy: 0.602\n",
      "Epoch 2 step 577: training loss: 1340.461731524392\n",
      "Epoch 2 step 578: training accuarcy: 0.6205\n",
      "Epoch 2 step 578: training loss: 1337.9544985162843\n",
      "Epoch 2 step 579: training accuarcy: 0.613\n",
      "Epoch 2 step 579: training loss: 1334.6197481375516\n",
      "Epoch 2 step 580: training accuarcy: 0.618\n",
      "Epoch 2 step 580: training loss: 1331.4817906401856\n",
      "Epoch 2 step 581: training accuarcy: 0.6245\n",
      "Epoch 2 step 581: training loss: 1332.427705283679\n",
      "Epoch 2 step 582: training accuarcy: 0.618\n",
      "Epoch 2 step 582: training loss: 1326.2246290133821\n",
      "Epoch 2 step 583: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 583: training loss: 1339.629485983075\n",
      "Epoch 2 step 584: training accuarcy: 0.612\n",
      "Epoch 2 step 584: training loss: 1341.8987497698436\n",
      "Epoch 2 step 585: training accuarcy: 0.614\n",
      "Epoch 2 step 585: training loss: 1348.690237482699\n",
      "Epoch 2 step 586: training accuarcy: 0.601\n",
      "Epoch 2 step 586: training loss: 1338.5329177353444\n",
      "Epoch 2 step 587: training accuarcy: 0.609\n",
      "Epoch 2 step 587: training loss: 1350.0351703228575\n",
      "Epoch 2 step 588: training accuarcy: 0.611\n",
      "Epoch 2 step 588: training loss: 1344.7742457217573\n",
      "Epoch 2 step 589: training accuarcy: 0.6105\n",
      "Epoch 2 step 589: training loss: 1337.4847280450895\n",
      "Epoch 2 step 590: training accuarcy: 0.62\n",
      "Epoch 2 step 590: training loss: 1334.6733893589037\n",
      "Epoch 2 step 591: training accuarcy: 0.6065\n",
      "Epoch 2 step 591: training loss: 1339.791440120226\n",
      "Epoch 2 step 592: training accuarcy: 0.6175\n",
      "Epoch 2 step 592: training loss: 1358.3005934113016\n",
      "Epoch 2 step 593: training accuarcy: 0.5915\n",
      "Epoch 2 step 593: training loss: 1340.8905047255425\n",
      "Epoch 2 step 594: training accuarcy: 0.6145\n",
      "Epoch 2 step 594: training loss: 1342.3513515080128\n",
      "Epoch 2 step 595: training accuarcy: 0.61\n",
      "Epoch 2 step 595: training loss: 1336.063289456236\n",
      "Epoch 2 step 596: training accuarcy: 0.618\n",
      "Epoch 2 step 596: training loss: 1343.1889378039716\n",
      "Epoch 2 step 597: training accuarcy: 0.609\n",
      "Epoch 2 step 597: training loss: 1335.0166999263017\n",
      "Epoch 2 step 598: training accuarcy: 0.621\n",
      "Epoch 2 step 598: training loss: 1336.4301097995472\n",
      "Epoch 2 step 599: training accuarcy: 0.6025\n",
      "Epoch 2 step 599: training loss: 1333.2892486301528\n",
      "Epoch 2 step 600: training accuarcy: 0.618\n",
      "Epoch 2 step 600: training loss: 1335.0042385723725\n",
      "Epoch 2 step 601: training accuarcy: 0.624\n",
      "Epoch 2 step 601: training loss: 1349.1033153442872\n",
      "Epoch 2 step 602: training accuarcy: 0.6045\n",
      "Epoch 2 step 602: training loss: 1339.2462657555825\n",
      "Epoch 2 step 603: training accuarcy: 0.6095\n",
      "Epoch 2 step 603: training loss: 1329.0981354908174\n",
      "Epoch 2 step 604: training accuarcy: 0.6195\n",
      "Epoch 2 step 604: training loss: 1338.9720182861247\n",
      "Epoch 2 step 605: training accuarcy: 0.62\n",
      "Epoch 2 step 605: training loss: 1345.5074354776941\n",
      "Epoch 2 step 606: training accuarcy: 0.6095\n",
      "Epoch 2 step 606: training loss: 1340.3631579180817\n",
      "Epoch 2 step 607: training accuarcy: 0.615\n",
      "Epoch 2 step 607: training loss: 1320.0984390933054\n",
      "Epoch 2 step 608: training accuarcy: 0.637\n",
      "Epoch 2 step 608: training loss: 1341.3883962606592\n",
      "Epoch 2 step 609: training accuarcy: 0.628\n",
      "Epoch 2 step 609: training loss: 1347.2306128896707\n",
      "Epoch 2 step 610: training accuarcy: 0.607\n",
      "Epoch 2 step 610: training loss: 1336.914570762191\n",
      "Epoch 2 step 611: training accuarcy: 0.6005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 611: training loss: 1335.7825161234243\n",
      "Epoch 2 step 612: training accuarcy: 0.615\n",
      "Epoch 2 step 612: training loss: 1348.9727597010929\n",
      "Epoch 2 step 613: training accuarcy: 0.597\n",
      "Epoch 2 step 613: training loss: 1339.3280813979002\n",
      "Epoch 2 step 614: training accuarcy: 0.609\n",
      "Epoch 2 step 614: training loss: 1336.4709557394322\n",
      "Epoch 2 step 615: training accuarcy: 0.6205\n",
      "Epoch 2 step 615: training loss: 1351.0218760244486\n",
      "Epoch 2 step 616: training accuarcy: 0.594\n",
      "Epoch 2 step 616: training loss: 1323.7339835738917\n",
      "Epoch 2 step 617: training accuarcy: 0.631\n",
      "Epoch 2 step 617: training loss: 1326.3643179915389\n",
      "Epoch 2 step 618: training accuarcy: 0.613\n",
      "Epoch 2 step 618: training loss: 1331.5759148559991\n",
      "Epoch 2 step 619: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 619: training loss: 1335.4812090858297\n",
      "Epoch 2 step 620: training accuarcy: 0.616\n",
      "Epoch 2 step 620: training loss: 1327.3559600834285\n",
      "Epoch 2 step 621: training accuarcy: 0.628\n",
      "Epoch 2 step 621: training loss: 1338.713817525312\n",
      "Epoch 2 step 622: training accuarcy: 0.6165\n",
      "Epoch 2 step 622: training loss: 1339.959502629515\n",
      "Epoch 2 step 623: training accuarcy: 0.616\n",
      "Epoch 2 step 623: training loss: 1349.4092208400566\n",
      "Epoch 2 step 624: training accuarcy: 0.6075\n",
      "Epoch 2 step 624: training loss: 1326.2836134639417\n",
      "Epoch 2 step 625: training accuarcy: 0.6135\n",
      "Epoch 2 step 625: training loss: 1335.663879186492\n",
      "Epoch 2 step 626: training accuarcy: 0.6055\n",
      "Epoch 2 step 626: training loss: 1326.6090801956498\n",
      "Epoch 2 step 627: training accuarcy: 0.6275000000000001\n",
      "Epoch 2 step 627: training loss: 1344.7547043595891\n",
      "Epoch 2 step 628: training accuarcy: 0.6005\n",
      "Epoch 2 step 628: training loss: 1326.9153268685259\n",
      "Epoch 2 step 629: training accuarcy: 0.6295000000000001\n",
      "Epoch 2 step 629: training loss: 1336.886880768991\n",
      "Epoch 2 step 630: training accuarcy: 0.619\n",
      "Epoch 2 step 630: training loss: 1339.5481137359122\n",
      "Epoch 2 step 631: training accuarcy: 0.614\n",
      "Epoch 2 step 631: training loss: 1334.1322477527024\n",
      "Epoch 2 step 632: training accuarcy: 0.6185\n",
      "Epoch 2 step 632: training loss: 1329.9759442722434\n",
      "Epoch 2 step 633: training accuarcy: 0.6275000000000001\n",
      "Epoch 2 step 633: training loss: 1333.9857061027021\n",
      "Epoch 2 step 634: training accuarcy: 0.6125\n",
      "Epoch 2 step 634: training loss: 1340.793571145097\n",
      "Epoch 2 step 635: training accuarcy: 0.613\n",
      "Epoch 2 step 635: training loss: 1353.3695730328284\n",
      "Epoch 2 step 636: training accuarcy: 0.5995\n",
      "Epoch 2 step 636: training loss: 1355.4324232450206\n",
      "Epoch 2 step 637: training accuarcy: 0.596\n",
      "Epoch 2 step 637: training loss: 1348.2011599490818\n",
      "Epoch 2 step 638: training accuarcy: 0.6055\n",
      "Epoch 2 step 638: training loss: 1327.9384214026932\n",
      "Epoch 2 step 639: training accuarcy: 0.6225\n",
      "Epoch 2 step 639: training loss: 1340.215735706458\n",
      "Epoch 2 step 640: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 640: training loss: 1331.1870917757215\n",
      "Epoch 2 step 641: training accuarcy: 0.6225\n",
      "Epoch 2 step 641: training loss: 1337.9065188289567\n",
      "Epoch 2 step 642: training accuarcy: 0.608\n",
      "Epoch 2 step 642: training loss: 1341.0841393677329\n",
      "Epoch 2 step 643: training accuarcy: 0.6135\n",
      "Epoch 2 step 643: training loss: 1333.3826040406639\n",
      "Epoch 2 step 644: training accuarcy: 0.6245\n",
      "Epoch 2 step 644: training loss: 1329.661365131593\n",
      "Epoch 2 step 645: training accuarcy: 0.619\n",
      "Epoch 2 step 645: training loss: 1343.3028437569712\n",
      "Epoch 2 step 646: training accuarcy: 0.594\n",
      "Epoch 2 step 646: training loss: 1335.7801925510018\n",
      "Epoch 2 step 647: training accuarcy: 0.6105\n",
      "Epoch 2 step 647: training loss: 1340.4290854443816\n",
      "Epoch 2 step 648: training accuarcy: 0.616\n",
      "Epoch 2 step 648: training loss: 1337.2595542261845\n",
      "Epoch 2 step 649: training accuarcy: 0.6175\n",
      "Epoch 2 step 649: training loss: 1338.2987428195213\n",
      "Epoch 2 step 650: training accuarcy: 0.6215\n",
      "Epoch 2 step 650: training loss: 1320.2446694072116\n",
      "Epoch 2 step 651: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 651: training loss: 1327.9977506556022\n",
      "Epoch 2 step 652: training accuarcy: 0.621\n",
      "Epoch 2 step 652: training loss: 1330.8721950942431\n",
      "Epoch 2 step 653: training accuarcy: 0.6235\n",
      "Epoch 2 step 653: training loss: 1343.930962421012\n",
      "Epoch 2 step 654: training accuarcy: 0.615\n",
      "Epoch 2 step 654: training loss: 1324.3149359455485\n",
      "Epoch 2 step 655: training accuarcy: 0.6265000000000001\n",
      "Epoch 2 step 655: training loss: 1337.62862586437\n",
      "Epoch 2 step 656: training accuarcy: 0.621\n",
      "Epoch 2 step 656: training loss: 1334.7803606275393\n",
      "Epoch 2 step 657: training accuarcy: 0.6065\n",
      "Epoch 2 step 657: training loss: 1339.4712416907037\n",
      "Epoch 2 step 658: training accuarcy: 0.6045\n",
      "Epoch 2 step 658: training loss: 1331.2390982099396\n",
      "Epoch 2 step 659: training accuarcy: 0.613\n",
      "Epoch 2 step 659: training loss: 1328.3125062446693\n",
      "Epoch 2 step 660: training accuarcy: 0.6155\n",
      "Epoch 2 step 660: training loss: 1343.0887430032333\n",
      "Epoch 2 step 661: training accuarcy: 0.6215\n",
      "Epoch 2 step 661: training loss: 1328.3240410492099\n",
      "Epoch 2 step 662: training accuarcy: 0.6205\n",
      "Epoch 2 step 662: training loss: 1339.6913487023469\n",
      "Epoch 2 step 663: training accuarcy: 0.622\n",
      "Epoch 2 step 663: training loss: 1344.5171594644635\n",
      "Epoch 2 step 664: training accuarcy: 0.617\n",
      "Epoch 2 step 664: training loss: 1328.4516218748824\n",
      "Epoch 2 step 665: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 665: training loss: 1333.0093265254493\n",
      "Epoch 2 step 666: training accuarcy: 0.6105\n",
      "Epoch 2 step 666: training loss: 1354.9111064918038\n",
      "Epoch 2 step 667: training accuarcy: 0.61\n",
      "Epoch 2 step 667: training loss: 1330.795256375058\n",
      "Epoch 2 step 668: training accuarcy: 0.635\n",
      "Epoch 2 step 668: training loss: 1330.5650341222806\n",
      "Epoch 2 step 669: training accuarcy: 0.619\n",
      "Epoch 2 step 669: training loss: 1340.9180775779541\n",
      "Epoch 2 step 670: training accuarcy: 0.6185\n",
      "Epoch 2 step 670: training loss: 1333.439370272121\n",
      "Epoch 2 step 671: training accuarcy: 0.6125\n",
      "Epoch 2 step 671: training loss: 1334.1068463964775\n",
      "Epoch 2 step 672: training accuarcy: 0.624\n",
      "Epoch 2 step 672: training loss: 1320.034027839076\n",
      "Epoch 2 step 673: training accuarcy: 0.6385000000000001\n",
      "Epoch 2 step 673: training loss: 1346.2781199061887\n",
      "Epoch 2 step 674: training accuarcy: 0.613\n",
      "Epoch 2 step 674: training loss: 1339.9083935039837\n",
      "Epoch 2 step 675: training accuarcy: 0.6195\n",
      "Epoch 2 step 675: training loss: 1332.0347926099057\n",
      "Epoch 2 step 676: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 676: training loss: 1330.8395925682437\n",
      "Epoch 2 step 677: training accuarcy: 0.619\n",
      "Epoch 2 step 677: training loss: 1345.815708850591\n",
      "Epoch 2 step 678: training accuarcy: 0.6125\n",
      "Epoch 2 step 678: training loss: 1325.000169126386\n",
      "Epoch 2 step 679: training accuarcy: 0.639\n",
      "Epoch 2 step 679: training loss: 1339.3248175845536\n",
      "Epoch 2 step 680: training accuarcy: 0.61\n",
      "Epoch 2 step 680: training loss: 1339.0229119439318\n",
      "Epoch 2 step 681: training accuarcy: 0.601\n",
      "Epoch 2 step 681: training loss: 1340.0033810967473\n",
      "Epoch 2 step 682: training accuarcy: 0.6155\n",
      "Epoch 2 step 682: training loss: 1341.570369882157\n",
      "Epoch 2 step 683: training accuarcy: 0.606\n",
      "Epoch 2 step 683: training loss: 1352.7191135023534\n",
      "Epoch 2 step 684: training accuarcy: 0.5945\n",
      "Epoch 2 step 684: training loss: 1334.8367576544913\n",
      "Epoch 2 step 685: training accuarcy: 0.618\n",
      "Epoch 2 step 685: training loss: 1351.6933204639918\n",
      "Epoch 2 step 686: training accuarcy: 0.586\n",
      "Epoch 2 step 686: training loss: 1341.506148999977\n",
      "Epoch 2 step 687: training accuarcy: 0.6085\n",
      "Epoch 2 step 687: training loss: 1345.5198829202482\n",
      "Epoch 2 step 688: training accuarcy: 0.602\n",
      "Epoch 2 step 688: training loss: 1346.068401401125\n",
      "Epoch 2 step 689: training accuarcy: 0.609\n",
      "Epoch 2 step 689: training loss: 1336.2351052728734\n",
      "Epoch 2 step 690: training accuarcy: 0.613\n",
      "Epoch 2 step 690: training loss: 1334.3112253569084\n",
      "Epoch 2 step 691: training accuarcy: 0.6225\n",
      "Epoch 2 step 691: training loss: 1338.7377516006152\n",
      "Epoch 2 step 692: training accuarcy: 0.609\n",
      "Epoch 2 step 692: training loss: 1330.7663632200838\n",
      "Epoch 2 step 693: training accuarcy: 0.6185\n",
      "Epoch 2 step 693: training loss: 1339.417748374742\n",
      "Epoch 2 step 694: training accuarcy: 0.606\n",
      "Epoch 2 step 694: training loss: 1351.179121023727\n",
      "Epoch 2 step 695: training accuarcy: 0.6055\n",
      "Epoch 2 step 695: training loss: 1343.8008263539248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 696: training accuarcy: 0.6205\n",
      "Epoch 2 step 696: training loss: 1340.6197349661902\n",
      "Epoch 2 step 697: training accuarcy: 0.618\n",
      "Epoch 2 step 697: training loss: 1351.9279247308089\n",
      "Epoch 2 step 698: training accuarcy: 0.6065\n",
      "Epoch 2 step 698: training loss: 1345.480098292504\n",
      "Epoch 2 step 699: training accuarcy: 0.606\n",
      "Epoch 2 step 699: training loss: 1343.0832931770265\n",
      "Epoch 2 step 700: training accuarcy: 0.604\n",
      "Epoch 2 step 700: training loss: 1325.0833989815007\n",
      "Epoch 2 step 701: training accuarcy: 0.626\n",
      "Epoch 2 step 701: training loss: 1343.1910135348285\n",
      "Epoch 2 step 702: training accuarcy: 0.6085\n",
      "Epoch 2 step 702: training loss: 1324.5598177215418\n",
      "Epoch 2 step 703: training accuarcy: 0.6145\n",
      "Epoch 2 step 703: training loss: 1339.0203777579616\n",
      "Epoch 2 step 704: training accuarcy: 0.634\n",
      "Epoch 2 step 704: training loss: 1336.8452828777763\n",
      "Epoch 2 step 705: training accuarcy: 0.6075\n",
      "Epoch 2 step 705: training loss: 1345.1111443199648\n",
      "Epoch 2 step 706: training accuarcy: 0.6125\n",
      "Epoch 2 step 706: training loss: 1346.0767335359415\n",
      "Epoch 2 step 707: training accuarcy: 0.608\n",
      "Epoch 2 step 707: training loss: 1345.3523732202104\n",
      "Epoch 2 step 708: training accuarcy: 0.6125\n",
      "Epoch 2 step 708: training loss: 1353.7579598393347\n",
      "Epoch 2 step 709: training accuarcy: 0.597\n",
      "Epoch 2 step 709: training loss: 1333.869947565475\n",
      "Epoch 2 step 710: training accuarcy: 0.599\n",
      "Epoch 2 step 710: training loss: 1341.0326109645605\n",
      "Epoch 2 step 711: training accuarcy: 0.609\n",
      "Epoch 2 step 711: training loss: 1346.1653531680859\n",
      "Epoch 2 step 712: training accuarcy: 0.6075\n",
      "Epoch 2 step 712: training loss: 1349.1800452768375\n",
      "Epoch 2 step 713: training accuarcy: 0.606\n",
      "Epoch 2 step 713: training loss: 1351.9525259261593\n",
      "Epoch 2 step 714: training accuarcy: 0.596\n",
      "Epoch 2 step 714: training loss: 1342.5203543137575\n",
      "Epoch 2 step 715: training accuarcy: 0.605\n",
      "Epoch 2 step 715: training loss: 1346.3433040175182\n",
      "Epoch 2 step 716: training accuarcy: 0.61\n",
      "Epoch 2 step 716: training loss: 1347.1513730448498\n",
      "Epoch 2 step 717: training accuarcy: 0.6105\n",
      "Epoch 2 step 717: training loss: 1334.3061828569973\n",
      "Epoch 2 step 718: training accuarcy: 0.625\n",
      "Epoch 2 step 718: training loss: 1328.4861774196602\n",
      "Epoch 2 step 719: training accuarcy: 0.63\n",
      "Epoch 2 step 719: training loss: 1330.3034379582862\n",
      "Epoch 2 step 720: training accuarcy: 0.624\n",
      "Epoch 2 step 720: training loss: 1325.3425187407706\n",
      "Epoch 2 step 721: training accuarcy: 0.631\n",
      "Epoch 2 step 721: training loss: 1328.571432673291\n",
      "Epoch 2 step 722: training accuarcy: 0.628\n",
      "Epoch 2 step 722: training loss: 1318.8214767777304\n",
      "Epoch 2 step 723: training accuarcy: 0.638\n",
      "Epoch 2 step 723: training loss: 1334.4292435555699\n",
      "Epoch 2 step 724: training accuarcy: 0.621\n",
      "Epoch 2 step 724: training loss: 1327.95793996293\n",
      "Epoch 2 step 725: training accuarcy: 0.622\n",
      "Epoch 2 step 725: training loss: 1338.3069856798295\n",
      "Epoch 2 step 726: training accuarcy: 0.6205\n",
      "Epoch 2 step 726: training loss: 1340.562426362937\n",
      "Epoch 2 step 727: training accuarcy: 0.6165\n",
      "Epoch 2 step 727: training loss: 1322.500519018794\n",
      "Epoch 2 step 728: training accuarcy: 0.6255000000000001\n",
      "Epoch 2 step 728: training loss: 1344.8960023846425\n",
      "Epoch 2 step 729: training accuarcy: 0.6065\n",
      "Epoch 2 step 729: training loss: 1330.2217826566277\n",
      "Epoch 2 step 730: training accuarcy: 0.6185\n",
      "Epoch 2 step 730: training loss: 1328.378869319622\n",
      "Epoch 2 step 731: training accuarcy: 0.623\n",
      "Epoch 2 step 731: training loss: 1324.9769706663599\n",
      "Epoch 2 step 732: training accuarcy: 0.61\n",
      "Epoch 2 step 732: training loss: 1324.0327124833275\n",
      "Epoch 2 step 733: training accuarcy: 0.6225\n",
      "Epoch 2 step 733: training loss: 1328.675723033765\n",
      "Epoch 2 step 734: training accuarcy: 0.6355000000000001\n",
      "Epoch 2 step 734: training loss: 1324.9729014294226\n",
      "Epoch 2 step 735: training accuarcy: 0.633\n",
      "Epoch 2 step 735: training loss: 1320.931232140343\n",
      "Epoch 2 step 736: training accuarcy: 0.6315000000000001\n",
      "Epoch 2 step 736: training loss: 1331.619130303391\n",
      "Epoch 2 step 737: training accuarcy: 0.621\n",
      "Epoch 2 step 737: training loss: 1345.4084061859085\n",
      "Epoch 2 step 738: training accuarcy: 0.608\n",
      "Epoch 2 step 738: training loss: 1340.2729705860852\n",
      "Epoch 2 step 739: training accuarcy: 0.603\n",
      "Epoch 2 step 739: training loss: 1345.9919820426653\n",
      "Epoch 2 step 740: training accuarcy: 0.601\n",
      "Epoch 2 step 740: training loss: 1324.0689290115972\n",
      "Epoch 2 step 741: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 741: training loss: 1331.0954955711738\n",
      "Epoch 2 step 742: training accuarcy: 0.6195\n",
      "Epoch 2 step 742: training loss: 1341.503860120492\n",
      "Epoch 2 step 743: training accuarcy: 0.6045\n",
      "Epoch 2 step 743: training loss: 1335.1721618240567\n",
      "Epoch 2 step 744: training accuarcy: 0.629\n",
      "Epoch 2 step 744: training loss: 1350.7834461894313\n",
      "Epoch 2 step 745: training accuarcy: 0.5975\n",
      "Epoch 2 step 745: training loss: 1330.7285370011837\n",
      "Epoch 2 step 746: training accuarcy: 0.6165\n",
      "Epoch 2 step 746: training loss: 1332.7914666062077\n",
      "Epoch 2 step 747: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 747: training loss: 1337.597812120429\n",
      "Epoch 2 step 748: training accuarcy: 0.612\n",
      "Epoch 2 step 748: training loss: 1350.233395183445\n",
      "Epoch 2 step 749: training accuarcy: 0.611\n",
      "Epoch 2 step 749: training loss: 1336.091163016177\n",
      "Epoch 2 step 750: training accuarcy: 0.6195\n",
      "Epoch 2 step 750: training loss: 1323.56553663156\n",
      "Epoch 2 step 751: training accuarcy: 0.637\n",
      "Epoch 2 step 751: training loss: 1330.69441294501\n",
      "Epoch 2 step 752: training accuarcy: 0.6145\n",
      "Epoch 2 step 752: training loss: 1339.5008207409799\n",
      "Epoch 2 step 753: training accuarcy: 0.6305000000000001\n",
      "Epoch 2 step 753: training loss: 1339.2666613357355\n",
      "Epoch 2 step 754: training accuarcy: 0.619\n",
      "Epoch 2 step 754: training loss: 1336.9026995272102\n",
      "Epoch 2 step 755: training accuarcy: 0.613\n",
      "Epoch 2 step 755: training loss: 1331.3981517341929\n",
      "Epoch 2 step 756: training accuarcy: 0.619\n",
      "Epoch 2 step 756: training loss: 1342.6432852215548\n",
      "Epoch 2 step 757: training accuarcy: 0.6165\n",
      "Epoch 2 step 757: training loss: 1332.528380592876\n",
      "Epoch 2 step 758: training accuarcy: 0.624\n",
      "Epoch 2 step 758: training loss: 1325.710064514029\n",
      "Epoch 2 step 759: training accuarcy: 0.621\n",
      "Epoch 2 step 759: training loss: 1326.7404828871713\n",
      "Epoch 2 step 760: training accuarcy: 0.6205\n",
      "Epoch 2 step 760: training loss: 1334.1606972051577\n",
      "Epoch 2 step 761: training accuarcy: 0.614\n",
      "Epoch 2 step 761: training loss: 1329.6208333246639\n",
      "Epoch 2 step 762: training accuarcy: 0.6235\n",
      "Epoch 2 step 762: training loss: 1327.9549340864917\n",
      "Epoch 2 step 763: training accuarcy: 0.627\n",
      "Epoch 2 step 763: training loss: 1319.352971426937\n",
      "Epoch 2 step 764: training accuarcy: 0.641\n",
      "Epoch 2 step 764: training loss: 1335.6300581030307\n",
      "Epoch 2 step 765: training accuarcy: 0.6275000000000001\n",
      "Epoch 2 step 765: training loss: 1327.8279701643573\n",
      "Epoch 2 step 766: training accuarcy: 0.628\n",
      "Epoch 2 step 766: training loss: 1338.7817963249483\n",
      "Epoch 2 step 767: training accuarcy: 0.6\n",
      "Epoch 2 step 767: training loss: 1350.7508590074353\n",
      "Epoch 2 step 768: training accuarcy: 0.61\n",
      "Epoch 2 step 768: training loss: 1343.180391827448\n",
      "Epoch 2 step 769: training accuarcy: 0.601\n",
      "Epoch 2 step 769: training loss: 1330.9948568577083\n",
      "Epoch 2 step 770: training accuarcy: 0.6155\n",
      "Epoch 2 step 770: training loss: 1332.5526888104962\n",
      "Epoch 2 step 771: training accuarcy: 0.6165\n",
      "Epoch 2 step 771: training loss: 1342.761832877243\n",
      "Epoch 2 step 772: training accuarcy: 0.5945\n",
      "Epoch 2 step 772: training loss: 1342.7233611282413\n",
      "Epoch 2 step 773: training accuarcy: 0.607\n",
      "Epoch 2 step 773: training loss: 1336.3419070865712\n",
      "Epoch 2 step 774: training accuarcy: 0.6175\n",
      "Epoch 2 step 774: training loss: 1330.0815619246441\n",
      "Epoch 2 step 775: training accuarcy: 0.6145\n",
      "Epoch 2 step 775: training loss: 1329.5503469631528\n",
      "Epoch 2 step 776: training accuarcy: 0.618\n",
      "Epoch 2 step 776: training loss: 1343.9071304441181\n",
      "Epoch 2 step 777: training accuarcy: 0.602\n",
      "Epoch 2 step 777: training loss: 1335.52116400913\n",
      "Epoch 2 step 778: training accuarcy: 0.6145\n",
      "Epoch 2 step 778: training loss: 1329.0769297625413\n",
      "Epoch 2 step 779: training accuarcy: 0.616\n",
      "Epoch 2 step 779: training loss: 1323.6880121722106\n",
      "Epoch 2 step 780: training accuarcy: 0.6195\n",
      "Epoch 2 step 780: training loss: 1334.7402009796872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 781: training accuarcy: 0.6325000000000001\n",
      "Epoch 2 step 781: training loss: 1320.5925265299336\n",
      "Epoch 2 step 782: training accuarcy: 0.631\n",
      "Epoch 2 step 782: training loss: 1296.802983699052\n",
      "Epoch 2 step 783: training accuarcy: 0.644\n",
      "Epoch 2 step 783: training loss: 1329.8728215776778\n",
      "Epoch 2 step 784: training accuarcy: 0.62\n",
      "Epoch 2 step 784: training loss: 1342.9708381433172\n",
      "Epoch 2 step 785: training accuarcy: 0.5975\n",
      "Epoch 2 step 785: training loss: 1339.16290007489\n",
      "Epoch 2 step 786: training accuarcy: 0.6135\n",
      "Epoch 2 step 786: training loss: 1342.003680902567\n",
      "Epoch 2 step 787: training accuarcy: 0.615\n",
      "Epoch 2 step 787: training loss: 1326.9764468283445\n",
      "Epoch 2 step 788: training accuarcy: 0.6275000000000001\n",
      "Epoch 2 step 788: training loss: 533.4391747115245\n",
      "Epoch 2 step 789: training accuarcy: 0.617948717948718\n",
      "Epoch 2: train loss 1334.020480710248, train accuarcy 0.6144036650657654\n",
      "Epoch 2: valid loss 6696.745640292126, valid accuarcy 0.585203230381012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 3/5 [16:20<10:52, 326.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 1330.755625683686\n",
      "Epoch 3 step 790: training accuarcy: 0.606\n",
      "Epoch 3 step 790: training loss: 1329.6213964746\n",
      "Epoch 3 step 791: training accuarcy: 0.604\n",
      "Epoch 3 step 791: training loss: 1333.3386167609624\n",
      "Epoch 3 step 792: training accuarcy: 0.616\n",
      "Epoch 3 step 792: training loss: 1325.8297232915388\n",
      "Epoch 3 step 793: training accuarcy: 0.6215\n",
      "Epoch 3 step 793: training loss: 1334.043270890644\n",
      "Epoch 3 step 794: training accuarcy: 0.617\n",
      "Epoch 3 step 794: training loss: 1349.4315718901357\n",
      "Epoch 3 step 795: training accuarcy: 0.616\n",
      "Epoch 3 step 795: training loss: 1329.556492265466\n",
      "Epoch 3 step 796: training accuarcy: 0.6265000000000001\n",
      "Epoch 3 step 796: training loss: 1333.0714773486063\n",
      "Epoch 3 step 797: training accuarcy: 0.6245\n",
      "Epoch 3 step 797: training loss: 1326.120102279428\n",
      "Epoch 3 step 798: training accuarcy: 0.614\n",
      "Epoch 3 step 798: training loss: 1341.0007845123946\n",
      "Epoch 3 step 799: training accuarcy: 0.6205\n",
      "Epoch 3 step 799: training loss: 1331.3377636553605\n",
      "Epoch 3 step 800: training accuarcy: 0.6195\n",
      "Epoch 3 step 800: training loss: 1333.8369620036108\n",
      "Epoch 3 step 801: training accuarcy: 0.6085\n",
      "Epoch 3 step 801: training loss: 1347.0121021245902\n",
      "Epoch 3 step 802: training accuarcy: 0.5915\n",
      "Epoch 3 step 802: training loss: 1334.6157725093165\n",
      "Epoch 3 step 803: training accuarcy: 0.6225\n",
      "Epoch 3 step 803: training loss: 1335.3035076588862\n",
      "Epoch 3 step 804: training accuarcy: 0.6085\n",
      "Epoch 3 step 804: training loss: 1339.71900876966\n",
      "Epoch 3 step 805: training accuarcy: 0.618\n",
      "Epoch 3 step 805: training loss: 1338.825826589002\n",
      "Epoch 3 step 806: training accuarcy: 0.6\n",
      "Epoch 3 step 806: training loss: 1344.410047667593\n",
      "Epoch 3 step 807: training accuarcy: 0.6055\n",
      "Epoch 3 step 807: training loss: 1327.9465283374527\n",
      "Epoch 3 step 808: training accuarcy: 0.621\n",
      "Epoch 3 step 808: training loss: 1323.951398070082\n",
      "Epoch 3 step 809: training accuarcy: 0.63\n",
      "Epoch 3 step 809: training loss: 1335.1995877273837\n",
      "Epoch 3 step 810: training accuarcy: 0.616\n",
      "Epoch 3 step 810: training loss: 1330.6628128018808\n",
      "Epoch 3 step 811: training accuarcy: 0.6285000000000001\n",
      "Epoch 3 step 811: training loss: 1339.8677856253018\n",
      "Epoch 3 step 812: training accuarcy: 0.616\n",
      "Epoch 3 step 812: training loss: 1333.9301066157236\n",
      "Epoch 3 step 813: training accuarcy: 0.623\n",
      "Epoch 3 step 813: training loss: 1340.0979908924191\n",
      "Epoch 3 step 814: training accuarcy: 0.613\n",
      "Epoch 3 step 814: training loss: 1333.2296719392305\n",
      "Epoch 3 step 815: training accuarcy: 0.6175\n",
      "Epoch 3 step 815: training loss: 1335.4926517387928\n",
      "Epoch 3 step 816: training accuarcy: 0.6155\n",
      "Epoch 3 step 816: training loss: 1339.1624771179597\n",
      "Epoch 3 step 817: training accuarcy: 0.608\n",
      "Epoch 3 step 817: training loss: 1339.7983884639764\n",
      "Epoch 3 step 818: training accuarcy: 0.619\n",
      "Epoch 3 step 818: training loss: 1344.384395554078\n",
      "Epoch 3 step 819: training accuarcy: 0.6095\n",
      "Epoch 3 step 819: training loss: 1325.813133987535\n",
      "Epoch 3 step 820: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 820: training loss: 1347.631780343172\n",
      "Epoch 3 step 821: training accuarcy: 0.612\n",
      "Epoch 3 step 821: training loss: 1361.075215735354\n",
      "Epoch 3 step 822: training accuarcy: 0.5885\n",
      "Epoch 3 step 822: training loss: 1329.3125974106956\n",
      "Epoch 3 step 823: training accuarcy: 0.6145\n",
      "Epoch 3 step 823: training loss: 1327.6504346648778\n",
      "Epoch 3 step 824: training accuarcy: 0.617\n",
      "Epoch 3 step 824: training loss: 1336.7085770452761\n",
      "Epoch 3 step 825: training accuarcy: 0.6125\n",
      "Epoch 3 step 825: training loss: 1353.7165554229396\n",
      "Epoch 3 step 826: training accuarcy: 0.588\n",
      "Epoch 3 step 826: training loss: 1338.9167659720379\n",
      "Epoch 3 step 827: training accuarcy: 0.6095\n",
      "Epoch 3 step 827: training loss: 1328.9849349710657\n",
      "Epoch 3 step 828: training accuarcy: 0.62\n",
      "Epoch 3 step 828: training loss: 1342.5971802492031\n",
      "Epoch 3 step 829: training accuarcy: 0.614\n",
      "Epoch 3 step 829: training loss: 1335.8856569896402\n",
      "Epoch 3 step 830: training accuarcy: 0.622\n",
      "Epoch 3 step 830: training loss: 1333.435181837819\n",
      "Epoch 3 step 831: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 831: training loss: 1343.7167983280897\n",
      "Epoch 3 step 832: training accuarcy: 0.6005\n",
      "Epoch 3 step 832: training loss: 1332.3174341063207\n",
      "Epoch 3 step 833: training accuarcy: 0.6185\n",
      "Epoch 3 step 833: training loss: 1329.4445352905288\n",
      "Epoch 3 step 834: training accuarcy: 0.6325000000000001\n",
      "Epoch 3 step 834: training loss: 1330.5372734044768\n",
      "Epoch 3 step 835: training accuarcy: 0.6345000000000001\n",
      "Epoch 3 step 835: training loss: 1343.056019480398\n",
      "Epoch 3 step 836: training accuarcy: 0.6025\n",
      "Epoch 3 step 836: training loss: 1324.3990252610206\n",
      "Epoch 3 step 837: training accuarcy: 0.632\n",
      "Epoch 3 step 837: training loss: 1338.8627305082935\n",
      "Epoch 3 step 838: training accuarcy: 0.6165\n",
      "Epoch 3 step 838: training loss: 1343.8885334791523\n",
      "Epoch 3 step 839: training accuarcy: 0.6175\n",
      "Epoch 3 step 839: training loss: 1320.4794039271476\n",
      "Epoch 3 step 840: training accuarcy: 0.6325000000000001\n",
      "Epoch 3 step 840: training loss: 1324.567999286966\n",
      "Epoch 3 step 841: training accuarcy: 0.62\n",
      "Epoch 3 step 841: training loss: 1332.658300294514\n",
      "Epoch 3 step 842: training accuarcy: 0.6145\n",
      "Epoch 3 step 842: training loss: 1339.8484471244735\n",
      "Epoch 3 step 843: training accuarcy: 0.6065\n",
      "Epoch 3 step 843: training loss: 1349.4221513081834\n",
      "Epoch 3 step 844: training accuarcy: 0.6045\n",
      "Epoch 3 step 844: training loss: 1347.5157248477508\n",
      "Epoch 3 step 845: training accuarcy: 0.605\n",
      "Epoch 3 step 845: training loss: 1332.5925233262662\n",
      "Epoch 3 step 846: training accuarcy: 0.632\n",
      "Epoch 3 step 846: training loss: 1321.3207187161656\n",
      "Epoch 3 step 847: training accuarcy: 0.6345000000000001\n",
      "Epoch 3 step 847: training loss: 1330.1219954853057\n",
      "Epoch 3 step 848: training accuarcy: 0.629\n",
      "Epoch 3 step 848: training loss: 1332.7498590935913\n",
      "Epoch 3 step 849: training accuarcy: 0.636\n",
      "Epoch 3 step 849: training loss: 1323.2587765900225\n",
      "Epoch 3 step 850: training accuarcy: 0.628\n",
      "Epoch 3 step 850: training loss: 1353.8569683282983\n",
      "Epoch 3 step 851: training accuarcy: 0.595\n",
      "Epoch 3 step 851: training loss: 1337.8601489832172\n",
      "Epoch 3 step 852: training accuarcy: 0.6235\n",
      "Epoch 3 step 852: training loss: 1345.7873065577703\n",
      "Epoch 3 step 853: training accuarcy: 0.615\n",
      "Epoch 3 step 853: training loss: 1338.4610701472136\n",
      "Epoch 3 step 854: training accuarcy: 0.6095\n",
      "Epoch 3 step 854: training loss: 1318.0810266298006\n",
      "Epoch 3 step 855: training accuarcy: 0.6375000000000001\n",
      "Epoch 3 step 855: training loss: 1331.8684059359211\n",
      "Epoch 3 step 856: training accuarcy: 0.62\n",
      "Epoch 3 step 856: training loss: 1337.5320424683814\n",
      "Epoch 3 step 857: training accuarcy: 0.618\n",
      "Epoch 3 step 857: training loss: 1331.178348843015\n",
      "Epoch 3 step 858: training accuarcy: 0.6235\n",
      "Epoch 3 step 858: training loss: 1322.5938251695268\n",
      "Epoch 3 step 859: training accuarcy: 0.629\n",
      "Epoch 3 step 859: training loss: 1338.6120015085933\n",
      "Epoch 3 step 860: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 860: training loss: 1323.9665824482133\n",
      "Epoch 3 step 861: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 861: training loss: 1337.3374191458815\n",
      "Epoch 3 step 862: training accuarcy: 0.619\n",
      "Epoch 3 step 862: training loss: 1352.1509579869069\n",
      "Epoch 3 step 863: training accuarcy: 0.6075\n",
      "Epoch 3 step 863: training loss: 1329.376970897192\n",
      "Epoch 3 step 864: training accuarcy: 0.616\n",
      "Epoch 3 step 864: training loss: 1332.3770753936114\n",
      "Epoch 3 step 865: training accuarcy: 0.6145\n",
      "Epoch 3 step 865: training loss: 1332.2942252360258\n",
      "Epoch 3 step 866: training accuarcy: 0.6195\n",
      "Epoch 3 step 866: training loss: 1330.9356017638738\n",
      "Epoch 3 step 867: training accuarcy: 0.6175\n",
      "Epoch 3 step 867: training loss: 1333.4665504361406\n",
      "Epoch 3 step 868: training accuarcy: 0.6215\n",
      "Epoch 3 step 868: training loss: 1344.3226741669282\n",
      "Epoch 3 step 869: training accuarcy: 0.6055\n",
      "Epoch 3 step 869: training loss: 1337.2642388062463\n",
      "Epoch 3 step 870: training accuarcy: 0.6055\n",
      "Epoch 3 step 870: training loss: 1325.835162652678\n",
      "Epoch 3 step 871: training accuarcy: 0.606\n",
      "Epoch 3 step 871: training loss: 1327.7019221741566\n",
      "Epoch 3 step 872: training accuarcy: 0.6195\n",
      "Epoch 3 step 872: training loss: 1328.4733769736824\n",
      "Epoch 3 step 873: training accuarcy: 0.632\n",
      "Epoch 3 step 873: training loss: 1326.673156407126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 874: training accuarcy: 0.6285000000000001\n",
      "Epoch 3 step 874: training loss: 1335.3852106168008\n",
      "Epoch 3 step 875: training accuarcy: 0.6155\n",
      "Epoch 3 step 875: training loss: 1346.4113788591872\n",
      "Epoch 3 step 876: training accuarcy: 0.6\n",
      "Epoch 3 step 876: training loss: 1346.342010581684\n",
      "Epoch 3 step 877: training accuarcy: 0.6045\n",
      "Epoch 3 step 877: training loss: 1340.1665353775084\n",
      "Epoch 3 step 878: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 878: training loss: 1340.9346420400086\n",
      "Epoch 3 step 879: training accuarcy: 0.606\n",
      "Epoch 3 step 879: training loss: 1335.4036031536752\n",
      "Epoch 3 step 880: training accuarcy: 0.6075\n",
      "Epoch 3 step 880: training loss: 1330.0087137723676\n",
      "Epoch 3 step 881: training accuarcy: 0.619\n",
      "Epoch 3 step 881: training loss: 1326.217285339248\n",
      "Epoch 3 step 882: training accuarcy: 0.617\n",
      "Epoch 3 step 882: training loss: 1328.740771391117\n",
      "Epoch 3 step 883: training accuarcy: 0.6165\n",
      "Epoch 3 step 883: training loss: 1336.8934080590413\n",
      "Epoch 3 step 884: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 884: training loss: 1320.820019993125\n",
      "Epoch 3 step 885: training accuarcy: 0.628\n",
      "Epoch 3 step 885: training loss: 1347.862592941087\n",
      "Epoch 3 step 886: training accuarcy: 0.5975\n",
      "Epoch 3 step 886: training loss: 1315.8371736323245\n",
      "Epoch 3 step 887: training accuarcy: 0.633\n",
      "Epoch 3 step 887: training loss: 1332.5921432825892\n",
      "Epoch 3 step 888: training accuarcy: 0.6155\n",
      "Epoch 3 step 888: training loss: 1333.6305687987365\n",
      "Epoch 3 step 889: training accuarcy: 0.6225\n",
      "Epoch 3 step 889: training loss: 1339.4971376189094\n",
      "Epoch 3 step 890: training accuarcy: 0.611\n",
      "Epoch 3 step 890: training loss: 1343.2670748851497\n",
      "Epoch 3 step 891: training accuarcy: 0.598\n",
      "Epoch 3 step 891: training loss: 1338.5002707421258\n",
      "Epoch 3 step 892: training accuarcy: 0.6125\n",
      "Epoch 3 step 892: training loss: 1336.896188965939\n",
      "Epoch 3 step 893: training accuarcy: 0.607\n",
      "Epoch 3 step 893: training loss: 1336.9892980050718\n",
      "Epoch 3 step 894: training accuarcy: 0.611\n",
      "Epoch 3 step 894: training loss: 1346.034980887434\n",
      "Epoch 3 step 895: training accuarcy: 0.6075\n",
      "Epoch 3 step 895: training loss: 1330.9039792906842\n",
      "Epoch 3 step 896: training accuarcy: 0.6115\n",
      "Epoch 3 step 896: training loss: 1334.5840027998436\n",
      "Epoch 3 step 897: training accuarcy: 0.6255000000000001\n",
      "Epoch 3 step 897: training loss: 1326.5800735010396\n",
      "Epoch 3 step 898: training accuarcy: 0.6175\n",
      "Epoch 3 step 898: training loss: 1342.8701686557167\n",
      "Epoch 3 step 899: training accuarcy: 0.6065\n",
      "Epoch 3 step 899: training loss: 1346.0358501292687\n",
      "Epoch 3 step 900: training accuarcy: 0.6065\n",
      "Epoch 3 step 900: training loss: 1333.7900907551827\n",
      "Epoch 3 step 901: training accuarcy: 0.62\n",
      "Epoch 3 step 901: training loss: 1343.6406107107312\n",
      "Epoch 3 step 902: training accuarcy: 0.6035\n",
      "Epoch 3 step 902: training loss: 1347.313889999397\n",
      "Epoch 3 step 903: training accuarcy: 0.6135\n",
      "Epoch 3 step 903: training loss: 1333.9323032693349\n",
      "Epoch 3 step 904: training accuarcy: 0.6225\n",
      "Epoch 3 step 904: training loss: 1331.173284774309\n",
      "Epoch 3 step 905: training accuarcy: 0.6225\n",
      "Epoch 3 step 905: training loss: 1336.8995938632318\n",
      "Epoch 3 step 906: training accuarcy: 0.606\n",
      "Epoch 3 step 906: training loss: 1336.5881504880135\n",
      "Epoch 3 step 907: training accuarcy: 0.617\n",
      "Epoch 3 step 907: training loss: 1337.5499070270707\n",
      "Epoch 3 step 908: training accuarcy: 0.611\n",
      "Epoch 3 step 908: training loss: 1334.065200567618\n",
      "Epoch 3 step 909: training accuarcy: 0.6245\n",
      "Epoch 3 step 909: training loss: 1331.983749613335\n",
      "Epoch 3 step 910: training accuarcy: 0.6345000000000001\n",
      "Epoch 3 step 910: training loss: 1335.7398568202416\n",
      "Epoch 3 step 911: training accuarcy: 0.615\n",
      "Epoch 3 step 911: training loss: 1343.9160437101157\n",
      "Epoch 3 step 912: training accuarcy: 0.604\n",
      "Epoch 3 step 912: training loss: 1328.8699388170076\n",
      "Epoch 3 step 913: training accuarcy: 0.6205\n",
      "Epoch 3 step 913: training loss: 1326.4920452049623\n",
      "Epoch 3 step 914: training accuarcy: 0.6365000000000001\n",
      "Epoch 3 step 914: training loss: 1325.1948443967751\n",
      "Epoch 3 step 915: training accuarcy: 0.6215\n",
      "Epoch 3 step 915: training loss: 1339.6537404492942\n",
      "Epoch 3 step 916: training accuarcy: 0.6205\n",
      "Epoch 3 step 916: training loss: 1332.829829146084\n",
      "Epoch 3 step 917: training accuarcy: 0.6205\n",
      "Epoch 3 step 917: training loss: 1327.8818025143696\n",
      "Epoch 3 step 918: training accuarcy: 0.6125\n",
      "Epoch 3 step 918: training loss: 1341.096700893923\n",
      "Epoch 3 step 919: training accuarcy: 0.604\n",
      "Epoch 3 step 919: training loss: 1333.5703280046469\n",
      "Epoch 3 step 920: training accuarcy: 0.6205\n",
      "Epoch 3 step 920: training loss: 1319.7817210806754\n",
      "Epoch 3 step 921: training accuarcy: 0.633\n",
      "Epoch 3 step 921: training loss: 1331.0931523240783\n",
      "Epoch 3 step 922: training accuarcy: 0.6265000000000001\n",
      "Epoch 3 step 922: training loss: 1343.3293414297545\n",
      "Epoch 3 step 923: training accuarcy: 0.6035\n",
      "Epoch 3 step 923: training loss: 1325.4006695141413\n",
      "Epoch 3 step 924: training accuarcy: 0.6195\n",
      "Epoch 3 step 924: training loss: 1319.8496549679242\n",
      "Epoch 3 step 925: training accuarcy: 0.6365000000000001\n",
      "Epoch 3 step 925: training loss: 1340.9526056173029\n",
      "Epoch 3 step 926: training accuarcy: 0.6105\n",
      "Epoch 3 step 926: training loss: 1342.8871993681607\n",
      "Epoch 3 step 927: training accuarcy: 0.6085\n",
      "Epoch 3 step 927: training loss: 1335.133826222446\n",
      "Epoch 3 step 928: training accuarcy: 0.6245\n",
      "Epoch 3 step 928: training loss: 1335.731411861439\n",
      "Epoch 3 step 929: training accuarcy: 0.605\n",
      "Epoch 3 step 929: training loss: 1338.2269411717118\n",
      "Epoch 3 step 930: training accuarcy: 0.602\n",
      "Epoch 3 step 930: training loss: 1328.633307003062\n",
      "Epoch 3 step 931: training accuarcy: 0.632\n",
      "Epoch 3 step 931: training loss: 1338.3889529602188\n",
      "Epoch 3 step 932: training accuarcy: 0.593\n",
      "Epoch 3 step 932: training loss: 1342.0459212138358\n",
      "Epoch 3 step 933: training accuarcy: 0.616\n",
      "Epoch 3 step 933: training loss: 1324.076086343244\n",
      "Epoch 3 step 934: training accuarcy: 0.626\n",
      "Epoch 3 step 934: training loss: 1317.9221839881288\n",
      "Epoch 3 step 935: training accuarcy: 0.627\n",
      "Epoch 3 step 935: training loss: 1296.9723119478585\n",
      "Epoch 3 step 936: training accuarcy: 0.6445\n",
      "Epoch 3 step 936: training loss: 1336.7342635090545\n",
      "Epoch 3 step 937: training accuarcy: 0.629\n",
      "Epoch 3 step 937: training loss: 1336.4446163540954\n",
      "Epoch 3 step 938: training accuarcy: 0.6095\n",
      "Epoch 3 step 938: training loss: 1330.218273820781\n",
      "Epoch 3 step 939: training accuarcy: 0.623\n",
      "Epoch 3 step 939: training loss: 1334.5093711847105\n",
      "Epoch 3 step 940: training accuarcy: 0.614\n",
      "Epoch 3 step 940: training loss: 1336.2618019684348\n",
      "Epoch 3 step 941: training accuarcy: 0.6125\n",
      "Epoch 3 step 941: training loss: 1313.6841313774999\n",
      "Epoch 3 step 942: training accuarcy: 0.632\n",
      "Epoch 3 step 942: training loss: 1337.7717833186043\n",
      "Epoch 3 step 943: training accuarcy: 0.6185\n",
      "Epoch 3 step 943: training loss: 1334.9483158416297\n",
      "Epoch 3 step 944: training accuarcy: 0.6125\n",
      "Epoch 3 step 944: training loss: 1337.0133414686406\n",
      "Epoch 3 step 945: training accuarcy: 0.6315000000000001\n",
      "Epoch 3 step 945: training loss: 1343.8547573241005\n",
      "Epoch 3 step 946: training accuarcy: 0.615\n",
      "Epoch 3 step 946: training loss: 1331.9381201297285\n",
      "Epoch 3 step 947: training accuarcy: 0.6175\n",
      "Epoch 3 step 947: training loss: 1315.730697948556\n",
      "Epoch 3 step 948: training accuarcy: 0.6405\n",
      "Epoch 3 step 948: training loss: 1328.7536605224216\n",
      "Epoch 3 step 949: training accuarcy: 0.621\n",
      "Epoch 3 step 949: training loss: 1339.1241884525791\n",
      "Epoch 3 step 950: training accuarcy: 0.6045\n",
      "Epoch 3 step 950: training loss: 1339.8715951593433\n",
      "Epoch 3 step 951: training accuarcy: 0.6135\n",
      "Epoch 3 step 951: training loss: 1332.0020308487801\n",
      "Epoch 3 step 952: training accuarcy: 0.619\n",
      "Epoch 3 step 952: training loss: 1351.2306970577781\n",
      "Epoch 3 step 953: training accuarcy: 0.6005\n",
      "Epoch 3 step 953: training loss: 1328.465754410674\n",
      "Epoch 3 step 954: training accuarcy: 0.62\n",
      "Epoch 3 step 954: training loss: 1333.7435128802167\n",
      "Epoch 3 step 955: training accuarcy: 0.613\n",
      "Epoch 3 step 955: training loss: 1332.4749101549637\n",
      "Epoch 3 step 956: training accuarcy: 0.623\n",
      "Epoch 3 step 956: training loss: 1334.738091183036\n",
      "Epoch 3 step 957: training accuarcy: 0.6155\n",
      "Epoch 3 step 957: training loss: 1333.8310117623344\n",
      "Epoch 3 step 958: training accuarcy: 0.621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 958: training loss: 1337.8067107111467\n",
      "Epoch 3 step 959: training accuarcy: 0.6095\n",
      "Epoch 3 step 959: training loss: 1337.3080971676152\n",
      "Epoch 3 step 960: training accuarcy: 0.6145\n",
      "Epoch 3 step 960: training loss: 1335.6716432501678\n",
      "Epoch 3 step 961: training accuarcy: 0.6275000000000001\n",
      "Epoch 3 step 961: training loss: 1338.4956425443247\n",
      "Epoch 3 step 962: training accuarcy: 0.6045\n",
      "Epoch 3 step 962: training loss: 1323.3131806669346\n",
      "Epoch 3 step 963: training accuarcy: 0.6215\n",
      "Epoch 3 step 963: training loss: 1342.5994801259912\n",
      "Epoch 3 step 964: training accuarcy: 0.611\n",
      "Epoch 3 step 964: training loss: 1328.644712559443\n",
      "Epoch 3 step 965: training accuarcy: 0.6055\n",
      "Epoch 3 step 965: training loss: 1339.3818996185348\n",
      "Epoch 3 step 966: training accuarcy: 0.607\n",
      "Epoch 3 step 966: training loss: 1328.3340417970899\n",
      "Epoch 3 step 967: training accuarcy: 0.6045\n",
      "Epoch 3 step 967: training loss: 1330.1169269208685\n",
      "Epoch 3 step 968: training accuarcy: 0.622\n",
      "Epoch 3 step 968: training loss: 1326.7946460276607\n",
      "Epoch 3 step 969: training accuarcy: 0.615\n",
      "Epoch 3 step 969: training loss: 1331.9882445408102\n",
      "Epoch 3 step 970: training accuarcy: 0.624\n",
      "Epoch 3 step 970: training loss: 1346.5947868845262\n",
      "Epoch 3 step 971: training accuarcy: 0.609\n",
      "Epoch 3 step 971: training loss: 1353.975489510663\n",
      "Epoch 3 step 972: training accuarcy: 0.6115\n",
      "Epoch 3 step 972: training loss: 1337.3315976061715\n",
      "Epoch 3 step 973: training accuarcy: 0.605\n",
      "Epoch 3 step 973: training loss: 1328.0422669779855\n",
      "Epoch 3 step 974: training accuarcy: 0.6405\n",
      "Epoch 3 step 974: training loss: 1334.9003186388456\n",
      "Epoch 3 step 975: training accuarcy: 0.624\n",
      "Epoch 3 step 975: training loss: 1330.5328395775678\n",
      "Epoch 3 step 976: training accuarcy: 0.617\n",
      "Epoch 3 step 976: training loss: 1339.3780945123842\n",
      "Epoch 3 step 977: training accuarcy: 0.6295000000000001\n",
      "Epoch 3 step 977: training loss: 1331.988720756116\n",
      "Epoch 3 step 978: training accuarcy: 0.6115\n",
      "Epoch 3 step 978: training loss: 1341.7201870026986\n",
      "Epoch 3 step 979: training accuarcy: 0.616\n",
      "Epoch 3 step 979: training loss: 1338.9967964998887\n",
      "Epoch 3 step 980: training accuarcy: 0.6075\n",
      "Epoch 3 step 980: training loss: 1331.5343523832632\n",
      "Epoch 3 step 981: training accuarcy: 0.623\n",
      "Epoch 3 step 981: training loss: 1336.3635442568848\n",
      "Epoch 3 step 982: training accuarcy: 0.608\n",
      "Epoch 3 step 982: training loss: 1330.738048238797\n",
      "Epoch 3 step 983: training accuarcy: 0.625\n",
      "Epoch 3 step 983: training loss: 1325.812291617878\n",
      "Epoch 3 step 984: training accuarcy: 0.627\n",
      "Epoch 3 step 984: training loss: 1351.428659353271\n",
      "Epoch 3 step 985: training accuarcy: 0.5855\n",
      "Epoch 3 step 985: training loss: 1352.8245836875149\n",
      "Epoch 3 step 986: training accuarcy: 0.598\n",
      "Epoch 3 step 986: training loss: 1328.0263741144743\n",
      "Epoch 3 step 987: training accuarcy: 0.618\n",
      "Epoch 3 step 987: training loss: 1340.8209475342117\n",
      "Epoch 3 step 988: training accuarcy: 0.6145\n",
      "Epoch 3 step 988: training loss: 1345.9754015506203\n",
      "Epoch 3 step 989: training accuarcy: 0.606\n",
      "Epoch 3 step 989: training loss: 1329.4315339261746\n",
      "Epoch 3 step 990: training accuarcy: 0.611\n",
      "Epoch 3 step 990: training loss: 1329.0681531537107\n",
      "Epoch 3 step 991: training accuarcy: 0.6185\n",
      "Epoch 3 step 991: training loss: 1320.068633158471\n",
      "Epoch 3 step 992: training accuarcy: 0.632\n",
      "Epoch 3 step 992: training loss: 1348.083146098481\n",
      "Epoch 3 step 993: training accuarcy: 0.5975\n",
      "Epoch 3 step 993: training loss: 1317.788303626376\n",
      "Epoch 3 step 994: training accuarcy: 0.629\n",
      "Epoch 3 step 994: training loss: 1330.5358786054699\n",
      "Epoch 3 step 995: training accuarcy: 0.6165\n",
      "Epoch 3 step 995: training loss: 1336.6133771535476\n",
      "Epoch 3 step 996: training accuarcy: 0.6215\n",
      "Epoch 3 step 996: training loss: 1342.6809027597171\n",
      "Epoch 3 step 997: training accuarcy: 0.623\n",
      "Epoch 3 step 997: training loss: 1344.8852920265558\n",
      "Epoch 3 step 998: training accuarcy: 0.6025\n",
      "Epoch 3 step 998: training loss: 1328.130736357324\n",
      "Epoch 3 step 999: training accuarcy: 0.6265000000000001\n",
      "Epoch 3 step 999: training loss: 1334.975904651938\n",
      "Epoch 3 step 1000: training accuarcy: 0.622\n",
      "Epoch 3 step 1000: training loss: 1326.0723420789088\n",
      "Epoch 3 step 1001: training accuarcy: 0.633\n",
      "Epoch 3 step 1001: training loss: 1337.733244566644\n",
      "Epoch 3 step 1002: training accuarcy: 0.609\n",
      "Epoch 3 step 1002: training loss: 1333.5883694517509\n",
      "Epoch 3 step 1003: training accuarcy: 0.627\n",
      "Epoch 3 step 1003: training loss: 1337.355749752024\n",
      "Epoch 3 step 1004: training accuarcy: 0.6195\n",
      "Epoch 3 step 1004: training loss: 1337.3200248569622\n",
      "Epoch 3 step 1005: training accuarcy: 0.6115\n",
      "Epoch 3 step 1005: training loss: 1336.8105209647572\n",
      "Epoch 3 step 1006: training accuarcy: 0.6125\n",
      "Epoch 3 step 1006: training loss: 1335.7067995682396\n",
      "Epoch 3 step 1007: training accuarcy: 0.615\n",
      "Epoch 3 step 1007: training loss: 1328.2985742477858\n",
      "Epoch 3 step 1008: training accuarcy: 0.618\n",
      "Epoch 3 step 1008: training loss: 1326.030858192847\n",
      "Epoch 3 step 1009: training accuarcy: 0.622\n",
      "Epoch 3 step 1009: training loss: 1348.116735081257\n",
      "Epoch 3 step 1010: training accuarcy: 0.6015\n",
      "Epoch 3 step 1010: training loss: 1340.6437256943607\n",
      "Epoch 3 step 1011: training accuarcy: 0.6105\n",
      "Epoch 3 step 1011: training loss: 1347.514629949558\n",
      "Epoch 3 step 1012: training accuarcy: 0.6115\n",
      "Epoch 3 step 1012: training loss: 1328.1959940447693\n",
      "Epoch 3 step 1013: training accuarcy: 0.608\n",
      "Epoch 3 step 1013: training loss: 1341.184882069943\n",
      "Epoch 3 step 1014: training accuarcy: 0.6125\n",
      "Epoch 3 step 1014: training loss: 1336.7111696549375\n",
      "Epoch 3 step 1015: training accuarcy: 0.611\n",
      "Epoch 3 step 1015: training loss: 1335.1075855867946\n",
      "Epoch 3 step 1016: training accuarcy: 0.623\n",
      "Epoch 3 step 1016: training loss: 1340.6114916727126\n",
      "Epoch 3 step 1017: training accuarcy: 0.5965\n",
      "Epoch 3 step 1017: training loss: 1343.0642935018118\n",
      "Epoch 3 step 1018: training accuarcy: 0.6035\n",
      "Epoch 3 step 1018: training loss: 1321.2714533190901\n",
      "Epoch 3 step 1019: training accuarcy: 0.6205\n",
      "Epoch 3 step 1019: training loss: 1339.370318093065\n",
      "Epoch 3 step 1020: training accuarcy: 0.611\n",
      "Epoch 3 step 1020: training loss: 1345.3741813248364\n",
      "Epoch 3 step 1021: training accuarcy: 0.61\n",
      "Epoch 3 step 1021: training loss: 1336.0399599007064\n",
      "Epoch 3 step 1022: training accuarcy: 0.6105\n",
      "Epoch 3 step 1022: training loss: 1329.9315104218917\n",
      "Epoch 3 step 1023: training accuarcy: 0.623\n",
      "Epoch 3 step 1023: training loss: 1320.5549640078088\n",
      "Epoch 3 step 1024: training accuarcy: 0.6305000000000001\n",
      "Epoch 3 step 1024: training loss: 1342.3157378646988\n",
      "Epoch 3 step 1025: training accuarcy: 0.614\n",
      "Epoch 3 step 1025: training loss: 1339.0173211424144\n",
      "Epoch 3 step 1026: training accuarcy: 0.613\n",
      "Epoch 3 step 1026: training loss: 1331.1173228111077\n",
      "Epoch 3 step 1027: training accuarcy: 0.6085\n",
      "Epoch 3 step 1027: training loss: 1321.580977374145\n",
      "Epoch 3 step 1028: training accuarcy: 0.626\n",
      "Epoch 3 step 1028: training loss: 1334.078599552472\n",
      "Epoch 3 step 1029: training accuarcy: 0.5995\n",
      "Epoch 3 step 1029: training loss: 1328.8306330928885\n",
      "Epoch 3 step 1030: training accuarcy: 0.606\n",
      "Epoch 3 step 1030: training loss: 1329.200527877464\n",
      "Epoch 3 step 1031: training accuarcy: 0.6055\n",
      "Epoch 3 step 1031: training loss: 1330.7696626378647\n",
      "Epoch 3 step 1032: training accuarcy: 0.6205\n",
      "Epoch 3 step 1032: training loss: 1322.9556028925422\n",
      "Epoch 3 step 1033: training accuarcy: 0.6155\n",
      "Epoch 3 step 1033: training loss: 1336.5635383384204\n",
      "Epoch 3 step 1034: training accuarcy: 0.6165\n",
      "Epoch 3 step 1034: training loss: 1335.0926073509079\n",
      "Epoch 3 step 1035: training accuarcy: 0.611\n",
      "Epoch 3 step 1035: training loss: 1332.4058474385538\n",
      "Epoch 3 step 1036: training accuarcy: 0.624\n",
      "Epoch 3 step 1036: training loss: 1334.1910877994976\n",
      "Epoch 3 step 1037: training accuarcy: 0.619\n",
      "Epoch 3 step 1037: training loss: 1344.4429090824203\n",
      "Epoch 3 step 1038: training accuarcy: 0.6065\n",
      "Epoch 3 step 1038: training loss: 1331.3553227656778\n",
      "Epoch 3 step 1039: training accuarcy: 0.6085\n",
      "Epoch 3 step 1039: training loss: 1343.5737742507586\n",
      "Epoch 3 step 1040: training accuarcy: 0.5995\n",
      "Epoch 3 step 1040: training loss: 1333.7898150248757\n",
      "Epoch 3 step 1041: training accuarcy: 0.609\n",
      "Epoch 3 step 1041: training loss: 1325.6481562694123\n",
      "Epoch 3 step 1042: training accuarcy: 0.612\n",
      "Epoch 3 step 1042: training loss: 1330.0845061923997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 1043: training accuarcy: 0.6095\n",
      "Epoch 3 step 1043: training loss: 1321.1676597863054\n",
      "Epoch 3 step 1044: training accuarcy: 0.617\n",
      "Epoch 3 step 1044: training loss: 1326.4585371900853\n",
      "Epoch 3 step 1045: training accuarcy: 0.625\n",
      "Epoch 3 step 1045: training loss: 1340.6061712193898\n",
      "Epoch 3 step 1046: training accuarcy: 0.611\n",
      "Epoch 3 step 1046: training loss: 1329.3108433416369\n",
      "Epoch 3 step 1047: training accuarcy: 0.6245\n",
      "Epoch 3 step 1047: training loss: 1319.4485375152597\n",
      "Epoch 3 step 1048: training accuarcy: 0.6295000000000001\n",
      "Epoch 3 step 1048: training loss: 1346.7429406975498\n",
      "Epoch 3 step 1049: training accuarcy: 0.608\n",
      "Epoch 3 step 1049: training loss: 1352.0318605308125\n",
      "Epoch 3 step 1050: training accuarcy: 0.6185\n",
      "Epoch 3 step 1050: training loss: 1338.3062320061883\n",
      "Epoch 3 step 1051: training accuarcy: 0.616\n",
      "Epoch 3 step 1051: training loss: 524.2369725879482\n",
      "Epoch 3 step 1052: training accuarcy: 0.65\n",
      "Epoch 3: train loss 1331.6161211012734, train accuarcy 0.6141251921653748\n",
      "Epoch 3: valid loss 6694.9276720608905, valid accuarcy 0.5847181081771851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 4/5 [21:49<05:26, 326.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 1330.4050943938764\n",
      "Epoch 4 step 1053: training accuarcy: 0.614\n",
      "Epoch 4 step 1053: training loss: 1321.4621151655\n",
      "Epoch 4 step 1054: training accuarcy: 0.6325000000000001\n",
      "Epoch 4 step 1054: training loss: 1333.322320605249\n",
      "Epoch 4 step 1055: training accuarcy: 0.629\n",
      "Epoch 4 step 1055: training loss: 1333.2582405160251\n",
      "Epoch 4 step 1056: training accuarcy: 0.614\n",
      "Epoch 4 step 1056: training loss: 1318.8609138760892\n",
      "Epoch 4 step 1057: training accuarcy: 0.6315000000000001\n",
      "Epoch 4 step 1057: training loss: 1337.4540537086648\n",
      "Epoch 4 step 1058: training accuarcy: 0.617\n",
      "Epoch 4 step 1058: training loss: 1327.9173864010597\n",
      "Epoch 4 step 1059: training accuarcy: 0.6285000000000001\n",
      "Epoch 4 step 1059: training loss: 1325.2039352832603\n",
      "Epoch 4 step 1060: training accuarcy: 0.631\n",
      "Epoch 4 step 1060: training loss: 1332.1277522854386\n",
      "Epoch 4 step 1061: training accuarcy: 0.6125\n",
      "Epoch 4 step 1061: training loss: 1348.116782975801\n",
      "Epoch 4 step 1062: training accuarcy: 0.604\n",
      "Epoch 4 step 1062: training loss: 1325.6759450291659\n",
      "Epoch 4 step 1063: training accuarcy: 0.63\n",
      "Epoch 4 step 1063: training loss: 1319.0737786161878\n",
      "Epoch 4 step 1064: training accuarcy: 0.621\n",
      "Epoch 4 step 1064: training loss: 1333.02489206468\n",
      "Epoch 4 step 1065: training accuarcy: 0.6205\n",
      "Epoch 4 step 1065: training loss: 1339.0217458764162\n",
      "Epoch 4 step 1066: training accuarcy: 0.6235\n",
      "Epoch 4 step 1066: training loss: 1343.0162443621255\n",
      "Epoch 4 step 1067: training accuarcy: 0.5975\n",
      "Epoch 4 step 1067: training loss: 1331.2173588113033\n",
      "Epoch 4 step 1068: training accuarcy: 0.623\n",
      "Epoch 4 step 1068: training loss: 1338.7699683274523\n",
      "Epoch 4 step 1069: training accuarcy: 0.6125\n",
      "Epoch 4 step 1069: training loss: 1326.750634968102\n",
      "Epoch 4 step 1070: training accuarcy: 0.6175\n",
      "Epoch 4 step 1070: training loss: 1340.3783618756524\n",
      "Epoch 4 step 1071: training accuarcy: 0.6155\n",
      "Epoch 4 step 1071: training loss: 1327.088659206028\n",
      "Epoch 4 step 1072: training accuarcy: 0.607\n",
      "Epoch 4 step 1072: training loss: 1338.591681969204\n",
      "Epoch 4 step 1073: training accuarcy: 0.605\n",
      "Epoch 4 step 1073: training loss: 1333.890179137681\n",
      "Epoch 4 step 1074: training accuarcy: 0.6055\n",
      "Epoch 4 step 1074: training loss: 1326.0368577430706\n",
      "Epoch 4 step 1075: training accuarcy: 0.6355000000000001\n",
      "Epoch 4 step 1075: training loss: 1337.8222455842715\n",
      "Epoch 4 step 1076: training accuarcy: 0.608\n",
      "Epoch 4 step 1076: training loss: 1336.9303901773455\n",
      "Epoch 4 step 1077: training accuarcy: 0.6105\n",
      "Epoch 4 step 1077: training loss: 1346.4596230385134\n",
      "Epoch 4 step 1078: training accuarcy: 0.6\n",
      "Epoch 4 step 1078: training loss: 1344.1036568620877\n",
      "Epoch 4 step 1079: training accuarcy: 0.6085\n",
      "Epoch 4 step 1079: training loss: 1340.2426351079603\n",
      "Epoch 4 step 1080: training accuarcy: 0.605\n",
      "Epoch 4 step 1080: training loss: 1328.7880055490737\n",
      "Epoch 4 step 1081: training accuarcy: 0.617\n",
      "Epoch 4 step 1081: training loss: 1344.3614991451648\n",
      "Epoch 4 step 1082: training accuarcy: 0.595\n",
      "Epoch 4 step 1082: training loss: 1338.870385565163\n",
      "Epoch 4 step 1083: training accuarcy: 0.6145\n",
      "Epoch 4 step 1083: training loss: 1344.0195405263762\n",
      "Epoch 4 step 1084: training accuarcy: 0.601\n",
      "Epoch 4 step 1084: training loss: 1340.0160211276402\n",
      "Epoch 4 step 1085: training accuarcy: 0.598\n",
      "Epoch 4 step 1085: training loss: 1318.315547119192\n",
      "Epoch 4 step 1086: training accuarcy: 0.641\n",
      "Epoch 4 step 1086: training loss: 1330.8180476693701\n",
      "Epoch 4 step 1087: training accuarcy: 0.62\n",
      "Epoch 4 step 1087: training loss: 1342.4402584660236\n",
      "Epoch 4 step 1088: training accuarcy: 0.601\n",
      "Epoch 4 step 1088: training loss: 1346.8909173772474\n",
      "Epoch 4 step 1089: training accuarcy: 0.6025\n",
      "Epoch 4 step 1089: training loss: 1330.7725714871005\n",
      "Epoch 4 step 1090: training accuarcy: 0.622\n",
      "Epoch 4 step 1090: training loss: 1323.7774689587527\n",
      "Epoch 4 step 1091: training accuarcy: 0.63\n",
      "Epoch 4 step 1091: training loss: 1324.5212958746704\n",
      "Epoch 4 step 1092: training accuarcy: 0.627\n",
      "Epoch 4 step 1092: training loss: 1335.9296543929154\n",
      "Epoch 4 step 1093: training accuarcy: 0.6065\n",
      "Epoch 4 step 1093: training loss: 1325.3317281689285\n",
      "Epoch 4 step 1094: training accuarcy: 0.613\n",
      "Epoch 4 step 1094: training loss: 1340.3713765032512\n",
      "Epoch 4 step 1095: training accuarcy: 0.5995\n",
      "Epoch 4 step 1095: training loss: 1331.1640172042635\n",
      "Epoch 4 step 1096: training accuarcy: 0.635\n",
      "Epoch 4 step 1096: training loss: 1351.008711430605\n",
      "Epoch 4 step 1097: training accuarcy: 0.6075\n",
      "Epoch 4 step 1097: training loss: 1337.0975257120706\n",
      "Epoch 4 step 1098: training accuarcy: 0.602\n",
      "Epoch 4 step 1098: training loss: 1323.5114820265853\n",
      "Epoch 4 step 1099: training accuarcy: 0.625\n",
      "Epoch 4 step 1099: training loss: 1342.3758903369842\n",
      "Epoch 4 step 1100: training accuarcy: 0.6095\n",
      "Epoch 4 step 1100: training loss: 1333.7742647551615\n",
      "Epoch 4 step 1101: training accuarcy: 0.63\n",
      "Epoch 4 step 1101: training loss: 1345.559525372587\n",
      "Epoch 4 step 1102: training accuarcy: 0.601\n",
      "Epoch 4 step 1102: training loss: 1349.5862762582196\n",
      "Epoch 4 step 1103: training accuarcy: 0.5965\n",
      "Epoch 4 step 1103: training loss: 1329.9881547904458\n",
      "Epoch 4 step 1104: training accuarcy: 0.609\n",
      "Epoch 4 step 1104: training loss: 1347.7460148125897\n",
      "Epoch 4 step 1105: training accuarcy: 0.6115\n",
      "Epoch 4 step 1105: training loss: 1335.8729011534622\n",
      "Epoch 4 step 1106: training accuarcy: 0.621\n",
      "Epoch 4 step 1106: training loss: 1343.7815652767172\n",
      "Epoch 4 step 1107: training accuarcy: 0.599\n",
      "Epoch 4 step 1107: training loss: 1334.5365587170677\n",
      "Epoch 4 step 1108: training accuarcy: 0.601\n",
      "Epoch 4 step 1108: training loss: 1332.4432108841843\n",
      "Epoch 4 step 1109: training accuarcy: 0.6185\n",
      "Epoch 4 step 1109: training loss: 1335.7461871162775\n",
      "Epoch 4 step 1110: training accuarcy: 0.625\n",
      "Epoch 4 step 1110: training loss: 1333.4806742681608\n",
      "Epoch 4 step 1111: training accuarcy: 0.6125\n",
      "Epoch 4 step 1111: training loss: 1342.9439964090557\n",
      "Epoch 4 step 1112: training accuarcy: 0.6045\n",
      "Epoch 4 step 1112: training loss: 1334.3303775973816\n",
      "Epoch 4 step 1113: training accuarcy: 0.6105\n",
      "Epoch 4 step 1113: training loss: 1333.3002891425128\n",
      "Epoch 4 step 1114: training accuarcy: 0.6205\n",
      "Epoch 4 step 1114: training loss: 1340.1938944275328\n",
      "Epoch 4 step 1115: training accuarcy: 0.623\n",
      "Epoch 4 step 1115: training loss: 1337.7075855680973\n",
      "Epoch 4 step 1116: training accuarcy: 0.6215\n",
      "Epoch 4 step 1116: training loss: 1333.3386834307996\n",
      "Epoch 4 step 1117: training accuarcy: 0.62\n",
      "Epoch 4 step 1117: training loss: 1342.6221715020317\n",
      "Epoch 4 step 1118: training accuarcy: 0.6095\n",
      "Epoch 4 step 1118: training loss: 1333.8132662390028\n",
      "Epoch 4 step 1119: training accuarcy: 0.622\n",
      "Epoch 4 step 1119: training loss: 1327.5494638759267\n",
      "Epoch 4 step 1120: training accuarcy: 0.63\n",
      "Epoch 4 step 1120: training loss: 1345.4204060249153\n",
      "Epoch 4 step 1121: training accuarcy: 0.5985\n",
      "Epoch 4 step 1121: training loss: 1337.8183232773067\n",
      "Epoch 4 step 1122: training accuarcy: 0.622\n",
      "Epoch 4 step 1122: training loss: 1323.9746528296744\n",
      "Epoch 4 step 1123: training accuarcy: 0.616\n",
      "Epoch 4 step 1123: training loss: 1351.7216055441925\n",
      "Epoch 4 step 1124: training accuarcy: 0.599\n",
      "Epoch 4 step 1124: training loss: 1321.8741072873793\n",
      "Epoch 4 step 1125: training accuarcy: 0.63\n",
      "Epoch 4 step 1125: training loss: 1311.8151151844668\n",
      "Epoch 4 step 1126: training accuarcy: 0.64\n",
      "Epoch 4 step 1126: training loss: 1327.376038286856\n",
      "Epoch 4 step 1127: training accuarcy: 0.6225\n",
      "Epoch 4 step 1127: training loss: 1331.5410437607836\n",
      "Epoch 4 step 1128: training accuarcy: 0.63\n",
      "Epoch 4 step 1128: training loss: 1345.130169148915\n",
      "Epoch 4 step 1129: training accuarcy: 0.613\n",
      "Epoch 4 step 1129: training loss: 1338.1676569969054\n",
      "Epoch 4 step 1130: training accuarcy: 0.6065\n",
      "Epoch 4 step 1130: training loss: 1336.405882241395\n",
      "Epoch 4 step 1131: training accuarcy: 0.6225\n",
      "Epoch 4 step 1131: training loss: 1340.9330406027425\n",
      "Epoch 4 step 1132: training accuarcy: 0.6245\n",
      "Epoch 4 step 1132: training loss: 1335.5648103338265\n",
      "Epoch 4 step 1133: training accuarcy: 0.6235\n",
      "Epoch 4 step 1133: training loss: 1339.7593035290847\n",
      "Epoch 4 step 1134: training accuarcy: 0.614\n",
      "Epoch 4 step 1134: training loss: 1353.5538049272327\n",
      "Epoch 4 step 1135: training accuarcy: 0.597\n",
      "Epoch 4 step 1135: training loss: 1339.4769835729264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1136: training accuarcy: 0.623\n",
      "Epoch 4 step 1136: training loss: 1324.7643355956347\n",
      "Epoch 4 step 1137: training accuarcy: 0.6195\n",
      "Epoch 4 step 1137: training loss: 1344.1825062753378\n",
      "Epoch 4 step 1138: training accuarcy: 0.5995\n",
      "Epoch 4 step 1138: training loss: 1333.4270629858445\n",
      "Epoch 4 step 1139: training accuarcy: 0.6215\n",
      "Epoch 4 step 1139: training loss: 1332.8734374454455\n",
      "Epoch 4 step 1140: training accuarcy: 0.6125\n",
      "Epoch 4 step 1140: training loss: 1318.342133323241\n",
      "Epoch 4 step 1141: training accuarcy: 0.6335000000000001\n"
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=5,\n",
    "                log_dir=get_log_dir('seq_topcoder', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:22:03.752840Z",
     "start_time": "2019-09-25T13:22:03.743839Z"
    }
   },
   "outputs": [],
   "source": [
    "del hrm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PRME FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:22:25.507537Z",
     "start_time": "2019-09-25T13:22:25.255536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchPrmeFM()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_model = TorchPrmeFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "prme_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:22:29.635222Z",
     "start_time": "2019-09-25T13:22:29.631221Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(prme_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:22:34.027515Z",
     "start_time": "2019-09-25T13:22:33.983528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1f20046b400>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_learner = FMLearner(prme_model, adam_opt, schedular, db)\n",
    "prme_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:39:05.138320Z",
     "start_time": "2019-09-25T13:22:57.640696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 37514.48137421203\n",
      "Epoch 0 step 1: training accuarcy: 0.527\n",
      "Epoch 0 step 1: training loss: 36473.747391143785\n",
      "Epoch 0 step 2: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 2: training loss: 35454.346101311516\n",
      "Epoch 0 step 3: training accuarcy: 0.5015000000000001\n",
      "Epoch 0 step 3: training loss: 34454.33436229627\n",
      "Epoch 0 step 4: training accuarcy: 0.484\n",
      "Epoch 0 step 4: training loss: 33472.234940401846\n",
      "Epoch 0 step 5: training accuarcy: 0.5105000000000001\n",
      "Epoch 0 step 5: training loss: 32522.437981548668\n",
      "Epoch 0 step 6: training accuarcy: 0.5055000000000001\n",
      "Epoch 0 step 6: training loss: 31582.993115826408\n",
      "Epoch 0 step 7: training accuarcy: 0.5135\n",
      "Epoch 0 step 7: training loss: 30670.02176044439\n",
      "Epoch 0 step 8: training accuarcy: 0.5155\n",
      "Epoch 0 step 8: training loss: 29788.87466102316\n",
      "Epoch 0 step 9: training accuarcy: 0.507\n",
      "Epoch 0 step 9: training loss: 28914.7417543135\n",
      "Epoch 0 step 10: training accuarcy: 0.5165\n",
      "Epoch 0 step 10: training loss: 28062.646270543024\n",
      "Epoch 0 step 11: training accuarcy: 0.522\n",
      "Epoch 0 step 11: training loss: 27239.842394624946\n",
      "Epoch 0 step 12: training accuarcy: 0.4915\n",
      "Epoch 0 step 12: training loss: 26424.54360148426\n",
      "Epoch 0 step 13: training accuarcy: 0.518\n",
      "Epoch 0 step 13: training loss: 25632.1226669722\n",
      "Epoch 0 step 14: training accuarcy: 0.541\n",
      "Epoch 0 step 14: training loss: 24868.546024228413\n",
      "Epoch 0 step 15: training accuarcy: 0.508\n",
      "Epoch 0 step 15: training loss: 24111.908461133906\n",
      "Epoch 0 step 16: training accuarcy: 0.535\n",
      "Epoch 0 step 16: training loss: 23383.531547629347\n",
      "Epoch 0 step 17: training accuarcy: 0.5245\n",
      "Epoch 0 step 17: training loss: 22672.647465358255\n",
      "Epoch 0 step 18: training accuarcy: 0.5305\n",
      "Epoch 0 step 18: training loss: 21974.311731257494\n",
      "Epoch 0 step 19: training accuarcy: 0.536\n",
      "Epoch 0 step 19: training loss: 21299.627592291352\n",
      "Epoch 0 step 20: training accuarcy: 0.5355\n",
      "Epoch 0 step 20: training loss: 20644.2404076104\n",
      "Epoch 0 step 21: training accuarcy: 0.534\n",
      "Epoch 0 step 21: training loss: 20002.970165223396\n",
      "Epoch 0 step 22: training accuarcy: 0.5535\n",
      "Epoch 0 step 22: training loss: 19381.89133334017\n",
      "Epoch 0 step 23: training accuarcy: 0.547\n",
      "Epoch 0 step 23: training loss: 18772.904280142484\n",
      "Epoch 0 step 24: training accuarcy: 0.5615\n",
      "Epoch 0 step 24: training loss: 18192.552851703236\n",
      "Epoch 0 step 25: training accuarcy: 0.546\n",
      "Epoch 0 step 25: training loss: 17623.0104615171\n",
      "Epoch 0 step 26: training accuarcy: 0.5305\n",
      "Epoch 0 step 26: training loss: 17064.722150059475\n",
      "Epoch 0 step 27: training accuarcy: 0.5515\n",
      "Epoch 0 step 27: training loss: 16527.414671065348\n",
      "Epoch 0 step 28: training accuarcy: 0.546\n",
      "Epoch 0 step 28: training loss: 16005.169937778894\n",
      "Epoch 0 step 29: training accuarcy: 0.552\n",
      "Epoch 0 step 29: training loss: 15501.943430343252\n",
      "Epoch 0 step 30: training accuarcy: 0.5505\n",
      "Epoch 0 step 30: training loss: 15009.480675598992\n",
      "Epoch 0 step 31: training accuarcy: 0.5485\n",
      "Epoch 0 step 31: training loss: 14532.774574120012\n",
      "Epoch 0 step 32: training accuarcy: 0.5505\n",
      "Epoch 0 step 32: training loss: 14065.008685162817\n",
      "Epoch 0 step 33: training accuarcy: 0.5655\n",
      "Epoch 0 step 33: training loss: 13614.968682862615\n",
      "Epoch 0 step 34: training accuarcy: 0.5775\n",
      "Epoch 0 step 34: training loss: 13187.78595862876\n",
      "Epoch 0 step 35: training accuarcy: 0.5615\n",
      "Epoch 0 step 35: training loss: 12764.746933347671\n",
      "Epoch 0 step 36: training accuarcy: 0.558\n",
      "Epoch 0 step 36: training loss: 12358.952760678196\n",
      "Epoch 0 step 37: training accuarcy: 0.5535\n",
      "Epoch 0 step 37: training loss: 11957.139184556752\n",
      "Epoch 0 step 38: training accuarcy: 0.5785\n",
      "Epoch 0 step 38: training loss: 11578.10089165549\n",
      "Epoch 0 step 39: training accuarcy: 0.5645\n",
      "Epoch 0 step 39: training loss: 11210.170839158214\n",
      "Epoch 0 step 40: training accuarcy: 0.5725\n",
      "Epoch 0 step 40: training loss: 10848.559030422053\n",
      "Epoch 0 step 41: training accuarcy: 0.577\n",
      "Epoch 0 step 41: training loss: 10506.835521014327\n",
      "Epoch 0 step 42: training accuarcy: 0.5625\n",
      "Epoch 0 step 42: training loss: 10166.629123765779\n",
      "Epoch 0 step 43: training accuarcy: 0.5775\n",
      "Epoch 0 step 43: training loss: 9838.61839602091\n",
      "Epoch 0 step 44: training accuarcy: 0.6035\n",
      "Epoch 0 step 44: training loss: 9530.07888466716\n",
      "Epoch 0 step 45: training accuarcy: 0.5655\n",
      "Epoch 0 step 45: training loss: 9225.996267689341\n",
      "Epoch 0 step 46: training accuarcy: 0.5730000000000001\n",
      "Epoch 0 step 46: training loss: 8934.510124479884\n",
      "Epoch 0 step 47: training accuarcy: 0.555\n",
      "Epoch 0 step 47: training loss: 8649.634706762448\n",
      "Epoch 0 step 48: training accuarcy: 0.5750000000000001\n",
      "Epoch 0 step 48: training loss: 8376.078467023763\n",
      "Epoch 0 step 49: training accuarcy: 0.5750000000000001\n",
      "Epoch 0 step 49: training loss: 8110.682623572238\n",
      "Epoch 0 step 50: training accuarcy: 0.585\n",
      "Epoch 0 step 50: training loss: 7855.969278212591\n",
      "Epoch 0 step 51: training accuarcy: 0.58\n",
      "Epoch 0 step 51: training loss: 7610.1542268712565\n",
      "Epoch 0 step 52: training accuarcy: 0.5690000000000001\n",
      "Epoch 0 step 52: training loss: 7371.133513344363\n",
      "Epoch 0 step 53: training accuarcy: 0.5635\n",
      "Epoch 0 step 53: training loss: 7142.285526240432\n",
      "Epoch 0 step 54: training accuarcy: 0.5655\n",
      "Epoch 0 step 54: training loss: 6914.717675149546\n",
      "Epoch 0 step 55: training accuarcy: 0.591\n",
      "Epoch 0 step 55: training loss: 6704.136983746221\n",
      "Epoch 0 step 56: training accuarcy: 0.5685\n",
      "Epoch 0 step 56: training loss: 6493.817573727631\n",
      "Epoch 0 step 57: training accuarcy: 0.5925\n",
      "Epoch 0 step 57: training loss: 6297.564206948133\n",
      "Epoch 0 step 58: training accuarcy: 0.5845\n",
      "Epoch 0 step 58: training loss: 6104.3439665957885\n",
      "Epoch 0 step 59: training accuarcy: 0.581\n",
      "Epoch 0 step 59: training loss: 5918.7684579295765\n",
      "Epoch 0 step 60: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 60: training loss: 5740.835450994186\n",
      "Epoch 0 step 61: training accuarcy: 0.5720000000000001\n",
      "Epoch 0 step 61: training loss: 5565.814478416676\n",
      "Epoch 0 step 62: training accuarcy: 0.578\n",
      "Epoch 0 step 62: training loss: 5398.3378888812895\n",
      "Epoch 0 step 63: training accuarcy: 0.5835\n",
      "Epoch 0 step 63: training loss: 5238.789459345968\n",
      "Epoch 0 step 64: training accuarcy: 0.5750000000000001\n",
      "Epoch 0 step 64: training loss: 5084.913463692763\n",
      "Epoch 0 step 65: training accuarcy: 0.5755\n",
      "Epoch 0 step 65: training loss: 4937.1446454835195\n",
      "Epoch 0 step 66: training accuarcy: 0.5635\n",
      "Epoch 0 step 66: training loss: 4791.225020637687\n",
      "Epoch 0 step 67: training accuarcy: 0.587\n",
      "Epoch 0 step 67: training loss: 4651.800419416375\n",
      "Epoch 0 step 68: training accuarcy: 0.606\n",
      "Epoch 0 step 68: training loss: 4520.711793959861\n",
      "Epoch 0 step 69: training accuarcy: 0.5865\n",
      "Epoch 0 step 69: training loss: 4390.584605515061\n",
      "Epoch 0 step 70: training accuarcy: 0.591\n",
      "Epoch 0 step 70: training loss: 4268.101368139625\n",
      "Epoch 0 step 71: training accuarcy: 0.5815\n",
      "Epoch 0 step 71: training loss: 4150.128345789615\n",
      "Epoch 0 step 72: training accuarcy: 0.5875\n",
      "Epoch 0 step 72: training loss: 4035.5357543067043\n",
      "Epoch 0 step 73: training accuarcy: 0.5915\n",
      "Epoch 0 step 73: training loss: 3926.8104214191408\n",
      "Epoch 0 step 74: training accuarcy: 0.577\n",
      "Epoch 0 step 74: training loss: 3818.1946579486416\n",
      "Epoch 0 step 75: training accuarcy: 0.604\n",
      "Epoch 0 step 75: training loss: 3719.2000771572893\n",
      "Epoch 0 step 76: training accuarcy: 0.577\n",
      "Epoch 0 step 76: training loss: 3620.328605774228\n",
      "Epoch 0 step 77: training accuarcy: 0.59\n",
      "Epoch 0 step 77: training loss: 3526.1575841736003\n",
      "Epoch 0 step 78: training accuarcy: 0.58\n",
      "Epoch 0 step 78: training loss: 3435.281575258935\n",
      "Epoch 0 step 79: training accuarcy: 0.5855\n",
      "Epoch 0 step 79: training loss: 3346.194536932623\n",
      "Epoch 0 step 80: training accuarcy: 0.5945\n",
      "Epoch 0 step 80: training loss: 3263.915735010875\n",
      "Epoch 0 step 81: training accuarcy: 0.5975\n",
      "Epoch 0 step 81: training loss: 3182.3496947119665\n",
      "Epoch 0 step 82: training accuarcy: 0.599\n",
      "Epoch 0 step 82: training loss: 3107.091412243074\n",
      "Epoch 0 step 83: training accuarcy: 0.585\n",
      "Epoch 0 step 83: training loss: 3032.524953609145\n",
      "Epoch 0 step 84: training accuarcy: 0.5855\n",
      "Epoch 0 step 84: training loss: 2963.495915734633\n",
      "Epoch 0 step 85: training accuarcy: 0.5725\n",
      "Epoch 0 step 85: training loss: 2894.1349756890495\n",
      "Epoch 0 step 86: training accuarcy: 0.5805\n",
      "Epoch 0 step 86: training loss: 2827.6093232192125\n",
      "Epoch 0 step 87: training accuarcy: 0.5925\n",
      "Epoch 0 step 87: training loss: 2762.5663323613303\n",
      "Epoch 0 step 88: training accuarcy: 0.599\n",
      "Epoch 0 step 88: training loss: 2706.2634834567843\n",
      "Epoch 0 step 89: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 89: training loss: 2644.5764398264546\n",
      "Epoch 0 step 90: training accuarcy: 0.5945\n",
      "Epoch 0 step 90: training loss: 2588.3158711829783\n",
      "Epoch 0 step 91: training accuarcy: 0.6055\n",
      "Epoch 0 step 91: training loss: 2536.562902300486\n",
      "Epoch 0 step 92: training accuarcy: 0.5885\n",
      "Epoch 0 step 92: training loss: 2486.265884515271\n",
      "Epoch 0 step 93: training accuarcy: 0.5915\n",
      "Epoch 0 step 93: training loss: 2435.0182627281147\n",
      "Epoch 0 step 94: training accuarcy: 0.6\n",
      "Epoch 0 step 94: training loss: 2389.118089535354\n",
      "Epoch 0 step 95: training accuarcy: 0.5975\n",
      "Epoch 0 step 95: training loss: 2344.12374985393\n",
      "Epoch 0 step 96: training accuarcy: 0.583\n",
      "Epoch 0 step 96: training loss: 2300.0552278803657\n",
      "Epoch 0 step 97: training accuarcy: 0.5915\n",
      "Epoch 0 step 97: training loss: 2258.84169809021\n",
      "Epoch 0 step 98: training accuarcy: 0.592\n",
      "Epoch 0 step 98: training loss: 2218.261067480604\n",
      "Epoch 0 step 99: training accuarcy: 0.6025\n",
      "Epoch 0 step 99: training loss: 2180.3362250752325\n",
      "Epoch 0 step 100: training accuarcy: 0.6005\n",
      "Epoch 0 step 100: training loss: 2144.171357156447\n",
      "Epoch 0 step 101: training accuarcy: 0.601\n",
      "Epoch 0 step 101: training loss: 2109.1529840377966\n",
      "Epoch 0 step 102: training accuarcy: 0.6045\n",
      "Epoch 0 step 102: training loss: 2075.7088889420174\n",
      "Epoch 0 step 103: training accuarcy: 0.609\n",
      "Epoch 0 step 103: training loss: 2045.4750881160594\n",
      "Epoch 0 step 104: training accuarcy: 0.5765\n",
      "Epoch 0 step 104: training loss: 2013.3139381632864\n",
      "Epoch 0 step 105: training accuarcy: 0.6065\n",
      "Epoch 0 step 105: training loss: 1983.8180910582432\n",
      "Epoch 0 step 106: training accuarcy: 0.599\n",
      "Epoch 0 step 106: training loss: 1955.9047423117145\n",
      "Epoch 0 step 107: training accuarcy: 0.605\n",
      "Epoch 0 step 107: training loss: 1929.9172010057246\n",
      "Epoch 0 step 108: training accuarcy: 0.6065\n",
      "Epoch 0 step 108: training loss: 1905.5602735935536\n",
      "Epoch 0 step 109: training accuarcy: 0.5915\n",
      "Epoch 0 step 109: training loss: 1878.200254827054\n",
      "Epoch 0 step 110: training accuarcy: 0.6145\n",
      "Epoch 0 step 110: training loss: 1856.3194113299887\n",
      "Epoch 0 step 111: training accuarcy: 0.5845\n",
      "Epoch 0 step 111: training loss: 1833.1653562649064\n",
      "Epoch 0 step 112: training accuarcy: 0.599\n",
      "Epoch 0 step 112: training loss: 1810.730713724317\n",
      "Epoch 0 step 113: training accuarcy: 0.6175\n",
      "Epoch 0 step 113: training loss: 1793.0042847002098\n",
      "Epoch 0 step 114: training accuarcy: 0.582\n",
      "Epoch 0 step 114: training loss: 1771.9726215934288\n",
      "Epoch 0 step 115: training accuarcy: 0.6035\n",
      "Epoch 0 step 115: training loss: 1753.7958016076232\n",
      "Epoch 0 step 116: training accuarcy: 0.599\n",
      "Epoch 0 step 116: training loss: 1736.5814465476424\n",
      "Epoch 0 step 117: training accuarcy: 0.5945\n",
      "Epoch 0 step 117: training loss: 1718.2588169586531\n",
      "Epoch 0 step 118: training accuarcy: 0.597\n",
      "Epoch 0 step 118: training loss: 1702.2030619010243\n",
      "Epoch 0 step 119: training accuarcy: 0.6\n",
      "Epoch 0 step 119: training loss: 1686.032512447158\n",
      "Epoch 0 step 120: training accuarcy: 0.612\n",
      "Epoch 0 step 120: training loss: 1672.4098045612598\n",
      "Epoch 0 step 121: training accuarcy: 0.605\n",
      "Epoch 0 step 121: training loss: 1658.1689433751576\n",
      "Epoch 0 step 122: training accuarcy: 0.596\n",
      "Epoch 0 step 122: training loss: 1644.2551075219703\n",
      "Epoch 0 step 123: training accuarcy: 0.602\n",
      "Epoch 0 step 123: training loss: 1631.5806442417763\n",
      "Epoch 0 step 124: training accuarcy: 0.6025\n",
      "Epoch 0 step 124: training loss: 1619.496605629231\n",
      "Epoch 0 step 125: training accuarcy: 0.615\n",
      "Epoch 0 step 125: training loss: 1606.8976203691761\n",
      "Epoch 0 step 126: training accuarcy: 0.611\n",
      "Epoch 0 step 126: training loss: 1596.5024368539691\n",
      "Epoch 0 step 127: training accuarcy: 0.606\n",
      "Epoch 0 step 127: training loss: 1584.7921793138867\n",
      "Epoch 0 step 128: training accuarcy: 0.6175\n",
      "Epoch 0 step 128: training loss: 1575.2523159819411\n",
      "Epoch 0 step 129: training accuarcy: 0.613\n",
      "Epoch 0 step 129: training loss: 1565.032998178794\n",
      "Epoch 0 step 130: training accuarcy: 0.6095\n",
      "Epoch 0 step 130: training loss: 1556.002008025446\n",
      "Epoch 0 step 131: training accuarcy: 0.6265000000000001\n",
      "Epoch 0 step 131: training loss: 1548.338549085878\n",
      "Epoch 0 step 132: training accuarcy: 0.6005\n",
      "Epoch 0 step 132: training loss: 1540.1532660490186\n",
      "Epoch 0 step 133: training accuarcy: 0.598\n",
      "Epoch 0 step 133: training loss: 1531.8461731828952\n",
      "Epoch 0 step 134: training accuarcy: 0.6145\n",
      "Epoch 0 step 134: training loss: 1524.9353633168428\n",
      "Epoch 0 step 135: training accuarcy: 0.6165\n",
      "Epoch 0 step 135: training loss: 1517.6208562577463\n",
      "Epoch 0 step 136: training accuarcy: 0.6195\n",
      "Epoch 0 step 136: training loss: 1511.12274413154\n",
      "Epoch 0 step 137: training accuarcy: 0.597\n",
      "Epoch 0 step 137: training loss: 1503.8677052806174\n",
      "Epoch 0 step 138: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 138: training loss: 1497.5420560295756\n",
      "Epoch 0 step 139: training accuarcy: 0.621\n",
      "Epoch 0 step 139: training loss: 1491.7375420309577\n",
      "Epoch 0 step 140: training accuarcy: 0.6135\n",
      "Epoch 0 step 140: training loss: 1485.7492623522478\n",
      "Epoch 0 step 141: training accuarcy: 0.6105\n",
      "Epoch 0 step 141: training loss: 1481.2769489604746\n",
      "Epoch 0 step 142: training accuarcy: 0.6005\n",
      "Epoch 0 step 142: training loss: 1476.467237532925\n",
      "Epoch 0 step 143: training accuarcy: 0.6195\n",
      "Epoch 0 step 143: training loss: 1471.6510381013193\n",
      "Epoch 0 step 144: training accuarcy: 0.608\n",
      "Epoch 0 step 144: training loss: 1466.0691178540371\n",
      "Epoch 0 step 145: training accuarcy: 0.63\n",
      "Epoch 0 step 145: training loss: 1462.9219211200461\n",
      "Epoch 0 step 146: training accuarcy: 0.612\n",
      "Epoch 0 step 146: training loss: 1458.189675497529\n",
      "Epoch 0 step 147: training accuarcy: 0.6385000000000001\n",
      "Epoch 0 step 147: training loss: 1453.5068012605707\n",
      "Epoch 0 step 148: training accuarcy: 0.6205\n",
      "Epoch 0 step 148: training loss: 1451.155245880636\n",
      "Epoch 0 step 149: training accuarcy: 0.615\n",
      "Epoch 0 step 149: training loss: 1447.0502113295217\n",
      "Epoch 0 step 150: training accuarcy: 0.6135\n",
      "Epoch 0 step 150: training loss: 1443.4691819451198\n",
      "Epoch 0 step 151: training accuarcy: 0.6365000000000001\n",
      "Epoch 0 step 151: training loss: 1440.4613575803126\n",
      "Epoch 0 step 152: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 152: training loss: 1437.4472436845383\n",
      "Epoch 0 step 153: training accuarcy: 0.628\n",
      "Epoch 0 step 153: training loss: 1434.3991041258785\n",
      "Epoch 0 step 154: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 154: training loss: 1432.0644138231792\n",
      "Epoch 0 step 155: training accuarcy: 0.6295000000000001\n",
      "Epoch 0 step 155: training loss: 1428.8674241253038\n",
      "Epoch 0 step 156: training accuarcy: 0.649\n",
      "Epoch 0 step 156: training loss: 1425.812155732883\n",
      "Epoch 0 step 157: training accuarcy: 0.646\n",
      "Epoch 0 step 157: training loss: 1424.2723725892429\n",
      "Epoch 0 step 158: training accuarcy: 0.639\n",
      "Epoch 0 step 158: training loss: 1423.3856810206964\n",
      "Epoch 0 step 159: training accuarcy: 0.6245\n",
      "Epoch 0 step 159: training loss: 1420.274828981289\n",
      "Epoch 0 step 160: training accuarcy: 0.6315000000000001\n",
      "Epoch 0 step 160: training loss: 1417.0586752030079\n",
      "Epoch 0 step 161: training accuarcy: 0.646\n",
      "Epoch 0 step 161: training loss: 1414.9820000853038\n",
      "Epoch 0 step 162: training accuarcy: 0.6365000000000001\n",
      "Epoch 0 step 162: training loss: 1413.856593048961\n",
      "Epoch 0 step 163: training accuarcy: 0.647\n",
      "Epoch 0 step 163: training loss: 1414.112900027208\n",
      "Epoch 0 step 164: training accuarcy: 0.628\n",
      "Epoch 0 step 164: training loss: 1411.9763416426\n",
      "Epoch 0 step 165: training accuarcy: 0.642\n",
      "Epoch 0 step 165: training loss: 1410.295267599146\n",
      "Epoch 0 step 166: training accuarcy: 0.654\n",
      "Epoch 0 step 166: training loss: 1407.4389863884332\n",
      "Epoch 0 step 167: training accuarcy: 0.6715\n",
      "Epoch 0 step 167: training loss: 1407.827917840289\n",
      "Epoch 0 step 168: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 168: training loss: 1405.9620360226809\n",
      "Epoch 0 step 169: training accuarcy: 0.6195\n",
      "Epoch 0 step 169: training loss: 1403.8561005064464\n",
      "Epoch 0 step 170: training accuarcy: 0.645\n",
      "Epoch 0 step 170: training loss: 1403.9883835980431\n",
      "Epoch 0 step 171: training accuarcy: 0.6295000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 171: training loss: 1403.2463424379605\n",
      "Epoch 0 step 172: training accuarcy: 0.64\n",
      "Epoch 0 step 172: training loss: 1402.6984465602586\n",
      "Epoch 0 step 173: training accuarcy: 0.6355000000000001\n",
      "Epoch 0 step 173: training loss: 1400.553143916913\n",
      "Epoch 0 step 174: training accuarcy: 0.66\n",
      "Epoch 0 step 174: training loss: 1400.4041205164006\n",
      "Epoch 0 step 175: training accuarcy: 0.648\n",
      "Epoch 0 step 175: training loss: 1397.8913055722246\n",
      "Epoch 0 step 176: training accuarcy: 0.662\n",
      "Epoch 0 step 176: training loss: 1397.7636800453208\n",
      "Epoch 0 step 177: training accuarcy: 0.6465\n",
      "Epoch 0 step 177: training loss: 1397.2701884359565\n",
      "Epoch 0 step 178: training accuarcy: 0.655\n",
      "Epoch 0 step 178: training loss: 1395.899843048871\n",
      "Epoch 0 step 179: training accuarcy: 0.6735\n",
      "Epoch 0 step 179: training loss: 1395.2441839108803\n",
      "Epoch 0 step 180: training accuarcy: 0.6715\n",
      "Epoch 0 step 180: training loss: 1395.9703598188728\n",
      "Epoch 0 step 181: training accuarcy: 0.646\n",
      "Epoch 0 step 181: training loss: 1394.592959389261\n",
      "Epoch 0 step 182: training accuarcy: 0.6485\n",
      "Epoch 0 step 182: training loss: 1395.201715373398\n",
      "Epoch 0 step 183: training accuarcy: 0.6485\n",
      "Epoch 0 step 183: training loss: 1393.1725726477807\n",
      "Epoch 0 step 184: training accuarcy: 0.6555\n",
      "Epoch 0 step 184: training loss: 1391.962095155837\n",
      "Epoch 0 step 185: training accuarcy: 0.652\n",
      "Epoch 0 step 185: training loss: 1392.4973048455136\n",
      "Epoch 0 step 186: training accuarcy: 0.6565\n",
      "Epoch 0 step 186: training loss: 1392.1386841382366\n",
      "Epoch 0 step 187: training accuarcy: 0.655\n",
      "Epoch 0 step 187: training loss: 1391.2142942183445\n",
      "Epoch 0 step 188: training accuarcy: 0.6705\n",
      "Epoch 0 step 188: training loss: 1391.5419161541681\n",
      "Epoch 0 step 189: training accuarcy: 0.668\n",
      "Epoch 0 step 189: training loss: 1391.0606822053853\n",
      "Epoch 0 step 190: training accuarcy: 0.6445\n",
      "Epoch 0 step 190: training loss: 1390.6904420543508\n",
      "Epoch 0 step 191: training accuarcy: 0.6505\n",
      "Epoch 0 step 191: training loss: 1389.8660571168675\n",
      "Epoch 0 step 192: training accuarcy: 0.652\n",
      "Epoch 0 step 192: training loss: 1390.148036857197\n",
      "Epoch 0 step 193: training accuarcy: 0.6675\n",
      "Epoch 0 step 193: training loss: 1388.3544254957706\n",
      "Epoch 0 step 194: training accuarcy: 0.678\n",
      "Epoch 0 step 194: training loss: 1389.922636267287\n",
      "Epoch 0 step 195: training accuarcy: 0.6555\n",
      "Epoch 0 step 195: training loss: 1388.784817198794\n",
      "Epoch 0 step 196: training accuarcy: 0.6415\n",
      "Epoch 0 step 196: training loss: 1388.1465998855485\n",
      "Epoch 0 step 197: training accuarcy: 0.666\n",
      "Epoch 0 step 197: training loss: 1387.7719592600793\n",
      "Epoch 0 step 198: training accuarcy: 0.6535\n",
      "Epoch 0 step 198: training loss: 1387.0369209408702\n",
      "Epoch 0 step 199: training accuarcy: 0.6795\n",
      "Epoch 0 step 199: training loss: 1387.711369567633\n",
      "Epoch 0 step 200: training accuarcy: 0.6495\n",
      "Epoch 0 step 200: training loss: 1387.8649643403828\n",
      "Epoch 0 step 201: training accuarcy: 0.6675\n",
      "Epoch 0 step 201: training loss: 1388.5663885564832\n",
      "Epoch 0 step 202: training accuarcy: 0.657\n",
      "Epoch 0 step 202: training loss: 1387.858229467672\n",
      "Epoch 0 step 203: training accuarcy: 0.666\n",
      "Epoch 0 step 203: training loss: 1388.084465459926\n",
      "Epoch 0 step 204: training accuarcy: 0.6415\n",
      "Epoch 0 step 204: training loss: 1386.7887834233763\n",
      "Epoch 0 step 205: training accuarcy: 0.6705\n",
      "Epoch 0 step 205: training loss: 1386.752693567284\n",
      "Epoch 0 step 206: training accuarcy: 0.6695\n",
      "Epoch 0 step 206: training loss: 1386.3132633537475\n",
      "Epoch 0 step 207: training accuarcy: 0.6755\n",
      "Epoch 0 step 207: training loss: 1386.8654818405041\n",
      "Epoch 0 step 208: training accuarcy: 0.667\n",
      "Epoch 0 step 208: training loss: 1386.2672044149572\n",
      "Epoch 0 step 209: training accuarcy: 0.658\n",
      "Epoch 0 step 209: training loss: 1384.452850829269\n",
      "Epoch 0 step 210: training accuarcy: 0.679\n",
      "Epoch 0 step 210: training loss: 1385.7260906029376\n",
      "Epoch 0 step 211: training accuarcy: 0.672\n",
      "Epoch 0 step 211: training loss: 1385.7068522083412\n",
      "Epoch 0 step 212: training accuarcy: 0.6695\n",
      "Epoch 0 step 212: training loss: 1386.0931529673078\n",
      "Epoch 0 step 213: training accuarcy: 0.6625\n",
      "Epoch 0 step 213: training loss: 1386.1559585273556\n",
      "Epoch 0 step 214: training accuarcy: 0.6745\n",
      "Epoch 0 step 214: training loss: 1385.041405409517\n",
      "Epoch 0 step 215: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 215: training loss: 1385.4045834460128\n",
      "Epoch 0 step 216: training accuarcy: 0.679\n",
      "Epoch 0 step 216: training loss: 1384.2982671503091\n",
      "Epoch 0 step 217: training accuarcy: 0.6795\n",
      "Epoch 0 step 217: training loss: 1384.7107267817992\n",
      "Epoch 0 step 218: training accuarcy: 0.676\n",
      "Epoch 0 step 218: training loss: 1385.0711765370643\n",
      "Epoch 0 step 219: training accuarcy: 0.675\n",
      "Epoch 0 step 219: training loss: 1384.76502772176\n",
      "Epoch 0 step 220: training accuarcy: 0.677\n",
      "Epoch 0 step 220: training loss: 1384.728783680713\n",
      "Epoch 0 step 221: training accuarcy: 0.6795\n",
      "Epoch 0 step 221: training loss: 1385.1618638440732\n",
      "Epoch 0 step 222: training accuarcy: 0.676\n",
      "Epoch 0 step 222: training loss: 1384.4903209629824\n",
      "Epoch 0 step 223: training accuarcy: 0.6885\n",
      "Epoch 0 step 223: training loss: 1385.0367317190078\n",
      "Epoch 0 step 224: training accuarcy: 0.671\n",
      "Epoch 0 step 224: training loss: 1384.6729797127293\n",
      "Epoch 0 step 225: training accuarcy: 0.68\n",
      "Epoch 0 step 225: training loss: 1384.3502104933013\n",
      "Epoch 0 step 226: training accuarcy: 0.6915\n",
      "Epoch 0 step 226: training loss: 1384.0359873021218\n",
      "Epoch 0 step 227: training accuarcy: 0.679\n",
      "Epoch 0 step 227: training loss: 1384.681468922643\n",
      "Epoch 0 step 228: training accuarcy: 0.6815\n",
      "Epoch 0 step 228: training loss: 1384.3720839359112\n",
      "Epoch 0 step 229: training accuarcy: 0.6825\n",
      "Epoch 0 step 229: training loss: 1384.5894604278703\n",
      "Epoch 0 step 230: training accuarcy: 0.6645\n",
      "Epoch 0 step 230: training loss: 1384.1300663884522\n",
      "Epoch 0 step 231: training accuarcy: 0.673\n",
      "Epoch 0 step 231: training loss: 1384.5417797348362\n",
      "Epoch 0 step 232: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 232: training loss: 1384.6430926680066\n",
      "Epoch 0 step 233: training accuarcy: 0.6675\n",
      "Epoch 0 step 233: training loss: 1384.7564316778794\n",
      "Epoch 0 step 234: training accuarcy: 0.6735\n",
      "Epoch 0 step 234: training loss: 1383.814292920936\n",
      "Epoch 0 step 235: training accuarcy: 0.6880000000000001\n",
      "Epoch 0 step 235: training loss: 1383.4597822019607\n",
      "Epoch 0 step 236: training accuarcy: 0.6845\n",
      "Epoch 0 step 236: training loss: 1383.520036681344\n",
      "Epoch 0 step 237: training accuarcy: 0.6905\n",
      "Epoch 0 step 237: training loss: 1383.7466198490222\n",
      "Epoch 0 step 238: training accuarcy: 0.6985\n",
      "Epoch 0 step 238: training loss: 1383.7335338168095\n",
      "Epoch 0 step 239: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 239: training loss: 1383.7897956756701\n",
      "Epoch 0 step 240: training accuarcy: 0.673\n",
      "Epoch 0 step 240: training loss: 1383.7881327972102\n",
      "Epoch 0 step 241: training accuarcy: 0.6895\n",
      "Epoch 0 step 241: training loss: 1384.7168372776828\n",
      "Epoch 0 step 242: training accuarcy: 0.6665\n",
      "Epoch 0 step 242: training loss: 1382.8973482997887\n",
      "Epoch 0 step 243: training accuarcy: 0.685\n",
      "Epoch 0 step 243: training loss: 1384.1229099567252\n",
      "Epoch 0 step 244: training accuarcy: 0.6930000000000001\n",
      "Epoch 0 step 244: training loss: 1384.0828556763552\n",
      "Epoch 0 step 245: training accuarcy: 0.6785\n",
      "Epoch 0 step 245: training loss: 1383.657407813066\n",
      "Epoch 0 step 246: training accuarcy: 0.6935\n",
      "Epoch 0 step 246: training loss: 1383.9463871150879\n",
      "Epoch 0 step 247: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 247: training loss: 1383.6306263543565\n",
      "Epoch 0 step 248: training accuarcy: 0.6955\n",
      "Epoch 0 step 248: training loss: 1382.9814067511486\n",
      "Epoch 0 step 249: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 249: training loss: 1383.2591692810315\n",
      "Epoch 0 step 250: training accuarcy: 0.687\n",
      "Epoch 0 step 250: training loss: 1383.591079858919\n",
      "Epoch 0 step 251: training accuarcy: 0.676\n",
      "Epoch 0 step 251: training loss: 1383.3625601234835\n",
      "Epoch 0 step 252: training accuarcy: 0.6875\n",
      "Epoch 0 step 252: training loss: 1383.0751395459727\n",
      "Epoch 0 step 253: training accuarcy: 0.6980000000000001\n",
      "Epoch 0 step 253: training loss: 1383.900468432526\n",
      "Epoch 0 step 254: training accuarcy: 0.677\n",
      "Epoch 0 step 254: training loss: 1383.7509882327959\n",
      "Epoch 0 step 255: training accuarcy: 0.687\n",
      "Epoch 0 step 255: training loss: 1383.949941451294\n",
      "Epoch 0 step 256: training accuarcy: 0.673\n",
      "Epoch 0 step 256: training loss: 1382.9072025880077\n",
      "Epoch 0 step 257: training accuarcy: 0.6890000000000001\n",
      "Epoch 0 step 257: training loss: 1383.3938732460037\n",
      "Epoch 0 step 258: training accuarcy: 0.6945\n",
      "Epoch 0 step 258: training loss: 1383.6231843877981\n",
      "Epoch 0 step 259: training accuarcy: 0.7035\n",
      "Epoch 0 step 259: training loss: 1383.1637076390673\n",
      "Epoch 0 step 260: training accuarcy: 0.683\n",
      "Epoch 0 step 260: training loss: 1383.6408113188377\n",
      "Epoch 0 step 261: training accuarcy: 0.6915\n",
      "Epoch 0 step 261: training loss: 1383.8668465713042\n",
      "Epoch 0 step 262: training accuarcy: 0.6755\n",
      "Epoch 0 step 262: training loss: 543.4043466801741\n",
      "Epoch 0 step 263: training accuarcy: 0.6948717948717948\n",
      "Epoch 0: train loss 5440.148758324528, train accuarcy 0.6237958073616028\n",
      "Epoch 0: valid loss 1364.5724493932605, valid accuarcy 0.7122498750686646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████                                                                                                                                     | 1/8 [01:56<13:35, 116.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1381.9288631656204\n",
      "Epoch 1 step 264: training accuarcy: 0.717\n",
      "Epoch 1 step 264: training loss: 1382.8740386957263\n",
      "Epoch 1 step 265: training accuarcy: 0.7205\n",
      "Epoch 1 step 265: training loss: 1383.163731968013\n",
      "Epoch 1 step 266: training accuarcy: 0.6905\n",
      "Epoch 1 step 266: training loss: 1382.8545891421277\n",
      "Epoch 1 step 267: training accuarcy: 0.6885\n",
      "Epoch 1 step 267: training loss: 1382.4083080815135\n",
      "Epoch 1 step 268: training accuarcy: 0.6975\n",
      "Epoch 1 step 268: training loss: 1382.9286292596848\n",
      "Epoch 1 step 269: training accuarcy: 0.7055\n",
      "Epoch 1 step 269: training loss: 1382.8100783364525\n",
      "Epoch 1 step 270: training accuarcy: 0.713\n",
      "Epoch 1 step 270: training loss: 1383.6546622408448\n",
      "Epoch 1 step 271: training accuarcy: 0.707\n",
      "Epoch 1 step 271: training loss: 1382.8825608359637\n",
      "Epoch 1 step 272: training accuarcy: 0.705\n",
      "Epoch 1 step 272: training loss: 1383.230800394471\n",
      "Epoch 1 step 273: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 273: training loss: 1382.6469405966727\n",
      "Epoch 1 step 274: training accuarcy: 0.708\n",
      "Epoch 1 step 274: training loss: 1382.7744678003885\n",
      "Epoch 1 step 275: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 275: training loss: 1381.9167723042701\n",
      "Epoch 1 step 276: training accuarcy: 0.7025\n",
      "Epoch 1 step 276: training loss: 1383.7791531918742\n",
      "Epoch 1 step 277: training accuarcy: 0.6935\n",
      "Epoch 1 step 277: training loss: 1382.618443430913\n",
      "Epoch 1 step 278: training accuarcy: 0.686\n",
      "Epoch 1 step 278: training loss: 1382.897000637387\n",
      "Epoch 1 step 279: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 279: training loss: 1383.1727559333065\n",
      "Epoch 1 step 280: training accuarcy: 0.7155\n",
      "Epoch 1 step 280: training loss: 1382.4588360380956\n",
      "Epoch 1 step 281: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 281: training loss: 1382.0950391056456\n",
      "Epoch 1 step 282: training accuarcy: 0.7045\n",
      "Epoch 1 step 282: training loss: 1382.855965572646\n",
      "Epoch 1 step 283: training accuarcy: 0.6855\n",
      "Epoch 1 step 283: training loss: 1382.347269550663\n",
      "Epoch 1 step 284: training accuarcy: 0.7105\n",
      "Epoch 1 step 284: training loss: 1382.1051984514772\n",
      "Epoch 1 step 285: training accuarcy: 0.7115\n",
      "Epoch 1 step 285: training loss: 1382.740802266272\n",
      "Epoch 1 step 286: training accuarcy: 0.7025\n",
      "Epoch 1 step 286: training loss: 1382.604197804284\n",
      "Epoch 1 step 287: training accuarcy: 0.7035\n",
      "Epoch 1 step 287: training loss: 1382.7160219494215\n",
      "Epoch 1 step 288: training accuarcy: 0.687\n",
      "Epoch 1 step 288: training loss: 1382.603521280423\n",
      "Epoch 1 step 289: training accuarcy: 0.721\n",
      "Epoch 1 step 289: training loss: 1383.6716749734348\n",
      "Epoch 1 step 290: training accuarcy: 0.6945\n",
      "Epoch 1 step 290: training loss: 1381.4903735619505\n",
      "Epoch 1 step 291: training accuarcy: 0.7125\n",
      "Epoch 1 step 291: training loss: 1381.8795672906072\n",
      "Epoch 1 step 292: training accuarcy: 0.715\n",
      "Epoch 1 step 292: training loss: 1382.0403105492915\n",
      "Epoch 1 step 293: training accuarcy: 0.704\n",
      "Epoch 1 step 293: training loss: 1382.213735580636\n",
      "Epoch 1 step 294: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 294: training loss: 1381.6778264213972\n",
      "Epoch 1 step 295: training accuarcy: 0.7105\n",
      "Epoch 1 step 295: training loss: 1382.3367607082669\n",
      "Epoch 1 step 296: training accuarcy: 0.6985\n",
      "Epoch 1 step 296: training loss: 1382.3709388904763\n",
      "Epoch 1 step 297: training accuarcy: 0.7015\n",
      "Epoch 1 step 297: training loss: 1382.2225037767334\n",
      "Epoch 1 step 298: training accuarcy: 0.704\n",
      "Epoch 1 step 298: training loss: 1383.0822877574485\n",
      "Epoch 1 step 299: training accuarcy: 0.7075\n",
      "Epoch 1 step 299: training loss: 1382.1294283193467\n",
      "Epoch 1 step 300: training accuarcy: 0.706\n",
      "Epoch 1 step 300: training loss: 1382.7811838232303\n",
      "Epoch 1 step 301: training accuarcy: 0.7085\n",
      "Epoch 1 step 301: training loss: 1382.1801357742652\n",
      "Epoch 1 step 302: training accuarcy: 0.7045\n",
      "Epoch 1 step 302: training loss: 1383.6370542622221\n",
      "Epoch 1 step 303: training accuarcy: 0.6945\n",
      "Epoch 1 step 303: training loss: 1381.5967318983458\n",
      "Epoch 1 step 304: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 304: training loss: 1382.0156740190553\n",
      "Epoch 1 step 305: training accuarcy: 0.6945\n",
      "Epoch 1 step 305: training loss: 1382.914604367383\n",
      "Epoch 1 step 306: training accuarcy: 0.7135\n",
      "Epoch 1 step 306: training loss: 1382.4159644398544\n",
      "Epoch 1 step 307: training accuarcy: 0.6945\n",
      "Epoch 1 step 307: training loss: 1382.2066528834987\n",
      "Epoch 1 step 308: training accuarcy: 0.715\n",
      "Epoch 1 step 308: training loss: 1382.8782353445918\n",
      "Epoch 1 step 309: training accuarcy: 0.6985\n",
      "Epoch 1 step 309: training loss: 1383.1439764667423\n",
      "Epoch 1 step 310: training accuarcy: 0.7155\n",
      "Epoch 1 step 310: training loss: 1383.7972791931427\n",
      "Epoch 1 step 311: training accuarcy: 0.678\n",
      "Epoch 1 step 311: training loss: 1383.442139773331\n",
      "Epoch 1 step 312: training accuarcy: 0.6835\n",
      "Epoch 1 step 312: training loss: 1383.9943493990477\n",
      "Epoch 1 step 313: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 313: training loss: 1383.5508810386207\n",
      "Epoch 1 step 314: training accuarcy: 0.6925\n",
      "Epoch 1 step 314: training loss: 1381.7084569186156\n",
      "Epoch 1 step 315: training accuarcy: 0.707\n",
      "Epoch 1 step 315: training loss: 1383.1185781323384\n",
      "Epoch 1 step 316: training accuarcy: 0.676\n",
      "Epoch 1 step 316: training loss: 1382.6813425345351\n",
      "Epoch 1 step 317: training accuarcy: 0.7085\n",
      "Epoch 1 step 317: training loss: 1382.9401829074354\n",
      "Epoch 1 step 318: training accuarcy: 0.704\n",
      "Epoch 1 step 318: training loss: 1382.4810101800197\n",
      "Epoch 1 step 319: training accuarcy: 0.705\n",
      "Epoch 1 step 319: training loss: 1382.7056506569033\n",
      "Epoch 1 step 320: training accuarcy: 0.6975\n",
      "Epoch 1 step 320: training loss: 1384.1321980873631\n",
      "Epoch 1 step 321: training accuarcy: 0.6990000000000001\n",
      "Epoch 1 step 321: training loss: 1382.7558694759014\n",
      "Epoch 1 step 322: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 322: training loss: 1384.0358880329659\n",
      "Epoch 1 step 323: training accuarcy: 0.678\n",
      "Epoch 1 step 323: training loss: 1381.5237783865325\n",
      "Epoch 1 step 324: training accuarcy: 0.7175\n",
      "Epoch 1 step 324: training loss: 1383.970747610062\n",
      "Epoch 1 step 325: training accuarcy: 0.67\n",
      "Epoch 1 step 325: training loss: 1382.3103967729448\n",
      "Epoch 1 step 326: training accuarcy: 0.7045\n",
      "Epoch 1 step 326: training loss: 1383.5611217801684\n",
      "Epoch 1 step 327: training accuarcy: 0.6765\n",
      "Epoch 1 step 327: training loss: 1382.9128421449436\n",
      "Epoch 1 step 328: training accuarcy: 0.6895\n",
      "Epoch 1 step 328: training loss: 1383.605240968852\n",
      "Epoch 1 step 329: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 329: training loss: 1383.2134913442394\n",
      "Epoch 1 step 330: training accuarcy: 0.687\n",
      "Epoch 1 step 330: training loss: 1383.0159960287324\n",
      "Epoch 1 step 331: training accuarcy: 0.7105\n",
      "Epoch 1 step 331: training loss: 1382.6558428546018\n",
      "Epoch 1 step 332: training accuarcy: 0.6865\n",
      "Epoch 1 step 332: training loss: 1382.1965389165664\n",
      "Epoch 1 step 333: training accuarcy: 0.7215\n",
      "Epoch 1 step 333: training loss: 1383.8771682819663\n",
      "Epoch 1 step 334: training accuarcy: 0.6865\n",
      "Epoch 1 step 334: training loss: 1382.6534074027395\n",
      "Epoch 1 step 335: training accuarcy: 0.7115\n",
      "Epoch 1 step 335: training loss: 1382.5338400096393\n",
      "Epoch 1 step 336: training accuarcy: 0.6955\n",
      "Epoch 1 step 336: training loss: 1382.6527385162049\n",
      "Epoch 1 step 337: training accuarcy: 0.6985\n",
      "Epoch 1 step 337: training loss: 1382.54035960109\n",
      "Epoch 1 step 338: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 338: training loss: 1383.4101876197615\n",
      "Epoch 1 step 339: training accuarcy: 0.6885\n",
      "Epoch 1 step 339: training loss: 1383.8339781694206\n",
      "Epoch 1 step 340: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 340: training loss: 1382.0082540682347\n",
      "Epoch 1 step 341: training accuarcy: 0.7065\n",
      "Epoch 1 step 341: training loss: 1382.569257412533\n",
      "Epoch 1 step 342: training accuarcy: 0.7185\n",
      "Epoch 1 step 342: training loss: 1384.028029550414\n",
      "Epoch 1 step 343: training accuarcy: 0.6835\n",
      "Epoch 1 step 343: training loss: 1382.5080820907128\n",
      "Epoch 1 step 344: training accuarcy: 0.705\n",
      "Epoch 1 step 344: training loss: 1381.7664094446702\n",
      "Epoch 1 step 345: training accuarcy: 0.7085\n",
      "Epoch 1 step 345: training loss: 1382.826825193834\n",
      "Epoch 1 step 346: training accuarcy: 0.686\n",
      "Epoch 1 step 346: training loss: 1382.8671398346373\n",
      "Epoch 1 step 347: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 347: training loss: 1383.0618994443694\n",
      "Epoch 1 step 348: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 348: training loss: 1382.913775212559\n",
      "Epoch 1 step 349: training accuarcy: 0.7135\n",
      "Epoch 1 step 349: training loss: 1382.9219114150665\n",
      "Epoch 1 step 350: training accuarcy: 0.676\n",
      "Epoch 1 step 350: training loss: 1382.7557897761963\n",
      "Epoch 1 step 351: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 351: training loss: 1383.6413111256625\n",
      "Epoch 1 step 352: training accuarcy: 0.7025\n",
      "Epoch 1 step 352: training loss: 1382.9694863399923\n",
      "Epoch 1 step 353: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 353: training loss: 1383.6165630754715\n",
      "Epoch 1 step 354: training accuarcy: 0.681\n",
      "Epoch 1 step 354: training loss: 1382.7907732011336\n",
      "Epoch 1 step 355: training accuarcy: 0.706\n",
      "Epoch 1 step 355: training loss: 1382.8123922637292\n",
      "Epoch 1 step 356: training accuarcy: 0.6995\n",
      "Epoch 1 step 356: training loss: 1382.4637338122902\n",
      "Epoch 1 step 357: training accuarcy: 0.7055\n",
      "Epoch 1 step 357: training loss: 1383.80813387469\n",
      "Epoch 1 step 358: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 358: training loss: 1382.7484392429235\n",
      "Epoch 1 step 359: training accuarcy: 0.7005\n",
      "Epoch 1 step 359: training loss: 1382.755986316608\n",
      "Epoch 1 step 360: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 360: training loss: 1383.4039634541498\n",
      "Epoch 1 step 361: training accuarcy: 0.6935\n",
      "Epoch 1 step 361: training loss: 1381.9585902691836\n",
      "Epoch 1 step 362: training accuarcy: 0.6935\n",
      "Epoch 1 step 362: training loss: 1382.4376920135655\n",
      "Epoch 1 step 363: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 363: training loss: 1383.142909074914\n",
      "Epoch 1 step 364: training accuarcy: 0.6915\n",
      "Epoch 1 step 364: training loss: 1382.7535364937187\n",
      "Epoch 1 step 365: training accuarcy: 0.683\n",
      "Epoch 1 step 365: training loss: 1382.480890714615\n",
      "Epoch 1 step 366: training accuarcy: 0.7055\n",
      "Epoch 1 step 366: training loss: 1382.3038037718766\n",
      "Epoch 1 step 367: training accuarcy: 0.709\n",
      "Epoch 1 step 367: training loss: 1382.739269251073\n",
      "Epoch 1 step 368: training accuarcy: 0.7175\n",
      "Epoch 1 step 368: training loss: 1383.689325831669\n",
      "Epoch 1 step 369: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 369: training loss: 1382.1653749408774\n",
      "Epoch 1 step 370: training accuarcy: 0.705\n",
      "Epoch 1 step 370: training loss: 1382.271805402761\n",
      "Epoch 1 step 371: training accuarcy: 0.6930000000000001\n",
      "Epoch 1 step 371: training loss: 1382.1772721829263\n",
      "Epoch 1 step 372: training accuarcy: 0.678\n",
      "Epoch 1 step 372: training loss: 1383.5498644290935\n",
      "Epoch 1 step 373: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 373: training loss: 1383.4072072534866\n",
      "Epoch 1 step 374: training accuarcy: 0.6975\n",
      "Epoch 1 step 374: training loss: 1383.7691793383644\n",
      "Epoch 1 step 375: training accuarcy: 0.6805\n",
      "Epoch 1 step 375: training loss: 1382.9624221801682\n",
      "Epoch 1 step 376: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 376: training loss: 1382.6337499822928\n",
      "Epoch 1 step 377: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 377: training loss: 1382.8181639895981\n",
      "Epoch 1 step 378: training accuarcy: 0.6855\n",
      "Epoch 1 step 378: training loss: 1382.8295721896102\n",
      "Epoch 1 step 379: training accuarcy: 0.6965\n",
      "Epoch 1 step 379: training loss: 1383.190071798451\n",
      "Epoch 1 step 380: training accuarcy: 0.686\n",
      "Epoch 1 step 380: training loss: 1383.2121395904833\n",
      "Epoch 1 step 381: training accuarcy: 0.6975\n",
      "Epoch 1 step 381: training loss: 1382.4927760719597\n",
      "Epoch 1 step 382: training accuarcy: 0.705\n",
      "Epoch 1 step 382: training loss: 1382.094002018633\n",
      "Epoch 1 step 383: training accuarcy: 0.6985\n",
      "Epoch 1 step 383: training loss: 1382.6909104015335\n",
      "Epoch 1 step 384: training accuarcy: 0.714\n",
      "Epoch 1 step 384: training loss: 1382.6558052446471\n",
      "Epoch 1 step 385: training accuarcy: 0.6925\n",
      "Epoch 1 step 385: training loss: 1382.9869926566366\n",
      "Epoch 1 step 386: training accuarcy: 0.7055\n",
      "Epoch 1 step 386: training loss: 1383.1745295630974\n",
      "Epoch 1 step 387: training accuarcy: 0.7015\n",
      "Epoch 1 step 387: training loss: 1383.740171688482\n",
      "Epoch 1 step 388: training accuarcy: 0.6815\n",
      "Epoch 1 step 388: training loss: 1382.3038971885746\n",
      "Epoch 1 step 389: training accuarcy: 0.6985\n",
      "Epoch 1 step 389: training loss: 1382.0784021321633\n",
      "Epoch 1 step 390: training accuarcy: 0.7125\n",
      "Epoch 1 step 390: training loss: 1382.910251339042\n",
      "Epoch 1 step 391: training accuarcy: 0.7025\n",
      "Epoch 1 step 391: training loss: 1382.7214430832714\n",
      "Epoch 1 step 392: training accuarcy: 0.7005\n",
      "Epoch 1 step 392: training loss: 1382.3957522901\n",
      "Epoch 1 step 393: training accuarcy: 0.6985\n",
      "Epoch 1 step 393: training loss: 1382.8067285333607\n",
      "Epoch 1 step 394: training accuarcy: 0.7015\n",
      "Epoch 1 step 394: training loss: 1383.1250562435052\n",
      "Epoch 1 step 395: training accuarcy: 0.6915\n",
      "Epoch 1 step 395: training loss: 1382.403334945549\n",
      "Epoch 1 step 396: training accuarcy: 0.685\n",
      "Epoch 1 step 396: training loss: 1383.9931185001024\n",
      "Epoch 1 step 397: training accuarcy: 0.68\n",
      "Epoch 1 step 397: training loss: 1383.5348718868042\n",
      "Epoch 1 step 398: training accuarcy: 0.6955\n",
      "Epoch 1 step 398: training loss: 1382.6526724172686\n",
      "Epoch 1 step 399: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 399: training loss: 1382.8618313583374\n",
      "Epoch 1 step 400: training accuarcy: 0.7000000000000001\n",
      "Epoch 1 step 400: training loss: 1381.962950276407\n",
      "Epoch 1 step 401: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 401: training loss: 1382.4095264174887\n",
      "Epoch 1 step 402: training accuarcy: 0.7065\n",
      "Epoch 1 step 402: training loss: 1382.7390575774025\n",
      "Epoch 1 step 403: training accuarcy: 0.704\n",
      "Epoch 1 step 403: training loss: 1382.163124460385\n",
      "Epoch 1 step 404: training accuarcy: 0.711\n",
      "Epoch 1 step 404: training loss: 1383.0447394999687\n",
      "Epoch 1 step 405: training accuarcy: 0.7065\n",
      "Epoch 1 step 405: training loss: 1382.676796858335\n",
      "Epoch 1 step 406: training accuarcy: 0.683\n",
      "Epoch 1 step 406: training loss: 1382.1000106814172\n",
      "Epoch 1 step 407: training accuarcy: 0.714\n",
      "Epoch 1 step 407: training loss: 1381.5575924586026\n",
      "Epoch 1 step 408: training accuarcy: 0.7065\n",
      "Epoch 1 step 408: training loss: 1383.2786066793713\n",
      "Epoch 1 step 409: training accuarcy: 0.6985\n",
      "Epoch 1 step 409: training loss: 1382.8534365679845\n",
      "Epoch 1 step 410: training accuarcy: 0.687\n",
      "Epoch 1 step 410: training loss: 1383.0992314519851\n",
      "Epoch 1 step 411: training accuarcy: 0.6825\n",
      "Epoch 1 step 411: training loss: 1383.1954309821247\n",
      "Epoch 1 step 412: training accuarcy: 0.685\n",
      "Epoch 1 step 412: training loss: 1382.0851836435083\n",
      "Epoch 1 step 413: training accuarcy: 0.7025\n",
      "Epoch 1 step 413: training loss: 1382.5531298113149\n",
      "Epoch 1 step 414: training accuarcy: 0.7115\n",
      "Epoch 1 step 414: training loss: 1384.082005387279\n",
      "Epoch 1 step 415: training accuarcy: 0.681\n",
      "Epoch 1 step 415: training loss: 1383.5540138643319\n",
      "Epoch 1 step 416: training accuarcy: 0.711\n",
      "Epoch 1 step 416: training loss: 1383.0056318061527\n",
      "Epoch 1 step 417: training accuarcy: 0.7035\n",
      "Epoch 1 step 417: training loss: 1382.7055028423792\n",
      "Epoch 1 step 418: training accuarcy: 0.6920000000000001\n",
      "Epoch 1 step 418: training loss: 1383.7597999085824\n",
      "Epoch 1 step 419: training accuarcy: 0.6875\n",
      "Epoch 1 step 419: training loss: 1383.160496842933\n",
      "Epoch 1 step 420: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 420: training loss: 1382.4256275730318\n",
      "Epoch 1 step 421: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 421: training loss: 1382.6962969181238\n",
      "Epoch 1 step 422: training accuarcy: 0.6945\n",
      "Epoch 1 step 422: training loss: 1381.8012752851005\n",
      "Epoch 1 step 423: training accuarcy: 0.705\n",
      "Epoch 1 step 423: training loss: 1382.7861544697892\n",
      "Epoch 1 step 424: training accuarcy: 0.7165\n",
      "Epoch 1 step 424: training loss: 1382.6314436500554\n",
      "Epoch 1 step 425: training accuarcy: 0.6895\n",
      "Epoch 1 step 425: training loss: 1383.3840346164664\n",
      "Epoch 1 step 426: training accuarcy: 0.6805\n",
      "Epoch 1 step 426: training loss: 1383.318399585697\n",
      "Epoch 1 step 427: training accuarcy: 0.6930000000000001\n",
      "Epoch 1 step 427: training loss: 1382.7635216684082\n",
      "Epoch 1 step 428: training accuarcy: 0.6915\n",
      "Epoch 1 step 428: training loss: 1382.6812118961857\n",
      "Epoch 1 step 429: training accuarcy: 0.6985\n",
      "Epoch 1 step 429: training loss: 1381.9758926390182\n",
      "Epoch 1 step 430: training accuarcy: 0.708\n",
      "Epoch 1 step 430: training loss: 1384.1621375146858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 431: training accuarcy: 0.687\n",
      "Epoch 1 step 431: training loss: 1383.544529975551\n",
      "Epoch 1 step 432: training accuarcy: 0.6935\n",
      "Epoch 1 step 432: training loss: 1382.7406523463846\n",
      "Epoch 1 step 433: training accuarcy: 0.705\n",
      "Epoch 1 step 433: training loss: 1383.8302853839634\n",
      "Epoch 1 step 434: training accuarcy: 0.685\n",
      "Epoch 1 step 434: training loss: 1382.263804137278\n",
      "Epoch 1 step 435: training accuarcy: 0.7065\n",
      "Epoch 1 step 435: training loss: 1383.2367647967167\n",
      "Epoch 1 step 436: training accuarcy: 0.7015\n",
      "Epoch 1 step 436: training loss: 1382.6569232613456\n",
      "Epoch 1 step 437: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 437: training loss: 1382.3876202297586\n",
      "Epoch 1 step 438: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 438: training loss: 1382.6201850273494\n",
      "Epoch 1 step 439: training accuarcy: 0.7025\n",
      "Epoch 1 step 439: training loss: 1383.1003555990235\n",
      "Epoch 1 step 440: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 440: training loss: 1383.0934235754376\n",
      "Epoch 1 step 441: training accuarcy: 0.6815\n",
      "Epoch 1 step 441: training loss: 1381.3731106128469\n",
      "Epoch 1 step 442: training accuarcy: 0.7265\n",
      "Epoch 1 step 442: training loss: 1382.7451155627007\n",
      "Epoch 1 step 443: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 443: training loss: 1384.6850145328424\n",
      "Epoch 1 step 444: training accuarcy: 0.687\n",
      "Epoch 1 step 444: training loss: 1382.4231795191288\n",
      "Epoch 1 step 445: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 445: training loss: 1381.3712672140468\n",
      "Epoch 1 step 446: training accuarcy: 0.7075\n",
      "Epoch 1 step 446: training loss: 1382.635748590452\n",
      "Epoch 1 step 447: training accuarcy: 0.707\n",
      "Epoch 1 step 447: training loss: 1382.7256532186032\n",
      "Epoch 1 step 448: training accuarcy: 0.7035\n",
      "Epoch 1 step 448: training loss: 1382.8298777321188\n",
      "Epoch 1 step 449: training accuarcy: 0.7010000000000001\n",
      "Epoch 1 step 449: training loss: 1382.5597717107937\n",
      "Epoch 1 step 450: training accuarcy: 0.7015\n",
      "Epoch 1 step 450: training loss: 1382.3024001630042\n",
      "Epoch 1 step 451: training accuarcy: 0.714\n",
      "Epoch 1 step 451: training loss: 1382.9823126706394\n",
      "Epoch 1 step 452: training accuarcy: 0.7095\n",
      "Epoch 1 step 452: training loss: 1383.359763906216\n",
      "Epoch 1 step 453: training accuarcy: 0.7075\n",
      "Epoch 1 step 453: training loss: 1382.7032253368802\n",
      "Epoch 1 step 454: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 454: training loss: 1383.0456283862827\n",
      "Epoch 1 step 455: training accuarcy: 0.7045\n",
      "Epoch 1 step 455: training loss: 1382.5595065311124\n",
      "Epoch 1 step 456: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 456: training loss: 1383.190646323468\n",
      "Epoch 1 step 457: training accuarcy: 0.6855\n",
      "Epoch 1 step 457: training loss: 1382.804726310822\n",
      "Epoch 1 step 458: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 458: training loss: 1382.5027377952918\n",
      "Epoch 1 step 459: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 459: training loss: 1382.3675509010604\n",
      "Epoch 1 step 460: training accuarcy: 0.7085\n",
      "Epoch 1 step 460: training loss: 1382.3358219454171\n",
      "Epoch 1 step 461: training accuarcy: 0.709\n",
      "Epoch 1 step 461: training loss: 1383.2184251041006\n",
      "Epoch 1 step 462: training accuarcy: 0.6925\n",
      "Epoch 1 step 462: training loss: 1382.2870642875325\n",
      "Epoch 1 step 463: training accuarcy: 0.6935\n",
      "Epoch 1 step 463: training loss: 1382.2643850882519\n",
      "Epoch 1 step 464: training accuarcy: 0.7045\n",
      "Epoch 1 step 464: training loss: 1383.1230374451648\n",
      "Epoch 1 step 465: training accuarcy: 0.6995\n",
      "Epoch 1 step 465: training loss: 1383.2987436631956\n",
      "Epoch 1 step 466: training accuarcy: 0.7045\n",
      "Epoch 1 step 466: training loss: 1382.7109683280892\n",
      "Epoch 1 step 467: training accuarcy: 0.7005\n",
      "Epoch 1 step 467: training loss: 1383.0758890406253\n",
      "Epoch 1 step 468: training accuarcy: 0.6825\n",
      "Epoch 1 step 468: training loss: 1382.9623144388481\n",
      "Epoch 1 step 469: training accuarcy: 0.6915\n",
      "Epoch 1 step 469: training loss: 1382.4193748619339\n",
      "Epoch 1 step 470: training accuarcy: 0.6990000000000001\n",
      "Epoch 1 step 470: training loss: 1383.2413406686633\n",
      "Epoch 1 step 471: training accuarcy: 0.705\n",
      "Epoch 1 step 471: training loss: 1382.8755779792227\n",
      "Epoch 1 step 472: training accuarcy: 0.71\n",
      "Epoch 1 step 472: training loss: 1384.5536326024753\n",
      "Epoch 1 step 473: training accuarcy: 0.7005\n",
      "Epoch 1 step 473: training loss: 1383.8650324625926\n",
      "Epoch 1 step 474: training accuarcy: 0.6825\n",
      "Epoch 1 step 474: training loss: 1382.5799854682343\n",
      "Epoch 1 step 475: training accuarcy: 0.7035\n",
      "Epoch 1 step 475: training loss: 1382.929384874539\n",
      "Epoch 1 step 476: training accuarcy: 0.6910000000000001\n",
      "Epoch 1 step 476: training loss: 1382.3162854235536\n",
      "Epoch 1 step 477: training accuarcy: 0.7030000000000001\n",
      "Epoch 1 step 477: training loss: 1382.4377693822971\n",
      "Epoch 1 step 478: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 478: training loss: 1382.3635648881257\n",
      "Epoch 1 step 479: training accuarcy: 0.71\n",
      "Epoch 1 step 479: training loss: 1383.436659744837\n",
      "Epoch 1 step 480: training accuarcy: 0.7005\n",
      "Epoch 1 step 480: training loss: 1384.059067303122\n",
      "Epoch 1 step 481: training accuarcy: 0.6785\n",
      "Epoch 1 step 481: training loss: 1382.4524769815416\n",
      "Epoch 1 step 482: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 482: training loss: 1383.1409915844922\n",
      "Epoch 1 step 483: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 483: training loss: 1383.416869857891\n",
      "Epoch 1 step 484: training accuarcy: 0.6985\n",
      "Epoch 1 step 484: training loss: 1382.9798447799778\n",
      "Epoch 1 step 485: training accuarcy: 0.7005\n",
      "Epoch 1 step 485: training loss: 1383.5609336322314\n",
      "Epoch 1 step 486: training accuarcy: 0.6935\n",
      "Epoch 1 step 486: training loss: 1382.8419990154086\n",
      "Epoch 1 step 487: training accuarcy: 0.6955\n",
      "Epoch 1 step 487: training loss: 1382.589517115585\n",
      "Epoch 1 step 488: training accuarcy: 0.7035\n",
      "Epoch 1 step 488: training loss: 1383.3036605562438\n",
      "Epoch 1 step 489: training accuarcy: 0.6735\n",
      "Epoch 1 step 489: training loss: 1383.928445826889\n",
      "Epoch 1 step 490: training accuarcy: 0.6835\n",
      "Epoch 1 step 490: training loss: 1383.2556960525767\n",
      "Epoch 1 step 491: training accuarcy: 0.6835\n",
      "Epoch 1 step 491: training loss: 1382.718364897062\n",
      "Epoch 1 step 492: training accuarcy: 0.7035\n",
      "Epoch 1 step 492: training loss: 1383.609179760611\n",
      "Epoch 1 step 493: training accuarcy: 0.6950000000000001\n",
      "Epoch 1 step 493: training loss: 1382.4327484714192\n",
      "Epoch 1 step 494: training accuarcy: 0.6890000000000001\n",
      "Epoch 1 step 494: training loss: 1383.6456773474176\n",
      "Epoch 1 step 495: training accuarcy: 0.681\n",
      "Epoch 1 step 495: training loss: 1383.8822177216502\n",
      "Epoch 1 step 496: training accuarcy: 0.6835\n",
      "Epoch 1 step 496: training loss: 1382.5479717040791\n",
      "Epoch 1 step 497: training accuarcy: 0.7000000000000001\n",
      "Epoch 1 step 497: training loss: 1383.143312785016\n",
      "Epoch 1 step 498: training accuarcy: 0.6985\n",
      "Epoch 1 step 498: training loss: 1383.096521693051\n",
      "Epoch 1 step 499: training accuarcy: 0.6895\n",
      "Epoch 1 step 499: training loss: 1383.738875247877\n",
      "Epoch 1 step 500: training accuarcy: 0.6855\n",
      "Epoch 1 step 500: training loss: 1382.5091192079044\n",
      "Epoch 1 step 501: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 501: training loss: 1382.6927263337775\n",
      "Epoch 1 step 502: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 502: training loss: 1382.9637716656537\n",
      "Epoch 1 step 503: training accuarcy: 0.6895\n",
      "Epoch 1 step 503: training loss: 1382.778721164391\n",
      "Epoch 1 step 504: training accuarcy: 0.7085\n",
      "Epoch 1 step 504: training loss: 1383.0861073809492\n",
      "Epoch 1 step 505: training accuarcy: 0.687\n",
      "Epoch 1 step 505: training loss: 1382.4815568495792\n",
      "Epoch 1 step 506: training accuarcy: 0.6955\n",
      "Epoch 1 step 506: training loss: 1383.5547116663681\n",
      "Epoch 1 step 507: training accuarcy: 0.6855\n",
      "Epoch 1 step 507: training loss: 1382.2612753187495\n",
      "Epoch 1 step 508: training accuarcy: 0.7035\n",
      "Epoch 1 step 508: training loss: 1383.7144544686528\n",
      "Epoch 1 step 509: training accuarcy: 0.6785\n",
      "Epoch 1 step 509: training loss: 1384.514737021985\n",
      "Epoch 1 step 510: training accuarcy: 0.681\n",
      "Epoch 1 step 510: training loss: 1382.6601887763309\n",
      "Epoch 1 step 511: training accuarcy: 0.7125\n",
      "Epoch 1 step 511: training loss: 1383.509457709399\n",
      "Epoch 1 step 512: training accuarcy: 0.6940000000000001\n",
      "Epoch 1 step 512: training loss: 1382.3316493009982\n",
      "Epoch 1 step 513: training accuarcy: 0.6980000000000001\n",
      "Epoch 1 step 513: training loss: 1383.1139720090362\n",
      "Epoch 1 step 514: training accuarcy: 0.6895\n",
      "Epoch 1 step 514: training loss: 1383.1273729447241\n",
      "Epoch 1 step 515: training accuarcy: 0.6885\n",
      "Epoch 1 step 515: training loss: 1383.5136744847298\n",
      "Epoch 1 step 516: training accuarcy: 0.6960000000000001\n",
      "Epoch 1 step 516: training loss: 1382.0365476969548\n",
      "Epoch 1 step 517: training accuarcy: 0.7185\n",
      "Epoch 1 step 517: training loss: 1383.3986803261205\n",
      "Epoch 1 step 518: training accuarcy: 0.6915\n",
      "Epoch 1 step 518: training loss: 1382.1109219179843\n",
      "Epoch 1 step 519: training accuarcy: 0.716\n",
      "Epoch 1 step 519: training loss: 1383.17402704786\n",
      "Epoch 1 step 520: training accuarcy: 0.6900000000000001\n",
      "Epoch 1 step 520: training loss: 1382.6616721640212\n",
      "Epoch 1 step 521: training accuarcy: 0.6995\n",
      "Epoch 1 step 521: training loss: 1382.4970014507155\n",
      "Epoch 1 step 522: training accuarcy: 0.7020000000000001\n",
      "Epoch 1 step 522: training loss: 1383.5249716979354\n",
      "Epoch 1 step 523: training accuarcy: 0.6975\n",
      "Epoch 1 step 523: training loss: 1383.379121082561\n",
      "Epoch 1 step 524: training accuarcy: 0.6880000000000001\n",
      "Epoch 1 step 524: training loss: 1382.9835506134104\n",
      "Epoch 1 step 525: training accuarcy: 0.7015\n",
      "Epoch 1 step 525: training loss: 543.3559587859495\n",
      "Epoch 1 step 526: training accuarcy: 0.7141025641025641\n",
      "Epoch 1: train loss 1379.6690073786979, train accuarcy 0.6994702816009521\n",
      "Epoch 1: valid loss 1363.9291752894183, valid accuarcy 0.7103294730186462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████                                                                                                                  | 2/8 [03:49<11:32, 115.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 1382.0775223145868\n",
      "Epoch 2 step 527: training accuarcy: 0.7135\n",
      "Epoch 2 step 527: training loss: 1381.8366147226334\n",
      "Epoch 2 step 528: training accuarcy: 0.7035\n",
      "Epoch 2 step 528: training loss: 1381.4339110246892\n",
      "Epoch 2 step 529: training accuarcy: 0.724\n",
      "Epoch 2 step 529: training loss: 1381.3559395621166\n",
      "Epoch 2 step 530: training accuarcy: 0.7245\n",
      "Epoch 2 step 530: training loss: 1381.7553624288194\n",
      "Epoch 2 step 531: training accuarcy: 0.724\n",
      "Epoch 2 step 531: training loss: 1382.7913779384473\n",
      "Epoch 2 step 532: training accuarcy: 0.7095\n",
      "Epoch 2 step 532: training loss: 1382.1608668271333\n",
      "Epoch 2 step 533: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 533: training loss: 1382.5028539405237\n",
      "Epoch 2 step 534: training accuarcy: 0.736\n",
      "Epoch 2 step 534: training loss: 1381.686959127311\n",
      "Epoch 2 step 535: training accuarcy: 0.7035\n",
      "Epoch 2 step 535: training loss: 1382.1422925118645\n",
      "Epoch 2 step 536: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 536: training loss: 1380.8483829766956\n",
      "Epoch 2 step 537: training accuarcy: 0.74\n",
      "Epoch 2 step 537: training loss: 1381.459103506052\n",
      "Epoch 2 step 538: training accuarcy: 0.709\n",
      "Epoch 2 step 538: training loss: 1382.6506693698955\n",
      "Epoch 2 step 539: training accuarcy: 0.7135\n",
      "Epoch 2 step 539: training loss: 1382.946005336632\n",
      "Epoch 2 step 540: training accuarcy: 0.7015\n",
      "Epoch 2 step 540: training loss: 1382.425615885395\n",
      "Epoch 2 step 541: training accuarcy: 0.7025\n",
      "Epoch 2 step 541: training loss: 1382.1615843897287\n",
      "Epoch 2 step 542: training accuarcy: 0.723\n",
      "Epoch 2 step 542: training loss: 1381.2599388601911\n",
      "Epoch 2 step 543: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 543: training loss: 1382.3394062940797\n",
      "Epoch 2 step 544: training accuarcy: 0.7055\n",
      "Epoch 2 step 544: training loss: 1381.8490602263178\n",
      "Epoch 2 step 545: training accuarcy: 0.711\n",
      "Epoch 2 step 545: training loss: 1383.1380969045706\n",
      "Epoch 2 step 546: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 546: training loss: 1382.6217208065177\n",
      "Epoch 2 step 547: training accuarcy: 0.7055\n",
      "Epoch 2 step 547: training loss: 1382.3038080249248\n",
      "Epoch 2 step 548: training accuarcy: 0.7145\n",
      "Epoch 2 step 548: training loss: 1382.7017177726773\n",
      "Epoch 2 step 549: training accuarcy: 0.7175\n",
      "Epoch 2 step 549: training loss: 1382.1393434131985\n",
      "Epoch 2 step 550: training accuarcy: 0.7185\n",
      "Epoch 2 step 550: training loss: 1382.8644835368923\n",
      "Epoch 2 step 551: training accuarcy: 0.709\n",
      "Epoch 2 step 551: training loss: 1383.0499767099543\n",
      "Epoch 2 step 552: training accuarcy: 0.6945\n",
      "Epoch 2 step 552: training loss: 1382.7743284555895\n",
      "Epoch 2 step 553: training accuarcy: 0.715\n",
      "Epoch 2 step 553: training loss: 1382.1317963906006\n",
      "Epoch 2 step 554: training accuarcy: 0.7295\n",
      "Epoch 2 step 554: training loss: 1382.9600050890813\n",
      "Epoch 2 step 555: training accuarcy: 0.7115\n",
      "Epoch 2 step 555: training loss: 1380.7057692106812\n",
      "Epoch 2 step 556: training accuarcy: 0.7165\n",
      "Epoch 2 step 556: training loss: 1381.8868243608506\n",
      "Epoch 2 step 557: training accuarcy: 0.7025\n",
      "Epoch 2 step 557: training loss: 1382.0466368745497\n",
      "Epoch 2 step 558: training accuarcy: 0.7125\n",
      "Epoch 2 step 558: training loss: 1382.0817199329545\n",
      "Epoch 2 step 559: training accuarcy: 0.705\n",
      "Epoch 2 step 559: training loss: 1382.039657444853\n",
      "Epoch 2 step 560: training accuarcy: 0.707\n",
      "Epoch 2 step 560: training loss: 1382.9732657471932\n",
      "Epoch 2 step 561: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 561: training loss: 1382.456950194924\n",
      "Epoch 2 step 562: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 562: training loss: 1382.77142043073\n",
      "Epoch 2 step 563: training accuarcy: 0.7045\n",
      "Epoch 2 step 563: training loss: 1382.6063736406925\n",
      "Epoch 2 step 564: training accuarcy: 0.711\n",
      "Epoch 2 step 564: training loss: 1383.1295143213936\n",
      "Epoch 2 step 565: training accuarcy: 0.7025\n",
      "Epoch 2 step 565: training loss: 1382.5614031895532\n",
      "Epoch 2 step 566: training accuarcy: 0.7025\n",
      "Epoch 2 step 566: training loss: 1382.16583469489\n",
      "Epoch 2 step 567: training accuarcy: 0.71\n",
      "Epoch 2 step 567: training loss: 1382.887021447832\n",
      "Epoch 2 step 568: training accuarcy: 0.706\n",
      "Epoch 2 step 568: training loss: 1382.3241982947836\n",
      "Epoch 2 step 569: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 569: training loss: 1382.975910044699\n",
      "Epoch 2 step 570: training accuarcy: 0.6895\n",
      "Epoch 2 step 570: training loss: 1381.8256193225163\n",
      "Epoch 2 step 571: training accuarcy: 0.715\n",
      "Epoch 2 step 571: training loss: 1382.6514640708965\n",
      "Epoch 2 step 572: training accuarcy: 0.7135\n",
      "Epoch 2 step 572: training loss: 1382.413119063012\n",
      "Epoch 2 step 573: training accuarcy: 0.7005\n",
      "Epoch 2 step 573: training loss: 1382.3735657687503\n",
      "Epoch 2 step 574: training accuarcy: 0.708\n",
      "Epoch 2 step 574: training loss: 1382.5207195749767\n",
      "Epoch 2 step 575: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 575: training loss: 1383.2660588880467\n",
      "Epoch 2 step 576: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 576: training loss: 1382.0294121429442\n",
      "Epoch 2 step 577: training accuarcy: 0.7145\n",
      "Epoch 2 step 577: training loss: 1381.8400538532444\n",
      "Epoch 2 step 578: training accuarcy: 0.7105\n",
      "Epoch 2 step 578: training loss: 1382.6353533377367\n",
      "Epoch 2 step 579: training accuarcy: 0.6985\n",
      "Epoch 2 step 579: training loss: 1382.5802660502836\n",
      "Epoch 2 step 580: training accuarcy: 0.712\n",
      "Epoch 2 step 580: training loss: 1382.8443507822872\n",
      "Epoch 2 step 581: training accuarcy: 0.7025\n",
      "Epoch 2 step 581: training loss: 1382.743059209935\n",
      "Epoch 2 step 582: training accuarcy: 0.7155\n",
      "Epoch 2 step 582: training loss: 1382.1422385312494\n",
      "Epoch 2 step 583: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 583: training loss: 1382.5638950477844\n",
      "Epoch 2 step 584: training accuarcy: 0.7135\n",
      "Epoch 2 step 584: training loss: 1381.6426126380825\n",
      "Epoch 2 step 585: training accuarcy: 0.706\n",
      "Epoch 2 step 585: training loss: 1382.1235057687545\n",
      "Epoch 2 step 586: training accuarcy: 0.6985\n",
      "Epoch 2 step 586: training loss: 1381.985552313317\n",
      "Epoch 2 step 587: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 587: training loss: 1381.5211724574442\n",
      "Epoch 2 step 588: training accuarcy: 0.7215\n",
      "Epoch 2 step 588: training loss: 1383.3973662896042\n",
      "Epoch 2 step 589: training accuarcy: 0.706\n",
      "Epoch 2 step 589: training loss: 1382.9553879613293\n",
      "Epoch 2 step 590: training accuarcy: 0.7195\n",
      "Epoch 2 step 590: training loss: 1383.1469015628634\n",
      "Epoch 2 step 591: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 591: training loss: 1382.386559732936\n",
      "Epoch 2 step 592: training accuarcy: 0.7065\n",
      "Epoch 2 step 592: training loss: 1382.7432123827264\n",
      "Epoch 2 step 593: training accuarcy: 0.6915\n",
      "Epoch 2 step 593: training loss: 1383.4450026999712\n",
      "Epoch 2 step 594: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 594: training loss: 1382.7666111388662\n",
      "Epoch 2 step 595: training accuarcy: 0.7085\n",
      "Epoch 2 step 595: training loss: 1383.2883549794415\n",
      "Epoch 2 step 596: training accuarcy: 0.6975\n",
      "Epoch 2 step 596: training loss: 1382.6502074193475\n",
      "Epoch 2 step 597: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 597: training loss: 1383.322300938248\n",
      "Epoch 2 step 598: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 598: training loss: 1383.1889638852049\n",
      "Epoch 2 step 599: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 599: training loss: 1382.8534706634816\n",
      "Epoch 2 step 600: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 600: training loss: 1382.1403959761751\n",
      "Epoch 2 step 601: training accuarcy: 0.7205\n",
      "Epoch 2 step 601: training loss: 1382.6137563244217\n",
      "Epoch 2 step 602: training accuarcy: 0.6995\n",
      "Epoch 2 step 602: training loss: 1382.5339464218903\n",
      "Epoch 2 step 603: training accuarcy: 0.7185\n",
      "Epoch 2 step 603: training loss: 1383.6565471048868\n",
      "Epoch 2 step 604: training accuarcy: 0.687\n",
      "Epoch 2 step 604: training loss: 1383.6155928696694\n",
      "Epoch 2 step 605: training accuarcy: 0.6785\n",
      "Epoch 2 step 605: training loss: 1382.4643095977071\n",
      "Epoch 2 step 606: training accuarcy: 0.712\n",
      "Epoch 2 step 606: training loss: 1382.0833769192068\n",
      "Epoch 2 step 607: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 607: training loss: 1383.1045825302858\n",
      "Epoch 2 step 608: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 608: training loss: 1382.3655671648744\n",
      "Epoch 2 step 609: training accuarcy: 0.6900000000000001\n",
      "Epoch 2 step 609: training loss: 1384.0077439191318\n",
      "Epoch 2 step 610: training accuarcy: 0.681\n",
      "Epoch 2 step 610: training loss: 1383.333358696333\n",
      "Epoch 2 step 611: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 611: training loss: 1382.9604646548516\n",
      "Epoch 2 step 612: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 612: training loss: 1382.1609066388241\n",
      "Epoch 2 step 613: training accuarcy: 0.705\n",
      "Epoch 2 step 613: training loss: 1382.6911006912296\n",
      "Epoch 2 step 614: training accuarcy: 0.6995\n",
      "Epoch 2 step 614: training loss: 1383.4066355362904\n",
      "Epoch 2 step 615: training accuarcy: 0.707\n",
      "Epoch 2 step 615: training loss: 1381.4360403452197\n",
      "Epoch 2 step 616: training accuarcy: 0.7195\n",
      "Epoch 2 step 616: training loss: 1382.7716758911672\n",
      "Epoch 2 step 617: training accuarcy: 0.715\n",
      "Epoch 2 step 617: training loss: 1382.7695450744252\n",
      "Epoch 2 step 618: training accuarcy: 0.712\n",
      "Epoch 2 step 618: training loss: 1383.9049502025598\n",
      "Epoch 2 step 619: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 619: training loss: 1382.8561509041651\n",
      "Epoch 2 step 620: training accuarcy: 0.6905\n",
      "Epoch 2 step 620: training loss: 1383.9467310454277\n",
      "Epoch 2 step 621: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 621: training loss: 1383.438825777874\n",
      "Epoch 2 step 622: training accuarcy: 0.6945\n",
      "Epoch 2 step 622: training loss: 1383.0388627812924\n",
      "Epoch 2 step 623: training accuarcy: 0.687\n",
      "Epoch 2 step 623: training loss: 1382.7839710469343\n",
      "Epoch 2 step 624: training accuarcy: 0.6975\n",
      "Epoch 2 step 624: training loss: 1382.5185883554448\n",
      "Epoch 2 step 625: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 625: training loss: 1383.2299663341803\n",
      "Epoch 2 step 626: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 626: training loss: 1381.7158049788698\n",
      "Epoch 2 step 627: training accuarcy: 0.709\n",
      "Epoch 2 step 627: training loss: 1383.4066965292773\n",
      "Epoch 2 step 628: training accuarcy: 0.682\n",
      "Epoch 2 step 628: training loss: 1382.0366631340364\n",
      "Epoch 2 step 629: training accuarcy: 0.718\n",
      "Epoch 2 step 629: training loss: 1383.477557043961\n",
      "Epoch 2 step 630: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 630: training loss: 1382.2661696331543\n",
      "Epoch 2 step 631: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 631: training loss: 1382.9625735816937\n",
      "Epoch 2 step 632: training accuarcy: 0.6955\n",
      "Epoch 2 step 632: training loss: 1382.5169155251008\n",
      "Epoch 2 step 633: training accuarcy: 0.7125\n",
      "Epoch 2 step 633: training loss: 1382.723513317283\n",
      "Epoch 2 step 634: training accuarcy: 0.706\n",
      "Epoch 2 step 634: training loss: 1382.63985143381\n",
      "Epoch 2 step 635: training accuarcy: 0.71\n",
      "Epoch 2 step 635: training loss: 1383.184724507169\n",
      "Epoch 2 step 636: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 636: training loss: 1383.0448937385247\n",
      "Epoch 2 step 637: training accuarcy: 0.6885\n",
      "Epoch 2 step 637: training loss: 1383.111499028473\n",
      "Epoch 2 step 638: training accuarcy: 0.7115\n",
      "Epoch 2 step 638: training loss: 1383.422072297219\n",
      "Epoch 2 step 639: training accuarcy: 0.6895\n",
      "Epoch 2 step 639: training loss: 1381.909137281159\n",
      "Epoch 2 step 640: training accuarcy: 0.7025\n",
      "Epoch 2 step 640: training loss: 1383.3857015808896\n",
      "Epoch 2 step 641: training accuarcy: 0.6905\n",
      "Epoch 2 step 641: training loss: 1381.994210211764\n",
      "Epoch 2 step 642: training accuarcy: 0.6955\n",
      "Epoch 2 step 642: training loss: 1382.7115735933162\n",
      "Epoch 2 step 643: training accuarcy: 0.712\n",
      "Epoch 2 step 643: training loss: 1382.1240608224539\n",
      "Epoch 2 step 644: training accuarcy: 0.7005\n",
      "Epoch 2 step 644: training loss: 1382.6640700115422\n",
      "Epoch 2 step 645: training accuarcy: 0.7025\n",
      "Epoch 2 step 645: training loss: 1383.3575392864202\n",
      "Epoch 2 step 646: training accuarcy: 0.6855\n",
      "Epoch 2 step 646: training loss: 1383.1232837158523\n",
      "Epoch 2 step 647: training accuarcy: 0.6815\n",
      "Epoch 2 step 647: training loss: 1383.3870299829782\n",
      "Epoch 2 step 648: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 648: training loss: 1382.3930390341638\n",
      "Epoch 2 step 649: training accuarcy: 0.7065\n",
      "Epoch 2 step 649: training loss: 1382.713366768408\n",
      "Epoch 2 step 650: training accuarcy: 0.712\n",
      "Epoch 2 step 650: training loss: 1383.2375300458143\n",
      "Epoch 2 step 651: training accuarcy: 0.676\n",
      "Epoch 2 step 651: training loss: 1382.4881397677277\n",
      "Epoch 2 step 652: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 652: training loss: 1383.2343907496474\n",
      "Epoch 2 step 653: training accuarcy: 0.7135\n",
      "Epoch 2 step 653: training loss: 1383.0278906005592\n",
      "Epoch 2 step 654: training accuarcy: 0.6905\n",
      "Epoch 2 step 654: training loss: 1382.2635997499228\n",
      "Epoch 2 step 655: training accuarcy: 0.7055\n",
      "Epoch 2 step 655: training loss: 1382.266277434648\n",
      "Epoch 2 step 656: training accuarcy: 0.7075\n",
      "Epoch 2 step 656: training loss: 1381.5856212028843\n",
      "Epoch 2 step 657: training accuarcy: 0.724\n",
      "Epoch 2 step 657: training loss: 1383.784060053382\n",
      "Epoch 2 step 658: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 658: training loss: 1382.7713257827538\n",
      "Epoch 2 step 659: training accuarcy: 0.686\n",
      "Epoch 2 step 659: training loss: 1382.4929338972327\n",
      "Epoch 2 step 660: training accuarcy: 0.6985\n",
      "Epoch 2 step 660: training loss: 1383.0698699382676\n",
      "Epoch 2 step 661: training accuarcy: 0.6985\n",
      "Epoch 2 step 661: training loss: 1383.068184715993\n",
      "Epoch 2 step 662: training accuarcy: 0.6905\n",
      "Epoch 2 step 662: training loss: 1382.786669378819\n",
      "Epoch 2 step 663: training accuarcy: 0.6935\n",
      "Epoch 2 step 663: training loss: 1381.8544002856252\n",
      "Epoch 2 step 664: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 664: training loss: 1382.9262153634384\n",
      "Epoch 2 step 665: training accuarcy: 0.712\n",
      "Epoch 2 step 665: training loss: 1383.6546176590357\n",
      "Epoch 2 step 666: training accuarcy: 0.6805\n",
      "Epoch 2 step 666: training loss: 1382.853900414878\n",
      "Epoch 2 step 667: training accuarcy: 0.7075\n",
      "Epoch 2 step 667: training loss: 1383.4277721997291\n",
      "Epoch 2 step 668: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 668: training loss: 1382.2751378680553\n",
      "Epoch 2 step 669: training accuarcy: 0.687\n",
      "Epoch 2 step 669: training loss: 1382.118242463249\n",
      "Epoch 2 step 670: training accuarcy: 0.6845\n",
      "Epoch 2 step 670: training loss: 1383.2660212075289\n",
      "Epoch 2 step 671: training accuarcy: 0.6825\n",
      "Epoch 2 step 671: training loss: 1382.862946697054\n",
      "Epoch 2 step 672: training accuarcy: 0.6745\n",
      "Epoch 2 step 672: training loss: 1382.7569647408793\n",
      "Epoch 2 step 673: training accuarcy: 0.712\n",
      "Epoch 2 step 673: training loss: 1383.720277489204\n",
      "Epoch 2 step 674: training accuarcy: 0.6920000000000001\n",
      "Epoch 2 step 674: training loss: 1382.314015513571\n",
      "Epoch 2 step 675: training accuarcy: 0.6945\n",
      "Epoch 2 step 675: training loss: 1383.6231542676721\n",
      "Epoch 2 step 676: training accuarcy: 0.6835\n",
      "Epoch 2 step 676: training loss: 1383.3245579038862\n",
      "Epoch 2 step 677: training accuarcy: 0.6985\n",
      "Epoch 2 step 677: training loss: 1382.8810695517268\n",
      "Epoch 2 step 678: training accuarcy: 0.6975\n",
      "Epoch 2 step 678: training loss: 1383.101287125229\n",
      "Epoch 2 step 679: training accuarcy: 0.6965\n",
      "Epoch 2 step 679: training loss: 1382.5151985538644\n",
      "Epoch 2 step 680: training accuarcy: 0.6905\n",
      "Epoch 2 step 680: training loss: 1382.5699876742058\n",
      "Epoch 2 step 681: training accuarcy: 0.7015\n",
      "Epoch 2 step 681: training loss: 1381.4214142330593\n",
      "Epoch 2 step 682: training accuarcy: 0.7115\n",
      "Epoch 2 step 682: training loss: 1380.981620048131\n",
      "Epoch 2 step 683: training accuarcy: 0.7155\n",
      "Epoch 2 step 683: training loss: 1383.8919607481384\n",
      "Epoch 2 step 684: training accuarcy: 0.6965\n",
      "Epoch 2 step 684: training loss: 1382.1721094873133\n",
      "Epoch 2 step 685: training accuarcy: 0.6955\n",
      "Epoch 2 step 685: training loss: 1382.5716028576571\n",
      "Epoch 2 step 686: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 686: training loss: 1382.2922710722569\n",
      "Epoch 2 step 687: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 687: training loss: 1381.9497010805062\n",
      "Epoch 2 step 688: training accuarcy: 0.7115\n",
      "Epoch 2 step 688: training loss: 1382.9749180794513\n",
      "Epoch 2 step 689: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 689: training loss: 1381.7073314019133\n",
      "Epoch 2 step 690: training accuarcy: 0.712\n",
      "Epoch 2 step 690: training loss: 1383.5030606348982\n",
      "Epoch 2 step 691: training accuarcy: 0.6965\n",
      "Epoch 2 step 691: training loss: 1383.700451284412\n",
      "Epoch 2 step 692: training accuarcy: 0.6925\n",
      "Epoch 2 step 692: training loss: 1382.7512948769697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 693: training accuarcy: 0.684\n",
      "Epoch 2 step 693: training loss: 1382.9571477016398\n",
      "Epoch 2 step 694: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 694: training loss: 1383.61796920351\n",
      "Epoch 2 step 695: training accuarcy: 0.7055\n",
      "Epoch 2 step 695: training loss: 1384.1448688496243\n",
      "Epoch 2 step 696: training accuarcy: 0.6930000000000001\n",
      "Epoch 2 step 696: training loss: 1383.276184700827\n",
      "Epoch 2 step 697: training accuarcy: 0.6925\n",
      "Epoch 2 step 697: training loss: 1382.7589471041933\n",
      "Epoch 2 step 698: training accuarcy: 0.6995\n",
      "Epoch 2 step 698: training loss: 1382.4871790511083\n",
      "Epoch 2 step 699: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 699: training loss: 1383.3348101030208\n",
      "Epoch 2 step 700: training accuarcy: 0.6835\n",
      "Epoch 2 step 700: training loss: 1382.8498780024174\n",
      "Epoch 2 step 701: training accuarcy: 0.6895\n",
      "Epoch 2 step 701: training loss: 1383.3917578101277\n",
      "Epoch 2 step 702: training accuarcy: 0.6975\n",
      "Epoch 2 step 702: training loss: 1382.619333216925\n",
      "Epoch 2 step 703: training accuarcy: 0.719\n",
      "Epoch 2 step 703: training loss: 1382.0572340991391\n",
      "Epoch 2 step 704: training accuarcy: 0.716\n",
      "Epoch 2 step 704: training loss: 1383.6900303685347\n",
      "Epoch 2 step 705: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 705: training loss: 1383.483009930662\n",
      "Epoch 2 step 706: training accuarcy: 0.685\n",
      "Epoch 2 step 706: training loss: 1382.5487549335614\n",
      "Epoch 2 step 707: training accuarcy: 0.7045\n",
      "Epoch 2 step 707: training loss: 1383.2792872532432\n",
      "Epoch 2 step 708: training accuarcy: 0.6915\n",
      "Epoch 2 step 708: training loss: 1383.630087984506\n",
      "Epoch 2 step 709: training accuarcy: 0.674\n",
      "Epoch 2 step 709: training loss: 1383.621593057755\n",
      "Epoch 2 step 710: training accuarcy: 0.687\n",
      "Epoch 2 step 710: training loss: 1383.3854446729404\n",
      "Epoch 2 step 711: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 711: training loss: 1383.9101734286771\n",
      "Epoch 2 step 712: training accuarcy: 0.706\n",
      "Epoch 2 step 712: training loss: 1383.047037770683\n",
      "Epoch 2 step 713: training accuarcy: 0.6915\n",
      "Epoch 2 step 713: training loss: 1383.0945234712012\n",
      "Epoch 2 step 714: training accuarcy: 0.6885\n",
      "Epoch 2 step 714: training loss: 1382.2167168285225\n",
      "Epoch 2 step 715: training accuarcy: 0.7015\n",
      "Epoch 2 step 715: training loss: 1381.8566023919784\n",
      "Epoch 2 step 716: training accuarcy: 0.7055\n",
      "Epoch 2 step 716: training loss: 1383.8855838021896\n",
      "Epoch 2 step 717: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 717: training loss: 1382.930676277873\n",
      "Epoch 2 step 718: training accuarcy: 0.7015\n",
      "Epoch 2 step 718: training loss: 1382.6663793554126\n",
      "Epoch 2 step 719: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 719: training loss: 1382.630164400065\n",
      "Epoch 2 step 720: training accuarcy: 0.6945\n",
      "Epoch 2 step 720: training loss: 1383.85654167295\n",
      "Epoch 2 step 721: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 721: training loss: 1382.438181862424\n",
      "Epoch 2 step 722: training accuarcy: 0.6990000000000001\n",
      "Epoch 2 step 722: training loss: 1383.1242114773725\n",
      "Epoch 2 step 723: training accuarcy: 0.7030000000000001\n",
      "Epoch 2 step 723: training loss: 1382.0747549598202\n",
      "Epoch 2 step 724: training accuarcy: 0.6970000000000001\n",
      "Epoch 2 step 724: training loss: 1383.0417988383463\n",
      "Epoch 2 step 725: training accuarcy: 0.706\n",
      "Epoch 2 step 725: training loss: 1382.6065320441373\n",
      "Epoch 2 step 726: training accuarcy: 0.6880000000000001\n",
      "Epoch 2 step 726: training loss: 1382.2663111101717\n",
      "Epoch 2 step 727: training accuarcy: 0.7005\n",
      "Epoch 2 step 727: training loss: 1383.1156000194678\n",
      "Epoch 2 step 728: training accuarcy: 0.6980000000000001\n",
      "Epoch 2 step 728: training loss: 1383.8236327660363\n",
      "Epoch 2 step 729: training accuarcy: 0.683\n",
      "Epoch 2 step 729: training loss: 1383.1849110341807\n",
      "Epoch 2 step 730: training accuarcy: 0.7115\n",
      "Epoch 2 step 730: training loss: 1382.8812223086727\n",
      "Epoch 2 step 731: training accuarcy: 0.7045\n",
      "Epoch 2 step 731: training loss: 1382.8431161375784\n",
      "Epoch 2 step 732: training accuarcy: 0.7000000000000001\n",
      "Epoch 2 step 732: training loss: 1382.7289178267165\n",
      "Epoch 2 step 733: training accuarcy: 0.6940000000000001\n",
      "Epoch 2 step 733: training loss: 1382.8594955317835\n",
      "Epoch 2 step 734: training accuarcy: 0.7025\n",
      "Epoch 2 step 734: training loss: 1383.5147204093903\n",
      "Epoch 2 step 735: training accuarcy: 0.6915\n",
      "Epoch 2 step 735: training loss: 1383.2558878275997\n",
      "Epoch 2 step 736: training accuarcy: 0.6785\n",
      "Epoch 2 step 736: training loss: 1382.389350707132\n",
      "Epoch 2 step 737: training accuarcy: 0.7095\n",
      "Epoch 2 step 737: training loss: 1383.3341908081356\n",
      "Epoch 2 step 738: training accuarcy: 0.679\n",
      "Epoch 2 step 738: training loss: 1383.7273234033594\n",
      "Epoch 2 step 739: training accuarcy: 0.6915\n",
      "Epoch 2 step 739: training loss: 1383.2129462259454\n",
      "Epoch 2 step 740: training accuarcy: 0.6855\n",
      "Epoch 2 step 740: training loss: 1383.1708572122272\n",
      "Epoch 2 step 741: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 741: training loss: 1381.9123069285404\n",
      "Epoch 2 step 742: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 742: training loss: 1382.42980147772\n",
      "Epoch 2 step 743: training accuarcy: 0.6945\n",
      "Epoch 2 step 743: training loss: 1383.8527552721646\n",
      "Epoch 2 step 744: training accuarcy: 0.6865\n",
      "Epoch 2 step 744: training loss: 1382.4811837693405\n",
      "Epoch 2 step 745: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 745: training loss: 1383.3403428957913\n",
      "Epoch 2 step 746: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 746: training loss: 1383.160598610122\n",
      "Epoch 2 step 747: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 747: training loss: 1381.4677640059065\n",
      "Epoch 2 step 748: training accuarcy: 0.7195\n",
      "Epoch 2 step 748: training loss: 1382.3217801705139\n",
      "Epoch 2 step 749: training accuarcy: 0.7005\n",
      "Epoch 2 step 749: training loss: 1383.0317836312995\n",
      "Epoch 2 step 750: training accuarcy: 0.7015\n",
      "Epoch 2 step 750: training loss: 1383.3715602547131\n",
      "Epoch 2 step 751: training accuarcy: 0.6935\n",
      "Epoch 2 step 751: training loss: 1382.3214357831785\n",
      "Epoch 2 step 752: training accuarcy: 0.708\n",
      "Epoch 2 step 752: training loss: 1382.4191779987086\n",
      "Epoch 2 step 753: training accuarcy: 0.7075\n",
      "Epoch 2 step 753: training loss: 1382.9380300031105\n",
      "Epoch 2 step 754: training accuarcy: 0.708\n",
      "Epoch 2 step 754: training loss: 1382.2137277395955\n",
      "Epoch 2 step 755: training accuarcy: 0.7065\n",
      "Epoch 2 step 755: training loss: 1382.201914099856\n",
      "Epoch 2 step 756: training accuarcy: 0.713\n",
      "Epoch 2 step 756: training loss: 1382.7409996478539\n",
      "Epoch 2 step 757: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 757: training loss: 1382.6704730866174\n",
      "Epoch 2 step 758: training accuarcy: 0.681\n",
      "Epoch 2 step 758: training loss: 1382.27105609461\n",
      "Epoch 2 step 759: training accuarcy: 0.7085\n",
      "Epoch 2 step 759: training loss: 1383.3237375352458\n",
      "Epoch 2 step 760: training accuarcy: 0.6905\n",
      "Epoch 2 step 760: training loss: 1383.0659820845704\n",
      "Epoch 2 step 761: training accuarcy: 0.7010000000000001\n",
      "Epoch 2 step 761: training loss: 1383.4517798317593\n",
      "Epoch 2 step 762: training accuarcy: 0.7015\n",
      "Epoch 2 step 762: training loss: 1383.4246678550528\n",
      "Epoch 2 step 763: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 763: training loss: 1383.5126520810177\n",
      "Epoch 2 step 764: training accuarcy: 0.6895\n",
      "Epoch 2 step 764: training loss: 1382.9494004562514\n",
      "Epoch 2 step 765: training accuarcy: 0.6885\n",
      "Epoch 2 step 765: training loss: 1382.730898133878\n",
      "Epoch 2 step 766: training accuarcy: 0.6890000000000001\n",
      "Epoch 2 step 766: training loss: 1382.795943056829\n",
      "Epoch 2 step 767: training accuarcy: 0.6995\n",
      "Epoch 2 step 767: training loss: 1383.3281892671148\n",
      "Epoch 2 step 768: training accuarcy: 0.6765\n",
      "Epoch 2 step 768: training loss: 1383.5152004113004\n",
      "Epoch 2 step 769: training accuarcy: 0.6985\n",
      "Epoch 2 step 769: training loss: 1382.8706314517642\n",
      "Epoch 2 step 770: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 770: training loss: 1383.0860897992056\n",
      "Epoch 2 step 771: training accuarcy: 0.6875\n",
      "Epoch 2 step 771: training loss: 1383.68985399313\n",
      "Epoch 2 step 772: training accuarcy: 0.6900000000000001\n",
      "Epoch 2 step 772: training loss: 1383.338439452954\n",
      "Epoch 2 step 773: training accuarcy: 0.6865\n",
      "Epoch 2 step 773: training loss: 1383.4581148854913\n",
      "Epoch 2 step 774: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 774: training loss: 1383.7470470966116\n",
      "Epoch 2 step 775: training accuarcy: 0.7075\n",
      "Epoch 2 step 775: training loss: 1383.2530691823151\n",
      "Epoch 2 step 776: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 776: training loss: 1383.3415074586235\n",
      "Epoch 2 step 777: training accuarcy: 0.6960000000000001\n",
      "Epoch 2 step 777: training loss: 1382.5645377528733\n",
      "Epoch 2 step 778: training accuarcy: 0.7035\n",
      "Epoch 2 step 778: training loss: 1383.3859154186994\n",
      "Epoch 2 step 779: training accuarcy: 0.6910000000000001\n",
      "Epoch 2 step 779: training loss: 1384.055620287605\n",
      "Epoch 2 step 780: training accuarcy: 0.685\n",
      "Epoch 2 step 780: training loss: 1383.277577422761\n",
      "Epoch 2 step 781: training accuarcy: 0.6955\n",
      "Epoch 2 step 781: training loss: 1382.9202428339286\n",
      "Epoch 2 step 782: training accuarcy: 0.7020000000000001\n",
      "Epoch 2 step 782: training loss: 1382.89904629041\n",
      "Epoch 2 step 783: training accuarcy: 0.678\n",
      "Epoch 2 step 783: training loss: 1383.291107787693\n",
      "Epoch 2 step 784: training accuarcy: 0.6795\n",
      "Epoch 2 step 784: training loss: 1382.7444881588337\n",
      "Epoch 2 step 785: training accuarcy: 0.706\n",
      "Epoch 2 step 785: training loss: 1383.4339932469034\n",
      "Epoch 2 step 786: training accuarcy: 0.7005\n",
      "Epoch 2 step 786: training loss: 1382.6893081396443\n",
      "Epoch 2 step 787: training accuarcy: 0.6950000000000001\n",
      "Epoch 2 step 787: training loss: 1382.6328515780692\n",
      "Epoch 2 step 788: training accuarcy: 0.6905\n",
      "Epoch 2 step 788: training loss: 542.4420208258769\n",
      "Epoch 2 step 789: training accuarcy: 0.7128205128205128\n",
      "Epoch 2: train loss 1379.5709567444717, train accuarcy 0.7031528353691101\n",
      "Epoch 2: valid loss 1363.9940319445864, valid accuarcy 0.7110369801521301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████                                                                                               | 3/8 [05:42<09:34, 114.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 1382.6948344196605\n",
      "Epoch 3 step 790: training accuarcy: 0.7185\n",
      "Epoch 3 step 790: training loss: 1381.2702644229284\n",
      "Epoch 3 step 791: training accuarcy: 0.7195\n",
      "Epoch 3 step 791: training loss: 1382.443284264446\n",
      "Epoch 3 step 792: training accuarcy: 0.7085\n",
      "Epoch 3 step 792: training loss: 1382.2035946184014\n",
      "Epoch 3 step 793: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 793: training loss: 1382.3513427074772\n",
      "Epoch 3 step 794: training accuarcy: 0.7025\n",
      "Epoch 3 step 794: training loss: 1380.8759247979915\n",
      "Epoch 3 step 795: training accuarcy: 0.72\n",
      "Epoch 3 step 795: training loss: 1381.8447831274518\n",
      "Epoch 3 step 796: training accuarcy: 0.7085\n",
      "Epoch 3 step 796: training loss: 1381.9371576360477\n",
      "Epoch 3 step 797: training accuarcy: 0.7055\n",
      "Epoch 3 step 797: training loss: 1381.8638093092452\n",
      "Epoch 3 step 798: training accuarcy: 0.73\n",
      "Epoch 3 step 798: training loss: 1381.8935691504855\n",
      "Epoch 3 step 799: training accuarcy: 0.707\n",
      "Epoch 3 step 799: training loss: 1383.0062404125374\n",
      "Epoch 3 step 800: training accuarcy: 0.6995\n",
      "Epoch 3 step 800: training loss: 1381.4162796636977\n",
      "Epoch 3 step 801: training accuarcy: 0.715\n",
      "Epoch 3 step 801: training loss: 1382.3762408700686\n",
      "Epoch 3 step 802: training accuarcy: 0.7085\n",
      "Epoch 3 step 802: training loss: 1382.436729670018\n",
      "Epoch 3 step 803: training accuarcy: 0.706\n",
      "Epoch 3 step 803: training loss: 1382.4072033791067\n",
      "Epoch 3 step 804: training accuarcy: 0.7215\n",
      "Epoch 3 step 804: training loss: 1381.5627637215528\n",
      "Epoch 3 step 805: training accuarcy: 0.718\n",
      "Epoch 3 step 805: training loss: 1381.9471944333077\n",
      "Epoch 3 step 806: training accuarcy: 0.7055\n",
      "Epoch 3 step 806: training loss: 1383.290355948219\n",
      "Epoch 3 step 807: training accuarcy: 0.6940000000000001\n",
      "Epoch 3 step 807: training loss: 1382.4786666844168\n",
      "Epoch 3 step 808: training accuarcy: 0.7045\n",
      "Epoch 3 step 808: training loss: 1381.993489265633\n",
      "Epoch 3 step 809: training accuarcy: 0.7135\n",
      "Epoch 3 step 809: training loss: 1382.0114748255169\n",
      "Epoch 3 step 810: training accuarcy: 0.7195\n",
      "Epoch 3 step 810: training loss: 1382.3311061714398\n",
      "Epoch 3 step 811: training accuarcy: 0.724\n",
      "Epoch 3 step 811: training loss: 1382.3417188313147\n",
      "Epoch 3 step 812: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 812: training loss: 1382.059928287919\n",
      "Epoch 3 step 813: training accuarcy: 0.7045\n",
      "Epoch 3 step 813: training loss: 1382.232057368389\n",
      "Epoch 3 step 814: training accuarcy: 0.716\n",
      "Epoch 3 step 814: training loss: 1382.1104185849138\n",
      "Epoch 3 step 815: training accuarcy: 0.711\n",
      "Epoch 3 step 815: training loss: 1382.9875853461879\n",
      "Epoch 3 step 816: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 816: training loss: 1381.952071188307\n",
      "Epoch 3 step 817: training accuarcy: 0.71\n",
      "Epoch 3 step 817: training loss: 1383.6543610686706\n",
      "Epoch 3 step 818: training accuarcy: 0.6915\n",
      "Epoch 3 step 818: training loss: 1382.4889731995065\n",
      "Epoch 3 step 819: training accuarcy: 0.7175\n",
      "Epoch 3 step 819: training loss: 1383.0124783187866\n",
      "Epoch 3 step 820: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 820: training loss: 1382.3075830426314\n",
      "Epoch 3 step 821: training accuarcy: 0.7035\n",
      "Epoch 3 step 821: training loss: 1382.1610015782426\n",
      "Epoch 3 step 822: training accuarcy: 0.6955\n",
      "Epoch 3 step 822: training loss: 1381.5979413878613\n",
      "Epoch 3 step 823: training accuarcy: 0.7205\n",
      "Epoch 3 step 823: training loss: 1381.702157244927\n",
      "Epoch 3 step 824: training accuarcy: 0.7125\n",
      "Epoch 3 step 824: training loss: 1381.9592829796295\n",
      "Epoch 3 step 825: training accuarcy: 0.709\n",
      "Epoch 3 step 825: training loss: 1382.9541043046072\n",
      "Epoch 3 step 826: training accuarcy: 0.6965\n",
      "Epoch 3 step 826: training loss: 1382.8674739008268\n",
      "Epoch 3 step 827: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 827: training loss: 1382.0412816992114\n",
      "Epoch 3 step 828: training accuarcy: 0.7025\n",
      "Epoch 3 step 828: training loss: 1381.2472111904608\n",
      "Epoch 3 step 829: training accuarcy: 0.7125\n",
      "Epoch 3 step 829: training loss: 1382.6269347032437\n",
      "Epoch 3 step 830: training accuarcy: 0.6815\n",
      "Epoch 3 step 830: training loss: 1383.4070396797306\n",
      "Epoch 3 step 831: training accuarcy: 0.7065\n",
      "Epoch 3 step 831: training loss: 1382.2561785881005\n",
      "Epoch 3 step 832: training accuarcy: 0.7135\n",
      "Epoch 3 step 832: training loss: 1381.9604599266534\n",
      "Epoch 3 step 833: training accuarcy: 0.7115\n",
      "Epoch 3 step 833: training loss: 1382.824231791628\n",
      "Epoch 3 step 834: training accuarcy: 0.7095\n",
      "Epoch 3 step 834: training loss: 1382.609160976341\n",
      "Epoch 3 step 835: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 835: training loss: 1382.6684239156696\n",
      "Epoch 3 step 836: training accuarcy: 0.7105\n",
      "Epoch 3 step 836: training loss: 1382.0486798001384\n",
      "Epoch 3 step 837: training accuarcy: 0.705\n",
      "Epoch 3 step 837: training loss: 1382.7273965976656\n",
      "Epoch 3 step 838: training accuarcy: 0.7035\n",
      "Epoch 3 step 838: training loss: 1382.2302404949387\n",
      "Epoch 3 step 839: training accuarcy: 0.687\n",
      "Epoch 3 step 839: training loss: 1381.197310886291\n",
      "Epoch 3 step 840: training accuarcy: 0.7075\n",
      "Epoch 3 step 840: training loss: 1382.6020572156374\n",
      "Epoch 3 step 841: training accuarcy: 0.7035\n",
      "Epoch 3 step 841: training loss: 1382.2088060722533\n",
      "Epoch 3 step 842: training accuarcy: 0.6975\n",
      "Epoch 3 step 842: training loss: 1381.3724331204344\n",
      "Epoch 3 step 843: training accuarcy: 0.711\n",
      "Epoch 3 step 843: training loss: 1382.568987908064\n",
      "Epoch 3 step 844: training accuarcy: 0.715\n",
      "Epoch 3 step 844: training loss: 1383.0828829722655\n",
      "Epoch 3 step 845: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 845: training loss: 1382.3933649995513\n",
      "Epoch 3 step 846: training accuarcy: 0.6965\n",
      "Epoch 3 step 846: training loss: 1383.3986447841646\n",
      "Epoch 3 step 847: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 847: training loss: 1381.6851929097438\n",
      "Epoch 3 step 848: training accuarcy: 0.712\n",
      "Epoch 3 step 848: training loss: 1381.340428116216\n",
      "Epoch 3 step 849: training accuarcy: 0.7005\n",
      "Epoch 3 step 849: training loss: 1382.7914767758134\n",
      "Epoch 3 step 850: training accuarcy: 0.7105\n",
      "Epoch 3 step 850: training loss: 1382.1773204483495\n",
      "Epoch 3 step 851: training accuarcy: 0.7185\n",
      "Epoch 3 step 851: training loss: 1382.882129734424\n",
      "Epoch 3 step 852: training accuarcy: 0.6965\n",
      "Epoch 3 step 852: training loss: 1382.1621333747162\n",
      "Epoch 3 step 853: training accuarcy: 0.718\n",
      "Epoch 3 step 853: training loss: 1382.930451597094\n",
      "Epoch 3 step 854: training accuarcy: 0.6985\n",
      "Epoch 3 step 854: training loss: 1383.1818834668686\n",
      "Epoch 3 step 855: training accuarcy: 0.683\n",
      "Epoch 3 step 855: training loss: 1382.257239184821\n",
      "Epoch 3 step 856: training accuarcy: 0.7045\n",
      "Epoch 3 step 856: training loss: 1382.6727422820557\n",
      "Epoch 3 step 857: training accuarcy: 0.677\n",
      "Epoch 3 step 857: training loss: 1382.9925376344847\n",
      "Epoch 3 step 858: training accuarcy: 0.678\n",
      "Epoch 3 step 858: training loss: 1382.5490840218563\n",
      "Epoch 3 step 859: training accuarcy: 0.709\n",
      "Epoch 3 step 859: training loss: 1382.6675609807726\n",
      "Epoch 3 step 860: training accuarcy: 0.7015\n",
      "Epoch 3 step 860: training loss: 1382.5070443067787\n",
      "Epoch 3 step 861: training accuarcy: 0.7035\n",
      "Epoch 3 step 861: training loss: 1382.1839989179332\n",
      "Epoch 3 step 862: training accuarcy: 0.7065\n",
      "Epoch 3 step 862: training loss: 1383.6449131176553\n",
      "Epoch 3 step 863: training accuarcy: 0.6925\n",
      "Epoch 3 step 863: training loss: 1382.142767088508\n",
      "Epoch 3 step 864: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 864: training loss: 1382.9263970865952\n",
      "Epoch 3 step 865: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 865: training loss: 1383.0190394736767\n",
      "Epoch 3 step 866: training accuarcy: 0.716\n",
      "Epoch 3 step 866: training loss: 1382.1665529692684\n",
      "Epoch 3 step 867: training accuarcy: 0.7085\n",
      "Epoch 3 step 867: training loss: 1382.0191066840862\n",
      "Epoch 3 step 868: training accuarcy: 0.714\n",
      "Epoch 3 step 868: training loss: 1382.817919359855\n",
      "Epoch 3 step 869: training accuarcy: 0.7115\n",
      "Epoch 3 step 869: training loss: 1382.8252443617507\n",
      "Epoch 3 step 870: training accuarcy: 0.6910000000000001\n",
      "Epoch 3 step 870: training loss: 1383.1009888372498\n",
      "Epoch 3 step 871: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 871: training loss: 1382.8875609471527\n",
      "Epoch 3 step 872: training accuarcy: 0.7065\n",
      "Epoch 3 step 872: training loss: 1383.3484837776637\n",
      "Epoch 3 step 873: training accuarcy: 0.706\n",
      "Epoch 3 step 873: training loss: 1383.0455499420316\n",
      "Epoch 3 step 874: training accuarcy: 0.6905\n",
      "Epoch 3 step 874: training loss: 1384.7291653067205\n",
      "Epoch 3 step 875: training accuarcy: 0.683\n",
      "Epoch 3 step 875: training loss: 1382.9634338978026\n",
      "Epoch 3 step 876: training accuarcy: 0.6885\n",
      "Epoch 3 step 876: training loss: 1382.3797467458207\n",
      "Epoch 3 step 877: training accuarcy: 0.7095\n",
      "Epoch 3 step 877: training loss: 1382.240119717867\n",
      "Epoch 3 step 878: training accuarcy: 0.6945\n",
      "Epoch 3 step 878: training loss: 1382.7587815712109\n",
      "Epoch 3 step 879: training accuarcy: 0.6985\n",
      "Epoch 3 step 879: training loss: 1381.8477590733555\n",
      "Epoch 3 step 880: training accuarcy: 0.6965\n",
      "Epoch 3 step 880: training loss: 1382.4198697183351\n",
      "Epoch 3 step 881: training accuarcy: 0.7025\n",
      "Epoch 3 step 881: training loss: 1382.6321639297348\n",
      "Epoch 3 step 882: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 882: training loss: 1382.6172944492027\n",
      "Epoch 3 step 883: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 883: training loss: 1383.5870150992982\n",
      "Epoch 3 step 884: training accuarcy: 0.6985\n",
      "Epoch 3 step 884: training loss: 1382.5334045866082\n",
      "Epoch 3 step 885: training accuarcy: 0.707\n",
      "Epoch 3 step 885: training loss: 1382.3182945264884\n",
      "Epoch 3 step 886: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 886: training loss: 1383.3637047290097\n",
      "Epoch 3 step 887: training accuarcy: 0.6910000000000001\n",
      "Epoch 3 step 887: training loss: 1382.8062242373903\n",
      "Epoch 3 step 888: training accuarcy: 0.6855\n",
      "Epoch 3 step 888: training loss: 1382.1092789190898\n",
      "Epoch 3 step 889: training accuarcy: 0.7105\n",
      "Epoch 3 step 889: training loss: 1383.111706193071\n",
      "Epoch 3 step 890: training accuarcy: 0.7065\n",
      "Epoch 3 step 890: training loss: 1382.919473950621\n",
      "Epoch 3 step 891: training accuarcy: 0.7035\n",
      "Epoch 3 step 891: training loss: 1382.9288396693055\n",
      "Epoch 3 step 892: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 892: training loss: 1383.069284320961\n",
      "Epoch 3 step 893: training accuarcy: 0.7085\n",
      "Epoch 3 step 893: training loss: 1382.5702774782987\n",
      "Epoch 3 step 894: training accuarcy: 0.6925\n",
      "Epoch 3 step 894: training loss: 1382.8958104422898\n",
      "Epoch 3 step 895: training accuarcy: 0.6925\n",
      "Epoch 3 step 895: training loss: 1382.9467039305675\n",
      "Epoch 3 step 896: training accuarcy: 0.6955\n",
      "Epoch 3 step 896: training loss: 1383.7964706655775\n",
      "Epoch 3 step 897: training accuarcy: 0.676\n",
      "Epoch 3 step 897: training loss: 1382.831668982114\n",
      "Epoch 3 step 898: training accuarcy: 0.6965\n",
      "Epoch 3 step 898: training loss: 1383.5518723279465\n",
      "Epoch 3 step 899: training accuarcy: 0.6905\n",
      "Epoch 3 step 899: training loss: 1382.5748076920156\n",
      "Epoch 3 step 900: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 900: training loss: 1381.9121720666358\n",
      "Epoch 3 step 901: training accuarcy: 0.6885\n",
      "Epoch 3 step 901: training loss: 1381.9833688320064\n",
      "Epoch 3 step 902: training accuarcy: 0.712\n",
      "Epoch 3 step 902: training loss: 1383.3229266510514\n",
      "Epoch 3 step 903: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 903: training loss: 1382.8253120779461\n",
      "Epoch 3 step 904: training accuarcy: 0.7025\n",
      "Epoch 3 step 904: training loss: 1383.0698134445126\n",
      "Epoch 3 step 905: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 905: training loss: 1382.436745936149\n",
      "Epoch 3 step 906: training accuarcy: 0.6935\n",
      "Epoch 3 step 906: training loss: 1383.0046554447372\n",
      "Epoch 3 step 907: training accuarcy: 0.6865\n",
      "Epoch 3 step 907: training loss: 1382.7420924974788\n",
      "Epoch 3 step 908: training accuarcy: 0.711\n",
      "Epoch 3 step 908: training loss: 1383.6867555468925\n",
      "Epoch 3 step 909: training accuarcy: 0.6765\n",
      "Epoch 3 step 909: training loss: 1383.5013546589141\n",
      "Epoch 3 step 910: training accuarcy: 0.6985\n",
      "Epoch 3 step 910: training loss: 1382.7549085233888\n",
      "Epoch 3 step 911: training accuarcy: 0.6995\n",
      "Epoch 3 step 911: training loss: 1383.1762268149928\n",
      "Epoch 3 step 912: training accuarcy: 0.7015\n",
      "Epoch 3 step 912: training loss: 1383.35775599615\n",
      "Epoch 3 step 913: training accuarcy: 0.6965\n",
      "Epoch 3 step 913: training loss: 1383.6947818989813\n",
      "Epoch 3 step 914: training accuarcy: 0.6955\n",
      "Epoch 3 step 914: training loss: 1382.5507375260213\n",
      "Epoch 3 step 915: training accuarcy: 0.712\n",
      "Epoch 3 step 915: training loss: 1383.6291750861424\n",
      "Epoch 3 step 916: training accuarcy: 0.6875\n",
      "Epoch 3 step 916: training loss: 1381.5056480850585\n",
      "Epoch 3 step 917: training accuarcy: 0.7025\n",
      "Epoch 3 step 917: training loss: 1383.7461710324746\n",
      "Epoch 3 step 918: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 918: training loss: 1382.561356903427\n",
      "Epoch 3 step 919: training accuarcy: 0.7115\n",
      "Epoch 3 step 919: training loss: 1383.1089647632102\n",
      "Epoch 3 step 920: training accuarcy: 0.7105\n",
      "Epoch 3 step 920: training loss: 1382.5908451149348\n",
      "Epoch 3 step 921: training accuarcy: 0.7055\n",
      "Epoch 3 step 921: training loss: 1382.1643334180194\n",
      "Epoch 3 step 922: training accuarcy: 0.708\n",
      "Epoch 3 step 922: training loss: 1382.7128776047525\n",
      "Epoch 3 step 923: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 923: training loss: 1382.465129067771\n",
      "Epoch 3 step 924: training accuarcy: 0.709\n",
      "Epoch 3 step 924: training loss: 1383.0114337375305\n",
      "Epoch 3 step 925: training accuarcy: 0.681\n",
      "Epoch 3 step 925: training loss: 1383.1402963505118\n",
      "Epoch 3 step 926: training accuarcy: 0.6985\n",
      "Epoch 3 step 926: training loss: 1383.311093963953\n",
      "Epoch 3 step 927: training accuarcy: 0.674\n",
      "Epoch 3 step 927: training loss: 1383.6242357338529\n",
      "Epoch 3 step 928: training accuarcy: 0.7005\n",
      "Epoch 3 step 928: training loss: 1382.3069548905946\n",
      "Epoch 3 step 929: training accuarcy: 0.6955\n",
      "Epoch 3 step 929: training loss: 1383.0720708624895\n",
      "Epoch 3 step 930: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 930: training loss: 1383.4758446592693\n",
      "Epoch 3 step 931: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 931: training loss: 1383.6130644268026\n",
      "Epoch 3 step 932: training accuarcy: 0.7025\n",
      "Epoch 3 step 932: training loss: 1382.953999490703\n",
      "Epoch 3 step 933: training accuarcy: 0.676\n",
      "Epoch 3 step 933: training loss: 1382.962713989948\n",
      "Epoch 3 step 934: training accuarcy: 0.6910000000000001\n",
      "Epoch 3 step 934: training loss: 1382.4641433408817\n",
      "Epoch 3 step 935: training accuarcy: 0.713\n",
      "Epoch 3 step 935: training loss: 1384.378718073146\n",
      "Epoch 3 step 936: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 936: training loss: 1382.3860936987032\n",
      "Epoch 3 step 937: training accuarcy: 0.6825\n",
      "Epoch 3 step 937: training loss: 1382.588395295429\n",
      "Epoch 3 step 938: training accuarcy: 0.6975\n",
      "Epoch 3 step 938: training loss: 1382.7643271693728\n",
      "Epoch 3 step 939: training accuarcy: 0.6865\n",
      "Epoch 3 step 939: training loss: 1383.8943599431989\n",
      "Epoch 3 step 940: training accuarcy: 0.6755\n",
      "Epoch 3 step 940: training loss: 1384.047440217447\n",
      "Epoch 3 step 941: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 941: training loss: 1384.175308670123\n",
      "Epoch 3 step 942: training accuarcy: 0.678\n",
      "Epoch 3 step 942: training loss: 1383.514182054013\n",
      "Epoch 3 step 943: training accuarcy: 0.6995\n",
      "Epoch 3 step 943: training loss: 1382.6567378156003\n",
      "Epoch 3 step 944: training accuarcy: 0.7115\n",
      "Epoch 3 step 944: training loss: 1383.1009046282782\n",
      "Epoch 3 step 945: training accuarcy: 0.707\n",
      "Epoch 3 step 945: training loss: 1383.8306729243106\n",
      "Epoch 3 step 946: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 946: training loss: 1383.0571942667439\n",
      "Epoch 3 step 947: training accuarcy: 0.6975\n",
      "Epoch 3 step 947: training loss: 1383.1396157542192\n",
      "Epoch 3 step 948: training accuarcy: 0.7005\n",
      "Epoch 3 step 948: training loss: 1382.6826081295471\n",
      "Epoch 3 step 949: training accuarcy: 0.6880000000000001\n",
      "Epoch 3 step 949: training loss: 1381.6206933165613\n",
      "Epoch 3 step 950: training accuarcy: 0.7015\n",
      "Epoch 3 step 950: training loss: 1382.4136178445701\n",
      "Epoch 3 step 951: training accuarcy: 0.6905\n",
      "Epoch 3 step 951: training loss: 1382.0542590965604\n",
      "Epoch 3 step 952: training accuarcy: 0.7225\n",
      "Epoch 3 step 952: training loss: 1382.887392476947\n",
      "Epoch 3 step 953: training accuarcy: 0.709\n",
      "Epoch 3 step 953: training loss: 1381.5628755901766\n",
      "Epoch 3 step 954: training accuarcy: 0.715\n",
      "Epoch 3 step 954: training loss: 1383.0759659893295\n",
      "Epoch 3 step 955: training accuarcy: 0.706\n",
      "Epoch 3 step 955: training loss: 1383.3912401029754\n",
      "Epoch 3 step 956: training accuarcy: 0.6955\n",
      "Epoch 3 step 956: training loss: 1382.8625479279717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 957: training accuarcy: 0.704\n",
      "Epoch 3 step 957: training loss: 1382.2913814021301\n",
      "Epoch 3 step 958: training accuarcy: 0.7005\n",
      "Epoch 3 step 958: training loss: 1383.4684908042232\n",
      "Epoch 3 step 959: training accuarcy: 0.6905\n",
      "Epoch 3 step 959: training loss: 1383.0728250134239\n",
      "Epoch 3 step 960: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 960: training loss: 1382.8998016617154\n",
      "Epoch 3 step 961: training accuarcy: 0.6895\n",
      "Epoch 3 step 961: training loss: 1382.62635410225\n",
      "Epoch 3 step 962: training accuarcy: 0.7105\n",
      "Epoch 3 step 962: training loss: 1383.510197930131\n",
      "Epoch 3 step 963: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 963: training loss: 1382.782339482078\n",
      "Epoch 3 step 964: training accuarcy: 0.7015\n",
      "Epoch 3 step 964: training loss: 1383.5770460562608\n",
      "Epoch 3 step 965: training accuarcy: 0.687\n",
      "Epoch 3 step 965: training loss: 1382.8944597063394\n",
      "Epoch 3 step 966: training accuarcy: 0.7025\n",
      "Epoch 3 step 966: training loss: 1382.318713068834\n",
      "Epoch 3 step 967: training accuarcy: 0.707\n",
      "Epoch 3 step 967: training loss: 1383.0662093079281\n",
      "Epoch 3 step 968: training accuarcy: 0.6940000000000001\n",
      "Epoch 3 step 968: training loss: 1383.2480104373806\n",
      "Epoch 3 step 969: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 969: training loss: 1382.971607827981\n",
      "Epoch 3 step 970: training accuarcy: 0.6905\n",
      "Epoch 3 step 970: training loss: 1382.8152003317666\n",
      "Epoch 3 step 971: training accuarcy: 0.707\n",
      "Epoch 3 step 971: training loss: 1383.4458717431353\n",
      "Epoch 3 step 972: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 972: training loss: 1382.6169213295598\n",
      "Epoch 3 step 973: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 973: training loss: 1383.3465944106188\n",
      "Epoch 3 step 974: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 974: training loss: 1383.6554896361129\n",
      "Epoch 3 step 975: training accuarcy: 0.685\n",
      "Epoch 3 step 975: training loss: 1383.0547438285034\n",
      "Epoch 3 step 976: training accuarcy: 0.6825\n",
      "Epoch 3 step 976: training loss: 1383.3603825457649\n",
      "Epoch 3 step 977: training accuarcy: 0.6915\n",
      "Epoch 3 step 977: training loss: 1383.3145648816146\n",
      "Epoch 3 step 978: training accuarcy: 0.6925\n",
      "Epoch 3 step 978: training loss: 1383.1752022363073\n",
      "Epoch 3 step 979: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 979: training loss: 1382.0790909395025\n",
      "Epoch 3 step 980: training accuarcy: 0.6815\n",
      "Epoch 3 step 980: training loss: 1382.266964329617\n",
      "Epoch 3 step 981: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 981: training loss: 1382.7466577892214\n",
      "Epoch 3 step 982: training accuarcy: 0.6955\n",
      "Epoch 3 step 982: training loss: 1383.286507174455\n",
      "Epoch 3 step 983: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 983: training loss: 1382.840888553229\n",
      "Epoch 3 step 984: training accuarcy: 0.6895\n",
      "Epoch 3 step 984: training loss: 1383.0345142305453\n",
      "Epoch 3 step 985: training accuarcy: 0.7115\n",
      "Epoch 3 step 985: training loss: 1382.6716240186158\n",
      "Epoch 3 step 986: training accuarcy: 0.7045\n",
      "Epoch 3 step 986: training loss: 1383.8485199595264\n",
      "Epoch 3 step 987: training accuarcy: 0.6885\n",
      "Epoch 3 step 987: training loss: 1383.290461058194\n",
      "Epoch 3 step 988: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 988: training loss: 1382.9812338038041\n",
      "Epoch 3 step 989: training accuarcy: 0.6905\n",
      "Epoch 3 step 989: training loss: 1383.3954528738798\n",
      "Epoch 3 step 990: training accuarcy: 0.6855\n",
      "Epoch 3 step 990: training loss: 1383.1605792965977\n",
      "Epoch 3 step 991: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 991: training loss: 1383.3361774116165\n",
      "Epoch 3 step 992: training accuarcy: 0.7005\n",
      "Epoch 3 step 992: training loss: 1383.066577631763\n",
      "Epoch 3 step 993: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 993: training loss: 1383.7958552225402\n",
      "Epoch 3 step 994: training accuarcy: 0.6915\n",
      "Epoch 3 step 994: training loss: 1382.43566207849\n",
      "Epoch 3 step 995: training accuarcy: 0.706\n",
      "Epoch 3 step 995: training loss: 1383.6932125344035\n",
      "Epoch 3 step 996: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 996: training loss: 1382.7914348366435\n",
      "Epoch 3 step 997: training accuarcy: 0.6985\n",
      "Epoch 3 step 997: training loss: 1382.9609750316263\n",
      "Epoch 3 step 998: training accuarcy: 0.7055\n",
      "Epoch 3 step 998: training loss: 1383.2335140463342\n",
      "Epoch 3 step 999: training accuarcy: 0.6890000000000001\n",
      "Epoch 3 step 999: training loss: 1383.0825438028542\n",
      "Epoch 3 step 1000: training accuarcy: 0.6980000000000001\n",
      "Epoch 3 step 1000: training loss: 1383.1758631493433\n",
      "Epoch 3 step 1001: training accuarcy: 0.6985\n",
      "Epoch 3 step 1001: training loss: 1382.5943080610482\n",
      "Epoch 3 step 1002: training accuarcy: 0.708\n",
      "Epoch 3 step 1002: training loss: 1383.1712213548135\n",
      "Epoch 3 step 1003: training accuarcy: 0.6965\n",
      "Epoch 3 step 1003: training loss: 1381.6319600180461\n",
      "Epoch 3 step 1004: training accuarcy: 0.6990000000000001\n",
      "Epoch 3 step 1004: training loss: 1383.7331695186301\n",
      "Epoch 3 step 1005: training accuarcy: 0.6745\n",
      "Epoch 3 step 1005: training loss: 1382.7037819886543\n",
      "Epoch 3 step 1006: training accuarcy: 0.6975\n",
      "Epoch 3 step 1006: training loss: 1383.3568136997765\n",
      "Epoch 3 step 1007: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 1007: training loss: 1382.6317009001962\n",
      "Epoch 3 step 1008: training accuarcy: 0.6905\n",
      "Epoch 3 step 1008: training loss: 1382.8640442224985\n",
      "Epoch 3 step 1009: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 1009: training loss: 1383.8827198516044\n",
      "Epoch 3 step 1010: training accuarcy: 0.6845\n",
      "Epoch 3 step 1010: training loss: 1382.3181600798202\n",
      "Epoch 3 step 1011: training accuarcy: 0.707\n",
      "Epoch 3 step 1011: training loss: 1382.367884250588\n",
      "Epoch 3 step 1012: training accuarcy: 0.715\n",
      "Epoch 3 step 1012: training loss: 1382.5813100818884\n",
      "Epoch 3 step 1013: training accuarcy: 0.7005\n",
      "Epoch 3 step 1013: training loss: 1382.8009129174284\n",
      "Epoch 3 step 1014: training accuarcy: 0.7020000000000001\n",
      "Epoch 3 step 1014: training loss: 1381.3296184513003\n",
      "Epoch 3 step 1015: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 1015: training loss: 1382.9580956928673\n",
      "Epoch 3 step 1016: training accuarcy: 0.7105\n",
      "Epoch 3 step 1016: training loss: 1383.2268028367287\n",
      "Epoch 3 step 1017: training accuarcy: 0.686\n",
      "Epoch 3 step 1017: training loss: 1381.7648541411527\n",
      "Epoch 3 step 1018: training accuarcy: 0.6970000000000001\n",
      "Epoch 3 step 1018: training loss: 1383.7410375505667\n",
      "Epoch 3 step 1019: training accuarcy: 0.6875\n",
      "Epoch 3 step 1019: training loss: 1383.3589404540758\n",
      "Epoch 3 step 1020: training accuarcy: 0.7015\n",
      "Epoch 3 step 1020: training loss: 1383.1176834308474\n",
      "Epoch 3 step 1021: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 1021: training loss: 1383.3478549687127\n",
      "Epoch 3 step 1022: training accuarcy: 0.6880000000000001\n",
      "Epoch 3 step 1022: training loss: 1381.8084800438196\n",
      "Epoch 3 step 1023: training accuarcy: 0.7045\n",
      "Epoch 3 step 1023: training loss: 1383.2007926355727\n",
      "Epoch 3 step 1024: training accuarcy: 0.6975\n",
      "Epoch 3 step 1024: training loss: 1382.4861545762933\n",
      "Epoch 3 step 1025: training accuarcy: 0.6965\n",
      "Epoch 3 step 1025: training loss: 1382.6511681624652\n",
      "Epoch 3 step 1026: training accuarcy: 0.7075\n",
      "Epoch 3 step 1026: training loss: 1382.6908408385661\n",
      "Epoch 3 step 1027: training accuarcy: 0.7065\n",
      "Epoch 3 step 1027: training loss: 1383.6487596329378\n",
      "Epoch 3 step 1028: training accuarcy: 0.6920000000000001\n",
      "Epoch 3 step 1028: training loss: 1382.452610242877\n",
      "Epoch 3 step 1029: training accuarcy: 0.7000000000000001\n",
      "Epoch 3 step 1029: training loss: 1382.1827777503354\n",
      "Epoch 3 step 1030: training accuarcy: 0.7055\n",
      "Epoch 3 step 1030: training loss: 1383.2447953854983\n",
      "Epoch 3 step 1031: training accuarcy: 0.6950000000000001\n",
      "Epoch 3 step 1031: training loss: 1384.067887958729\n",
      "Epoch 3 step 1032: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 1032: training loss: 1383.670526632963\n",
      "Epoch 3 step 1033: training accuarcy: 0.6935\n",
      "Epoch 3 step 1033: training loss: 1383.0198082992313\n",
      "Epoch 3 step 1034: training accuarcy: 0.678\n",
      "Epoch 3 step 1034: training loss: 1383.4498935009985\n",
      "Epoch 3 step 1035: training accuarcy: 0.684\n",
      "Epoch 3 step 1035: training loss: 1382.6092098689064\n",
      "Epoch 3 step 1036: training accuarcy: 0.7075\n",
      "Epoch 3 step 1036: training loss: 1382.9767935032742\n",
      "Epoch 3 step 1037: training accuarcy: 0.7010000000000001\n",
      "Epoch 3 step 1037: training loss: 1382.6044368688247\n",
      "Epoch 3 step 1038: training accuarcy: 0.6985\n",
      "Epoch 3 step 1038: training loss: 1383.7750460755167\n",
      "Epoch 3 step 1039: training accuarcy: 0.686\n",
      "Epoch 3 step 1039: training loss: 1384.15248415367\n",
      "Epoch 3 step 1040: training accuarcy: 0.665\n",
      "Epoch 3 step 1040: training loss: 1383.3735926124186\n",
      "Epoch 3 step 1041: training accuarcy: 0.6900000000000001\n",
      "Epoch 3 step 1041: training loss: 1382.433210542612\n",
      "Epoch 3 step 1042: training accuarcy: 0.686\n",
      "Epoch 3 step 1042: training loss: 1382.2463133346428\n",
      "Epoch 3 step 1043: training accuarcy: 0.6960000000000001\n",
      "Epoch 3 step 1043: training loss: 1383.6066352128764\n",
      "Epoch 3 step 1044: training accuarcy: 0.684\n",
      "Epoch 3 step 1044: training loss: 1382.914305794032\n",
      "Epoch 3 step 1045: training accuarcy: 0.7045\n",
      "Epoch 3 step 1045: training loss: 1382.8207407805858\n",
      "Epoch 3 step 1046: training accuarcy: 0.7035\n",
      "Epoch 3 step 1046: training loss: 1383.5247245725898\n",
      "Epoch 3 step 1047: training accuarcy: 0.6995\n",
      "Epoch 3 step 1047: training loss: 1381.4452054903948\n",
      "Epoch 3 step 1048: training accuarcy: 0.6930000000000001\n",
      "Epoch 3 step 1048: training loss: 1382.2720086335064\n",
      "Epoch 3 step 1049: training accuarcy: 0.7030000000000001\n",
      "Epoch 3 step 1049: training loss: 1382.2614505558422\n",
      "Epoch 3 step 1050: training accuarcy: 0.7095\n",
      "Epoch 3 step 1050: training loss: 1381.9401037423122\n",
      "Epoch 3 step 1051: training accuarcy: 0.713\n",
      "Epoch 3 step 1051: training loss: 542.3112190109429\n",
      "Epoch 3 step 1052: training accuarcy: 0.6935897435897436\n",
      "Epoch 3: train loss 1379.568299516494, train accuarcy 0.700809895992279\n",
      "Epoch 3: valid loss 1363.6248650571988, valid accuarcy 0.7157873511314392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 4/8 [07:42<07:45, 116.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 1382.5246919652498\n",
      "Epoch 4 step 1053: training accuarcy: 0.71\n",
      "Epoch 4 step 1053: training loss: 1381.8458729592346\n",
      "Epoch 4 step 1054: training accuarcy: 0.708\n",
      "Epoch 4 step 1054: training loss: 1383.148648274755\n",
      "Epoch 4 step 1055: training accuarcy: 0.704\n",
      "Epoch 4 step 1055: training loss: 1382.3012138236195\n",
      "Epoch 4 step 1056: training accuarcy: 0.72\n",
      "Epoch 4 step 1056: training loss: 1381.511025055811\n",
      "Epoch 4 step 1057: training accuarcy: 0.717\n",
      "Epoch 4 step 1057: training loss: 1382.7251593311412\n",
      "Epoch 4 step 1058: training accuarcy: 0.71\n",
      "Epoch 4 step 1058: training loss: 1382.137966299077\n",
      "Epoch 4 step 1059: training accuarcy: 0.723\n",
      "Epoch 4 step 1059: training loss: 1381.7830035196864\n",
      "Epoch 4 step 1060: training accuarcy: 0.712\n",
      "Epoch 4 step 1060: training loss: 1382.1192834601509\n",
      "Epoch 4 step 1061: training accuarcy: 0.7035\n",
      "Epoch 4 step 1061: training loss: 1382.2242559358194\n",
      "Epoch 4 step 1062: training accuarcy: 0.708\n",
      "Epoch 4 step 1062: training loss: 1381.8645179256189\n",
      "Epoch 4 step 1063: training accuarcy: 0.7045\n",
      "Epoch 4 step 1063: training loss: 1382.870882602911\n",
      "Epoch 4 step 1064: training accuarcy: 0.713\n",
      "Epoch 4 step 1064: training loss: 1380.9165022224627\n",
      "Epoch 4 step 1065: training accuarcy: 0.7165\n",
      "Epoch 4 step 1065: training loss: 1383.3864889511324\n",
      "Epoch 4 step 1066: training accuarcy: 0.6855\n",
      "Epoch 4 step 1066: training loss: 1381.9812122531005\n",
      "Epoch 4 step 1067: training accuarcy: 0.6955\n",
      "Epoch 4 step 1067: training loss: 1380.3996422966327\n",
      "Epoch 4 step 1068: training accuarcy: 0.709\n",
      "Epoch 4 step 1068: training loss: 1382.0115797301853\n",
      "Epoch 4 step 1069: training accuarcy: 0.7165\n",
      "Epoch 4 step 1069: training loss: 1381.468797235042\n",
      "Epoch 4 step 1070: training accuarcy: 0.7085\n",
      "Epoch 4 step 1070: training loss: 1383.0875019043442\n",
      "Epoch 4 step 1071: training accuarcy: 0.707\n",
      "Epoch 4 step 1071: training loss: 1382.2472468907433\n",
      "Epoch 4 step 1072: training accuarcy: 0.7225\n",
      "Epoch 4 step 1072: training loss: 1382.1161021404057\n",
      "Epoch 4 step 1073: training accuarcy: 0.7025\n",
      "Epoch 4 step 1073: training loss: 1381.7533539434346\n",
      "Epoch 4 step 1074: training accuarcy: 0.706\n",
      "Epoch 4 step 1074: training loss: 1381.4559377676567\n",
      "Epoch 4 step 1075: training accuarcy: 0.7135\n",
      "Epoch 4 step 1075: training loss: 1381.0766771225485\n",
      "Epoch 4 step 1076: training accuarcy: 0.7255\n",
      "Epoch 4 step 1076: training loss: 1383.0991128015885\n",
      "Epoch 4 step 1077: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1077: training loss: 1382.8765839008304\n",
      "Epoch 4 step 1078: training accuarcy: 0.711\n",
      "Epoch 4 step 1078: training loss: 1382.69640618908\n",
      "Epoch 4 step 1079: training accuarcy: 0.708\n",
      "Epoch 4 step 1079: training loss: 1382.8883079140885\n",
      "Epoch 4 step 1080: training accuarcy: 0.6945\n",
      "Epoch 4 step 1080: training loss: 1383.5652293953833\n",
      "Epoch 4 step 1081: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1081: training loss: 1382.13966476656\n",
      "Epoch 4 step 1082: training accuarcy: 0.718\n",
      "Epoch 4 step 1082: training loss: 1382.6144966716636\n",
      "Epoch 4 step 1083: training accuarcy: 0.709\n",
      "Epoch 4 step 1083: training loss: 1382.375913267942\n",
      "Epoch 4 step 1084: training accuarcy: 0.713\n",
      "Epoch 4 step 1084: training loss: 1381.9375152084235\n",
      "Epoch 4 step 1085: training accuarcy: 0.711\n",
      "Epoch 4 step 1085: training loss: 1382.1021903922501\n",
      "Epoch 4 step 1086: training accuarcy: 0.7135\n",
      "Epoch 4 step 1086: training loss: 1382.7374352752017\n",
      "Epoch 4 step 1087: training accuarcy: 0.7055\n",
      "Epoch 4 step 1087: training loss: 1382.0080746180536\n",
      "Epoch 4 step 1088: training accuarcy: 0.707\n",
      "Epoch 4 step 1088: training loss: 1382.4070738828736\n",
      "Epoch 4 step 1089: training accuarcy: 0.709\n",
      "Epoch 4 step 1089: training loss: 1382.02985549696\n",
      "Epoch 4 step 1090: training accuarcy: 0.721\n",
      "Epoch 4 step 1090: training loss: 1383.453915892891\n",
      "Epoch 4 step 1091: training accuarcy: 0.7005\n",
      "Epoch 4 step 1091: training loss: 1382.0874838345137\n",
      "Epoch 4 step 1092: training accuarcy: 0.7215\n",
      "Epoch 4 step 1092: training loss: 1383.5476587448286\n",
      "Epoch 4 step 1093: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1093: training loss: 1382.0026208542454\n",
      "Epoch 4 step 1094: training accuarcy: 0.7065\n",
      "Epoch 4 step 1094: training loss: 1381.4363107130762\n",
      "Epoch 4 step 1095: training accuarcy: 0.718\n",
      "Epoch 4 step 1095: training loss: 1382.724268492269\n",
      "Epoch 4 step 1096: training accuarcy: 0.705\n",
      "Epoch 4 step 1096: training loss: 1381.8377170716412\n",
      "Epoch 4 step 1097: training accuarcy: 0.7075\n",
      "Epoch 4 step 1097: training loss: 1381.8158633546996\n",
      "Epoch 4 step 1098: training accuarcy: 0.7105\n",
      "Epoch 4 step 1098: training loss: 1382.4281433850108\n",
      "Epoch 4 step 1099: training accuarcy: 0.7105\n",
      "Epoch 4 step 1099: training loss: 1382.2433589947188\n",
      "Epoch 4 step 1100: training accuarcy: 0.718\n",
      "Epoch 4 step 1100: training loss: 1382.7753817829275\n",
      "Epoch 4 step 1101: training accuarcy: 0.7015\n",
      "Epoch 4 step 1101: training loss: 1382.001591552839\n",
      "Epoch 4 step 1102: training accuarcy: 0.705\n",
      "Epoch 4 step 1102: training loss: 1381.628451067273\n",
      "Epoch 4 step 1103: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1103: training loss: 1383.2671988989164\n",
      "Epoch 4 step 1104: training accuarcy: 0.71\n",
      "Epoch 4 step 1104: training loss: 1383.096384995542\n",
      "Epoch 4 step 1105: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1105: training loss: 1383.6938443545557\n",
      "Epoch 4 step 1106: training accuarcy: 0.6925\n",
      "Epoch 4 step 1106: training loss: 1382.9890182850017\n",
      "Epoch 4 step 1107: training accuarcy: 0.7055\n",
      "Epoch 4 step 1107: training loss: 1382.5561620337169\n",
      "Epoch 4 step 1108: training accuarcy: 0.7115\n",
      "Epoch 4 step 1108: training loss: 1381.8575838431948\n",
      "Epoch 4 step 1109: training accuarcy: 0.7095\n",
      "Epoch 4 step 1109: training loss: 1383.3660477108515\n",
      "Epoch 4 step 1110: training accuarcy: 0.6975\n",
      "Epoch 4 step 1110: training loss: 1381.8435447717409\n",
      "Epoch 4 step 1111: training accuarcy: 0.7105\n",
      "Epoch 4 step 1111: training loss: 1382.9712178132029\n",
      "Epoch 4 step 1112: training accuarcy: 0.6965\n",
      "Epoch 4 step 1112: training loss: 1381.7814957576643\n",
      "Epoch 4 step 1113: training accuarcy: 0.7145\n",
      "Epoch 4 step 1113: training loss: 1382.4054846645988\n",
      "Epoch 4 step 1114: training accuarcy: 0.717\n",
      "Epoch 4 step 1114: training loss: 1382.6677745977152\n",
      "Epoch 4 step 1115: training accuarcy: 0.6965\n",
      "Epoch 4 step 1115: training loss: 1382.2913078135691\n",
      "Epoch 4 step 1116: training accuarcy: 0.7165\n",
      "Epoch 4 step 1116: training loss: 1382.2806523578379\n",
      "Epoch 4 step 1117: training accuarcy: 0.711\n",
      "Epoch 4 step 1117: training loss: 1383.021032986505\n",
      "Epoch 4 step 1118: training accuarcy: 0.7095\n",
      "Epoch 4 step 1118: training loss: 1382.442373797845\n",
      "Epoch 4 step 1119: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1119: training loss: 1383.7716768063751\n",
      "Epoch 4 step 1120: training accuarcy: 0.6855\n",
      "Epoch 4 step 1120: training loss: 1383.2161369727069\n",
      "Epoch 4 step 1121: training accuarcy: 0.712\n",
      "Epoch 4 step 1121: training loss: 1383.5018757701373\n",
      "Epoch 4 step 1122: training accuarcy: 0.7000000000000001\n",
      "Epoch 4 step 1122: training loss: 1382.5853124867097\n",
      "Epoch 4 step 1123: training accuarcy: 0.6955\n",
      "Epoch 4 step 1123: training loss: 1382.4945164570886\n",
      "Epoch 4 step 1124: training accuarcy: 0.7125\n",
      "Epoch 4 step 1124: training loss: 1381.75168389833\n",
      "Epoch 4 step 1125: training accuarcy: 0.7125\n",
      "Epoch 4 step 1125: training loss: 1383.2234116520476\n",
      "Epoch 4 step 1126: training accuarcy: 0.6985\n",
      "Epoch 4 step 1126: training loss: 1383.0332918829\n",
      "Epoch 4 step 1127: training accuarcy: 0.7095\n",
      "Epoch 4 step 1127: training loss: 1383.6438040309013\n",
      "Epoch 4 step 1128: training accuarcy: 0.6905\n",
      "Epoch 4 step 1128: training loss: 1382.2627212057143\n",
      "Epoch 4 step 1129: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1129: training loss: 1381.2620396875639\n",
      "Epoch 4 step 1130: training accuarcy: 0.7075\n",
      "Epoch 4 step 1130: training loss: 1383.246616963283\n",
      "Epoch 4 step 1131: training accuarcy: 0.7085\n",
      "Epoch 4 step 1131: training loss: 1383.189207585979\n",
      "Epoch 4 step 1132: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1132: training loss: 1382.9823398961673\n",
      "Epoch 4 step 1133: training accuarcy: 0.7005\n",
      "Epoch 4 step 1133: training loss: 1382.6765201630299\n",
      "Epoch 4 step 1134: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1134: training loss: 1382.825506868949\n",
      "Epoch 4 step 1135: training accuarcy: 0.6965\n",
      "Epoch 4 step 1135: training loss: 1382.5509399401856\n",
      "Epoch 4 step 1136: training accuarcy: 0.6955\n",
      "Epoch 4 step 1136: training loss: 1382.303881698267\n",
      "Epoch 4 step 1137: training accuarcy: 0.6965\n",
      "Epoch 4 step 1137: training loss: 1382.4545808159153\n",
      "Epoch 4 step 1138: training accuarcy: 0.7185\n",
      "Epoch 4 step 1138: training loss: 1381.7943073951442\n",
      "Epoch 4 step 1139: training accuarcy: 0.7025\n",
      "Epoch 4 step 1139: training loss: 1381.9264066065105\n",
      "Epoch 4 step 1140: training accuarcy: 0.712\n",
      "Epoch 4 step 1140: training loss: 1383.2061433088438\n",
      "Epoch 4 step 1141: training accuarcy: 0.704\n",
      "Epoch 4 step 1141: training loss: 1382.2743686269353\n",
      "Epoch 4 step 1142: training accuarcy: 0.7185\n",
      "Epoch 4 step 1142: training loss: 1383.247236501556\n",
      "Epoch 4 step 1143: training accuarcy: 0.686\n",
      "Epoch 4 step 1143: training loss: 1382.0403790674904\n",
      "Epoch 4 step 1144: training accuarcy: 0.7010000000000001\n",
      "Epoch 4 step 1144: training loss: 1383.4319116190868\n",
      "Epoch 4 step 1145: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 1145: training loss: 1383.1307511989519\n",
      "Epoch 4 step 1146: training accuarcy: 0.707\n",
      "Epoch 4 step 1146: training loss: 1383.0046433858554\n",
      "Epoch 4 step 1147: training accuarcy: 0.6805\n",
      "Epoch 4 step 1147: training loss: 1382.2645718788558\n",
      "Epoch 4 step 1148: training accuarcy: 0.7125\n",
      "Epoch 4 step 1148: training loss: 1383.8387737674457\n",
      "Epoch 4 step 1149: training accuarcy: 0.68\n",
      "Epoch 4 step 1149: training loss: 1382.37047267295\n",
      "Epoch 4 step 1150: training accuarcy: 0.7065\n",
      "Epoch 4 step 1150: training loss: 1383.3386577972622\n",
      "Epoch 4 step 1151: training accuarcy: 0.6945\n",
      "Epoch 4 step 1151: training loss: 1382.6290832758627\n",
      "Epoch 4 step 1152: training accuarcy: 0.7075\n",
      "Epoch 4 step 1152: training loss: 1382.6313336732458\n",
      "Epoch 4 step 1153: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1153: training loss: 1382.9459854862805\n",
      "Epoch 4 step 1154: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1154: training loss: 1383.0895001518127\n",
      "Epoch 4 step 1155: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1155: training loss: 1383.6926458206294\n",
      "Epoch 4 step 1156: training accuarcy: 0.6855\n",
      "Epoch 4 step 1156: training loss: 1383.4612352155866\n",
      "Epoch 4 step 1157: training accuarcy: 0.677\n",
      "Epoch 4 step 1157: training loss: 1381.8953555923385\n",
      "Epoch 4 step 1158: training accuarcy: 0.6965\n",
      "Epoch 4 step 1158: training loss: 1383.788787983831\n",
      "Epoch 4 step 1159: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1159: training loss: 1383.344137439292\n",
      "Epoch 4 step 1160: training accuarcy: 0.6975\n",
      "Epoch 4 step 1160: training loss: 1382.9243662862054\n",
      "Epoch 4 step 1161: training accuarcy: 0.709\n",
      "Epoch 4 step 1161: training loss: 1384.4103723992132\n",
      "Epoch 4 step 1162: training accuarcy: 0.684\n",
      "Epoch 4 step 1162: training loss: 1383.5218182578444\n",
      "Epoch 4 step 1163: training accuarcy: 0.6945\n",
      "Epoch 4 step 1163: training loss: 1383.2485617404427\n",
      "Epoch 4 step 1164: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1164: training loss: 1383.3981601780833\n",
      "Epoch 4 step 1165: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1165: training loss: 1382.3650302416445\n",
      "Epoch 4 step 1166: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1166: training loss: 1382.916603673417\n",
      "Epoch 4 step 1167: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1167: training loss: 1383.072070642687\n",
      "Epoch 4 step 1168: training accuarcy: 0.6915\n",
      "Epoch 4 step 1168: training loss: 1381.919229977157\n",
      "Epoch 4 step 1169: training accuarcy: 0.7165\n",
      "Epoch 4 step 1169: training loss: 1383.3382279743926\n",
      "Epoch 4 step 1170: training accuarcy: 0.6815\n",
      "Epoch 4 step 1170: training loss: 1382.6410799981625\n",
      "Epoch 4 step 1171: training accuarcy: 0.6975\n",
      "Epoch 4 step 1171: training loss: 1382.8436437967616\n",
      "Epoch 4 step 1172: training accuarcy: 0.7025\n",
      "Epoch 4 step 1172: training loss: 1383.7522428174339\n",
      "Epoch 4 step 1173: training accuarcy: 0.6835\n",
      "Epoch 4 step 1173: training loss: 1384.042432229929\n",
      "Epoch 4 step 1174: training accuarcy: 0.6815\n",
      "Epoch 4 step 1174: training loss: 1382.869508478283\n",
      "Epoch 4 step 1175: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1175: training loss: 1382.6417747535938\n",
      "Epoch 4 step 1176: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1176: training loss: 1384.261961077231\n",
      "Epoch 4 step 1177: training accuarcy: 0.684\n",
      "Epoch 4 step 1177: training loss: 1382.1217952025047\n",
      "Epoch 4 step 1178: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1178: training loss: 1382.5246678850779\n",
      "Epoch 4 step 1179: training accuarcy: 0.706\n",
      "Epoch 4 step 1179: training loss: 1383.8668644401716\n",
      "Epoch 4 step 1180: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1180: training loss: 1383.3004002590997\n",
      "Epoch 4 step 1181: training accuarcy: 0.707\n",
      "Epoch 4 step 1181: training loss: 1382.241572589099\n",
      "Epoch 4 step 1182: training accuarcy: 0.6945\n",
      "Epoch 4 step 1182: training loss: 1382.880752328525\n",
      "Epoch 4 step 1183: training accuarcy: 0.6895\n",
      "Epoch 4 step 1183: training loss: 1383.9278020534464\n",
      "Epoch 4 step 1184: training accuarcy: 0.675\n",
      "Epoch 4 step 1184: training loss: 1382.2284323846307\n",
      "Epoch 4 step 1185: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1185: training loss: 1383.9415608748766\n",
      "Epoch 4 step 1186: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1186: training loss: 1382.0479656800176\n",
      "Epoch 4 step 1187: training accuarcy: 0.6975\n",
      "Epoch 4 step 1187: training loss: 1382.2734984413585\n",
      "Epoch 4 step 1188: training accuarcy: 0.6925\n",
      "Epoch 4 step 1188: training loss: 1383.9165761890993\n",
      "Epoch 4 step 1189: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1189: training loss: 1381.7339154220429\n",
      "Epoch 4 step 1190: training accuarcy: 0.7065\n",
      "Epoch 4 step 1190: training loss: 1382.1036324278941\n",
      "Epoch 4 step 1191: training accuarcy: 0.7045\n",
      "Epoch 4 step 1191: training loss: 1382.5862177096012\n",
      "Epoch 4 step 1192: training accuarcy: 0.675\n",
      "Epoch 4 step 1192: training loss: 1383.04676570879\n",
      "Epoch 4 step 1193: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1193: training loss: 1382.5874836638316\n",
      "Epoch 4 step 1194: training accuarcy: 0.7005\n",
      "Epoch 4 step 1194: training loss: 1383.4971179787988\n",
      "Epoch 4 step 1195: training accuarcy: 0.7115\n",
      "Epoch 4 step 1195: training loss: 1382.8972291401467\n",
      "Epoch 4 step 1196: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1196: training loss: 1384.0762414908645\n",
      "Epoch 4 step 1197: training accuarcy: 0.6900000000000001\n",
      "Epoch 4 step 1197: training loss: 1382.77558446805\n",
      "Epoch 4 step 1198: training accuarcy: 0.708\n",
      "Epoch 4 step 1198: training loss: 1383.118629185266\n",
      "Epoch 4 step 1199: training accuarcy: 0.6965\n",
      "Epoch 4 step 1199: training loss: 1383.546364669714\n",
      "Epoch 4 step 1200: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1200: training loss: 1383.2083546899235\n",
      "Epoch 4 step 1201: training accuarcy: 0.7035\n",
      "Epoch 4 step 1201: training loss: 1383.0448026892855\n",
      "Epoch 4 step 1202: training accuarcy: 0.6875\n",
      "Epoch 4 step 1202: training loss: 1382.8708919652863\n",
      "Epoch 4 step 1203: training accuarcy: 0.7075\n",
      "Epoch 4 step 1203: training loss: 1382.6040956392742\n",
      "Epoch 4 step 1204: training accuarcy: 0.6975\n",
      "Epoch 4 step 1204: training loss: 1382.7879502250148\n",
      "Epoch 4 step 1205: training accuarcy: 0.7105\n",
      "Epoch 4 step 1205: training loss: 1382.4433817104618\n",
      "Epoch 4 step 1206: training accuarcy: 0.7075\n",
      "Epoch 4 step 1206: training loss: 1383.4640053260082\n",
      "Epoch 4 step 1207: training accuarcy: 0.6795\n",
      "Epoch 4 step 1207: training loss: 1383.1397159755359\n",
      "Epoch 4 step 1208: training accuarcy: 0.685\n",
      "Epoch 4 step 1208: training loss: 1382.6508130689338\n",
      "Epoch 4 step 1209: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1209: training loss: 1383.486899209177\n",
      "Epoch 4 step 1210: training accuarcy: 0.6945\n",
      "Epoch 4 step 1210: training loss: 1384.100116611686\n",
      "Epoch 4 step 1211: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1211: training loss: 1381.9898043623737\n",
      "Epoch 4 step 1212: training accuarcy: 0.7015\n",
      "Epoch 4 step 1212: training loss: 1383.0348029668078\n",
      "Epoch 4 step 1213: training accuarcy: 0.6935\n",
      "Epoch 4 step 1213: training loss: 1383.6729505398985\n",
      "Epoch 4 step 1214: training accuarcy: 0.6975\n",
      "Epoch 4 step 1214: training loss: 1383.3638833539712\n",
      "Epoch 4 step 1215: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1215: training loss: 1384.238067820306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1216: training accuarcy: 0.668\n",
      "Epoch 4 step 1216: training loss: 1382.9617814476774\n",
      "Epoch 4 step 1217: training accuarcy: 0.712\n",
      "Epoch 4 step 1217: training loss: 1383.166450067319\n",
      "Epoch 4 step 1218: training accuarcy: 0.7025\n",
      "Epoch 4 step 1218: training loss: 1382.0339700890731\n",
      "Epoch 4 step 1219: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1219: training loss: 1382.6973158426463\n",
      "Epoch 4 step 1220: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1220: training loss: 1383.7232188561632\n",
      "Epoch 4 step 1221: training accuarcy: 0.6920000000000001\n",
      "Epoch 4 step 1221: training loss: 1381.9970947061893\n",
      "Epoch 4 step 1222: training accuarcy: 0.714\n",
      "Epoch 4 step 1222: training loss: 1383.2876036518117\n",
      "Epoch 4 step 1223: training accuarcy: 0.6935\n",
      "Epoch 4 step 1223: training loss: 1383.3480438914078\n",
      "Epoch 4 step 1224: training accuarcy: 0.7055\n",
      "Epoch 4 step 1224: training loss: 1383.440626057764\n",
      "Epoch 4 step 1225: training accuarcy: 0.7015\n",
      "Epoch 4 step 1225: training loss: 1383.3946085388015\n",
      "Epoch 4 step 1226: training accuarcy: 0.685\n",
      "Epoch 4 step 1226: training loss: 1382.717356856836\n",
      "Epoch 4 step 1227: training accuarcy: 0.6965\n",
      "Epoch 4 step 1227: training loss: 1383.4382776824116\n",
      "Epoch 4 step 1228: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1228: training loss: 1382.7030864046417\n",
      "Epoch 4 step 1229: training accuarcy: 0.6955\n",
      "Epoch 4 step 1229: training loss: 1383.006849717402\n",
      "Epoch 4 step 1230: training accuarcy: 0.6985\n",
      "Epoch 4 step 1230: training loss: 1383.9816545947356\n",
      "Epoch 4 step 1231: training accuarcy: 0.6875\n",
      "Epoch 4 step 1231: training loss: 1382.823549586863\n",
      "Epoch 4 step 1232: training accuarcy: 0.6985\n",
      "Epoch 4 step 1232: training loss: 1382.755883064133\n",
      "Epoch 4 step 1233: training accuarcy: 0.7075\n",
      "Epoch 4 step 1233: training loss: 1383.2413387739784\n",
      "Epoch 4 step 1234: training accuarcy: 0.6945\n",
      "Epoch 4 step 1234: training loss: 1383.0245072481518\n",
      "Epoch 4 step 1235: training accuarcy: 0.7055\n",
      "Epoch 4 step 1235: training loss: 1383.1797095724594\n",
      "Epoch 4 step 1236: training accuarcy: 0.674\n",
      "Epoch 4 step 1236: training loss: 1383.4791848545592\n",
      "Epoch 4 step 1237: training accuarcy: 0.6855\n",
      "Epoch 4 step 1237: training loss: 1382.1925008556443\n",
      "Epoch 4 step 1238: training accuarcy: 0.7105\n",
      "Epoch 4 step 1238: training loss: 1382.6521441473365\n",
      "Epoch 4 step 1239: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1239: training loss: 1383.1395992255177\n",
      "Epoch 4 step 1240: training accuarcy: 0.6995\n",
      "Epoch 4 step 1240: training loss: 1382.5292200196518\n",
      "Epoch 4 step 1241: training accuarcy: 0.6975\n",
      "Epoch 4 step 1241: training loss: 1383.3511921343918\n",
      "Epoch 4 step 1242: training accuarcy: 0.6995\n",
      "Epoch 4 step 1242: training loss: 1382.7325362366219\n",
      "Epoch 4 step 1243: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1243: training loss: 1383.2878524607652\n",
      "Epoch 4 step 1244: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1244: training loss: 1381.6197321980949\n",
      "Epoch 4 step 1245: training accuarcy: 0.719\n",
      "Epoch 4 step 1245: training loss: 1382.8896129435238\n",
      "Epoch 4 step 1246: training accuarcy: 0.6905\n",
      "Epoch 4 step 1246: training loss: 1382.3481170239413\n",
      "Epoch 4 step 1247: training accuarcy: 0.6985\n",
      "Epoch 4 step 1247: training loss: 1383.468906385795\n",
      "Epoch 4 step 1248: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1248: training loss: 1381.978874514369\n",
      "Epoch 4 step 1249: training accuarcy: 0.7065\n",
      "Epoch 4 step 1249: training loss: 1382.5720331458745\n",
      "Epoch 4 step 1250: training accuarcy: 0.686\n",
      "Epoch 4 step 1250: training loss: 1382.9807012292247\n",
      "Epoch 4 step 1251: training accuarcy: 0.6930000000000001\n",
      "Epoch 4 step 1251: training loss: 1382.993673430494\n",
      "Epoch 4 step 1252: training accuarcy: 0.6955\n",
      "Epoch 4 step 1252: training loss: 1382.4462378110247\n",
      "Epoch 4 step 1253: training accuarcy: 0.712\n",
      "Epoch 4 step 1253: training loss: 1383.4802309569711\n",
      "Epoch 4 step 1254: training accuarcy: 0.683\n",
      "Epoch 4 step 1254: training loss: 1383.175083665975\n",
      "Epoch 4 step 1255: training accuarcy: 0.7075\n",
      "Epoch 4 step 1255: training loss: 1383.8110049738789\n",
      "Epoch 4 step 1256: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1256: training loss: 1383.2503777161285\n",
      "Epoch 4 step 1257: training accuarcy: 0.6865\n",
      "Epoch 4 step 1257: training loss: 1381.5793561016035\n",
      "Epoch 4 step 1258: training accuarcy: 0.6970000000000001\n",
      "Epoch 4 step 1258: training loss: 1382.4094091935306\n",
      "Epoch 4 step 1259: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1259: training loss: 1382.673922040631\n",
      "Epoch 4 step 1260: training accuarcy: 0.7175\n",
      "Epoch 4 step 1260: training loss: 1382.4097728734318\n",
      "Epoch 4 step 1261: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1261: training loss: 1382.4604772454732\n",
      "Epoch 4 step 1262: training accuarcy: 0.7025\n",
      "Epoch 4 step 1262: training loss: 1383.44474084762\n",
      "Epoch 4 step 1263: training accuarcy: 0.6945\n",
      "Epoch 4 step 1263: training loss: 1383.3975814100627\n",
      "Epoch 4 step 1264: training accuarcy: 0.6975\n",
      "Epoch 4 step 1264: training loss: 1381.7607107467854\n",
      "Epoch 4 step 1265: training accuarcy: 0.6995\n",
      "Epoch 4 step 1265: training loss: 1382.782517158179\n",
      "Epoch 4 step 1266: training accuarcy: 0.685\n",
      "Epoch 4 step 1266: training loss: 1383.450170518286\n",
      "Epoch 4 step 1267: training accuarcy: 0.6875\n",
      "Epoch 4 step 1267: training loss: 1381.8907065151027\n",
      "Epoch 4 step 1268: training accuarcy: 0.7025\n",
      "Epoch 4 step 1268: training loss: 1382.0445823701782\n",
      "Epoch 4 step 1269: training accuarcy: 0.6950000000000001\n",
      "Epoch 4 step 1269: training loss: 1383.891815304113\n",
      "Epoch 4 step 1270: training accuarcy: 0.6895\n",
      "Epoch 4 step 1270: training loss: 1382.4373296038023\n",
      "Epoch 4 step 1271: training accuarcy: 0.7065\n",
      "Epoch 4 step 1271: training loss: 1382.240662712702\n",
      "Epoch 4 step 1272: training accuarcy: 0.7005\n",
      "Epoch 4 step 1272: training loss: 1382.1451568522082\n",
      "Epoch 4 step 1273: training accuarcy: 0.7145\n",
      "Epoch 4 step 1273: training loss: 1382.6366186587736\n",
      "Epoch 4 step 1274: training accuarcy: 0.6960000000000001\n",
      "Epoch 4 step 1274: training loss: 1382.182725291315\n",
      "Epoch 4 step 1275: training accuarcy: 0.711\n",
      "Epoch 4 step 1275: training loss: 1382.4531663728558\n",
      "Epoch 4 step 1276: training accuarcy: 0.6905\n",
      "Epoch 4 step 1276: training loss: 1382.7712450212914\n",
      "Epoch 4 step 1277: training accuarcy: 0.6955\n",
      "Epoch 4 step 1277: training loss: 1383.2699798719925\n",
      "Epoch 4 step 1278: training accuarcy: 0.687\n",
      "Epoch 4 step 1278: training loss: 1383.2092100893792\n",
      "Epoch 4 step 1279: training accuarcy: 0.706\n",
      "Epoch 4 step 1279: training loss: 1382.725958753093\n",
      "Epoch 4 step 1280: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1280: training loss: 1382.2294678343167\n",
      "Epoch 4 step 1281: training accuarcy: 0.7025\n",
      "Epoch 4 step 1281: training loss: 1382.6584267516528\n",
      "Epoch 4 step 1282: training accuarcy: 0.6935\n",
      "Epoch 4 step 1282: training loss: 1382.33012032181\n",
      "Epoch 4 step 1283: training accuarcy: 0.6980000000000001\n",
      "Epoch 4 step 1283: training loss: 1382.2151073071705\n",
      "Epoch 4 step 1284: training accuarcy: 0.7035\n",
      "Epoch 4 step 1284: training loss: 1383.386834074184\n",
      "Epoch 4 step 1285: training accuarcy: 0.6910000000000001\n",
      "Epoch 4 step 1285: training loss: 1383.2260256704121\n",
      "Epoch 4 step 1286: training accuarcy: 0.682\n",
      "Epoch 4 step 1286: training loss: 1382.7713598859305\n",
      "Epoch 4 step 1287: training accuarcy: 0.7020000000000001\n",
      "Epoch 4 step 1287: training loss: 1382.9161862711344\n",
      "Epoch 4 step 1288: training accuarcy: 0.7030000000000001\n",
      "Epoch 4 step 1288: training loss: 1382.7465660033558\n",
      "Epoch 4 step 1289: training accuarcy: 0.7015\n",
      "Epoch 4 step 1289: training loss: 1383.1521192143537\n",
      "Epoch 4 step 1290: training accuarcy: 0.6825\n",
      "Epoch 4 step 1290: training loss: 1382.919535725292\n",
      "Epoch 4 step 1291: training accuarcy: 0.6890000000000001\n",
      "Epoch 4 step 1291: training loss: 1383.417946632399\n",
      "Epoch 4 step 1292: training accuarcy: 0.6795\n",
      "Epoch 4 step 1292: training loss: 1384.2938694509587\n",
      "Epoch 4 step 1293: training accuarcy: 0.681\n",
      "Epoch 4 step 1293: training loss: 1381.89921531084\n",
      "Epoch 4 step 1294: training accuarcy: 0.6985\n",
      "Epoch 4 step 1294: training loss: 1383.2747393477102\n",
      "Epoch 4 step 1295: training accuarcy: 0.7035\n",
      "Epoch 4 step 1295: training loss: 1383.8229885932556\n",
      "Epoch 4 step 1296: training accuarcy: 0.687\n",
      "Epoch 4 step 1296: training loss: 1382.7725136029621\n",
      "Epoch 4 step 1297: training accuarcy: 0.708\n",
      "Epoch 4 step 1297: training loss: 1383.4334950204632\n",
      "Epoch 4 step 1298: training accuarcy: 0.679\n",
      "Epoch 4 step 1298: training loss: 1382.6049478996752\n",
      "Epoch 4 step 1299: training accuarcy: 0.6985\n",
      "Epoch 4 step 1299: training loss: 1383.1202348315921\n",
      "Epoch 4 step 1300: training accuarcy: 0.6940000000000001\n",
      "Epoch 4 step 1300: training loss: 1383.5203515307292\n",
      "Epoch 4 step 1301: training accuarcy: 0.681\n",
      "Epoch 4 step 1301: training loss: 1383.1862973337197\n",
      "Epoch 4 step 1302: training accuarcy: 0.708\n",
      "Epoch 4 step 1302: training loss: 1382.9444791556332\n",
      "Epoch 4 step 1303: training accuarcy: 0.684\n",
      "Epoch 4 step 1303: training loss: 1382.371725197218\n",
      "Epoch 4 step 1304: training accuarcy: 0.6975\n",
      "Epoch 4 step 1304: training loss: 1382.606559640893\n",
      "Epoch 4 step 1305: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1305: training loss: 1382.8515706737464\n",
      "Epoch 4 step 1306: training accuarcy: 0.7035\n",
      "Epoch 4 step 1306: training loss: 1383.454628888206\n",
      "Epoch 4 step 1307: training accuarcy: 0.6875\n",
      "Epoch 4 step 1307: training loss: 1383.7621143081903\n",
      "Epoch 4 step 1308: training accuarcy: 0.6795\n",
      "Epoch 4 step 1308: training loss: 1382.3130457341572\n",
      "Epoch 4 step 1309: training accuarcy: 0.6935\n",
      "Epoch 4 step 1309: training loss: 1383.1834271176986\n",
      "Epoch 4 step 1310: training accuarcy: 0.683\n",
      "Epoch 4 step 1310: training loss: 1383.412112273902\n",
      "Epoch 4 step 1311: training accuarcy: 0.6880000000000001\n",
      "Epoch 4 step 1311: training loss: 1383.4289710979676\n",
      "Epoch 4 step 1312: training accuarcy: 0.686\n",
      "Epoch 4 step 1312: training loss: 1382.7629612603437\n",
      "Epoch 4 step 1313: training accuarcy: 0.684\n",
      "Epoch 4 step 1313: training loss: 1383.1199585790146\n",
      "Epoch 4 step 1314: training accuarcy: 0.6990000000000001\n",
      "Epoch 4 step 1314: training loss: 543.1382792785935\n",
      "Epoch 4 step 1315: training accuarcy: 0.6923076923076923\n",
      "Epoch 4: train loss 1379.587826988156, train accuarcy 0.7011027932167053\n",
      "Epoch 4: valid loss 1363.6866410149555, valid accuarcy 0.7122498750686646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                         | 5/8 [09:51<06:00, 120.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 1315: training loss: 1381.7430139295107\n",
      "Epoch 5 step 1316: training accuarcy: 0.7095\n",
      "Epoch 5 step 1316: training loss: 1382.0481881308806\n",
      "Epoch 5 step 1317: training accuarcy: 0.7095\n",
      "Epoch 5 step 1317: training loss: 1380.6654792173902\n",
      "Epoch 5 step 1318: training accuarcy: 0.7185\n",
      "Epoch 5 step 1318: training loss: 1382.1917287253682\n",
      "Epoch 5 step 1319: training accuarcy: 0.6925\n",
      "Epoch 5 step 1319: training loss: 1382.9747114238955\n",
      "Epoch 5 step 1320: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1320: training loss: 1381.605863538038\n",
      "Epoch 5 step 1321: training accuarcy: 0.7265\n",
      "Epoch 5 step 1321: training loss: 1382.7886336902134\n",
      "Epoch 5 step 1322: training accuarcy: 0.717\n",
      "Epoch 5 step 1322: training loss: 1382.2918614195526\n",
      "Epoch 5 step 1323: training accuarcy: 0.7075\n",
      "Epoch 5 step 1323: training loss: 1382.7890369344545\n",
      "Epoch 5 step 1324: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1324: training loss: 1381.0536386608583\n",
      "Epoch 5 step 1325: training accuarcy: 0.7355\n",
      "Epoch 5 step 1325: training loss: 1381.4990234430454\n",
      "Epoch 5 step 1326: training accuarcy: 0.7255\n",
      "Epoch 5 step 1326: training loss: 1382.1771793684345\n",
      "Epoch 5 step 1327: training accuarcy: 0.713\n",
      "Epoch 5 step 1327: training loss: 1381.7147141787048\n",
      "Epoch 5 step 1328: training accuarcy: 0.7235\n",
      "Epoch 5 step 1328: training loss: 1381.3933862506485\n",
      "Epoch 5 step 1329: training accuarcy: 0.716\n",
      "Epoch 5 step 1329: training loss: 1382.168421562897\n",
      "Epoch 5 step 1330: training accuarcy: 0.723\n",
      "Epoch 5 step 1330: training loss: 1381.9565367555529\n",
      "Epoch 5 step 1331: training accuarcy: 0.7205\n",
      "Epoch 5 step 1331: training loss: 1382.7577670226833\n",
      "Epoch 5 step 1332: training accuarcy: 0.7020000000000001\n",
      "Epoch 5 step 1332: training loss: 1382.0274896683422\n",
      "Epoch 5 step 1333: training accuarcy: 0.7165\n",
      "Epoch 5 step 1333: training loss: 1382.6823489305975\n",
      "Epoch 5 step 1334: training accuarcy: 0.716\n",
      "Epoch 5 step 1334: training loss: 1382.1648263686907\n",
      "Epoch 5 step 1335: training accuarcy: 0.723\n",
      "Epoch 5 step 1335: training loss: 1382.1522106787947\n",
      "Epoch 5 step 1336: training accuarcy: 0.719\n",
      "Epoch 5 step 1336: training loss: 1382.7630124287527\n",
      "Epoch 5 step 1337: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1337: training loss: 1382.728895442681\n",
      "Epoch 5 step 1338: training accuarcy: 0.6995\n",
      "Epoch 5 step 1338: training loss: 1382.594956906891\n",
      "Epoch 5 step 1339: training accuarcy: 0.715\n",
      "Epoch 5 step 1339: training loss: 1382.7014103027057\n",
      "Epoch 5 step 1340: training accuarcy: 0.7295\n",
      "Epoch 5 step 1340: training loss: 1382.6285203720759\n",
      "Epoch 5 step 1341: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1341: training loss: 1384.07666763788\n",
      "Epoch 5 step 1342: training accuarcy: 0.7085\n",
      "Epoch 5 step 1342: training loss: 1382.525481902144\n",
      "Epoch 5 step 1343: training accuarcy: 0.715\n",
      "Epoch 5 step 1343: training loss: 1382.3238878442755\n",
      "Epoch 5 step 1344: training accuarcy: 0.7085\n",
      "Epoch 5 step 1344: training loss: 1382.599617037363\n",
      "Epoch 5 step 1345: training accuarcy: 0.709\n",
      "Epoch 5 step 1345: training loss: 1383.3073867372227\n",
      "Epoch 5 step 1346: training accuarcy: 0.6945\n",
      "Epoch 5 step 1346: training loss: 1382.9058881337453\n",
      "Epoch 5 step 1347: training accuarcy: 0.7035\n",
      "Epoch 5 step 1347: training loss: 1381.2480632766358\n",
      "Epoch 5 step 1348: training accuarcy: 0.7235\n",
      "Epoch 5 step 1348: training loss: 1382.561288975631\n",
      "Epoch 5 step 1349: training accuarcy: 0.712\n",
      "Epoch 5 step 1349: training loss: 1382.3670718407484\n",
      "Epoch 5 step 1350: training accuarcy: 0.709\n",
      "Epoch 5 step 1350: training loss: 1382.6971105417317\n",
      "Epoch 5 step 1351: training accuarcy: 0.6955\n",
      "Epoch 5 step 1351: training loss: 1381.9597317058754\n",
      "Epoch 5 step 1352: training accuarcy: 0.72\n",
      "Epoch 5 step 1352: training loss: 1383.3086941367958\n",
      "Epoch 5 step 1353: training accuarcy: 0.6975\n",
      "Epoch 5 step 1353: training loss: 1382.2819382304835\n",
      "Epoch 5 step 1354: training accuarcy: 0.711\n",
      "Epoch 5 step 1354: training loss: 1383.1743567174783\n",
      "Epoch 5 step 1355: training accuarcy: 0.6965\n",
      "Epoch 5 step 1355: training loss: 1382.1885106138848\n",
      "Epoch 5 step 1356: training accuarcy: 0.7045\n",
      "Epoch 5 step 1356: training loss: 1382.6595766707314\n",
      "Epoch 5 step 1357: training accuarcy: 0.7075\n",
      "Epoch 5 step 1357: training loss: 1382.8952865557783\n",
      "Epoch 5 step 1358: training accuarcy: 0.7000000000000001\n",
      "Epoch 5 step 1358: training loss: 1382.1975232020372\n",
      "Epoch 5 step 1359: training accuarcy: 0.715\n",
      "Epoch 5 step 1359: training loss: 1381.8930015839205\n",
      "Epoch 5 step 1360: training accuarcy: 0.7005\n",
      "Epoch 5 step 1360: training loss: 1382.340972063037\n",
      "Epoch 5 step 1361: training accuarcy: 0.7055\n",
      "Epoch 5 step 1361: training loss: 1383.0985831860758\n",
      "Epoch 5 step 1362: training accuarcy: 0.7135\n",
      "Epoch 5 step 1362: training loss: 1382.0056637603739\n",
      "Epoch 5 step 1363: training accuarcy: 0.6945\n",
      "Epoch 5 step 1363: training loss: 1382.4215144264044\n",
      "Epoch 5 step 1364: training accuarcy: 0.6975\n",
      "Epoch 5 step 1364: training loss: 1382.5113139176847\n",
      "Epoch 5 step 1365: training accuarcy: 0.6865\n",
      "Epoch 5 step 1365: training loss: 1381.642814729337\n",
      "Epoch 5 step 1366: training accuarcy: 0.7055\n",
      "Epoch 5 step 1366: training loss: 1382.5917927293306\n",
      "Epoch 5 step 1367: training accuarcy: 0.687\n",
      "Epoch 5 step 1367: training loss: 1382.2917283118215\n",
      "Epoch 5 step 1368: training accuarcy: 0.726\n",
      "Epoch 5 step 1368: training loss: 1382.3499836845936\n",
      "Epoch 5 step 1369: training accuarcy: 0.7025\n",
      "Epoch 5 step 1369: training loss: 1383.2470042162984\n",
      "Epoch 5 step 1370: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1370: training loss: 1382.5862368666772\n",
      "Epoch 5 step 1371: training accuarcy: 0.7145\n",
      "Epoch 5 step 1371: training loss: 1383.2817897537343\n",
      "Epoch 5 step 1372: training accuarcy: 0.6905\n",
      "Epoch 5 step 1372: training loss: 1383.0545011114632\n",
      "Epoch 5 step 1373: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1373: training loss: 1383.7393124020582\n",
      "Epoch 5 step 1374: training accuarcy: 0.6910000000000001\n",
      "Epoch 5 step 1374: training loss: 1382.5157015495215\n",
      "Epoch 5 step 1375: training accuarcy: 0.716\n",
      "Epoch 5 step 1375: training loss: 1381.7623287302397\n",
      "Epoch 5 step 1376: training accuarcy: 0.7115\n",
      "Epoch 5 step 1376: training loss: 1383.5676495273417\n",
      "Epoch 5 step 1377: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1377: training loss: 1382.9085356244964\n",
      "Epoch 5 step 1378: training accuarcy: 0.6995\n",
      "Epoch 5 step 1378: training loss: 1382.4580594385711\n",
      "Epoch 5 step 1379: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1379: training loss: 1382.3471386079045\n",
      "Epoch 5 step 1380: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1380: training loss: 1382.367609724157\n",
      "Epoch 5 step 1381: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1381: training loss: 1383.5819791992028\n",
      "Epoch 5 step 1382: training accuarcy: 0.6895\n",
      "Epoch 5 step 1382: training loss: 1382.014700899339\n",
      "Epoch 5 step 1383: training accuarcy: 0.7135\n",
      "Epoch 5 step 1383: training loss: 1384.0662074544318\n",
      "Epoch 5 step 1384: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1384: training loss: 1382.4797349159705\n",
      "Epoch 5 step 1385: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1385: training loss: 1382.276192638554\n",
      "Epoch 5 step 1386: training accuarcy: 0.7155\n",
      "Epoch 5 step 1386: training loss: 1382.3502760250592\n",
      "Epoch 5 step 1387: training accuarcy: 0.7045\n",
      "Epoch 5 step 1387: training loss: 1381.967949058758\n",
      "Epoch 5 step 1388: training accuarcy: 0.7185\n",
      "Epoch 5 step 1388: training loss: 1382.9239596730295\n",
      "Epoch 5 step 1389: training accuarcy: 0.7065\n",
      "Epoch 5 step 1389: training loss: 1382.9070524510403\n",
      "Epoch 5 step 1390: training accuarcy: 0.7035\n",
      "Epoch 5 step 1390: training loss: 1382.4031485377284\n",
      "Epoch 5 step 1391: training accuarcy: 0.7095\n",
      "Epoch 5 step 1391: training loss: 1382.9887058363029\n",
      "Epoch 5 step 1392: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1392: training loss: 1382.8549500473064\n",
      "Epoch 5 step 1393: training accuarcy: 0.687\n",
      "Epoch 5 step 1393: training loss: 1382.2470238201543\n",
      "Epoch 5 step 1394: training accuarcy: 0.6995\n",
      "Epoch 5 step 1394: training loss: 1382.8606774795512\n",
      "Epoch 5 step 1395: training accuarcy: 0.7020000000000001\n",
      "Epoch 5 step 1395: training loss: 1383.4043662530019\n",
      "Epoch 5 step 1396: training accuarcy: 0.6965\n",
      "Epoch 5 step 1396: training loss: 1382.5012266479446\n",
      "Epoch 5 step 1397: training accuarcy: 0.708\n",
      "Epoch 5 step 1397: training loss: 1382.8126362361938\n",
      "Epoch 5 step 1398: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1398: training loss: 1382.391627564742\n",
      "Epoch 5 step 1399: training accuarcy: 0.705\n",
      "Epoch 5 step 1399: training loss: 1383.3915142571245\n",
      "Epoch 5 step 1400: training accuarcy: 0.7045\n",
      "Epoch 5 step 1400: training loss: 1382.441820889064\n",
      "Epoch 5 step 1401: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1401: training loss: 1383.5258328182952\n",
      "Epoch 5 step 1402: training accuarcy: 0.7005\n",
      "Epoch 5 step 1402: training loss: 1382.9377967390399\n",
      "Epoch 5 step 1403: training accuarcy: 0.6935\n",
      "Epoch 5 step 1403: training loss: 1381.9188440805449\n",
      "Epoch 5 step 1404: training accuarcy: 0.712\n",
      "Epoch 5 step 1404: training loss: 1382.8926918317238\n",
      "Epoch 5 step 1405: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1405: training loss: 1383.3602291657726\n",
      "Epoch 5 step 1406: training accuarcy: 0.6945\n",
      "Epoch 5 step 1406: training loss: 1383.2008238190926\n",
      "Epoch 5 step 1407: training accuarcy: 0.7015\n",
      "Epoch 5 step 1407: training loss: 1382.7844434447159\n",
      "Epoch 5 step 1408: training accuarcy: 0.711\n",
      "Epoch 5 step 1408: training loss: 1382.6051478332458\n",
      "Epoch 5 step 1409: training accuarcy: 0.7065\n",
      "Epoch 5 step 1409: training loss: 1381.1059622534685\n",
      "Epoch 5 step 1410: training accuarcy: 0.716\n",
      "Epoch 5 step 1410: training loss: 1381.8871400209216\n",
      "Epoch 5 step 1411: training accuarcy: 0.7075\n",
      "Epoch 5 step 1411: training loss: 1382.9164714117474\n",
      "Epoch 5 step 1412: training accuarcy: 0.7045\n",
      "Epoch 5 step 1412: training loss: 1383.2101231663491\n",
      "Epoch 5 step 1413: training accuarcy: 0.6915\n",
      "Epoch 5 step 1413: training loss: 1383.0582316933987\n",
      "Epoch 5 step 1414: training accuarcy: 0.6955\n",
      "Epoch 5 step 1414: training loss: 1382.4925299457077\n",
      "Epoch 5 step 1415: training accuarcy: 0.7105\n",
      "Epoch 5 step 1415: training loss: 1382.7377140208496\n",
      "Epoch 5 step 1416: training accuarcy: 0.713\n",
      "Epoch 5 step 1416: training loss: 1383.526802301007\n",
      "Epoch 5 step 1417: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1417: training loss: 1382.7760600124411\n",
      "Epoch 5 step 1418: training accuarcy: 0.6855\n",
      "Epoch 5 step 1418: training loss: 1383.5280697862177\n",
      "Epoch 5 step 1419: training accuarcy: 0.6915\n",
      "Epoch 5 step 1419: training loss: 1383.5512790298924\n",
      "Epoch 5 step 1420: training accuarcy: 0.6950000000000001\n",
      "Epoch 5 step 1420: training loss: 1383.0258164036018\n",
      "Epoch 5 step 1421: training accuarcy: 0.6975\n",
      "Epoch 5 step 1421: training loss: 1383.3649494544936\n",
      "Epoch 5 step 1422: training accuarcy: 0.6965\n",
      "Epoch 5 step 1422: training loss: 1382.2027039083382\n",
      "Epoch 5 step 1423: training accuarcy: 0.7175\n",
      "Epoch 5 step 1423: training loss: 1383.507029707887\n",
      "Epoch 5 step 1424: training accuarcy: 0.6795\n",
      "Epoch 5 step 1424: training loss: 1383.6915459363454\n",
      "Epoch 5 step 1425: training accuarcy: 0.6885\n",
      "Epoch 5 step 1425: training loss: 1382.7983415428798\n",
      "Epoch 5 step 1426: training accuarcy: 0.704\n",
      "Epoch 5 step 1426: training loss: 1382.5784888202438\n",
      "Epoch 5 step 1427: training accuarcy: 0.7115\n",
      "Epoch 5 step 1427: training loss: 1382.6735001927896\n",
      "Epoch 5 step 1428: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1428: training loss: 1383.143476851463\n",
      "Epoch 5 step 1429: training accuarcy: 0.6915\n",
      "Epoch 5 step 1429: training loss: 1382.0723947590159\n",
      "Epoch 5 step 1430: training accuarcy: 0.727\n",
      "Epoch 5 step 1430: training loss: 1384.139090667186\n",
      "Epoch 5 step 1431: training accuarcy: 0.6825\n",
      "Epoch 5 step 1431: training loss: 1382.6835527360822\n",
      "Epoch 5 step 1432: training accuarcy: 0.686\n",
      "Epoch 5 step 1432: training loss: 1382.687121134065\n",
      "Epoch 5 step 1433: training accuarcy: 0.7005\n",
      "Epoch 5 step 1433: training loss: 1383.2058319533837\n",
      "Epoch 5 step 1434: training accuarcy: 0.6985\n",
      "Epoch 5 step 1434: training loss: 1381.6574095736173\n",
      "Epoch 5 step 1435: training accuarcy: 0.7125\n",
      "Epoch 5 step 1435: training loss: 1383.5830643310906\n",
      "Epoch 5 step 1436: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1436: training loss: 1383.2206082098876\n",
      "Epoch 5 step 1437: training accuarcy: 0.6945\n",
      "Epoch 5 step 1437: training loss: 1382.9644829193421\n",
      "Epoch 5 step 1438: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1438: training loss: 1383.1544585015354\n",
      "Epoch 5 step 1439: training accuarcy: 0.6865\n",
      "Epoch 5 step 1439: training loss: 1382.0913852594256\n",
      "Epoch 5 step 1440: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1440: training loss: 1383.1198256350408\n",
      "Epoch 5 step 1441: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1441: training loss: 1382.5270106987757\n",
      "Epoch 5 step 1442: training accuarcy: 0.7115\n",
      "Epoch 5 step 1442: training loss: 1382.9920785529316\n",
      "Epoch 5 step 1443: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1443: training loss: 1383.002153325405\n",
      "Epoch 5 step 1444: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1444: training loss: 1382.502709100123\n",
      "Epoch 5 step 1445: training accuarcy: 0.687\n",
      "Epoch 5 step 1445: training loss: 1382.4973770295362\n",
      "Epoch 5 step 1446: training accuarcy: 0.6805\n",
      "Epoch 5 step 1446: training loss: 1382.7969827901807\n",
      "Epoch 5 step 1447: training accuarcy: 0.7035\n",
      "Epoch 5 step 1447: training loss: 1383.6517270108716\n",
      "Epoch 5 step 1448: training accuarcy: 0.6835\n",
      "Epoch 5 step 1448: training loss: 1382.1039567751625\n",
      "Epoch 5 step 1449: training accuarcy: 0.7085\n",
      "Epoch 5 step 1449: training loss: 1383.395107285104\n",
      "Epoch 5 step 1450: training accuarcy: 0.683\n",
      "Epoch 5 step 1450: training loss: 1383.0567009354386\n",
      "Epoch 5 step 1451: training accuarcy: 0.683\n",
      "Epoch 5 step 1451: training loss: 1383.7683891798908\n",
      "Epoch 5 step 1452: training accuarcy: 0.6765\n",
      "Epoch 5 step 1452: training loss: 1382.4569574143316\n",
      "Epoch 5 step 1453: training accuarcy: 0.7035\n",
      "Epoch 5 step 1453: training loss: 1382.5800672410098\n",
      "Epoch 5 step 1454: training accuarcy: 0.7025\n",
      "Epoch 5 step 1454: training loss: 1382.2754698790895\n",
      "Epoch 5 step 1455: training accuarcy: 0.7035\n",
      "Epoch 5 step 1455: training loss: 1383.532889608138\n",
      "Epoch 5 step 1456: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1456: training loss: 1381.4004755581861\n",
      "Epoch 5 step 1457: training accuarcy: 0.712\n",
      "Epoch 5 step 1457: training loss: 1382.4081588589586\n",
      "Epoch 5 step 1458: training accuarcy: 0.71\n",
      "Epoch 5 step 1458: training loss: 1382.6435844331727\n",
      "Epoch 5 step 1459: training accuarcy: 0.7085\n",
      "Epoch 5 step 1459: training loss: 1382.4032846742507\n",
      "Epoch 5 step 1460: training accuarcy: 0.6955\n",
      "Epoch 5 step 1460: training loss: 1382.2496584988294\n",
      "Epoch 5 step 1461: training accuarcy: 0.7095\n",
      "Epoch 5 step 1461: training loss: 1383.2161649203197\n",
      "Epoch 5 step 1462: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1462: training loss: 1382.949123710178\n",
      "Epoch 5 step 1463: training accuarcy: 0.6925\n",
      "Epoch 5 step 1463: training loss: 1382.665128360635\n",
      "Epoch 5 step 1464: training accuarcy: 0.7045\n",
      "Epoch 5 step 1464: training loss: 1381.9745305281024\n",
      "Epoch 5 step 1465: training accuarcy: 0.709\n",
      "Epoch 5 step 1465: training loss: 1382.933640655328\n",
      "Epoch 5 step 1466: training accuarcy: 0.6920000000000001\n",
      "Epoch 5 step 1466: training loss: 1383.1969612013015\n",
      "Epoch 5 step 1467: training accuarcy: 0.6875\n",
      "Epoch 5 step 1467: training loss: 1382.5509886558416\n",
      "Epoch 5 step 1468: training accuarcy: 0.7035\n",
      "Epoch 5 step 1468: training loss: 1384.0078272877952\n",
      "Epoch 5 step 1469: training accuarcy: 0.6705\n",
      "Epoch 5 step 1469: training loss: 1382.8095877056628\n",
      "Epoch 5 step 1470: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1470: training loss: 1382.5209201799041\n",
      "Epoch 5 step 1471: training accuarcy: 0.7095\n",
      "Epoch 5 step 1471: training loss: 1383.3913102398662\n",
      "Epoch 5 step 1472: training accuarcy: 0.6985\n",
      "Epoch 5 step 1472: training loss: 1383.3066841297848\n",
      "Epoch 5 step 1473: training accuarcy: 0.6895\n",
      "Epoch 5 step 1473: training loss: 1382.9987023530723\n",
      "Epoch 5 step 1474: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1474: training loss: 1382.8309153225923\n",
      "Epoch 5 step 1475: training accuarcy: 0.707\n",
      "Epoch 5 step 1475: training loss: 1382.7883892306158\n",
      "Epoch 5 step 1476: training accuarcy: 0.6945\n",
      "Epoch 5 step 1476: training loss: 1383.712634597902\n",
      "Epoch 5 step 1477: training accuarcy: 0.6785\n",
      "Epoch 5 step 1477: training loss: 1382.1731872599025\n",
      "Epoch 5 step 1478: training accuarcy: 0.726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1478: training loss: 1382.9995514607767\n",
      "Epoch 5 step 1479: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1479: training loss: 1382.914911136799\n",
      "Epoch 5 step 1480: training accuarcy: 0.6815\n",
      "Epoch 5 step 1480: training loss: 1382.897355626293\n",
      "Epoch 5 step 1481: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1481: training loss: 1383.0658116648556\n",
      "Epoch 5 step 1482: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1482: training loss: 1382.016961750663\n",
      "Epoch 5 step 1483: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1483: training loss: 1383.6033826369344\n",
      "Epoch 5 step 1484: training accuarcy: 0.6865\n",
      "Epoch 5 step 1484: training loss: 1382.7465324346747\n",
      "Epoch 5 step 1485: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1485: training loss: 1382.7022507684408\n",
      "Epoch 5 step 1486: training accuarcy: 0.7055\n",
      "Epoch 5 step 1486: training loss: 1382.9449926222337\n",
      "Epoch 5 step 1487: training accuarcy: 0.6895\n",
      "Epoch 5 step 1487: training loss: 1383.5467038109782\n",
      "Epoch 5 step 1488: training accuarcy: 0.7065\n",
      "Epoch 5 step 1488: training loss: 1382.9110757047647\n",
      "Epoch 5 step 1489: training accuarcy: 0.7020000000000001\n",
      "Epoch 5 step 1489: training loss: 1383.0170378647206\n",
      "Epoch 5 step 1490: training accuarcy: 0.68\n",
      "Epoch 5 step 1490: training loss: 1383.6310011574108\n",
      "Epoch 5 step 1491: training accuarcy: 0.6845\n",
      "Epoch 5 step 1491: training loss: 1382.3506476263422\n",
      "Epoch 5 step 1492: training accuarcy: 0.6890000000000001\n",
      "Epoch 5 step 1492: training loss: 1383.1248238832789\n",
      "Epoch 5 step 1493: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1493: training loss: 1382.5681212410605\n",
      "Epoch 5 step 1494: training accuarcy: 0.705\n",
      "Epoch 5 step 1494: training loss: 1383.015297817909\n",
      "Epoch 5 step 1495: training accuarcy: 0.6910000000000001\n",
      "Epoch 5 step 1495: training loss: 1382.3674882453643\n",
      "Epoch 5 step 1496: training accuarcy: 0.7045\n",
      "Epoch 5 step 1496: training loss: 1382.5215627308878\n",
      "Epoch 5 step 1497: training accuarcy: 0.6905\n",
      "Epoch 5 step 1497: training loss: 1382.3113305135696\n",
      "Epoch 5 step 1498: training accuarcy: 0.7030000000000001\n",
      "Epoch 5 step 1498: training loss: 1383.4038016844056\n",
      "Epoch 5 step 1499: training accuarcy: 0.6925\n",
      "Epoch 5 step 1499: training loss: 1383.6470970211028\n",
      "Epoch 5 step 1500: training accuarcy: 0.6775\n",
      "Epoch 5 step 1500: training loss: 1382.3434216845606\n",
      "Epoch 5 step 1501: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1501: training loss: 1382.6353066682932\n",
      "Epoch 5 step 1502: training accuarcy: 0.713\n",
      "Epoch 5 step 1502: training loss: 1382.9335171595928\n",
      "Epoch 5 step 1503: training accuarcy: 0.6955\n",
      "Epoch 5 step 1503: training loss: 1383.2733187699803\n",
      "Epoch 5 step 1504: training accuarcy: 0.7025\n",
      "Epoch 5 step 1504: training loss: 1383.9975388219552\n",
      "Epoch 5 step 1505: training accuarcy: 0.676\n",
      "Epoch 5 step 1505: training loss: 1383.6715199901741\n",
      "Epoch 5 step 1506: training accuarcy: 0.6970000000000001\n",
      "Epoch 5 step 1506: training loss: 1381.8861172985758\n",
      "Epoch 5 step 1507: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1507: training loss: 1383.6867704362735\n",
      "Epoch 5 step 1508: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1508: training loss: 1382.5442233058764\n",
      "Epoch 5 step 1509: training accuarcy: 0.6995\n",
      "Epoch 5 step 1509: training loss: 1383.2393870401386\n",
      "Epoch 5 step 1510: training accuarcy: 0.684\n",
      "Epoch 5 step 1510: training loss: 1383.4421836518843\n",
      "Epoch 5 step 1511: training accuarcy: 0.7045\n",
      "Epoch 5 step 1511: training loss: 1383.049471679082\n",
      "Epoch 5 step 1512: training accuarcy: 0.706\n",
      "Epoch 5 step 1512: training loss: 1381.3009987709813\n",
      "Epoch 5 step 1513: training accuarcy: 0.7105\n",
      "Epoch 5 step 1513: training loss: 1384.4338233646415\n",
      "Epoch 5 step 1514: training accuarcy: 0.685\n",
      "Epoch 5 step 1514: training loss: 1382.4068682930651\n",
      "Epoch 5 step 1515: training accuarcy: 0.7010000000000001\n",
      "Epoch 5 step 1515: training loss: 1382.8388608407395\n",
      "Epoch 5 step 1516: training accuarcy: 0.708\n",
      "Epoch 5 step 1516: training loss: 1382.830058468603\n",
      "Epoch 5 step 1517: training accuarcy: 0.7000000000000001\n",
      "Epoch 5 step 1517: training loss: 1383.0562411497278\n",
      "Epoch 5 step 1518: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1518: training loss: 1382.7965428759517\n",
      "Epoch 5 step 1519: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1519: training loss: 1382.7786202014047\n",
      "Epoch 5 step 1520: training accuarcy: 0.686\n",
      "Epoch 5 step 1520: training loss: 1382.8167335564508\n",
      "Epoch 5 step 1521: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1521: training loss: 1382.7728069132713\n",
      "Epoch 5 step 1522: training accuarcy: 0.7045\n",
      "Epoch 5 step 1522: training loss: 1383.6685177501518\n",
      "Epoch 5 step 1523: training accuarcy: 0.6915\n",
      "Epoch 5 step 1523: training loss: 1383.2256417119263\n",
      "Epoch 5 step 1524: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1524: training loss: 1383.0736195663108\n",
      "Epoch 5 step 1525: training accuarcy: 0.6895\n",
      "Epoch 5 step 1525: training loss: 1383.2996919668806\n",
      "Epoch 5 step 1526: training accuarcy: 0.6910000000000001\n",
      "Epoch 5 step 1526: training loss: 1383.288430479522\n",
      "Epoch 5 step 1527: training accuarcy: 0.6895\n",
      "Epoch 5 step 1527: training loss: 1382.9997793883006\n",
      "Epoch 5 step 1528: training accuarcy: 0.6890000000000001\n",
      "Epoch 5 step 1528: training loss: 1382.003679603772\n",
      "Epoch 5 step 1529: training accuarcy: 0.7045\n",
      "Epoch 5 step 1529: training loss: 1383.0085267976274\n",
      "Epoch 5 step 1530: training accuarcy: 0.6990000000000001\n",
      "Epoch 5 step 1530: training loss: 1382.8151420265583\n",
      "Epoch 5 step 1531: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1531: training loss: 1383.6678459968593\n",
      "Epoch 5 step 1532: training accuarcy: 0.7155\n",
      "Epoch 5 step 1532: training loss: 1381.7529209499476\n",
      "Epoch 5 step 1533: training accuarcy: 0.7115\n",
      "Epoch 5 step 1533: training loss: 1383.862016891811\n",
      "Epoch 5 step 1534: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1534: training loss: 1383.6277466192676\n",
      "Epoch 5 step 1535: training accuarcy: 0.684\n",
      "Epoch 5 step 1535: training loss: 1383.0588501807706\n",
      "Epoch 5 step 1536: training accuarcy: 0.6885\n",
      "Epoch 5 step 1536: training loss: 1383.7543380499953\n",
      "Epoch 5 step 1537: training accuarcy: 0.681\n",
      "Epoch 5 step 1537: training loss: 1383.2111636137508\n",
      "Epoch 5 step 1538: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1538: training loss: 1382.6757958796263\n",
      "Epoch 5 step 1539: training accuarcy: 0.683\n",
      "Epoch 5 step 1539: training loss: 1383.0631808188296\n",
      "Epoch 5 step 1540: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1540: training loss: 1383.0813097048263\n",
      "Epoch 5 step 1541: training accuarcy: 0.6950000000000001\n",
      "Epoch 5 step 1541: training loss: 1383.248999479072\n",
      "Epoch 5 step 1542: training accuarcy: 0.6985\n",
      "Epoch 5 step 1542: training loss: 1382.9840239763691\n",
      "Epoch 5 step 1543: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1543: training loss: 1383.1787891304423\n",
      "Epoch 5 step 1544: training accuarcy: 0.6865\n",
      "Epoch 5 step 1544: training loss: 1383.4672694255155\n",
      "Epoch 5 step 1545: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1545: training loss: 1384.8951293287478\n",
      "Epoch 5 step 1546: training accuarcy: 0.6795\n",
      "Epoch 5 step 1546: training loss: 1381.5036152808295\n",
      "Epoch 5 step 1547: training accuarcy: 0.7165\n",
      "Epoch 5 step 1547: training loss: 1382.9546264105738\n",
      "Epoch 5 step 1548: training accuarcy: 0.7015\n",
      "Epoch 5 step 1548: training loss: 1383.1797442835527\n",
      "Epoch 5 step 1549: training accuarcy: 0.672\n",
      "Epoch 5 step 1549: training loss: 1382.2864824165054\n",
      "Epoch 5 step 1550: training accuarcy: 0.6925\n",
      "Epoch 5 step 1550: training loss: 1382.0808482960047\n",
      "Epoch 5 step 1551: training accuarcy: 0.7095\n",
      "Epoch 5 step 1551: training loss: 1383.0447850791447\n",
      "Epoch 5 step 1552: training accuarcy: 0.6930000000000001\n",
      "Epoch 5 step 1552: training loss: 1384.007826511675\n",
      "Epoch 5 step 1553: training accuarcy: 0.6845\n",
      "Epoch 5 step 1553: training loss: 1382.9157205449835\n",
      "Epoch 5 step 1554: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1554: training loss: 1383.038237839451\n",
      "Epoch 5 step 1555: training accuarcy: 0.7025\n",
      "Epoch 5 step 1555: training loss: 1382.311694300821\n",
      "Epoch 5 step 1556: training accuarcy: 0.6955\n",
      "Epoch 5 step 1556: training loss: 1382.9184849498251\n",
      "Epoch 5 step 1557: training accuarcy: 0.6950000000000001\n",
      "Epoch 5 step 1557: training loss: 1381.8293015258707\n",
      "Epoch 5 step 1558: training accuarcy: 0.6980000000000001\n",
      "Epoch 5 step 1558: training loss: 1383.0066986680454\n",
      "Epoch 5 step 1559: training accuarcy: 0.6915\n",
      "Epoch 5 step 1559: training loss: 1382.1709518592154\n",
      "Epoch 5 step 1560: training accuarcy: 0.717\n",
      "Epoch 5 step 1560: training loss: 1383.010156249655\n",
      "Epoch 5 step 1561: training accuarcy: 0.6900000000000001\n",
      "Epoch 5 step 1561: training loss: 1383.0715730022264\n",
      "Epoch 5 step 1562: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1562: training loss: 1382.89232141381\n",
      "Epoch 5 step 1563: training accuarcy: 0.6935\n",
      "Epoch 5 step 1563: training loss: 1382.9613326499905\n",
      "Epoch 5 step 1564: training accuarcy: 0.6880000000000001\n",
      "Epoch 5 step 1564: training loss: 1382.5014619569124\n",
      "Epoch 5 step 1565: training accuarcy: 0.6940000000000001\n",
      "Epoch 5 step 1565: training loss: 1382.7205394417886\n",
      "Epoch 5 step 1566: training accuarcy: 0.711\n",
      "Epoch 5 step 1566: training loss: 1382.7197085007454\n",
      "Epoch 5 step 1567: training accuarcy: 0.7135\n",
      "Epoch 5 step 1567: training loss: 1383.3869820178697\n",
      "Epoch 5 step 1568: training accuarcy: 0.6855\n",
      "Epoch 5 step 1568: training loss: 1383.3667229475143\n",
      "Epoch 5 step 1569: training accuarcy: 0.6960000000000001\n",
      "Epoch 5 step 1569: training loss: 1382.3542842302745\n",
      "Epoch 5 step 1570: training accuarcy: 0.7075\n",
      "Epoch 5 step 1570: training loss: 1384.0596020185553\n",
      "Epoch 5 step 1571: training accuarcy: 0.68\n",
      "Epoch 5 step 1571: training loss: 1382.7819025439826\n",
      "Epoch 5 step 1572: training accuarcy: 0.6865\n",
      "Epoch 5 step 1572: training loss: 1382.3503130588003\n",
      "Epoch 5 step 1573: training accuarcy: 0.6945\n",
      "Epoch 5 step 1573: training loss: 1383.5837746015932\n",
      "Epoch 5 step 1574: training accuarcy: 0.704\n",
      "Epoch 5 step 1574: training loss: 1381.7941270700296\n",
      "Epoch 5 step 1575: training accuarcy: 0.7105\n",
      "Epoch 5 step 1575: training loss: 1383.864787320412\n",
      "Epoch 5 step 1576: training accuarcy: 0.687\n",
      "Epoch 5 step 1576: training loss: 1382.5587172623423\n",
      "Epoch 5 step 1577: training accuarcy: 0.709\n",
      "Epoch 5 step 1577: training loss: 542.6906170146822\n",
      "Epoch 5 step 1578: training accuarcy: 0.6974358974358974\n",
      "Epoch 5: train loss 1379.5859887868876, train accuarcy 0.7022712826728821\n",
      "Epoch 5: valid loss 1363.8610506653429, valid accuarcy 0.7128562927246094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                      | 6/8 [11:58<04:04, 122.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 1578: training loss: 1381.9675770691683\n",
      "Epoch 6 step 1579: training accuarcy: 0.712\n",
      "Epoch 6 step 1579: training loss: 1382.0272766838964\n",
      "Epoch 6 step 1580: training accuarcy: 0.715\n",
      "Epoch 6 step 1580: training loss: 1381.6563032627062\n",
      "Epoch 6 step 1581: training accuarcy: 0.723\n",
      "Epoch 6 step 1581: training loss: 1381.134598980179\n",
      "Epoch 6 step 1582: training accuarcy: 0.724\n",
      "Epoch 6 step 1582: training loss: 1382.3083225159285\n",
      "Epoch 6 step 1583: training accuarcy: 0.7145\n",
      "Epoch 6 step 1583: training loss: 1380.7826923964485\n",
      "Epoch 6 step 1584: training accuarcy: 0.728\n",
      "Epoch 6 step 1584: training loss: 1381.892443708029\n",
      "Epoch 6 step 1585: training accuarcy: 0.7385\n",
      "Epoch 6 step 1585: training loss: 1381.9904319996722\n",
      "Epoch 6 step 1586: training accuarcy: 0.7115\n",
      "Epoch 6 step 1586: training loss: 1382.297759629386\n",
      "Epoch 6 step 1587: training accuarcy: 0.7255\n",
      "Epoch 6 step 1587: training loss: 1381.7433709642182\n",
      "Epoch 6 step 1588: training accuarcy: 0.713\n",
      "Epoch 6 step 1588: training loss: 1382.1604648000466\n",
      "Epoch 6 step 1589: training accuarcy: 0.713\n",
      "Epoch 6 step 1589: training loss: 1382.511790612164\n",
      "Epoch 6 step 1590: training accuarcy: 0.7075\n",
      "Epoch 6 step 1590: training loss: 1381.4905722388544\n",
      "Epoch 6 step 1591: training accuarcy: 0.7175\n",
      "Epoch 6 step 1591: training loss: 1382.275203931083\n",
      "Epoch 6 step 1592: training accuarcy: 0.7155\n",
      "Epoch 6 step 1592: training loss: 1383.546969341364\n",
      "Epoch 6 step 1593: training accuarcy: 0.7085\n",
      "Epoch 6 step 1593: training loss: 1382.5285730494704\n",
      "Epoch 6 step 1594: training accuarcy: 0.7045\n",
      "Epoch 6 step 1594: training loss: 1382.7431361004037\n",
      "Epoch 6 step 1595: training accuarcy: 0.6935\n",
      "Epoch 6 step 1595: training loss: 1382.0569092672126\n",
      "Epoch 6 step 1596: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1596: training loss: 1381.7993406862001\n",
      "Epoch 6 step 1597: training accuarcy: 0.716\n",
      "Epoch 6 step 1597: training loss: 1382.587739026875\n",
      "Epoch 6 step 1598: training accuarcy: 0.7165\n",
      "Epoch 6 step 1598: training loss: 1382.170754155124\n",
      "Epoch 6 step 1599: training accuarcy: 0.71\n",
      "Epoch 6 step 1599: training loss: 1382.9195702010593\n",
      "Epoch 6 step 1600: training accuarcy: 0.7085\n",
      "Epoch 6 step 1600: training loss: 1381.7025425376307\n",
      "Epoch 6 step 1601: training accuarcy: 0.7185\n",
      "Epoch 6 step 1601: training loss: 1382.591333151828\n",
      "Epoch 6 step 1602: training accuarcy: 0.72\n",
      "Epoch 6 step 1602: training loss: 1382.09542787208\n",
      "Epoch 6 step 1603: training accuarcy: 0.7225\n",
      "Epoch 6 step 1603: training loss: 1382.755982586662\n",
      "Epoch 6 step 1604: training accuarcy: 0.7215\n",
      "Epoch 6 step 1604: training loss: 1382.3125832773853\n",
      "Epoch 6 step 1605: training accuarcy: 0.704\n",
      "Epoch 6 step 1605: training loss: 1383.3175496430101\n",
      "Epoch 6 step 1606: training accuarcy: 0.7065\n",
      "Epoch 6 step 1606: training loss: 1382.6072957771564\n",
      "Epoch 6 step 1607: training accuarcy: 0.6975\n",
      "Epoch 6 step 1607: training loss: 1381.9863832403403\n",
      "Epoch 6 step 1608: training accuarcy: 0.6935\n",
      "Epoch 6 step 1608: training loss: 1382.564633362234\n",
      "Epoch 6 step 1609: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1609: training loss: 1382.6830397390936\n",
      "Epoch 6 step 1610: training accuarcy: 0.7015\n",
      "Epoch 6 step 1610: training loss: 1383.2709651962393\n",
      "Epoch 6 step 1611: training accuarcy: 0.7005\n",
      "Epoch 6 step 1611: training loss: 1384.6355372173055\n",
      "Epoch 6 step 1612: training accuarcy: 0.6755\n",
      "Epoch 6 step 1612: training loss: 1382.290494184569\n",
      "Epoch 6 step 1613: training accuarcy: 0.71\n",
      "Epoch 6 step 1613: training loss: 1383.3439664590176\n",
      "Epoch 6 step 1614: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1614: training loss: 1382.774308503921\n",
      "Epoch 6 step 1615: training accuarcy: 0.7015\n",
      "Epoch 6 step 1615: training loss: 1381.809342148173\n",
      "Epoch 6 step 1616: training accuarcy: 0.712\n",
      "Epoch 6 step 1616: training loss: 1382.7985144999143\n",
      "Epoch 6 step 1617: training accuarcy: 0.6965\n",
      "Epoch 6 step 1617: training loss: 1381.5232688939648\n",
      "Epoch 6 step 1618: training accuarcy: 0.7195\n",
      "Epoch 6 step 1618: training loss: 1383.0455208794474\n",
      "Epoch 6 step 1619: training accuarcy: 0.7025\n",
      "Epoch 6 step 1619: training loss: 1382.2003748852287\n",
      "Epoch 6 step 1620: training accuarcy: 0.7155\n",
      "Epoch 6 step 1620: training loss: 1382.494334265082\n",
      "Epoch 6 step 1621: training accuarcy: 0.707\n",
      "Epoch 6 step 1621: training loss: 1382.666792327653\n",
      "Epoch 6 step 1622: training accuarcy: 0.711\n",
      "Epoch 6 step 1622: training loss: 1382.6381638664566\n",
      "Epoch 6 step 1623: training accuarcy: 0.7025\n",
      "Epoch 6 step 1623: training loss: 1383.149133092724\n",
      "Epoch 6 step 1624: training accuarcy: 0.7005\n",
      "Epoch 6 step 1624: training loss: 1382.1897127764837\n",
      "Epoch 6 step 1625: training accuarcy: 0.6910000000000001\n",
      "Epoch 6 step 1625: training loss: 1382.3803415728055\n",
      "Epoch 6 step 1626: training accuarcy: 0.708\n",
      "Epoch 6 step 1626: training loss: 1381.6357289376892\n",
      "Epoch 6 step 1627: training accuarcy: 0.713\n",
      "Epoch 6 step 1627: training loss: 1382.916535001895\n",
      "Epoch 6 step 1628: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1628: training loss: 1381.8539534547945\n",
      "Epoch 6 step 1629: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1629: training loss: 1381.9342073855562\n",
      "Epoch 6 step 1630: training accuarcy: 0.708\n",
      "Epoch 6 step 1630: training loss: 1382.5489044210092\n",
      "Epoch 6 step 1631: training accuarcy: 0.6985\n",
      "Epoch 6 step 1631: training loss: 1382.7504483996593\n",
      "Epoch 6 step 1632: training accuarcy: 0.716\n",
      "Epoch 6 step 1632: training loss: 1382.6578169915647\n",
      "Epoch 6 step 1633: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1633: training loss: 1383.5417490390873\n",
      "Epoch 6 step 1634: training accuarcy: 0.6995\n",
      "Epoch 6 step 1634: training loss: 1382.509568207115\n",
      "Epoch 6 step 1635: training accuarcy: 0.6925\n",
      "Epoch 6 step 1635: training loss: 1383.9173597948115\n",
      "Epoch 6 step 1636: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1636: training loss: 1382.3536859067956\n",
      "Epoch 6 step 1637: training accuarcy: 0.6945\n",
      "Epoch 6 step 1637: training loss: 1383.6165499506076\n",
      "Epoch 6 step 1638: training accuarcy: 0.677\n",
      "Epoch 6 step 1638: training loss: 1380.8409978767436\n",
      "Epoch 6 step 1639: training accuarcy: 0.7115\n",
      "Epoch 6 step 1639: training loss: 1382.73524772723\n",
      "Epoch 6 step 1640: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1640: training loss: 1382.8993694354795\n",
      "Epoch 6 step 1641: training accuarcy: 0.7085\n",
      "Epoch 6 step 1641: training loss: 1382.3444044578519\n",
      "Epoch 6 step 1642: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1642: training loss: 1382.90295766026\n",
      "Epoch 6 step 1643: training accuarcy: 0.7065\n",
      "Epoch 6 step 1643: training loss: 1381.7096254305088\n",
      "Epoch 6 step 1644: training accuarcy: 0.7155\n",
      "Epoch 6 step 1644: training loss: 1383.3856893283037\n",
      "Epoch 6 step 1645: training accuarcy: 0.6855\n",
      "Epoch 6 step 1645: training loss: 1382.1270620968803\n",
      "Epoch 6 step 1646: training accuarcy: 0.714\n",
      "Epoch 6 step 1646: training loss: 1381.4240574491716\n",
      "Epoch 6 step 1647: training accuarcy: 0.7175\n",
      "Epoch 6 step 1647: training loss: 1382.5350147237275\n",
      "Epoch 6 step 1648: training accuarcy: 0.6975\n",
      "Epoch 6 step 1648: training loss: 1382.1326053092937\n",
      "Epoch 6 step 1649: training accuarcy: 0.714\n",
      "Epoch 6 step 1649: training loss: 1382.6752641107134\n",
      "Epoch 6 step 1650: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1650: training loss: 1382.758718567189\n",
      "Epoch 6 step 1651: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1651: training loss: 1383.2755646978499\n",
      "Epoch 6 step 1652: training accuarcy: 0.6975\n",
      "Epoch 6 step 1652: training loss: 1382.9898136191412\n",
      "Epoch 6 step 1653: training accuarcy: 0.6995\n",
      "Epoch 6 step 1653: training loss: 1382.0708966989598\n",
      "Epoch 6 step 1654: training accuarcy: 0.708\n",
      "Epoch 6 step 1654: training loss: 1383.5214567762675\n",
      "Epoch 6 step 1655: training accuarcy: 0.6935\n",
      "Epoch 6 step 1655: training loss: 1383.4971984403621\n",
      "Epoch 6 step 1656: training accuarcy: 0.6845\n",
      "Epoch 6 step 1656: training loss: 1382.3422455431794\n",
      "Epoch 6 step 1657: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1657: training loss: 1382.2894165083485\n",
      "Epoch 6 step 1658: training accuarcy: 0.7225\n",
      "Epoch 6 step 1658: training loss: 1383.1340629353924\n",
      "Epoch 6 step 1659: training accuarcy: 0.6925\n",
      "Epoch 6 step 1659: training loss: 1382.3521395997811\n",
      "Epoch 6 step 1660: training accuarcy: 0.6985\n",
      "Epoch 6 step 1660: training loss: 1382.513903374078\n",
      "Epoch 6 step 1661: training accuarcy: 0.706\n",
      "Epoch 6 step 1661: training loss: 1382.1241602869293\n",
      "Epoch 6 step 1662: training accuarcy: 0.7015\n",
      "Epoch 6 step 1662: training loss: 1382.8935729634572\n",
      "Epoch 6 step 1663: training accuarcy: 0.7075\n",
      "Epoch 6 step 1663: training loss: 1383.4001257190487\n",
      "Epoch 6 step 1664: training accuarcy: 0.7035\n",
      "Epoch 6 step 1664: training loss: 1383.573750430798\n",
      "Epoch 6 step 1665: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1665: training loss: 1383.2886481336964\n",
      "Epoch 6 step 1666: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1666: training loss: 1383.1548836876104\n",
      "Epoch 6 step 1667: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1667: training loss: 1381.9606028417168\n",
      "Epoch 6 step 1668: training accuarcy: 0.7045\n",
      "Epoch 6 step 1668: training loss: 1383.0989607509284\n",
      "Epoch 6 step 1669: training accuarcy: 0.6995\n",
      "Epoch 6 step 1669: training loss: 1382.4265137468155\n",
      "Epoch 6 step 1670: training accuarcy: 0.7025\n",
      "Epoch 6 step 1670: training loss: 1382.797340926649\n",
      "Epoch 6 step 1671: training accuarcy: 0.7025\n",
      "Epoch 6 step 1671: training loss: 1382.9063088904963\n",
      "Epoch 6 step 1672: training accuarcy: 0.7045\n",
      "Epoch 6 step 1672: training loss: 1382.534678753206\n",
      "Epoch 6 step 1673: training accuarcy: 0.7045\n",
      "Epoch 6 step 1673: training loss: 1383.1814082593503\n",
      "Epoch 6 step 1674: training accuarcy: 0.7075\n",
      "Epoch 6 step 1674: training loss: 1383.5722310814422\n",
      "Epoch 6 step 1675: training accuarcy: 0.6915\n",
      "Epoch 6 step 1675: training loss: 1383.3721403437046\n",
      "Epoch 6 step 1676: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1676: training loss: 1382.5232186198866\n",
      "Epoch 6 step 1677: training accuarcy: 0.705\n",
      "Epoch 6 step 1677: training loss: 1382.5562125582428\n",
      "Epoch 6 step 1678: training accuarcy: 0.707\n",
      "Epoch 6 step 1678: training loss: 1382.8631997653024\n",
      "Epoch 6 step 1679: training accuarcy: 0.7015\n",
      "Epoch 6 step 1679: training loss: 1382.8742263332613\n",
      "Epoch 6 step 1680: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1680: training loss: 1383.413527106638\n",
      "Epoch 6 step 1681: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1681: training loss: 1383.453876416113\n",
      "Epoch 6 step 1682: training accuarcy: 0.6865\n",
      "Epoch 6 step 1682: training loss: 1382.4975031667311\n",
      "Epoch 6 step 1683: training accuarcy: 0.706\n",
      "Epoch 6 step 1683: training loss: 1382.717652903344\n",
      "Epoch 6 step 1684: training accuarcy: 0.7135\n",
      "Epoch 6 step 1684: training loss: 1383.487031033267\n",
      "Epoch 6 step 1685: training accuarcy: 0.6890000000000001\n",
      "Epoch 6 step 1685: training loss: 1383.3624193203948\n",
      "Epoch 6 step 1686: training accuarcy: 0.684\n",
      "Epoch 6 step 1686: training loss: 1382.710407958306\n",
      "Epoch 6 step 1687: training accuarcy: 0.704\n",
      "Epoch 6 step 1687: training loss: 1383.9572697041933\n",
      "Epoch 6 step 1688: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1688: training loss: 1381.6633207946516\n",
      "Epoch 6 step 1689: training accuarcy: 0.71\n",
      "Epoch 6 step 1689: training loss: 1382.619039680641\n",
      "Epoch 6 step 1690: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1690: training loss: 1382.6465293343122\n",
      "Epoch 6 step 1691: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1691: training loss: 1382.4667138541988\n",
      "Epoch 6 step 1692: training accuarcy: 0.7065\n",
      "Epoch 6 step 1692: training loss: 1382.8178259174267\n",
      "Epoch 6 step 1693: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1693: training loss: 1382.541676782104\n",
      "Epoch 6 step 1694: training accuarcy: 0.714\n",
      "Epoch 6 step 1694: training loss: 1383.5483028907663\n",
      "Epoch 6 step 1695: training accuarcy: 0.6925\n",
      "Epoch 6 step 1695: training loss: 1382.9456047396104\n",
      "Epoch 6 step 1696: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1696: training loss: 1382.709889820627\n",
      "Epoch 6 step 1697: training accuarcy: 0.7155\n",
      "Epoch 6 step 1697: training loss: 1382.2885841296609\n",
      "Epoch 6 step 1698: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1698: training loss: 1383.5846912655975\n",
      "Epoch 6 step 1699: training accuarcy: 0.708\n",
      "Epoch 6 step 1699: training loss: 1382.6043613848537\n",
      "Epoch 6 step 1700: training accuarcy: 0.7085\n",
      "Epoch 6 step 1700: training loss: 1381.9617386987165\n",
      "Epoch 6 step 1701: training accuarcy: 0.705\n",
      "Epoch 6 step 1701: training loss: 1382.2391445050666\n",
      "Epoch 6 step 1702: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1702: training loss: 1382.7898408086833\n",
      "Epoch 6 step 1703: training accuarcy: 0.706\n",
      "Epoch 6 step 1703: training loss: 1383.0171864517733\n",
      "Epoch 6 step 1704: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1704: training loss: 1383.5056892474895\n",
      "Epoch 6 step 1705: training accuarcy: 0.7095\n",
      "Epoch 6 step 1705: training loss: 1383.5226120873851\n",
      "Epoch 6 step 1706: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1706: training loss: 1383.2614203809912\n",
      "Epoch 6 step 1707: training accuarcy: 0.7015\n",
      "Epoch 6 step 1707: training loss: 1383.0378681912596\n",
      "Epoch 6 step 1708: training accuarcy: 0.6985\n",
      "Epoch 6 step 1708: training loss: 1382.95414933248\n",
      "Epoch 6 step 1709: training accuarcy: 0.7025\n",
      "Epoch 6 step 1709: training loss: 1383.6187523086917\n",
      "Epoch 6 step 1710: training accuarcy: 0.6845\n",
      "Epoch 6 step 1710: training loss: 1383.5906566268543\n",
      "Epoch 6 step 1711: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1711: training loss: 1382.939851275537\n",
      "Epoch 6 step 1712: training accuarcy: 0.7005\n",
      "Epoch 6 step 1712: training loss: 1381.8337870581147\n",
      "Epoch 6 step 1713: training accuarcy: 0.708\n",
      "Epoch 6 step 1713: training loss: 1383.1155823138067\n",
      "Epoch 6 step 1714: training accuarcy: 0.6895\n",
      "Epoch 6 step 1714: training loss: 1381.6948399788967\n",
      "Epoch 6 step 1715: training accuarcy: 0.713\n",
      "Epoch 6 step 1715: training loss: 1383.1216591607174\n",
      "Epoch 6 step 1716: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1716: training loss: 1382.0895650157274\n",
      "Epoch 6 step 1717: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1717: training loss: 1382.805164204408\n",
      "Epoch 6 step 1718: training accuarcy: 0.711\n",
      "Epoch 6 step 1718: training loss: 1382.0969232852483\n",
      "Epoch 6 step 1719: training accuarcy: 0.711\n",
      "Epoch 6 step 1719: training loss: 1382.9066911229577\n",
      "Epoch 6 step 1720: training accuarcy: 0.6855\n",
      "Epoch 6 step 1720: training loss: 1381.390194589318\n",
      "Epoch 6 step 1721: training accuarcy: 0.707\n",
      "Epoch 6 step 1721: training loss: 1383.3631665788034\n",
      "Epoch 6 step 1722: training accuarcy: 0.6805\n",
      "Epoch 6 step 1722: training loss: 1381.8292994625208\n",
      "Epoch 6 step 1723: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1723: training loss: 1382.8312706356057\n",
      "Epoch 6 step 1724: training accuarcy: 0.708\n",
      "Epoch 6 step 1724: training loss: 1381.9144336205245\n",
      "Epoch 6 step 1725: training accuarcy: 0.708\n",
      "Epoch 6 step 1725: training loss: 1382.5919551755944\n",
      "Epoch 6 step 1726: training accuarcy: 0.6885\n",
      "Epoch 6 step 1726: training loss: 1383.634995996167\n",
      "Epoch 6 step 1727: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1727: training loss: 1382.2005613100512\n",
      "Epoch 6 step 1728: training accuarcy: 0.709\n",
      "Epoch 6 step 1728: training loss: 1381.66859189788\n",
      "Epoch 6 step 1729: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1729: training loss: 1383.7381491991239\n",
      "Epoch 6 step 1730: training accuarcy: 0.6945\n",
      "Epoch 6 step 1730: training loss: 1382.359733938593\n",
      "Epoch 6 step 1731: training accuarcy: 0.722\n",
      "Epoch 6 step 1731: training loss: 1383.5919702652664\n",
      "Epoch 6 step 1732: training accuarcy: 0.684\n",
      "Epoch 6 step 1732: training loss: 1383.488562227385\n",
      "Epoch 6 step 1733: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1733: training loss: 1382.864103666358\n",
      "Epoch 6 step 1734: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1734: training loss: 1383.2511250029715\n",
      "Epoch 6 step 1735: training accuarcy: 0.6975\n",
      "Epoch 6 step 1735: training loss: 1382.5647346326239\n",
      "Epoch 6 step 1736: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1736: training loss: 1383.1639511379005\n",
      "Epoch 6 step 1737: training accuarcy: 0.706\n",
      "Epoch 6 step 1737: training loss: 1381.845857446796\n",
      "Epoch 6 step 1738: training accuarcy: 0.7085\n",
      "Epoch 6 step 1738: training loss: 1383.8571630615072\n",
      "Epoch 6 step 1739: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1739: training loss: 1382.7050727324508\n",
      "Epoch 6 step 1740: training accuarcy: 0.6955\n",
      "Epoch 6 step 1740: training loss: 1382.9146916228913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1741: training accuarcy: 0.6955\n",
      "Epoch 6 step 1741: training loss: 1382.666794818975\n",
      "Epoch 6 step 1742: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1742: training loss: 1382.29850806044\n",
      "Epoch 6 step 1743: training accuarcy: 0.7045\n",
      "Epoch 6 step 1743: training loss: 1383.816248352409\n",
      "Epoch 6 step 1744: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1744: training loss: 1383.292257358835\n",
      "Epoch 6 step 1745: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1745: training loss: 1382.367831435093\n",
      "Epoch 6 step 1746: training accuarcy: 0.711\n",
      "Epoch 6 step 1746: training loss: 1383.8754677500765\n",
      "Epoch 6 step 1747: training accuarcy: 0.6845\n",
      "Epoch 6 step 1747: training loss: 1382.769616835386\n",
      "Epoch 6 step 1748: training accuarcy: 0.7030000000000001\n",
      "Epoch 6 step 1748: training loss: 1382.6848318598868\n",
      "Epoch 6 step 1749: training accuarcy: 0.6845\n",
      "Epoch 6 step 1749: training loss: 1383.3193289278927\n",
      "Epoch 6 step 1750: training accuarcy: 0.6890000000000001\n",
      "Epoch 6 step 1750: training loss: 1382.8285402801152\n",
      "Epoch 6 step 1751: training accuarcy: 0.7005\n",
      "Epoch 6 step 1751: training loss: 1383.6699847304399\n",
      "Epoch 6 step 1752: training accuarcy: 0.685\n",
      "Epoch 6 step 1752: training loss: 1383.3840737245641\n",
      "Epoch 6 step 1753: training accuarcy: 0.6955\n",
      "Epoch 6 step 1753: training loss: 1382.177050260439\n",
      "Epoch 6 step 1754: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1754: training loss: 1382.130112507747\n",
      "Epoch 6 step 1755: training accuarcy: 0.7025\n",
      "Epoch 6 step 1755: training loss: 1383.8764599508384\n",
      "Epoch 6 step 1756: training accuarcy: 0.7000000000000001\n",
      "Epoch 6 step 1756: training loss: 1382.470811107652\n",
      "Epoch 6 step 1757: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1757: training loss: 1382.5538925071357\n",
      "Epoch 6 step 1758: training accuarcy: 0.707\n",
      "Epoch 6 step 1758: training loss: 1382.4278056891008\n",
      "Epoch 6 step 1759: training accuarcy: 0.7035\n",
      "Epoch 6 step 1759: training loss: 1383.0420372711778\n",
      "Epoch 6 step 1760: training accuarcy: 0.686\n",
      "Epoch 6 step 1760: training loss: 1383.1775317906963\n",
      "Epoch 6 step 1761: training accuarcy: 0.687\n",
      "Epoch 6 step 1761: training loss: 1383.576735381077\n",
      "Epoch 6 step 1762: training accuarcy: 0.676\n",
      "Epoch 6 step 1762: training loss: 1383.4171166746041\n",
      "Epoch 6 step 1763: training accuarcy: 0.687\n",
      "Epoch 6 step 1763: training loss: 1382.7280811650119\n",
      "Epoch 6 step 1764: training accuarcy: 0.711\n",
      "Epoch 6 step 1764: training loss: 1383.3871876264002\n",
      "Epoch 6 step 1765: training accuarcy: 0.6905\n",
      "Epoch 6 step 1765: training loss: 1383.1165103570631\n",
      "Epoch 6 step 1766: training accuarcy: 0.6925\n",
      "Epoch 6 step 1766: training loss: 1383.136502256417\n",
      "Epoch 6 step 1767: training accuarcy: 0.6995\n",
      "Epoch 6 step 1767: training loss: 1383.5336711094942\n",
      "Epoch 6 step 1768: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1768: training loss: 1382.398765736321\n",
      "Epoch 6 step 1769: training accuarcy: 0.7065\n",
      "Epoch 6 step 1769: training loss: 1383.4535150928314\n",
      "Epoch 6 step 1770: training accuarcy: 0.6970000000000001\n",
      "Epoch 6 step 1770: training loss: 1383.766207150462\n",
      "Epoch 6 step 1771: training accuarcy: 0.6925\n",
      "Epoch 6 step 1771: training loss: 1382.4899964084616\n",
      "Epoch 6 step 1772: training accuarcy: 0.705\n",
      "Epoch 6 step 1772: training loss: 1382.2003744110511\n",
      "Epoch 6 step 1773: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1773: training loss: 1382.973767523179\n",
      "Epoch 6 step 1774: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1774: training loss: 1383.872338359432\n",
      "Epoch 6 step 1775: training accuarcy: 0.6785\n",
      "Epoch 6 step 1775: training loss: 1382.947365569849\n",
      "Epoch 6 step 1776: training accuarcy: 0.6965\n",
      "Epoch 6 step 1776: training loss: 1382.7607130690371\n",
      "Epoch 6 step 1777: training accuarcy: 0.712\n",
      "Epoch 6 step 1777: training loss: 1382.7787482759838\n",
      "Epoch 6 step 1778: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1778: training loss: 1382.8790991885758\n",
      "Epoch 6 step 1779: training accuarcy: 0.6945\n",
      "Epoch 6 step 1779: training loss: 1383.2227513380722\n",
      "Epoch 6 step 1780: training accuarcy: 0.7005\n",
      "Epoch 6 step 1780: training loss: 1382.3641958347814\n",
      "Epoch 6 step 1781: training accuarcy: 0.6805\n",
      "Epoch 6 step 1781: training loss: 1383.0780949219434\n",
      "Epoch 6 step 1782: training accuarcy: 0.6775\n",
      "Epoch 6 step 1782: training loss: 1382.8186963676703\n",
      "Epoch 6 step 1783: training accuarcy: 0.6905\n",
      "Epoch 6 step 1783: training loss: 1383.1011116281654\n",
      "Epoch 6 step 1784: training accuarcy: 0.6975\n",
      "Epoch 6 step 1784: training loss: 1383.8317017435809\n",
      "Epoch 6 step 1785: training accuarcy: 0.6880000000000001\n",
      "Epoch 6 step 1785: training loss: 1382.7276213666826\n",
      "Epoch 6 step 1786: training accuarcy: 0.712\n",
      "Epoch 6 step 1786: training loss: 1382.1356747428242\n",
      "Epoch 6 step 1787: training accuarcy: 0.7035\n",
      "Epoch 6 step 1787: training loss: 1383.1538892869453\n",
      "Epoch 6 step 1788: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1788: training loss: 1381.9626563551117\n",
      "Epoch 6 step 1789: training accuarcy: 0.7165\n",
      "Epoch 6 step 1789: training loss: 1383.5275602176116\n",
      "Epoch 6 step 1790: training accuarcy: 0.6910000000000001\n",
      "Epoch 6 step 1790: training loss: 1383.5504789285737\n",
      "Epoch 6 step 1791: training accuarcy: 0.6895\n",
      "Epoch 6 step 1791: training loss: 1382.7263334454017\n",
      "Epoch 6 step 1792: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1792: training loss: 1382.9165927623094\n",
      "Epoch 6 step 1793: training accuarcy: 0.6970000000000001\n",
      "Epoch 6 step 1793: training loss: 1382.8644334694923\n",
      "Epoch 6 step 1794: training accuarcy: 0.6935\n",
      "Epoch 6 step 1794: training loss: 1382.780748373219\n",
      "Epoch 6 step 1795: training accuarcy: 0.706\n",
      "Epoch 6 step 1795: training loss: 1382.8432722943203\n",
      "Epoch 6 step 1796: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1796: training loss: 1384.0041744633077\n",
      "Epoch 6 step 1797: training accuarcy: 0.6975\n",
      "Epoch 6 step 1797: training loss: 1382.5972917024342\n",
      "Epoch 6 step 1798: training accuarcy: 0.708\n",
      "Epoch 6 step 1798: training loss: 1383.1760603182884\n",
      "Epoch 6 step 1799: training accuarcy: 0.6910000000000001\n",
      "Epoch 6 step 1799: training loss: 1383.5621943157296\n",
      "Epoch 6 step 1800: training accuarcy: 0.678\n",
      "Epoch 6 step 1800: training loss: 1383.0738484390633\n",
      "Epoch 6 step 1801: training accuarcy: 0.6895\n",
      "Epoch 6 step 1801: training loss: 1383.9349121103453\n",
      "Epoch 6 step 1802: training accuarcy: 0.6745\n",
      "Epoch 6 step 1802: training loss: 1383.6223816802217\n",
      "Epoch 6 step 1803: training accuarcy: 0.6685\n",
      "Epoch 6 step 1803: training loss: 1382.4116776380147\n",
      "Epoch 6 step 1804: training accuarcy: 0.71\n",
      "Epoch 6 step 1804: training loss: 1382.9078309893148\n",
      "Epoch 6 step 1805: training accuarcy: 0.6875\n",
      "Epoch 6 step 1805: training loss: 1382.5159890417938\n",
      "Epoch 6 step 1806: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1806: training loss: 1383.4802845187223\n",
      "Epoch 6 step 1807: training accuarcy: 0.6925\n",
      "Epoch 6 step 1807: training loss: 1383.2959309836208\n",
      "Epoch 6 step 1808: training accuarcy: 0.6845\n",
      "Epoch 6 step 1808: training loss: 1383.5394679502438\n",
      "Epoch 6 step 1809: training accuarcy: 0.6875\n",
      "Epoch 6 step 1809: training loss: 1382.6545965766775\n",
      "Epoch 6 step 1810: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1810: training loss: 1383.1816439618717\n",
      "Epoch 6 step 1811: training accuarcy: 0.6925\n",
      "Epoch 6 step 1811: training loss: 1382.6983093292868\n",
      "Epoch 6 step 1812: training accuarcy: 0.6955\n",
      "Epoch 6 step 1812: training loss: 1382.9635225054776\n",
      "Epoch 6 step 1813: training accuarcy: 0.6895\n",
      "Epoch 6 step 1813: training loss: 1383.1469929699347\n",
      "Epoch 6 step 1814: training accuarcy: 0.6985\n",
      "Epoch 6 step 1814: training loss: 1382.7958451754064\n",
      "Epoch 6 step 1815: training accuarcy: 0.6925\n",
      "Epoch 6 step 1815: training loss: 1382.930372633199\n",
      "Epoch 6 step 1816: training accuarcy: 0.6835\n",
      "Epoch 6 step 1816: training loss: 1383.3474607125286\n",
      "Epoch 6 step 1817: training accuarcy: 0.6815\n",
      "Epoch 6 step 1817: training loss: 1382.7318563224892\n",
      "Epoch 6 step 1818: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1818: training loss: 1382.7912274289718\n",
      "Epoch 6 step 1819: training accuarcy: 0.6895\n",
      "Epoch 6 step 1819: training loss: 1383.7379910990055\n",
      "Epoch 6 step 1820: training accuarcy: 0.6950000000000001\n",
      "Epoch 6 step 1820: training loss: 1383.0445131883505\n",
      "Epoch 6 step 1821: training accuarcy: 0.6940000000000001\n",
      "Epoch 6 step 1821: training loss: 1382.4626598323562\n",
      "Epoch 6 step 1822: training accuarcy: 0.6990000000000001\n",
      "Epoch 6 step 1822: training loss: 1383.444971459152\n",
      "Epoch 6 step 1823: training accuarcy: 0.6945\n",
      "Epoch 6 step 1823: training loss: 1383.5062680591793\n",
      "Epoch 6 step 1824: training accuarcy: 0.6900000000000001\n",
      "Epoch 6 step 1824: training loss: 1382.4972642726038\n",
      "Epoch 6 step 1825: training accuarcy: 0.6930000000000001\n",
      "Epoch 6 step 1825: training loss: 1382.6726609411392\n",
      "Epoch 6 step 1826: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1826: training loss: 1381.5563306478848\n",
      "Epoch 6 step 1827: training accuarcy: 0.7010000000000001\n",
      "Epoch 6 step 1827: training loss: 1381.7114711750353\n",
      "Epoch 6 step 1828: training accuarcy: 0.7020000000000001\n",
      "Epoch 6 step 1828: training loss: 1382.963432468188\n",
      "Epoch 6 step 1829: training accuarcy: 0.7085\n",
      "Epoch 6 step 1829: training loss: 1382.7810744554927\n",
      "Epoch 6 step 1830: training accuarcy: 0.7030000000000001\n",
      "Epoch 6 step 1830: training loss: 1383.7072831957094\n",
      "Epoch 6 step 1831: training accuarcy: 0.6935\n",
      "Epoch 6 step 1831: training loss: 1383.3338795125494\n",
      "Epoch 6 step 1832: training accuarcy: 0.681\n",
      "Epoch 6 step 1832: training loss: 1383.944339014659\n",
      "Epoch 6 step 1833: training accuarcy: 0.6960000000000001\n",
      "Epoch 6 step 1833: training loss: 1382.683440417752\n",
      "Epoch 6 step 1834: training accuarcy: 0.7005\n",
      "Epoch 6 step 1834: training loss: 1383.506656850785\n",
      "Epoch 6 step 1835: training accuarcy: 0.6980000000000001\n",
      "Epoch 6 step 1835: training loss: 1381.6502849363374\n",
      "Epoch 6 step 1836: training accuarcy: 0.6920000000000001\n",
      "Epoch 6 step 1836: training loss: 1382.2358967724965\n",
      "Epoch 6 step 1837: training accuarcy: 0.6925\n",
      "Epoch 6 step 1837: training loss: 1383.3485204497558\n",
      "Epoch 6 step 1838: training accuarcy: 0.685\n",
      "Epoch 6 step 1838: training loss: 1383.5846211462995\n",
      "Epoch 6 step 1839: training accuarcy: 0.687\n",
      "Epoch 6 step 1839: training loss: 1381.7283761228439\n",
      "Epoch 6 step 1840: training accuarcy: 0.712\n",
      "Epoch 6 step 1840: training loss: 543.4673908861304\n",
      "Epoch 6 step 1841: training accuarcy: 0.6858974358974359\n",
      "Epoch 6: train loss 1379.585207503064, train accuarcy 0.7014636397361755\n",
      "Epoch 6: valid loss 1364.0169861741695, valid accuarcy 0.7081059217453003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                   | 7/8 [14:03<02:03, 123.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 1841: training loss: 1382.0603360998941\n",
      "Epoch 7 step 1842: training accuarcy: 0.7085\n",
      "Epoch 7 step 1842: training loss: 1381.9663085083093\n",
      "Epoch 7 step 1843: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1843: training loss: 1382.155464600916\n",
      "Epoch 7 step 1844: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1844: training loss: 1381.8843937109814\n",
      "Epoch 7 step 1845: training accuarcy: 0.713\n",
      "Epoch 7 step 1845: training loss: 1383.6670187086052\n",
      "Epoch 7 step 1846: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1846: training loss: 1381.3106253079434\n",
      "Epoch 7 step 1847: training accuarcy: 0.715\n",
      "Epoch 7 step 1847: training loss: 1381.496890954251\n",
      "Epoch 7 step 1848: training accuarcy: 0.7135\n",
      "Epoch 7 step 1848: training loss: 1382.0257215977017\n",
      "Epoch 7 step 1849: training accuarcy: 0.7145\n",
      "Epoch 7 step 1849: training loss: 1382.4808050050588\n",
      "Epoch 7 step 1850: training accuarcy: 0.7125\n",
      "Epoch 7 step 1850: training loss: 1381.6245515092833\n",
      "Epoch 7 step 1851: training accuarcy: 0.72\n",
      "Epoch 7 step 1851: training loss: 1381.9438639168332\n",
      "Epoch 7 step 1852: training accuarcy: 0.7175\n",
      "Epoch 7 step 1852: training loss: 1382.4762624070863\n",
      "Epoch 7 step 1853: training accuarcy: 0.717\n",
      "Epoch 7 step 1853: training loss: 1382.7301643995213\n",
      "Epoch 7 step 1854: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 1854: training loss: 1382.0812947280397\n",
      "Epoch 7 step 1855: training accuarcy: 0.7055\n",
      "Epoch 7 step 1855: training loss: 1382.395209155815\n",
      "Epoch 7 step 1856: training accuarcy: 0.709\n",
      "Epoch 7 step 1856: training loss: 1381.303611004135\n",
      "Epoch 7 step 1857: training accuarcy: 0.718\n",
      "Epoch 7 step 1857: training loss: 1381.50301018183\n",
      "Epoch 7 step 1858: training accuarcy: 0.7235\n",
      "Epoch 7 step 1858: training loss: 1382.512272689947\n",
      "Epoch 7 step 1859: training accuarcy: 0.7055\n",
      "Epoch 7 step 1859: training loss: 1382.467369597758\n",
      "Epoch 7 step 1860: training accuarcy: 0.712\n",
      "Epoch 7 step 1860: training loss: 1381.5499270974503\n",
      "Epoch 7 step 1861: training accuarcy: 0.724\n",
      "Epoch 7 step 1861: training loss: 1382.1145669845066\n",
      "Epoch 7 step 1862: training accuarcy: 0.6995\n",
      "Epoch 7 step 1862: training loss: 1382.8821679840892\n",
      "Epoch 7 step 1863: training accuarcy: 0.7105\n",
      "Epoch 7 step 1863: training loss: 1381.41040928361\n",
      "Epoch 7 step 1864: training accuarcy: 0.718\n",
      "Epoch 7 step 1864: training loss: 1381.4357918044684\n",
      "Epoch 7 step 1865: training accuarcy: 0.7105\n",
      "Epoch 7 step 1865: training loss: 1382.4400369523273\n",
      "Epoch 7 step 1866: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 1866: training loss: 1382.2633364925748\n",
      "Epoch 7 step 1867: training accuarcy: 0.71\n",
      "Epoch 7 step 1867: training loss: 1381.7852884225215\n",
      "Epoch 7 step 1868: training accuarcy: 0.7085\n",
      "Epoch 7 step 1868: training loss: 1381.9455047066654\n",
      "Epoch 7 step 1869: training accuarcy: 0.7095\n",
      "Epoch 7 step 1869: training loss: 1382.9169412949614\n",
      "Epoch 7 step 1870: training accuarcy: 0.7045\n",
      "Epoch 7 step 1870: training loss: 1382.6654753559276\n",
      "Epoch 7 step 1871: training accuarcy: 0.6995\n",
      "Epoch 7 step 1871: training loss: 1383.522933049939\n",
      "Epoch 7 step 1872: training accuarcy: 0.6995\n",
      "Epoch 7 step 1872: training loss: 1382.4615214223359\n",
      "Epoch 7 step 1873: training accuarcy: 0.713\n",
      "Epoch 7 step 1873: training loss: 1382.609943580792\n",
      "Epoch 7 step 1874: training accuarcy: 0.722\n",
      "Epoch 7 step 1874: training loss: 1383.234736855301\n",
      "Epoch 7 step 1875: training accuarcy: 0.7025\n",
      "Epoch 7 step 1875: training loss: 1383.2545180920529\n",
      "Epoch 7 step 1876: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 1876: training loss: 1381.8730353684393\n",
      "Epoch 7 step 1877: training accuarcy: 0.718\n",
      "Epoch 7 step 1877: training loss: 1382.9877341426657\n",
      "Epoch 7 step 1878: training accuarcy: 0.6915\n",
      "Epoch 7 step 1878: training loss: 1382.3982781363861\n",
      "Epoch 7 step 1879: training accuarcy: 0.7095\n",
      "Epoch 7 step 1879: training loss: 1382.5154961590217\n",
      "Epoch 7 step 1880: training accuarcy: 0.71\n",
      "Epoch 7 step 1880: training loss: 1382.463252377173\n",
      "Epoch 7 step 1881: training accuarcy: 0.7035\n",
      "Epoch 7 step 1881: training loss: 1382.4216335284254\n",
      "Epoch 7 step 1882: training accuarcy: 0.7135\n",
      "Epoch 7 step 1882: training loss: 1382.3137435523977\n",
      "Epoch 7 step 1883: training accuarcy: 0.7175\n",
      "Epoch 7 step 1883: training loss: 1382.6465847333689\n",
      "Epoch 7 step 1884: training accuarcy: 0.7125\n",
      "Epoch 7 step 1884: training loss: 1383.4704985793721\n",
      "Epoch 7 step 1885: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 1885: training loss: 1381.771683032104\n",
      "Epoch 7 step 1886: training accuarcy: 0.715\n",
      "Epoch 7 step 1886: training loss: 1383.1445992409276\n",
      "Epoch 7 step 1887: training accuarcy: 0.7005\n",
      "Epoch 7 step 1887: training loss: 1382.4350465620907\n",
      "Epoch 7 step 1888: training accuarcy: 0.7085\n",
      "Epoch 7 step 1888: training loss: 1382.670441240985\n",
      "Epoch 7 step 1889: training accuarcy: 0.704\n",
      "Epoch 7 step 1889: training loss: 1381.9690057041505\n",
      "Epoch 7 step 1890: training accuarcy: 0.7095\n",
      "Epoch 7 step 1890: training loss: 1381.8897881168418\n",
      "Epoch 7 step 1891: training accuarcy: 0.707\n",
      "Epoch 7 step 1891: training loss: 1382.0255423552035\n",
      "Epoch 7 step 1892: training accuarcy: 0.6995\n",
      "Epoch 7 step 1892: training loss: 1382.193955132465\n",
      "Epoch 7 step 1893: training accuarcy: 0.7065\n",
      "Epoch 7 step 1893: training loss: 1383.265974285161\n",
      "Epoch 7 step 1894: training accuarcy: 0.711\n",
      "Epoch 7 step 1894: training loss: 1381.7225246118442\n",
      "Epoch 7 step 1895: training accuarcy: 0.716\n",
      "Epoch 7 step 1895: training loss: 1383.916911724728\n",
      "Epoch 7 step 1896: training accuarcy: 0.683\n",
      "Epoch 7 step 1896: training loss: 1382.8207235514353\n",
      "Epoch 7 step 1897: training accuarcy: 0.7025\n",
      "Epoch 7 step 1897: training loss: 1383.0943508971443\n",
      "Epoch 7 step 1898: training accuarcy: 0.7125\n",
      "Epoch 7 step 1898: training loss: 1382.9600572166983\n",
      "Epoch 7 step 1899: training accuarcy: 0.71\n",
      "Epoch 7 step 1899: training loss: 1382.8062250287587\n",
      "Epoch 7 step 1900: training accuarcy: 0.7015\n",
      "Epoch 7 step 1900: training loss: 1383.232296927972\n",
      "Epoch 7 step 1901: training accuarcy: 0.6895\n",
      "Epoch 7 step 1901: training loss: 1383.1619776536031\n",
      "Epoch 7 step 1902: training accuarcy: 0.687\n",
      "Epoch 7 step 1902: training loss: 1382.926480348751\n",
      "Epoch 7 step 1903: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 1903: training loss: 1383.3602114382265\n",
      "Epoch 7 step 1904: training accuarcy: 0.7085\n",
      "Epoch 7 step 1904: training loss: 1381.6211606506222\n",
      "Epoch 7 step 1905: training accuarcy: 0.6950000000000001\n",
      "Epoch 7 step 1905: training loss: 1383.3446404023298\n",
      "Epoch 7 step 1906: training accuarcy: 0.7005\n",
      "Epoch 7 step 1906: training loss: 1382.7273940021119\n",
      "Epoch 7 step 1907: training accuarcy: 0.7065\n",
      "Epoch 7 step 1907: training loss: 1382.646120403467\n",
      "Epoch 7 step 1908: training accuarcy: 0.6960000000000001\n",
      "Epoch 7 step 1908: training loss: 1382.8194861123766\n",
      "Epoch 7 step 1909: training accuarcy: 0.6995\n",
      "Epoch 7 step 1909: training loss: 1381.786212984366\n",
      "Epoch 7 step 1910: training accuarcy: 0.7175\n",
      "Epoch 7 step 1910: training loss: 1382.5412420918915\n",
      "Epoch 7 step 1911: training accuarcy: 0.7045\n",
      "Epoch 7 step 1911: training loss: 1382.653296698967\n",
      "Epoch 7 step 1912: training accuarcy: 0.7095\n",
      "Epoch 7 step 1912: training loss: 1380.9802450187117\n",
      "Epoch 7 step 1913: training accuarcy: 0.71\n",
      "Epoch 7 step 1913: training loss: 1383.3964697277922\n",
      "Epoch 7 step 1914: training accuarcy: 0.6950000000000001\n",
      "Epoch 7 step 1914: training loss: 1383.6943471669742\n",
      "Epoch 7 step 1915: training accuarcy: 0.6965\n",
      "Epoch 7 step 1915: training loss: 1383.3175242691643\n",
      "Epoch 7 step 1916: training accuarcy: 0.708\n",
      "Epoch 7 step 1916: training loss: 1382.6933976806874\n",
      "Epoch 7 step 1917: training accuarcy: 0.704\n",
      "Epoch 7 step 1917: training loss: 1382.049911762732\n",
      "Epoch 7 step 1918: training accuarcy: 0.706\n",
      "Epoch 7 step 1918: training loss: 1382.122684561887\n",
      "Epoch 7 step 1919: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1919: training loss: 1383.41519911101\n",
      "Epoch 7 step 1920: training accuarcy: 0.6945\n",
      "Epoch 7 step 1920: training loss: 1383.566347015459\n",
      "Epoch 7 step 1921: training accuarcy: 0.6985\n",
      "Epoch 7 step 1921: training loss: 1382.6503568430396\n",
      "Epoch 7 step 1922: training accuarcy: 0.7115\n",
      "Epoch 7 step 1922: training loss: 1383.6203613187745\n",
      "Epoch 7 step 1923: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 1923: training loss: 1383.0912891277162\n",
      "Epoch 7 step 1924: training accuarcy: 0.706\n",
      "Epoch 7 step 1924: training loss: 1382.6613293629712\n",
      "Epoch 7 step 1925: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 1925: training loss: 1383.015189897159\n",
      "Epoch 7 step 1926: training accuarcy: 0.6915\n",
      "Epoch 7 step 1926: training loss: 1383.0345461815352\n",
      "Epoch 7 step 1927: training accuarcy: 0.682\n",
      "Epoch 7 step 1927: training loss: 1382.1299860127394\n",
      "Epoch 7 step 1928: training accuarcy: 0.704\n",
      "Epoch 7 step 1928: training loss: 1382.3489006138675\n",
      "Epoch 7 step 1929: training accuarcy: 0.7065\n",
      "Epoch 7 step 1929: training loss: 1381.8290284077368\n",
      "Epoch 7 step 1930: training accuarcy: 0.713\n",
      "Epoch 7 step 1930: training loss: 1382.6977309232643\n",
      "Epoch 7 step 1931: training accuarcy: 0.7015\n",
      "Epoch 7 step 1931: training loss: 1383.862271691608\n",
      "Epoch 7 step 1932: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1932: training loss: 1383.1418919672076\n",
      "Epoch 7 step 1933: training accuarcy: 0.6960000000000001\n",
      "Epoch 7 step 1933: training loss: 1382.4142663364478\n",
      "Epoch 7 step 1934: training accuarcy: 0.7025\n",
      "Epoch 7 step 1934: training loss: 1382.38403526993\n",
      "Epoch 7 step 1935: training accuarcy: 0.7105\n",
      "Epoch 7 step 1935: training loss: 1383.6570604728372\n",
      "Epoch 7 step 1936: training accuarcy: 0.687\n",
      "Epoch 7 step 1936: training loss: 1382.109301513553\n",
      "Epoch 7 step 1937: training accuarcy: 0.713\n",
      "Epoch 7 step 1937: training loss: 1382.9437740201593\n",
      "Epoch 7 step 1938: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1938: training loss: 1382.4747998880964\n",
      "Epoch 7 step 1939: training accuarcy: 0.7145\n",
      "Epoch 7 step 1939: training loss: 1382.7663886336431\n",
      "Epoch 7 step 1940: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 1940: training loss: 1383.0607511141714\n",
      "Epoch 7 step 1941: training accuarcy: 0.6805\n",
      "Epoch 7 step 1941: training loss: 1382.9087208400847\n",
      "Epoch 7 step 1942: training accuarcy: 0.6910000000000001\n",
      "Epoch 7 step 1942: training loss: 1382.7603611847487\n",
      "Epoch 7 step 1943: training accuarcy: 0.6815\n",
      "Epoch 7 step 1943: training loss: 1382.5468228030602\n",
      "Epoch 7 step 1944: training accuarcy: 0.6825\n",
      "Epoch 7 step 1944: training loss: 1382.309472033533\n",
      "Epoch 7 step 1945: training accuarcy: 0.708\n",
      "Epoch 7 step 1945: training loss: 1383.056189212135\n",
      "Epoch 7 step 1946: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 1946: training loss: 1383.3934256242542\n",
      "Epoch 7 step 1947: training accuarcy: 0.687\n",
      "Epoch 7 step 1947: training loss: 1382.3195306266744\n",
      "Epoch 7 step 1948: training accuarcy: 0.71\n",
      "Epoch 7 step 1948: training loss: 1382.9997608676808\n",
      "Epoch 7 step 1949: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1949: training loss: 1382.5805194957136\n",
      "Epoch 7 step 1950: training accuarcy: 0.71\n",
      "Epoch 7 step 1950: training loss: 1382.9742599089416\n",
      "Epoch 7 step 1951: training accuarcy: 0.6845\n",
      "Epoch 7 step 1951: training loss: 1383.1883284036714\n",
      "Epoch 7 step 1952: training accuarcy: 0.685\n",
      "Epoch 7 step 1952: training loss: 1382.375398776669\n",
      "Epoch 7 step 1953: training accuarcy: 0.712\n",
      "Epoch 7 step 1953: training loss: 1383.8692390482674\n",
      "Epoch 7 step 1954: training accuarcy: 0.682\n",
      "Epoch 7 step 1954: training loss: 1382.7509743373632\n",
      "Epoch 7 step 1955: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 1955: training loss: 1382.668599438273\n",
      "Epoch 7 step 1956: training accuarcy: 0.6865\n",
      "Epoch 7 step 1956: training loss: 1384.1517825044145\n",
      "Epoch 7 step 1957: training accuarcy: 0.6920000000000001\n",
      "Epoch 7 step 1957: training loss: 1383.1545781870177\n",
      "Epoch 7 step 1958: training accuarcy: 0.7055\n",
      "Epoch 7 step 1958: training loss: 1383.943635417747\n",
      "Epoch 7 step 1959: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 1959: training loss: 1383.6097305447345\n",
      "Epoch 7 step 1960: training accuarcy: 0.683\n",
      "Epoch 7 step 1960: training loss: 1382.9062469031326\n",
      "Epoch 7 step 1961: training accuarcy: 0.6915\n",
      "Epoch 7 step 1961: training loss: 1382.06782703944\n",
      "Epoch 7 step 1962: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 1962: training loss: 1382.2447278049788\n",
      "Epoch 7 step 1963: training accuarcy: 0.6895\n",
      "Epoch 7 step 1963: training loss: 1383.0677643702365\n",
      "Epoch 7 step 1964: training accuarcy: 0.6745\n",
      "Epoch 7 step 1964: training loss: 1383.360088850567\n",
      "Epoch 7 step 1965: training accuarcy: 0.707\n",
      "Epoch 7 step 1965: training loss: 1383.8877804429521\n",
      "Epoch 7 step 1966: training accuarcy: 0.6815\n",
      "Epoch 7 step 1966: training loss: 1383.4579115174834\n",
      "Epoch 7 step 1967: training accuarcy: 0.676\n",
      "Epoch 7 step 1967: training loss: 1382.675751058588\n",
      "Epoch 7 step 1968: training accuarcy: 0.708\n",
      "Epoch 7 step 1968: training loss: 1382.2699148427669\n",
      "Epoch 7 step 1969: training accuarcy: 0.711\n",
      "Epoch 7 step 1969: training loss: 1383.0710203140754\n",
      "Epoch 7 step 1970: training accuarcy: 0.7030000000000001\n",
      "Epoch 7 step 1970: training loss: 1382.3037581045623\n",
      "Epoch 7 step 1971: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 1971: training loss: 1382.2059671022469\n",
      "Epoch 7 step 1972: training accuarcy: 0.7135\n",
      "Epoch 7 step 1972: training loss: 1382.2604718850587\n",
      "Epoch 7 step 1973: training accuarcy: 0.721\n",
      "Epoch 7 step 1973: training loss: 1383.0036519461592\n",
      "Epoch 7 step 1974: training accuarcy: 0.6960000000000001\n",
      "Epoch 7 step 1974: training loss: 1382.1671415093783\n",
      "Epoch 7 step 1975: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 1975: training loss: 1382.9670422122367\n",
      "Epoch 7 step 1976: training accuarcy: 0.6945\n",
      "Epoch 7 step 1976: training loss: 1383.4971224898622\n",
      "Epoch 7 step 1977: training accuarcy: 0.6955\n",
      "Epoch 7 step 1977: training loss: 1383.3525837879215\n",
      "Epoch 7 step 1978: training accuarcy: 0.687\n",
      "Epoch 7 step 1978: training loss: 1381.8530450222763\n",
      "Epoch 7 step 1979: training accuarcy: 0.706\n",
      "Epoch 7 step 1979: training loss: 1383.7667891031435\n",
      "Epoch 7 step 1980: training accuarcy: 0.687\n",
      "Epoch 7 step 1980: training loss: 1382.3199007640633\n",
      "Epoch 7 step 1981: training accuarcy: 0.6975\n",
      "Epoch 7 step 1981: training loss: 1383.0844098888858\n",
      "Epoch 7 step 1982: training accuarcy: 0.6890000000000001\n",
      "Epoch 7 step 1982: training loss: 1382.5709306010276\n",
      "Epoch 7 step 1983: training accuarcy: 0.7085\n",
      "Epoch 7 step 1983: training loss: 1381.997343259998\n",
      "Epoch 7 step 1984: training accuarcy: 0.71\n",
      "Epoch 7 step 1984: training loss: 1383.634625736233\n",
      "Epoch 7 step 1985: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 1985: training loss: 1383.1872719981238\n",
      "Epoch 7 step 1986: training accuarcy: 0.6845\n",
      "Epoch 7 step 1986: training loss: 1381.8646021029138\n",
      "Epoch 7 step 1987: training accuarcy: 0.7165\n",
      "Epoch 7 step 1987: training loss: 1383.5459810045272\n",
      "Epoch 7 step 1988: training accuarcy: 0.7025\n",
      "Epoch 7 step 1988: training loss: 1383.0309385660457\n",
      "Epoch 7 step 1989: training accuarcy: 0.6995\n",
      "Epoch 7 step 1989: training loss: 1382.8277724383781\n",
      "Epoch 7 step 1990: training accuarcy: 0.7085\n",
      "Epoch 7 step 1990: training loss: 1382.6038722386859\n",
      "Epoch 7 step 1991: training accuarcy: 0.7065\n",
      "Epoch 7 step 1991: training loss: 1384.0395375407847\n",
      "Epoch 7 step 1992: training accuarcy: 0.6705\n",
      "Epoch 7 step 1992: training loss: 1383.1960398643203\n",
      "Epoch 7 step 1993: training accuarcy: 0.7025\n",
      "Epoch 7 step 1993: training loss: 1382.8137285993187\n",
      "Epoch 7 step 1994: training accuarcy: 0.7025\n",
      "Epoch 7 step 1994: training loss: 1383.4785105767578\n",
      "Epoch 7 step 1995: training accuarcy: 0.6805\n",
      "Epoch 7 step 1995: training loss: 1382.7106518278881\n",
      "Epoch 7 step 1996: training accuarcy: 0.6865\n",
      "Epoch 7 step 1996: training loss: 1383.9801436972891\n",
      "Epoch 7 step 1997: training accuarcy: 0.6765\n",
      "Epoch 7 step 1997: training loss: 1383.0360996882653\n",
      "Epoch 7 step 1998: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 1998: training loss: 1383.1800720793244\n",
      "Epoch 7 step 1999: training accuarcy: 0.684\n",
      "Epoch 7 step 1999: training loss: 1383.6784106163109\n",
      "Epoch 7 step 2000: training accuarcy: 0.6845\n",
      "Epoch 7 step 2000: training loss: 1383.154967569966\n",
      "Epoch 7 step 2001: training accuarcy: 0.705\n",
      "Epoch 7 step 2001: training loss: 1383.0302225666048\n",
      "Epoch 7 step 2002: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 2002: training loss: 1382.8736945950473\n",
      "Epoch 7 step 2003: training accuarcy: 0.6915\n",
      "Epoch 7 step 2003: training loss: 1383.1499878263\n",
      "Epoch 7 step 2004: training accuarcy: 0.6985\n",
      "Epoch 7 step 2004: training loss: 1383.7902294568473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2005: training accuarcy: 0.677\n",
      "Epoch 7 step 2005: training loss: 1382.4426488788642\n",
      "Epoch 7 step 2006: training accuarcy: 0.7115\n",
      "Epoch 7 step 2006: training loss: 1384.0143155934102\n",
      "Epoch 7 step 2007: training accuarcy: 0.6895\n",
      "Epoch 7 step 2007: training loss: 1381.786588647689\n",
      "Epoch 7 step 2008: training accuarcy: 0.7020000000000001\n",
      "Epoch 7 step 2008: training loss: 1382.724263304761\n",
      "Epoch 7 step 2009: training accuarcy: 0.6945\n",
      "Epoch 7 step 2009: training loss: 1383.3853444019803\n",
      "Epoch 7 step 2010: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2010: training loss: 1382.1493924876793\n",
      "Epoch 7 step 2011: training accuarcy: 0.7115\n",
      "Epoch 7 step 2011: training loss: 1382.429153467883\n",
      "Epoch 7 step 2012: training accuarcy: 0.711\n",
      "Epoch 7 step 2012: training loss: 1381.6580251088676\n",
      "Epoch 7 step 2013: training accuarcy: 0.707\n",
      "Epoch 7 step 2013: training loss: 1382.0272257235463\n",
      "Epoch 7 step 2014: training accuarcy: 0.715\n",
      "Epoch 7 step 2014: training loss: 1383.463280344703\n",
      "Epoch 7 step 2015: training accuarcy: 0.68\n",
      "Epoch 7 step 2015: training loss: 1383.3854367992765\n",
      "Epoch 7 step 2016: training accuarcy: 0.6995\n",
      "Epoch 7 step 2016: training loss: 1383.1893180102234\n",
      "Epoch 7 step 2017: training accuarcy: 0.6805\n",
      "Epoch 7 step 2017: training loss: 1382.2574699271618\n",
      "Epoch 7 step 2018: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 2018: training loss: 1381.7713659772248\n",
      "Epoch 7 step 2019: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 2019: training loss: 1382.6251003740852\n",
      "Epoch 7 step 2020: training accuarcy: 0.7225\n",
      "Epoch 7 step 2020: training loss: 1382.6989417714576\n",
      "Epoch 7 step 2021: training accuarcy: 0.685\n",
      "Epoch 7 step 2021: training loss: 1383.3184181671552\n",
      "Epoch 7 step 2022: training accuarcy: 0.6895\n",
      "Epoch 7 step 2022: training loss: 1383.3959091111437\n",
      "Epoch 7 step 2023: training accuarcy: 0.6975\n",
      "Epoch 7 step 2023: training loss: 1383.4034246631988\n",
      "Epoch 7 step 2024: training accuarcy: 0.6955\n",
      "Epoch 7 step 2024: training loss: 1381.8268139009629\n",
      "Epoch 7 step 2025: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 2025: training loss: 1383.3749052270855\n",
      "Epoch 7 step 2026: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 2026: training loss: 1384.0016848437513\n",
      "Epoch 7 step 2027: training accuarcy: 0.6835\n",
      "Epoch 7 step 2027: training loss: 1382.148509634583\n",
      "Epoch 7 step 2028: training accuarcy: 0.7015\n",
      "Epoch 7 step 2028: training loss: 1383.2195894752633\n",
      "Epoch 7 step 2029: training accuarcy: 0.6715\n",
      "Epoch 7 step 2029: training loss: 1382.2087802492035\n",
      "Epoch 7 step 2030: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 2030: training loss: 1381.568950656667\n",
      "Epoch 7 step 2031: training accuarcy: 0.7225\n",
      "Epoch 7 step 2031: training loss: 1383.1441528252167\n",
      "Epoch 7 step 2032: training accuarcy: 0.6950000000000001\n",
      "Epoch 7 step 2032: training loss: 1381.7738328935784\n",
      "Epoch 7 step 2033: training accuarcy: 0.7005\n",
      "Epoch 7 step 2033: training loss: 1382.5142156132465\n",
      "Epoch 7 step 2034: training accuarcy: 0.7045\n",
      "Epoch 7 step 2034: training loss: 1383.3433812358376\n",
      "Epoch 7 step 2035: training accuarcy: 0.7035\n",
      "Epoch 7 step 2035: training loss: 1381.6758552725903\n",
      "Epoch 7 step 2036: training accuarcy: 0.6985\n",
      "Epoch 7 step 2036: training loss: 1382.9492043371129\n",
      "Epoch 7 step 2037: training accuarcy: 0.6985\n",
      "Epoch 7 step 2037: training loss: 1383.191589789876\n",
      "Epoch 7 step 2038: training accuarcy: 0.6895\n",
      "Epoch 7 step 2038: training loss: 1383.0451275457776\n",
      "Epoch 7 step 2039: training accuarcy: 0.6980000000000001\n",
      "Epoch 7 step 2039: training loss: 1384.8761668834325\n",
      "Epoch 7 step 2040: training accuarcy: 0.684\n",
      "Epoch 7 step 2040: training loss: 1382.315535318513\n",
      "Epoch 7 step 2041: training accuarcy: 0.6965\n",
      "Epoch 7 step 2041: training loss: 1382.9946755194778\n",
      "Epoch 7 step 2042: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 2042: training loss: 1382.347136125648\n",
      "Epoch 7 step 2043: training accuarcy: 0.7185\n",
      "Epoch 7 step 2043: training loss: 1383.390883793246\n",
      "Epoch 7 step 2044: training accuarcy: 0.704\n",
      "Epoch 7 step 2044: training loss: 1381.9063198891429\n",
      "Epoch 7 step 2045: training accuarcy: 0.7005\n",
      "Epoch 7 step 2045: training loss: 1382.4886581789438\n",
      "Epoch 7 step 2046: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 2046: training loss: 1382.5702973043956\n",
      "Epoch 7 step 2047: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2047: training loss: 1383.4694350407758\n",
      "Epoch 7 step 2048: training accuarcy: 0.6955\n",
      "Epoch 7 step 2048: training loss: 1382.4956106742504\n",
      "Epoch 7 step 2049: training accuarcy: 0.685\n",
      "Epoch 7 step 2049: training loss: 1382.5135140079985\n",
      "Epoch 7 step 2050: training accuarcy: 0.678\n",
      "Epoch 7 step 2050: training loss: 1383.558775972797\n",
      "Epoch 7 step 2051: training accuarcy: 0.6930000000000001\n",
      "Epoch 7 step 2051: training loss: 1382.4660468600568\n",
      "Epoch 7 step 2052: training accuarcy: 0.7055\n",
      "Epoch 7 step 2052: training loss: 1382.7933164365163\n",
      "Epoch 7 step 2053: training accuarcy: 0.6955\n",
      "Epoch 7 step 2053: training loss: 1382.9793721051658\n",
      "Epoch 7 step 2054: training accuarcy: 0.6955\n",
      "Epoch 7 step 2054: training loss: 1383.0868765929588\n",
      "Epoch 7 step 2055: training accuarcy: 0.6910000000000001\n",
      "Epoch 7 step 2055: training loss: 1383.2520644337146\n",
      "Epoch 7 step 2056: training accuarcy: 0.7005\n",
      "Epoch 7 step 2056: training loss: 1382.6117511996072\n",
      "Epoch 7 step 2057: training accuarcy: 0.7055\n",
      "Epoch 7 step 2057: training loss: 1383.1640257911342\n",
      "Epoch 7 step 2058: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2058: training loss: 1382.9740542258644\n",
      "Epoch 7 step 2059: training accuarcy: 0.6890000000000001\n",
      "Epoch 7 step 2059: training loss: 1383.297283502518\n",
      "Epoch 7 step 2060: training accuarcy: 0.6895\n",
      "Epoch 7 step 2060: training loss: 1382.7589975163887\n",
      "Epoch 7 step 2061: training accuarcy: 0.687\n",
      "Epoch 7 step 2061: training loss: 1383.4250146023487\n",
      "Epoch 7 step 2062: training accuarcy: 0.705\n",
      "Epoch 7 step 2062: training loss: 1383.1919483399793\n",
      "Epoch 7 step 2063: training accuarcy: 0.6935\n",
      "Epoch 7 step 2063: training loss: 1382.301573322696\n",
      "Epoch 7 step 2064: training accuarcy: 0.7045\n",
      "Epoch 7 step 2064: training loss: 1382.3481636013687\n",
      "Epoch 7 step 2065: training accuarcy: 0.6825\n",
      "Epoch 7 step 2065: training loss: 1382.721566870602\n",
      "Epoch 7 step 2066: training accuarcy: 0.6825\n",
      "Epoch 7 step 2066: training loss: 1382.5898181583511\n",
      "Epoch 7 step 2067: training accuarcy: 0.7000000000000001\n",
      "Epoch 7 step 2067: training loss: 1382.5633876823506\n",
      "Epoch 7 step 2068: training accuarcy: 0.71\n",
      "Epoch 7 step 2068: training loss: 1383.546104719856\n",
      "Epoch 7 step 2069: training accuarcy: 0.678\n",
      "Epoch 7 step 2069: training loss: 1382.5308906948135\n",
      "Epoch 7 step 2070: training accuarcy: 0.6805\n",
      "Epoch 7 step 2070: training loss: 1383.3669578686822\n",
      "Epoch 7 step 2071: training accuarcy: 0.687\n",
      "Epoch 7 step 2071: training loss: 1383.3987621237263\n",
      "Epoch 7 step 2072: training accuarcy: 0.6895\n",
      "Epoch 7 step 2072: training loss: 1382.7972369641752\n",
      "Epoch 7 step 2073: training accuarcy: 0.6970000000000001\n",
      "Epoch 7 step 2073: training loss: 1383.9657718759267\n",
      "Epoch 7 step 2074: training accuarcy: 0.6975\n",
      "Epoch 7 step 2074: training loss: 1383.2739988994238\n",
      "Epoch 7 step 2075: training accuarcy: 0.7035\n",
      "Epoch 7 step 2075: training loss: 1383.6034375621978\n",
      "Epoch 7 step 2076: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2076: training loss: 1381.8546343935025\n",
      "Epoch 7 step 2077: training accuarcy: 0.709\n",
      "Epoch 7 step 2077: training loss: 1382.5917915487757\n",
      "Epoch 7 step 2078: training accuarcy: 0.716\n",
      "Epoch 7 step 2078: training loss: 1381.7020321113912\n",
      "Epoch 7 step 2079: training accuarcy: 0.7115\n",
      "Epoch 7 step 2079: training loss: 1384.2011504735474\n",
      "Epoch 7 step 2080: training accuarcy: 0.6885\n",
      "Epoch 7 step 2080: training loss: 1382.384190891219\n",
      "Epoch 7 step 2081: training accuarcy: 0.7075\n",
      "Epoch 7 step 2081: training loss: 1383.374364854654\n",
      "Epoch 7 step 2082: training accuarcy: 0.6945\n",
      "Epoch 7 step 2082: training loss: 1382.954434501625\n",
      "Epoch 7 step 2083: training accuarcy: 0.6990000000000001\n",
      "Epoch 7 step 2083: training loss: 1383.8024422043643\n",
      "Epoch 7 step 2084: training accuarcy: 0.6880000000000001\n",
      "Epoch 7 step 2084: training loss: 1382.6606739560916\n",
      "Epoch 7 step 2085: training accuarcy: 0.6900000000000001\n",
      "Epoch 7 step 2085: training loss: 1383.9491932681844\n",
      "Epoch 7 step 2086: training accuarcy: 0.683\n",
      "Epoch 7 step 2086: training loss: 1383.475166157004\n",
      "Epoch 7 step 2087: training accuarcy: 0.6910000000000001\n",
      "Epoch 7 step 2087: training loss: 1382.605241274103\n",
      "Epoch 7 step 2088: training accuarcy: 0.7065\n",
      "Epoch 7 step 2088: training loss: 1382.853922574848\n",
      "Epoch 7 step 2089: training accuarcy: 0.7135\n",
      "Epoch 7 step 2089: training loss: 1382.7739855277407\n",
      "Epoch 7 step 2090: training accuarcy: 0.7035\n",
      "Epoch 7 step 2090: training loss: 1381.6857171824581\n",
      "Epoch 7 step 2091: training accuarcy: 0.7175\n",
      "Epoch 7 step 2091: training loss: 1382.2089479325002\n",
      "Epoch 7 step 2092: training accuarcy: 0.6975\n",
      "Epoch 7 step 2092: training loss: 1384.6542699408378\n",
      "Epoch 7 step 2093: training accuarcy: 0.679\n",
      "Epoch 7 step 2093: training loss: 1382.7846881925848\n",
      "Epoch 7 step 2094: training accuarcy: 0.7015\n",
      "Epoch 7 step 2094: training loss: 1382.872421434367\n",
      "Epoch 7 step 2095: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 2095: training loss: 1383.661988762389\n",
      "Epoch 7 step 2096: training accuarcy: 0.6940000000000001\n",
      "Epoch 7 step 2096: training loss: 1382.8828193362726\n",
      "Epoch 7 step 2097: training accuarcy: 0.706\n",
      "Epoch 7 step 2097: training loss: 1382.8054833610279\n",
      "Epoch 7 step 2098: training accuarcy: 0.6955\n",
      "Epoch 7 step 2098: training loss: 1383.1855861176098\n",
      "Epoch 7 step 2099: training accuarcy: 0.6895\n",
      "Epoch 7 step 2099: training loss: 1382.9760957318902\n",
      "Epoch 7 step 2100: training accuarcy: 0.7005\n",
      "Epoch 7 step 2100: training loss: 1383.4437447501978\n",
      "Epoch 7 step 2101: training accuarcy: 0.6925\n",
      "Epoch 7 step 2101: training loss: 1383.1937816222232\n",
      "Epoch 7 step 2102: training accuarcy: 0.7010000000000001\n",
      "Epoch 7 step 2102: training loss: 1383.5634458112738\n",
      "Epoch 7 step 2103: training accuarcy: 0.6935\n",
      "Epoch 7 step 2103: training loss: 543.6967183521883\n",
      "Epoch 7 step 2104: training accuarcy: 0.6807692307692308\n",
      "Epoch 7: train loss 1379.5888962898016, train accuarcy 0.702133297920227\n",
      "Epoch 7: valid loss 1363.7015398173155, valid accuarcy 0.7098241448402405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [16:07<00:00, 123.26s/it]\n"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=5,\n",
    "                 log_dir=get_log_dir('seq_topcoder', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:40:50.761855Z",
     "start_time": "2019-09-25T13:40:50.753854Z"
    }
   },
   "outputs": [],
   "source": [
    "del prme_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Trans FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:41:20.750866Z",
     "start_time": "2019-09-25T13:41:20.279842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTransFM()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model = TorchTransFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "trans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:41:25.540686Z",
     "start_time": "2019-09-25T13:41:25.535685Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(trans_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:01:44.545467Z",
     "start_time": "2019-09-25T14:01:44.416483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1f2000ef860>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_learner = FMLearner(trans_model, adam_opt, schedular, db)\n",
    "trans_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=trans_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:59:33.576534Z",
     "start_time": "2019-09-25T13:41:49.976665Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 37807.26401193977\n",
      "Epoch 0 step 1: training accuarcy: 0.4955\n",
      "Epoch 0 step 1: training loss: 36749.778786841816\n",
      "Epoch 0 step 2: training accuarcy: 0.504\n",
      "Epoch 0 step 2: training loss: 35706.60817088014\n",
      "Epoch 0 step 3: training accuarcy: 0.5\n",
      "Epoch 0 step 3: training loss: 34674.50367937326\n",
      "Epoch 0 step 4: training accuarcy: 0.5275\n",
      "Epoch 0 step 4: training loss: 33686.3987004469\n",
      "Epoch 0 step 5: training accuarcy: 0.5125\n",
      "Epoch 0 step 5: training loss: 32706.96820491349\n",
      "Epoch 0 step 6: training accuarcy: 0.5165\n",
      "Epoch 0 step 6: training loss: 31751.5109393562\n",
      "Epoch 0 step 7: training accuarcy: 0.505\n",
      "Epoch 0 step 7: training loss: 30820.65553823673\n",
      "Epoch 0 step 8: training accuarcy: 0.524\n",
      "Epoch 0 step 8: training loss: 29908.863742371064\n",
      "Epoch 0 step 9: training accuarcy: 0.522\n",
      "Epoch 0 step 9: training loss: 29012.36624609563\n",
      "Epoch 0 step 10: training accuarcy: 0.529\n",
      "Epoch 0 step 10: training loss: 28151.494881656392\n",
      "Epoch 0 step 11: training accuarcy: 0.531\n",
      "Epoch 0 step 11: training loss: 27296.73065409167\n",
      "Epoch 0 step 12: training accuarcy: 0.532\n",
      "Epoch 0 step 12: training loss: 26465.293305611896\n",
      "Epoch 0 step 13: training accuarcy: 0.5525\n",
      "Epoch 0 step 13: training loss: 25663.33159232767\n",
      "Epoch 0 step 14: training accuarcy: 0.5465\n",
      "Epoch 0 step 14: training loss: 24874.398401647133\n",
      "Epoch 0 step 15: training accuarcy: 0.5735\n",
      "Epoch 0 step 15: training loss: 24108.81702105178\n",
      "Epoch 0 step 16: training accuarcy: 0.5575\n",
      "Epoch 0 step 16: training loss: 23364.372776034852\n",
      "Epoch 0 step 17: training accuarcy: 0.5635\n",
      "Epoch 0 step 17: training loss: 22632.536561241533\n",
      "Epoch 0 step 18: training accuarcy: 0.581\n",
      "Epoch 0 step 18: training loss: 21932.3174302705\n",
      "Epoch 0 step 19: training accuarcy: 0.5675\n",
      "Epoch 0 step 19: training loss: 21241.2289315382\n",
      "Epoch 0 step 20: training accuarcy: 0.5755\n",
      "Epoch 0 step 20: training loss: 20565.63908159412\n",
      "Epoch 0 step 21: training accuarcy: 0.5855\n",
      "Epoch 0 step 21: training loss: 19914.348633807673\n",
      "Epoch 0 step 22: training accuarcy: 0.5760000000000001\n",
      "Epoch 0 step 22: training loss: 19290.43030440168\n",
      "Epoch 0 step 23: training accuarcy: 0.5760000000000001\n",
      "Epoch 0 step 23: training loss: 18673.217514074706\n",
      "Epoch 0 step 24: training accuarcy: 0.5675\n",
      "Epoch 0 step 24: training loss: 18074.13889747675\n",
      "Epoch 0 step 25: training accuarcy: 0.5765\n",
      "Epoch 0 step 25: training loss: 17515.124039011876\n",
      "Epoch 0 step 26: training accuarcy: 0.5675\n",
      "Epoch 0 step 26: training loss: 16928.95624648848\n",
      "Epoch 0 step 27: training accuarcy: 0.5995\n",
      "Epoch 0 step 27: training loss: 16376.251210331942\n",
      "Epoch 0 step 28: training accuarcy: 0.612\n",
      "Epoch 0 step 28: training loss: 15849.988447244388\n",
      "Epoch 0 step 29: training accuarcy: 0.61\n",
      "Epoch 0 step 29: training loss: 15336.521834996098\n",
      "Epoch 0 step 30: training accuarcy: 0.6\n",
      "Epoch 0 step 30: training loss: 14847.306555761625\n",
      "Epoch 0 step 31: training accuarcy: 0.6\n",
      "Epoch 0 step 31: training loss: 14366.568087915792\n",
      "Epoch 0 step 32: training accuarcy: 0.5855\n",
      "Epoch 0 step 32: training loss: 13884.270569369459\n",
      "Epoch 0 step 33: training accuarcy: 0.6135\n",
      "Epoch 0 step 33: training loss: 13429.200777405837\n",
      "Epoch 0 step 34: training accuarcy: 0.6285000000000001\n",
      "Epoch 0 step 34: training loss: 12993.172393196342\n",
      "Epoch 0 step 35: training accuarcy: 0.61\n",
      "Epoch 0 step 35: training loss: 12561.847787314573\n",
      "Epoch 0 step 36: training accuarcy: 0.6335000000000001\n",
      "Epoch 0 step 36: training loss: 12151.778393121518\n",
      "Epoch 0 step 37: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 37: training loss: 11747.452603597381\n",
      "Epoch 0 step 38: training accuarcy: 0.6355000000000001\n",
      "Epoch 0 step 38: training loss: 11370.18956021107\n",
      "Epoch 0 step 39: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 39: training loss: 11003.954083230707\n",
      "Epoch 0 step 40: training accuarcy: 0.6145\n",
      "Epoch 0 step 40: training loss: 10646.632036427458\n",
      "Epoch 0 step 41: training accuarcy: 0.6095\n",
      "Epoch 0 step 41: training loss: 10277.94445478633\n",
      "Epoch 0 step 42: training accuarcy: 0.6415\n",
      "Epoch 0 step 42: training loss: 9944.35775023208\n",
      "Epoch 0 step 43: training accuarcy: 0.6415\n",
      "Epoch 0 step 43: training loss: 9620.457316117445\n",
      "Epoch 0 step 44: training accuarcy: 0.6335000000000001\n",
      "Epoch 0 step 44: training loss: 9291.776773222145\n",
      "Epoch 0 step 45: training accuarcy: 0.6625\n",
      "Epoch 0 step 45: training loss: 8999.448261826157\n",
      "Epoch 0 step 46: training accuarcy: 0.631\n",
      "Epoch 0 step 46: training loss: 8705.566687939012\n",
      "Epoch 0 step 47: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 47: training loss: 8411.011325638465\n",
      "Epoch 0 step 48: training accuarcy: 0.6395000000000001\n",
      "Epoch 0 step 48: training loss: 8125.302741168825\n",
      "Epoch 0 step 49: training accuarcy: 0.674\n",
      "Epoch 0 step 49: training loss: 7870.697359500158\n",
      "Epoch 0 step 50: training accuarcy: 0.655\n",
      "Epoch 0 step 50: training loss: 7620.628442819659\n",
      "Epoch 0 step 51: training accuarcy: 0.6445\n",
      "Epoch 0 step 51: training loss: 7369.017516477856\n",
      "Epoch 0 step 52: training accuarcy: 0.6515\n",
      "Epoch 0 step 52: training loss: 7134.474059178719\n",
      "Epoch 0 step 53: training accuarcy: 0.65\n",
      "Epoch 0 step 53: training loss: 6897.915740659644\n",
      "Epoch 0 step 54: training accuarcy: 0.663\n",
      "Epoch 0 step 54: training loss: 6674.080289255842\n",
      "Epoch 0 step 55: training accuarcy: 0.67\n",
      "Epoch 0 step 55: training loss: 6472.874866710912\n",
      "Epoch 0 step 56: training accuarcy: 0.6455\n",
      "Epoch 0 step 56: training loss: 6238.577805738282\n",
      "Epoch 0 step 57: training accuarcy: 0.6785\n",
      "Epoch 0 step 57: training loss: 6068.08365176819\n",
      "Epoch 0 step 58: training accuarcy: 0.6485\n",
      "Epoch 0 step 58: training loss: 5872.644907243513\n",
      "Epoch 0 step 59: training accuarcy: 0.6495\n",
      "Epoch 0 step 59: training loss: 5687.595115027241\n",
      "Epoch 0 step 60: training accuarcy: 0.6525\n",
      "Epoch 0 step 60: training loss: 5486.259298334925\n",
      "Epoch 0 step 61: training accuarcy: 0.677\n",
      "Epoch 0 step 61: training loss: 5334.817678426318\n",
      "Epoch 0 step 62: training accuarcy: 0.6645\n",
      "Epoch 0 step 62: training loss: 5144.1925596263545\n",
      "Epoch 0 step 63: training accuarcy: 0.6935\n",
      "Epoch 0 step 63: training loss: 4980.7562944827405\n",
      "Epoch 0 step 64: training accuarcy: 0.7010000000000001\n",
      "Epoch 0 step 64: training loss: 4847.351369488293\n",
      "Epoch 0 step 65: training accuarcy: 0.675\n",
      "Epoch 0 step 65: training loss: 4680.587684682028\n",
      "Epoch 0 step 66: training accuarcy: 0.682\n",
      "Epoch 0 step 66: training loss: 4554.403222704429\n",
      "Epoch 0 step 67: training accuarcy: 0.677\n",
      "Epoch 0 step 67: training loss: 4424.322252587183\n",
      "Epoch 0 step 68: training accuarcy: 0.6655\n",
      "Epoch 0 step 68: training loss: 4284.222575264397\n",
      "Epoch 0 step 69: training accuarcy: 0.6845\n",
      "Epoch 0 step 69: training loss: 4136.81348425163\n",
      "Epoch 0 step 70: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 70: training loss: 4034.7659112444258\n",
      "Epoch 0 step 71: training accuarcy: 0.6805\n",
      "Epoch 0 step 71: training loss: 3910.9619909563066\n",
      "Epoch 0 step 72: training accuarcy: 0.6895\n",
      "Epoch 0 step 72: training loss: 3797.9764625649073\n",
      "Epoch 0 step 73: training accuarcy: 0.674\n",
      "Epoch 0 step 73: training loss: 3686.4941960656242\n",
      "Epoch 0 step 74: training accuarcy: 0.6815\n",
      "Epoch 0 step 74: training loss: 3583.539933729381\n",
      "Epoch 0 step 75: training accuarcy: 0.683\n",
      "Epoch 0 step 75: training loss: 3476.531925526665\n",
      "Epoch 0 step 76: training accuarcy: 0.705\n",
      "Epoch 0 step 76: training loss: 3382.8494498929367\n",
      "Epoch 0 step 77: training accuarcy: 0.7030000000000001\n",
      "Epoch 0 step 77: training loss: 3291.016697247184\n",
      "Epoch 0 step 78: training accuarcy: 0.6900000000000001\n",
      "Epoch 0 step 78: training loss: 3207.264472623237\n",
      "Epoch 0 step 79: training accuarcy: 0.6915\n",
      "Epoch 0 step 79: training loss: 3103.8994136020506\n",
      "Epoch 0 step 80: training accuarcy: 0.708\n",
      "Epoch 0 step 80: training loss: 3043.7345031251625\n",
      "Epoch 0 step 81: training accuarcy: 0.673\n",
      "Epoch 0 step 81: training loss: 2958.5010603649885\n",
      "Epoch 0 step 82: training accuarcy: 0.6885\n",
      "Epoch 0 step 82: training loss: 2874.053798820428\n",
      "Epoch 0 step 83: training accuarcy: 0.712\n",
      "Epoch 0 step 83: training loss: 2811.552514865482\n",
      "Epoch 0 step 84: training accuarcy: 0.68\n",
      "Epoch 0 step 84: training loss: 2729.337244245081\n",
      "Epoch 0 step 85: training accuarcy: 0.6875\n",
      "Epoch 0 step 85: training loss: 2666.2523024403354\n",
      "Epoch 0 step 86: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 86: training loss: 2608.6593532551665\n",
      "Epoch 0 step 87: training accuarcy: 0.7025\n",
      "Epoch 0 step 87: training loss: 2544.652764805356\n",
      "Epoch 0 step 88: training accuarcy: 0.6880000000000001\n",
      "Epoch 0 step 88: training loss: 2468.6590752649295\n",
      "Epoch 0 step 89: training accuarcy: 0.71\n",
      "Epoch 0 step 89: training loss: 2423.153420795797\n",
      "Epoch 0 step 90: training accuarcy: 0.6975\n",
      "Epoch 0 step 90: training loss: 2361.7523445788593\n",
      "Epoch 0 step 91: training accuarcy: 0.713\n",
      "Epoch 0 step 91: training loss: 2310.594598037431\n",
      "Epoch 0 step 92: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 92: training loss: 2254.93353079578\n",
      "Epoch 0 step 93: training accuarcy: 0.712\n",
      "Epoch 0 step 93: training loss: 2206.2737715561316\n",
      "Epoch 0 step 94: training accuarcy: 0.7135\n",
      "Epoch 0 step 94: training loss: 2179.5454209468944\n",
      "Epoch 0 step 95: training accuarcy: 0.684\n",
      "Epoch 0 step 95: training loss: 2129.032255605367\n",
      "Epoch 0 step 96: training accuarcy: 0.6985\n",
      "Epoch 0 step 96: training loss: 2075.8736270671952\n",
      "Epoch 0 step 97: training accuarcy: 0.7085\n",
      "Epoch 0 step 97: training loss: 2035.9709134901077\n",
      "Epoch 0 step 98: training accuarcy: 0.7085\n",
      "Epoch 0 step 98: training loss: 1993.3797435417275\n",
      "Epoch 0 step 99: training accuarcy: 0.7085\n",
      "Epoch 0 step 99: training loss: 1954.3972386910968\n",
      "Epoch 0 step 100: training accuarcy: 0.7105\n",
      "Epoch 0 step 100: training loss: 1927.6730427420675\n",
      "Epoch 0 step 101: training accuarcy: 0.7035\n",
      "Epoch 0 step 101: training loss: 1886.1198058622754\n",
      "Epoch 0 step 102: training accuarcy: 0.7005\n",
      "Epoch 0 step 102: training loss: 1850.0905513083387\n",
      "Epoch 0 step 103: training accuarcy: 0.711\n",
      "Epoch 0 step 103: training loss: 1841.0903347695662\n",
      "Epoch 0 step 104: training accuarcy: 0.6915\n",
      "Epoch 0 step 104: training loss: 1798.8394565356461\n",
      "Epoch 0 step 105: training accuarcy: 0.6995\n",
      "Epoch 0 step 105: training loss: 1762.9487344945678\n",
      "Epoch 0 step 106: training accuarcy: 0.7185\n",
      "Epoch 0 step 106: training loss: 1733.0626431074666\n",
      "Epoch 0 step 107: training accuarcy: 0.715\n",
      "Epoch 0 step 107: training loss: 1710.380037864818\n",
      "Epoch 0 step 108: training accuarcy: 0.721\n",
      "Epoch 0 step 108: training loss: 1675.265609888339\n",
      "Epoch 0 step 109: training accuarcy: 0.711\n",
      "Epoch 0 step 109: training loss: 1648.5779112702803\n",
      "Epoch 0 step 110: training accuarcy: 0.7245\n",
      "Epoch 0 step 110: training loss: 1647.3809420328232\n",
      "Epoch 0 step 111: training accuarcy: 0.7095\n",
      "Epoch 0 step 111: training loss: 1633.3702005702396\n",
      "Epoch 0 step 112: training accuarcy: 0.6905\n",
      "Epoch 0 step 112: training loss: 1597.4880324723638\n",
      "Epoch 0 step 113: training accuarcy: 0.705\n",
      "Epoch 0 step 113: training loss: 1579.078647840429\n",
      "Epoch 0 step 114: training accuarcy: 0.6990000000000001\n",
      "Epoch 0 step 114: training loss: 1566.2107715791728\n",
      "Epoch 0 step 115: training accuarcy: 0.714\n",
      "Epoch 0 step 115: training loss: 1536.0951812720323\n",
      "Epoch 0 step 116: training accuarcy: 0.716\n",
      "Epoch 0 step 116: training loss: 1501.7811420943224\n",
      "Epoch 0 step 117: training accuarcy: 0.741\n",
      "Epoch 0 step 117: training loss: 1517.2548790899996\n",
      "Epoch 0 step 118: training accuarcy: 0.7145\n",
      "Epoch 0 step 118: training loss: 1501.2234539052015\n",
      "Epoch 0 step 119: training accuarcy: 0.7010000000000001\n",
      "Epoch 0 step 119: training loss: 1466.312368547694\n",
      "Epoch 0 step 120: training accuarcy: 0.723\n",
      "Epoch 0 step 120: training loss: 1450.4940142050527\n",
      "Epoch 0 step 121: training accuarcy: 0.7185\n",
      "Epoch 0 step 121: training loss: 1432.6769432313476\n",
      "Epoch 0 step 122: training accuarcy: 0.732\n",
      "Epoch 0 step 122: training loss: 1429.1484376279031\n",
      "Epoch 0 step 123: training accuarcy: 0.724\n",
      "Epoch 0 step 123: training loss: 1420.510733923596\n",
      "Epoch 0 step 124: training accuarcy: 0.7105\n",
      "Epoch 0 step 124: training loss: 1402.4185365007986\n",
      "Epoch 0 step 125: training accuarcy: 0.7195\n",
      "Epoch 0 step 125: training loss: 1397.7160325818302\n",
      "Epoch 0 step 126: training accuarcy: 0.711\n",
      "Epoch 0 step 126: training loss: 1357.4998526525949\n",
      "Epoch 0 step 127: training accuarcy: 0.731\n",
      "Epoch 0 step 127: training loss: 1361.6563353731374\n",
      "Epoch 0 step 128: training accuarcy: 0.7145\n",
      "Epoch 0 step 128: training loss: 1361.5360167508948\n",
      "Epoch 0 step 129: training accuarcy: 0.724\n",
      "Epoch 0 step 129: training loss: 1372.4237233278684\n",
      "Epoch 0 step 130: training accuarcy: 0.7105\n",
      "Epoch 0 step 130: training loss: 1349.5937886560278\n",
      "Epoch 0 step 131: training accuarcy: 0.7005\n",
      "Epoch 0 step 131: training loss: 1326.600667653221\n",
      "Epoch 0 step 132: training accuarcy: 0.729\n",
      "Epoch 0 step 132: training loss: 1315.50424680294\n",
      "Epoch 0 step 133: training accuarcy: 0.7315\n",
      "Epoch 0 step 133: training loss: 1312.919796029183\n",
      "Epoch 0 step 134: training accuarcy: 0.7235\n",
      "Epoch 0 step 134: training loss: 1282.4709758879862\n",
      "Epoch 0 step 135: training accuarcy: 0.738\n",
      "Epoch 0 step 135: training loss: 1284.5860610171092\n",
      "Epoch 0 step 136: training accuarcy: 0.7315\n",
      "Epoch 0 step 136: training loss: 1283.8376772029837\n",
      "Epoch 0 step 137: training accuarcy: 0.7375\n",
      "Epoch 0 step 137: training loss: 1295.186555747755\n",
      "Epoch 0 step 138: training accuarcy: 0.7165\n",
      "Epoch 0 step 138: training loss: 1280.9211586904385\n",
      "Epoch 0 step 139: training accuarcy: 0.712\n",
      "Epoch 0 step 139: training loss: 1282.1534772507393\n",
      "Epoch 0 step 140: training accuarcy: 0.7125\n",
      "Epoch 0 step 140: training loss: 1269.354818383104\n",
      "Epoch 0 step 141: training accuarcy: 0.718\n",
      "Epoch 0 step 141: training loss: 1256.3801779009002\n",
      "Epoch 0 step 142: training accuarcy: 0.7335\n",
      "Epoch 0 step 142: training loss: 1260.5525024095418\n",
      "Epoch 0 step 143: training accuarcy: 0.7165\n",
      "Epoch 0 step 143: training loss: 1222.5887550519124\n",
      "Epoch 0 step 144: training accuarcy: 0.7425\n",
      "Epoch 0 step 144: training loss: 1228.7804044552704\n",
      "Epoch 0 step 145: training accuarcy: 0.7315\n",
      "Epoch 0 step 145: training loss: 1244.1737650023947\n",
      "Epoch 0 step 146: training accuarcy: 0.724\n",
      "Epoch 0 step 146: training loss: 1236.5794005125633\n",
      "Epoch 0 step 147: training accuarcy: 0.7365\n",
      "Epoch 0 step 147: training loss: 1226.266435221166\n",
      "Epoch 0 step 148: training accuarcy: 0.7325\n",
      "Epoch 0 step 148: training loss: 1213.10731553315\n",
      "Epoch 0 step 149: training accuarcy: 0.742\n",
      "Epoch 0 step 149: training loss: 1225.0384420131065\n",
      "Epoch 0 step 150: training accuarcy: 0.714\n",
      "Epoch 0 step 150: training loss: 1217.2996274897494\n",
      "Epoch 0 step 151: training accuarcy: 0.722\n",
      "Epoch 0 step 151: training loss: 1227.84155184617\n",
      "Epoch 0 step 152: training accuarcy: 0.7145\n",
      "Epoch 0 step 152: training loss: 1202.5860792428082\n",
      "Epoch 0 step 153: training accuarcy: 0.7385\n",
      "Epoch 0 step 153: training loss: 1207.9447280215845\n",
      "Epoch 0 step 154: training accuarcy: 0.7185\n",
      "Epoch 0 step 154: training loss: 1195.2877449141567\n",
      "Epoch 0 step 155: training accuarcy: 0.732\n",
      "Epoch 0 step 155: training loss: 1196.2440751727117\n",
      "Epoch 0 step 156: training accuarcy: 0.7295\n",
      "Epoch 0 step 156: training loss: 1194.6329608790734\n",
      "Epoch 0 step 157: training accuarcy: 0.729\n",
      "Epoch 0 step 157: training loss: 1184.7649059468536\n",
      "Epoch 0 step 158: training accuarcy: 0.7395\n",
      "Epoch 0 step 158: training loss: 1193.032734805529\n",
      "Epoch 0 step 159: training accuarcy: 0.7305\n",
      "Epoch 0 step 159: training loss: 1165.6210284946724\n",
      "Epoch 0 step 160: training accuarcy: 0.737\n",
      "Epoch 0 step 160: training loss: 1193.6654953404163\n",
      "Epoch 0 step 161: training accuarcy: 0.7155\n",
      "Epoch 0 step 161: training loss: 1180.39103466858\n",
      "Epoch 0 step 162: training accuarcy: 0.736\n",
      "Epoch 0 step 162: training loss: 1178.7229866295888\n",
      "Epoch 0 step 163: training accuarcy: 0.737\n",
      "Epoch 0 step 163: training loss: 1179.7303721261203\n",
      "Epoch 0 step 164: training accuarcy: 0.7225\n",
      "Epoch 0 step 164: training loss: 1161.5225663087497\n",
      "Epoch 0 step 165: training accuarcy: 0.7405\n",
      "Epoch 0 step 165: training loss: 1167.1448120644477\n",
      "Epoch 0 step 166: training accuarcy: 0.7465\n",
      "Epoch 0 step 166: training loss: 1189.4974800599095\n",
      "Epoch 0 step 167: training accuarcy: 0.7195\n",
      "Epoch 0 step 167: training loss: 1151.9930560984008\n",
      "Epoch 0 step 168: training accuarcy: 0.7475\n",
      "Epoch 0 step 168: training loss: 1168.1985040603033\n",
      "Epoch 0 step 169: training accuarcy: 0.739\n",
      "Epoch 0 step 169: training loss: 1158.230081184211\n",
      "Epoch 0 step 170: training accuarcy: 0.752\n",
      "Epoch 0 step 170: training loss: 1163.2051637503212\n",
      "Epoch 0 step 171: training accuarcy: 0.733\n",
      "Epoch 0 step 171: training loss: 1163.0653006023238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 172: training accuarcy: 0.7435\n",
      "Epoch 0 step 172: training loss: 1142.4687222205114\n",
      "Epoch 0 step 173: training accuarcy: 0.7425\n",
      "Epoch 0 step 173: training loss: 1148.4498302713594\n",
      "Epoch 0 step 174: training accuarcy: 0.748\n",
      "Epoch 0 step 174: training loss: 1126.3964042349307\n",
      "Epoch 0 step 175: training accuarcy: 0.751\n",
      "Epoch 0 step 175: training loss: 1175.33577255694\n",
      "Epoch 0 step 176: training accuarcy: 0.7195\n",
      "Epoch 0 step 176: training loss: 1150.6492550502567\n",
      "Epoch 0 step 177: training accuarcy: 0.738\n",
      "Epoch 0 step 177: training loss: 1154.2970654569183\n",
      "Epoch 0 step 178: training accuarcy: 0.751\n",
      "Epoch 0 step 178: training loss: 1132.282888711028\n",
      "Epoch 0 step 179: training accuarcy: 0.7485\n",
      "Epoch 0 step 179: training loss: 1129.9585267991922\n",
      "Epoch 0 step 180: training accuarcy: 0.755\n",
      "Epoch 0 step 180: training loss: 1151.8311434767393\n",
      "Epoch 0 step 181: training accuarcy: 0.737\n",
      "Epoch 0 step 181: training loss: 1144.1199802066885\n",
      "Epoch 0 step 182: training accuarcy: 0.7335\n",
      "Epoch 0 step 182: training loss: 1149.5717620679309\n",
      "Epoch 0 step 183: training accuarcy: 0.736\n",
      "Epoch 0 step 183: training loss: 1145.04565135365\n",
      "Epoch 0 step 184: training accuarcy: 0.7315\n",
      "Epoch 0 step 184: training loss: 1139.879609137377\n",
      "Epoch 0 step 185: training accuarcy: 0.734\n",
      "Epoch 0 step 185: training loss: 1139.1484468014248\n",
      "Epoch 0 step 186: training accuarcy: 0.7315\n",
      "Epoch 0 step 186: training loss: 1134.104896027933\n",
      "Epoch 0 step 187: training accuarcy: 0.754\n",
      "Epoch 0 step 187: training loss: 1138.1229620292077\n",
      "Epoch 0 step 188: training accuarcy: 0.731\n",
      "Epoch 0 step 188: training loss: 1113.0948424942744\n",
      "Epoch 0 step 189: training accuarcy: 0.752\n",
      "Epoch 0 step 189: training loss: 1136.3698136300725\n",
      "Epoch 0 step 190: training accuarcy: 0.7355\n",
      "Epoch 0 step 190: training loss: 1159.7078459241334\n",
      "Epoch 0 step 191: training accuarcy: 0.7295\n",
      "Epoch 0 step 191: training loss: 1116.5934299755656\n",
      "Epoch 0 step 192: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 192: training loss: 1139.8301478179762\n",
      "Epoch 0 step 193: training accuarcy: 0.7275\n",
      "Epoch 0 step 193: training loss: 1118.052967666961\n",
      "Epoch 0 step 194: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 194: training loss: 1115.6900541361024\n",
      "Epoch 0 step 195: training accuarcy: 0.7515000000000001\n",
      "Epoch 0 step 195: training loss: 1126.165256556415\n",
      "Epoch 0 step 196: training accuarcy: 0.743\n",
      "Epoch 0 step 196: training loss: 1116.6201583998072\n",
      "Epoch 0 step 197: training accuarcy: 0.735\n",
      "Epoch 0 step 197: training loss: 1108.0160008193236\n",
      "Epoch 0 step 198: training accuarcy: 0.7445\n",
      "Epoch 0 step 198: training loss: 1124.7942906608514\n",
      "Epoch 0 step 199: training accuarcy: 0.7455\n",
      "Epoch 0 step 199: training loss: 1094.414690128558\n",
      "Epoch 0 step 200: training accuarcy: 0.756\n",
      "Epoch 0 step 200: training loss: 1136.8417697555096\n",
      "Epoch 0 step 201: training accuarcy: 0.7315\n",
      "Epoch 0 step 201: training loss: 1098.304564864787\n",
      "Epoch 0 step 202: training accuarcy: 0.766\n",
      "Epoch 0 step 202: training loss: 1118.7497192760854\n",
      "Epoch 0 step 203: training accuarcy: 0.7335\n",
      "Epoch 0 step 203: training loss: 1098.2509754300397\n",
      "Epoch 0 step 204: training accuarcy: 0.7505000000000001\n",
      "Epoch 0 step 204: training loss: 1084.7903029102504\n",
      "Epoch 0 step 205: training accuarcy: 0.7685\n",
      "Epoch 0 step 205: training loss: 1114.3538648747558\n",
      "Epoch 0 step 206: training accuarcy: 0.7375\n",
      "Epoch 0 step 206: training loss: 1109.1628794110952\n",
      "Epoch 0 step 207: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 207: training loss: 1101.90181965778\n",
      "Epoch 0 step 208: training accuarcy: 0.7495\n",
      "Epoch 0 step 208: training loss: 1098.353421050054\n",
      "Epoch 0 step 209: training accuarcy: 0.757\n",
      "Epoch 0 step 209: training loss: 1112.8900540336288\n",
      "Epoch 0 step 210: training accuarcy: 0.737\n",
      "Epoch 0 step 210: training loss: 1114.7670777665633\n",
      "Epoch 0 step 211: training accuarcy: 0.7325\n",
      "Epoch 0 step 211: training loss: 1122.8590159727923\n",
      "Epoch 0 step 212: training accuarcy: 0.736\n",
      "Epoch 0 step 212: training loss: 1123.302358444635\n",
      "Epoch 0 step 213: training accuarcy: 0.744\n",
      "Epoch 0 step 213: training loss: 1104.201056691348\n",
      "Epoch 0 step 214: training accuarcy: 0.747\n",
      "Epoch 0 step 214: training loss: 1096.7656611658979\n",
      "Epoch 0 step 215: training accuarcy: 0.761\n",
      "Epoch 0 step 215: training loss: 1108.66013939918\n",
      "Epoch 0 step 216: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 216: training loss: 1092.3197274873003\n",
      "Epoch 0 step 217: training accuarcy: 0.749\n",
      "Epoch 0 step 217: training loss: 1080.2445909111714\n",
      "Epoch 0 step 218: training accuarcy: 0.753\n",
      "Epoch 0 step 218: training loss: 1105.93240397533\n",
      "Epoch 0 step 219: training accuarcy: 0.7415\n",
      "Epoch 0 step 219: training loss: 1088.1897985383157\n",
      "Epoch 0 step 220: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 220: training loss: 1094.5257326100902\n",
      "Epoch 0 step 221: training accuarcy: 0.754\n",
      "Epoch 0 step 221: training loss: 1096.1290052438412\n",
      "Epoch 0 step 222: training accuarcy: 0.7695\n",
      "Epoch 0 step 222: training loss: 1084.7712796017945\n",
      "Epoch 0 step 223: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 223: training loss: 1097.080853931582\n",
      "Epoch 0 step 224: training accuarcy: 0.752\n",
      "Epoch 0 step 224: training loss: 1124.7750507094092\n",
      "Epoch 0 step 225: training accuarcy: 0.7275\n",
      "Epoch 0 step 225: training loss: 1078.0326860853854\n",
      "Epoch 0 step 226: training accuarcy: 0.755\n",
      "Epoch 0 step 226: training loss: 1090.6661707782384\n",
      "Epoch 0 step 227: training accuarcy: 0.7485\n",
      "Epoch 0 step 227: training loss: 1103.1397659234194\n",
      "Epoch 0 step 228: training accuarcy: 0.7495\n",
      "Epoch 0 step 228: training loss: 1101.838882137178\n",
      "Epoch 0 step 229: training accuarcy: 0.738\n",
      "Epoch 0 step 229: training loss: 1085.552859717642\n",
      "Epoch 0 step 230: training accuarcy: 0.751\n",
      "Epoch 0 step 230: training loss: 1077.2558804575128\n",
      "Epoch 0 step 231: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 231: training loss: 1093.9134904191671\n",
      "Epoch 0 step 232: training accuarcy: 0.7535000000000001\n",
      "Epoch 0 step 232: training loss: 1096.421543036433\n",
      "Epoch 0 step 233: training accuarcy: 0.759\n",
      "Epoch 0 step 233: training loss: 1079.8327969811635\n",
      "Epoch 0 step 234: training accuarcy: 0.7505000000000001\n",
      "Epoch 0 step 234: training loss: 1118.7340116938014\n",
      "Epoch 0 step 235: training accuarcy: 0.7385\n",
      "Epoch 0 step 235: training loss: 1103.9720289932052\n",
      "Epoch 0 step 236: training accuarcy: 0.7465\n",
      "Epoch 0 step 236: training loss: 1095.5555931653569\n",
      "Epoch 0 step 237: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 237: training loss: 1076.9513968918732\n",
      "Epoch 0 step 238: training accuarcy: 0.7685\n",
      "Epoch 0 step 238: training loss: 1072.517621425396\n",
      "Epoch 0 step 239: training accuarcy: 0.7585000000000001\n",
      "Epoch 0 step 239: training loss: 1054.017455433905\n",
      "Epoch 0 step 240: training accuarcy: 0.7705\n",
      "Epoch 0 step 240: training loss: 1081.4099765563258\n",
      "Epoch 0 step 241: training accuarcy: 0.7525000000000001\n",
      "Epoch 0 step 241: training loss: 1078.7900167183884\n",
      "Epoch 0 step 242: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 242: training loss: 1084.7120022892088\n",
      "Epoch 0 step 243: training accuarcy: 0.7505000000000001\n",
      "Epoch 0 step 243: training loss: 1048.059120968565\n",
      "Epoch 0 step 244: training accuarcy: 0.776\n",
      "Epoch 0 step 244: training loss: 1057.6020296892452\n",
      "Epoch 0 step 245: training accuarcy: 0.767\n",
      "Epoch 0 step 245: training loss: 1070.747337109404\n",
      "Epoch 0 step 246: training accuarcy: 0.7625000000000001\n",
      "Epoch 0 step 246: training loss: 1083.7991155223049\n",
      "Epoch 0 step 247: training accuarcy: 0.7455\n",
      "Epoch 0 step 247: training loss: 1093.2811430275992\n",
      "Epoch 0 step 248: training accuarcy: 0.751\n",
      "Epoch 0 step 248: training loss: 1107.259535008458\n",
      "Epoch 0 step 249: training accuarcy: 0.739\n",
      "Epoch 0 step 249: training loss: 1050.6813750660008\n",
      "Epoch 0 step 250: training accuarcy: 0.7755\n",
      "Epoch 0 step 250: training loss: 1083.5120033443106\n",
      "Epoch 0 step 251: training accuarcy: 0.759\n",
      "Epoch 0 step 251: training loss: 1055.6936243028965\n",
      "Epoch 0 step 252: training accuarcy: 0.7635000000000001\n",
      "Epoch 0 step 252: training loss: 1069.2158951827535\n",
      "Epoch 0 step 253: training accuarcy: 0.755\n",
      "Epoch 0 step 253: training loss: 1075.0355515731055\n",
      "Epoch 0 step 254: training accuarcy: 0.7455\n",
      "Epoch 0 step 254: training loss: 1074.3076205464038\n",
      "Epoch 0 step 255: training accuarcy: 0.76\n",
      "Epoch 0 step 255: training loss: 1084.4392297650447\n",
      "Epoch 0 step 256: training accuarcy: 0.751\n",
      "Epoch 0 step 256: training loss: 1064.7772086371895\n",
      "Epoch 0 step 257: training accuarcy: 0.7495\n",
      "Epoch 0 step 257: training loss: 1078.0242551238125\n",
      "Epoch 0 step 258: training accuarcy: 0.7555000000000001\n",
      "Epoch 0 step 258: training loss: 1080.5240449647833\n",
      "Epoch 0 step 259: training accuarcy: 0.7445\n",
      "Epoch 0 step 259: training loss: 1070.4557052705234\n",
      "Epoch 0 step 260: training accuarcy: 0.76\n",
      "Epoch 0 step 260: training loss: 1051.8578109743326\n",
      "Epoch 0 step 261: training accuarcy: 0.7795\n",
      "Epoch 0 step 261: training loss: 1052.9890355081523\n",
      "Epoch 0 step 262: training accuarcy: 0.7675000000000001\n",
      "Epoch 0 step 262: training loss: 419.62126561278643\n",
      "Epoch 0 step 263: training accuarcy: 0.7769230769230769\n",
      "Epoch 0: train loss 5223.848272861555, train accuarcy 0.6741687059402466\n",
      "Epoch 0: valid loss 1091.9839639880483, valid accuarcy 0.7284212708473206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████                                                                                                                                     | 1/8 [02:13<15:37, 133.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 968.1247141387189\n",
      "Epoch 1 step 264: training accuarcy: 0.8155\n",
      "Epoch 1 step 264: training loss: 974.9155207398045\n",
      "Epoch 1 step 265: training accuarcy: 0.799\n",
      "Epoch 1 step 265: training loss: 968.9799277918569\n",
      "Epoch 1 step 266: training accuarcy: 0.8115\n",
      "Epoch 1 step 266: training loss: 969.3743419102765\n",
      "Epoch 1 step 267: training accuarcy: 0.8095\n",
      "Epoch 1 step 267: training loss: 975.4692887293817\n",
      "Epoch 1 step 268: training accuarcy: 0.802\n",
      "Epoch 1 step 268: training loss: 986.443471727153\n",
      "Epoch 1 step 269: training accuarcy: 0.801\n",
      "Epoch 1 step 269: training loss: 965.1668782718472\n",
      "Epoch 1 step 270: training accuarcy: 0.809\n",
      "Epoch 1 step 270: training loss: 978.3685398036984\n",
      "Epoch 1 step 271: training accuarcy: 0.7915\n",
      "Epoch 1 step 271: training loss: 977.2293235825683\n",
      "Epoch 1 step 272: training accuarcy: 0.799\n",
      "Epoch 1 step 272: training loss: 976.9488489259388\n",
      "Epoch 1 step 273: training accuarcy: 0.798\n",
      "Epoch 1 step 273: training loss: 968.6765519553475\n",
      "Epoch 1 step 274: training accuarcy: 0.811\n",
      "Epoch 1 step 274: training loss: 967.3988338807497\n",
      "Epoch 1 step 275: training accuarcy: 0.808\n",
      "Epoch 1 step 275: training loss: 990.0202723941416\n",
      "Epoch 1 step 276: training accuarcy: 0.798\n",
      "Epoch 1 step 276: training loss: 991.5557820134724\n",
      "Epoch 1 step 277: training accuarcy: 0.799\n",
      "Epoch 1 step 277: training loss: 980.1655755840899\n",
      "Epoch 1 step 278: training accuarcy: 0.8005\n",
      "Epoch 1 step 278: training loss: 959.4941410540938\n",
      "Epoch 1 step 279: training accuarcy: 0.8140000000000001\n",
      "Epoch 1 step 279: training loss: 955.9214431318514\n",
      "Epoch 1 step 280: training accuarcy: 0.8180000000000001\n",
      "Epoch 1 step 280: training loss: 958.0553189057822\n",
      "Epoch 1 step 281: training accuarcy: 0.8095\n",
      "Epoch 1 step 281: training loss: 961.2588368668607\n",
      "Epoch 1 step 282: training accuarcy: 0.8025\n",
      "Epoch 1 step 282: training loss: 963.8433784536728\n",
      "Epoch 1 step 283: training accuarcy: 0.8025\n",
      "Epoch 1 step 283: training loss: 979.3783438952615\n",
      "Epoch 1 step 284: training accuarcy: 0.806\n",
      "Epoch 1 step 284: training loss: 963.0782442812564\n",
      "Epoch 1 step 285: training accuarcy: 0.81\n",
      "Epoch 1 step 285: training loss: 948.8312387673778\n",
      "Epoch 1 step 286: training accuarcy: 0.8185\n",
      "Epoch 1 step 286: training loss: 979.6909189725178\n",
      "Epoch 1 step 287: training accuarcy: 0.7965\n",
      "Epoch 1 step 287: training loss: 946.9660894498473\n",
      "Epoch 1 step 288: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 288: training loss: 974.6851141170539\n",
      "Epoch 1 step 289: training accuarcy: 0.802\n",
      "Epoch 1 step 289: training loss: 975.0458939221426\n",
      "Epoch 1 step 290: training accuarcy: 0.7945\n",
      "Epoch 1 step 290: training loss: 958.4915186084331\n",
      "Epoch 1 step 291: training accuarcy: 0.806\n",
      "Epoch 1 step 291: training loss: 969.8158073687748\n",
      "Epoch 1 step 292: training accuarcy: 0.8075\n",
      "Epoch 1 step 292: training loss: 953.9938644044348\n",
      "Epoch 1 step 293: training accuarcy: 0.8095\n",
      "Epoch 1 step 293: training loss: 978.3582524453193\n",
      "Epoch 1 step 294: training accuarcy: 0.7995\n",
      "Epoch 1 step 294: training loss: 947.5335551926571\n",
      "Epoch 1 step 295: training accuarcy: 0.8095\n",
      "Epoch 1 step 295: training loss: 961.4748081959683\n",
      "Epoch 1 step 296: training accuarcy: 0.802\n",
      "Epoch 1 step 296: training loss: 948.4041588850102\n",
      "Epoch 1 step 297: training accuarcy: 0.8085\n",
      "Epoch 1 step 297: training loss: 963.366037473171\n",
      "Epoch 1 step 298: training accuarcy: 0.8155\n",
      "Epoch 1 step 298: training loss: 949.2099318898507\n",
      "Epoch 1 step 299: training accuarcy: 0.792\n",
      "Epoch 1 step 299: training loss: 960.7395407782798\n",
      "Epoch 1 step 300: training accuarcy: 0.7945\n",
      "Epoch 1 step 300: training loss: 967.0012332772461\n",
      "Epoch 1 step 301: training accuarcy: 0.8085\n",
      "Epoch 1 step 301: training loss: 962.5263537158971\n",
      "Epoch 1 step 302: training accuarcy: 0.801\n",
      "Epoch 1 step 302: training loss: 950.7168434309668\n",
      "Epoch 1 step 303: training accuarcy: 0.808\n",
      "Epoch 1 step 303: training loss: 956.1449412421084\n",
      "Epoch 1 step 304: training accuarcy: 0.8015\n",
      "Epoch 1 step 304: training loss: 939.7307271607176\n",
      "Epoch 1 step 305: training accuarcy: 0.8135\n",
      "Epoch 1 step 305: training loss: 952.7095173690778\n",
      "Epoch 1 step 306: training accuarcy: 0.808\n",
      "Epoch 1 step 306: training loss: 965.0924585090116\n",
      "Epoch 1 step 307: training accuarcy: 0.8025\n",
      "Epoch 1 step 307: training loss: 947.6073854411834\n",
      "Epoch 1 step 308: training accuarcy: 0.811\n",
      "Epoch 1 step 308: training loss: 957.2226910915473\n",
      "Epoch 1 step 309: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 309: training loss: 955.7768564621261\n",
      "Epoch 1 step 310: training accuarcy: 0.8025\n",
      "Epoch 1 step 310: training loss: 937.6446772560804\n",
      "Epoch 1 step 311: training accuarcy: 0.8150000000000001\n",
      "Epoch 1 step 311: training loss: 970.2076418352667\n",
      "Epoch 1 step 312: training accuarcy: 0.8\n",
      "Epoch 1 step 312: training loss: 969.6621306618093\n",
      "Epoch 1 step 313: training accuarcy: 0.7995\n",
      "Epoch 1 step 313: training loss: 968.5107792350655\n",
      "Epoch 1 step 314: training accuarcy: 0.804\n",
      "Epoch 1 step 314: training loss: 926.1903379542814\n",
      "Epoch 1 step 315: training accuarcy: 0.8135\n",
      "Epoch 1 step 315: training loss: 944.6543444484688\n",
      "Epoch 1 step 316: training accuarcy: 0.8125\n",
      "Epoch 1 step 316: training loss: 979.5799436237542\n",
      "Epoch 1 step 317: training accuarcy: 0.7865\n",
      "Epoch 1 step 317: training loss: 943.1074887301318\n",
      "Epoch 1 step 318: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 318: training loss: 953.1796370105519\n",
      "Epoch 1 step 319: training accuarcy: 0.803\n",
      "Epoch 1 step 319: training loss: 967.042661787043\n",
      "Epoch 1 step 320: training accuarcy: 0.7995\n",
      "Epoch 1 step 320: training loss: 951.2773874169641\n",
      "Epoch 1 step 321: training accuarcy: 0.806\n",
      "Epoch 1 step 321: training loss: 968.2184369891289\n",
      "Epoch 1 step 322: training accuarcy: 0.7955\n",
      "Epoch 1 step 322: training loss: 968.7574056842118\n",
      "Epoch 1 step 323: training accuarcy: 0.793\n",
      "Epoch 1 step 323: training loss: 958.4653967528717\n",
      "Epoch 1 step 324: training accuarcy: 0.7955\n",
      "Epoch 1 step 324: training loss: 937.6073201946\n",
      "Epoch 1 step 325: training accuarcy: 0.812\n",
      "Epoch 1 step 325: training loss: 956.7467555866848\n",
      "Epoch 1 step 326: training accuarcy: 0.8015\n",
      "Epoch 1 step 326: training loss: 947.6129490351016\n",
      "Epoch 1 step 327: training accuarcy: 0.804\n",
      "Epoch 1 step 327: training loss: 944.2791409380202\n",
      "Epoch 1 step 328: training accuarcy: 0.798\n",
      "Epoch 1 step 328: training loss: 978.3855271004462\n",
      "Epoch 1 step 329: training accuarcy: 0.7955\n",
      "Epoch 1 step 329: training loss: 969.299055822711\n",
      "Epoch 1 step 330: training accuarcy: 0.789\n",
      "Epoch 1 step 330: training loss: 968.1583027739466\n",
      "Epoch 1 step 331: training accuarcy: 0.7945\n",
      "Epoch 1 step 331: training loss: 951.3468477448588\n",
      "Epoch 1 step 332: training accuarcy: 0.8055\n",
      "Epoch 1 step 332: training loss: 941.2527685464956\n",
      "Epoch 1 step 333: training accuarcy: 0.8175\n",
      "Epoch 1 step 333: training loss: 970.2815934874245\n",
      "Epoch 1 step 334: training accuarcy: 0.7925\n",
      "Epoch 1 step 334: training loss: 969.297863384904\n",
      "Epoch 1 step 335: training accuarcy: 0.797\n",
      "Epoch 1 step 335: training loss: 970.3446747411999\n",
      "Epoch 1 step 336: training accuarcy: 0.7935\n",
      "Epoch 1 step 336: training loss: 964.8972781144962\n",
      "Epoch 1 step 337: training accuarcy: 0.7955\n",
      "Epoch 1 step 337: training loss: 969.9226577456288\n",
      "Epoch 1 step 338: training accuarcy: 0.7835\n",
      "Epoch 1 step 338: training loss: 944.8713206901162\n",
      "Epoch 1 step 339: training accuarcy: 0.802\n",
      "Epoch 1 step 339: training loss: 942.6613706548015\n",
      "Epoch 1 step 340: training accuarcy: 0.804\n",
      "Epoch 1 step 340: training loss: 943.8016912033653\n",
      "Epoch 1 step 341: training accuarcy: 0.8015\n",
      "Epoch 1 step 341: training loss: 967.8787080837488\n",
      "Epoch 1 step 342: training accuarcy: 0.7985\n",
      "Epoch 1 step 342: training loss: 990.9678462621906\n",
      "Epoch 1 step 343: training accuarcy: 0.7755\n",
      "Epoch 1 step 343: training loss: 952.0363119666803\n",
      "Epoch 1 step 344: training accuarcy: 0.8\n",
      "Epoch 1 step 344: training loss: 916.7883767792899\n",
      "Epoch 1 step 345: training accuarcy: 0.8290000000000001\n",
      "Epoch 1 step 345: training loss: 947.4478747122247\n",
      "Epoch 1 step 346: training accuarcy: 0.807\n",
      "Epoch 1 step 346: training loss: 941.0675009885224\n",
      "Epoch 1 step 347: training accuarcy: 0.8055\n",
      "Epoch 1 step 347: training loss: 958.88465518413\n",
      "Epoch 1 step 348: training accuarcy: 0.797\n",
      "Epoch 1 step 348: training loss: 939.266019383642\n",
      "Epoch 1 step 349: training accuarcy: 0.7975\n",
      "Epoch 1 step 349: training loss: 950.8717818009897\n",
      "Epoch 1 step 350: training accuarcy: 0.798\n",
      "Epoch 1 step 350: training loss: 959.7410874457715\n",
      "Epoch 1 step 351: training accuarcy: 0.8015\n",
      "Epoch 1 step 351: training loss: 939.7930656540661\n",
      "Epoch 1 step 352: training accuarcy: 0.7995\n",
      "Epoch 1 step 352: training loss: 928.928823646394\n",
      "Epoch 1 step 353: training accuarcy: 0.809\n",
      "Epoch 1 step 353: training loss: 940.3219438879934\n",
      "Epoch 1 step 354: training accuarcy: 0.809\n",
      "Epoch 1 step 354: training loss: 940.3015358129531\n",
      "Epoch 1 step 355: training accuarcy: 0.8025\n",
      "Epoch 1 step 355: training loss: 968.1069796320263\n",
      "Epoch 1 step 356: training accuarcy: 0.788\n",
      "Epoch 1 step 356: training loss: 937.4825802975965\n",
      "Epoch 1 step 357: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 357: training loss: 955.3890597764367\n",
      "Epoch 1 step 358: training accuarcy: 0.7975\n",
      "Epoch 1 step 358: training loss: 944.1715121254935\n",
      "Epoch 1 step 359: training accuarcy: 0.808\n",
      "Epoch 1 step 359: training loss: 933.8347758888716\n",
      "Epoch 1 step 360: training accuarcy: 0.804\n",
      "Epoch 1 step 360: training loss: 939.6722865493296\n",
      "Epoch 1 step 361: training accuarcy: 0.8185\n",
      "Epoch 1 step 361: training loss: 951.0383936829347\n",
      "Epoch 1 step 362: training accuarcy: 0.7955\n",
      "Epoch 1 step 362: training loss: 956.2790038183771\n",
      "Epoch 1 step 363: training accuarcy: 0.798\n",
      "Epoch 1 step 363: training loss: 937.7298053035425\n",
      "Epoch 1 step 364: training accuarcy: 0.8\n",
      "Epoch 1 step 364: training loss: 950.7118929488483\n",
      "Epoch 1 step 365: training accuarcy: 0.794\n",
      "Epoch 1 step 365: training loss: 941.7862002485521\n",
      "Epoch 1 step 366: training accuarcy: 0.805\n",
      "Epoch 1 step 366: training loss: 951.347708758121\n",
      "Epoch 1 step 367: training accuarcy: 0.7925\n",
      "Epoch 1 step 367: training loss: 935.434140683991\n",
      "Epoch 1 step 368: training accuarcy: 0.8035\n",
      "Epoch 1 step 368: training loss: 951.0877090468025\n",
      "Epoch 1 step 369: training accuarcy: 0.8015\n",
      "Epoch 1 step 369: training loss: 921.8178164790243\n",
      "Epoch 1 step 370: training accuarcy: 0.8095\n",
      "Epoch 1 step 370: training loss: 970.6076282343855\n",
      "Epoch 1 step 371: training accuarcy: 0.7875\n",
      "Epoch 1 step 371: training loss: 953.8891183334576\n",
      "Epoch 1 step 372: training accuarcy: 0.7945\n",
      "Epoch 1 step 372: training loss: 942.9652281278939\n",
      "Epoch 1 step 373: training accuarcy: 0.804\n",
      "Epoch 1 step 373: training loss: 936.6591442353787\n",
      "Epoch 1 step 374: training accuarcy: 0.8025\n",
      "Epoch 1 step 374: training loss: 936.2162562731104\n",
      "Epoch 1 step 375: training accuarcy: 0.8065\n",
      "Epoch 1 step 375: training loss: 960.5325249280536\n",
      "Epoch 1 step 376: training accuarcy: 0.7905\n",
      "Epoch 1 step 376: training loss: 914.1658470667687\n",
      "Epoch 1 step 377: training accuarcy: 0.8210000000000001\n",
      "Epoch 1 step 377: training loss: 952.869497642497\n",
      "Epoch 1 step 378: training accuarcy: 0.788\n",
      "Epoch 1 step 378: training loss: 936.3909806273913\n",
      "Epoch 1 step 379: training accuarcy: 0.8\n",
      "Epoch 1 step 379: training loss: 935.9300595345738\n",
      "Epoch 1 step 380: training accuarcy: 0.8140000000000001\n",
      "Epoch 1 step 380: training loss: 912.5004421218201\n",
      "Epoch 1 step 381: training accuarcy: 0.8065\n",
      "Epoch 1 step 381: training loss: 925.4782541803469\n",
      "Epoch 1 step 382: training accuarcy: 0.8025\n",
      "Epoch 1 step 382: training loss: 942.2457101190175\n",
      "Epoch 1 step 383: training accuarcy: 0.7965\n",
      "Epoch 1 step 383: training loss: 932.8718085021969\n",
      "Epoch 1 step 384: training accuarcy: 0.808\n",
      "Epoch 1 step 384: training loss: 942.30310038748\n",
      "Epoch 1 step 385: training accuarcy: 0.8035\n",
      "Epoch 1 step 385: training loss: 951.3529421956471\n",
      "Epoch 1 step 386: training accuarcy: 0.8045\n",
      "Epoch 1 step 386: training loss: 937.6604256704239\n",
      "Epoch 1 step 387: training accuarcy: 0.803\n",
      "Epoch 1 step 387: training loss: 925.1137362112374\n",
      "Epoch 1 step 388: training accuarcy: 0.809\n",
      "Epoch 1 step 388: training loss: 912.6404746063693\n",
      "Epoch 1 step 389: training accuarcy: 0.8035\n",
      "Epoch 1 step 389: training loss: 937.9285439075761\n",
      "Epoch 1 step 390: training accuarcy: 0.807\n",
      "Epoch 1 step 390: training loss: 921.35391197255\n",
      "Epoch 1 step 391: training accuarcy: 0.8135\n",
      "Epoch 1 step 391: training loss: 949.9008014535598\n",
      "Epoch 1 step 392: training accuarcy: 0.7945\n",
      "Epoch 1 step 392: training loss: 917.2238450169878\n",
      "Epoch 1 step 393: training accuarcy: 0.8015\n",
      "Epoch 1 step 393: training loss: 916.2184092415521\n",
      "Epoch 1 step 394: training accuarcy: 0.8085\n",
      "Epoch 1 step 394: training loss: 937.8858734511876\n",
      "Epoch 1 step 395: training accuarcy: 0.8005\n",
      "Epoch 1 step 395: training loss: 941.6326896315127\n",
      "Epoch 1 step 396: training accuarcy: 0.799\n",
      "Epoch 1 step 396: training loss: 934.7199448412409\n",
      "Epoch 1 step 397: training accuarcy: 0.7995\n",
      "Epoch 1 step 397: training loss: 896.5518311030104\n",
      "Epoch 1 step 398: training accuarcy: 0.8205\n",
      "Epoch 1 step 398: training loss: 945.6226002953235\n",
      "Epoch 1 step 399: training accuarcy: 0.7935\n",
      "Epoch 1 step 399: training loss: 920.882861414484\n",
      "Epoch 1 step 400: training accuarcy: 0.8085\n",
      "Epoch 1 step 400: training loss: 914.9200032784006\n",
      "Epoch 1 step 401: training accuarcy: 0.811\n",
      "Epoch 1 step 401: training loss: 910.2487511471386\n",
      "Epoch 1 step 402: training accuarcy: 0.8095\n",
      "Epoch 1 step 402: training loss: 919.1600305589118\n",
      "Epoch 1 step 403: training accuarcy: 0.8175\n",
      "Epoch 1 step 403: training loss: 937.5576225344529\n",
      "Epoch 1 step 404: training accuarcy: 0.7955\n",
      "Epoch 1 step 404: training loss: 919.2465895280275\n",
      "Epoch 1 step 405: training accuarcy: 0.8125\n",
      "Epoch 1 step 405: training loss: 921.6093277159958\n",
      "Epoch 1 step 406: training accuarcy: 0.8045\n",
      "Epoch 1 step 406: training loss: 937.8979919583327\n",
      "Epoch 1 step 407: training accuarcy: 0.807\n",
      "Epoch 1 step 407: training loss: 921.9845183353134\n",
      "Epoch 1 step 408: training accuarcy: 0.804\n",
      "Epoch 1 step 408: training loss: 947.904140442438\n",
      "Epoch 1 step 409: training accuarcy: 0.7955\n",
      "Epoch 1 step 409: training loss: 912.9436093734262\n",
      "Epoch 1 step 410: training accuarcy: 0.8115\n",
      "Epoch 1 step 410: training loss: 911.3383235941433\n",
      "Epoch 1 step 411: training accuarcy: 0.8125\n",
      "Epoch 1 step 411: training loss: 936.4861990949207\n",
      "Epoch 1 step 412: training accuarcy: 0.7975\n",
      "Epoch 1 step 412: training loss: 933.0653903369282\n",
      "Epoch 1 step 413: training accuarcy: 0.809\n",
      "Epoch 1 step 413: training loss: 915.9646951154637\n",
      "Epoch 1 step 414: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 414: training loss: 924.8916070634398\n",
      "Epoch 1 step 415: training accuarcy: 0.8150000000000001\n",
      "Epoch 1 step 415: training loss: 908.9119101488735\n",
      "Epoch 1 step 416: training accuarcy: 0.8145\n",
      "Epoch 1 step 416: training loss: 922.4731873765932\n",
      "Epoch 1 step 417: training accuarcy: 0.8005\n",
      "Epoch 1 step 417: training loss: 913.8617550076947\n",
      "Epoch 1 step 418: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 418: training loss: 906.8866662111049\n",
      "Epoch 1 step 419: training accuarcy: 0.8165\n",
      "Epoch 1 step 419: training loss: 922.1091455786659\n",
      "Epoch 1 step 420: training accuarcy: 0.799\n",
      "Epoch 1 step 420: training loss: 945.959534165528\n",
      "Epoch 1 step 421: training accuarcy: 0.804\n",
      "Epoch 1 step 421: training loss: 888.5537389367439\n",
      "Epoch 1 step 422: training accuarcy: 0.8150000000000001\n",
      "Epoch 1 step 422: training loss: 894.5591547413176\n",
      "Epoch 1 step 423: training accuarcy: 0.8220000000000001\n",
      "Epoch 1 step 423: training loss: 918.6947448245932\n",
      "Epoch 1 step 424: training accuarcy: 0.809\n",
      "Epoch 1 step 424: training loss: 872.4844575083742\n",
      "Epoch 1 step 425: training accuarcy: 0.8365\n",
      "Epoch 1 step 425: training loss: 924.3589328152015\n",
      "Epoch 1 step 426: training accuarcy: 0.802\n",
      "Epoch 1 step 426: training loss: 939.2774772256321\n",
      "Epoch 1 step 427: training accuarcy: 0.7945\n",
      "Epoch 1 step 427: training loss: 915.3645851679022\n",
      "Epoch 1 step 428: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 428: training loss: 904.3285171995814\n",
      "Epoch 1 step 429: training accuarcy: 0.8175\n",
      "Epoch 1 step 429: training loss: 907.0549746289603\n",
      "Epoch 1 step 430: training accuarcy: 0.8185\n",
      "Epoch 1 step 430: training loss: 926.1696315488589\n",
      "Epoch 1 step 431: training accuarcy: 0.804\n",
      "Epoch 1 step 431: training loss: 921.9289293093663\n",
      "Epoch 1 step 432: training accuarcy: 0.7965\n",
      "Epoch 1 step 432: training loss: 940.7365518159972\n",
      "Epoch 1 step 433: training accuarcy: 0.801\n",
      "Epoch 1 step 433: training loss: 894.5801148440013\n",
      "Epoch 1 step 434: training accuarcy: 0.8255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 434: training loss: 915.6342369499238\n",
      "Epoch 1 step 435: training accuarcy: 0.805\n",
      "Epoch 1 step 435: training loss: 885.4818930078619\n",
      "Epoch 1 step 436: training accuarcy: 0.8295\n",
      "Epoch 1 step 436: training loss: 921.8968032519256\n",
      "Epoch 1 step 437: training accuarcy: 0.8\n",
      "Epoch 1 step 437: training loss: 926.9152894747033\n",
      "Epoch 1 step 438: training accuarcy: 0.8055\n",
      "Epoch 1 step 438: training loss: 880.693478972882\n",
      "Epoch 1 step 439: training accuarcy: 0.8240000000000001\n",
      "Epoch 1 step 439: training loss: 870.7242582027243\n",
      "Epoch 1 step 440: training accuarcy: 0.8300000000000001\n",
      "Epoch 1 step 440: training loss: 932.6479720911238\n",
      "Epoch 1 step 441: training accuarcy: 0.805\n",
      "Epoch 1 step 441: training loss: 930.8609398827883\n",
      "Epoch 1 step 442: training accuarcy: 0.79\n",
      "Epoch 1 step 442: training loss: 902.011950630336\n",
      "Epoch 1 step 443: training accuarcy: 0.8145\n",
      "Epoch 1 step 443: training loss: 910.8092236093823\n",
      "Epoch 1 step 444: training accuarcy: 0.8115\n",
      "Epoch 1 step 444: training loss: 907.5459113044874\n",
      "Epoch 1 step 445: training accuarcy: 0.806\n",
      "Epoch 1 step 445: training loss: 899.9013172340759\n",
      "Epoch 1 step 446: training accuarcy: 0.809\n",
      "Epoch 1 step 446: training loss: 935.4835202867666\n",
      "Epoch 1 step 447: training accuarcy: 0.781\n",
      "Epoch 1 step 447: training loss: 932.2388146065932\n",
      "Epoch 1 step 448: training accuarcy: 0.8015\n",
      "Epoch 1 step 448: training loss: 888.9967642474976\n",
      "Epoch 1 step 449: training accuarcy: 0.8215\n",
      "Epoch 1 step 449: training loss: 896.3712045654295\n",
      "Epoch 1 step 450: training accuarcy: 0.8115\n",
      "Epoch 1 step 450: training loss: 906.9516502291317\n",
      "Epoch 1 step 451: training accuarcy: 0.8085\n",
      "Epoch 1 step 451: training loss: 926.518875936674\n",
      "Epoch 1 step 452: training accuarcy: 0.8055\n",
      "Epoch 1 step 452: training loss: 896.1471669386129\n",
      "Epoch 1 step 453: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 453: training loss: 903.0236803883255\n",
      "Epoch 1 step 454: training accuarcy: 0.8135\n",
      "Epoch 1 step 454: training loss: 910.4570528890227\n",
      "Epoch 1 step 455: training accuarcy: 0.7985\n",
      "Epoch 1 step 455: training loss: 916.6905392721122\n",
      "Epoch 1 step 456: training accuarcy: 0.8\n",
      "Epoch 1 step 456: training loss: 888.644256799868\n",
      "Epoch 1 step 457: training accuarcy: 0.8230000000000001\n",
      "Epoch 1 step 457: training loss: 910.9373919543998\n",
      "Epoch 1 step 458: training accuarcy: 0.8095\n",
      "Epoch 1 step 458: training loss: 876.2955754677255\n",
      "Epoch 1 step 459: training accuarcy: 0.8215\n",
      "Epoch 1 step 459: training loss: 909.8011121376614\n",
      "Epoch 1 step 460: training accuarcy: 0.812\n",
      "Epoch 1 step 460: training loss: 923.3029049028219\n",
      "Epoch 1 step 461: training accuarcy: 0.8055\n",
      "Epoch 1 step 461: training loss: 901.0071309076233\n",
      "Epoch 1 step 462: training accuarcy: 0.8075\n",
      "Epoch 1 step 462: training loss: 891.658978157895\n",
      "Epoch 1 step 463: training accuarcy: 0.811\n",
      "Epoch 1 step 463: training loss: 902.6462428213697\n",
      "Epoch 1 step 464: training accuarcy: 0.8055\n",
      "Epoch 1 step 464: training loss: 918.2684055501819\n",
      "Epoch 1 step 465: training accuarcy: 0.799\n",
      "Epoch 1 step 465: training loss: 918.857511604727\n",
      "Epoch 1 step 466: training accuarcy: 0.805\n",
      "Epoch 1 step 466: training loss: 898.6284284122821\n",
      "Epoch 1 step 467: training accuarcy: 0.8215\n",
      "Epoch 1 step 467: training loss: 906.5723257740599\n",
      "Epoch 1 step 468: training accuarcy: 0.8045\n",
      "Epoch 1 step 468: training loss: 894.3112882000252\n",
      "Epoch 1 step 469: training accuarcy: 0.7995\n",
      "Epoch 1 step 469: training loss: 919.9069982971466\n",
      "Epoch 1 step 470: training accuarcy: 0.8025\n",
      "Epoch 1 step 470: training loss: 916.4603871298983\n",
      "Epoch 1 step 471: training accuarcy: 0.8035\n",
      "Epoch 1 step 471: training loss: 933.108476170444\n",
      "Epoch 1 step 472: training accuarcy: 0.795\n",
      "Epoch 1 step 472: training loss: 913.8912252410336\n",
      "Epoch 1 step 473: training accuarcy: 0.806\n",
      "Epoch 1 step 473: training loss: 874.8664263867635\n",
      "Epoch 1 step 474: training accuarcy: 0.8240000000000001\n",
      "Epoch 1 step 474: training loss: 885.7294628253601\n",
      "Epoch 1 step 475: training accuarcy: 0.8230000000000001\n",
      "Epoch 1 step 475: training loss: 909.1329010094801\n",
      "Epoch 1 step 476: training accuarcy: 0.8065\n",
      "Epoch 1 step 476: training loss: 857.2437984675829\n",
      "Epoch 1 step 477: training accuarcy: 0.8305\n",
      "Epoch 1 step 477: training loss: 880.8734379434\n",
      "Epoch 1 step 478: training accuarcy: 0.8145\n",
      "Epoch 1 step 478: training loss: 878.9144141103945\n",
      "Epoch 1 step 479: training accuarcy: 0.8190000000000001\n",
      "Epoch 1 step 479: training loss: 918.7785675059289\n",
      "Epoch 1 step 480: training accuarcy: 0.8025\n",
      "Epoch 1 step 480: training loss: 930.7052117829851\n",
      "Epoch 1 step 481: training accuarcy: 0.7985\n",
      "Epoch 1 step 481: training loss: 885.4559885385233\n",
      "Epoch 1 step 482: training accuarcy: 0.811\n",
      "Epoch 1 step 482: training loss: 889.7213023655793\n",
      "Epoch 1 step 483: training accuarcy: 0.8175\n",
      "Epoch 1 step 483: training loss: 885.7981052666208\n",
      "Epoch 1 step 484: training accuarcy: 0.8190000000000001\n",
      "Epoch 1 step 484: training loss: 922.0987496064316\n",
      "Epoch 1 step 485: training accuarcy: 0.803\n",
      "Epoch 1 step 485: training loss: 891.8345118585848\n",
      "Epoch 1 step 486: training accuarcy: 0.807\n",
      "Epoch 1 step 486: training loss: 897.2903981632684\n",
      "Epoch 1 step 487: training accuarcy: 0.8175\n",
      "Epoch 1 step 487: training loss: 892.8032927756367\n",
      "Epoch 1 step 488: training accuarcy: 0.8200000000000001\n",
      "Epoch 1 step 488: training loss: 866.3113012517231\n",
      "Epoch 1 step 489: training accuarcy: 0.8280000000000001\n",
      "Epoch 1 step 489: training loss: 902.5375705083869\n",
      "Epoch 1 step 490: training accuarcy: 0.8035\n",
      "Epoch 1 step 490: training loss: 872.6728417489616\n",
      "Epoch 1 step 491: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 491: training loss: 871.2843748879059\n",
      "Epoch 1 step 492: training accuarcy: 0.833\n",
      "Epoch 1 step 492: training loss: 901.0084990249843\n",
      "Epoch 1 step 493: training accuarcy: 0.8145\n",
      "Epoch 1 step 493: training loss: 849.7258926275658\n",
      "Epoch 1 step 494: training accuarcy: 0.8265\n",
      "Epoch 1 step 494: training loss: 895.5202434097649\n",
      "Epoch 1 step 495: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 495: training loss: 884.1703010475971\n",
      "Epoch 1 step 496: training accuarcy: 0.8220000000000001\n",
      "Epoch 1 step 496: training loss: 907.1518772986575\n",
      "Epoch 1 step 497: training accuarcy: 0.812\n",
      "Epoch 1 step 497: training loss: 872.2367188638381\n",
      "Epoch 1 step 498: training accuarcy: 0.8135\n",
      "Epoch 1 step 498: training loss: 889.5158741102001\n",
      "Epoch 1 step 499: training accuarcy: 0.8195\n",
      "Epoch 1 step 499: training loss: 909.3100718058804\n",
      "Epoch 1 step 500: training accuarcy: 0.793\n",
      "Epoch 1 step 500: training loss: 875.0622097471374\n",
      "Epoch 1 step 501: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 501: training loss: 870.1154296071131\n",
      "Epoch 1 step 502: training accuarcy: 0.8170000000000001\n",
      "Epoch 1 step 502: training loss: 893.1824907903302\n",
      "Epoch 1 step 503: training accuarcy: 0.8140000000000001\n",
      "Epoch 1 step 503: training loss: 873.2346954779141\n",
      "Epoch 1 step 504: training accuarcy: 0.8235\n",
      "Epoch 1 step 504: training loss: 908.8416711714375\n",
      "Epoch 1 step 505: training accuarcy: 0.796\n",
      "Epoch 1 step 505: training loss: 888.7945036225116\n",
      "Epoch 1 step 506: training accuarcy: 0.8180000000000001\n",
      "Epoch 1 step 506: training loss: 888.8565525700604\n",
      "Epoch 1 step 507: training accuarcy: 0.8155\n",
      "Epoch 1 step 507: training loss: 896.7533669772008\n",
      "Epoch 1 step 508: training accuarcy: 0.808\n",
      "Epoch 1 step 508: training loss: 878.5022133234602\n",
      "Epoch 1 step 509: training accuarcy: 0.8240000000000001\n",
      "Epoch 1 step 509: training loss: 877.989763332877\n",
      "Epoch 1 step 510: training accuarcy: 0.8130000000000001\n",
      "Epoch 1 step 510: training loss: 881.1878902106114\n",
      "Epoch 1 step 511: training accuarcy: 0.8190000000000001\n",
      "Epoch 1 step 511: training loss: 869.3232602165872\n",
      "Epoch 1 step 512: training accuarcy: 0.8165\n",
      "Epoch 1 step 512: training loss: 882.5952988343997\n",
      "Epoch 1 step 513: training accuarcy: 0.8180000000000001\n",
      "Epoch 1 step 513: training loss: 902.1990519491507\n",
      "Epoch 1 step 514: training accuarcy: 0.805\n",
      "Epoch 1 step 514: training loss: 883.1765129798016\n",
      "Epoch 1 step 515: training accuarcy: 0.8190000000000001\n",
      "Epoch 1 step 515: training loss: 897.7491943638788\n",
      "Epoch 1 step 516: training accuarcy: 0.812\n",
      "Epoch 1 step 516: training loss: 907.0049547776216\n",
      "Epoch 1 step 517: training accuarcy: 0.795\n",
      "Epoch 1 step 517: training loss: 885.0305855787253\n",
      "Epoch 1 step 518: training accuarcy: 0.809\n",
      "Epoch 1 step 518: training loss: 847.6455863895205\n",
      "Epoch 1 step 519: training accuarcy: 0.834\n",
      "Epoch 1 step 519: training loss: 879.2882706613302\n",
      "Epoch 1 step 520: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 520: training loss: 881.0519361443079\n",
      "Epoch 1 step 521: training accuarcy: 0.8160000000000001\n",
      "Epoch 1 step 521: training loss: 882.7376274444861\n",
      "Epoch 1 step 522: training accuarcy: 0.8290000000000001\n",
      "Epoch 1 step 522: training loss: 884.1038942775203\n",
      "Epoch 1 step 523: training accuarcy: 0.8095\n",
      "Epoch 1 step 523: training loss: 859.9500563503879\n",
      "Epoch 1 step 524: training accuarcy: 0.8230000000000001\n",
      "Epoch 1 step 524: training loss: 921.6668859603786\n",
      "Epoch 1 step 525: training accuarcy: 0.7945\n",
      "Epoch 1 step 525: training loss: 366.212572380409\n",
      "Epoch 1 step 526: training accuarcy: 0.808974358974359\n",
      "Epoch 1: train loss 926.5457459866907, train accuarcy 0.780495822429657\n",
      "Epoch 1: valid loss 1030.1578735028932, valid accuarcy 0.7507580518722534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████                                                                                                                  | 2/8 [04:27<13:23, 133.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 796.1800074425452\n",
      "Epoch 2 step 527: training accuarcy: 0.85\n",
      "Epoch 2 step 527: training loss: 812.4522747391758\n",
      "Epoch 2 step 528: training accuarcy: 0.847\n",
      "Epoch 2 step 528: training loss: 780.7816190859206\n",
      "Epoch 2 step 529: training accuarcy: 0.854\n",
      "Epoch 2 step 529: training loss: 776.5738854869907\n",
      "Epoch 2 step 530: training accuarcy: 0.8525\n",
      "Epoch 2 step 530: training loss: 820.261246135934\n",
      "Epoch 2 step 531: training accuarcy: 0.8415\n",
      "Epoch 2 step 531: training loss: 796.089065489707\n",
      "Epoch 2 step 532: training accuarcy: 0.8455\n",
      "Epoch 2 step 532: training loss: 807.46911121999\n",
      "Epoch 2 step 533: training accuarcy: 0.85\n",
      "Epoch 2 step 533: training loss: 804.490586968923\n",
      "Epoch 2 step 534: training accuarcy: 0.8535\n",
      "Epoch 2 step 534: training loss: 783.8217581882924\n",
      "Epoch 2 step 535: training accuarcy: 0.8655\n",
      "Epoch 2 step 535: training loss: 796.2572558360356\n",
      "Epoch 2 step 536: training accuarcy: 0.846\n",
      "Epoch 2 step 536: training loss: 797.5429647445313\n",
      "Epoch 2 step 537: training accuarcy: 0.852\n",
      "Epoch 2 step 537: training loss: 785.9269450117723\n",
      "Epoch 2 step 538: training accuarcy: 0.855\n",
      "Epoch 2 step 538: training loss: 777.1120275225402\n",
      "Epoch 2 step 539: training accuarcy: 0.8535\n",
      "Epoch 2 step 539: training loss: 765.9641825566367\n",
      "Epoch 2 step 540: training accuarcy: 0.868\n",
      "Epoch 2 step 540: training loss: 792.5543227951563\n",
      "Epoch 2 step 541: training accuarcy: 0.8575\n",
      "Epoch 2 step 541: training loss: 771.7797940969472\n",
      "Epoch 2 step 542: training accuarcy: 0.854\n",
      "Epoch 2 step 542: training loss: 779.7850392772558\n",
      "Epoch 2 step 543: training accuarcy: 0.8625\n",
      "Epoch 2 step 543: training loss: 807.4489979849913\n",
      "Epoch 2 step 544: training accuarcy: 0.8405\n",
      "Epoch 2 step 544: training loss: 781.2263413252878\n",
      "Epoch 2 step 545: training accuarcy: 0.863\n",
      "Epoch 2 step 545: training loss: 785.0842019010652\n",
      "Epoch 2 step 546: training accuarcy: 0.8525\n",
      "Epoch 2 step 546: training loss: 806.8972463462004\n",
      "Epoch 2 step 547: training accuarcy: 0.85\n",
      "Epoch 2 step 547: training loss: 789.6397449029377\n",
      "Epoch 2 step 548: training accuarcy: 0.847\n",
      "Epoch 2 step 548: training loss: 817.4891479424759\n",
      "Epoch 2 step 549: training accuarcy: 0.849\n",
      "Epoch 2 step 549: training loss: 806.1026027410845\n",
      "Epoch 2 step 550: training accuarcy: 0.8415\n",
      "Epoch 2 step 550: training loss: 824.2791145610278\n",
      "Epoch 2 step 551: training accuarcy: 0.836\n",
      "Epoch 2 step 551: training loss: 796.2750680470148\n",
      "Epoch 2 step 552: training accuarcy: 0.8505\n",
      "Epoch 2 step 552: training loss: 784.3591450339752\n",
      "Epoch 2 step 553: training accuarcy: 0.847\n",
      "Epoch 2 step 553: training loss: 778.5189321969697\n",
      "Epoch 2 step 554: training accuarcy: 0.869\n",
      "Epoch 2 step 554: training loss: 780.5581326098114\n",
      "Epoch 2 step 555: training accuarcy: 0.8595\n",
      "Epoch 2 step 555: training loss: 773.2501561867882\n",
      "Epoch 2 step 556: training accuarcy: 0.8615\n",
      "Epoch 2 step 556: training loss: 796.966390189815\n",
      "Epoch 2 step 557: training accuarcy: 0.85\n",
      "Epoch 2 step 557: training loss: 795.8470273280439\n",
      "Epoch 2 step 558: training accuarcy: 0.8335\n",
      "Epoch 2 step 558: training loss: 779.8341563612703\n",
      "Epoch 2 step 559: training accuarcy: 0.867\n",
      "Epoch 2 step 559: training loss: 814.8248293079744\n",
      "Epoch 2 step 560: training accuarcy: 0.8335\n",
      "Epoch 2 step 560: training loss: 798.6936999691505\n",
      "Epoch 2 step 561: training accuarcy: 0.8435\n",
      "Epoch 2 step 561: training loss: 803.0200896508421\n",
      "Epoch 2 step 562: training accuarcy: 0.8485\n",
      "Epoch 2 step 562: training loss: 776.9975872058865\n",
      "Epoch 2 step 563: training accuarcy: 0.859\n",
      "Epoch 2 step 563: training loss: 794.2429341944941\n",
      "Epoch 2 step 564: training accuarcy: 0.85\n",
      "Epoch 2 step 564: training loss: 816.5554855231147\n",
      "Epoch 2 step 565: training accuarcy: 0.842\n",
      "Epoch 2 step 565: training loss: 792.3792507684763\n",
      "Epoch 2 step 566: training accuarcy: 0.8485\n",
      "Epoch 2 step 566: training loss: 802.8030237370067\n",
      "Epoch 2 step 567: training accuarcy: 0.8435\n",
      "Epoch 2 step 567: training loss: 765.1355112632274\n",
      "Epoch 2 step 568: training accuarcy: 0.865\n",
      "Epoch 2 step 568: training loss: 795.8728498697774\n",
      "Epoch 2 step 569: training accuarcy: 0.847\n",
      "Epoch 2 step 569: training loss: 786.4804795969313\n",
      "Epoch 2 step 570: training accuarcy: 0.8525\n",
      "Epoch 2 step 570: training loss: 783.0475673099291\n",
      "Epoch 2 step 571: training accuarcy: 0.851\n",
      "Epoch 2 step 571: training loss: 777.4583192272395\n",
      "Epoch 2 step 572: training accuarcy: 0.8535\n",
      "Epoch 2 step 572: training loss: 804.7296217485965\n",
      "Epoch 2 step 573: training accuarcy: 0.839\n",
      "Epoch 2 step 573: training loss: 759.3682774132584\n",
      "Epoch 2 step 574: training accuarcy: 0.8605\n",
      "Epoch 2 step 574: training loss: 775.3414939414196\n",
      "Epoch 2 step 575: training accuarcy: 0.849\n",
      "Epoch 2 step 575: training loss: 798.8087537101989\n",
      "Epoch 2 step 576: training accuarcy: 0.841\n",
      "Epoch 2 step 576: training loss: 792.3110181076376\n",
      "Epoch 2 step 577: training accuarcy: 0.844\n",
      "Epoch 2 step 577: training loss: 781.6918829587345\n",
      "Epoch 2 step 578: training accuarcy: 0.856\n",
      "Epoch 2 step 578: training loss: 809.259773019416\n",
      "Epoch 2 step 579: training accuarcy: 0.8355\n",
      "Epoch 2 step 579: training loss: 782.229389121675\n",
      "Epoch 2 step 580: training accuarcy: 0.854\n",
      "Epoch 2 step 580: training loss: 803.3952065318404\n",
      "Epoch 2 step 581: training accuarcy: 0.846\n",
      "Epoch 2 step 581: training loss: 783.9860159672567\n",
      "Epoch 2 step 582: training accuarcy: 0.844\n",
      "Epoch 2 step 582: training loss: 802.3450089465293\n",
      "Epoch 2 step 583: training accuarcy: 0.8505\n",
      "Epoch 2 step 583: training loss: 777.0472496151967\n",
      "Epoch 2 step 584: training accuarcy: 0.858\n",
      "Epoch 2 step 584: training loss: 779.0114278335346\n",
      "Epoch 2 step 585: training accuarcy: 0.8495\n",
      "Epoch 2 step 585: training loss: 810.0261401462268\n",
      "Epoch 2 step 586: training accuarcy: 0.8425\n",
      "Epoch 2 step 586: training loss: 788.1766293224549\n",
      "Epoch 2 step 587: training accuarcy: 0.8475\n",
      "Epoch 2 step 587: training loss: 785.4503132102133\n",
      "Epoch 2 step 588: training accuarcy: 0.8485\n",
      "Epoch 2 step 588: training loss: 753.5111042541085\n",
      "Epoch 2 step 589: training accuarcy: 0.8645\n",
      "Epoch 2 step 589: training loss: 773.5931075631677\n",
      "Epoch 2 step 590: training accuarcy: 0.8515\n",
      "Epoch 2 step 590: training loss: 778.829069920378\n",
      "Epoch 2 step 591: training accuarcy: 0.848\n",
      "Epoch 2 step 591: training loss: 798.359446113896\n",
      "Epoch 2 step 592: training accuarcy: 0.8475\n",
      "Epoch 2 step 592: training loss: 775.0327734993836\n",
      "Epoch 2 step 593: training accuarcy: 0.8505\n",
      "Epoch 2 step 593: training loss: 777.4259914029711\n",
      "Epoch 2 step 594: training accuarcy: 0.8435\n",
      "Epoch 2 step 594: training loss: 786.5295927071585\n",
      "Epoch 2 step 595: training accuarcy: 0.847\n",
      "Epoch 2 step 595: training loss: 788.057773779904\n",
      "Epoch 2 step 596: training accuarcy: 0.854\n",
      "Epoch 2 step 596: training loss: 800.1674212347297\n",
      "Epoch 2 step 597: training accuarcy: 0.8415\n",
      "Epoch 2 step 597: training loss: 776.8951370046502\n",
      "Epoch 2 step 598: training accuarcy: 0.859\n",
      "Epoch 2 step 598: training loss: 777.4886086123083\n",
      "Epoch 2 step 599: training accuarcy: 0.8585\n",
      "Epoch 2 step 599: training loss: 810.5682967054661\n",
      "Epoch 2 step 600: training accuarcy: 0.8355\n",
      "Epoch 2 step 600: training loss: 774.3854857451448\n",
      "Epoch 2 step 601: training accuarcy: 0.856\n",
      "Epoch 2 step 601: training loss: 767.5171351513667\n",
      "Epoch 2 step 602: training accuarcy: 0.8645\n",
      "Epoch 2 step 602: training loss: 760.8901683672561\n",
      "Epoch 2 step 603: training accuarcy: 0.8595\n",
      "Epoch 2 step 603: training loss: 785.8663314723315\n",
      "Epoch 2 step 604: training accuarcy: 0.8535\n",
      "Epoch 2 step 604: training loss: 768.8507269270973\n",
      "Epoch 2 step 605: training accuarcy: 0.854\n",
      "Epoch 2 step 605: training loss: 771.812861281134\n",
      "Epoch 2 step 606: training accuarcy: 0.8495\n",
      "Epoch 2 step 606: training loss: 769.831779972039\n",
      "Epoch 2 step 607: training accuarcy: 0.857\n",
      "Epoch 2 step 607: training loss: 794.7295236064163\n",
      "Epoch 2 step 608: training accuarcy: 0.845\n",
      "Epoch 2 step 608: training loss: 772.40084148998\n",
      "Epoch 2 step 609: training accuarcy: 0.841\n",
      "Epoch 2 step 609: training loss: 787.4582924583556\n",
      "Epoch 2 step 610: training accuarcy: 0.847\n",
      "Epoch 2 step 610: training loss: 777.5521872534661\n",
      "Epoch 2 step 611: training accuarcy: 0.845\n",
      "Epoch 2 step 611: training loss: 755.0290466317182\n",
      "Epoch 2 step 612: training accuarcy: 0.8575\n",
      "Epoch 2 step 612: training loss: 784.1157688799981\n",
      "Epoch 2 step 613: training accuarcy: 0.852\n",
      "Epoch 2 step 613: training loss: 774.1487747745001\n",
      "Epoch 2 step 614: training accuarcy: 0.859\n",
      "Epoch 2 step 614: training loss: 749.2056961632467\n",
      "Epoch 2 step 615: training accuarcy: 0.8595\n",
      "Epoch 2 step 615: training loss: 782.1607213728528\n",
      "Epoch 2 step 616: training accuarcy: 0.8575\n",
      "Epoch 2 step 616: training loss: 792.8569721290753\n",
      "Epoch 2 step 617: training accuarcy: 0.8465\n",
      "Epoch 2 step 617: training loss: 777.4729972415197\n",
      "Epoch 2 step 618: training accuarcy: 0.851\n",
      "Epoch 2 step 618: training loss: 760.3674300183445\n",
      "Epoch 2 step 619: training accuarcy: 0.859\n",
      "Epoch 2 step 619: training loss: 787.8744260398984\n",
      "Epoch 2 step 620: training accuarcy: 0.845\n",
      "Epoch 2 step 620: training loss: 774.7002469387361\n",
      "Epoch 2 step 621: training accuarcy: 0.8615\n",
      "Epoch 2 step 621: training loss: 779.4076274004615\n",
      "Epoch 2 step 622: training accuarcy: 0.8545\n",
      "Epoch 2 step 622: training loss: 776.9889290073825\n",
      "Epoch 2 step 623: training accuarcy: 0.8525\n",
      "Epoch 2 step 623: training loss: 776.0932673511003\n",
      "Epoch 2 step 624: training accuarcy: 0.854\n",
      "Epoch 2 step 624: training loss: 785.0107807753343\n",
      "Epoch 2 step 625: training accuarcy: 0.851\n",
      "Epoch 2 step 625: training loss: 780.975754353276\n",
      "Epoch 2 step 626: training accuarcy: 0.848\n",
      "Epoch 2 step 626: training loss: 775.8098861252827\n",
      "Epoch 2 step 627: training accuarcy: 0.8525\n",
      "Epoch 2 step 627: training loss: 761.3655655248332\n",
      "Epoch 2 step 628: training accuarcy: 0.862\n",
      "Epoch 2 step 628: training loss: 770.0572732243741\n",
      "Epoch 2 step 629: training accuarcy: 0.853\n",
      "Epoch 2 step 629: training loss: 778.2999160918622\n",
      "Epoch 2 step 630: training accuarcy: 0.854\n",
      "Epoch 2 step 630: training loss: 772.7535712361791\n",
      "Epoch 2 step 631: training accuarcy: 0.8525\n",
      "Epoch 2 step 631: training loss: 769.3868537490658\n",
      "Epoch 2 step 632: training accuarcy: 0.855\n",
      "Epoch 2 step 632: training loss: 771.6138539556387\n",
      "Epoch 2 step 633: training accuarcy: 0.856\n",
      "Epoch 2 step 633: training loss: 765.0664224499462\n",
      "Epoch 2 step 634: training accuarcy: 0.8615\n",
      "Epoch 2 step 634: training loss: 780.3875188627078\n",
      "Epoch 2 step 635: training accuarcy: 0.8495\n",
      "Epoch 2 step 635: training loss: 780.7319629310111\n",
      "Epoch 2 step 636: training accuarcy: 0.8555\n",
      "Epoch 2 step 636: training loss: 782.7540875841175\n",
      "Epoch 2 step 637: training accuarcy: 0.8525\n",
      "Epoch 2 step 637: training loss: 787.976686306184\n",
      "Epoch 2 step 638: training accuarcy: 0.849\n",
      "Epoch 2 step 638: training loss: 759.6975334118107\n",
      "Epoch 2 step 639: training accuarcy: 0.854\n",
      "Epoch 2 step 639: training loss: 761.6121343558058\n",
      "Epoch 2 step 640: training accuarcy: 0.865\n",
      "Epoch 2 step 640: training loss: 760.9241653678525\n",
      "Epoch 2 step 641: training accuarcy: 0.858\n",
      "Epoch 2 step 641: training loss: 763.568208564113\n",
      "Epoch 2 step 642: training accuarcy: 0.8475\n",
      "Epoch 2 step 642: training loss: 739.5957405528545\n",
      "Epoch 2 step 643: training accuarcy: 0.869\n",
      "Epoch 2 step 643: training loss: 789.1622131039193\n",
      "Epoch 2 step 644: training accuarcy: 0.85\n",
      "Epoch 2 step 644: training loss: 764.8013934267304\n",
      "Epoch 2 step 645: training accuarcy: 0.8505\n",
      "Epoch 2 step 645: training loss: 779.3581441849065\n",
      "Epoch 2 step 646: training accuarcy: 0.8465\n",
      "Epoch 2 step 646: training loss: 795.4815743276704\n",
      "Epoch 2 step 647: training accuarcy: 0.8495\n",
      "Epoch 2 step 647: training loss: 743.9794967730041\n",
      "Epoch 2 step 648: training accuarcy: 0.872\n",
      "Epoch 2 step 648: training loss: 774.6925177390859\n",
      "Epoch 2 step 649: training accuarcy: 0.8445\n",
      "Epoch 2 step 649: training loss: 771.5479087456979\n",
      "Epoch 2 step 650: training accuarcy: 0.857\n",
      "Epoch 2 step 650: training loss: 764.1144426749715\n",
      "Epoch 2 step 651: training accuarcy: 0.8565\n",
      "Epoch 2 step 651: training loss: 781.0216742555757\n",
      "Epoch 2 step 652: training accuarcy: 0.856\n",
      "Epoch 2 step 652: training loss: 786.676372529573\n",
      "Epoch 2 step 653: training accuarcy: 0.8475\n",
      "Epoch 2 step 653: training loss: 769.0049395151542\n",
      "Epoch 2 step 654: training accuarcy: 0.854\n",
      "Epoch 2 step 654: training loss: 783.6552562304842\n",
      "Epoch 2 step 655: training accuarcy: 0.856\n",
      "Epoch 2 step 655: training loss: 753.9610330910269\n",
      "Epoch 2 step 656: training accuarcy: 0.8645\n",
      "Epoch 2 step 656: training loss: 761.3682643799087\n",
      "Epoch 2 step 657: training accuarcy: 0.8575\n",
      "Epoch 2 step 657: training loss: 743.9377425817737\n",
      "Epoch 2 step 658: training accuarcy: 0.866\n",
      "Epoch 2 step 658: training loss: 745.9318550898979\n",
      "Epoch 2 step 659: training accuarcy: 0.8695\n",
      "Epoch 2 step 659: training loss: 768.9552388290748\n",
      "Epoch 2 step 660: training accuarcy: 0.858\n",
      "Epoch 2 step 660: training loss: 759.2576830308204\n",
      "Epoch 2 step 661: training accuarcy: 0.8595\n",
      "Epoch 2 step 661: training loss: 751.0162538979622\n",
      "Epoch 2 step 662: training accuarcy: 0.8615\n",
      "Epoch 2 step 662: training loss: 762.1051088166209\n",
      "Epoch 2 step 663: training accuarcy: 0.8525\n",
      "Epoch 2 step 663: training loss: 786.0133588413612\n",
      "Epoch 2 step 664: training accuarcy: 0.854\n",
      "Epoch 2 step 664: training loss: 756.8745579189905\n",
      "Epoch 2 step 665: training accuarcy: 0.855\n",
      "Epoch 2 step 665: training loss: 770.9939005549421\n",
      "Epoch 2 step 666: training accuarcy: 0.856\n",
      "Epoch 2 step 666: training loss: 746.0445930610174\n",
      "Epoch 2 step 667: training accuarcy: 0.8695\n",
      "Epoch 2 step 667: training loss: 734.9015563415734\n",
      "Epoch 2 step 668: training accuarcy: 0.8585\n",
      "Epoch 2 step 668: training loss: 768.0104518835254\n",
      "Epoch 2 step 669: training accuarcy: 0.8495\n",
      "Epoch 2 step 669: training loss: 768.7262361666657\n",
      "Epoch 2 step 670: training accuarcy: 0.845\n",
      "Epoch 2 step 670: training loss: 759.9097026431725\n",
      "Epoch 2 step 671: training accuarcy: 0.8605\n",
      "Epoch 2 step 671: training loss: 771.7165326183127\n",
      "Epoch 2 step 672: training accuarcy: 0.854\n",
      "Epoch 2 step 672: training loss: 767.9320838988161\n",
      "Epoch 2 step 673: training accuarcy: 0.8535\n",
      "Epoch 2 step 673: training loss: 754.1070105628121\n",
      "Epoch 2 step 674: training accuarcy: 0.8575\n",
      "Epoch 2 step 674: training loss: 746.0823090282474\n",
      "Epoch 2 step 675: training accuarcy: 0.8665\n",
      "Epoch 2 step 675: training loss: 776.3511752785063\n",
      "Epoch 2 step 676: training accuarcy: 0.851\n",
      "Epoch 2 step 676: training loss: 755.0862092999873\n",
      "Epoch 2 step 677: training accuarcy: 0.86\n",
      "Epoch 2 step 677: training loss: 761.239720387776\n",
      "Epoch 2 step 678: training accuarcy: 0.847\n",
      "Epoch 2 step 678: training loss: 728.6638007863052\n",
      "Epoch 2 step 679: training accuarcy: 0.865\n",
      "Epoch 2 step 679: training loss: 769.9685451715546\n",
      "Epoch 2 step 680: training accuarcy: 0.8515\n",
      "Epoch 2 step 680: training loss: 773.2159675184118\n",
      "Epoch 2 step 681: training accuarcy: 0.8505\n",
      "Epoch 2 step 681: training loss: 757.4394793083762\n",
      "Epoch 2 step 682: training accuarcy: 0.863\n",
      "Epoch 2 step 682: training loss: 726.3250747838565\n",
      "Epoch 2 step 683: training accuarcy: 0.867\n",
      "Epoch 2 step 683: training loss: 730.7210961761962\n",
      "Epoch 2 step 684: training accuarcy: 0.8665\n",
      "Epoch 2 step 684: training loss: 745.9295965504441\n",
      "Epoch 2 step 685: training accuarcy: 0.865\n",
      "Epoch 2 step 685: training loss: 780.9295685746807\n",
      "Epoch 2 step 686: training accuarcy: 0.844\n",
      "Epoch 2 step 686: training loss: 742.7374691922289\n",
      "Epoch 2 step 687: training accuarcy: 0.865\n",
      "Epoch 2 step 687: training loss: 762.2696966759561\n",
      "Epoch 2 step 688: training accuarcy: 0.863\n",
      "Epoch 2 step 688: training loss: 769.6379545550515\n",
      "Epoch 2 step 689: training accuarcy: 0.8505\n",
      "Epoch 2 step 689: training loss: 755.3367875041054\n",
      "Epoch 2 step 690: training accuarcy: 0.8565\n",
      "Epoch 2 step 690: training loss: 769.0627680952451\n",
      "Epoch 2 step 691: training accuarcy: 0.845\n",
      "Epoch 2 step 691: training loss: 776.2723836262605\n",
      "Epoch 2 step 692: training accuarcy: 0.8405\n",
      "Epoch 2 step 692: training loss: 754.9158066905314\n",
      "Epoch 2 step 693: training accuarcy: 0.8665\n",
      "Epoch 2 step 693: training loss: 736.5276547988788\n",
      "Epoch 2 step 694: training accuarcy: 0.8675\n",
      "Epoch 2 step 694: training loss: 752.15397998294\n",
      "Epoch 2 step 695: training accuarcy: 0.8595\n",
      "Epoch 2 step 695: training loss: 776.0422431623896\n",
      "Epoch 2 step 696: training accuarcy: 0.8525\n",
      "Epoch 2 step 696: training loss: 731.1373060755727\n",
      "Epoch 2 step 697: training accuarcy: 0.8675\n",
      "Epoch 2 step 697: training loss: 755.672278102905\n",
      "Epoch 2 step 698: training accuarcy: 0.8555\n",
      "Epoch 2 step 698: training loss: 738.0780874973877\n",
      "Epoch 2 step 699: training accuarcy: 0.8635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 699: training loss: 743.8939994949307\n",
      "Epoch 2 step 700: training accuarcy: 0.859\n",
      "Epoch 2 step 700: training loss: 767.8181413563457\n",
      "Epoch 2 step 701: training accuarcy: 0.845\n",
      "Epoch 2 step 701: training loss: 756.4298216116127\n",
      "Epoch 2 step 702: training accuarcy: 0.853\n",
      "Epoch 2 step 702: training loss: 770.4160470194092\n",
      "Epoch 2 step 703: training accuarcy: 0.8555\n",
      "Epoch 2 step 703: training loss: 743.8601747379605\n",
      "Epoch 2 step 704: training accuarcy: 0.8615\n",
      "Epoch 2 step 704: training loss: 738.7535914079012\n",
      "Epoch 2 step 705: training accuarcy: 0.8685\n",
      "Epoch 2 step 705: training loss: 744.9842776048594\n",
      "Epoch 2 step 706: training accuarcy: 0.8625\n",
      "Epoch 2 step 706: training loss: 739.8717323576851\n",
      "Epoch 2 step 707: training accuarcy: 0.8685\n",
      "Epoch 2 step 707: training loss: 731.1224802882629\n",
      "Epoch 2 step 708: training accuarcy: 0.8585\n",
      "Epoch 2 step 708: training loss: 743.7190371733068\n",
      "Epoch 2 step 709: training accuarcy: 0.852\n",
      "Epoch 2 step 709: training loss: 747.0002384990096\n",
      "Epoch 2 step 710: training accuarcy: 0.859\n",
      "Epoch 2 step 710: training loss: 748.294675320831\n",
      "Epoch 2 step 711: training accuarcy: 0.858\n",
      "Epoch 2 step 711: training loss: 744.4939607846619\n",
      "Epoch 2 step 712: training accuarcy: 0.856\n",
      "Epoch 2 step 712: training loss: 778.8816227379613\n",
      "Epoch 2 step 713: training accuarcy: 0.845\n",
      "Epoch 2 step 713: training loss: 772.1955057683982\n",
      "Epoch 2 step 714: training accuarcy: 0.851\n",
      "Epoch 2 step 714: training loss: 720.3522673125993\n",
      "Epoch 2 step 715: training accuarcy: 0.874\n",
      "Epoch 2 step 715: training loss: 766.7374839044995\n",
      "Epoch 2 step 716: training accuarcy: 0.8505\n",
      "Epoch 2 step 716: training loss: 781.6398505607975\n",
      "Epoch 2 step 717: training accuarcy: 0.839\n",
      "Epoch 2 step 717: training loss: 753.8787228258934\n",
      "Epoch 2 step 718: training accuarcy: 0.856\n",
      "Epoch 2 step 718: training loss: 756.5417712310484\n",
      "Epoch 2 step 719: training accuarcy: 0.8625\n",
      "Epoch 2 step 719: training loss: 743.6211990968493\n",
      "Epoch 2 step 720: training accuarcy: 0.8645\n",
      "Epoch 2 step 720: training loss: 766.0375005627575\n",
      "Epoch 2 step 721: training accuarcy: 0.858\n",
      "Epoch 2 step 721: training loss: 727.17240834098\n",
      "Epoch 2 step 722: training accuarcy: 0.8745\n",
      "Epoch 2 step 722: training loss: 739.6107849459924\n",
      "Epoch 2 step 723: training accuarcy: 0.8635\n",
      "Epoch 2 step 723: training loss: 743.2353859847403\n",
      "Epoch 2 step 724: training accuarcy: 0.8635\n",
      "Epoch 2 step 724: training loss: 741.0599474673826\n",
      "Epoch 2 step 725: training accuarcy: 0.8695\n",
      "Epoch 2 step 725: training loss: 755.7625036536975\n",
      "Epoch 2 step 726: training accuarcy: 0.8605\n",
      "Epoch 2 step 726: training loss: 743.795561573342\n",
      "Epoch 2 step 727: training accuarcy: 0.8585\n",
      "Epoch 2 step 727: training loss: 703.9519240225426\n",
      "Epoch 2 step 728: training accuarcy: 0.877\n",
      "Epoch 2 step 728: training loss: 763.6234866267303\n",
      "Epoch 2 step 729: training accuarcy: 0.857\n",
      "Epoch 2 step 729: training loss: 747.2374650007444\n",
      "Epoch 2 step 730: training accuarcy: 0.8595\n",
      "Epoch 2 step 730: training loss: 721.1882187360585\n",
      "Epoch 2 step 731: training accuarcy: 0.873\n",
      "Epoch 2 step 731: training loss: 714.3015299095359\n",
      "Epoch 2 step 732: training accuarcy: 0.881\n",
      "Epoch 2 step 732: training loss: 758.2395626960665\n",
      "Epoch 2 step 733: training accuarcy: 0.849\n",
      "Epoch 2 step 733: training loss: 732.9036383942816\n",
      "Epoch 2 step 734: training accuarcy: 0.8695\n",
      "Epoch 2 step 734: training loss: 749.9274516733692\n",
      "Epoch 2 step 735: training accuarcy: 0.857\n",
      "Epoch 2 step 735: training loss: 726.6082130647709\n",
      "Epoch 2 step 736: training accuarcy: 0.8705\n",
      "Epoch 2 step 736: training loss: 764.442533394659\n",
      "Epoch 2 step 737: training accuarcy: 0.85\n",
      "Epoch 2 step 737: training loss: 725.0888468231249\n",
      "Epoch 2 step 738: training accuarcy: 0.868\n",
      "Epoch 2 step 738: training loss: 759.2056667610639\n",
      "Epoch 2 step 739: training accuarcy: 0.8585\n",
      "Epoch 2 step 739: training loss: 750.6478810998636\n",
      "Epoch 2 step 740: training accuarcy: 0.855\n",
      "Epoch 2 step 740: training loss: 751.1540865993941\n",
      "Epoch 2 step 741: training accuarcy: 0.8615\n",
      "Epoch 2 step 741: training loss: 752.9432339050387\n",
      "Epoch 2 step 742: training accuarcy: 0.8695\n",
      "Epoch 2 step 742: training loss: 729.5136364054541\n",
      "Epoch 2 step 743: training accuarcy: 0.8675\n",
      "Epoch 2 step 743: training loss: 743.3728313632253\n",
      "Epoch 2 step 744: training accuarcy: 0.854\n",
      "Epoch 2 step 744: training loss: 734.5572913104745\n",
      "Epoch 2 step 745: training accuarcy: 0.8625\n",
      "Epoch 2 step 745: training loss: 765.740091376465\n",
      "Epoch 2 step 746: training accuarcy: 0.8535\n",
      "Epoch 2 step 746: training loss: 750.351437346983\n",
      "Epoch 2 step 747: training accuarcy: 0.861\n",
      "Epoch 2 step 747: training loss: 724.0932876294044\n",
      "Epoch 2 step 748: training accuarcy: 0.8675\n",
      "Epoch 2 step 748: training loss: 749.4197439398228\n",
      "Epoch 2 step 749: training accuarcy: 0.853\n",
      "Epoch 2 step 749: training loss: 779.1412835889441\n",
      "Epoch 2 step 750: training accuarcy: 0.8455\n",
      "Epoch 2 step 750: training loss: 761.9957352863408\n",
      "Epoch 2 step 751: training accuarcy: 0.8525\n",
      "Epoch 2 step 751: training loss: 734.0107120510773\n",
      "Epoch 2 step 752: training accuarcy: 0.8625\n",
      "Epoch 2 step 752: training loss: 720.8442807286928\n",
      "Epoch 2 step 753: training accuarcy: 0.8705\n",
      "Epoch 2 step 753: training loss: 761.5228387718412\n",
      "Epoch 2 step 754: training accuarcy: 0.854\n",
      "Epoch 2 step 754: training loss: 749.6025699534661\n",
      "Epoch 2 step 755: training accuarcy: 0.852\n",
      "Epoch 2 step 755: training loss: 751.5843721464942\n",
      "Epoch 2 step 756: training accuarcy: 0.856\n",
      "Epoch 2 step 756: training loss: 721.2138039301998\n",
      "Epoch 2 step 757: training accuarcy: 0.8705\n",
      "Epoch 2 step 757: training loss: 735.3401843551148\n",
      "Epoch 2 step 758: training accuarcy: 0.859\n",
      "Epoch 2 step 758: training loss: 738.8078575248521\n",
      "Epoch 2 step 759: training accuarcy: 0.8535\n",
      "Epoch 2 step 759: training loss: 706.5600499310553\n",
      "Epoch 2 step 760: training accuarcy: 0.874\n",
      "Epoch 2 step 760: training loss: 729.2526697820941\n",
      "Epoch 2 step 761: training accuarcy: 0.866\n",
      "Epoch 2 step 761: training loss: 767.9226867603619\n",
      "Epoch 2 step 762: training accuarcy: 0.8545\n",
      "Epoch 2 step 762: training loss: 716.342706516616\n",
      "Epoch 2 step 763: training accuarcy: 0.8705\n",
      "Epoch 2 step 763: training loss: 744.613310420205\n",
      "Epoch 2 step 764: training accuarcy: 0.8625\n",
      "Epoch 2 step 764: training loss: 737.2714397729812\n",
      "Epoch 2 step 765: training accuarcy: 0.8605\n",
      "Epoch 2 step 765: training loss: 741.2447091633207\n",
      "Epoch 2 step 766: training accuarcy: 0.8675\n",
      "Epoch 2 step 766: training loss: 741.7135623904335\n",
      "Epoch 2 step 767: training accuarcy: 0.8615\n",
      "Epoch 2 step 767: training loss: 735.1288018654847\n",
      "Epoch 2 step 768: training accuarcy: 0.8625\n",
      "Epoch 2 step 768: training loss: 739.5984370721707\n",
      "Epoch 2 step 769: training accuarcy: 0.863\n",
      "Epoch 2 step 769: training loss: 724.6353805252223\n",
      "Epoch 2 step 770: training accuarcy: 0.863\n",
      "Epoch 2 step 770: training loss: 719.2229209049711\n",
      "Epoch 2 step 771: training accuarcy: 0.8695\n",
      "Epoch 2 step 771: training loss: 732.4137822190867\n",
      "Epoch 2 step 772: training accuarcy: 0.8625\n",
      "Epoch 2 step 772: training loss: 740.1639552903606\n",
      "Epoch 2 step 773: training accuarcy: 0.86\n",
      "Epoch 2 step 773: training loss: 699.8642412052849\n",
      "Epoch 2 step 774: training accuarcy: 0.8725\n",
      "Epoch 2 step 774: training loss: 728.3033732659502\n",
      "Epoch 2 step 775: training accuarcy: 0.86\n",
      "Epoch 2 step 775: training loss: 710.4161075760886\n",
      "Epoch 2 step 776: training accuarcy: 0.883\n",
      "Epoch 2 step 776: training loss: 709.2309252109319\n",
      "Epoch 2 step 777: training accuarcy: 0.8705\n",
      "Epoch 2 step 777: training loss: 716.419184550129\n",
      "Epoch 2 step 778: training accuarcy: 0.872\n",
      "Epoch 2 step 778: training loss: 716.2584458908352\n",
      "Epoch 2 step 779: training accuarcy: 0.871\n",
      "Epoch 2 step 779: training loss: 719.777804180573\n",
      "Epoch 2 step 780: training accuarcy: 0.867\n",
      "Epoch 2 step 780: training loss: 736.2886871131082\n",
      "Epoch 2 step 781: training accuarcy: 0.853\n",
      "Epoch 2 step 781: training loss: 751.1175293195354\n",
      "Epoch 2 step 782: training accuarcy: 0.8585\n",
      "Epoch 2 step 782: training loss: 701.1516736168925\n",
      "Epoch 2 step 783: training accuarcy: 0.885\n",
      "Epoch 2 step 783: training loss: 706.7191819808181\n",
      "Epoch 2 step 784: training accuarcy: 0.8765000000000001\n",
      "Epoch 2 step 784: training loss: 730.0881508810436\n",
      "Epoch 2 step 785: training accuarcy: 0.869\n",
      "Epoch 2 step 785: training loss: 725.6723783543562\n",
      "Epoch 2 step 786: training accuarcy: 0.8635\n",
      "Epoch 2 step 786: training loss: 705.8044547701887\n",
      "Epoch 2 step 787: training accuarcy: 0.875\n",
      "Epoch 2 step 787: training loss: 745.0177152427982\n",
      "Epoch 2 step 788: training accuarcy: 0.8595\n",
      "Epoch 2 step 788: training loss: 294.58378716480166\n",
      "Epoch 2 step 789: training accuarcy: 0.8692307692307693\n",
      "Epoch 2: train loss 762.3531646269417, train accuarcy 0.8305734395980835\n",
      "Epoch 2: valid loss 989.3576875305267, valid accuarcy 0.761976957321167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████                                                                                               | 3/8 [06:41<11:09, 133.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 662.8766847026562\n",
      "Epoch 3 step 790: training accuarcy: 0.8915000000000001\n",
      "Epoch 3 step 790: training loss: 641.7494012508415\n",
      "Epoch 3 step 791: training accuarcy: 0.897\n",
      "Epoch 3 step 791: training loss: 653.0795543581189\n",
      "Epoch 3 step 792: training accuarcy: 0.89\n",
      "Epoch 3 step 792: training loss: 641.8297138849573\n",
      "Epoch 3 step 793: training accuarcy: 0.8995\n",
      "Epoch 3 step 793: training loss: 652.5418764175977\n",
      "Epoch 3 step 794: training accuarcy: 0.8835000000000001\n",
      "Epoch 3 step 794: training loss: 615.5849772180482\n",
      "Epoch 3 step 795: training accuarcy: 0.909\n",
      "Epoch 3 step 795: training loss: 636.9408397253416\n",
      "Epoch 3 step 796: training accuarcy: 0.9035\n",
      "Epoch 3 step 796: training loss: 657.5432947642714\n",
      "Epoch 3 step 797: training accuarcy: 0.893\n",
      "Epoch 3 step 797: training loss: 655.2370914042366\n",
      "Epoch 3 step 798: training accuarcy: 0.8915000000000001\n",
      "Epoch 3 step 798: training loss: 652.8417217789472\n",
      "Epoch 3 step 799: training accuarcy: 0.886\n",
      "Epoch 3 step 799: training loss: 627.4010480464716\n",
      "Epoch 3 step 800: training accuarcy: 0.8965\n",
      "Epoch 3 step 800: training loss: 644.6436265513939\n",
      "Epoch 3 step 801: training accuarcy: 0.8905000000000001\n",
      "Epoch 3 step 801: training loss: 654.4724262355905\n",
      "Epoch 3 step 802: training accuarcy: 0.892\n",
      "Epoch 3 step 802: training loss: 615.619207011632\n",
      "Epoch 3 step 803: training accuarcy: 0.9115\n",
      "Epoch 3 step 803: training loss: 662.776048578862\n",
      "Epoch 3 step 804: training accuarcy: 0.884\n",
      "Epoch 3 step 804: training loss: 643.2340863580932\n",
      "Epoch 3 step 805: training accuarcy: 0.893\n",
      "Epoch 3 step 805: training loss: 634.5783797609706\n",
      "Epoch 3 step 806: training accuarcy: 0.9045\n",
      "Epoch 3 step 806: training loss: 633.7534603023989\n",
      "Epoch 3 step 807: training accuarcy: 0.892\n",
      "Epoch 3 step 807: training loss: 642.0109064680227\n",
      "Epoch 3 step 808: training accuarcy: 0.901\n",
      "Epoch 3 step 808: training loss: 618.8613450016787\n",
      "Epoch 3 step 809: training accuarcy: 0.908\n",
      "Epoch 3 step 809: training loss: 620.8374095157193\n",
      "Epoch 3 step 810: training accuarcy: 0.904\n",
      "Epoch 3 step 810: training loss: 635.7238149166745\n",
      "Epoch 3 step 811: training accuarcy: 0.8955000000000001\n",
      "Epoch 3 step 811: training loss: 670.1702278287014\n",
      "Epoch 3 step 812: training accuarcy: 0.8825000000000001\n",
      "Epoch 3 step 812: training loss: 635.3453621058246\n",
      "Epoch 3 step 813: training accuarcy: 0.8895000000000001\n",
      "Epoch 3 step 813: training loss: 653.9157852564011\n",
      "Epoch 3 step 814: training accuarcy: 0.886\n",
      "Epoch 3 step 814: training loss: 642.0495867139908\n",
      "Epoch 3 step 815: training accuarcy: 0.8935000000000001\n",
      "Epoch 3 step 815: training loss: 624.9100671763329\n",
      "Epoch 3 step 816: training accuarcy: 0.899\n",
      "Epoch 3 step 816: training loss: 632.3728065308904\n",
      "Epoch 3 step 817: training accuarcy: 0.903\n",
      "Epoch 3 step 817: training loss: 639.8965589240551\n",
      "Epoch 3 step 818: training accuarcy: 0.8945000000000001\n",
      "Epoch 3 step 818: training loss: 648.764901893936\n",
      "Epoch 3 step 819: training accuarcy: 0.8845000000000001\n",
      "Epoch 3 step 819: training loss: 637.3282402966926\n",
      "Epoch 3 step 820: training accuarcy: 0.8965\n",
      "Epoch 3 step 820: training loss: 636.7309131173433\n",
      "Epoch 3 step 821: training accuarcy: 0.8955000000000001\n",
      "Epoch 3 step 821: training loss: 652.701254566443\n",
      "Epoch 3 step 822: training accuarcy: 0.8925000000000001\n",
      "Epoch 3 step 822: training loss: 634.2687739246242\n",
      "Epoch 3 step 823: training accuarcy: 0.904\n",
      "Epoch 3 step 823: training loss: 642.7637444252917\n",
      "Epoch 3 step 824: training accuarcy: 0.893\n",
      "Epoch 3 step 824: training loss: 635.4527437008979\n",
      "Epoch 3 step 825: training accuarcy: 0.897\n",
      "Epoch 3 step 825: training loss: 622.779915959506\n",
      "Epoch 3 step 826: training accuarcy: 0.901\n",
      "Epoch 3 step 826: training loss: 637.6645223526884\n",
      "Epoch 3 step 827: training accuarcy: 0.892\n",
      "Epoch 3 step 827: training loss: 641.5748015652212\n",
      "Epoch 3 step 828: training accuarcy: 0.8895000000000001\n",
      "Epoch 3 step 828: training loss: 630.739388554526\n",
      "Epoch 3 step 829: training accuarcy: 0.9\n",
      "Epoch 3 step 829: training loss: 662.1406690844304\n",
      "Epoch 3 step 830: training accuarcy: 0.8835000000000001\n",
      "Epoch 3 step 830: training loss: 630.6148683153795\n",
      "Epoch 3 step 831: training accuarcy: 0.899\n",
      "Epoch 3 step 831: training loss: 636.8501924057136\n",
      "Epoch 3 step 832: training accuarcy: 0.8975\n",
      "Epoch 3 step 832: training loss: 657.821666868991\n",
      "Epoch 3 step 833: training accuarcy: 0.8875000000000001\n",
      "Epoch 3 step 833: training loss: 637.391573587845\n",
      "Epoch 3 step 834: training accuarcy: 0.902\n",
      "Epoch 3 step 834: training loss: 652.586760623071\n",
      "Epoch 3 step 835: training accuarcy: 0.8785000000000001\n",
      "Epoch 3 step 835: training loss: 640.5356468709936\n",
      "Epoch 3 step 836: training accuarcy: 0.895\n",
      "Epoch 3 step 836: training loss: 650.6529660146996\n",
      "Epoch 3 step 837: training accuarcy: 0.893\n",
      "Epoch 3 step 837: training loss: 613.1759842192798\n",
      "Epoch 3 step 838: training accuarcy: 0.908\n",
      "Epoch 3 step 838: training loss: 627.7530330898999\n",
      "Epoch 3 step 839: training accuarcy: 0.897\n",
      "Epoch 3 step 839: training loss: 641.6298346800905\n",
      "Epoch 3 step 840: training accuarcy: 0.887\n",
      "Epoch 3 step 840: training loss: 653.5036135503079\n",
      "Epoch 3 step 841: training accuarcy: 0.894\n",
      "Epoch 3 step 841: training loss: 609.3570538941638\n",
      "Epoch 3 step 842: training accuarcy: 0.9045\n",
      "Epoch 3 step 842: training loss: 623.1652240505256\n",
      "Epoch 3 step 843: training accuarcy: 0.9015\n",
      "Epoch 3 step 843: training loss: 641.5794107573162\n",
      "Epoch 3 step 844: training accuarcy: 0.8975\n",
      "Epoch 3 step 844: training loss: 639.5110796306161\n",
      "Epoch 3 step 845: training accuarcy: 0.8965\n",
      "Epoch 3 step 845: training loss: 627.0107720029714\n",
      "Epoch 3 step 846: training accuarcy: 0.9015\n",
      "Epoch 3 step 846: training loss: 635.887399723688\n",
      "Epoch 3 step 847: training accuarcy: 0.904\n",
      "Epoch 3 step 847: training loss: 624.4079866901488\n",
      "Epoch 3 step 848: training accuarcy: 0.898\n",
      "Epoch 3 step 848: training loss: 623.7424182976496\n",
      "Epoch 3 step 849: training accuarcy: 0.902\n",
      "Epoch 3 step 849: training loss: 638.9948531402808\n",
      "Epoch 3 step 850: training accuarcy: 0.8975\n",
      "Epoch 3 step 850: training loss: 631.7945887660054\n",
      "Epoch 3 step 851: training accuarcy: 0.8975\n",
      "Epoch 3 step 851: training loss: 663.1124195523128\n",
      "Epoch 3 step 852: training accuarcy: 0.887\n",
      "Epoch 3 step 852: training loss: 635.3366540489152\n",
      "Epoch 3 step 853: training accuarcy: 0.898\n",
      "Epoch 3 step 853: training loss: 636.3894985701598\n",
      "Epoch 3 step 854: training accuarcy: 0.9015\n",
      "Epoch 3 step 854: training loss: 642.2503416066736\n",
      "Epoch 3 step 855: training accuarcy: 0.8925000000000001\n",
      "Epoch 3 step 855: training loss: 646.3583732158486\n",
      "Epoch 3 step 856: training accuarcy: 0.893\n",
      "Epoch 3 step 856: training loss: 630.2192890823669\n",
      "Epoch 3 step 857: training accuarcy: 0.896\n",
      "Epoch 3 step 857: training loss: 619.4339587843289\n",
      "Epoch 3 step 858: training accuarcy: 0.904\n",
      "Epoch 3 step 858: training loss: 620.9405762706504\n",
      "Epoch 3 step 859: training accuarcy: 0.903\n",
      "Epoch 3 step 859: training loss: 607.2325271556595\n",
      "Epoch 3 step 860: training accuarcy: 0.908\n",
      "Epoch 3 step 860: training loss: 608.9860031469888\n",
      "Epoch 3 step 861: training accuarcy: 0.9065\n",
      "Epoch 3 step 861: training loss: 612.571657958063\n",
      "Epoch 3 step 862: training accuarcy: 0.902\n",
      "Epoch 3 step 862: training loss: 610.6489481511056\n",
      "Epoch 3 step 863: training accuarcy: 0.9055\n",
      "Epoch 3 step 863: training loss: 637.5982586721499\n",
      "Epoch 3 step 864: training accuarcy: 0.8945000000000001\n",
      "Epoch 3 step 864: training loss: 629.5807653761664\n",
      "Epoch 3 step 865: training accuarcy: 0.8955000000000001\n",
      "Epoch 3 step 865: training loss: 616.400982617663\n",
      "Epoch 3 step 866: training accuarcy: 0.8955000000000001\n",
      "Epoch 3 step 866: training loss: 641.4233517488124\n",
      "Epoch 3 step 867: training accuarcy: 0.896\n",
      "Epoch 3 step 867: training loss: 616.8728021028965\n",
      "Epoch 3 step 868: training accuarcy: 0.906\n",
      "Epoch 3 step 868: training loss: 629.5249357094846\n",
      "Epoch 3 step 869: training accuarcy: 0.8995\n",
      "Epoch 3 step 869: training loss: 617.3762964110273\n",
      "Epoch 3 step 870: training accuarcy: 0.907\n",
      "Epoch 3 step 870: training loss: 619.1636913060552\n",
      "Epoch 3 step 871: training accuarcy: 0.901\n",
      "Epoch 3 step 871: training loss: 638.9212699006945\n",
      "Epoch 3 step 872: training accuarcy: 0.901\n",
      "Epoch 3 step 872: training loss: 625.2038069954167\n",
      "Epoch 3 step 873: training accuarcy: 0.8995\n",
      "Epoch 3 step 873: training loss: 631.7020458312045\n",
      "Epoch 3 step 874: training accuarcy: 0.898\n",
      "Epoch 3 step 874: training loss: 614.8507439320256\n",
      "Epoch 3 step 875: training accuarcy: 0.904\n",
      "Epoch 3 step 875: training loss: 610.3429885348747\n",
      "Epoch 3 step 876: training accuarcy: 0.9055\n",
      "Epoch 3 step 876: training loss: 640.9670313974943\n",
      "Epoch 3 step 877: training accuarcy: 0.899\n",
      "Epoch 3 step 877: training loss: 594.5607772476968\n",
      "Epoch 3 step 878: training accuarcy: 0.9145\n",
      "Epoch 3 step 878: training loss: 633.6067317804761\n",
      "Epoch 3 step 879: training accuarcy: 0.886\n",
      "Epoch 3 step 879: training loss: 617.5884767642145\n",
      "Epoch 3 step 880: training accuarcy: 0.9015\n",
      "Epoch 3 step 880: training loss: 617.2515007800687\n",
      "Epoch 3 step 881: training accuarcy: 0.903\n",
      "Epoch 3 step 881: training loss: 625.2582513765694\n",
      "Epoch 3 step 882: training accuarcy: 0.899\n",
      "Epoch 3 step 882: training loss: 632.0769984484552\n",
      "Epoch 3 step 883: training accuarcy: 0.889\n",
      "Epoch 3 step 883: training loss: 629.4311883065981\n",
      "Epoch 3 step 884: training accuarcy: 0.89\n",
      "Epoch 3 step 884: training loss: 636.0218044660703\n",
      "Epoch 3 step 885: training accuarcy: 0.893\n",
      "Epoch 3 step 885: training loss: 640.2831274834982\n",
      "Epoch 3 step 886: training accuarcy: 0.89\n",
      "Epoch 3 step 886: training loss: 621.3258795178285\n",
      "Epoch 3 step 887: training accuarcy: 0.889\n",
      "Epoch 3 step 887: training loss: 594.6628768576041\n",
      "Epoch 3 step 888: training accuarcy: 0.9065\n",
      "Epoch 3 step 888: training loss: 644.3059301173025\n",
      "Epoch 3 step 889: training accuarcy: 0.894\n",
      "Epoch 3 step 889: training loss: 614.5492085787586\n",
      "Epoch 3 step 890: training accuarcy: 0.8975\n",
      "Epoch 3 step 890: training loss: 605.3429985323482\n",
      "Epoch 3 step 891: training accuarcy: 0.9005\n",
      "Epoch 3 step 891: training loss: 635.6359236988071\n",
      "Epoch 3 step 892: training accuarcy: 0.9005\n",
      "Epoch 3 step 892: training loss: 628.2588484496522\n",
      "Epoch 3 step 893: training accuarcy: 0.897\n",
      "Epoch 3 step 893: training loss: 585.1763676270598\n",
      "Epoch 3 step 894: training accuarcy: 0.9115\n",
      "Epoch 3 step 894: training loss: 616.1109799078569\n",
      "Epoch 3 step 895: training accuarcy: 0.902\n",
      "Epoch 3 step 895: training loss: 628.6738027367205\n",
      "Epoch 3 step 896: training accuarcy: 0.895\n",
      "Epoch 3 step 896: training loss: 617.8218047725396\n",
      "Epoch 3 step 897: training accuarcy: 0.905\n",
      "Epoch 3 step 897: training loss: 620.7613919482644\n",
      "Epoch 3 step 898: training accuarcy: 0.897\n",
      "Epoch 3 step 898: training loss: 597.048298233279\n",
      "Epoch 3 step 899: training accuarcy: 0.913\n",
      "Epoch 3 step 899: training loss: 612.4749080160636\n",
      "Epoch 3 step 900: training accuarcy: 0.8995\n",
      "Epoch 3 step 900: training loss: 648.2250130191632\n",
      "Epoch 3 step 901: training accuarcy: 0.887\n",
      "Epoch 3 step 901: training loss: 608.0781588871788\n",
      "Epoch 3 step 902: training accuarcy: 0.909\n",
      "Epoch 3 step 902: training loss: 603.9472195689664\n",
      "Epoch 3 step 903: training accuarcy: 0.9065\n",
      "Epoch 3 step 903: training loss: 611.7732319156844\n",
      "Epoch 3 step 904: training accuarcy: 0.9085\n",
      "Epoch 3 step 904: training loss: 627.8959345378587\n",
      "Epoch 3 step 905: training accuarcy: 0.896\n",
      "Epoch 3 step 905: training loss: 615.1567905406131\n",
      "Epoch 3 step 906: training accuarcy: 0.905\n",
      "Epoch 3 step 906: training loss: 604.9696658957182\n",
      "Epoch 3 step 907: training accuarcy: 0.9005\n",
      "Epoch 3 step 907: training loss: 590.6502954665166\n",
      "Epoch 3 step 908: training accuarcy: 0.9135\n",
      "Epoch 3 step 908: training loss: 628.8610017313854\n",
      "Epoch 3 step 909: training accuarcy: 0.889\n",
      "Epoch 3 step 909: training loss: 622.6320440787288\n",
      "Epoch 3 step 910: training accuarcy: 0.899\n",
      "Epoch 3 step 910: training loss: 617.4641835563815\n",
      "Epoch 3 step 911: training accuarcy: 0.904\n",
      "Epoch 3 step 911: training loss: 612.2195536809227\n",
      "Epoch 3 step 912: training accuarcy: 0.9055\n",
      "Epoch 3 step 912: training loss: 614.1183731608432\n",
      "Epoch 3 step 913: training accuarcy: 0.902\n",
      "Epoch 3 step 913: training loss: 614.3386314150512\n",
      "Epoch 3 step 914: training accuarcy: 0.9\n",
      "Epoch 3 step 914: training loss: 623.413209953815\n",
      "Epoch 3 step 915: training accuarcy: 0.8915000000000001\n",
      "Epoch 3 step 915: training loss: 619.0747931942964\n",
      "Epoch 3 step 916: training accuarcy: 0.8995\n",
      "Epoch 3 step 916: training loss: 614.1736417222934\n",
      "Epoch 3 step 917: training accuarcy: 0.906\n",
      "Epoch 3 step 917: training loss: 617.6238456617184\n",
      "Epoch 3 step 918: training accuarcy: 0.897\n",
      "Epoch 3 step 918: training loss: 617.2282031995603\n",
      "Epoch 3 step 919: training accuarcy: 0.9015\n",
      "Epoch 3 step 919: training loss: 599.7559328714614\n",
      "Epoch 3 step 920: training accuarcy: 0.9035\n",
      "Epoch 3 step 920: training loss: 612.7890372561017\n",
      "Epoch 3 step 921: training accuarcy: 0.904\n",
      "Epoch 3 step 921: training loss: 648.5319068276281\n",
      "Epoch 3 step 922: training accuarcy: 0.887\n",
      "Epoch 3 step 922: training loss: 603.0151002633504\n",
      "Epoch 3 step 923: training accuarcy: 0.908\n",
      "Epoch 3 step 923: training loss: 603.5081856751358\n",
      "Epoch 3 step 924: training accuarcy: 0.908\n",
      "Epoch 3 step 924: training loss: 610.7756038096791\n",
      "Epoch 3 step 925: training accuarcy: 0.898\n",
      "Epoch 3 step 925: training loss: 621.0711065515324\n",
      "Epoch 3 step 926: training accuarcy: 0.9045\n",
      "Epoch 3 step 926: training loss: 614.0582243652938\n",
      "Epoch 3 step 927: training accuarcy: 0.9055\n",
      "Epoch 3 step 927: training loss: 638.4707466947801\n",
      "Epoch 3 step 928: training accuarcy: 0.8915000000000001\n",
      "Epoch 3 step 928: training loss: 614.2656577449316\n",
      "Epoch 3 step 929: training accuarcy: 0.904\n",
      "Epoch 3 step 929: training loss: 592.059218701082\n",
      "Epoch 3 step 930: training accuarcy: 0.907\n",
      "Epoch 3 step 930: training loss: 619.2149902850215\n",
      "Epoch 3 step 931: training accuarcy: 0.9005\n",
      "Epoch 3 step 931: training loss: 622.8691594482268\n",
      "Epoch 3 step 932: training accuarcy: 0.8925000000000001\n",
      "Epoch 3 step 932: training loss: 594.1848622625462\n",
      "Epoch 3 step 933: training accuarcy: 0.91\n",
      "Epoch 3 step 933: training loss: 622.6910832774295\n",
      "Epoch 3 step 934: training accuarcy: 0.894\n",
      "Epoch 3 step 934: training loss: 596.7669238265926\n",
      "Epoch 3 step 935: training accuarcy: 0.896\n",
      "Epoch 3 step 935: training loss: 594.4813641754157\n",
      "Epoch 3 step 936: training accuarcy: 0.9075\n",
      "Epoch 3 step 936: training loss: 620.6535328338666\n",
      "Epoch 3 step 937: training accuarcy: 0.91\n",
      "Epoch 3 step 937: training loss: 611.7301696594615\n",
      "Epoch 3 step 938: training accuarcy: 0.901\n",
      "Epoch 3 step 938: training loss: 602.257455085793\n",
      "Epoch 3 step 939: training accuarcy: 0.912\n",
      "Epoch 3 step 939: training loss: 612.5936784569078\n",
      "Epoch 3 step 940: training accuarcy: 0.9035\n",
      "Epoch 3 step 940: training loss: 605.0225981395077\n",
      "Epoch 3 step 941: training accuarcy: 0.905\n",
      "Epoch 3 step 941: training loss: 569.9780435622803\n",
      "Epoch 3 step 942: training accuarcy: 0.927\n",
      "Epoch 3 step 942: training loss: 606.5895663160769\n",
      "Epoch 3 step 943: training accuarcy: 0.905\n",
      "Epoch 3 step 943: training loss: 603.6268600203846\n",
      "Epoch 3 step 944: training accuarcy: 0.908\n",
      "Epoch 3 step 944: training loss: 603.7864444099603\n",
      "Epoch 3 step 945: training accuarcy: 0.9\n",
      "Epoch 3 step 945: training loss: 598.2732330204936\n",
      "Epoch 3 step 946: training accuarcy: 0.9015\n",
      "Epoch 3 step 946: training loss: 584.4926720781169\n",
      "Epoch 3 step 947: training accuarcy: 0.9095\n",
      "Epoch 3 step 947: training loss: 610.847036771957\n",
      "Epoch 3 step 948: training accuarcy: 0.9005\n",
      "Epoch 3 step 948: training loss: 600.1153895177847\n",
      "Epoch 3 step 949: training accuarcy: 0.904\n",
      "Epoch 3 step 949: training loss: 580.9029844032609\n",
      "Epoch 3 step 950: training accuarcy: 0.906\n",
      "Epoch 3 step 950: training loss: 599.8507765035732\n",
      "Epoch 3 step 951: training accuarcy: 0.913\n",
      "Epoch 3 step 951: training loss: 609.6629516988905\n",
      "Epoch 3 step 952: training accuarcy: 0.913\n",
      "Epoch 3 step 952: training loss: 596.8273247410076\n",
      "Epoch 3 step 953: training accuarcy: 0.9125\n",
      "Epoch 3 step 953: training loss: 608.2346354564096\n",
      "Epoch 3 step 954: training accuarcy: 0.905\n",
      "Epoch 3 step 954: training loss: 639.0171106163374\n",
      "Epoch 3 step 955: training accuarcy: 0.89\n",
      "Epoch 3 step 955: training loss: 590.4817245137676\n",
      "Epoch 3 step 956: training accuarcy: 0.9055\n",
      "Epoch 3 step 956: training loss: 597.6731801951302\n",
      "Epoch 3 step 957: training accuarcy: 0.911\n",
      "Epoch 3 step 957: training loss: 609.5462539238164\n",
      "Epoch 3 step 958: training accuarcy: 0.8995\n",
      "Epoch 3 step 958: training loss: 566.6245692437208\n",
      "Epoch 3 step 959: training accuarcy: 0.9175\n",
      "Epoch 3 step 959: training loss: 635.1996001739645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 step 960: training accuarcy: 0.887\n",
      "Epoch 3 step 960: training loss: 613.5972854995517\n",
      "Epoch 3 step 961: training accuarcy: 0.903\n",
      "Epoch 3 step 961: training loss: 592.151937238631\n",
      "Epoch 3 step 962: training accuarcy: 0.914\n",
      "Epoch 3 step 962: training loss: 597.8531012286089\n",
      "Epoch 3 step 963: training accuarcy: 0.91\n",
      "Epoch 3 step 963: training loss: 586.7394798985896\n",
      "Epoch 3 step 964: training accuarcy: 0.909\n",
      "Epoch 3 step 964: training loss: 597.3925989849918\n",
      "Epoch 3 step 965: training accuarcy: 0.906\n",
      "Epoch 3 step 965: training loss: 626.650744639278\n",
      "Epoch 3 step 966: training accuarcy: 0.89\n",
      "Epoch 3 step 966: training loss: 602.5752597330887\n",
      "Epoch 3 step 967: training accuarcy: 0.909\n",
      "Epoch 3 step 967: training loss: 587.6273881538823\n",
      "Epoch 3 step 968: training accuarcy: 0.9115\n",
      "Epoch 3 step 968: training loss: 581.9788915852397\n",
      "Epoch 3 step 969: training accuarcy: 0.9095\n",
      "Epoch 3 step 969: training loss: 597.8279681108854\n",
      "Epoch 3 step 970: training accuarcy: 0.905\n",
      "Epoch 3 step 970: training loss: 595.887427655117\n",
      "Epoch 3 step 971: training accuarcy: 0.9115\n",
      "Epoch 3 step 971: training loss: 579.918642455101\n",
      "Epoch 3 step 972: training accuarcy: 0.91\n",
      "Epoch 3 step 972: training loss: 608.8978900420308\n",
      "Epoch 3 step 973: training accuarcy: 0.9025\n",
      "Epoch 3 step 973: training loss: 583.2930142824478\n",
      "Epoch 3 step 974: training accuarcy: 0.9065\n",
      "Epoch 3 step 974: training loss: 601.7538885494221\n",
      "Epoch 3 step 975: training accuarcy: 0.903\n",
      "Epoch 3 step 975: training loss: 608.1859029249816\n",
      "Epoch 3 step 976: training accuarcy: 0.9035\n",
      "Epoch 3 step 976: training loss: 618.0520413662524\n",
      "Epoch 3 step 977: training accuarcy: 0.903\n",
      "Epoch 3 step 977: training loss: 610.1439650780485\n",
      "Epoch 3 step 978: training accuarcy: 0.899\n",
      "Epoch 3 step 978: training loss: 618.6864538712755\n",
      "Epoch 3 step 979: training accuarcy: 0.899\n",
      "Epoch 3 step 979: training loss: 618.1358507561155\n",
      "Epoch 3 step 980: training accuarcy: 0.902\n",
      "Epoch 3 step 980: training loss: 589.0634884712142\n",
      "Epoch 3 step 981: training accuarcy: 0.9105\n",
      "Epoch 3 step 981: training loss: 595.695770176505\n",
      "Epoch 3 step 982: training accuarcy: 0.8995\n",
      "Epoch 3 step 982: training loss: 589.9886863075566\n",
      "Epoch 3 step 983: training accuarcy: 0.906\n",
      "Epoch 3 step 983: training loss: 587.8143278876553\n",
      "Epoch 3 step 984: training accuarcy: 0.903\n",
      "Epoch 3 step 984: training loss: 586.4992072311305\n",
      "Epoch 3 step 985: training accuarcy: 0.9145\n",
      "Epoch 3 step 985: training loss: 586.5661179458039\n",
      "Epoch 3 step 986: training accuarcy: 0.917\n",
      "Epoch 3 step 986: training loss: 584.271630244891\n",
      "Epoch 3 step 987: training accuarcy: 0.9055\n",
      "Epoch 3 step 987: training loss: 615.4598080302783\n",
      "Epoch 3 step 988: training accuarcy: 0.8975\n",
      "Epoch 3 step 988: training loss: 579.2274740945895\n",
      "Epoch 3 step 989: training accuarcy: 0.9115\n",
      "Epoch 3 step 989: training loss: 607.9767214870938\n",
      "Epoch 3 step 990: training accuarcy: 0.904\n",
      "Epoch 3 step 990: training loss: 566.0900550127619\n",
      "Epoch 3 step 991: training accuarcy: 0.911\n",
      "Epoch 3 step 991: training loss: 587.6654273753486\n",
      "Epoch 3 step 992: training accuarcy: 0.906\n",
      "Epoch 3 step 992: training loss: 601.8593939464142\n",
      "Epoch 3 step 993: training accuarcy: 0.9055\n",
      "Epoch 3 step 993: training loss: 578.2546215959427\n",
      "Epoch 3 step 994: training accuarcy: 0.912\n",
      "Epoch 3 step 994: training loss: 610.2583221453964\n",
      "Epoch 3 step 995: training accuarcy: 0.9015\n",
      "Epoch 3 step 995: training loss: 591.7000481996579\n",
      "Epoch 3 step 996: training accuarcy: 0.906\n",
      "Epoch 3 step 996: training loss: 629.1199699085199\n",
      "Epoch 3 step 997: training accuarcy: 0.8885000000000001\n",
      "Epoch 3 step 997: training loss: 600.1461449870511\n",
      "Epoch 3 step 998: training accuarcy: 0.899\n",
      "Epoch 3 step 998: training loss: 591.6737528501423\n",
      "Epoch 3 step 999: training accuarcy: 0.9125\n",
      "Epoch 3 step 999: training loss: 571.809202489584\n",
      "Epoch 3 step 1000: training accuarcy: 0.9215\n",
      "Epoch 3 step 1000: training loss: 602.8771018867285\n",
      "Epoch 3 step 1001: training accuarcy: 0.908\n",
      "Epoch 3 step 1001: training loss: 594.4310280688755\n",
      "Epoch 3 step 1002: training accuarcy: 0.905\n",
      "Epoch 3 step 1002: training loss: 583.3323177083583\n",
      "Epoch 3 step 1003: training accuarcy: 0.914\n",
      "Epoch 3 step 1003: training loss: 595.5102940159478\n",
      "Epoch 3 step 1004: training accuarcy: 0.9035\n",
      "Epoch 3 step 1004: training loss: 593.4943575136024\n",
      "Epoch 3 step 1005: training accuarcy: 0.913\n",
      "Epoch 3 step 1005: training loss: 599.5851873559865\n",
      "Epoch 3 step 1006: training accuarcy: 0.9065\n",
      "Epoch 3 step 1006: training loss: 582.565812502121\n",
      "Epoch 3 step 1007: training accuarcy: 0.907\n",
      "Epoch 3 step 1007: training loss: 592.3066686181711\n",
      "Epoch 3 step 1008: training accuarcy: 0.911\n",
      "Epoch 3 step 1008: training loss: 575.8012874660891\n",
      "Epoch 3 step 1009: training accuarcy: 0.9095\n",
      "Epoch 3 step 1009: training loss: 568.7500459496277\n",
      "Epoch 3 step 1010: training accuarcy: 0.9135\n",
      "Epoch 3 step 1010: training loss: 579.7113263629296\n",
      "Epoch 3 step 1011: training accuarcy: 0.9125\n",
      "Epoch 3 step 1011: training loss: 581.9738401189942\n",
      "Epoch 3 step 1012: training accuarcy: 0.9005\n",
      "Epoch 3 step 1012: training loss: 604.2483045890458\n",
      "Epoch 3 step 1013: training accuarcy: 0.9015\n",
      "Epoch 3 step 1013: training loss: 586.0197896233167\n",
      "Epoch 3 step 1014: training accuarcy: 0.9035\n",
      "Epoch 3 step 1014: training loss: 567.9399079337837\n",
      "Epoch 3 step 1015: training accuarcy: 0.9155\n",
      "Epoch 3 step 1015: training loss: 561.760935050534\n",
      "Epoch 3 step 1016: training accuarcy: 0.9135\n",
      "Epoch 3 step 1016: training loss: 559.7460975103452\n",
      "Epoch 3 step 1017: training accuarcy: 0.923\n",
      "Epoch 3 step 1017: training loss: 614.7643770411747\n",
      "Epoch 3 step 1018: training accuarcy: 0.9\n",
      "Epoch 3 step 1018: training loss: 586.7376176881052\n",
      "Epoch 3 step 1019: training accuarcy: 0.91\n",
      "Epoch 3 step 1019: training loss: 570.6301050376721\n",
      "Epoch 3 step 1020: training accuarcy: 0.9175\n",
      "Epoch 3 step 1020: training loss: 567.3682272274789\n",
      "Epoch 3 step 1021: training accuarcy: 0.9185\n",
      "Epoch 3 step 1021: training loss: 586.5477087304015\n",
      "Epoch 3 step 1022: training accuarcy: 0.907\n",
      "Epoch 3 step 1022: training loss: 593.8765051759469\n",
      "Epoch 3 step 1023: training accuarcy: 0.905\n",
      "Epoch 3 step 1023: training loss: 584.2963790698755\n",
      "Epoch 3 step 1024: training accuarcy: 0.917\n",
      "Epoch 3 step 1024: training loss: 572.5872099572371\n",
      "Epoch 3 step 1025: training accuarcy: 0.9195\n",
      "Epoch 3 step 1025: training loss: 551.0740768034016\n",
      "Epoch 3 step 1026: training accuarcy: 0.9185\n",
      "Epoch 3 step 1026: training loss: 589.3804780269711\n",
      "Epoch 3 step 1027: training accuarcy: 0.905\n",
      "Epoch 3 step 1027: training loss: 591.0624367783429\n",
      "Epoch 3 step 1028: training accuarcy: 0.9025\n",
      "Epoch 3 step 1028: training loss: 591.5213233537523\n",
      "Epoch 3 step 1029: training accuarcy: 0.902\n",
      "Epoch 3 step 1029: training loss: 599.1010180709221\n",
      "Epoch 3 step 1030: training accuarcy: 0.8985\n",
      "Epoch 3 step 1030: training loss: 573.0326172507318\n",
      "Epoch 3 step 1031: training accuarcy: 0.919\n",
      "Epoch 3 step 1031: training loss: 571.043234586487\n",
      "Epoch 3 step 1032: training accuarcy: 0.9165\n",
      "Epoch 3 step 1032: training loss: 582.5309370494087\n",
      "Epoch 3 step 1033: training accuarcy: 0.904\n",
      "Epoch 3 step 1033: training loss: 579.8306543992453\n",
      "Epoch 3 step 1034: training accuarcy: 0.9\n",
      "Epoch 3 step 1034: training loss: 593.4149993822388\n",
      "Epoch 3 step 1035: training accuarcy: 0.9045\n",
      "Epoch 3 step 1035: training loss: 574.6428448593431\n",
      "Epoch 3 step 1036: training accuarcy: 0.904\n",
      "Epoch 3 step 1036: training loss: 591.7875230046909\n",
      "Epoch 3 step 1037: training accuarcy: 0.908\n",
      "Epoch 3 step 1037: training loss: 588.9954904042659\n",
      "Epoch 3 step 1038: training accuarcy: 0.9005\n",
      "Epoch 3 step 1038: training loss: 585.3212945524281\n",
      "Epoch 3 step 1039: training accuarcy: 0.9075\n",
      "Epoch 3 step 1039: training loss: 578.9755308816832\n",
      "Epoch 3 step 1040: training accuarcy: 0.9075\n",
      "Epoch 3 step 1040: training loss: 583.4360486548418\n",
      "Epoch 3 step 1041: training accuarcy: 0.899\n",
      "Epoch 3 step 1041: training loss: 594.2413550434547\n",
      "Epoch 3 step 1042: training accuarcy: 0.8985\n",
      "Epoch 3 step 1042: training loss: 583.3787980281887\n",
      "Epoch 3 step 1043: training accuarcy: 0.901\n",
      "Epoch 3 step 1043: training loss: 539.8795088263919\n",
      "Epoch 3 step 1044: training accuarcy: 0.92\n",
      "Epoch 3 step 1044: training loss: 560.8154531365071\n",
      "Epoch 3 step 1045: training accuarcy: 0.9065\n",
      "Epoch 3 step 1045: training loss: 563.8318709796129\n",
      "Epoch 3 step 1046: training accuarcy: 0.915\n",
      "Epoch 3 step 1046: training loss: 605.913150342893\n",
      "Epoch 3 step 1047: training accuarcy: 0.901\n",
      "Epoch 3 step 1047: training loss: 572.8340791039749\n",
      "Epoch 3 step 1048: training accuarcy: 0.913\n",
      "Epoch 3 step 1048: training loss: 565.5807341276164\n",
      "Epoch 3 step 1049: training accuarcy: 0.915\n",
      "Epoch 3 step 1049: training loss: 585.5508826188516\n",
      "Epoch 3 step 1050: training accuarcy: 0.9105\n",
      "Epoch 3 step 1050: training loss: 570.3915620121885\n",
      "Epoch 3 step 1051: training accuarcy: 0.9155\n",
      "Epoch 3 step 1051: training loss: 233.66633421860715\n",
      "Epoch 3 step 1052: training accuarcy: 0.9166666666666666\n",
      "Epoch 3: train loss 610.4419720792491, train accuarcy 0.8780782222747803\n",
      "Epoch 3: valid loss 924.4322790288536, valid accuarcy 0.7848190665245056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 4/8 [08:55<08:55, 133.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Epoch 4 step 1052: training loss: 492.93881175929664\n",
      "Epoch 4 step 1053: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1053: training loss: 490.8071296319089\n",
      "Epoch 4 step 1054: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1054: training loss: 487.3139157325017\n",
      "Epoch 4 step 1055: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1055: training loss: 492.41937599976586\n",
      "Epoch 4 step 1056: training accuarcy: 0.9415\n",
      "Epoch 4 step 1056: training loss: 507.1195418844647\n",
      "Epoch 4 step 1057: training accuarcy: 0.9315\n",
      "Epoch 4 step 1057: training loss: 502.2970678688273\n",
      "Epoch 4 step 1058: training accuarcy: 0.9345\n",
      "Epoch 4 step 1058: training loss: 483.9729111697408\n",
      "Epoch 4 step 1059: training accuarcy: 0.9455\n",
      "Epoch 4 step 1059: training loss: 495.1825056675923\n",
      "Epoch 4 step 1060: training accuarcy: 0.933\n",
      "Epoch 4 step 1060: training loss: 519.3698487515812\n",
      "Epoch 4 step 1061: training accuarcy: 0.9325\n",
      "Epoch 4 step 1061: training loss: 486.655807742015\n",
      "Epoch 4 step 1062: training accuarcy: 0.9365\n",
      "Epoch 4 step 1062: training loss: 481.0401568187343\n",
      "Epoch 4 step 1063: training accuarcy: 0.9465\n",
      "Epoch 4 step 1063: training loss: 505.9441301532514\n",
      "Epoch 4 step 1064: training accuarcy: 0.928\n",
      "Epoch 4 step 1064: training loss: 489.44840689268597\n",
      "Epoch 4 step 1065: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1065: training loss: 491.55856039938817\n",
      "Epoch 4 step 1066: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1066: training loss: 480.79706390623045\n",
      "Epoch 4 step 1067: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1067: training loss: 506.741285033615\n",
      "Epoch 4 step 1068: training accuarcy: 0.9295\n",
      "Epoch 4 step 1068: training loss: 503.0876351403099\n",
      "Epoch 4 step 1069: training accuarcy: 0.932\n",
      "Epoch 4 step 1069: training loss: 492.41876734886347\n",
      "Epoch 4 step 1070: training accuarcy: 0.9395\n",
      "Epoch 4 step 1070: training loss: 504.1055805284022\n",
      "Epoch 4 step 1071: training accuarcy: 0.9405\n",
      "Epoch 4 step 1071: training loss: 487.36557995994457\n",
      "Epoch 4 step 1072: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1072: training loss: 515.982663592892\n",
      "Epoch 4 step 1073: training accuarcy: 0.9325\n",
      "Epoch 4 step 1073: training loss: 477.5710713678779\n",
      "Epoch 4 step 1074: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1074: training loss: 482.57301084212986\n",
      "Epoch 4 step 1075: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1075: training loss: 513.8809230516515\n",
      "Epoch 4 step 1076: training accuarcy: 0.931\n",
      "Epoch 4 step 1076: training loss: 485.1578830071843\n",
      "Epoch 4 step 1077: training accuarcy: 0.9395\n",
      "Epoch 4 step 1077: training loss: 506.35101969949835\n",
      "Epoch 4 step 1078: training accuarcy: 0.9305\n",
      "Epoch 4 step 1078: training loss: 494.26679100933313\n",
      "Epoch 4 step 1079: training accuarcy: 0.9375\n",
      "Epoch 4 step 1079: training loss: 483.9070337005747\n",
      "Epoch 4 step 1080: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1080: training loss: 482.7931725748046\n",
      "Epoch 4 step 1081: training accuarcy: 0.9385\n",
      "Epoch 4 step 1081: training loss: 487.1626764366959\n",
      "Epoch 4 step 1082: training accuarcy: 0.9375\n",
      "Epoch 4 step 1082: training loss: 513.5229301915459\n",
      "Epoch 4 step 1083: training accuarcy: 0.9305\n",
      "Epoch 4 step 1083: training loss: 484.08584994614466\n",
      "Epoch 4 step 1084: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1084: training loss: 486.36489731257694\n",
      "Epoch 4 step 1085: training accuarcy: 0.9355\n",
      "Epoch 4 step 1085: training loss: 489.2453181632451\n",
      "Epoch 4 step 1086: training accuarcy: 0.9355\n",
      "Epoch 4 step 1086: training loss: 498.11195862098805\n",
      "Epoch 4 step 1087: training accuarcy: 0.927\n",
      "Epoch 4 step 1087: training loss: 482.5883601082131\n",
      "Epoch 4 step 1088: training accuarcy: 0.9385\n",
      "Epoch 4 step 1088: training loss: 489.52532987384154\n",
      "Epoch 4 step 1089: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1089: training loss: 508.4867784143641\n",
      "Epoch 4 step 1090: training accuarcy: 0.9305\n",
      "Epoch 4 step 1090: training loss: 476.8947951409796\n",
      "Epoch 4 step 1091: training accuarcy: 0.937\n",
      "Epoch 4 step 1091: training loss: 497.1274088058961\n",
      "Epoch 4 step 1092: training accuarcy: 0.9305\n",
      "Epoch 4 step 1092: training loss: 491.8131978327222\n",
      "Epoch 4 step 1093: training accuarcy: 0.935\n",
      "Epoch 4 step 1093: training loss: 520.9107600547014\n",
      "Epoch 4 step 1094: training accuarcy: 0.9275\n",
      "Epoch 4 step 1094: training loss: 479.1923649458463\n",
      "Epoch 4 step 1095: training accuarcy: 0.9475\n",
      "Epoch 4 step 1095: training loss: 510.47856395089065\n",
      "Epoch 4 step 1096: training accuarcy: 0.929\n",
      "Epoch 4 step 1096: training loss: 503.07840183641565\n",
      "Epoch 4 step 1097: training accuarcy: 0.9275\n",
      "Epoch 4 step 1097: training loss: 477.2045303024169\n",
      "Epoch 4 step 1098: training accuarcy: 0.9380000000000001\n",
      "Epoch 4 step 1098: training loss: 479.20470315299673\n",
      "Epoch 4 step 1099: training accuarcy: 0.9335\n",
      "Epoch 4 step 1099: training loss: 486.9747050771465\n",
      "Epoch 4 step 1100: training accuarcy: 0.9395\n",
      "Epoch 4 step 1100: training loss: 501.724341159249\n",
      "Epoch 4 step 1101: training accuarcy: 0.936\n",
      "Epoch 4 step 1101: training loss: 495.8574947040828\n",
      "Epoch 4 step 1102: training accuarcy: 0.934\n",
      "Epoch 4 step 1102: training loss: 483.7454094179629\n",
      "Epoch 4 step 1103: training accuarcy: 0.9355\n",
      "Epoch 4 step 1103: training loss: 490.216556473647\n",
      "Epoch 4 step 1104: training accuarcy: 0.9395\n",
      "Epoch 4 step 1104: training loss: 488.4609502242444\n",
      "Epoch 4 step 1105: training accuarcy: 0.9415\n",
      "Epoch 4 step 1105: training loss: 485.21077979399814\n",
      "Epoch 4 step 1106: training accuarcy: 0.9395\n",
      "Epoch 4 step 1106: training loss: 491.2287754338998\n",
      "Epoch 4 step 1107: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1107: training loss: 495.7187902805075\n",
      "Epoch 4 step 1108: training accuarcy: 0.9355\n",
      "Epoch 4 step 1108: training loss: 480.2735847685666\n",
      "Epoch 4 step 1109: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1109: training loss: 459.21444232547753\n",
      "Epoch 4 step 1110: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1110: training loss: 486.1683843591104\n",
      "Epoch 4 step 1111: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1111: training loss: 510.21431344191546\n",
      "Epoch 4 step 1112: training accuarcy: 0.9255\n",
      "Epoch 4 step 1112: training loss: 480.76790989211804\n",
      "Epoch 4 step 1113: training accuarcy: 0.937\n",
      "Epoch 4 step 1113: training loss: 495.4401222501357\n",
      "Epoch 4 step 1114: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1114: training loss: 475.0190035854394\n",
      "Epoch 4 step 1115: training accuarcy: 0.9485\n",
      "Epoch 4 step 1115: training loss: 475.75277958459367\n",
      "Epoch 4 step 1116: training accuarcy: 0.9355\n",
      "Epoch 4 step 1116: training loss: 458.08456818668515\n",
      "Epoch 4 step 1117: training accuarcy: 0.9505\n",
      "Epoch 4 step 1117: training loss: 494.6280662408893\n",
      "Epoch 4 step 1118: training accuarcy: 0.9285\n",
      "Epoch 4 step 1118: training loss: 508.36455522891526\n",
      "Epoch 4 step 1119: training accuarcy: 0.928\n",
      "Epoch 4 step 1119: training loss: 480.0170450703848\n",
      "Epoch 4 step 1120: training accuarcy: 0.937\n",
      "Epoch 4 step 1120: training loss: 477.7233031430887\n",
      "Epoch 4 step 1121: training accuarcy: 0.934\n",
      "Epoch 4 step 1121: training loss: 502.1187306881064\n",
      "Epoch 4 step 1122: training accuarcy: 0.932\n",
      "Epoch 4 step 1122: training loss: 488.4507824736268\n",
      "Epoch 4 step 1123: training accuarcy: 0.933\n",
      "Epoch 4 step 1123: training loss: 492.67796324185014\n",
      "Epoch 4 step 1124: training accuarcy: 0.9365\n",
      "Epoch 4 step 1124: training loss: 479.01401273603693\n",
      "Epoch 4 step 1125: training accuarcy: 0.9345\n",
      "Epoch 4 step 1125: training loss: 484.15462673381273\n",
      "Epoch 4 step 1126: training accuarcy: 0.9380000000000001\n",
      "Epoch 4 step 1126: training loss: 507.6524436980206\n",
      "Epoch 4 step 1127: training accuarcy: 0.929\n",
      "Epoch 4 step 1127: training loss: 489.8342151837762\n",
      "Epoch 4 step 1128: training accuarcy: 0.935\n",
      "Epoch 4 step 1128: training loss: 495.87133408520333\n",
      "Epoch 4 step 1129: training accuarcy: 0.9365\n",
      "Epoch 4 step 1129: training loss: 466.04770171084294\n",
      "Epoch 4 step 1130: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1130: training loss: 502.6790779578909\n",
      "Epoch 4 step 1131: training accuarcy: 0.93\n",
      "Epoch 4 step 1131: training loss: 499.9209573032777\n",
      "Epoch 4 step 1132: training accuarcy: 0.9265\n",
      "Epoch 4 step 1132: training loss: 471.4471025388642\n",
      "Epoch 4 step 1133: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1133: training loss: 505.28503635829\n",
      "Epoch 4 step 1134: training accuarcy: 0.933\n",
      "Epoch 4 step 1134: training loss: 500.6057543846951\n",
      "Epoch 4 step 1135: training accuarcy: 0.931\n",
      "Epoch 4 step 1135: training loss: 476.81830133964263\n",
      "Epoch 4 step 1136: training accuarcy: 0.9355\n",
      "Epoch 4 step 1136: training loss: 500.23706419609647\n",
      "Epoch 4 step 1137: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1137: training loss: 492.7494687494114\n",
      "Epoch 4 step 1138: training accuarcy: 0.933\n",
      "Epoch 4 step 1138: training loss: 472.97427583483204\n",
      "Epoch 4 step 1139: training accuarcy: 0.9385\n",
      "Epoch 4 step 1139: training loss: 484.9685377202417\n",
      "Epoch 4 step 1140: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1140: training loss: 483.04747723974407\n",
      "Epoch 4 step 1141: training accuarcy: 0.9355\n",
      "Epoch 4 step 1141: training loss: 478.6710343315687\n",
      "Epoch 4 step 1142: training accuarcy: 0.937\n",
      "Epoch 4 step 1142: training loss: 490.88491428158267\n",
      "Epoch 4 step 1143: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1143: training loss: 494.25228382533703\n",
      "Epoch 4 step 1144: training accuarcy: 0.9295\n",
      "Epoch 4 step 1144: training loss: 509.45868677935204\n",
      "Epoch 4 step 1145: training accuarcy: 0.9235\n",
      "Epoch 4 step 1145: training loss: 492.22455720255795\n",
      "Epoch 4 step 1146: training accuarcy: 0.9395\n",
      "Epoch 4 step 1146: training loss: 471.8001596620112\n",
      "Epoch 4 step 1147: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1147: training loss: 494.846420510376\n",
      "Epoch 4 step 1148: training accuarcy: 0.9395\n",
      "Epoch 4 step 1148: training loss: 468.49638585173216\n",
      "Epoch 4 step 1149: training accuarcy: 0.9375\n",
      "Epoch 4 step 1149: training loss: 483.4378160788602\n",
      "Epoch 4 step 1150: training accuarcy: 0.9365\n",
      "Epoch 4 step 1150: training loss: 462.3801238177543\n",
      "Epoch 4 step 1151: training accuarcy: 0.9505\n",
      "Epoch 4 step 1151: training loss: 461.494410407486\n",
      "Epoch 4 step 1152: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1152: training loss: 469.84480631447485\n",
      "Epoch 4 step 1153: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1153: training loss: 464.4749573275054\n",
      "Epoch 4 step 1154: training accuarcy: 0.9435\n",
      "Epoch 4 step 1154: training loss: 479.8925389967162\n",
      "Epoch 4 step 1155: training accuarcy: 0.9365\n",
      "Epoch 4 step 1155: training loss: 469.9890287249264\n",
      "Epoch 4 step 1156: training accuarcy: 0.9435\n",
      "Epoch 4 step 1156: training loss: 486.3382226347667\n",
      "Epoch 4 step 1157: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1157: training loss: 468.401664988185\n",
      "Epoch 4 step 1158: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 1158: training loss: 469.5267514052423\n",
      "Epoch 4 step 1159: training accuarcy: 0.9425\n",
      "Epoch 4 step 1159: training loss: 481.23053195669655\n",
      "Epoch 4 step 1160: training accuarcy: 0.9335\n",
      "Epoch 4 step 1160: training loss: 489.709140763926\n",
      "Epoch 4 step 1161: training accuarcy: 0.9385\n",
      "Epoch 4 step 1161: training loss: 499.8584116968265\n",
      "Epoch 4 step 1162: training accuarcy: 0.9305\n",
      "Epoch 4 step 1162: training loss: 465.1122260206523\n",
      "Epoch 4 step 1163: training accuarcy: 0.9465\n",
      "Epoch 4 step 1163: training loss: 457.73204845113776\n",
      "Epoch 4 step 1164: training accuarcy: 0.9445\n",
      "Epoch 4 step 1164: training loss: 476.8544240600423\n",
      "Epoch 4 step 1165: training accuarcy: 0.9445\n",
      "Epoch 4 step 1165: training loss: 455.3588765323231\n",
      "Epoch 4 step 1166: training accuarcy: 0.9455\n",
      "Epoch 4 step 1166: training loss: 487.3999335480377\n",
      "Epoch 4 step 1167: training accuarcy: 0.934\n",
      "Epoch 4 step 1167: training loss: 461.71594309010914\n",
      "Epoch 4 step 1168: training accuarcy: 0.9445\n",
      "Epoch 4 step 1168: training loss: 470.50076215117264\n",
      "Epoch 4 step 1169: training accuarcy: 0.9395\n",
      "Epoch 4 step 1169: training loss: 468.4199258333835\n",
      "Epoch 4 step 1170: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1170: training loss: 465.5133105714327\n",
      "Epoch 4 step 1171: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1171: training loss: 484.5346706754307\n",
      "Epoch 4 step 1172: training accuarcy: 0.935\n",
      "Epoch 4 step 1172: training loss: 488.02206773670906\n",
      "Epoch 4 step 1173: training accuarcy: 0.926\n",
      "Epoch 4 step 1173: training loss: 471.76880001373735\n",
      "Epoch 4 step 1174: training accuarcy: 0.9365\n",
      "Epoch 4 step 1174: training loss: 472.7614209171197\n",
      "Epoch 4 step 1175: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1175: training loss: 494.3852302811872\n",
      "Epoch 4 step 1176: training accuarcy: 0.93\n",
      "Epoch 4 step 1176: training loss: 483.71007606125175\n",
      "Epoch 4 step 1177: training accuarcy: 0.9375\n",
      "Epoch 4 step 1177: training loss: 479.5029007183392\n",
      "Epoch 4 step 1178: training accuarcy: 0.932\n",
      "Epoch 4 step 1178: training loss: 473.381348554976\n",
      "Epoch 4 step 1179: training accuarcy: 0.9335\n",
      "Epoch 4 step 1179: training loss: 465.11073405430943\n",
      "Epoch 4 step 1180: training accuarcy: 0.9405\n",
      "Epoch 4 step 1180: training loss: 465.328812839948\n",
      "Epoch 4 step 1181: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1181: training loss: 476.50874380373205\n",
      "Epoch 4 step 1182: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1182: training loss: 481.43641422895735\n",
      "Epoch 4 step 1183: training accuarcy: 0.934\n",
      "Epoch 4 step 1183: training loss: 469.7141408969687\n",
      "Epoch 4 step 1184: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1184: training loss: 460.6425719121952\n",
      "Epoch 4 step 1185: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1185: training loss: 462.2993968596558\n",
      "Epoch 4 step 1186: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1186: training loss: 467.80833151042725\n",
      "Epoch 4 step 1187: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1187: training loss: 455.6187874795244\n",
      "Epoch 4 step 1188: training accuarcy: 0.9395\n",
      "Epoch 4 step 1188: training loss: 462.42309359294165\n",
      "Epoch 4 step 1189: training accuarcy: 0.9405\n",
      "Epoch 4 step 1189: training loss: 479.5481842955949\n",
      "Epoch 4 step 1190: training accuarcy: 0.9385\n",
      "Epoch 4 step 1190: training loss: 471.7810265002425\n",
      "Epoch 4 step 1191: training accuarcy: 0.9375\n",
      "Epoch 4 step 1191: training loss: 484.1462238803597\n",
      "Epoch 4 step 1192: training accuarcy: 0.9345\n",
      "Epoch 4 step 1192: training loss: 469.73108678763117\n",
      "Epoch 4 step 1193: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1193: training loss: 466.0396115755104\n",
      "Epoch 4 step 1194: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1194: training loss: 473.7569949764664\n",
      "Epoch 4 step 1195: training accuarcy: 0.9365\n",
      "Epoch 4 step 1195: training loss: 482.44462185981394\n",
      "Epoch 4 step 1196: training accuarcy: 0.936\n",
      "Epoch 4 step 1196: training loss: 484.8918547637384\n",
      "Epoch 4 step 1197: training accuarcy: 0.9395\n",
      "Epoch 4 step 1197: training loss: 481.58321099182996\n",
      "Epoch 4 step 1198: training accuarcy: 0.9425\n",
      "Epoch 4 step 1198: training loss: 482.76267814694137\n",
      "Epoch 4 step 1199: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1199: training loss: 455.09709150075616\n",
      "Epoch 4 step 1200: training accuarcy: 0.9465\n",
      "Epoch 4 step 1200: training loss: 472.7010102197713\n",
      "Epoch 4 step 1201: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1201: training loss: 491.4278390384545\n",
      "Epoch 4 step 1202: training accuarcy: 0.933\n",
      "Epoch 4 step 1202: training loss: 462.1368040257156\n",
      "Epoch 4 step 1203: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1203: training loss: 480.19431698784024\n",
      "Epoch 4 step 1204: training accuarcy: 0.9395\n",
      "Epoch 4 step 1204: training loss: 483.5022266024404\n",
      "Epoch 4 step 1205: training accuarcy: 0.9405\n",
      "Epoch 4 step 1205: training loss: 481.1574636665825\n",
      "Epoch 4 step 1206: training accuarcy: 0.9395\n",
      "Epoch 4 step 1206: training loss: 456.54305548946695\n",
      "Epoch 4 step 1207: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1207: training loss: 455.3013469556173\n",
      "Epoch 4 step 1208: training accuarcy: 0.9525\n",
      "Epoch 4 step 1208: training loss: 506.1382926635429\n",
      "Epoch 4 step 1209: training accuarcy: 0.9275\n",
      "Epoch 4 step 1209: training loss: 490.71657855168803\n",
      "Epoch 4 step 1210: training accuarcy: 0.9375\n",
      "Epoch 4 step 1210: training loss: 474.10651690065816\n",
      "Epoch 4 step 1211: training accuarcy: 0.936\n",
      "Epoch 4 step 1211: training loss: 445.5364072256615\n",
      "Epoch 4 step 1212: training accuarcy: 0.9445\n",
      "Epoch 4 step 1212: training loss: 455.56855665962684\n",
      "Epoch 4 step 1213: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1213: training loss: 460.92138550567165\n",
      "Epoch 4 step 1214: training accuarcy: 0.9445\n",
      "Epoch 4 step 1214: training loss: 447.17951914716593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 step 1215: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 1215: training loss: 469.529197750791\n",
      "Epoch 4 step 1216: training accuarcy: 0.9385\n",
      "Epoch 4 step 1216: training loss: 464.0903094449788\n",
      "Epoch 4 step 1217: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1217: training loss: 444.28034476058025\n",
      "Epoch 4 step 1218: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 1218: training loss: 460.46076550649326\n",
      "Epoch 4 step 1219: training accuarcy: 0.9415\n",
      "Epoch 4 step 1219: training loss: 450.6857025809599\n",
      "Epoch 4 step 1220: training accuarcy: 0.9455\n",
      "Epoch 4 step 1220: training loss: 470.5263183588476\n",
      "Epoch 4 step 1221: training accuarcy: 0.937\n",
      "Epoch 4 step 1221: training loss: 476.2984204497551\n",
      "Epoch 4 step 1222: training accuarcy: 0.9380000000000001\n",
      "Epoch 4 step 1222: training loss: 475.4403628080806\n",
      "Epoch 4 step 1223: training accuarcy: 0.9380000000000001\n",
      "Epoch 4 step 1223: training loss: 457.44684615694365\n",
      "Epoch 4 step 1224: training accuarcy: 0.9495\n",
      "Epoch 4 step 1224: training loss: 471.9558452999809\n",
      "Epoch 4 step 1225: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1225: training loss: 457.7986752798578\n",
      "Epoch 4 step 1226: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1226: training loss: 443.8056574290286\n",
      "Epoch 4 step 1227: training accuarcy: 0.9415\n",
      "Epoch 4 step 1227: training loss: 432.8606430643479\n",
      "Epoch 4 step 1228: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 1228: training loss: 455.7023687379576\n",
      "Epoch 4 step 1229: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1229: training loss: 467.4789656488462\n",
      "Epoch 4 step 1230: training accuarcy: 0.9385\n",
      "Epoch 4 step 1230: training loss: 460.12522795187175\n",
      "Epoch 4 step 1231: training accuarcy: 0.9435\n",
      "Epoch 4 step 1231: training loss: 477.201759595757\n",
      "Epoch 4 step 1232: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1232: training loss: 480.1777416894801\n",
      "Epoch 4 step 1233: training accuarcy: 0.935\n",
      "Epoch 4 step 1233: training loss: 430.9951029202602\n",
      "Epoch 4 step 1234: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 1234: training loss: 474.62740433157296\n",
      "Epoch 4 step 1235: training accuarcy: 0.9375\n",
      "Epoch 4 step 1235: training loss: 455.5872366932029\n",
      "Epoch 4 step 1236: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 1236: training loss: 445.9681893524446\n",
      "Epoch 4 step 1237: training accuarcy: 0.9375\n",
      "Epoch 4 step 1237: training loss: 447.076585326452\n",
      "Epoch 4 step 1238: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 1238: training loss: 461.65606362375235\n",
      "Epoch 4 step 1239: training accuarcy: 0.9395\n",
      "Epoch 4 step 1239: training loss: 442.28539422217716\n",
      "Epoch 4 step 1240: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1240: training loss: 442.1063857898716\n",
      "Epoch 4 step 1241: training accuarcy: 0.9530000000000001\n",
      "Epoch 4 step 1241: training loss: 445.9413995177998\n",
      "Epoch 4 step 1242: training accuarcy: 0.9475\n",
      "Epoch 4 step 1242: training loss: 456.19445187494534\n",
      "Epoch 4 step 1243: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1243: training loss: 476.9939325801484\n",
      "Epoch 4 step 1244: training accuarcy: 0.9405\n",
      "Epoch 4 step 1244: training loss: 457.069751998839\n",
      "Epoch 4 step 1245: training accuarcy: 0.9385\n",
      "Epoch 4 step 1245: training loss: 479.7018867543972\n",
      "Epoch 4 step 1246: training accuarcy: 0.9405\n",
      "Epoch 4 step 1246: training loss: 445.647366195505\n",
      "Epoch 4 step 1247: training accuarcy: 0.9495\n",
      "Epoch 4 step 1247: training loss: 444.0147201748429\n",
      "Epoch 4 step 1248: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1248: training loss: 469.330674067352\n",
      "Epoch 4 step 1249: training accuarcy: 0.9425\n",
      "Epoch 4 step 1249: training loss: 438.6148025956452\n",
      "Epoch 4 step 1250: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1250: training loss: 447.0933075827592\n",
      "Epoch 4 step 1251: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 1251: training loss: 438.42254482038516\n",
      "Epoch 4 step 1252: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1252: training loss: 457.4336590970104\n",
      "Epoch 4 step 1253: training accuarcy: 0.9490000000000001\n",
      "Epoch 4 step 1253: training loss: 438.7206899838776\n",
      "Epoch 4 step 1254: training accuarcy: 0.9445\n",
      "Epoch 4 step 1254: training loss: 468.395995774145\n",
      "Epoch 4 step 1255: training accuarcy: 0.935\n",
      "Epoch 4 step 1255: training loss: 446.9731290611329\n",
      "Epoch 4 step 1256: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1256: training loss: 447.1388504772879\n",
      "Epoch 4 step 1257: training accuarcy: 0.9445\n",
      "Epoch 4 step 1257: training loss: 471.4811762022541\n",
      "Epoch 4 step 1258: training accuarcy: 0.934\n",
      "Epoch 4 step 1258: training loss: 455.10926135566973\n",
      "Epoch 4 step 1259: training accuarcy: 0.9465\n",
      "Epoch 4 step 1259: training loss: 464.69079424067945\n",
      "Epoch 4 step 1260: training accuarcy: 0.9355\n",
      "Epoch 4 step 1260: training loss: 464.67178485267834\n",
      "Epoch 4 step 1261: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1261: training loss: 454.55164745757247\n",
      "Epoch 4 step 1262: training accuarcy: 0.9450000000000001\n",
      "Epoch 4 step 1262: training loss: 451.9843377201054\n",
      "Epoch 4 step 1263: training accuarcy: 0.9395\n",
      "Epoch 4 step 1263: training loss: 443.76614287443607\n",
      "Epoch 4 step 1264: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 1264: training loss: 454.01820049405086\n",
      "Epoch 4 step 1265: training accuarcy: 0.9445\n",
      "Epoch 4 step 1265: training loss: 450.94618384846314\n",
      "Epoch 4 step 1266: training accuarcy: 0.9410000000000001\n",
      "Epoch 4 step 1266: training loss: 450.36609009856767\n",
      "Epoch 4 step 1267: training accuarcy: 0.9445\n",
      "Epoch 4 step 1267: training loss: 465.2033796765868\n",
      "Epoch 4 step 1268: training accuarcy: 0.9405\n",
      "Epoch 4 step 1268: training loss: 444.2031341873577\n",
      "Epoch 4 step 1269: training accuarcy: 0.9400000000000001\n",
      "Epoch 4 step 1269: training loss: 455.9952694442733\n",
      "Epoch 4 step 1270: training accuarcy: 0.9425\n",
      "Epoch 4 step 1270: training loss: 460.0478680698582\n",
      "Epoch 4 step 1271: training accuarcy: 0.937\n",
      "Epoch 4 step 1271: training loss: 440.04450557936565\n",
      "Epoch 4 step 1272: training accuarcy: 0.9425\n",
      "Epoch 4 step 1272: training loss: 434.4513021411015\n",
      "Epoch 4 step 1273: training accuarcy: 0.9480000000000001\n",
      "Epoch 4 step 1273: training loss: 450.98038773258975\n",
      "Epoch 4 step 1274: training accuarcy: 0.9390000000000001\n",
      "Epoch 4 step 1274: training loss: 437.3272533811769\n",
      "Epoch 4 step 1275: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1275: training loss: 437.2229026668565\n",
      "Epoch 4 step 1276: training accuarcy: 0.9455\n",
      "Epoch 4 step 1276: training loss: 461.2048961424606\n",
      "Epoch 4 step 1277: training accuarcy: 0.935\n",
      "Epoch 4 step 1277: training loss: 428.41042582559135\n",
      "Epoch 4 step 1278: training accuarcy: 0.9455\n",
      "Epoch 4 step 1278: training loss: 444.3160709339044\n",
      "Epoch 4 step 1279: training accuarcy: 0.9395\n",
      "Epoch 4 step 1279: training loss: 455.168858874797\n",
      "Epoch 4 step 1280: training accuarcy: 0.937\n",
      "Epoch 4 step 1280: training loss: 448.90782903199494\n",
      "Epoch 4 step 1281: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1281: training loss: 452.9701473633489\n",
      "Epoch 4 step 1282: training accuarcy: 0.9415\n",
      "Epoch 4 step 1282: training loss: 449.18324693380157\n",
      "Epoch 4 step 1283: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1283: training loss: 435.06067261938495\n",
      "Epoch 4 step 1284: training accuarcy: 0.9445\n",
      "Epoch 4 step 1284: training loss: 441.24530764822333\n",
      "Epoch 4 step 1285: training accuarcy: 0.9510000000000001\n",
      "Epoch 4 step 1285: training loss: 423.47247450575264\n",
      "Epoch 4 step 1286: training accuarcy: 0.9470000000000001\n",
      "Epoch 4 step 1286: training loss: 430.73200725643744\n",
      "Epoch 4 step 1287: training accuarcy: 0.9540000000000001\n",
      "Epoch 4 step 1287: training loss: 452.13582095990586\n",
      "Epoch 4 step 1288: training accuarcy: 0.9385\n",
      "Epoch 4 step 1288: training loss: 450.76895380224596\n",
      "Epoch 4 step 1289: training accuarcy: 0.9425\n",
      "Epoch 4 step 1289: training loss: 443.1329476891979\n",
      "Epoch 4 step 1290: training accuarcy: 0.9475\n",
      "Epoch 4 step 1290: training loss: 456.5185784517656\n",
      "Epoch 4 step 1291: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1291: training loss: 440.2267482490948\n",
      "Epoch 4 step 1292: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1292: training loss: 442.84806355752863\n",
      "Epoch 4 step 1293: training accuarcy: 0.9530000000000001\n",
      "Epoch 4 step 1293: training loss: 425.1855362044043\n",
      "Epoch 4 step 1294: training accuarcy: 0.9520000000000001\n",
      "Epoch 4 step 1294: training loss: 437.1273217584302\n",
      "Epoch 4 step 1295: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1295: training loss: 444.1141464316622\n",
      "Epoch 4 step 1296: training accuarcy: 0.9385\n",
      "Epoch 4 step 1296: training loss: 437.65787754953857\n",
      "Epoch 4 step 1297: training accuarcy: 0.9460000000000001\n",
      "Epoch 4 step 1297: training loss: 460.94384804461305\n",
      "Epoch 4 step 1298: training accuarcy: 0.936\n",
      "Epoch 4 step 1298: training loss: 440.6641427777167\n",
      "Epoch 4 step 1299: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1299: training loss: 432.6464711463914\n",
      "Epoch 4 step 1300: training accuarcy: 0.9440000000000001\n",
      "Epoch 4 step 1300: training loss: 443.52887251346385\n",
      "Epoch 4 step 1301: training accuarcy: 0.9435\n",
      "Epoch 4 step 1301: training loss: 435.30653725169236\n",
      "Epoch 4 step 1302: training accuarcy: 0.9505\n",
      "Epoch 4 step 1302: training loss: 427.79114290583607\n",
      "Epoch 4 step 1303: training accuarcy: 0.9465\n",
      "Epoch 4 step 1303: training loss: 429.83220819100416\n",
      "Epoch 4 step 1304: training accuarcy: 0.9435\n",
      "Epoch 4 step 1304: training loss: 472.9038115941386\n",
      "Epoch 4 step 1305: training accuarcy: 0.927\n",
      "Epoch 4 step 1305: training loss: 452.0153038690386\n",
      "Epoch 4 step 1306: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1306: training loss: 438.03376195206914\n",
      "Epoch 4 step 1307: training accuarcy: 0.9495\n",
      "Epoch 4 step 1307: training loss: 466.2403743986378\n",
      "Epoch 4 step 1308: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1308: training loss: 450.1246393626815\n",
      "Epoch 4 step 1309: training accuarcy: 0.9430000000000001\n",
      "Epoch 4 step 1309: training loss: 445.32260632101094\n",
      "Epoch 4 step 1310: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1310: training loss: 437.6925308676833\n",
      "Epoch 4 step 1311: training accuarcy: 0.9500000000000001\n",
      "Epoch 4 step 1311: training loss: 434.6035986935275\n",
      "Epoch 4 step 1312: training accuarcy: 0.9505\n",
      "Epoch 4 step 1312: training loss: 450.9117122959288\n",
      "Epoch 4 step 1313: training accuarcy: 0.9420000000000001\n",
      "Epoch 4 step 1313: training loss: 417.12124505241593\n",
      "Epoch 4 step 1314: training accuarcy: 0.9530000000000001\n",
      "Epoch 4 step 1314: training loss: 201.572068130793\n",
      "Epoch 4 step 1315: training accuarcy: 0.9320512820512821\n",
      "Epoch 4: train loss 470.036798754853, train accuarcy 0.9173548221588135\n",
      "Epoch 4: valid loss 854.5999364682344, valid accuarcy 0.8117040395736694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                         | 5/8 [11:08<06:40, 133.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Epoch 5 step 1315: training loss: 382.69158540254244\n",
      "Epoch 5 step 1316: training accuarcy: 0.9635\n",
      "Epoch 5 step 1316: training loss: 378.9544995619815\n",
      "Epoch 5 step 1317: training accuarcy: 0.9675\n",
      "Epoch 5 step 1317: training loss: 363.4918250079975\n",
      "Epoch 5 step 1318: training accuarcy: 0.9655\n",
      "Epoch 5 step 1318: training loss: 377.13418737692473\n",
      "Epoch 5 step 1319: training accuarcy: 0.9645\n",
      "Epoch 5 step 1319: training loss: 369.4395039650103\n",
      "Epoch 5 step 1320: training accuarcy: 0.968\n",
      "Epoch 5 step 1320: training loss: 368.33549376382643\n",
      "Epoch 5 step 1321: training accuarcy: 0.967\n",
      "Epoch 5 step 1321: training loss: 376.05285522612644\n",
      "Epoch 5 step 1322: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 1322: training loss: 383.3455247175375\n",
      "Epoch 5 step 1323: training accuarcy: 0.9560000000000001\n",
      "Epoch 5 step 1323: training loss: 394.03600890055907\n",
      "Epoch 5 step 1324: training accuarcy: 0.96\n",
      "Epoch 5 step 1324: training loss: 368.10827830408823\n",
      "Epoch 5 step 1325: training accuarcy: 0.9605\n",
      "Epoch 5 step 1325: training loss: 357.2667778923073\n",
      "Epoch 5 step 1326: training accuarcy: 0.9665\n",
      "Epoch 5 step 1326: training loss: 385.37176866324285\n",
      "Epoch 5 step 1327: training accuarcy: 0.9555\n",
      "Epoch 5 step 1327: training loss: 389.00166681687546\n",
      "Epoch 5 step 1328: training accuarcy: 0.9525\n",
      "Epoch 5 step 1328: training loss: 376.73723843418446\n",
      "Epoch 5 step 1329: training accuarcy: 0.9685\n",
      "Epoch 5 step 1329: training loss: 373.0386316723108\n",
      "Epoch 5 step 1330: training accuarcy: 0.9615\n",
      "Epoch 5 step 1330: training loss: 384.1369031983517\n",
      "Epoch 5 step 1331: training accuarcy: 0.963\n",
      "Epoch 5 step 1331: training loss: 389.1043689828713\n",
      "Epoch 5 step 1332: training accuarcy: 0.964\n",
      "Epoch 5 step 1332: training loss: 380.9112900758134\n",
      "Epoch 5 step 1333: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1333: training loss: 391.9172942477719\n",
      "Epoch 5 step 1334: training accuarcy: 0.9575\n",
      "Epoch 5 step 1334: training loss: 388.8439137647177\n",
      "Epoch 5 step 1335: training accuarcy: 0.9585\n",
      "Epoch 5 step 1335: training loss: 374.87974920685036\n",
      "Epoch 5 step 1336: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1336: training loss: 368.54480448553215\n",
      "Epoch 5 step 1337: training accuarcy: 0.9625\n",
      "Epoch 5 step 1337: training loss: 360.83882997419045\n",
      "Epoch 5 step 1338: training accuarcy: 0.9685\n",
      "Epoch 5 step 1338: training loss: 362.49080658433564\n",
      "Epoch 5 step 1339: training accuarcy: 0.9655\n",
      "Epoch 5 step 1339: training loss: 369.37269297567155\n",
      "Epoch 5 step 1340: training accuarcy: 0.9675\n",
      "Epoch 5 step 1340: training loss: 354.9684758337568\n",
      "Epoch 5 step 1341: training accuarcy: 0.9675\n",
      "Epoch 5 step 1341: training loss: 356.2151078093665\n",
      "Epoch 5 step 1342: training accuarcy: 0.97\n",
      "Epoch 5 step 1342: training loss: 369.45502012874124\n",
      "Epoch 5 step 1343: training accuarcy: 0.965\n",
      "Epoch 5 step 1343: training loss: 385.3505082581804\n",
      "Epoch 5 step 1344: training accuarcy: 0.965\n",
      "Epoch 5 step 1344: training loss: 373.0715085828656\n",
      "Epoch 5 step 1345: training accuarcy: 0.9605\n",
      "Epoch 5 step 1345: training loss: 368.9012939472809\n",
      "Epoch 5 step 1346: training accuarcy: 0.9645\n",
      "Epoch 5 step 1346: training loss: 360.8844567659201\n",
      "Epoch 5 step 1347: training accuarcy: 0.966\n",
      "Epoch 5 step 1347: training loss: 379.11061297692413\n",
      "Epoch 5 step 1348: training accuarcy: 0.962\n",
      "Epoch 5 step 1348: training loss: 342.44666994510885\n",
      "Epoch 5 step 1349: training accuarcy: 0.967\n",
      "Epoch 5 step 1349: training loss: 372.8796569881777\n",
      "Epoch 5 step 1350: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1350: training loss: 368.33157205627987\n",
      "Epoch 5 step 1351: training accuarcy: 0.9635\n",
      "Epoch 5 step 1351: training loss: 371.73619924450577\n",
      "Epoch 5 step 1352: training accuarcy: 0.962\n",
      "Epoch 5 step 1352: training loss: 374.5977572445264\n",
      "Epoch 5 step 1353: training accuarcy: 0.961\n",
      "Epoch 5 step 1353: training loss: 361.19481168317566\n",
      "Epoch 5 step 1354: training accuarcy: 0.9655\n",
      "Epoch 5 step 1354: training loss: 382.75568958326437\n",
      "Epoch 5 step 1355: training accuarcy: 0.9575\n",
      "Epoch 5 step 1355: training loss: 371.43406137498215\n",
      "Epoch 5 step 1356: training accuarcy: 0.9605\n",
      "Epoch 5 step 1356: training loss: 366.161139341541\n",
      "Epoch 5 step 1357: training accuarcy: 0.9625\n",
      "Epoch 5 step 1357: training loss: 365.4045230382417\n",
      "Epoch 5 step 1358: training accuarcy: 0.9605\n",
      "Epoch 5 step 1358: training loss: 376.73902495310557\n",
      "Epoch 5 step 1359: training accuarcy: 0.9585\n",
      "Epoch 5 step 1359: training loss: 343.105716176177\n",
      "Epoch 5 step 1360: training accuarcy: 0.9705\n",
      "Epoch 5 step 1360: training loss: 383.26556953612214\n",
      "Epoch 5 step 1361: training accuarcy: 0.9595\n",
      "Epoch 5 step 1361: training loss: 367.3349134494623\n",
      "Epoch 5 step 1362: training accuarcy: 0.9605\n",
      "Epoch 5 step 1362: training loss: 372.37156609854026\n",
      "Epoch 5 step 1363: training accuarcy: 0.96\n",
      "Epoch 5 step 1363: training loss: 369.0460028449929\n",
      "Epoch 5 step 1364: training accuarcy: 0.962\n",
      "Epoch 5 step 1364: training loss: 372.3194152366676\n",
      "Epoch 5 step 1365: training accuarcy: 0.964\n",
      "Epoch 5 step 1365: training loss: 388.2204354919293\n",
      "Epoch 5 step 1366: training accuarcy: 0.9555\n",
      "Epoch 5 step 1366: training loss: 368.5187463442473\n",
      "Epoch 5 step 1367: training accuarcy: 0.962\n",
      "Epoch 5 step 1367: training loss: 371.0222410766313\n",
      "Epoch 5 step 1368: training accuarcy: 0.9585\n",
      "Epoch 5 step 1368: training loss: 360.6768512767868\n",
      "Epoch 5 step 1369: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 1369: training loss: 360.01559048845655\n",
      "Epoch 5 step 1370: training accuarcy: 0.964\n",
      "Epoch 5 step 1370: training loss: 354.3760515096719\n",
      "Epoch 5 step 1371: training accuarcy: 0.968\n",
      "Epoch 5 step 1371: training loss: 368.5888367295678\n",
      "Epoch 5 step 1372: training accuarcy: 0.963\n",
      "Epoch 5 step 1372: training loss: 361.2251911302937\n",
      "Epoch 5 step 1373: training accuarcy: 0.9625\n",
      "Epoch 5 step 1373: training loss: 389.5425313718113\n",
      "Epoch 5 step 1374: training accuarcy: 0.9555\n",
      "Epoch 5 step 1374: training loss: 359.0013039747652\n",
      "Epoch 5 step 1375: training accuarcy: 0.9675\n",
      "Epoch 5 step 1375: training loss: 376.09999198336124\n",
      "Epoch 5 step 1376: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 1376: training loss: 353.60269003695134\n",
      "Epoch 5 step 1377: training accuarcy: 0.967\n",
      "Epoch 5 step 1377: training loss: 356.32404841826167\n",
      "Epoch 5 step 1378: training accuarcy: 0.965\n",
      "Epoch 5 step 1378: training loss: 353.91622839794195\n",
      "Epoch 5 step 1379: training accuarcy: 0.963\n",
      "Epoch 5 step 1379: training loss: 357.72491450618446\n",
      "Epoch 5 step 1380: training accuarcy: 0.967\n",
      "Epoch 5 step 1380: training loss: 369.8785130250578\n",
      "Epoch 5 step 1381: training accuarcy: 0.964\n",
      "Epoch 5 step 1381: training loss: 367.81600941468423\n",
      "Epoch 5 step 1382: training accuarcy: 0.9575\n",
      "Epoch 5 step 1382: training loss: 356.5015154263381\n",
      "Epoch 5 step 1383: training accuarcy: 0.9695\n",
      "Epoch 5 step 1383: training loss: 369.286776017886\n",
      "Epoch 5 step 1384: training accuarcy: 0.9615\n",
      "Epoch 5 step 1384: training loss: 380.5699752941365\n",
      "Epoch 5 step 1385: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 1385: training loss: 358.44908121216434\n",
      "Epoch 5 step 1386: training accuarcy: 0.965\n",
      "Epoch 5 step 1386: training loss: 355.022224200475\n",
      "Epoch 5 step 1387: training accuarcy: 0.966\n",
      "Epoch 5 step 1387: training loss: 357.78514459163796\n",
      "Epoch 5 step 1388: training accuarcy: 0.969\n",
      "Epoch 5 step 1388: training loss: 349.62268363137645\n",
      "Epoch 5 step 1389: training accuarcy: 0.968\n",
      "Epoch 5 step 1389: training loss: 361.9305219410088\n",
      "Epoch 5 step 1390: training accuarcy: 0.9615\n",
      "Epoch 5 step 1390: training loss: 362.4485247186824\n",
      "Epoch 5 step 1391: training accuarcy: 0.9665\n",
      "Epoch 5 step 1391: training loss: 385.7788082755617\n",
      "Epoch 5 step 1392: training accuarcy: 0.9480000000000001\n",
      "Epoch 5 step 1392: training loss: 364.5551727300041\n",
      "Epoch 5 step 1393: training accuarcy: 0.9615\n",
      "Epoch 5 step 1393: training loss: 354.2507025471991\n",
      "Epoch 5 step 1394: training accuarcy: 0.967\n",
      "Epoch 5 step 1394: training loss: 353.0645352520993\n",
      "Epoch 5 step 1395: training accuarcy: 0.971\n",
      "Epoch 5 step 1395: training loss: 358.2446742843462\n",
      "Epoch 5 step 1396: training accuarcy: 0.9655\n",
      "Epoch 5 step 1396: training loss: 360.2377696111586\n",
      "Epoch 5 step 1397: training accuarcy: 0.963\n",
      "Epoch 5 step 1397: training loss: 376.3593117604212\n",
      "Epoch 5 step 1398: training accuarcy: 0.9580000000000001\n",
      "Epoch 5 step 1398: training loss: 370.61162255082525\n",
      "Epoch 5 step 1399: training accuarcy: 0.965\n",
      "Epoch 5 step 1399: training loss: 352.5954774327864\n",
      "Epoch 5 step 1400: training accuarcy: 0.9655\n",
      "Epoch 5 step 1400: training loss: 372.20125989883155\n",
      "Epoch 5 step 1401: training accuarcy: 0.9605\n",
      "Epoch 5 step 1401: training loss: 367.08795141030123\n",
      "Epoch 5 step 1402: training accuarcy: 0.96\n",
      "Epoch 5 step 1402: training loss: 354.8000940037484\n",
      "Epoch 5 step 1403: training accuarcy: 0.968\n",
      "Epoch 5 step 1403: training loss: 367.3864738985663\n",
      "Epoch 5 step 1404: training accuarcy: 0.9585\n",
      "Epoch 5 step 1404: training loss: 358.9317877069067\n",
      "Epoch 5 step 1405: training accuarcy: 0.967\n",
      "Epoch 5 step 1405: training loss: 366.36194844782074\n",
      "Epoch 5 step 1406: training accuarcy: 0.9605\n",
      "Epoch 5 step 1406: training loss: 371.16212999958697\n",
      "Epoch 5 step 1407: training accuarcy: 0.9575\n",
      "Epoch 5 step 1407: training loss: 349.032322132999\n",
      "Epoch 5 step 1408: training accuarcy: 0.965\n",
      "Epoch 5 step 1408: training loss: 358.2526190063952\n",
      "Epoch 5 step 1409: training accuarcy: 0.966\n",
      "Epoch 5 step 1409: training loss: 364.61490573103475\n",
      "Epoch 5 step 1410: training accuarcy: 0.963\n",
      "Epoch 5 step 1410: training loss: 367.0863498712427\n",
      "Epoch 5 step 1411: training accuarcy: 0.9605\n",
      "Epoch 5 step 1411: training loss: 343.38363423700736\n",
      "Epoch 5 step 1412: training accuarcy: 0.97\n",
      "Epoch 5 step 1412: training loss: 357.2337516810921\n",
      "Epoch 5 step 1413: training accuarcy: 0.963\n",
      "Epoch 5 step 1413: training loss: 366.47524018676086\n",
      "Epoch 5 step 1414: training accuarcy: 0.9575\n",
      "Epoch 5 step 1414: training loss: 350.403840332308\n",
      "Epoch 5 step 1415: training accuarcy: 0.967\n",
      "Epoch 5 step 1415: training loss: 363.8162357412618\n",
      "Epoch 5 step 1416: training accuarcy: 0.963\n",
      "Epoch 5 step 1416: training loss: 364.63344048445055\n",
      "Epoch 5 step 1417: training accuarcy: 0.9625\n",
      "Epoch 5 step 1417: training loss: 361.16233400629164\n",
      "Epoch 5 step 1418: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1418: training loss: 388.7908599634804\n",
      "Epoch 5 step 1419: training accuarcy: 0.9500000000000001\n",
      "Epoch 5 step 1419: training loss: 346.6927080073898\n",
      "Epoch 5 step 1420: training accuarcy: 0.972\n",
      "Epoch 5 step 1420: training loss: 358.27901023607683\n",
      "Epoch 5 step 1421: training accuarcy: 0.9625\n",
      "Epoch 5 step 1421: training loss: 361.0686723852575\n",
      "Epoch 5 step 1422: training accuarcy: 0.963\n",
      "Epoch 5 step 1422: training loss: 352.0415604391552\n",
      "Epoch 5 step 1423: training accuarcy: 0.966\n",
      "Epoch 5 step 1423: training loss: 361.1486457862621\n",
      "Epoch 5 step 1424: training accuarcy: 0.966\n",
      "Epoch 5 step 1424: training loss: 347.2244688368544\n",
      "Epoch 5 step 1425: training accuarcy: 0.966\n",
      "Epoch 5 step 1425: training loss: 376.1165733419272\n",
      "Epoch 5 step 1426: training accuarcy: 0.96\n",
      "Epoch 5 step 1426: training loss: 377.4489074725743\n",
      "Epoch 5 step 1427: training accuarcy: 0.9605\n",
      "Epoch 5 step 1427: training loss: 346.4394765900143\n",
      "Epoch 5 step 1428: training accuarcy: 0.966\n",
      "Epoch 5 step 1428: training loss: 354.1851974650243\n",
      "Epoch 5 step 1429: training accuarcy: 0.969\n",
      "Epoch 5 step 1429: training loss: 360.65270195116494\n",
      "Epoch 5 step 1430: training accuarcy: 0.965\n",
      "Epoch 5 step 1430: training loss: 370.83477884705684\n",
      "Epoch 5 step 1431: training accuarcy: 0.9565\n",
      "Epoch 5 step 1431: training loss: 365.1891086383447\n",
      "Epoch 5 step 1432: training accuarcy: 0.965\n",
      "Epoch 5 step 1432: training loss: 357.43189153602316\n",
      "Epoch 5 step 1433: training accuarcy: 0.964\n",
      "Epoch 5 step 1433: training loss: 350.93545405002556\n",
      "Epoch 5 step 1434: training accuarcy: 0.964\n",
      "Epoch 5 step 1434: training loss: 347.36028082263385\n",
      "Epoch 5 step 1435: training accuarcy: 0.9655\n",
      "Epoch 5 step 1435: training loss: 351.72863533377955\n",
      "Epoch 5 step 1436: training accuarcy: 0.9665\n",
      "Epoch 5 step 1436: training loss: 362.7678980001666\n",
      "Epoch 5 step 1437: training accuarcy: 0.963\n",
      "Epoch 5 step 1437: training loss: 348.38045479655256\n",
      "Epoch 5 step 1438: training accuarcy: 0.963\n",
      "Epoch 5 step 1438: training loss: 351.14206135710157\n",
      "Epoch 5 step 1439: training accuarcy: 0.969\n",
      "Epoch 5 step 1439: training loss: 347.14807425850336\n",
      "Epoch 5 step 1440: training accuarcy: 0.97\n",
      "Epoch 5 step 1440: training loss: 359.9599815866347\n",
      "Epoch 5 step 1441: training accuarcy: 0.96\n",
      "Epoch 5 step 1441: training loss: 351.43981631212944\n",
      "Epoch 5 step 1442: training accuarcy: 0.965\n",
      "Epoch 5 step 1442: training loss: 343.63756342483043\n",
      "Epoch 5 step 1443: training accuarcy: 0.968\n",
      "Epoch 5 step 1443: training loss: 350.4423985600214\n",
      "Epoch 5 step 1444: training accuarcy: 0.9675\n",
      "Epoch 5 step 1444: training loss: 351.9021356778893\n",
      "Epoch 5 step 1445: training accuarcy: 0.9655\n",
      "Epoch 5 step 1445: training loss: 363.8479465401379\n",
      "Epoch 5 step 1446: training accuarcy: 0.9595\n",
      "Epoch 5 step 1446: training loss: 357.37071484099357\n",
      "Epoch 5 step 1447: training accuarcy: 0.962\n",
      "Epoch 5 step 1447: training loss: 350.4840675754159\n",
      "Epoch 5 step 1448: training accuarcy: 0.964\n",
      "Epoch 5 step 1448: training loss: 372.1700948606445\n",
      "Epoch 5 step 1449: training accuarcy: 0.9535\n",
      "Epoch 5 step 1449: training loss: 352.5597559063592\n",
      "Epoch 5 step 1450: training accuarcy: 0.966\n",
      "Epoch 5 step 1450: training loss: 357.821348152241\n",
      "Epoch 5 step 1451: training accuarcy: 0.9595\n",
      "Epoch 5 step 1451: training loss: 358.10026354836805\n",
      "Epoch 5 step 1452: training accuarcy: 0.9625\n",
      "Epoch 5 step 1452: training loss: 338.15404411957724\n",
      "Epoch 5 step 1453: training accuarcy: 0.967\n",
      "Epoch 5 step 1453: training loss: 365.36176451532344\n",
      "Epoch 5 step 1454: training accuarcy: 0.9595\n",
      "Epoch 5 step 1454: training loss: 352.92065893081696\n",
      "Epoch 5 step 1455: training accuarcy: 0.9635\n",
      "Epoch 5 step 1455: training loss: 362.06760328642105\n",
      "Epoch 5 step 1456: training accuarcy: 0.9585\n",
      "Epoch 5 step 1456: training loss: 364.94957818809644\n",
      "Epoch 5 step 1457: training accuarcy: 0.9585\n",
      "Epoch 5 step 1457: training loss: 373.93476899939367\n",
      "Epoch 5 step 1458: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1458: training loss: 344.8152491998192\n",
      "Epoch 5 step 1459: training accuarcy: 0.9655\n",
      "Epoch 5 step 1459: training loss: 339.2026520154003\n",
      "Epoch 5 step 1460: training accuarcy: 0.9695\n",
      "Epoch 5 step 1460: training loss: 357.27123623249673\n",
      "Epoch 5 step 1461: training accuarcy: 0.9635\n",
      "Epoch 5 step 1461: training loss: 350.86821191564087\n",
      "Epoch 5 step 1462: training accuarcy: 0.9635\n",
      "Epoch 5 step 1462: training loss: 346.0553484415009\n",
      "Epoch 5 step 1463: training accuarcy: 0.968\n",
      "Epoch 5 step 1463: training loss: 364.28233047747574\n",
      "Epoch 5 step 1464: training accuarcy: 0.9595\n",
      "Epoch 5 step 1464: training loss: 349.1902274971392\n",
      "Epoch 5 step 1465: training accuarcy: 0.963\n",
      "Epoch 5 step 1465: training loss: 344.95552818493854\n",
      "Epoch 5 step 1466: training accuarcy: 0.9695\n",
      "Epoch 5 step 1466: training loss: 356.8022948868434\n",
      "Epoch 5 step 1467: training accuarcy: 0.9685\n",
      "Epoch 5 step 1467: training loss: 350.7642699287267\n",
      "Epoch 5 step 1468: training accuarcy: 0.97\n",
      "Epoch 5 step 1468: training loss: 358.64813680716736\n",
      "Epoch 5 step 1469: training accuarcy: 0.961\n",
      "Epoch 5 step 1469: training loss: 341.3574511392756\n",
      "Epoch 5 step 1470: training accuarcy: 0.9705\n",
      "Epoch 5 step 1470: training loss: 336.09941768972317\n",
      "Epoch 5 step 1471: training accuarcy: 0.967\n",
      "Epoch 5 step 1471: training loss: 324.04923193690394\n",
      "Epoch 5 step 1472: training accuarcy: 0.9685\n",
      "Epoch 5 step 1472: training loss: 357.57086938870316\n",
      "Epoch 5 step 1473: training accuarcy: 0.9625\n",
      "Epoch 5 step 1473: training loss: 334.6831816276475\n",
      "Epoch 5 step 1474: training accuarcy: 0.9685\n",
      "Epoch 5 step 1474: training loss: 339.56200030400186\n",
      "Epoch 5 step 1475: training accuarcy: 0.968\n",
      "Epoch 5 step 1475: training loss: 344.5076804043756\n",
      "Epoch 5 step 1476: training accuarcy: 0.9645\n",
      "Epoch 5 step 1476: training loss: 341.9376039661485\n",
      "Epoch 5 step 1477: training accuarcy: 0.968\n",
      "Epoch 5 step 1477: training loss: 342.7846843922527\n",
      "Epoch 5 step 1478: training accuarcy: 0.971\n",
      "Epoch 5 step 1478: training loss: 353.97350463684853\n",
      "Epoch 5 step 1479: training accuarcy: 0.9605\n",
      "Epoch 5 step 1479: training loss: 338.2625325923947\n",
      "Epoch 5 step 1480: training accuarcy: 0.9645\n",
      "Epoch 5 step 1480: training loss: 329.70009437969753\n",
      "Epoch 5 step 1481: training accuarcy: 0.9685\n",
      "Epoch 5 step 1481: training loss: 350.97892265134107\n",
      "Epoch 5 step 1482: training accuarcy: 0.9635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 step 1482: training loss: 350.86489094437417\n",
      "Epoch 5 step 1483: training accuarcy: 0.9585\n",
      "Epoch 5 step 1483: training loss: 347.93031699317316\n",
      "Epoch 5 step 1484: training accuarcy: 0.969\n",
      "Epoch 5 step 1484: training loss: 342.0816574469401\n",
      "Epoch 5 step 1485: training accuarcy: 0.9645\n",
      "Epoch 5 step 1485: training loss: 350.72908298711667\n",
      "Epoch 5 step 1486: training accuarcy: 0.9595\n",
      "Epoch 5 step 1486: training loss: 360.46153950730786\n",
      "Epoch 5 step 1487: training accuarcy: 0.9635\n",
      "Epoch 5 step 1487: training loss: 341.6690652410034\n",
      "Epoch 5 step 1488: training accuarcy: 0.964\n",
      "Epoch 5 step 1488: training loss: 324.1891203548594\n",
      "Epoch 5 step 1489: training accuarcy: 0.965\n",
      "Epoch 5 step 1489: training loss: 355.0459323375949\n",
      "Epoch 5 step 1490: training accuarcy: 0.9625\n",
      "Epoch 5 step 1490: training loss: 344.19741632898473\n",
      "Epoch 5 step 1491: training accuarcy: 0.965\n",
      "Epoch 5 step 1491: training loss: 335.5746837353208\n",
      "Epoch 5 step 1492: training accuarcy: 0.9625\n",
      "Epoch 5 step 1492: training loss: 338.1034167076275\n",
      "Epoch 5 step 1493: training accuarcy: 0.9685\n",
      "Epoch 5 step 1493: training loss: 335.0188969287787\n",
      "Epoch 5 step 1494: training accuarcy: 0.9645\n",
      "Epoch 5 step 1494: training loss: 346.4105404026589\n",
      "Epoch 5 step 1495: training accuarcy: 0.9655\n",
      "Epoch 5 step 1495: training loss: 332.464994823811\n",
      "Epoch 5 step 1496: training accuarcy: 0.968\n",
      "Epoch 5 step 1496: training loss: 346.2958350368761\n",
      "Epoch 5 step 1497: training accuarcy: 0.9605\n",
      "Epoch 5 step 1497: training loss: 333.2902438443681\n",
      "Epoch 5 step 1498: training accuarcy: 0.969\n",
      "Epoch 5 step 1498: training loss: 342.4445639849189\n",
      "Epoch 5 step 1499: training accuarcy: 0.9635\n",
      "Epoch 5 step 1499: training loss: 349.3626434533507\n",
      "Epoch 5 step 1500: training accuarcy: 0.964\n",
      "Epoch 5 step 1500: training loss: 336.15056097876726\n",
      "Epoch 5 step 1501: training accuarcy: 0.9645\n",
      "Epoch 5 step 1501: training loss: 343.86075355880587\n",
      "Epoch 5 step 1502: training accuarcy: 0.963\n",
      "Epoch 5 step 1502: training loss: 342.7205490871597\n",
      "Epoch 5 step 1503: training accuarcy: 0.9635\n",
      "Epoch 5 step 1503: training loss: 340.7326435804172\n",
      "Epoch 5 step 1504: training accuarcy: 0.9665\n",
      "Epoch 5 step 1504: training loss: 333.1061582602347\n",
      "Epoch 5 step 1505: training accuarcy: 0.9655\n",
      "Epoch 5 step 1505: training loss: 329.88571735466076\n",
      "Epoch 5 step 1506: training accuarcy: 0.9685\n",
      "Epoch 5 step 1506: training loss: 335.7823331980342\n",
      "Epoch 5 step 1507: training accuarcy: 0.9675\n",
      "Epoch 5 step 1507: training loss: 351.8194668421435\n",
      "Epoch 5 step 1508: training accuarcy: 0.965\n",
      "Epoch 5 step 1508: training loss: 348.0018739878736\n",
      "Epoch 5 step 1509: training accuarcy: 0.9625\n",
      "Epoch 5 step 1509: training loss: 355.3727269803479\n",
      "Epoch 5 step 1510: training accuarcy: 0.9605\n",
      "Epoch 5 step 1510: training loss: 335.32163987213687\n",
      "Epoch 5 step 1511: training accuarcy: 0.964\n",
      "Epoch 5 step 1511: training loss: 340.05386802073724\n",
      "Epoch 5 step 1512: training accuarcy: 0.9645\n",
      "Epoch 5 step 1512: training loss: 338.560891280071\n",
      "Epoch 5 step 1513: training accuarcy: 0.965\n",
      "Epoch 5 step 1513: training loss: 362.6283887938932\n",
      "Epoch 5 step 1514: training accuarcy: 0.9575\n",
      "Epoch 5 step 1514: training loss: 343.53027367427313\n",
      "Epoch 5 step 1515: training accuarcy: 0.966\n",
      "Epoch 5 step 1515: training loss: 329.74007239110614\n",
      "Epoch 5 step 1516: training accuarcy: 0.967\n",
      "Epoch 5 step 1516: training loss: 343.9430919658257\n",
      "Epoch 5 step 1517: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1517: training loss: 322.3671456669002\n",
      "Epoch 5 step 1518: training accuarcy: 0.971\n",
      "Epoch 5 step 1518: training loss: 341.6984182377673\n",
      "Epoch 5 step 1519: training accuarcy: 0.9615\n",
      "Epoch 5 step 1519: training loss: 331.26108093457293\n",
      "Epoch 5 step 1520: training accuarcy: 0.9655\n",
      "Epoch 5 step 1520: training loss: 350.99262080280755\n",
      "Epoch 5 step 1521: training accuarcy: 0.9625\n",
      "Epoch 5 step 1521: training loss: 336.95114224270924\n",
      "Epoch 5 step 1522: training accuarcy: 0.9655\n",
      "Epoch 5 step 1522: training loss: 350.53343372401747\n",
      "Epoch 5 step 1523: training accuarcy: 0.9645\n",
      "Epoch 5 step 1523: training loss: 330.5800437299222\n",
      "Epoch 5 step 1524: training accuarcy: 0.971\n",
      "Epoch 5 step 1524: training loss: 352.305155191768\n",
      "Epoch 5 step 1525: training accuarcy: 0.9595\n",
      "Epoch 5 step 1525: training loss: 328.62914772991826\n",
      "Epoch 5 step 1526: training accuarcy: 0.966\n",
      "Epoch 5 step 1526: training loss: 344.1184476856398\n",
      "Epoch 5 step 1527: training accuarcy: 0.964\n",
      "Epoch 5 step 1527: training loss: 354.53197745353674\n",
      "Epoch 5 step 1528: training accuarcy: 0.963\n",
      "Epoch 5 step 1528: training loss: 335.6298122138025\n",
      "Epoch 5 step 1529: training accuarcy: 0.9685\n",
      "Epoch 5 step 1529: training loss: 332.8333117666503\n",
      "Epoch 5 step 1530: training accuarcy: 0.965\n",
      "Epoch 5 step 1530: training loss: 337.43335420933244\n",
      "Epoch 5 step 1531: training accuarcy: 0.971\n",
      "Epoch 5 step 1531: training loss: 325.0814883780845\n",
      "Epoch 5 step 1532: training accuarcy: 0.9645\n",
      "Epoch 5 step 1532: training loss: 346.50785808904993\n",
      "Epoch 5 step 1533: training accuarcy: 0.9665\n",
      "Epoch 5 step 1533: training loss: 342.53403252141715\n",
      "Epoch 5 step 1534: training accuarcy: 0.9655\n",
      "Epoch 5 step 1534: training loss: 348.2689945506066\n",
      "Epoch 5 step 1535: training accuarcy: 0.964\n",
      "Epoch 5 step 1535: training loss: 334.74345573684013\n",
      "Epoch 5 step 1536: training accuarcy: 0.9655\n",
      "Epoch 5 step 1536: training loss: 328.0241075823676\n",
      "Epoch 5 step 1537: training accuarcy: 0.9655\n",
      "Epoch 5 step 1537: training loss: 337.72831625379587\n",
      "Epoch 5 step 1538: training accuarcy: 0.968\n",
      "Epoch 5 step 1538: training loss: 332.51093122247914\n",
      "Epoch 5 step 1539: training accuarcy: 0.9685\n",
      "Epoch 5 step 1539: training loss: 337.27816914084326\n",
      "Epoch 5 step 1540: training accuarcy: 0.965\n",
      "Epoch 5 step 1540: training loss: 338.32348950910455\n",
      "Epoch 5 step 1541: training accuarcy: 0.966\n",
      "Epoch 5 step 1541: training loss: 349.0463752040977\n",
      "Epoch 5 step 1542: training accuarcy: 0.9625\n",
      "Epoch 5 step 1542: training loss: 359.5964860427521\n",
      "Epoch 5 step 1543: training accuarcy: 0.9585\n",
      "Epoch 5 step 1543: training loss: 349.0503275363311\n",
      "Epoch 5 step 1544: training accuarcy: 0.9635\n",
      "Epoch 5 step 1544: training loss: 341.37754142712276\n",
      "Epoch 5 step 1545: training accuarcy: 0.9635\n",
      "Epoch 5 step 1545: training loss: 309.3950904778985\n",
      "Epoch 5 step 1546: training accuarcy: 0.9695\n",
      "Epoch 5 step 1546: training loss: 329.30930155212536\n",
      "Epoch 5 step 1547: training accuarcy: 0.9655\n",
      "Epoch 5 step 1547: training loss: 325.33541023890893\n",
      "Epoch 5 step 1548: training accuarcy: 0.971\n",
      "Epoch 5 step 1548: training loss: 344.0563164962989\n",
      "Epoch 5 step 1549: training accuarcy: 0.9570000000000001\n",
      "Epoch 5 step 1549: training loss: 332.2532272002021\n",
      "Epoch 5 step 1550: training accuarcy: 0.9605\n",
      "Epoch 5 step 1550: training loss: 338.233560893496\n",
      "Epoch 5 step 1551: training accuarcy: 0.9655\n",
      "Epoch 5 step 1551: training loss: 327.5752345031427\n",
      "Epoch 5 step 1552: training accuarcy: 0.9685\n",
      "Epoch 5 step 1552: training loss: 333.3265750704217\n",
      "Epoch 5 step 1553: training accuarcy: 0.97\n",
      "Epoch 5 step 1553: training loss: 318.0734818788854\n",
      "Epoch 5 step 1554: training accuarcy: 0.9685\n",
      "Epoch 5 step 1554: training loss: 353.13880498379143\n",
      "Epoch 5 step 1555: training accuarcy: 0.96\n",
      "Epoch 5 step 1555: training loss: 327.01647990660365\n",
      "Epoch 5 step 1556: training accuarcy: 0.9695\n",
      "Epoch 5 step 1556: training loss: 346.44596437130934\n",
      "Epoch 5 step 1557: training accuarcy: 0.9590000000000001\n",
      "Epoch 5 step 1557: training loss: 321.1964847613762\n",
      "Epoch 5 step 1558: training accuarcy: 0.9665\n",
      "Epoch 5 step 1558: training loss: 336.53292805264346\n",
      "Epoch 5 step 1559: training accuarcy: 0.9675\n",
      "Epoch 5 step 1559: training loss: 329.35187399134145\n",
      "Epoch 5 step 1560: training accuarcy: 0.9645\n",
      "Epoch 5 step 1560: training loss: 330.88553322705815\n",
      "Epoch 5 step 1561: training accuarcy: 0.9725\n",
      "Epoch 5 step 1561: training loss: 327.8326955934555\n",
      "Epoch 5 step 1562: training accuarcy: 0.971\n",
      "Epoch 5 step 1562: training loss: 330.4411393714402\n",
      "Epoch 5 step 1563: training accuarcy: 0.9665\n",
      "Epoch 5 step 1563: training loss: 309.48608623280785\n",
      "Epoch 5 step 1564: training accuarcy: 0.975\n",
      "Epoch 5 step 1564: training loss: 326.4398962341057\n",
      "Epoch 5 step 1565: training accuarcy: 0.9705\n",
      "Epoch 5 step 1565: training loss: 325.726141946895\n",
      "Epoch 5 step 1566: training accuarcy: 0.9685\n",
      "Epoch 5 step 1566: training loss: 319.46091384713014\n",
      "Epoch 5 step 1567: training accuarcy: 0.9705\n",
      "Epoch 5 step 1567: training loss: 336.4508112412377\n",
      "Epoch 5 step 1568: training accuarcy: 0.9665\n",
      "Epoch 5 step 1568: training loss: 323.92785401255634\n",
      "Epoch 5 step 1569: training accuarcy: 0.9715\n",
      "Epoch 5 step 1569: training loss: 339.8402999860718\n",
      "Epoch 5 step 1570: training accuarcy: 0.963\n",
      "Epoch 5 step 1570: training loss: 341.8542358580971\n",
      "Epoch 5 step 1571: training accuarcy: 0.961\n",
      "Epoch 5 step 1571: training loss: 339.8648487810889\n",
      "Epoch 5 step 1572: training accuarcy: 0.9625\n",
      "Epoch 5 step 1572: training loss: 331.38994079883923\n",
      "Epoch 5 step 1573: training accuarcy: 0.969\n",
      "Epoch 5 step 1573: training loss: 320.7305441607089\n",
      "Epoch 5 step 1574: training accuarcy: 0.969\n",
      "Epoch 5 step 1574: training loss: 324.4055315365067\n",
      "Epoch 5 step 1575: training accuarcy: 0.969\n",
      "Epoch 5 step 1575: training loss: 311.67969606415227\n",
      "Epoch 5 step 1576: training accuarcy: 0.9695\n",
      "Epoch 5 step 1576: training loss: 324.06062842315373\n",
      "Epoch 5 step 1577: training accuarcy: 0.966\n",
      "Epoch 5 step 1577: training loss: 140.02689109158143\n",
      "Epoch 5 step 1578: training accuarcy: 0.9692307692307692\n",
      "Epoch 5: train loss 352.3288985248026, train accuarcy 0.9495946168899536\n",
      "Epoch 5: valid loss 802.4176493329032, valid accuarcy 0.8263593912124634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                      | 6/8 [13:22<04:27, 133.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Epoch 6 step 1578: training loss: 270.0170837335595\n",
      "Epoch 6 step 1579: training accuarcy: 0.9805\n",
      "Epoch 6 step 1579: training loss: 281.39022914885066\n",
      "Epoch 6 step 1580: training accuarcy: 0.9825\n",
      "Epoch 6 step 1580: training loss: 287.0902149166919\n",
      "Epoch 6 step 1581: training accuarcy: 0.9765\n",
      "Epoch 6 step 1581: training loss: 276.83760292112345\n",
      "Epoch 6 step 1582: training accuarcy: 0.979\n",
      "Epoch 6 step 1582: training loss: 282.16422260623045\n",
      "Epoch 6 step 1583: training accuarcy: 0.977\n",
      "Epoch 6 step 1583: training loss: 294.42673460693646\n",
      "Epoch 6 step 1584: training accuarcy: 0.976\n",
      "Epoch 6 step 1584: training loss: 268.28140452610694\n",
      "Epoch 6 step 1585: training accuarcy: 0.984\n",
      "Epoch 6 step 1585: training loss: 280.8093418639373\n",
      "Epoch 6 step 1586: training accuarcy: 0.9765\n",
      "Epoch 6 step 1586: training loss: 280.86573416291475\n",
      "Epoch 6 step 1587: training accuarcy: 0.9765\n",
      "Epoch 6 step 1587: training loss: 280.5462040540863\n",
      "Epoch 6 step 1588: training accuarcy: 0.9765\n",
      "Epoch 6 step 1588: training loss: 285.58117621949214\n",
      "Epoch 6 step 1589: training accuarcy: 0.976\n",
      "Epoch 6 step 1589: training loss: 272.7977748299742\n",
      "Epoch 6 step 1590: training accuarcy: 0.9805\n",
      "Epoch 6 step 1590: training loss: 278.9236678338517\n",
      "Epoch 6 step 1591: training accuarcy: 0.9835\n",
      "Epoch 6 step 1591: training loss: 281.63060565653115\n",
      "Epoch 6 step 1592: training accuarcy: 0.974\n",
      "Epoch 6 step 1592: training loss: 280.938081211283\n",
      "Epoch 6 step 1593: training accuarcy: 0.977\n",
      "Epoch 6 step 1593: training loss: 276.081406316355\n",
      "Epoch 6 step 1594: training accuarcy: 0.9785\n",
      "Epoch 6 step 1594: training loss: 280.160700200939\n",
      "Epoch 6 step 1595: training accuarcy: 0.974\n",
      "Epoch 6 step 1595: training loss: 286.25082377251096\n",
      "Epoch 6 step 1596: training accuarcy: 0.9795\n",
      "Epoch 6 step 1596: training loss: 278.172627516598\n",
      "Epoch 6 step 1597: training accuarcy: 0.9765\n",
      "Epoch 6 step 1597: training loss: 270.34142155204603\n",
      "Epoch 6 step 1598: training accuarcy: 0.981\n",
      "Epoch 6 step 1598: training loss: 268.6250639141191\n",
      "Epoch 6 step 1599: training accuarcy: 0.98\n",
      "Epoch 6 step 1599: training loss: 271.5184000115381\n",
      "Epoch 6 step 1600: training accuarcy: 0.979\n",
      "Epoch 6 step 1600: training loss: 278.26316874313073\n",
      "Epoch 6 step 1601: training accuarcy: 0.976\n",
      "Epoch 6 step 1601: training loss: 272.1294241754911\n",
      "Epoch 6 step 1602: training accuarcy: 0.983\n",
      "Epoch 6 step 1602: training loss: 264.8159012468623\n",
      "Epoch 6 step 1603: training accuarcy: 0.984\n",
      "Epoch 6 step 1603: training loss: 266.9353363377707\n",
      "Epoch 6 step 1604: training accuarcy: 0.9775\n",
      "Epoch 6 step 1604: training loss: 281.7272305184764\n",
      "Epoch 6 step 1605: training accuarcy: 0.98\n",
      "Epoch 6 step 1605: training loss: 273.9934651110955\n",
      "Epoch 6 step 1606: training accuarcy: 0.9775\n",
      "Epoch 6 step 1606: training loss: 267.32707268208463\n",
      "Epoch 6 step 1607: training accuarcy: 0.981\n",
      "Epoch 6 step 1607: training loss: 271.48881802503956\n",
      "Epoch 6 step 1608: training accuarcy: 0.977\n",
      "Epoch 6 step 1608: training loss: 276.995267815898\n",
      "Epoch 6 step 1609: training accuarcy: 0.977\n",
      "Epoch 6 step 1609: training loss: 278.45708787611403\n",
      "Epoch 6 step 1610: training accuarcy: 0.978\n",
      "Epoch 6 step 1610: training loss: 276.74235749435167\n",
      "Epoch 6 step 1611: training accuarcy: 0.9745\n",
      "Epoch 6 step 1611: training loss: 283.9929408560065\n",
      "Epoch 6 step 1612: training accuarcy: 0.9745\n",
      "Epoch 6 step 1612: training loss: 256.25900264692007\n",
      "Epoch 6 step 1613: training accuarcy: 0.9815\n",
      "Epoch 6 step 1613: training loss: 280.75294194195027\n",
      "Epoch 6 step 1614: training accuarcy: 0.9795\n",
      "Epoch 6 step 1614: training loss: 258.2171078895964\n",
      "Epoch 6 step 1615: training accuarcy: 0.983\n",
      "Epoch 6 step 1615: training loss: 259.6775768147659\n",
      "Epoch 6 step 1616: training accuarcy: 0.986\n",
      "Epoch 6 step 1616: training loss: 273.8932175755521\n",
      "Epoch 6 step 1617: training accuarcy: 0.9775\n",
      "Epoch 6 step 1617: training loss: 263.3175353296605\n",
      "Epoch 6 step 1618: training accuarcy: 0.981\n",
      "Epoch 6 step 1618: training loss: 274.1495953280165\n",
      "Epoch 6 step 1619: training accuarcy: 0.9805\n",
      "Epoch 6 step 1619: training loss: 276.1791956182646\n",
      "Epoch 6 step 1620: training accuarcy: 0.9765\n",
      "Epoch 6 step 1620: training loss: 271.2131629556698\n",
      "Epoch 6 step 1621: training accuarcy: 0.9765\n",
      "Epoch 6 step 1621: training loss: 285.3706027248065\n",
      "Epoch 6 step 1622: training accuarcy: 0.974\n",
      "Epoch 6 step 1622: training loss: 255.44989844867953\n",
      "Epoch 6 step 1623: training accuarcy: 0.9805\n",
      "Epoch 6 step 1623: training loss: 263.8870007709947\n",
      "Epoch 6 step 1624: training accuarcy: 0.982\n",
      "Epoch 6 step 1624: training loss: 291.00834384074847\n",
      "Epoch 6 step 1625: training accuarcy: 0.9695\n",
      "Epoch 6 step 1625: training loss: 262.89219324377416\n",
      "Epoch 6 step 1626: training accuarcy: 0.9815\n",
      "Epoch 6 step 1626: training loss: 268.46857077812706\n",
      "Epoch 6 step 1627: training accuarcy: 0.9815\n",
      "Epoch 6 step 1627: training loss: 273.89211195650813\n",
      "Epoch 6 step 1628: training accuarcy: 0.979\n",
      "Epoch 6 step 1628: training loss: 273.69120054914515\n",
      "Epoch 6 step 1629: training accuarcy: 0.98\n",
      "Epoch 6 step 1629: training loss: 279.17479742511904\n",
      "Epoch 6 step 1630: training accuarcy: 0.977\n",
      "Epoch 6 step 1630: training loss: 271.3598788190899\n",
      "Epoch 6 step 1631: training accuarcy: 0.981\n",
      "Epoch 6 step 1631: training loss: 267.33602078991305\n",
      "Epoch 6 step 1632: training accuarcy: 0.979\n",
      "Epoch 6 step 1632: training loss: 267.5690667862284\n",
      "Epoch 6 step 1633: training accuarcy: 0.98\n",
      "Epoch 6 step 1633: training loss: 270.0686805463954\n",
      "Epoch 6 step 1634: training accuarcy: 0.978\n",
      "Epoch 6 step 1634: training loss: 276.1315232823255\n",
      "Epoch 6 step 1635: training accuarcy: 0.979\n",
      "Epoch 6 step 1635: training loss: 283.1505619402868\n",
      "Epoch 6 step 1636: training accuarcy: 0.972\n",
      "Epoch 6 step 1636: training loss: 286.6300186849585\n",
      "Epoch 6 step 1637: training accuarcy: 0.974\n",
      "Epoch 6 step 1637: training loss: 269.96489131216714\n",
      "Epoch 6 step 1638: training accuarcy: 0.9775\n",
      "Epoch 6 step 1638: training loss: 266.0297890019558\n",
      "Epoch 6 step 1639: training accuarcy: 0.9805\n",
      "Epoch 6 step 1639: training loss: 277.5604597419505\n",
      "Epoch 6 step 1640: training accuarcy: 0.9715\n",
      "Epoch 6 step 1640: training loss: 296.30689998635546\n",
      "Epoch 6 step 1641: training accuarcy: 0.977\n",
      "Epoch 6 step 1641: training loss: 273.2654402249805\n",
      "Epoch 6 step 1642: training accuarcy: 0.9775\n",
      "Epoch 6 step 1642: training loss: 268.8891634720089\n",
      "Epoch 6 step 1643: training accuarcy: 0.977\n",
      "Epoch 6 step 1643: training loss: 263.5368056481297\n",
      "Epoch 6 step 1644: training accuarcy: 0.984\n",
      "Epoch 6 step 1644: training loss: 282.02631637096886\n",
      "Epoch 6 step 1645: training accuarcy: 0.9745\n",
      "Epoch 6 step 1645: training loss: 262.1185434065985\n",
      "Epoch 6 step 1646: training accuarcy: 0.9785\n",
      "Epoch 6 step 1646: training loss: 263.4193749415298\n",
      "Epoch 6 step 1647: training accuarcy: 0.981\n",
      "Epoch 6 step 1647: training loss: 259.12879855415247\n",
      "Epoch 6 step 1648: training accuarcy: 0.9805\n",
      "Epoch 6 step 1648: training loss: 276.1462945636002\n",
      "Epoch 6 step 1649: training accuarcy: 0.979\n",
      "Epoch 6 step 1649: training loss: 277.74620032257246\n",
      "Epoch 6 step 1650: training accuarcy: 0.977\n",
      "Epoch 6 step 1650: training loss: 276.2624753843141\n",
      "Epoch 6 step 1651: training accuarcy: 0.981\n",
      "Epoch 6 step 1651: training loss: 259.6986749349745\n",
      "Epoch 6 step 1652: training accuarcy: 0.9785\n",
      "Epoch 6 step 1652: training loss: 274.5135957873615\n",
      "Epoch 6 step 1653: training accuarcy: 0.9795\n",
      "Epoch 6 step 1653: training loss: 262.11393875107075\n",
      "Epoch 6 step 1654: training accuarcy: 0.9815\n",
      "Epoch 6 step 1654: training loss: 264.95611272392455\n",
      "Epoch 6 step 1655: training accuarcy: 0.9775\n",
      "Epoch 6 step 1655: training loss: 283.2503979631502\n",
      "Epoch 6 step 1656: training accuarcy: 0.976\n",
      "Epoch 6 step 1656: training loss: 274.7658798832755\n",
      "Epoch 6 step 1657: training accuarcy: 0.9735\n",
      "Epoch 6 step 1657: training loss: 255.633827454966\n",
      "Epoch 6 step 1658: training accuarcy: 0.985\n",
      "Epoch 6 step 1658: training loss: 274.24146135335906\n",
      "Epoch 6 step 1659: training accuarcy: 0.9785\n",
      "Epoch 6 step 1659: training loss: 258.7500039195834\n",
      "Epoch 6 step 1660: training accuarcy: 0.9795\n",
      "Epoch 6 step 1660: training loss: 257.94103732665144\n",
      "Epoch 6 step 1661: training accuarcy: 0.9835\n",
      "Epoch 6 step 1661: training loss: 275.3171947919614\n",
      "Epoch 6 step 1662: training accuarcy: 0.979\n",
      "Epoch 6 step 1662: training loss: 255.36261909054792\n",
      "Epoch 6 step 1663: training accuarcy: 0.982\n",
      "Epoch 6 step 1663: training loss: 272.74372363398527\n",
      "Epoch 6 step 1664: training accuarcy: 0.974\n",
      "Epoch 6 step 1664: training loss: 260.3027794584119\n",
      "Epoch 6 step 1665: training accuarcy: 0.982\n",
      "Epoch 6 step 1665: training loss: 260.45961682298406\n",
      "Epoch 6 step 1666: training accuarcy: 0.9825\n",
      "Epoch 6 step 1666: training loss: 264.2485271111472\n",
      "Epoch 6 step 1667: training accuarcy: 0.976\n",
      "Epoch 6 step 1667: training loss: 250.7108814927727\n",
      "Epoch 6 step 1668: training accuarcy: 0.985\n",
      "Epoch 6 step 1668: training loss: 277.4262102871315\n",
      "Epoch 6 step 1669: training accuarcy: 0.9725\n",
      "Epoch 6 step 1669: training loss: 277.7818438752277\n",
      "Epoch 6 step 1670: training accuarcy: 0.977\n",
      "Epoch 6 step 1670: training loss: 247.6617572399866\n",
      "Epoch 6 step 1671: training accuarcy: 0.9835\n",
      "Epoch 6 step 1671: training loss: 263.3573428730975\n",
      "Epoch 6 step 1672: training accuarcy: 0.9785\n",
      "Epoch 6 step 1672: training loss: 290.92088549129676\n",
      "Epoch 6 step 1673: training accuarcy: 0.972\n",
      "Epoch 6 step 1673: training loss: 263.8942009695222\n",
      "Epoch 6 step 1674: training accuarcy: 0.981\n",
      "Epoch 6 step 1674: training loss: 260.26832539324676\n",
      "Epoch 6 step 1675: training accuarcy: 0.9785\n",
      "Epoch 6 step 1675: training loss: 274.1068358144867\n",
      "Epoch 6 step 1676: training accuarcy: 0.975\n",
      "Epoch 6 step 1676: training loss: 273.40048832839597\n",
      "Epoch 6 step 1677: training accuarcy: 0.9805\n",
      "Epoch 6 step 1677: training loss: 261.22257199836184\n",
      "Epoch 6 step 1678: training accuarcy: 0.983\n",
      "Epoch 6 step 1678: training loss: 264.8551496309749\n",
      "Epoch 6 step 1679: training accuarcy: 0.9825\n",
      "Epoch 6 step 1679: training loss: 260.88947797648035\n",
      "Epoch 6 step 1680: training accuarcy: 0.981\n",
      "Epoch 6 step 1680: training loss: 272.7302599766638\n",
      "Epoch 6 step 1681: training accuarcy: 0.9775\n",
      "Epoch 6 step 1681: training loss: 269.37535821800003\n",
      "Epoch 6 step 1682: training accuarcy: 0.976\n",
      "Epoch 6 step 1682: training loss: 267.5768951528557\n",
      "Epoch 6 step 1683: training accuarcy: 0.9805\n",
      "Epoch 6 step 1683: training loss: 261.033311630623\n",
      "Epoch 6 step 1684: training accuarcy: 0.977\n",
      "Epoch 6 step 1684: training loss: 280.0650897941534\n",
      "Epoch 6 step 1685: training accuarcy: 0.9755\n",
      "Epoch 6 step 1685: training loss: 253.430523252086\n",
      "Epoch 6 step 1686: training accuarcy: 0.981\n",
      "Epoch 6 step 1686: training loss: 264.0397321383741\n",
      "Epoch 6 step 1687: training accuarcy: 0.982\n",
      "Epoch 6 step 1687: training loss: 267.04877498004294\n",
      "Epoch 6 step 1688: training accuarcy: 0.976\n",
      "Epoch 6 step 1688: training loss: 253.46289256459806\n",
      "Epoch 6 step 1689: training accuarcy: 0.9835\n",
      "Epoch 6 step 1689: training loss: 262.70769869049934\n",
      "Epoch 6 step 1690: training accuarcy: 0.9795\n",
      "Epoch 6 step 1690: training loss: 282.1544558237107\n",
      "Epoch 6 step 1691: training accuarcy: 0.974\n",
      "Epoch 6 step 1691: training loss: 248.11256470757337\n",
      "Epoch 6 step 1692: training accuarcy: 0.983\n",
      "Epoch 6 step 1692: training loss: 260.00558014111436\n",
      "Epoch 6 step 1693: training accuarcy: 0.9815\n",
      "Epoch 6 step 1693: training loss: 278.8272784282808\n",
      "Epoch 6 step 1694: training accuarcy: 0.9765\n",
      "Epoch 6 step 1694: training loss: 258.723044936743\n",
      "Epoch 6 step 1695: training accuarcy: 0.9825\n",
      "Epoch 6 step 1695: training loss: 258.7900460749452\n",
      "Epoch 6 step 1696: training accuarcy: 0.9835\n",
      "Epoch 6 step 1696: training loss: 267.5639968107928\n",
      "Epoch 6 step 1697: training accuarcy: 0.978\n",
      "Epoch 6 step 1697: training loss: 267.98892516446745\n",
      "Epoch 6 step 1698: training accuarcy: 0.981\n",
      "Epoch 6 step 1698: training loss: 265.2252341291449\n",
      "Epoch 6 step 1699: training accuarcy: 0.982\n",
      "Epoch 6 step 1699: training loss: 254.4797245190806\n",
      "Epoch 6 step 1700: training accuarcy: 0.9795\n",
      "Epoch 6 step 1700: training loss: 272.9674928224901\n",
      "Epoch 6 step 1701: training accuarcy: 0.9765\n",
      "Epoch 6 step 1701: training loss: 267.5638114163887\n",
      "Epoch 6 step 1702: training accuarcy: 0.974\n",
      "Epoch 6 step 1702: training loss: 257.9938498687566\n",
      "Epoch 6 step 1703: training accuarcy: 0.9795\n",
      "Epoch 6 step 1703: training loss: 270.5840436889749\n",
      "Epoch 6 step 1704: training accuarcy: 0.9755\n",
      "Epoch 6 step 1704: training loss: 263.56928697058333\n",
      "Epoch 6 step 1705: training accuarcy: 0.976\n",
      "Epoch 6 step 1705: training loss: 270.67272749691386\n",
      "Epoch 6 step 1706: training accuarcy: 0.9775\n",
      "Epoch 6 step 1706: training loss: 266.79376417105846\n",
      "Epoch 6 step 1707: training accuarcy: 0.977\n",
      "Epoch 6 step 1707: training loss: 266.449997459825\n",
      "Epoch 6 step 1708: training accuarcy: 0.9745\n",
      "Epoch 6 step 1708: training loss: 262.48309674944153\n",
      "Epoch 6 step 1709: training accuarcy: 0.9785\n",
      "Epoch 6 step 1709: training loss: 254.40875097226095\n",
      "Epoch 6 step 1710: training accuarcy: 0.9835\n",
      "Epoch 6 step 1710: training loss: 259.6766761125917\n",
      "Epoch 6 step 1711: training accuarcy: 0.98\n",
      "Epoch 6 step 1711: training loss: 277.6837879867001\n",
      "Epoch 6 step 1712: training accuarcy: 0.9775\n",
      "Epoch 6 step 1712: training loss: 267.60614324742664\n",
      "Epoch 6 step 1713: training accuarcy: 0.9775\n",
      "Epoch 6 step 1713: training loss: 269.65131635386683\n",
      "Epoch 6 step 1714: training accuarcy: 0.979\n",
      "Epoch 6 step 1714: training loss: 283.71627128291203\n",
      "Epoch 6 step 1715: training accuarcy: 0.97\n",
      "Epoch 6 step 1715: training loss: 262.46827721002416\n",
      "Epoch 6 step 1716: training accuarcy: 0.9775\n",
      "Epoch 6 step 1716: training loss: 261.41895614905206\n",
      "Epoch 6 step 1717: training accuarcy: 0.9845\n",
      "Epoch 6 step 1717: training loss: 263.50584353876474\n",
      "Epoch 6 step 1718: training accuarcy: 0.979\n",
      "Epoch 6 step 1718: training loss: 270.33753434990507\n",
      "Epoch 6 step 1719: training accuarcy: 0.979\n",
      "Epoch 6 step 1719: training loss: 261.5131764280716\n",
      "Epoch 6 step 1720: training accuarcy: 0.9785\n",
      "Epoch 6 step 1720: training loss: 259.08333473442775\n",
      "Epoch 6 step 1721: training accuarcy: 0.981\n",
      "Epoch 6 step 1721: training loss: 257.8313532986698\n",
      "Epoch 6 step 1722: training accuarcy: 0.979\n",
      "Epoch 6 step 1722: training loss: 256.53667030129657\n",
      "Epoch 6 step 1723: training accuarcy: 0.984\n",
      "Epoch 6 step 1723: training loss: 267.8412529214317\n",
      "Epoch 6 step 1724: training accuarcy: 0.9765\n",
      "Epoch 6 step 1724: training loss: 264.23730245031396\n",
      "Epoch 6 step 1725: training accuarcy: 0.9725\n",
      "Epoch 6 step 1725: training loss: 256.4297984488082\n",
      "Epoch 6 step 1726: training accuarcy: 0.981\n",
      "Epoch 6 step 1726: training loss: 277.77485795732775\n",
      "Epoch 6 step 1727: training accuarcy: 0.9775\n",
      "Epoch 6 step 1727: training loss: 266.36563506879077\n",
      "Epoch 6 step 1728: training accuarcy: 0.977\n",
      "Epoch 6 step 1728: training loss: 266.14001929856664\n",
      "Epoch 6 step 1729: training accuarcy: 0.977\n",
      "Epoch 6 step 1729: training loss: 270.2616248868637\n",
      "Epoch 6 step 1730: training accuarcy: 0.9775\n",
      "Epoch 6 step 1730: training loss: 274.7949002442893\n",
      "Epoch 6 step 1731: training accuarcy: 0.9765\n",
      "Epoch 6 step 1731: training loss: 263.1661097774419\n",
      "Epoch 6 step 1732: training accuarcy: 0.9835\n",
      "Epoch 6 step 1732: training loss: 256.40085727717667\n",
      "Epoch 6 step 1733: training accuarcy: 0.983\n",
      "Epoch 6 step 1733: training loss: 244.02128464220442\n",
      "Epoch 6 step 1734: training accuarcy: 0.9835\n",
      "Epoch 6 step 1734: training loss: 273.50001981095727\n",
      "Epoch 6 step 1735: training accuarcy: 0.976\n",
      "Epoch 6 step 1735: training loss: 263.3828392106512\n",
      "Epoch 6 step 1736: training accuarcy: 0.9785\n",
      "Epoch 6 step 1736: training loss: 259.23715724651123\n",
      "Epoch 6 step 1737: training accuarcy: 0.975\n",
      "Epoch 6 step 1737: training loss: 243.57481883021515\n",
      "Epoch 6 step 1738: training accuarcy: 0.9795\n",
      "Epoch 6 step 1738: training loss: 252.65132228346465\n",
      "Epoch 6 step 1739: training accuarcy: 0.984\n",
      "Epoch 6 step 1739: training loss: 255.01007347903067\n",
      "Epoch 6 step 1740: training accuarcy: 0.979\n",
      "Epoch 6 step 1740: training loss: 250.25535417451542\n",
      "Epoch 6 step 1741: training accuarcy: 0.985\n",
      "Epoch 6 step 1741: training loss: 251.8631334836184\n",
      "Epoch 6 step 1742: training accuarcy: 0.983\n",
      "Epoch 6 step 1742: training loss: 270.3495310734739\n",
      "Epoch 6 step 1743: training accuarcy: 0.9775\n",
      "Epoch 6 step 1743: training loss: 260.07093732603585\n",
      "Epoch 6 step 1744: training accuarcy: 0.975\n",
      "Epoch 6 step 1744: training loss: 260.40606978259046\n",
      "Epoch 6 step 1745: training accuarcy: 0.9795\n",
      "Epoch 6 step 1745: training loss: 259.51936510967727\n",
      "Epoch 6 step 1746: training accuarcy: 0.976\n",
      "Epoch 6 step 1746: training loss: 260.32376106762484\n",
      "Epoch 6 step 1747: training accuarcy: 0.9835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 step 1747: training loss: 239.25604234662714\n",
      "Epoch 6 step 1748: training accuarcy: 0.984\n",
      "Epoch 6 step 1748: training loss: 250.5090920832568\n",
      "Epoch 6 step 1749: training accuarcy: 0.9795\n",
      "Epoch 6 step 1749: training loss: 270.89029412113\n",
      "Epoch 6 step 1750: training accuarcy: 0.978\n",
      "Epoch 6 step 1750: training loss: 252.03609499469488\n",
      "Epoch 6 step 1751: training accuarcy: 0.983\n",
      "Epoch 6 step 1751: training loss: 260.92424193545634\n",
      "Epoch 6 step 1752: training accuarcy: 0.978\n",
      "Epoch 6 step 1752: training loss: 250.57131871772398\n",
      "Epoch 6 step 1753: training accuarcy: 0.979\n",
      "Epoch 6 step 1753: training loss: 260.8822594586854\n",
      "Epoch 6 step 1754: training accuarcy: 0.981\n",
      "Epoch 6 step 1754: training loss: 277.7113655219941\n",
      "Epoch 6 step 1755: training accuarcy: 0.97\n",
      "Epoch 6 step 1755: training loss: 244.93968000144952\n",
      "Epoch 6 step 1756: training accuarcy: 0.985\n",
      "Epoch 6 step 1756: training loss: 244.50587167179933\n",
      "Epoch 6 step 1757: training accuarcy: 0.9825\n",
      "Epoch 6 step 1757: training loss: 275.94315535762985\n",
      "Epoch 6 step 1758: training accuarcy: 0.973\n",
      "Epoch 6 step 1758: training loss: 241.4897174038524\n",
      "Epoch 6 step 1759: training accuarcy: 0.9845\n",
      "Epoch 6 step 1759: training loss: 256.035754816181\n",
      "Epoch 6 step 1760: training accuarcy: 0.9795\n",
      "Epoch 6 step 1760: training loss: 257.71811619637015\n",
      "Epoch 6 step 1761: training accuarcy: 0.9795\n",
      "Epoch 6 step 1761: training loss: 262.7257399169824\n",
      "Epoch 6 step 1762: training accuarcy: 0.979\n",
      "Epoch 6 step 1762: training loss: 268.3139347165613\n",
      "Epoch 6 step 1763: training accuarcy: 0.972\n",
      "Epoch 6 step 1763: training loss: 257.28933729297785\n",
      "Epoch 6 step 1764: training accuarcy: 0.9785\n",
      "Epoch 6 step 1764: training loss: 278.81043417448296\n",
      "Epoch 6 step 1765: training accuarcy: 0.9725\n",
      "Epoch 6 step 1765: training loss: 249.2661061042268\n",
      "Epoch 6 step 1766: training accuarcy: 0.9805\n",
      "Epoch 6 step 1766: training loss: 254.4940428517244\n",
      "Epoch 6 step 1767: training accuarcy: 0.979\n",
      "Epoch 6 step 1767: training loss: 247.2847766267866\n",
      "Epoch 6 step 1768: training accuarcy: 0.9825\n",
      "Epoch 6 step 1768: training loss: 249.0398636475485\n",
      "Epoch 6 step 1769: training accuarcy: 0.974\n",
      "Epoch 6 step 1769: training loss: 247.62503253982274\n",
      "Epoch 6 step 1770: training accuarcy: 0.981\n",
      "Epoch 6 step 1770: training loss: 259.30532953480395\n",
      "Epoch 6 step 1771: training accuarcy: 0.979\n",
      "Epoch 6 step 1771: training loss: 263.3945955329253\n",
      "Epoch 6 step 1772: training accuarcy: 0.9745\n",
      "Epoch 6 step 1772: training loss: 253.93944075874427\n",
      "Epoch 6 step 1773: training accuarcy: 0.977\n",
      "Epoch 6 step 1773: training loss: 257.6256590548361\n",
      "Epoch 6 step 1774: training accuarcy: 0.9795\n",
      "Epoch 6 step 1774: training loss: 246.07140032177628\n",
      "Epoch 6 step 1775: training accuarcy: 0.9815\n",
      "Epoch 6 step 1775: training loss: 252.24862196208565\n",
      "Epoch 6 step 1776: training accuarcy: 0.979\n",
      "Epoch 6 step 1776: training loss: 251.16566958192956\n",
      "Epoch 6 step 1777: training accuarcy: 0.98\n",
      "Epoch 6 step 1777: training loss: 257.8733214970596\n",
      "Epoch 6 step 1778: training accuarcy: 0.9785\n",
      "Epoch 6 step 1778: training loss: 244.8770815054065\n",
      "Epoch 6 step 1779: training accuarcy: 0.982\n",
      "Epoch 6 step 1779: training loss: 268.6301245721637\n",
      "Epoch 6 step 1780: training accuarcy: 0.9785\n",
      "Epoch 6 step 1780: training loss: 249.06234070962668\n",
      "Epoch 6 step 1781: training accuarcy: 0.98\n",
      "Epoch 6 step 1781: training loss: 251.64418287603237\n",
      "Epoch 6 step 1782: training accuarcy: 0.9805\n",
      "Epoch 6 step 1782: training loss: 239.35787069012997\n",
      "Epoch 6 step 1783: training accuarcy: 0.984\n",
      "Epoch 6 step 1783: training loss: 261.14754244698145\n",
      "Epoch 6 step 1784: training accuarcy: 0.981\n",
      "Epoch 6 step 1784: training loss: 232.35752799502748\n",
      "Epoch 6 step 1785: training accuarcy: 0.9865\n",
      "Epoch 6 step 1785: training loss: 246.66336676395443\n",
      "Epoch 6 step 1786: training accuarcy: 0.9795\n",
      "Epoch 6 step 1786: training loss: 245.01233835187574\n",
      "Epoch 6 step 1787: training accuarcy: 0.9815\n",
      "Epoch 6 step 1787: training loss: 254.03923881851526\n",
      "Epoch 6 step 1788: training accuarcy: 0.9735\n",
      "Epoch 6 step 1788: training loss: 263.1899697604548\n",
      "Epoch 6 step 1789: training accuarcy: 0.977\n",
      "Epoch 6 step 1789: training loss: 252.62166756519872\n",
      "Epoch 6 step 1790: training accuarcy: 0.984\n",
      "Epoch 6 step 1790: training loss: 257.2471443455303\n",
      "Epoch 6 step 1791: training accuarcy: 0.979\n",
      "Epoch 6 step 1791: training loss: 258.45453480353706\n",
      "Epoch 6 step 1792: training accuarcy: 0.9745\n",
      "Epoch 6 step 1792: training loss: 248.08072209227913\n",
      "Epoch 6 step 1793: training accuarcy: 0.9815\n",
      "Epoch 6 step 1793: training loss: 252.33819238968738\n",
      "Epoch 6 step 1794: training accuarcy: 0.977\n",
      "Epoch 6 step 1794: training loss: 251.4093181149011\n",
      "Epoch 6 step 1795: training accuarcy: 0.9815\n",
      "Epoch 6 step 1795: training loss: 244.8244139970017\n",
      "Epoch 6 step 1796: training accuarcy: 0.98\n",
      "Epoch 6 step 1796: training loss: 238.64620648992477\n",
      "Epoch 6 step 1797: training accuarcy: 0.983\n",
      "Epoch 6 step 1797: training loss: 269.8079739804908\n",
      "Epoch 6 step 1798: training accuarcy: 0.9735\n",
      "Epoch 6 step 1798: training loss: 254.19337154366997\n",
      "Epoch 6 step 1799: training accuarcy: 0.9805\n",
      "Epoch 6 step 1799: training loss: 237.73741590248446\n",
      "Epoch 6 step 1800: training accuarcy: 0.984\n",
      "Epoch 6 step 1800: training loss: 245.3562263867922\n",
      "Epoch 6 step 1801: training accuarcy: 0.9805\n",
      "Epoch 6 step 1801: training loss: 256.47953881585954\n",
      "Epoch 6 step 1802: training accuarcy: 0.977\n",
      "Epoch 6 step 1802: training loss: 237.28103721011405\n",
      "Epoch 6 step 1803: training accuarcy: 0.981\n",
      "Epoch 6 step 1803: training loss: 259.5292243578821\n",
      "Epoch 6 step 1804: training accuarcy: 0.977\n",
      "Epoch 6 step 1804: training loss: 270.9612594496008\n",
      "Epoch 6 step 1805: training accuarcy: 0.975\n",
      "Epoch 6 step 1805: training loss: 242.45224056329275\n",
      "Epoch 6 step 1806: training accuarcy: 0.9835\n",
      "Epoch 6 step 1806: training loss: 258.81291629957127\n",
      "Epoch 6 step 1807: training accuarcy: 0.977\n",
      "Epoch 6 step 1807: training loss: 245.82492768834203\n",
      "Epoch 6 step 1808: training accuarcy: 0.9765\n",
      "Epoch 6 step 1808: training loss: 251.51543386053677\n",
      "Epoch 6 step 1809: training accuarcy: 0.9805\n",
      "Epoch 6 step 1809: training loss: 236.2341073234748\n",
      "Epoch 6 step 1810: training accuarcy: 0.982\n",
      "Epoch 6 step 1810: training loss: 254.45739199584182\n",
      "Epoch 6 step 1811: training accuarcy: 0.98\n",
      "Epoch 6 step 1811: training loss: 255.52121004901437\n",
      "Epoch 6 step 1812: training accuarcy: 0.9735\n",
      "Epoch 6 step 1812: training loss: 255.12113387743403\n",
      "Epoch 6 step 1813: training accuarcy: 0.9765\n",
      "Epoch 6 step 1813: training loss: 260.44650347940666\n",
      "Epoch 6 step 1814: training accuarcy: 0.9785\n",
      "Epoch 6 step 1814: training loss: 251.47734441443754\n",
      "Epoch 6 step 1815: training accuarcy: 0.979\n",
      "Epoch 6 step 1815: training loss: 248.42193518552247\n",
      "Epoch 6 step 1816: training accuarcy: 0.9825\n",
      "Epoch 6 step 1816: training loss: 250.47870144084405\n",
      "Epoch 6 step 1817: training accuarcy: 0.9765\n",
      "Epoch 6 step 1817: training loss: 268.0226121957191\n",
      "Epoch 6 step 1818: training accuarcy: 0.9765\n",
      "Epoch 6 step 1818: training loss: 244.1862447047771\n",
      "Epoch 6 step 1819: training accuarcy: 0.981\n",
      "Epoch 6 step 1819: training loss: 254.5950402200072\n",
      "Epoch 6 step 1820: training accuarcy: 0.977\n",
      "Epoch 6 step 1820: training loss: 242.71185699834217\n",
      "Epoch 6 step 1821: training accuarcy: 0.986\n",
      "Epoch 6 step 1821: training loss: 257.9550995131125\n",
      "Epoch 6 step 1822: training accuarcy: 0.982\n",
      "Epoch 6 step 1822: training loss: 237.90502269109322\n",
      "Epoch 6 step 1823: training accuarcy: 0.982\n",
      "Epoch 6 step 1823: training loss: 235.21757969490199\n",
      "Epoch 6 step 1824: training accuarcy: 0.9805\n",
      "Epoch 6 step 1824: training loss: 259.4864811785262\n",
      "Epoch 6 step 1825: training accuarcy: 0.975\n",
      "Epoch 6 step 1825: training loss: 242.27586397904543\n",
      "Epoch 6 step 1826: training accuarcy: 0.9835\n",
      "Epoch 6 step 1826: training loss: 250.5727823694232\n",
      "Epoch 6 step 1827: training accuarcy: 0.98\n",
      "Epoch 6 step 1827: training loss: 256.0310314560261\n",
      "Epoch 6 step 1828: training accuarcy: 0.979\n",
      "Epoch 6 step 1828: training loss: 260.66424387995374\n",
      "Epoch 6 step 1829: training accuarcy: 0.974\n",
      "Epoch 6 step 1829: training loss: 249.02252095722676\n",
      "Epoch 6 step 1830: training accuarcy: 0.982\n",
      "Epoch 6 step 1830: training loss: 243.6317316114198\n",
      "Epoch 6 step 1831: training accuarcy: 0.9825\n",
      "Epoch 6 step 1831: training loss: 256.0917421326599\n",
      "Epoch 6 step 1832: training accuarcy: 0.977\n",
      "Epoch 6 step 1832: training loss: 242.16521695225123\n",
      "Epoch 6 step 1833: training accuarcy: 0.98\n",
      "Epoch 6 step 1833: training loss: 245.67361457117\n",
      "Epoch 6 step 1834: training accuarcy: 0.9795\n",
      "Epoch 6 step 1834: training loss: 240.71289769909637\n",
      "Epoch 6 step 1835: training accuarcy: 0.979\n",
      "Epoch 6 step 1835: training loss: 245.31565645645395\n",
      "Epoch 6 step 1836: training accuarcy: 0.979\n",
      "Epoch 6 step 1836: training loss: 256.6094074278575\n",
      "Epoch 6 step 1837: training accuarcy: 0.979\n",
      "Epoch 6 step 1837: training loss: 233.37632116617718\n",
      "Epoch 6 step 1838: training accuarcy: 0.986\n",
      "Epoch 6 step 1838: training loss: 234.0348094966601\n",
      "Epoch 6 step 1839: training accuarcy: 0.9845\n",
      "Epoch 6 step 1839: training loss: 244.20991161044654\n",
      "Epoch 6 step 1840: training accuarcy: 0.981\n",
      "Epoch 6 step 1840: training loss: 100.71726507129797\n",
      "Epoch 6 step 1841: training accuarcy: 0.985897435897436\n",
      "Epoch 6: train loss 262.19572522030217, train accuarcy 0.9682431817054749\n",
      "Epoch 6: valid loss 734.2455726166758, valid accuarcy 0.8498079776763916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                   | 7/8 [15:33<02:12, 132.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Epoch 7 step 1841: training loss: 207.70664988120637\n",
      "Epoch 7 step 1842: training accuarcy: 0.987\n",
      "Epoch 7 step 1842: training loss: 201.9350882288199\n",
      "Epoch 7 step 1843: training accuarcy: 0.9905\n",
      "Epoch 7 step 1843: training loss: 200.32081485993274\n",
      "Epoch 7 step 1844: training accuarcy: 0.9895\n",
      "Epoch 7 step 1844: training loss: 211.9872521946416\n",
      "Epoch 7 step 1845: training accuarcy: 0.9875\n",
      "Epoch 7 step 1845: training loss: 211.92228309816176\n",
      "Epoch 7 step 1846: training accuarcy: 0.9875\n",
      "Epoch 7 step 1846: training loss: 201.46405688122888\n",
      "Epoch 7 step 1847: training accuarcy: 0.99\n",
      "Epoch 7 step 1847: training loss: 209.99295785035767\n",
      "Epoch 7 step 1848: training accuarcy: 0.99\n",
      "Epoch 7 step 1848: training loss: 212.418770928042\n",
      "Epoch 7 step 1849: training accuarcy: 0.9915\n",
      "Epoch 7 step 1849: training loss: 198.96235930242898\n",
      "Epoch 7 step 1850: training accuarcy: 0.989\n",
      "Epoch 7 step 1850: training loss: 210.20298751367457\n",
      "Epoch 7 step 1851: training accuarcy: 0.9865\n",
      "Epoch 7 step 1851: training loss: 206.28328152527143\n",
      "Epoch 7 step 1852: training accuarcy: 0.987\n",
      "Epoch 7 step 1852: training loss: 212.21004746160577\n",
      "Epoch 7 step 1853: training accuarcy: 0.989\n",
      "Epoch 7 step 1853: training loss: 205.25245702461083\n",
      "Epoch 7 step 1854: training accuarcy: 0.99\n",
      "Epoch 7 step 1854: training loss: 207.03804603198563\n",
      "Epoch 7 step 1855: training accuarcy: 0.987\n",
      "Epoch 7 step 1855: training loss: 216.46610950002713\n",
      "Epoch 7 step 1856: training accuarcy: 0.9835\n",
      "Epoch 7 step 1856: training loss: 203.73785600015194\n",
      "Epoch 7 step 1857: training accuarcy: 0.992\n",
      "Epoch 7 step 1857: training loss: 199.68019644183815\n",
      "Epoch 7 step 1858: training accuarcy: 0.9925\n",
      "Epoch 7 step 1858: training loss: 203.8936169723514\n",
      "Epoch 7 step 1859: training accuarcy: 0.9865\n",
      "Epoch 7 step 1859: training loss: 206.8329228607105\n",
      "Epoch 7 step 1860: training accuarcy: 0.988\n",
      "Epoch 7 step 1860: training loss: 209.40738050590758\n",
      "Epoch 7 step 1861: training accuarcy: 0.986\n",
      "Epoch 7 step 1861: training loss: 216.34653107898498\n",
      "Epoch 7 step 1862: training accuarcy: 0.9865\n",
      "Epoch 7 step 1862: training loss: 196.20570418592675\n",
      "Epoch 7 step 1863: training accuarcy: 0.9925\n",
      "Epoch 7 step 1863: training loss: 218.90210548357902\n",
      "Epoch 7 step 1864: training accuarcy: 0.984\n",
      "Epoch 7 step 1864: training loss: 220.80608181557324\n",
      "Epoch 7 step 1865: training accuarcy: 0.9845\n",
      "Epoch 7 step 1865: training loss: 199.7191261786881\n",
      "Epoch 7 step 1866: training accuarcy: 0.988\n",
      "Epoch 7 step 1866: training loss: 209.15978368340532\n",
      "Epoch 7 step 1867: training accuarcy: 0.987\n",
      "Epoch 7 step 1867: training loss: 197.66564552967057\n",
      "Epoch 7 step 1868: training accuarcy: 0.991\n",
      "Epoch 7 step 1868: training loss: 194.39877216583437\n",
      "Epoch 7 step 1869: training accuarcy: 0.992\n",
      "Epoch 7 step 1869: training loss: 206.53974390207526\n",
      "Epoch 7 step 1870: training accuarcy: 0.9885\n",
      "Epoch 7 step 1870: training loss: 205.64486093595596\n",
      "Epoch 7 step 1871: training accuarcy: 0.987\n",
      "Epoch 7 step 1871: training loss: 208.27479352607037\n",
      "Epoch 7 step 1872: training accuarcy: 0.9855\n",
      "Epoch 7 step 1872: training loss: 210.34860504273257\n",
      "Epoch 7 step 1873: training accuarcy: 0.986\n",
      "Epoch 7 step 1873: training loss: 206.3676189988021\n",
      "Epoch 7 step 1874: training accuarcy: 0.985\n",
      "Epoch 7 step 1874: training loss: 196.79478599101955\n",
      "Epoch 7 step 1875: training accuarcy: 0.99\n",
      "Epoch 7 step 1875: training loss: 210.77153170845563\n",
      "Epoch 7 step 1876: training accuarcy: 0.9825\n",
      "Epoch 7 step 1876: training loss: 201.0320534732664\n",
      "Epoch 7 step 1877: training accuarcy: 0.9885\n",
      "Epoch 7 step 1877: training loss: 223.19123227634645\n",
      "Epoch 7 step 1878: training accuarcy: 0.9835\n",
      "Epoch 7 step 1878: training loss: 207.84727550643893\n",
      "Epoch 7 step 1879: training accuarcy: 0.9915\n",
      "Epoch 7 step 1879: training loss: 212.26499436710253\n",
      "Epoch 7 step 1880: training accuarcy: 0.987\n",
      "Epoch 7 step 1880: training loss: 205.5824762407209\n",
      "Epoch 7 step 1881: training accuarcy: 0.9835\n",
      "Epoch 7 step 1881: training loss: 212.916578981712\n",
      "Epoch 7 step 1882: training accuarcy: 0.9845\n",
      "Epoch 7 step 1882: training loss: 199.19871843771577\n",
      "Epoch 7 step 1883: training accuarcy: 0.992\n",
      "Epoch 7 step 1883: training loss: 208.20969171851533\n",
      "Epoch 7 step 1884: training accuarcy: 0.9885\n",
      "Epoch 7 step 1884: training loss: 207.0822084322706\n",
      "Epoch 7 step 1885: training accuarcy: 0.9915\n",
      "Epoch 7 step 1885: training loss: 215.89584840198268\n",
      "Epoch 7 step 1886: training accuarcy: 0.987\n",
      "Epoch 7 step 1886: training loss: 201.756712916904\n",
      "Epoch 7 step 1887: training accuarcy: 0.993\n",
      "Epoch 7 step 1887: training loss: 198.17100482913378\n",
      "Epoch 7 step 1888: training accuarcy: 0.9905\n",
      "Epoch 7 step 1888: training loss: 186.12629840702314\n",
      "Epoch 7 step 1889: training accuarcy: 0.993\n",
      "Epoch 7 step 1889: training loss: 196.13258398652175\n",
      "Epoch 7 step 1890: training accuarcy: 0.988\n",
      "Epoch 7 step 1890: training loss: 199.34010475357076\n",
      "Epoch 7 step 1891: training accuarcy: 0.988\n",
      "Epoch 7 step 1891: training loss: 208.90641850264302\n",
      "Epoch 7 step 1892: training accuarcy: 0.9875\n",
      "Epoch 7 step 1892: training loss: 201.05307091780773\n",
      "Epoch 7 step 1893: training accuarcy: 0.9895\n",
      "Epoch 7 step 1893: training loss: 198.40222576840728\n",
      "Epoch 7 step 1894: training accuarcy: 0.989\n",
      "Epoch 7 step 1894: training loss: 200.41799014830212\n",
      "Epoch 7 step 1895: training accuarcy: 0.9865\n",
      "Epoch 7 step 1895: training loss: 206.0597351567057\n",
      "Epoch 7 step 1896: training accuarcy: 0.987\n",
      "Epoch 7 step 1896: training loss: 201.04561977728264\n",
      "Epoch 7 step 1897: training accuarcy: 0.991\n",
      "Epoch 7 step 1897: training loss: 208.93453316002797\n",
      "Epoch 7 step 1898: training accuarcy: 0.9835\n",
      "Epoch 7 step 1898: training loss: 197.4842003085442\n",
      "Epoch 7 step 1899: training accuarcy: 0.985\n",
      "Epoch 7 step 1899: training loss: 196.65385662236508\n",
      "Epoch 7 step 1900: training accuarcy: 0.9885\n",
      "Epoch 7 step 1900: training loss: 201.8013241509314\n",
      "Epoch 7 step 1901: training accuarcy: 0.9895\n",
      "Epoch 7 step 1901: training loss: 204.01851140246262\n",
      "Epoch 7 step 1902: training accuarcy: 0.9885\n",
      "Epoch 7 step 1902: training loss: 210.53709441447893\n",
      "Epoch 7 step 1903: training accuarcy: 0.9855\n",
      "Epoch 7 step 1903: training loss: 196.83829263895285\n",
      "Epoch 7 step 1904: training accuarcy: 0.9865\n",
      "Epoch 7 step 1904: training loss: 210.4778962181575\n",
      "Epoch 7 step 1905: training accuarcy: 0.9875\n",
      "Epoch 7 step 1905: training loss: 204.5471636297518\n",
      "Epoch 7 step 1906: training accuarcy: 0.985\n",
      "Epoch 7 step 1906: training loss: 206.06966534713692\n",
      "Epoch 7 step 1907: training accuarcy: 0.9875\n",
      "Epoch 7 step 1907: training loss: 197.9910080986268\n",
      "Epoch 7 step 1908: training accuarcy: 0.9915\n",
      "Epoch 7 step 1908: training loss: 201.06860680874303\n",
      "Epoch 7 step 1909: training accuarcy: 0.9875\n",
      "Epoch 7 step 1909: training loss: 198.2796926241906\n",
      "Epoch 7 step 1910: training accuarcy: 0.9875\n",
      "Epoch 7 step 1910: training loss: 204.05249855637157\n",
      "Epoch 7 step 1911: training accuarcy: 0.9875\n",
      "Epoch 7 step 1911: training loss: 215.64568078583463\n",
      "Epoch 7 step 1912: training accuarcy: 0.9815\n",
      "Epoch 7 step 1912: training loss: 201.51768013759047\n",
      "Epoch 7 step 1913: training accuarcy: 0.988\n",
      "Epoch 7 step 1913: training loss: 198.85645156867508\n",
      "Epoch 7 step 1914: training accuarcy: 0.9895\n",
      "Epoch 7 step 1914: training loss: 200.6896932764546\n",
      "Epoch 7 step 1915: training accuarcy: 0.988\n",
      "Epoch 7 step 1915: training loss: 200.52126072762235\n",
      "Epoch 7 step 1916: training accuarcy: 0.986\n",
      "Epoch 7 step 1916: training loss: 211.13272561373245\n",
      "Epoch 7 step 1917: training accuarcy: 0.9875\n",
      "Epoch 7 step 1917: training loss: 201.4299673616234\n",
      "Epoch 7 step 1918: training accuarcy: 0.984\n",
      "Epoch 7 step 1918: training loss: 210.011861055806\n",
      "Epoch 7 step 1919: training accuarcy: 0.984\n",
      "Epoch 7 step 1919: training loss: 209.27923374761755\n",
      "Epoch 7 step 1920: training accuarcy: 0.9865\n",
      "Epoch 7 step 1920: training loss: 213.8370305881519\n",
      "Epoch 7 step 1921: training accuarcy: 0.985\n",
      "Epoch 7 step 1921: training loss: 203.47938612126683\n",
      "Epoch 7 step 1922: training accuarcy: 0.9855\n",
      "Epoch 7 step 1922: training loss: 182.47372060774933\n",
      "Epoch 7 step 1923: training accuarcy: 0.991\n",
      "Epoch 7 step 1923: training loss: 204.3209083862531\n",
      "Epoch 7 step 1924: training accuarcy: 0.9905\n",
      "Epoch 7 step 1924: training loss: 210.20890107110307\n",
      "Epoch 7 step 1925: training accuarcy: 0.985\n",
      "Epoch 7 step 1925: training loss: 194.71575382818938\n",
      "Epoch 7 step 1926: training accuarcy: 0.989\n",
      "Epoch 7 step 1926: training loss: 202.36474075876674\n",
      "Epoch 7 step 1927: training accuarcy: 0.9845\n",
      "Epoch 7 step 1927: training loss: 195.91515723538737\n",
      "Epoch 7 step 1928: training accuarcy: 0.9875\n",
      "Epoch 7 step 1928: training loss: 191.8330441657476\n",
      "Epoch 7 step 1929: training accuarcy: 0.9895\n",
      "Epoch 7 step 1929: training loss: 201.41853329777916\n",
      "Epoch 7 step 1930: training accuarcy: 0.987\n",
      "Epoch 7 step 1930: training loss: 209.73906444579933\n",
      "Epoch 7 step 1931: training accuarcy: 0.986\n",
      "Epoch 7 step 1931: training loss: 200.4897903799368\n",
      "Epoch 7 step 1932: training accuarcy: 0.99\n",
      "Epoch 7 step 1932: training loss: 201.1701422003739\n",
      "Epoch 7 step 1933: training accuarcy: 0.99\n",
      "Epoch 7 step 1933: training loss: 191.6006720319378\n",
      "Epoch 7 step 1934: training accuarcy: 0.99\n",
      "Epoch 7 step 1934: training loss: 204.16885072806537\n",
      "Epoch 7 step 1935: training accuarcy: 0.988\n",
      "Epoch 7 step 1935: training loss: 198.74664771077443\n",
      "Epoch 7 step 1936: training accuarcy: 0.9865\n",
      "Epoch 7 step 1936: training loss: 213.47148690419525\n",
      "Epoch 7 step 1937: training accuarcy: 0.982\n",
      "Epoch 7 step 1937: training loss: 207.84459242494327\n",
      "Epoch 7 step 1938: training accuarcy: 0.985\n",
      "Epoch 7 step 1938: training loss: 201.60023047268984\n",
      "Epoch 7 step 1939: training accuarcy: 0.9885\n",
      "Epoch 7 step 1939: training loss: 209.7801363203075\n",
      "Epoch 7 step 1940: training accuarcy: 0.9845\n",
      "Epoch 7 step 1940: training loss: 197.54670242346373\n",
      "Epoch 7 step 1941: training accuarcy: 0.987\n",
      "Epoch 7 step 1941: training loss: 198.21055272053238\n",
      "Epoch 7 step 1942: training accuarcy: 0.9855\n",
      "Epoch 7 step 1942: training loss: 195.50967000738547\n",
      "Epoch 7 step 1943: training accuarcy: 0.9895\n",
      "Epoch 7 step 1943: training loss: 199.33828677231728\n",
      "Epoch 7 step 1944: training accuarcy: 0.9865\n",
      "Epoch 7 step 1944: training loss: 209.98100839365097\n",
      "Epoch 7 step 1945: training accuarcy: 0.9845\n",
      "Epoch 7 step 1945: training loss: 194.4666235470349\n",
      "Epoch 7 step 1946: training accuarcy: 0.989\n",
      "Epoch 7 step 1946: training loss: 206.44200386616953\n",
      "Epoch 7 step 1947: training accuarcy: 0.9865\n",
      "Epoch 7 step 1947: training loss: 197.67111978269708\n",
      "Epoch 7 step 1948: training accuarcy: 0.9885\n",
      "Epoch 7 step 1948: training loss: 201.37410786076435\n",
      "Epoch 7 step 1949: training accuarcy: 0.988\n",
      "Epoch 7 step 1949: training loss: 196.47088668835818\n",
      "Epoch 7 step 1950: training accuarcy: 0.986\n",
      "Epoch 7 step 1950: training loss: 191.04163304970857\n",
      "Epoch 7 step 1951: training accuarcy: 0.9885\n",
      "Epoch 7 step 1951: training loss: 197.27841809450135\n",
      "Epoch 7 step 1952: training accuarcy: 0.988\n",
      "Epoch 7 step 1952: training loss: 205.82140466503495\n",
      "Epoch 7 step 1953: training accuarcy: 0.986\n",
      "Epoch 7 step 1953: training loss: 203.10865649024007\n",
      "Epoch 7 step 1954: training accuarcy: 0.9865\n",
      "Epoch 7 step 1954: training loss: 207.035770225772\n",
      "Epoch 7 step 1955: training accuarcy: 0.985\n",
      "Epoch 7 step 1955: training loss: 205.97336136898127\n",
      "Epoch 7 step 1956: training accuarcy: 0.991\n",
      "Epoch 7 step 1956: training loss: 197.2440452127474\n",
      "Epoch 7 step 1957: training accuarcy: 0.9855\n",
      "Epoch 7 step 1957: training loss: 200.02210792097776\n",
      "Epoch 7 step 1958: training accuarcy: 0.988\n",
      "Epoch 7 step 1958: training loss: 180.46491338651143\n",
      "Epoch 7 step 1959: training accuarcy: 0.9935\n",
      "Epoch 7 step 1959: training loss: 213.08817781437548\n",
      "Epoch 7 step 1960: training accuarcy: 0.984\n",
      "Epoch 7 step 1960: training loss: 200.85004781878473\n",
      "Epoch 7 step 1961: training accuarcy: 0.99\n",
      "Epoch 7 step 1961: training loss: 211.23806002233056\n",
      "Epoch 7 step 1962: training accuarcy: 0.9845\n",
      "Epoch 7 step 1962: training loss: 191.12599953704284\n",
      "Epoch 7 step 1963: training accuarcy: 0.9905\n",
      "Epoch 7 step 1963: training loss: 199.8681200860777\n",
      "Epoch 7 step 1964: training accuarcy: 0.99\n",
      "Epoch 7 step 1964: training loss: 197.5919619902008\n",
      "Epoch 7 step 1965: training accuarcy: 0.9905\n",
      "Epoch 7 step 1965: training loss: 191.8435230537722\n",
      "Epoch 7 step 1966: training accuarcy: 0.99\n",
      "Epoch 7 step 1966: training loss: 192.63991299493273\n",
      "Epoch 7 step 1967: training accuarcy: 0.9885\n",
      "Epoch 7 step 1967: training loss: 196.63968493693608\n",
      "Epoch 7 step 1968: training accuarcy: 0.9865\n",
      "Epoch 7 step 1968: training loss: 203.30959703656043\n",
      "Epoch 7 step 1969: training accuarcy: 0.9835\n",
      "Epoch 7 step 1969: training loss: 212.65313173429706\n",
      "Epoch 7 step 1970: training accuarcy: 0.983\n",
      "Epoch 7 step 1970: training loss: 191.11323596396767\n",
      "Epoch 7 step 1971: training accuarcy: 0.991\n",
      "Epoch 7 step 1971: training loss: 202.2275146407526\n",
      "Epoch 7 step 1972: training accuarcy: 0.984\n",
      "Epoch 7 step 1972: training loss: 202.94064708674244\n",
      "Epoch 7 step 1973: training accuarcy: 0.986\n",
      "Epoch 7 step 1973: training loss: 192.56344414030292\n",
      "Epoch 7 step 1974: training accuarcy: 0.989\n",
      "Epoch 7 step 1974: training loss: 204.03016873888907\n",
      "Epoch 7 step 1975: training accuarcy: 0.9855\n",
      "Epoch 7 step 1975: training loss: 205.22476052198516\n",
      "Epoch 7 step 1976: training accuarcy: 0.985\n",
      "Epoch 7 step 1976: training loss: 198.82506497053208\n",
      "Epoch 7 step 1977: training accuarcy: 0.9875\n",
      "Epoch 7 step 1977: training loss: 193.90618673103452\n",
      "Epoch 7 step 1978: training accuarcy: 0.9865\n",
      "Epoch 7 step 1978: training loss: 186.19781962227609\n",
      "Epoch 7 step 1979: training accuarcy: 0.9915\n",
      "Epoch 7 step 1979: training loss: 201.53266485911212\n",
      "Epoch 7 step 1980: training accuarcy: 0.985\n",
      "Epoch 7 step 1980: training loss: 203.1395460522974\n",
      "Epoch 7 step 1981: training accuarcy: 0.987\n",
      "Epoch 7 step 1981: training loss: 201.94741133514486\n",
      "Epoch 7 step 1982: training accuarcy: 0.986\n",
      "Epoch 7 step 1982: training loss: 185.27685996095758\n",
      "Epoch 7 step 1983: training accuarcy: 0.9905\n",
      "Epoch 7 step 1983: training loss: 193.9234018274758\n",
      "Epoch 7 step 1984: training accuarcy: 0.9905\n",
      "Epoch 7 step 1984: training loss: 195.5698908991922\n",
      "Epoch 7 step 1985: training accuarcy: 0.99\n",
      "Epoch 7 step 1985: training loss: 188.28098755807721\n",
      "Epoch 7 step 1986: training accuarcy: 0.9905\n",
      "Epoch 7 step 1986: training loss: 206.3608874661516\n",
      "Epoch 7 step 1987: training accuarcy: 0.985\n",
      "Epoch 7 step 1987: training loss: 198.62140155505048\n",
      "Epoch 7 step 1988: training accuarcy: 0.987\n",
      "Epoch 7 step 1988: training loss: 198.78647780080698\n",
      "Epoch 7 step 1989: training accuarcy: 0.9895\n",
      "Epoch 7 step 1989: training loss: 200.68356044309226\n",
      "Epoch 7 step 1990: training accuarcy: 0.989\n",
      "Epoch 7 step 1990: training loss: 197.96384078905317\n",
      "Epoch 7 step 1991: training accuarcy: 0.9895\n",
      "Epoch 7 step 1991: training loss: 202.32091299680184\n",
      "Epoch 7 step 1992: training accuarcy: 0.988\n",
      "Epoch 7 step 1992: training loss: 207.1027944162582\n",
      "Epoch 7 step 1993: training accuarcy: 0.983\n",
      "Epoch 7 step 1993: training loss: 198.69508829122765\n",
      "Epoch 7 step 1994: training accuarcy: 0.985\n",
      "Epoch 7 step 1994: training loss: 200.68728667292635\n",
      "Epoch 7 step 1995: training accuarcy: 0.988\n",
      "Epoch 7 step 1995: training loss: 201.70874841066\n",
      "Epoch 7 step 1996: training accuarcy: 0.988\n",
      "Epoch 7 step 1996: training loss: 204.1426079823822\n",
      "Epoch 7 step 1997: training accuarcy: 0.9845\n",
      "Epoch 7 step 1997: training loss: 200.30664951380268\n",
      "Epoch 7 step 1998: training accuarcy: 0.9885\n",
      "Epoch 7 step 1998: training loss: 193.848050018941\n",
      "Epoch 7 step 1999: training accuarcy: 0.9905\n",
      "Epoch 7 step 1999: training loss: 197.39485712266907\n",
      "Epoch 7 step 2000: training accuarcy: 0.9855\n",
      "Epoch 7 step 2000: training loss: 207.72221698265668\n",
      "Epoch 7 step 2001: training accuarcy: 0.9855\n",
      "Epoch 7 step 2001: training loss: 179.10700673977752\n",
      "Epoch 7 step 2002: training accuarcy: 0.994\n",
      "Epoch 7 step 2002: training loss: 201.19844097024207\n",
      "Epoch 7 step 2003: training accuarcy: 0.9875\n",
      "Epoch 7 step 2003: training loss: 185.09509158644244\n",
      "Epoch 7 step 2004: training accuarcy: 0.9885\n",
      "Epoch 7 step 2004: training loss: 191.6743682502882\n",
      "Epoch 7 step 2005: training accuarcy: 0.991\n",
      "Epoch 7 step 2005: training loss: 195.402850712939\n",
      "Epoch 7 step 2006: training accuarcy: 0.9845\n",
      "Epoch 7 step 2006: training loss: 187.46562219729825\n",
      "Epoch 7 step 2007: training accuarcy: 0.989\n",
      "Epoch 7 step 2007: training loss: 193.03300272817557\n",
      "Epoch 7 step 2008: training accuarcy: 0.9885\n",
      "Epoch 7 step 2008: training loss: 199.88918413295312\n",
      "Epoch 7 step 2009: training accuarcy: 0.9845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 step 2009: training loss: 189.85619358721257\n",
      "Epoch 7 step 2010: training accuarcy: 0.989\n",
      "Epoch 7 step 2010: training loss: 190.03284790052487\n",
      "Epoch 7 step 2011: training accuarcy: 0.9855\n",
      "Epoch 7 step 2011: training loss: 188.68278290596368\n",
      "Epoch 7 step 2012: training accuarcy: 0.985\n",
      "Epoch 7 step 2012: training loss: 192.35876979405282\n",
      "Epoch 7 step 2013: training accuarcy: 0.99\n",
      "Epoch 7 step 2013: training loss: 194.51239370097494\n",
      "Epoch 7 step 2014: training accuarcy: 0.989\n",
      "Epoch 7 step 2014: training loss: 198.96255752038684\n",
      "Epoch 7 step 2015: training accuarcy: 0.991\n",
      "Epoch 7 step 2015: training loss: 195.3538354390031\n",
      "Epoch 7 step 2016: training accuarcy: 0.9875\n",
      "Epoch 7 step 2016: training loss: 205.26478259370407\n",
      "Epoch 7 step 2017: training accuarcy: 0.9865\n",
      "Epoch 7 step 2017: training loss: 198.26959940048633\n",
      "Epoch 7 step 2018: training accuarcy: 0.986\n",
      "Epoch 7 step 2018: training loss: 199.2528931640351\n",
      "Epoch 7 step 2019: training accuarcy: 0.9865\n",
      "Epoch 7 step 2019: training loss: 199.7590547529936\n",
      "Epoch 7 step 2020: training accuarcy: 0.9875\n",
      "Epoch 7 step 2020: training loss: 188.27624475792013\n",
      "Epoch 7 step 2021: training accuarcy: 0.989\n",
      "Epoch 7 step 2021: training loss: 181.34892453168501\n",
      "Epoch 7 step 2022: training accuarcy: 0.9935\n",
      "Epoch 7 step 2022: training loss: 197.83602276026335\n",
      "Epoch 7 step 2023: training accuarcy: 0.9895\n",
      "Epoch 7 step 2023: training loss: 195.7729010874447\n",
      "Epoch 7 step 2024: training accuarcy: 0.99\n",
      "Epoch 7 step 2024: training loss: 190.42880594596636\n",
      "Epoch 7 step 2025: training accuarcy: 0.99\n",
      "Epoch 7 step 2025: training loss: 185.79472392948955\n",
      "Epoch 7 step 2026: training accuarcy: 0.9895\n",
      "Epoch 7 step 2026: training loss: 209.58440086630597\n",
      "Epoch 7 step 2027: training accuarcy: 0.9825\n",
      "Epoch 7 step 2027: training loss: 198.38913875452613\n",
      "Epoch 7 step 2028: training accuarcy: 0.9865\n",
      "Epoch 7 step 2028: training loss: 198.44707735483303\n",
      "Epoch 7 step 2029: training accuarcy: 0.9875\n",
      "Epoch 7 step 2029: training loss: 196.30683988173192\n",
      "Epoch 7 step 2030: training accuarcy: 0.9835\n",
      "Epoch 7 step 2030: training loss: 199.60591834061208\n",
      "Epoch 7 step 2031: training accuarcy: 0.9845\n",
      "Epoch 7 step 2031: training loss: 173.67882995352292\n",
      "Epoch 7 step 2032: training accuarcy: 0.993\n",
      "Epoch 7 step 2032: training loss: 194.71329452790442\n",
      "Epoch 7 step 2033: training accuarcy: 0.986\n",
      "Epoch 7 step 2033: training loss: 172.83044820915825\n",
      "Epoch 7 step 2034: training accuarcy: 0.9945\n",
      "Epoch 7 step 2034: training loss: 189.52351565214897\n",
      "Epoch 7 step 2035: training accuarcy: 0.987\n",
      "Epoch 7 step 2035: training loss: 173.64356547589992\n",
      "Epoch 7 step 2036: training accuarcy: 0.993\n",
      "Epoch 7 step 2036: training loss: 197.30656723116135\n",
      "Epoch 7 step 2037: training accuarcy: 0.987\n",
      "Epoch 7 step 2037: training loss: 192.29480510301516\n",
      "Epoch 7 step 2038: training accuarcy: 0.987\n",
      "Epoch 7 step 2038: training loss: 194.53694280264844\n",
      "Epoch 7 step 2039: training accuarcy: 0.987\n",
      "Epoch 7 step 2039: training loss: 187.69421798533608\n",
      "Epoch 7 step 2040: training accuarcy: 0.9905\n",
      "Epoch 7 step 2040: training loss: 198.555644407369\n",
      "Epoch 7 step 2041: training accuarcy: 0.983\n",
      "Epoch 7 step 2041: training loss: 191.93193296575194\n",
      "Epoch 7 step 2042: training accuarcy: 0.9875\n",
      "Epoch 7 step 2042: training loss: 192.45203923080987\n",
      "Epoch 7 step 2043: training accuarcy: 0.9905\n",
      "Epoch 7 step 2043: training loss: 189.39820519648387\n",
      "Epoch 7 step 2044: training accuarcy: 0.992\n",
      "Epoch 7 step 2044: training loss: 190.69307676882633\n",
      "Epoch 7 step 2045: training accuarcy: 0.991\n",
      "Epoch 7 step 2045: training loss: 196.52907024321266\n",
      "Epoch 7 step 2046: training accuarcy: 0.988\n",
      "Epoch 7 step 2046: training loss: 191.86125264137468\n",
      "Epoch 7 step 2047: training accuarcy: 0.9825\n",
      "Epoch 7 step 2047: training loss: 206.69035759094822\n",
      "Epoch 7 step 2048: training accuarcy: 0.985\n",
      "Epoch 7 step 2048: training loss: 185.41801711236423\n",
      "Epoch 7 step 2049: training accuarcy: 0.9855\n",
      "Epoch 7 step 2049: training loss: 182.71136341669524\n",
      "Epoch 7 step 2050: training accuarcy: 0.9935\n",
      "Epoch 7 step 2050: training loss: 187.49942972915073\n",
      "Epoch 7 step 2051: training accuarcy: 0.989\n",
      "Epoch 7 step 2051: training loss: 189.62607332813013\n",
      "Epoch 7 step 2052: training accuarcy: 0.988\n",
      "Epoch 7 step 2052: training loss: 194.80141304551734\n",
      "Epoch 7 step 2053: training accuarcy: 0.9875\n",
      "Epoch 7 step 2053: training loss: 205.79302416930517\n",
      "Epoch 7 step 2054: training accuarcy: 0.983\n",
      "Epoch 7 step 2054: training loss: 184.89396442867303\n",
      "Epoch 7 step 2055: training accuarcy: 0.9855\n",
      "Epoch 7 step 2055: training loss: 183.95724765061362\n",
      "Epoch 7 step 2056: training accuarcy: 0.9895\n",
      "Epoch 7 step 2056: training loss: 192.08901278275178\n",
      "Epoch 7 step 2057: training accuarcy: 0.988\n",
      "Epoch 7 step 2057: training loss: 187.84568237999935\n",
      "Epoch 7 step 2058: training accuarcy: 0.987\n",
      "Epoch 7 step 2058: training loss: 184.0788129070872\n",
      "Epoch 7 step 2059: training accuarcy: 0.9885\n",
      "Epoch 7 step 2059: training loss: 193.69681684047129\n",
      "Epoch 7 step 2060: training accuarcy: 0.9865\n",
      "Epoch 7 step 2060: training loss: 201.23998206451972\n",
      "Epoch 7 step 2061: training accuarcy: 0.982\n",
      "Epoch 7 step 2061: training loss: 192.2542157447082\n",
      "Epoch 7 step 2062: training accuarcy: 0.9865\n",
      "Epoch 7 step 2062: training loss: 180.66511268384042\n",
      "Epoch 7 step 2063: training accuarcy: 0.991\n",
      "Epoch 7 step 2063: training loss: 195.98270056589848\n",
      "Epoch 7 step 2064: training accuarcy: 0.9845\n",
      "Epoch 7 step 2064: training loss: 192.9654278037764\n",
      "Epoch 7 step 2065: training accuarcy: 0.9895\n",
      "Epoch 7 step 2065: training loss: 195.50076917454024\n",
      "Epoch 7 step 2066: training accuarcy: 0.984\n",
      "Epoch 7 step 2066: training loss: 205.94770168418367\n",
      "Epoch 7 step 2067: training accuarcy: 0.9845\n",
      "Epoch 7 step 2067: training loss: 192.32612773351752\n",
      "Epoch 7 step 2068: training accuarcy: 0.987\n",
      "Epoch 7 step 2068: training loss: 178.38090083410864\n",
      "Epoch 7 step 2069: training accuarcy: 0.992\n",
      "Epoch 7 step 2069: training loss: 190.74094878630427\n",
      "Epoch 7 step 2070: training accuarcy: 0.991\n",
      "Epoch 7 step 2070: training loss: 185.96453725993788\n",
      "Epoch 7 step 2071: training accuarcy: 0.99\n",
      "Epoch 7 step 2071: training loss: 182.22145757716066\n",
      "Epoch 7 step 2072: training accuarcy: 0.9895\n",
      "Epoch 7 step 2072: training loss: 192.0989869058445\n",
      "Epoch 7 step 2073: training accuarcy: 0.9875\n",
      "Epoch 7 step 2073: training loss: 185.49345528783212\n",
      "Epoch 7 step 2074: training accuarcy: 0.9885\n",
      "Epoch 7 step 2074: training loss: 184.29696136444073\n",
      "Epoch 7 step 2075: training accuarcy: 0.9885\n",
      "Epoch 7 step 2075: training loss: 192.48282668414708\n",
      "Epoch 7 step 2076: training accuarcy: 0.988\n",
      "Epoch 7 step 2076: training loss: 191.0164560232529\n",
      "Epoch 7 step 2077: training accuarcy: 0.9855\n",
      "Epoch 7 step 2077: training loss: 183.9878441542758\n",
      "Epoch 7 step 2078: training accuarcy: 0.9905\n",
      "Epoch 7 step 2078: training loss: 200.77817819576717\n",
      "Epoch 7 step 2079: training accuarcy: 0.983\n",
      "Epoch 7 step 2079: training loss: 194.41916817337096\n",
      "Epoch 7 step 2080: training accuarcy: 0.985\n",
      "Epoch 7 step 2080: training loss: 190.40002338679088\n",
      "Epoch 7 step 2081: training accuarcy: 0.984\n",
      "Epoch 7 step 2081: training loss: 191.12751448896162\n",
      "Epoch 7 step 2082: training accuarcy: 0.9845\n",
      "Epoch 7 step 2082: training loss: 192.18796050301884\n",
      "Epoch 7 step 2083: training accuarcy: 0.9905\n",
      "Epoch 7 step 2083: training loss: 192.56957501876468\n",
      "Epoch 7 step 2084: training accuarcy: 0.9825\n",
      "Epoch 7 step 2084: training loss: 186.33359235461464\n",
      "Epoch 7 step 2085: training accuarcy: 0.989\n",
      "Epoch 7 step 2085: training loss: 186.03518050064932\n",
      "Epoch 7 step 2086: training accuarcy: 0.9885\n",
      "Epoch 7 step 2086: training loss: 192.14680697194305\n",
      "Epoch 7 step 2087: training accuarcy: 0.9835\n",
      "Epoch 7 step 2087: training loss: 186.64037300681636\n",
      "Epoch 7 step 2088: training accuarcy: 0.9895\n",
      "Epoch 7 step 2088: training loss: 171.380399580007\n",
      "Epoch 7 step 2089: training accuarcy: 0.9895\n",
      "Epoch 7 step 2089: training loss: 186.04106786328725\n",
      "Epoch 7 step 2090: training accuarcy: 0.9885\n",
      "Epoch 7 step 2090: training loss: 186.28147805399107\n",
      "Epoch 7 step 2091: training accuarcy: 0.989\n",
      "Epoch 7 step 2091: training loss: 212.10677828226704\n",
      "Epoch 7 step 2092: training accuarcy: 0.9795\n",
      "Epoch 7 step 2092: training loss: 185.1045714290892\n",
      "Epoch 7 step 2093: training accuarcy: 0.99\n",
      "Epoch 7 step 2093: training loss: 179.8702585323107\n",
      "Epoch 7 step 2094: training accuarcy: 0.9905\n",
      "Epoch 7 step 2094: training loss: 174.41150387827287\n",
      "Epoch 7 step 2095: training accuarcy: 0.9925\n",
      "Epoch 7 step 2095: training loss: 198.10309414644811\n",
      "Epoch 7 step 2096: training accuarcy: 0.985\n",
      "Epoch 7 step 2096: training loss: 180.61786192221035\n",
      "Epoch 7 step 2097: training accuarcy: 0.99\n",
      "Epoch 7 step 2097: training loss: 189.88928679214618\n",
      "Epoch 7 step 2098: training accuarcy: 0.99\n",
      "Epoch 7 step 2098: training loss: 189.52055193954126\n",
      "Epoch 7 step 2099: training accuarcy: 0.985\n",
      "Epoch 7 step 2099: training loss: 185.10759346590456\n",
      "Epoch 7 step 2100: training accuarcy: 0.9885\n",
      "Epoch 7 step 2100: training loss: 186.21774012971227\n",
      "Epoch 7 step 2101: training accuarcy: 0.987\n",
      "Epoch 7 step 2101: training loss: 188.7504374050971\n",
      "Epoch 7 step 2102: training accuarcy: 0.987\n",
      "Epoch 7 step 2102: training loss: 177.66305252278585\n",
      "Epoch 7 step 2103: training accuarcy: 0.9925\n",
      "Epoch 7 step 2103: training loss: 87.69625613753814\n",
      "Epoch 7 step 2104: training accuarcy: 0.9782051282051282\n",
      "Epoch 7: train loss 197.56988383337168, train accuarcy 0.9818708300590515\n",
      "Epoch 7: valid loss 689.107076028713, valid accuarcy 0.8642611503601074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [17:43<00:00, 132.13s/it]\n"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=5,\n",
    "                  log_dir=get_log_dir('seq_topcoder', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:02:29.409573Z",
     "start_time": "2019-09-25T14:02:29.280561Z"
    }
   },
   "outputs": [],
   "source": [
    "del trans_model\n",
    "T.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
