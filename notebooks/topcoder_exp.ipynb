{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T14:01:30.581717Z",
     "start_time": "2019-09-25T14:01:30.456659Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:01.167984Z",
     "start_time": "2019-10-09T11:57:01.161981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\python\\recommender\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:02.352854Z",
     "start_time": "2019-10-09T11:57:01.785338Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import get_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:03.963821Z",
     "start_time": "2019-10-09T11:57:02.354823Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import SeqTopcoder\n",
    "from models import FMLearner, TorchFM, TorchHrmFM, TorchPrmeFM, TorchTransFM\n",
    "from models.fm_learner import simple_loss, trans_loss, simple_weight_loss, trans_weight_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:04.868802Z",
     "start_time": "2019-10-09T11:57:04.648737Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = T.cuda.current_device()\n",
    "BATCH = 2000\n",
    "SHUFFLE = True\n",
    "WORKERS = 0\n",
    "NEG_SAMPLE = 5\n",
    "REGS_PATH = Path(\"./inputs/topcoder/regs.csv\")\n",
    "CHAG_PATH = Path(\"./inputs/topcoder/challenge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:07.827940Z",
     "start_time": "2019-10-09T11:57:05.335943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset in inputs\\topcoder\\regs.csv\n",
      "Original regs shape: (610025, 3)\n",
      "Original registants size: 60017\n",
      "Original challenges size: 39916\n",
      "Filter dataframe shape: (544568, 3)\n",
      "Index(['challengeId', 'period', 'date', 'prizes', 'technologies', 'platforms'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datasets.torch_topcoder.SeqTopcoder at 0x20bfe33cba8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = SeqTopcoder(regs_path=REGS_PATH, chag_path=CHAG_PATH)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:07.832969Z",
     "start_time": "2019-10-09T11:57:07.829940Z"
    }
   },
   "outputs": [],
   "source": [
    "db.config_db(batch_size=BATCH,\n",
    "             shuffle=SHUFFLE,\n",
    "             num_workers=WORKERS,\n",
    "             device=DEVICE,\n",
    "             neg_sample=NEG_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:08.202795Z",
     "start_time": "2019-10-09T11:57:08.200794Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:09.123661Z",
     "start_time": "2019-10-09T11:57:09.120663Z"
    }
   },
   "outputs": [],
   "source": [
    "# regst setting\n",
    "LINEAR_REG = 1\n",
    "EMB_REG = 1\n",
    "TRANS_REG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:10.846652Z",
     "start_time": "2019-10-09T11:57:10.841654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_loss at 0x0000020B88782620>, 1, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_loss_callback = partial(simple_loss, LINEAR_REG, EMB_REG)\n",
    "simple_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:11.197180Z",
     "start_time": "2019-10-09T11:57:11.192188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_loss at 0x0000020B887D6840>, 1, 1, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_loss_callback = partial(trans_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:11.644804Z",
     "start_time": "2019-10-09T11:57:11.640833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function simple_weight_loss at 0x0000020B887D68C8>, 1, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weight_loss_callback = partial(simple_weight_loss, LINEAR_REG, EMB_REG)\n",
    "simple_weight_loss_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:12.045018Z",
     "start_time": "2019-10-09T11:57:12.040017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function trans_weight_loss at 0x0000020B887D6950>, 1, 1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_weight_loss_callback = partial(trans_weight_loss, LINEAR_REG, EMB_REG, TRANS_REG)\n",
    "trans_weight_loss_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "#### Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:14.355456Z",
     "start_time": "2019-10-09T11:57:14.352475Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_dim = db.feat_dim\n",
    "NUM_DIM = 124\n",
    "INIT_MEAN = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:57:15.234087Z",
     "start_time": "2019-10-09T11:57:15.232086Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "DECAY_FREQ = 1000\n",
    "DECAY_GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:36.552338Z",
     "start_time": "2019-10-09T11:41:36.280341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchFM()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model = TorchFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:37.388025Z",
     "start_time": "2019-10-09T11:41:37.384028Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(fm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:38.693328Z",
     "start_time": "2019-10-09T11:41:38.648293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x1939e327ba8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_learner = FMLearner(fm_model, adam_opt, schedular, db)\n",
    "fm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:48:25.856807Z",
     "start_time": "2019-10-07T13:48:25.853806Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:26:24.690026Z",
     "start_time": "2019-10-09T11:26:24.687055Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='seq',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:40.324650Z",
     "start_time": "2019-10-09T11:41:40.320652Z"
    }
   },
   "outputs": [],
   "source": [
    "fm_learner.compile(train_col='seq',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:19.805716Z",
     "start_time": "2019-10-09T11:26:29.768670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 46546.154640854336\n",
      "Epoch 0 step 1: training accuarcy: 0.36760000000000004\n",
      "Epoch 0 step 1: training loss: 44140.504792487875\n",
      "Epoch 0 step 2: training accuarcy: 0.4319\n",
      "Epoch 0 step 2: training loss: 41675.07565998279\n",
      "Epoch 0 step 3: training accuarcy: 0.5108\n",
      "Epoch 0 step 3: training loss: 39450.99958670329\n",
      "Epoch 0 step 4: training accuarcy: 0.5817\n",
      "Epoch 0 step 4: training loss: 37480.27146773438\n",
      "Epoch 0 step 5: training accuarcy: 0.6623\n",
      "Epoch 0 step 5: training loss: 35875.01403513422\n",
      "Epoch 0 step 6: training accuarcy: 0.7083\n",
      "Epoch 0 step 6: training loss: 34544.81596775044\n",
      "Epoch 0 step 7: training accuarcy: 0.7407\n",
      "Epoch 0 step 7: training loss: 33181.54690224211\n",
      "Epoch 0 step 8: training accuarcy: 0.7913\n",
      "Epoch 0 step 8: training loss: 32084.20438950817\n",
      "Epoch 0 step 9: training accuarcy: 0.8089000000000001\n",
      "Epoch 0 step 9: training loss: 31093.71356452987\n",
      "Epoch 0 step 10: training accuarcy: 0.8297\n",
      "Epoch 0 step 10: training loss: 30231.732255741055\n",
      "Epoch 0 step 11: training accuarcy: 0.8453\n",
      "Epoch 0 step 11: training loss: 29520.315584328186\n",
      "Epoch 0 step 12: training accuarcy: 0.8402000000000001\n",
      "Epoch 0 step 12: training loss: 28884.354913167757\n",
      "Epoch 0 step 13: training accuarcy: 0.8398\n",
      "Epoch 0 step 13: training loss: 27905.745133619086\n",
      "Epoch 0 step 14: training accuarcy: 0.8664000000000001\n",
      "Epoch 0 step 14: training loss: 27171.315296167573\n",
      "Epoch 0 step 15: training accuarcy: 0.8752000000000001\n",
      "Epoch 0 step 15: training loss: 26567.28917460179\n",
      "Epoch 0 step 16: training accuarcy: 0.871\n",
      "Epoch 0 step 16: training loss: 25935.438804682497\n",
      "Epoch 0 step 17: training accuarcy: 0.8761\n",
      "Epoch 0 step 17: training loss: 25063.242458599463\n",
      "Epoch 0 step 18: training accuarcy: 0.9003\n",
      "Epoch 0 step 18: training loss: 24662.890787161403\n",
      "Epoch 0 step 19: training accuarcy: 0.8833000000000001\n",
      "Epoch 0 step 19: training loss: 24150.429451553682\n",
      "Epoch 0 step 20: training accuarcy: 0.8882\n",
      "Epoch 0 step 20: training loss: 23338.00173500339\n",
      "Epoch 0 step 21: training accuarcy: 0.9049\n",
      "Epoch 0 step 21: training loss: 22915.09829797726\n",
      "Epoch 0 step 22: training accuarcy: 0.8966000000000001\n",
      "Epoch 0 step 22: training loss: 22218.21218858137\n",
      "Epoch 0 step 23: training accuarcy: 0.9137000000000001\n",
      "Epoch 0 step 23: training loss: 21826.613483607995\n",
      "Epoch 0 step 24: training accuarcy: 0.9023\n",
      "Epoch 0 step 24: training loss: 21293.742965627564\n",
      "Epoch 0 step 25: training accuarcy: 0.9092\n",
      "Epoch 0 step 25: training loss: 20760.689774483093\n",
      "Epoch 0 step 26: training accuarcy: 0.9136000000000001\n",
      "Epoch 0 step 26: training loss: 20263.351203293496\n",
      "Epoch 0 step 27: training accuarcy: 0.9203\n",
      "Epoch 0 step 27: training loss: 19720.54019230831\n",
      "Epoch 0 step 28: training accuarcy: 0.9247000000000001\n",
      "Epoch 0 step 28: training loss: 19490.808730795514\n",
      "Epoch 0 step 29: training accuarcy: 0.9159\n",
      "Epoch 0 step 29: training loss: 18836.18159112953\n",
      "Epoch 0 step 30: training accuarcy: 0.9322\n",
      "Epoch 0 step 30: training loss: 18458.397152266683\n",
      "Epoch 0 step 31: training accuarcy: 0.9288000000000001\n",
      "Epoch 0 step 31: training loss: 18164.47932100472\n",
      "Epoch 0 step 32: training accuarcy: 0.9165000000000001\n",
      "Epoch 0 step 32: training loss: 17732.237286048134\n",
      "Epoch 0 step 33: training accuarcy: 0.9246000000000001\n",
      "Epoch 0 step 33: training loss: 17333.765721345455\n",
      "Epoch 0 step 34: training accuarcy: 0.9298000000000001\n",
      "Epoch 0 step 34: training loss: 16927.19440547125\n",
      "Epoch 0 step 35: training accuarcy: 0.9311\n",
      "Epoch 0 step 35: training loss: 16573.69296637209\n",
      "Epoch 0 step 36: training accuarcy: 0.9331\n",
      "Epoch 0 step 36: training loss: 16151.36839337222\n",
      "Epoch 0 step 37: training accuarcy: 0.9404\n",
      "Epoch 0 step 37: training loss: 15708.833770822308\n",
      "Epoch 0 step 38: training accuarcy: 0.9489000000000001\n",
      "Epoch 0 step 38: training loss: 15564.171523687444\n",
      "Epoch 0 step 39: training accuarcy: 0.9311\n",
      "Epoch 0 step 39: training loss: 15121.363922667972\n",
      "Epoch 0 step 40: training accuarcy: 0.9445\n",
      "Epoch 0 step 40: training loss: 14819.06057104719\n",
      "Epoch 0 step 41: training accuarcy: 0.9409000000000001\n",
      "Epoch 0 step 41: training loss: 14503.302605305325\n",
      "Epoch 0 step 42: training accuarcy: 0.9477000000000001\n",
      "Epoch 0 step 42: training loss: 14188.560423289051\n",
      "Epoch 0 step 43: training accuarcy: 0.9499000000000001\n",
      "Epoch 0 step 43: training loss: 13981.649054906664\n",
      "Epoch 0 step 44: training accuarcy: 0.9471\n",
      "Epoch 0 step 44: training loss: 13697.81775616869\n",
      "Epoch 0 step 45: training accuarcy: 0.9496\n",
      "Epoch 0 step 45: training loss: 13390.965856939478\n",
      "Epoch 0 step 46: training accuarcy: 0.9445\n",
      "Epoch 0 step 46: training loss: 13072.525224967161\n",
      "Epoch 0 step 47: training accuarcy: 0.9561000000000001\n",
      "Epoch 0 step 47: training loss: 12863.386391707756\n",
      "Epoch 0 step 48: training accuarcy: 0.9534\n",
      "Epoch 0 step 48: training loss: 12517.594751065391\n",
      "Epoch 0 step 49: training accuarcy: 0.9599000000000001\n",
      "Epoch 0 step 49: training loss: 12277.26593375544\n",
      "Epoch 0 step 50: training accuarcy: 0.9607\n",
      "Epoch 0 step 50: training loss: 12023.484709600978\n",
      "Epoch 0 step 51: training accuarcy: 0.9603\n",
      "Epoch 0 step 51: training loss: 11871.820783373429\n",
      "Epoch 0 step 52: training accuarcy: 0.9522\n",
      "Epoch 0 step 52: training loss: 11665.736990592095\n",
      "Epoch 0 step 53: training accuarcy: 0.9529000000000001\n",
      "Epoch 0 step 53: training loss: 11418.759232387505\n",
      "Epoch 0 step 54: training accuarcy: 0.9574\n",
      "Epoch 0 step 54: training loss: 11149.857302320948\n",
      "Epoch 0 step 55: training accuarcy: 0.9602\n",
      "Epoch 0 step 55: training loss: 10964.472566560516\n",
      "Epoch 0 step 56: training accuarcy: 0.9602\n",
      "Epoch 0 step 56: training loss: 10831.934317620402\n",
      "Epoch 0 step 57: training accuarcy: 0.9546\n",
      "Epoch 0 step 57: training loss: 10655.3275359064\n",
      "Epoch 0 step 58: training accuarcy: 0.9558000000000001\n",
      "Epoch 0 step 58: training loss: 10409.00993191277\n",
      "Epoch 0 step 59: training accuarcy: 0.9606\n",
      "Epoch 0 step 59: training loss: 10190.126736025262\n",
      "Epoch 0 step 60: training accuarcy: 0.9647\n",
      "Epoch 0 step 60: training loss: 10040.85403106833\n",
      "Epoch 0 step 61: training accuarcy: 0.9598000000000001\n",
      "Epoch 0 step 61: training loss: 9859.918656030142\n",
      "Epoch 0 step 62: training accuarcy: 0.9586\n",
      "Epoch 0 step 62: training loss: 9623.71167509555\n",
      "Epoch 0 step 63: training accuarcy: 0.9632000000000001\n",
      "Epoch 0 step 63: training loss: 9491.337163915134\n",
      "Epoch 0 step 64: training accuarcy: 0.9679000000000001\n",
      "Epoch 0 step 64: training loss: 9257.10484107236\n",
      "Epoch 0 step 65: training accuarcy: 0.9707\n",
      "Epoch 0 step 65: training loss: 9100.489427533488\n",
      "Epoch 0 step 66: training accuarcy: 0.9677\n",
      "Epoch 0 step 66: training loss: 9061.441009053218\n",
      "Epoch 0 step 67: training accuarcy: 0.9622\n",
      "Epoch 0 step 67: training loss: 8865.98514064014\n",
      "Epoch 0 step 68: training accuarcy: 0.9627\n",
      "Epoch 0 step 68: training loss: 8671.758932109413\n",
      "Epoch 0 step 69: training accuarcy: 0.9698\n",
      "Epoch 0 step 69: training loss: 8527.474412722964\n",
      "Epoch 0 step 70: training accuarcy: 0.9688\n",
      "Epoch 0 step 70: training loss: 8360.770416182342\n",
      "Epoch 0 step 71: training accuarcy: 0.9711000000000001\n",
      "Epoch 0 step 71: training loss: 8270.345836047847\n",
      "Epoch 0 step 72: training accuarcy: 0.9677\n",
      "Epoch 0 step 72: training loss: 8154.891683617339\n",
      "Epoch 0 step 73: training accuarcy: 0.9698\n",
      "Epoch 0 step 73: training loss: 8056.6704549640335\n",
      "Epoch 0 step 74: training accuarcy: 0.9666\n",
      "Epoch 0 step 74: training loss: 7895.8409671925565\n",
      "Epoch 0 step 75: training accuarcy: 0.9681000000000001\n",
      "Epoch 0 step 75: training loss: 7669.371218413247\n",
      "Epoch 0 step 76: training accuarcy: 0.9766\n",
      "Epoch 0 step 76: training loss: 7695.470488851995\n",
      "Epoch 0 step 77: training accuarcy: 0.9668\n",
      "Epoch 0 step 77: training loss: 7537.177961282316\n",
      "Epoch 0 step 78: training accuarcy: 0.9702000000000001\n",
      "Epoch 0 step 78: training loss: 7424.542323741191\n",
      "Epoch 0 step 79: training accuarcy: 0.9705\n",
      "Epoch 0 step 79: training loss: 7258.05852192447\n",
      "Epoch 0 step 80: training accuarcy: 0.9749000000000001\n",
      "Epoch 0 step 80: training loss: 7123.409775129712\n",
      "Epoch 0 step 81: training accuarcy: 0.9774\n",
      "Epoch 0 step 81: training loss: 7149.951348710446\n",
      "Epoch 0 step 82: training accuarcy: 0.9692000000000001\n",
      "Epoch 0 step 82: training loss: 6994.272865394437\n",
      "Epoch 0 step 83: training accuarcy: 0.9710000000000001\n",
      "Epoch 0 step 83: training loss: 6872.508768189162\n",
      "Epoch 0 step 84: training accuarcy: 0.9709000000000001\n",
      "Epoch 0 step 84: training loss: 6749.962539922964\n",
      "Epoch 0 step 85: training accuarcy: 0.9766\n",
      "Epoch 0 step 85: training loss: 6750.392220424044\n",
      "Epoch 0 step 86: training accuarcy: 0.9708\n",
      "Epoch 0 step 86: training loss: 6635.5681847329215\n",
      "Epoch 0 step 87: training accuarcy: 0.9713\n",
      "Epoch 0 step 87: training loss: 6420.73477351625\n",
      "Epoch 0 step 88: training accuarcy: 0.9803000000000001\n",
      "Epoch 0 step 88: training loss: 6430.323353674918\n",
      "Epoch 0 step 89: training accuarcy: 0.9726\n",
      "Epoch 0 step 89: training loss: 6281.644858462743\n",
      "Epoch 0 step 90: training accuarcy: 0.9779\n",
      "Epoch 0 step 90: training loss: 6135.84968099944\n",
      "Epoch 0 step 91: training accuarcy: 0.9788\n",
      "Epoch 0 step 91: training loss: 6184.901493894874\n",
      "Epoch 0 step 92: training accuarcy: 0.9747\n",
      "Epoch 0 step 92: training loss: 6092.918878513035\n",
      "Epoch 0 step 93: training accuarcy: 0.9746\n",
      "Epoch 0 step 93: training loss: 5974.349425806837\n",
      "Epoch 0 step 94: training accuarcy: 0.9783000000000001\n",
      "Epoch 0 step 94: training loss: 5904.541500218362\n",
      "Epoch 0 step 95: training accuarcy: 0.9772000000000001\n",
      "Epoch 0 step 95: training loss: 5847.7118776709285\n",
      "Epoch 0 step 96: training accuarcy: 0.9732000000000001\n",
      "Epoch 0 step 96: training loss: 5775.329463710406\n",
      "Epoch 0 step 97: training accuarcy: 0.9745\n",
      "Epoch 0 step 97: training loss: 5614.9177729564535\n",
      "Epoch 0 step 98: training accuarcy: 0.9806\n",
      "Epoch 0 step 98: training loss: 5633.57121793618\n",
      "Epoch 0 step 99: training accuarcy: 0.9728\n",
      "Epoch 0 step 99: training loss: 5540.982463589767\n",
      "Epoch 0 step 100: training accuarcy: 0.9762000000000001\n",
      "Epoch 0 step 100: training loss: 5435.732534662382\n",
      "Epoch 0 step 101: training accuarcy: 0.9811000000000001\n",
      "Epoch 0 step 101: training loss: 5372.06910012488\n",
      "Epoch 0 step 102: training accuarcy: 0.9809\n",
      "Epoch 0 step 102: training loss: 5383.64370447561\n",
      "Epoch 0 step 103: training accuarcy: 0.9714\n",
      "Epoch 0 step 103: training loss: 5256.06309815715\n",
      "Epoch 0 step 104: training accuarcy: 0.9756\n",
      "Epoch 0 step 104: training loss: 5188.328238497314\n",
      "Epoch 0 step 105: training accuarcy: 0.9777\n",
      "Epoch 0 step 105: training loss: 5102.182009835425\n",
      "Epoch 0 step 106: training accuarcy: 0.9812000000000001\n",
      "Epoch 0 step 106: training loss: 5001.069988832107\n",
      "Epoch 0 step 107: training accuarcy: 0.9841000000000001\n",
      "Epoch 0 step 107: training loss: 5025.377163797662\n",
      "Epoch 0 step 108: training accuarcy: 0.9767\n",
      "Epoch 0 step 108: training loss: 4933.676857219854\n",
      "Epoch 0 step 109: training accuarcy: 0.9796\n",
      "Epoch 0 step 109: training loss: 4887.736419090107\n",
      "Epoch 0 step 110: training accuarcy: 0.9780000000000001\n",
      "Epoch 0 step 110: training loss: 4817.331432497175\n",
      "Epoch 0 step 111: training accuarcy: 0.9814\n",
      "Epoch 0 step 111: training loss: 4756.09957185815\n",
      "Epoch 0 step 112: training accuarcy: 0.9830000000000001\n",
      "Epoch 0 step 112: training loss: 4624.585910760974\n",
      "Epoch 0 step 113: training accuarcy: 0.9857\n",
      "Epoch 0 step 113: training loss: 4547.303001172361\n",
      "Epoch 0 step 114: training accuarcy: 0.9888\n",
      "Epoch 0 step 114: training loss: 4630.402063804201\n",
      "Epoch 0 step 115: training accuarcy: 0.9783000000000001\n",
      "Epoch 0 step 115: training loss: 4511.174952655375\n",
      "Epoch 0 step 116: training accuarcy: 0.9826\n",
      "Epoch 0 step 116: training loss: 4494.662926032201\n",
      "Epoch 0 step 117: training accuarcy: 0.9816\n",
      "Epoch 0 step 117: training loss: 4505.873360218991\n",
      "Epoch 0 step 118: training accuarcy: 0.9788\n",
      "Epoch 0 step 118: training loss: 4380.222573111397\n",
      "Epoch 0 step 119: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 119: training loss: 4340.90247958169\n",
      "Epoch 0 step 120: training accuarcy: 0.9813000000000001\n",
      "Epoch 0 step 120: training loss: 4310.211258663809\n",
      "Epoch 0 step 121: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 121: training loss: 4212.357510685064\n",
      "Epoch 0 step 122: training accuarcy: 0.9834\n",
      "Epoch 0 step 122: training loss: 4273.310672992935\n",
      "Epoch 0 step 123: training accuarcy: 0.9777\n",
      "Epoch 0 step 123: training loss: 4158.980775446714\n",
      "Epoch 0 step 124: training accuarcy: 0.9826\n",
      "Epoch 0 step 124: training loss: 4069.474364180968\n",
      "Epoch 0 step 125: training accuarcy: 0.9861000000000001\n",
      "Epoch 0 step 125: training loss: 4032.709004971245\n",
      "Epoch 0 step 126: training accuarcy: 0.9864\n",
      "Epoch 0 step 126: training loss: 4068.9175125624083\n",
      "Epoch 0 step 127: training accuarcy: 0.9791000000000001\n",
      "Epoch 0 step 127: training loss: 4021.514799095072\n",
      "Epoch 0 step 128: training accuarcy: 0.9767\n",
      "Epoch 0 step 128: training loss: 3897.2253326967566\n",
      "Epoch 0 step 129: training accuarcy: 0.9870000000000001\n",
      "Epoch 0 step 129: training loss: 3934.829396123743\n",
      "Epoch 0 step 130: training accuarcy: 0.9807\n",
      "Epoch 0 step 130: training loss: 3871.6727901103254\n",
      "Epoch 0 step 131: training accuarcy: 0.9812000000000001\n",
      "Epoch 0 step 131: training loss: 3776.247236149556\n",
      "Epoch 0 step 132: training accuarcy: 0.9855\n",
      "Epoch 0 step 132: training loss: 3742.22935360402\n",
      "Epoch 0 step 133: training accuarcy: 0.9833000000000001\n",
      "Epoch 0 step 133: training loss: 3747.3275383411055\n",
      "Epoch 0 step 134: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 134: training loss: 3702.493218446947\n",
      "Epoch 0 step 135: training accuarcy: 0.9821000000000001\n",
      "Epoch 0 step 135: training loss: 3634.5809583590785\n",
      "Epoch 0 step 136: training accuarcy: 0.9848\n",
      "Epoch 0 step 136: training loss: 3577.2521160519977\n",
      "Epoch 0 step 137: training accuarcy: 0.9869\n",
      "Epoch 0 step 137: training loss: 3614.7615144124775\n",
      "Epoch 0 step 138: training accuarcy: 0.9828\n",
      "Epoch 0 step 138: training loss: 3516.497603060079\n",
      "Epoch 0 step 139: training accuarcy: 0.9879\n",
      "Epoch 0 step 139: training loss: 3486.2250470360614\n",
      "Epoch 0 step 140: training accuarcy: 0.9858\n",
      "Epoch 0 step 140: training loss: 3470.5107341025646\n",
      "Epoch 0 step 141: training accuarcy: 0.9846\n",
      "Epoch 0 step 141: training loss: 3408.3581450087686\n",
      "Epoch 0 step 142: training accuarcy: 0.9847\n",
      "Epoch 0 step 142: training loss: 3387.5121543324985\n",
      "Epoch 0 step 143: training accuarcy: 0.9882000000000001\n",
      "Epoch 0 step 143: training loss: 3368.030965761244\n",
      "Epoch 0 step 144: training accuarcy: 0.9861000000000001\n",
      "Epoch 0 step 144: training loss: 3317.2066941266335\n",
      "Epoch 0 step 145: training accuarcy: 0.9838\n",
      "Epoch 0 step 145: training loss: 3369.8291854380127\n",
      "Epoch 0 step 146: training accuarcy: 0.9804\n",
      "Epoch 0 step 146: training loss: 3257.679084437724\n",
      "Epoch 0 step 147: training accuarcy: 0.9858\n",
      "Epoch 0 step 147: training loss: 3237.699959532336\n",
      "Epoch 0 step 148: training accuarcy: 0.9860000000000001\n",
      "Epoch 0 step 148: training loss: 3163.171903962855\n",
      "Epoch 0 step 149: training accuarcy: 0.9854\n",
      "Epoch 0 step 149: training loss: 3136.522401951986\n",
      "Epoch 0 step 150: training accuarcy: 0.9866\n",
      "Epoch 0 step 150: training loss: 3080.362407065442\n",
      "Epoch 0 step 151: training accuarcy: 0.9874\n",
      "Epoch 0 step 151: training loss: 3044.605643573601\n",
      "Epoch 0 step 152: training accuarcy: 0.9869\n",
      "Epoch 0 step 152: training loss: 3040.24860202671\n",
      "Epoch 0 step 153: training accuarcy: 0.9883000000000001\n",
      "Epoch 0 step 153: training loss: 3030.757933014829\n",
      "Epoch 0 step 154: training accuarcy: 0.9866\n",
      "Epoch 0 step 154: training loss: 2996.7602216914643\n",
      "Epoch 0 step 155: training accuarcy: 0.9871000000000001\n",
      "Epoch 0 step 155: training loss: 2996.6829223189598\n",
      "Epoch 0 step 156: training accuarcy: 0.9867\n",
      "Epoch 0 step 156: training loss: 2953.0852356220576\n",
      "Epoch 0 step 157: training accuarcy: 0.9863000000000001\n",
      "Epoch 0 step 157: training loss: 2913.206685985385\n",
      "Epoch 0 step 158: training accuarcy: 0.9853000000000001\n",
      "Epoch 0 step 158: training loss: 2893.467817648129\n",
      "Epoch 0 step 159: training accuarcy: 0.9851000000000001\n",
      "Epoch 0 step 159: training loss: 2840.0471484972936\n",
      "Epoch 0 step 160: training accuarcy: 0.9865\n",
      "Epoch 0 step 160: training loss: 2858.0709469488747\n",
      "Epoch 0 step 161: training accuarcy: 0.9849\n",
      "Epoch 0 step 161: training loss: 2754.3068372058665\n",
      "Epoch 0 step 162: training accuarcy: 0.9890000000000001\n",
      "Epoch 0 step 162: training loss: 2700.817522507656\n",
      "Epoch 0 step 163: training accuarcy: 0.9912000000000001\n",
      "Epoch 0 step 163: training loss: 2742.381666573158\n",
      "Epoch 0 step 164: training accuarcy: 0.9862000000000001\n",
      "Epoch 0 step 164: training loss: 2759.401268134354\n",
      "Epoch 0 step 165: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 165: training loss: 2721.381621159014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 166: training accuarcy: 0.9862000000000001\n",
      "Epoch 0 step 166: training loss: 2696.7118351185077\n",
      "Epoch 0 step 167: training accuarcy: 0.9857\n",
      "Epoch 0 step 167: training loss: 2681.0778855298404\n",
      "Epoch 0 step 168: training accuarcy: 0.9868\n",
      "Epoch 0 step 168: training loss: 2626.978286697358\n",
      "Epoch 0 step 169: training accuarcy: 0.9872000000000001\n",
      "Epoch 0 step 169: training loss: 2614.225238317762\n",
      "Epoch 0 step 170: training accuarcy: 0.9849\n",
      "Epoch 0 step 170: training loss: 2572.3588066466223\n",
      "Epoch 0 step 171: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 171: training loss: 2562.2526673500697\n",
      "Epoch 0 step 172: training accuarcy: 0.9870000000000001\n",
      "Epoch 0 step 172: training loss: 2502.9905367897736\n",
      "Epoch 0 step 173: training accuarcy: 0.9887\n",
      "Epoch 0 step 173: training loss: 2456.2448976557566\n",
      "Epoch 0 step 174: training accuarcy: 0.9894000000000001\n",
      "Epoch 0 step 174: training loss: 2546.7299974681673\n",
      "Epoch 0 step 175: training accuarcy: 0.9840000000000001\n",
      "Epoch 0 step 175: training loss: 2446.250919528472\n",
      "Epoch 0 step 176: training accuarcy: 0.9862000000000001\n",
      "Epoch 0 step 176: training loss: 2417.4392507389334\n",
      "Epoch 0 step 177: training accuarcy: 0.9887\n",
      "Epoch 0 step 177: training loss: 2379.888968613569\n",
      "Epoch 0 step 178: training accuarcy: 0.9885\n",
      "Epoch 0 step 178: training loss: 2395.066967219495\n",
      "Epoch 0 step 179: training accuarcy: 0.9890000000000001\n",
      "Epoch 0 step 179: training loss: 2403.815887845468\n",
      "Epoch 0 step 180: training accuarcy: 0.9844\n",
      "Epoch 0 step 180: training loss: 2298.8957178466585\n",
      "Epoch 0 step 181: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 181: training loss: 2306.3661220142376\n",
      "Epoch 0 step 182: training accuarcy: 0.9899\n",
      "Epoch 0 step 182: training loss: 2330.535158785832\n",
      "Epoch 0 step 183: training accuarcy: 0.9852000000000001\n",
      "Epoch 0 step 183: training loss: 2263.0311468628606\n",
      "Epoch 0 step 184: training accuarcy: 0.9901000000000001\n",
      "Epoch 0 step 184: training loss: 2224.7376392453198\n",
      "Epoch 0 step 185: training accuarcy: 0.9935\n",
      "Epoch 0 step 185: training loss: 2217.062941420424\n",
      "Epoch 0 step 186: training accuarcy: 0.9894000000000001\n",
      "Epoch 0 step 186: training loss: 2186.2503864825867\n",
      "Epoch 0 step 187: training accuarcy: 0.9896\n",
      "Epoch 0 step 187: training loss: 2181.0959518249674\n",
      "Epoch 0 step 188: training accuarcy: 0.9889\n",
      "Epoch 0 step 188: training loss: 2176.1340617859287\n",
      "Epoch 0 step 189: training accuarcy: 0.9893000000000001\n",
      "Epoch 0 step 189: training loss: 2225.3244411126116\n",
      "Epoch 0 step 190: training accuarcy: 0.9846\n",
      "Epoch 0 step 190: training loss: 2141.478094766137\n",
      "Epoch 0 step 191: training accuarcy: 0.9894000000000001\n",
      "Epoch 0 step 191: training loss: 2081.4668135512243\n",
      "Epoch 0 step 192: training accuarcy: 0.9918\n",
      "Epoch 0 step 192: training loss: 2057.6409301536446\n",
      "Epoch 0 step 193: training accuarcy: 0.9898\n",
      "Epoch 0 step 193: training loss: 2064.249503288501\n",
      "Epoch 0 step 194: training accuarcy: 0.9893000000000001\n",
      "Epoch 0 step 194: training loss: 1975.0453244800215\n",
      "Epoch 0 step 195: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 195: training loss: 2049.4297634784016\n",
      "Epoch 0 step 196: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 196: training loss: 2009.3606467826532\n",
      "Epoch 0 step 197: training accuarcy: 0.9899\n",
      "Epoch 0 step 197: training loss: 2003.8950953143947\n",
      "Epoch 0 step 198: training accuarcy: 0.9905\n",
      "Epoch 0 step 198: training loss: 2005.1547377373786\n",
      "Epoch 0 step 199: training accuarcy: 0.9870000000000001\n",
      "Epoch 0 step 199: training loss: 1970.0508469378685\n",
      "Epoch 0 step 200: training accuarcy: 0.9909\n",
      "Epoch 0 step 200: training loss: 1916.9482702173018\n",
      "Epoch 0 step 201: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 201: training loss: 1943.0323041380998\n",
      "Epoch 0 step 202: training accuarcy: 0.9878\n",
      "Epoch 0 step 202: training loss: 1953.074549548568\n",
      "Epoch 0 step 203: training accuarcy: 0.9896\n",
      "Epoch 0 step 203: training loss: 1926.129135107347\n",
      "Epoch 0 step 204: training accuarcy: 0.9886\n",
      "Epoch 0 step 204: training loss: 1915.289626201195\n",
      "Epoch 0 step 205: training accuarcy: 0.9845\n",
      "Epoch 0 step 205: training loss: 1864.1003447325543\n",
      "Epoch 0 step 206: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 206: training loss: 1860.302959444375\n",
      "Epoch 0 step 207: training accuarcy: 0.9892000000000001\n",
      "Epoch 0 step 207: training loss: 1881.5090925206052\n",
      "Epoch 0 step 208: training accuarcy: 0.9875\n",
      "Epoch 0 step 208: training loss: 1812.192555939457\n",
      "Epoch 0 step 209: training accuarcy: 0.9899\n",
      "Epoch 0 step 209: training loss: 1796.8013236150189\n",
      "Epoch 0 step 210: training accuarcy: 0.993\n",
      "Epoch 0 step 210: training loss: 1779.9852605129802\n",
      "Epoch 0 step 211: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 211: training loss: 1787.3225024390888\n",
      "Epoch 0 step 212: training accuarcy: 0.9884000000000001\n",
      "Epoch 0 step 212: training loss: 1741.5176462584536\n",
      "Epoch 0 step 213: training accuarcy: 0.9902000000000001\n",
      "Epoch 0 step 213: training loss: 1721.9866731163413\n",
      "Epoch 0 step 214: training accuarcy: 0.9918\n",
      "Epoch 0 step 214: training loss: 1712.6917498373055\n",
      "Epoch 0 step 215: training accuarcy: 0.9923000000000001\n",
      "Epoch 0 step 215: training loss: 1679.0101311076692\n",
      "Epoch 0 step 216: training accuarcy: 0.9917\n",
      "Epoch 0 step 216: training loss: 1675.4583372385694\n",
      "Epoch 0 step 217: training accuarcy: 0.9899\n",
      "Epoch 0 step 217: training loss: 1718.1300047546674\n",
      "Epoch 0 step 218: training accuarcy: 0.9889\n",
      "Epoch 0 step 218: training loss: 1658.091293695234\n",
      "Epoch 0 step 219: training accuarcy: 0.9901000000000001\n",
      "Epoch 0 step 219: training loss: 1641.7008259206032\n",
      "Epoch 0 step 220: training accuarcy: 0.9925\n",
      "Epoch 0 step 220: training loss: 1621.5263222894866\n",
      "Epoch 0 step 221: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 221: training loss: 1623.1098969268685\n",
      "Epoch 0 step 222: training accuarcy: 0.9904000000000001\n",
      "Epoch 0 step 222: training loss: 1567.7158222942608\n",
      "Epoch 0 step 223: training accuarcy: 0.9926\n",
      "Epoch 0 step 223: training loss: 1603.0899142155113\n",
      "Epoch 0 step 224: training accuarcy: 0.9889\n",
      "Epoch 0 step 224: training loss: 1596.094053113762\n",
      "Epoch 0 step 225: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 225: training loss: 1583.5453593512916\n",
      "Epoch 0 step 226: training accuarcy: 0.9893000000000001\n",
      "Epoch 0 step 226: training loss: 1557.5326706955984\n",
      "Epoch 0 step 227: training accuarcy: 0.9911000000000001\n",
      "Epoch 0 step 227: training loss: 1528.2863355406867\n",
      "Epoch 0 step 228: training accuarcy: 0.9928\n",
      "Epoch 0 step 228: training loss: 1557.586241784738\n",
      "Epoch 0 step 229: training accuarcy: 0.9900000000000001\n",
      "Epoch 0 step 229: training loss: 1497.6474161888839\n",
      "Epoch 0 step 230: training accuarcy: 0.9915\n",
      "Epoch 0 step 230: training loss: 1499.9791637595413\n",
      "Epoch 0 step 231: training accuarcy: 0.9924000000000001\n",
      "Epoch 0 step 231: training loss: 1541.292554034927\n",
      "Epoch 0 step 232: training accuarcy: 0.9881000000000001\n",
      "Epoch 0 step 232: training loss: 1443.6259035933963\n",
      "Epoch 0 step 233: training accuarcy: 0.9941000000000001\n",
      "Epoch 0 step 233: training loss: 1462.7369213467791\n",
      "Epoch 0 step 234: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 234: training loss: 1442.406035412787\n",
      "Epoch 0 step 235: training accuarcy: 0.9912000000000001\n",
      "Epoch 0 step 235: training loss: 1460.7337371363137\n",
      "Epoch 0 step 236: training accuarcy: 0.9880000000000001\n",
      "Epoch 0 step 236: training loss: 1452.5161493224402\n",
      "Epoch 0 step 237: training accuarcy: 0.9895\n",
      "Epoch 0 step 237: training loss: 1444.3908702270548\n",
      "Epoch 0 step 238: training accuarcy: 0.9891000000000001\n",
      "Epoch 0 step 238: training loss: 1412.7633974061278\n",
      "Epoch 0 step 239: training accuarcy: 0.9925\n",
      "Epoch 0 step 239: training loss: 1423.4700414432675\n",
      "Epoch 0 step 240: training accuarcy: 0.9889\n",
      "Epoch 0 step 240: training loss: 1377.2120521737422\n",
      "Epoch 0 step 241: training accuarcy: 0.9927\n",
      "Epoch 0 step 241: training loss: 1371.2294890755677\n",
      "Epoch 0 step 242: training accuarcy: 0.9932000000000001\n",
      "Epoch 0 step 242: training loss: 1346.5086628707631\n",
      "Epoch 0 step 243: training accuarcy: 0.9919\n",
      "Epoch 0 step 243: training loss: 1345.4425230710006\n",
      "Epoch 0 step 244: training accuarcy: 0.9917\n",
      "Epoch 0 step 244: training loss: 1357.8805676183938\n",
      "Epoch 0 step 245: training accuarcy: 0.9912000000000001\n",
      "Epoch 0 step 245: training loss: 1360.0220524423014\n",
      "Epoch 0 step 246: training accuarcy: 0.9890000000000001\n",
      "Epoch 0 step 246: training loss: 1299.316307948798\n",
      "Epoch 0 step 247: training accuarcy: 0.9935\n",
      "Epoch 0 step 247: training loss: 1306.382292371756\n",
      "Epoch 0 step 248: training accuarcy: 0.9931000000000001\n",
      "Epoch 0 step 248: training loss: 1297.280236087457\n",
      "Epoch 0 step 249: training accuarcy: 0.9924000000000001\n",
      "Epoch 0 step 249: training loss: 1317.9026961631646\n",
      "Epoch 0 step 250: training accuarcy: 0.9892000000000001\n",
      "Epoch 0 step 250: training loss: 1303.8867386063735\n",
      "Epoch 0 step 251: training accuarcy: 0.9895\n",
      "Epoch 0 step 251: training loss: 1272.722001783706\n",
      "Epoch 0 step 252: training accuarcy: 0.9934000000000001\n",
      "Epoch 0 step 252: training loss: 1238.0084262240712\n",
      "Epoch 0 step 253: training accuarcy: 0.994\n",
      "Epoch 0 step 253: training loss: 1257.2068688782333\n",
      "Epoch 0 step 254: training accuarcy: 0.9914000000000001\n",
      "Epoch 0 step 254: training loss: 1250.4342858559987\n",
      "Epoch 0 step 255: training accuarcy: 0.9927\n",
      "Epoch 0 step 255: training loss: 1207.3746565724925\n",
      "Epoch 0 step 256: training accuarcy: 0.9941000000000001\n",
      "Epoch 0 step 256: training loss: 1200.0420141717118\n",
      "Epoch 0 step 257: training accuarcy: 0.9921000000000001\n",
      "Epoch 0 step 257: training loss: 1219.2398452935556\n",
      "Epoch 0 step 258: training accuarcy: 0.9913000000000001\n",
      "Epoch 0 step 258: training loss: 1233.5203263910478\n",
      "Epoch 0 step 259: training accuarcy: 0.9903000000000001\n",
      "Epoch 0 step 259: training loss: 1215.3067919145192\n",
      "Epoch 0 step 260: training accuarcy: 0.9916\n",
      "Epoch 0 step 260: training loss: 1197.2055246704967\n",
      "Epoch 0 step 261: training accuarcy: 0.9919\n",
      "Epoch 0 step 261: training loss: 1155.3120774584265\n",
      "Epoch 0 step 262: training accuarcy: 0.9941000000000001\n",
      "Epoch 0 step 262: training loss: 1064.7214108383223\n",
      "Epoch 0 step 263: training accuarcy: 0.9933333333333333\n",
      "Epoch 0: train loss 7588.774127760777, train accuarcy 0.942011296749115\n",
      "Epoch 0: valid loss 1541.5706435176821, valid accuarcy 0.9870830178260803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████████▍                                                                                                                                         | 1/5 [04:56<19:45, 296.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 1141.8540969200897\n",
      "Epoch 1 step 264: training accuarcy: 0.9913000000000001\n",
      "Epoch 1 step 264: training loss: 1119.3933070159264\n",
      "Epoch 1 step 265: training accuarcy: 0.995\n",
      "Epoch 1 step 265: training loss: 1100.372395073889\n",
      "Epoch 1 step 266: training accuarcy: 0.9957\n",
      "Epoch 1 step 266: training loss: 1091.5917523604219\n",
      "Epoch 1 step 267: training accuarcy: 0.995\n",
      "Epoch 1 step 267: training loss: 1053.180513373251\n",
      "Epoch 1 step 268: training accuarcy: 0.9955\n",
      "Epoch 1 step 268: training loss: 1078.1888165328292\n",
      "Epoch 1 step 269: training accuarcy: 0.9936\n",
      "Epoch 1 step 269: training loss: 1075.3834084892223\n",
      "Epoch 1 step 270: training accuarcy: 0.9936\n",
      "Epoch 1 step 270: training loss: 1027.8584271406962\n",
      "Epoch 1 step 271: training accuarcy: 0.9966\n",
      "Epoch 1 step 271: training loss: 1046.4623134771666\n",
      "Epoch 1 step 272: training accuarcy: 0.9955\n",
      "Epoch 1 step 272: training loss: 1028.8387499158332\n",
      "Epoch 1 step 273: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 273: training loss: 1031.2907730820255\n",
      "Epoch 1 step 274: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 274: training loss: 1030.7934998697203\n",
      "Epoch 1 step 275: training accuarcy: 0.9946\n",
      "Epoch 1 step 275: training loss: 999.572254856207\n",
      "Epoch 1 step 276: training accuarcy: 0.9955\n",
      "Epoch 1 step 276: training loss: 1038.0148568033374\n",
      "Epoch 1 step 277: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 277: training loss: 1006.7016423513958\n",
      "Epoch 1 step 278: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 278: training loss: 964.9087828717451\n",
      "Epoch 1 step 279: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 279: training loss: 997.5648413466465\n",
      "Epoch 1 step 280: training accuarcy: 0.995\n",
      "Epoch 1 step 280: training loss: 977.3380236985763\n",
      "Epoch 1 step 281: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 281: training loss: 989.8550228780382\n",
      "Epoch 1 step 282: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 282: training loss: 977.9734003668892\n",
      "Epoch 1 step 283: training accuarcy: 0.9937\n",
      "Epoch 1 step 283: training loss: 929.6690847040342\n",
      "Epoch 1 step 284: training accuarcy: 0.997\n",
      "Epoch 1 step 284: training loss: 951.4429066391147\n",
      "Epoch 1 step 285: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 285: training loss: 939.2104839247002\n",
      "Epoch 1 step 286: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 286: training loss: 946.6793850745621\n",
      "Epoch 1 step 287: training accuarcy: 0.9949\n",
      "Epoch 1 step 287: training loss: 925.7881074865579\n",
      "Epoch 1 step 288: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 288: training loss: 920.7339944524034\n",
      "Epoch 1 step 289: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 289: training loss: 909.7788890561646\n",
      "Epoch 1 step 290: training accuarcy: 0.9977\n",
      "Epoch 1 step 290: training loss: 882.828168141316\n",
      "Epoch 1 step 291: training accuarcy: 0.9967\n",
      "Epoch 1 step 291: training loss: 915.3541116928759\n",
      "Epoch 1 step 292: training accuarcy: 0.9947\n",
      "Epoch 1 step 292: training loss: 910.473462104249\n",
      "Epoch 1 step 293: training accuarcy: 0.996\n",
      "Epoch 1 step 293: training loss: 919.4437959090785\n",
      "Epoch 1 step 294: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 294: training loss: 906.959069044718\n",
      "Epoch 1 step 295: training accuarcy: 0.9945\n",
      "Epoch 1 step 295: training loss: 888.4814519045469\n",
      "Epoch 1 step 296: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 296: training loss: 853.0178556075159\n",
      "Epoch 1 step 297: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 297: training loss: 887.488244109936\n",
      "Epoch 1 step 298: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 298: training loss: 862.8841872824332\n",
      "Epoch 1 step 299: training accuarcy: 0.996\n",
      "Epoch 1 step 299: training loss: 851.340266182408\n",
      "Epoch 1 step 300: training accuarcy: 0.996\n",
      "Epoch 1 step 300: training loss: 843.150227060905\n",
      "Epoch 1 step 301: training accuarcy: 0.9948\n",
      "Epoch 1 step 301: training loss: 838.9449130164927\n",
      "Epoch 1 step 302: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 302: training loss: 849.4182359218621\n",
      "Epoch 1 step 303: training accuarcy: 0.9955\n",
      "Epoch 1 step 303: training loss: 844.2450362236561\n",
      "Epoch 1 step 304: training accuarcy: 0.9956\n",
      "Epoch 1 step 304: training loss: 819.0923855789015\n",
      "Epoch 1 step 305: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 305: training loss: 872.9130145069477\n",
      "Epoch 1 step 306: training accuarcy: 0.994\n",
      "Epoch 1 step 306: training loss: 860.4435258873254\n",
      "Epoch 1 step 307: training accuarcy: 0.9939\n",
      "Epoch 1 step 307: training loss: 849.8864812523889\n",
      "Epoch 1 step 308: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 308: training loss: 845.8738228553676\n",
      "Epoch 1 step 309: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 309: training loss: 805.3308452672231\n",
      "Epoch 1 step 310: training accuarcy: 0.9976\n",
      "Epoch 1 step 310: training loss: 826.5595357748984\n",
      "Epoch 1 step 311: training accuarcy: 0.9945\n",
      "Epoch 1 step 311: training loss: 823.7916550409263\n",
      "Epoch 1 step 312: training accuarcy: 0.9931000000000001\n",
      "Epoch 1 step 312: training loss: 857.5012412636297\n",
      "Epoch 1 step 313: training accuarcy: 0.9925\n",
      "Epoch 1 step 313: training loss: 828.9584189178823\n",
      "Epoch 1 step 314: training accuarcy: 0.9929\n",
      "Epoch 1 step 314: training loss: 803.9898601109333\n",
      "Epoch 1 step 315: training accuarcy: 0.9945\n",
      "Epoch 1 step 315: training loss: 791.817758538654\n",
      "Epoch 1 step 316: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 316: training loss: 769.812205357412\n",
      "Epoch 1 step 317: training accuarcy: 0.9958\n",
      "Epoch 1 step 317: training loss: 782.7129571521039\n",
      "Epoch 1 step 318: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 318: training loss: 772.5075862098603\n",
      "Epoch 1 step 319: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 319: training loss: 778.9520033726551\n",
      "Epoch 1 step 320: training accuarcy: 0.995\n",
      "Epoch 1 step 320: training loss: 804.6366967664569\n",
      "Epoch 1 step 321: training accuarcy: 0.9919\n",
      "Epoch 1 step 321: training loss: 780.4553606430212\n",
      "Epoch 1 step 322: training accuarcy: 0.9948\n",
      "Epoch 1 step 322: training loss: 771.7453731109072\n",
      "Epoch 1 step 323: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 323: training loss: 774.1553000795341\n",
      "Epoch 1 step 324: training accuarcy: 0.9948\n",
      "Epoch 1 step 324: training loss: 763.8615039295962\n",
      "Epoch 1 step 325: training accuarcy: 0.9938\n",
      "Epoch 1 step 325: training loss: 714.2365685790187\n",
      "Epoch 1 step 326: training accuarcy: 0.9978\n",
      "Epoch 1 step 326: training loss: 766.3116019377119\n",
      "Epoch 1 step 327: training accuarcy: 0.9948\n",
      "Epoch 1 step 327: training loss: 720.1209616239474\n",
      "Epoch 1 step 328: training accuarcy: 0.9957\n",
      "Epoch 1 step 328: training loss: 720.9535450457668\n",
      "Epoch 1 step 329: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 329: training loss: 740.4600060987863\n",
      "Epoch 1 step 330: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 330: training loss: 708.6014037625894\n",
      "Epoch 1 step 331: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 331: training loss: 727.8590152121931\n",
      "Epoch 1 step 332: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 332: training loss: 722.9378067085224\n",
      "Epoch 1 step 333: training accuarcy: 0.9956\n",
      "Epoch 1 step 333: training loss: 709.0735065785293\n",
      "Epoch 1 step 334: training accuarcy: 0.9958\n",
      "Epoch 1 step 334: training loss: 706.419156308354\n",
      "Epoch 1 step 335: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 335: training loss: 733.351458787662\n",
      "Epoch 1 step 336: training accuarcy: 0.9926\n",
      "Epoch 1 step 336: training loss: 717.3641267230178\n",
      "Epoch 1 step 337: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 337: training loss: 727.3740092526369\n",
      "Epoch 1 step 338: training accuarcy: 0.9955\n",
      "Epoch 1 step 338: training loss: 668.9260129236533\n",
      "Epoch 1 step 339: training accuarcy: 0.9959\n",
      "Epoch 1 step 339: training loss: 705.4940512079634\n",
      "Epoch 1 step 340: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 340: training loss: 667.5488895763041\n",
      "Epoch 1 step 341: training accuarcy: 0.996\n",
      "Epoch 1 step 341: training loss: 754.6366288541248\n",
      "Epoch 1 step 342: training accuarcy: 0.993\n",
      "Epoch 1 step 342: training loss: 669.9458578027321\n",
      "Epoch 1 step 343: training accuarcy: 0.9977\n",
      "Epoch 1 step 343: training loss: 706.4281923478466\n",
      "Epoch 1 step 344: training accuarcy: 0.9936\n",
      "Epoch 1 step 344: training loss: 651.8229565575164\n",
      "Epoch 1 step 345: training accuarcy: 0.9956\n",
      "Epoch 1 step 345: training loss: 667.7384451014498\n",
      "Epoch 1 step 346: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 346: training loss: 668.925549671285\n",
      "Epoch 1 step 347: training accuarcy: 0.996\n",
      "Epoch 1 step 347: training loss: 694.9472886853931\n",
      "Epoch 1 step 348: training accuarcy: 0.995\n",
      "Epoch 1 step 348: training loss: 651.7745569783302\n",
      "Epoch 1 step 349: training accuarcy: 0.9955\n",
      "Epoch 1 step 349: training loss: 712.7055659411735\n",
      "Epoch 1 step 350: training accuarcy: 0.992\n",
      "Epoch 1 step 350: training loss: 690.7257632154144\n",
      "Epoch 1 step 351: training accuarcy: 0.9955\n",
      "Epoch 1 step 351: training loss: 655.5577172238508\n",
      "Epoch 1 step 352: training accuarcy: 0.9949\n",
      "Epoch 1 step 352: training loss: 635.6901486220396\n",
      "Epoch 1 step 353: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 353: training loss: 657.709630081275\n",
      "Epoch 1 step 354: training accuarcy: 0.994\n",
      "Epoch 1 step 354: training loss: 695.7517271916912\n",
      "Epoch 1 step 355: training accuarcy: 0.9926\n",
      "Epoch 1 step 355: training loss: 667.6055843756192\n",
      "Epoch 1 step 356: training accuarcy: 0.9937\n",
      "Epoch 1 step 356: training loss: 616.1819211805923\n",
      "Epoch 1 step 357: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 357: training loss: 629.5965841198239\n",
      "Epoch 1 step 358: training accuarcy: 0.9956\n",
      "Epoch 1 step 358: training loss: 641.9806790896936\n",
      "Epoch 1 step 359: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 359: training loss: 630.3832346574939\n",
      "Epoch 1 step 360: training accuarcy: 0.9968\n",
      "Epoch 1 step 360: training loss: 645.3348530469721\n",
      "Epoch 1 step 361: training accuarcy: 0.9947\n",
      "Epoch 1 step 361: training loss: 641.606781312468\n",
      "Epoch 1 step 362: training accuarcy: 0.9949\n",
      "Epoch 1 step 362: training loss: 628.634386814967\n",
      "Epoch 1 step 363: training accuarcy: 0.9937\n",
      "Epoch 1 step 363: training loss: 647.6135522874164\n",
      "Epoch 1 step 364: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 364: training loss: 651.1458391078343\n",
      "Epoch 1 step 365: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 365: training loss: 624.7879553727598\n",
      "Epoch 1 step 366: training accuarcy: 0.9957\n",
      "Epoch 1 step 366: training loss: 592.1564282727539\n",
      "Epoch 1 step 367: training accuarcy: 0.9982000000000001\n",
      "Epoch 1 step 367: training loss: 616.3829093356981\n",
      "Epoch 1 step 368: training accuarcy: 0.9965\n",
      "Epoch 1 step 368: training loss: 618.714284615713\n",
      "Epoch 1 step 369: training accuarcy: 0.9968\n",
      "Epoch 1 step 369: training loss: 599.5811229316146\n",
      "Epoch 1 step 370: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 370: training loss: 679.6775353743474\n",
      "Epoch 1 step 371: training accuarcy: 0.9922000000000001\n",
      "Epoch 1 step 371: training loss: 602.497609058744\n",
      "Epoch 1 step 372: training accuarcy: 0.9957\n",
      "Epoch 1 step 372: training loss: 584.4807945613352\n",
      "Epoch 1 step 373: training accuarcy: 0.9967\n",
      "Epoch 1 step 373: training loss: 595.4670136879513\n",
      "Epoch 1 step 374: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 374: training loss: 583.1051682855029\n",
      "Epoch 1 step 375: training accuarcy: 0.9968\n",
      "Epoch 1 step 375: training loss: 610.7595973342144\n",
      "Epoch 1 step 376: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 376: training loss: 621.6511852039318\n",
      "Epoch 1 step 377: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 377: training loss: 616.7121495203905\n",
      "Epoch 1 step 378: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 378: training loss: 599.7542341935605\n",
      "Epoch 1 step 379: training accuarcy: 0.9946\n",
      "Epoch 1 step 379: training loss: 599.541630009884\n",
      "Epoch 1 step 380: training accuarcy: 0.9955\n",
      "Epoch 1 step 380: training loss: 617.7029454091708\n",
      "Epoch 1 step 381: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 381: training loss: 589.1483998642248\n",
      "Epoch 1 step 382: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 382: training loss: 619.3888409607\n",
      "Epoch 1 step 383: training accuarcy: 0.9937\n",
      "Epoch 1 step 383: training loss: 622.7175346646333\n",
      "Epoch 1 step 384: training accuarcy: 0.9933000000000001\n",
      "Epoch 1 step 384: training loss: 592.6579235738227\n",
      "Epoch 1 step 385: training accuarcy: 0.9955\n",
      "Epoch 1 step 385: training loss: 573.7641887990625\n",
      "Epoch 1 step 386: training accuarcy: 0.9959\n",
      "Epoch 1 step 386: training loss: 545.5840475922041\n",
      "Epoch 1 step 387: training accuarcy: 0.9978\n",
      "Epoch 1 step 387: training loss: 619.4360633424777\n",
      "Epoch 1 step 388: training accuarcy: 0.9906\n",
      "Epoch 1 step 388: training loss: 580.2825698618292\n",
      "Epoch 1 step 389: training accuarcy: 0.9945\n",
      "Epoch 1 step 389: training loss: 586.7252229883876\n",
      "Epoch 1 step 390: training accuarcy: 0.9937\n",
      "Epoch 1 step 390: training loss: 595.1130807627646\n",
      "Epoch 1 step 391: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 391: training loss: 539.4743275131603\n",
      "Epoch 1 step 392: training accuarcy: 0.998\n",
      "Epoch 1 step 392: training loss: 512.5580005204263\n",
      "Epoch 1 step 393: training accuarcy: 0.9976\n",
      "Epoch 1 step 393: training loss: 568.3956336411952\n",
      "Epoch 1 step 394: training accuarcy: 0.9936\n",
      "Epoch 1 step 394: training loss: 566.4877310539696\n",
      "Epoch 1 step 395: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 395: training loss: 550.0467858526949\n",
      "Epoch 1 step 396: training accuarcy: 0.995\n",
      "Epoch 1 step 396: training loss: 548.9915979962209\n",
      "Epoch 1 step 397: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 397: training loss: 528.9474451809597\n",
      "Epoch 1 step 398: training accuarcy: 0.9969\n",
      "Epoch 1 step 398: training loss: 529.5244174072932\n",
      "Epoch 1 step 399: training accuarcy: 0.9971000000000001\n",
      "Epoch 1 step 399: training loss: 584.966418004688\n",
      "Epoch 1 step 400: training accuarcy: 0.9928\n",
      "Epoch 1 step 400: training loss: 576.8214502334914\n",
      "Epoch 1 step 401: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 401: training loss: 539.7327019697296\n",
      "Epoch 1 step 402: training accuarcy: 0.9944000000000001\n",
      "Epoch 1 step 402: training loss: 561.9549524337348\n",
      "Epoch 1 step 403: training accuarcy: 0.9946\n",
      "Epoch 1 step 403: training loss: 557.2449175185666\n",
      "Epoch 1 step 404: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 404: training loss: 541.6929702402024\n",
      "Epoch 1 step 405: training accuarcy: 0.9966\n",
      "Epoch 1 step 405: training loss: 558.6442070266561\n",
      "Epoch 1 step 406: training accuarcy: 0.994\n",
      "Epoch 1 step 406: training loss: 557.6917092993017\n",
      "Epoch 1 step 407: training accuarcy: 0.9943000000000001\n",
      "Epoch 1 step 407: training loss: 534.3993165435072\n",
      "Epoch 1 step 408: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 408: training loss: 529.7956832494089\n",
      "Epoch 1 step 409: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 409: training loss: 562.8359852656044\n",
      "Epoch 1 step 410: training accuarcy: 0.9945\n",
      "Epoch 1 step 410: training loss: 540.2349964463953\n",
      "Epoch 1 step 411: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 411: training loss: 543.8685513332017\n",
      "Epoch 1 step 412: training accuarcy: 0.9946\n",
      "Epoch 1 step 412: training loss: 535.3000306921547\n",
      "Epoch 1 step 413: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 413: training loss: 534.0050679008648\n",
      "Epoch 1 step 414: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 414: training loss: 553.1099678152905\n",
      "Epoch 1 step 415: training accuarcy: 0.9948\n",
      "Epoch 1 step 415: training loss: 515.7013627822898\n",
      "Epoch 1 step 416: training accuarcy: 0.9946\n",
      "Epoch 1 step 416: training loss: 510.49603384839247\n",
      "Epoch 1 step 417: training accuarcy: 0.9966\n",
      "Epoch 1 step 417: training loss: 511.6905968120963\n",
      "Epoch 1 step 418: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 418: training loss: 505.4809991986483\n",
      "Epoch 1 step 419: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 419: training loss: 576.2570176794898\n",
      "Epoch 1 step 420: training accuarcy: 0.994\n",
      "Epoch 1 step 420: training loss: 498.7614944586058\n",
      "Epoch 1 step 421: training accuarcy: 0.997\n",
      "Epoch 1 step 421: training loss: 535.5090485578606\n",
      "Epoch 1 step 422: training accuarcy: 0.9956\n",
      "Epoch 1 step 422: training loss: 518.5730091879955\n",
      "Epoch 1 step 423: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 423: training loss: 490.2746726092519\n",
      "Epoch 1 step 424: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 424: training loss: 499.8794313127031\n",
      "Epoch 1 step 425: training accuarcy: 0.9959\n",
      "Epoch 1 step 425: training loss: 507.3600808489392\n",
      "Epoch 1 step 426: training accuarcy: 0.9947\n",
      "Epoch 1 step 426: training loss: 482.5441341402169\n",
      "Epoch 1 step 427: training accuarcy: 0.9959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 427: training loss: 481.791156457174\n",
      "Epoch 1 step 428: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 428: training loss: 493.6798024119568\n",
      "Epoch 1 step 429: training accuarcy: 0.9966\n",
      "Epoch 1 step 429: training loss: 496.97522602462345\n",
      "Epoch 1 step 430: training accuarcy: 0.9955\n",
      "Epoch 1 step 430: training loss: 526.967835088629\n",
      "Epoch 1 step 431: training accuarcy: 0.9927\n",
      "Epoch 1 step 431: training loss: 482.9606008390582\n",
      "Epoch 1 step 432: training accuarcy: 0.9969\n",
      "Epoch 1 step 432: training loss: 496.5840363391192\n",
      "Epoch 1 step 433: training accuarcy: 0.9956\n",
      "Epoch 1 step 433: training loss: 501.66837869207257\n",
      "Epoch 1 step 434: training accuarcy: 0.9955\n",
      "Epoch 1 step 434: training loss: 558.4313912829242\n",
      "Epoch 1 step 435: training accuarcy: 0.991\n",
      "Epoch 1 step 435: training loss: 498.7550573230224\n",
      "Epoch 1 step 436: training accuarcy: 0.9966\n",
      "Epoch 1 step 436: training loss: 484.0770827239389\n",
      "Epoch 1 step 437: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 437: training loss: 513.1064759525857\n",
      "Epoch 1 step 438: training accuarcy: 0.9938\n",
      "Epoch 1 step 438: training loss: 477.02155064826934\n",
      "Epoch 1 step 439: training accuarcy: 0.9967\n",
      "Epoch 1 step 439: training loss: 494.20547749543044\n",
      "Epoch 1 step 440: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 440: training loss: 468.3725418311261\n",
      "Epoch 1 step 441: training accuarcy: 0.9975\n",
      "Epoch 1 step 441: training loss: 489.3756137802477\n",
      "Epoch 1 step 442: training accuarcy: 0.9948\n",
      "Epoch 1 step 442: training loss: 468.50547401082514\n",
      "Epoch 1 step 443: training accuarcy: 0.9976\n",
      "Epoch 1 step 443: training loss: 481.55606105227935\n",
      "Epoch 1 step 444: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 444: training loss: 459.46324147663165\n",
      "Epoch 1 step 445: training accuarcy: 0.997\n",
      "Epoch 1 step 445: training loss: 464.18534040173756\n",
      "Epoch 1 step 446: training accuarcy: 0.9959\n",
      "Epoch 1 step 446: training loss: 472.8009125481773\n",
      "Epoch 1 step 447: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 447: training loss: 473.65364384589435\n",
      "Epoch 1 step 448: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 448: training loss: 465.50365368339686\n",
      "Epoch 1 step 449: training accuarcy: 0.996\n",
      "Epoch 1 step 449: training loss: 464.847173900539\n",
      "Epoch 1 step 450: training accuarcy: 0.9961000000000001\n",
      "Epoch 1 step 450: training loss: 474.0402165381368\n",
      "Epoch 1 step 451: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 451: training loss: 506.98958439959836\n",
      "Epoch 1 step 452: training accuarcy: 0.9948\n",
      "Epoch 1 step 452: training loss: 472.2789822601302\n",
      "Epoch 1 step 453: training accuarcy: 0.9967\n",
      "Epoch 1 step 453: training loss: 436.07901863334075\n",
      "Epoch 1 step 454: training accuarcy: 0.9977\n",
      "Epoch 1 step 454: training loss: 464.98112552065913\n",
      "Epoch 1 step 455: training accuarcy: 0.9965\n",
      "Epoch 1 step 455: training loss: 463.3130403238203\n",
      "Epoch 1 step 456: training accuarcy: 0.9965\n",
      "Epoch 1 step 456: training loss: 478.6772271316259\n",
      "Epoch 1 step 457: training accuarcy: 0.9949\n",
      "Epoch 1 step 457: training loss: 468.39188859246224\n",
      "Epoch 1 step 458: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 458: training loss: 481.7023964040579\n",
      "Epoch 1 step 459: training accuarcy: 0.9955\n",
      "Epoch 1 step 459: training loss: 418.8294737999759\n",
      "Epoch 1 step 460: training accuarcy: 0.9976\n",
      "Epoch 1 step 460: training loss: 458.0887214394792\n",
      "Epoch 1 step 461: training accuarcy: 0.9946\n",
      "Epoch 1 step 461: training loss: 488.712958752535\n",
      "Epoch 1 step 462: training accuarcy: 0.9947\n",
      "Epoch 1 step 462: training loss: 451.0889440263099\n",
      "Epoch 1 step 463: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 463: training loss: 451.86792737116366\n",
      "Epoch 1 step 464: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 464: training loss: 472.2896934987312\n",
      "Epoch 1 step 465: training accuarcy: 0.9967\n",
      "Epoch 1 step 465: training loss: 499.5934583151535\n",
      "Epoch 1 step 466: training accuarcy: 0.9948\n",
      "Epoch 1 step 466: training loss: 473.51500879741707\n",
      "Epoch 1 step 467: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 467: training loss: 488.4397321819383\n",
      "Epoch 1 step 468: training accuarcy: 0.9958\n",
      "Epoch 1 step 468: training loss: 442.26615689478194\n",
      "Epoch 1 step 469: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 469: training loss: 447.75627189268675\n",
      "Epoch 1 step 470: training accuarcy: 0.9968\n",
      "Epoch 1 step 470: training loss: 458.756195495865\n",
      "Epoch 1 step 471: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 471: training loss: 445.73284618177337\n",
      "Epoch 1 step 472: training accuarcy: 0.9983000000000001\n",
      "Epoch 1 step 472: training loss: 444.97751128963847\n",
      "Epoch 1 step 473: training accuarcy: 0.995\n",
      "Epoch 1 step 473: training loss: 454.97464354472066\n",
      "Epoch 1 step 474: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 474: training loss: 431.99185937694097\n",
      "Epoch 1 step 475: training accuarcy: 0.9955\n",
      "Epoch 1 step 475: training loss: 431.3595969258089\n",
      "Epoch 1 step 476: training accuarcy: 0.9965\n",
      "Epoch 1 step 476: training loss: 438.25590877391056\n",
      "Epoch 1 step 477: training accuarcy: 0.9956\n",
      "Epoch 1 step 477: training loss: 417.9945836289743\n",
      "Epoch 1 step 478: training accuarcy: 0.9979\n",
      "Epoch 1 step 478: training loss: 466.37711564964104\n",
      "Epoch 1 step 479: training accuarcy: 0.9958\n",
      "Epoch 1 step 479: training loss: 474.6519835355888\n",
      "Epoch 1 step 480: training accuarcy: 0.9938\n",
      "Epoch 1 step 480: training loss: 444.09494422817727\n",
      "Epoch 1 step 481: training accuarcy: 0.9959\n",
      "Epoch 1 step 481: training loss: 401.93323260304135\n",
      "Epoch 1 step 482: training accuarcy: 0.9978\n",
      "Epoch 1 step 482: training loss: 455.8807651773336\n",
      "Epoch 1 step 483: training accuarcy: 0.9967\n",
      "Epoch 1 step 483: training loss: 415.01752042507917\n",
      "Epoch 1 step 484: training accuarcy: 0.9967\n",
      "Epoch 1 step 484: training loss: 435.50874916464164\n",
      "Epoch 1 step 485: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 485: training loss: 422.8759349159179\n",
      "Epoch 1 step 486: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 486: training loss: 443.706740193898\n",
      "Epoch 1 step 487: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 487: training loss: 437.93121595994234\n",
      "Epoch 1 step 488: training accuarcy: 0.9959\n",
      "Epoch 1 step 488: training loss: 440.147625760705\n",
      "Epoch 1 step 489: training accuarcy: 0.9945\n",
      "Epoch 1 step 489: training loss: 438.37188016527557\n",
      "Epoch 1 step 490: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 490: training loss: 427.0531057359364\n",
      "Epoch 1 step 491: training accuarcy: 0.9975\n",
      "Epoch 1 step 491: training loss: 441.56226942228795\n",
      "Epoch 1 step 492: training accuarcy: 0.9954000000000001\n",
      "Epoch 1 step 492: training loss: 466.23147158325776\n",
      "Epoch 1 step 493: training accuarcy: 0.9958\n",
      "Epoch 1 step 493: training loss: 401.5138615029424\n",
      "Epoch 1 step 494: training accuarcy: 0.998\n",
      "Epoch 1 step 494: training loss: 433.8744011728088\n",
      "Epoch 1 step 495: training accuarcy: 0.9965\n",
      "Epoch 1 step 495: training loss: 439.1194523001107\n",
      "Epoch 1 step 496: training accuarcy: 0.9967\n",
      "Epoch 1 step 496: training loss: 449.2886990484467\n",
      "Epoch 1 step 497: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 497: training loss: 455.0526007238744\n",
      "Epoch 1 step 498: training accuarcy: 0.9946\n",
      "Epoch 1 step 498: training loss: 466.2495364993873\n",
      "Epoch 1 step 499: training accuarcy: 0.9936\n",
      "Epoch 1 step 499: training loss: 448.54188291873766\n",
      "Epoch 1 step 500: training accuarcy: 0.9926\n",
      "Epoch 1 step 500: training loss: 434.4289988940816\n",
      "Epoch 1 step 501: training accuarcy: 0.9941000000000001\n",
      "Epoch 1 step 501: training loss: 411.09487081389386\n",
      "Epoch 1 step 502: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 502: training loss: 420.54142623182423\n",
      "Epoch 1 step 503: training accuarcy: 0.9967\n",
      "Epoch 1 step 503: training loss: 426.2665852668441\n",
      "Epoch 1 step 504: training accuarcy: 0.9967\n",
      "Epoch 1 step 504: training loss: 395.92506068179296\n",
      "Epoch 1 step 505: training accuarcy: 0.9971000000000001\n",
      "Epoch 1 step 505: training loss: 422.1887324740164\n",
      "Epoch 1 step 506: training accuarcy: 0.9976\n",
      "Epoch 1 step 506: training loss: 430.7447251917735\n",
      "Epoch 1 step 507: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 507: training loss: 440.9722836084977\n",
      "Epoch 1 step 508: training accuarcy: 0.9952000000000001\n",
      "Epoch 1 step 508: training loss: 447.07499649794653\n",
      "Epoch 1 step 509: training accuarcy: 0.9953000000000001\n",
      "Epoch 1 step 509: training loss: 444.0392063827576\n",
      "Epoch 1 step 510: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 510: training loss: 445.97621883856783\n",
      "Epoch 1 step 511: training accuarcy: 0.9935\n",
      "Epoch 1 step 511: training loss: 408.6690696548517\n",
      "Epoch 1 step 512: training accuarcy: 0.997\n",
      "Epoch 1 step 512: training loss: 451.5340753109575\n",
      "Epoch 1 step 513: training accuarcy: 0.9951000000000001\n",
      "Epoch 1 step 513: training loss: 406.97472861717534\n",
      "Epoch 1 step 514: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 514: training loss: 410.8431999635772\n",
      "Epoch 1 step 515: training accuarcy: 0.9959\n",
      "Epoch 1 step 515: training loss: 413.0774699028161\n",
      "Epoch 1 step 516: training accuarcy: 0.9968\n",
      "Epoch 1 step 516: training loss: 405.5417411382557\n",
      "Epoch 1 step 517: training accuarcy: 0.9971000000000001\n",
      "Epoch 1 step 517: training loss: 427.15928973223015\n",
      "Epoch 1 step 518: training accuarcy: 0.9957\n",
      "Epoch 1 step 518: training loss: 406.32419004660034\n",
      "Epoch 1 step 519: training accuarcy: 0.9962000000000001\n",
      "Epoch 1 step 519: training loss: 413.04127009837214\n",
      "Epoch 1 step 520: training accuarcy: 0.9974000000000001\n",
      "Epoch 1 step 520: training loss: 412.44680023155246\n",
      "Epoch 1 step 521: training accuarcy: 0.9963000000000001\n",
      "Epoch 1 step 521: training loss: 406.91240302679216\n",
      "Epoch 1 step 522: training accuarcy: 0.9965\n",
      "Epoch 1 step 522: training loss: 389.6707242744376\n",
      "Epoch 1 step 523: training accuarcy: 0.9964000000000001\n",
      "Epoch 1 step 523: training loss: 405.5664837330436\n",
      "Epoch 1 step 524: training accuarcy: 0.9968\n",
      "Epoch 1 step 524: training loss: 397.4477996468483\n",
      "Epoch 1 step 525: training accuarcy: 0.9973000000000001\n",
      "Epoch 1 step 525: training loss: 340.4436111627402\n",
      "Epoch 1 step 526: training accuarcy: 0.9966666666666667\n",
      "Epoch 1: train loss 621.6931889058443, train accuarcy 0.9897975921630859\n",
      "Epoch 1: valid loss 727.7551166987381, valid accuarcy 0.9922782182693481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████████████████████████████▊                                                                                                       | 2/5 [09:46<14:43, 294.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 383.67212274899856\n",
      "Epoch 2 step 527: training accuarcy: 0.9966\n",
      "Epoch 2 step 527: training loss: 384.8049289700171\n",
      "Epoch 2 step 528: training accuarcy: 0.9978\n",
      "Epoch 2 step 528: training loss: 356.31919884529856\n",
      "Epoch 2 step 529: training accuarcy: 0.9978\n",
      "Epoch 2 step 529: training loss: 356.761232820384\n",
      "Epoch 2 step 530: training accuarcy: 0.999\n",
      "Epoch 2 step 530: training loss: 362.7373632910302\n",
      "Epoch 2 step 531: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 531: training loss: 409.8708334306421\n",
      "Epoch 2 step 532: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 532: training loss: 370.6629892123538\n",
      "Epoch 2 step 533: training accuarcy: 0.9965\n",
      "Epoch 2 step 533: training loss: 355.17098244522646\n",
      "Epoch 2 step 534: training accuarcy: 0.999\n",
      "Epoch 2 step 534: training loss: 378.93532783609265\n",
      "Epoch 2 step 535: training accuarcy: 0.9966\n",
      "Epoch 2 step 535: training loss: 339.88251935452354\n",
      "Epoch 2 step 536: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 536: training loss: 367.0548180342944\n",
      "Epoch 2 step 537: training accuarcy: 0.9996\n",
      "Epoch 2 step 537: training loss: 371.08341750030286\n",
      "Epoch 2 step 538: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 538: training loss: 391.7499176977963\n",
      "Epoch 2 step 539: training accuarcy: 0.9975\n",
      "Epoch 2 step 539: training loss: 359.8930280941354\n",
      "Epoch 2 step 540: training accuarcy: 0.9987\n",
      "Epoch 2 step 540: training loss: 393.08508090112423\n",
      "Epoch 2 step 541: training accuarcy: 0.9978\n",
      "Epoch 2 step 541: training loss: 370.3265336623399\n",
      "Epoch 2 step 542: training accuarcy: 0.9976\n",
      "Epoch 2 step 542: training loss: 371.0167451738488\n",
      "Epoch 2 step 543: training accuarcy: 0.9977\n",
      "Epoch 2 step 543: training loss: 358.2590293014148\n",
      "Epoch 2 step 544: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 544: training loss: 354.7116501426177\n",
      "Epoch 2 step 545: training accuarcy: 0.9978\n",
      "Epoch 2 step 545: training loss: 386.72488043662895\n",
      "Epoch 2 step 546: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 546: training loss: 354.0927187404975\n",
      "Epoch 2 step 547: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 547: training loss: 358.0693629588526\n",
      "Epoch 2 step 548: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 548: training loss: 345.785810889547\n",
      "Epoch 2 step 549: training accuarcy: 0.9991000000000001\n",
      "Epoch 2 step 549: training loss: 366.8371913332817\n",
      "Epoch 2 step 550: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 550: training loss: 358.046824763467\n",
      "Epoch 2 step 551: training accuarcy: 0.9979\n",
      "Epoch 2 step 551: training loss: 382.0166893576918\n",
      "Epoch 2 step 552: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 552: training loss: 373.6764051015116\n",
      "Epoch 2 step 553: training accuarcy: 0.996\n",
      "Epoch 2 step 553: training loss: 374.3137338054959\n",
      "Epoch 2 step 554: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 554: training loss: 358.5423605798652\n",
      "Epoch 2 step 555: training accuarcy: 0.9986\n",
      "Epoch 2 step 555: training loss: 363.27722702822905\n",
      "Epoch 2 step 556: training accuarcy: 0.9979\n",
      "Epoch 2 step 556: training loss: 375.19048032181644\n",
      "Epoch 2 step 557: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 557: training loss: 376.35054418433504\n",
      "Epoch 2 step 558: training accuarcy: 0.9969\n",
      "Epoch 2 step 558: training loss: 364.5917799597546\n",
      "Epoch 2 step 559: training accuarcy: 0.997\n",
      "Epoch 2 step 559: training loss: 377.49228124121896\n",
      "Epoch 2 step 560: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 560: training loss: 343.7059458924774\n",
      "Epoch 2 step 561: training accuarcy: 0.999\n",
      "Epoch 2 step 561: training loss: 377.3798867043789\n",
      "Epoch 2 step 562: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 562: training loss: 343.68167147062223\n",
      "Epoch 2 step 563: training accuarcy: 0.998\n",
      "Epoch 2 step 563: training loss: 360.30006774202513\n",
      "Epoch 2 step 564: training accuarcy: 0.9978\n",
      "Epoch 2 step 564: training loss: 352.9811638129926\n",
      "Epoch 2 step 565: training accuarcy: 0.9978\n",
      "Epoch 2 step 565: training loss: 356.28254703009884\n",
      "Epoch 2 step 566: training accuarcy: 0.9977\n",
      "Epoch 2 step 566: training loss: 367.8802407886041\n",
      "Epoch 2 step 567: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 567: training loss: 344.2162036331962\n",
      "Epoch 2 step 568: training accuarcy: 0.9992000000000001\n",
      "Epoch 2 step 568: training loss: 368.8813451366322\n",
      "Epoch 2 step 569: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 569: training loss: 324.3280475075035\n",
      "Epoch 2 step 570: training accuarcy: 0.9991000000000001\n",
      "Epoch 2 step 570: training loss: 396.2827879701968\n",
      "Epoch 2 step 571: training accuarcy: 0.9968\n",
      "Epoch 2 step 571: training loss: 361.71093942272154\n",
      "Epoch 2 step 572: training accuarcy: 0.9977\n",
      "Epoch 2 step 572: training loss: 362.38791172855974\n",
      "Epoch 2 step 573: training accuarcy: 0.9968\n",
      "Epoch 2 step 573: training loss: 331.7720972610663\n",
      "Epoch 2 step 574: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 574: training loss: 367.5233362856871\n",
      "Epoch 2 step 575: training accuarcy: 0.9967\n",
      "Epoch 2 step 575: training loss: 370.47064525448985\n",
      "Epoch 2 step 576: training accuarcy: 0.9967\n",
      "Epoch 2 step 576: training loss: 384.7079172333103\n",
      "Epoch 2 step 577: training accuarcy: 0.9941000000000001\n",
      "Epoch 2 step 577: training loss: 363.77183003314616\n",
      "Epoch 2 step 578: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 578: training loss: 360.45922702610756\n",
      "Epoch 2 step 579: training accuarcy: 0.9984000000000001\n",
      "Epoch 2 step 579: training loss: 360.7458437191511\n",
      "Epoch 2 step 580: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 580: training loss: 348.3126320080865\n",
      "Epoch 2 step 581: training accuarcy: 0.9977\n",
      "Epoch 2 step 581: training loss: 369.732953604175\n",
      "Epoch 2 step 582: training accuarcy: 0.9979\n",
      "Epoch 2 step 582: training loss: 344.6878195994676\n",
      "Epoch 2 step 583: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 583: training loss: 377.4115929532494\n",
      "Epoch 2 step 584: training accuarcy: 0.9965\n",
      "Epoch 2 step 584: training loss: 398.9636556550332\n",
      "Epoch 2 step 585: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 585: training loss: 365.46618652807337\n",
      "Epoch 2 step 586: training accuarcy: 0.9967\n",
      "Epoch 2 step 586: training loss: 342.7412948322632\n",
      "Epoch 2 step 587: training accuarcy: 0.998\n",
      "Epoch 2 step 587: training loss: 361.53337176196175\n",
      "Epoch 2 step 588: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 588: training loss: 377.3810550737307\n",
      "Epoch 2 step 589: training accuarcy: 0.9965\n",
      "Epoch 2 step 589: training loss: 336.6361612240375\n",
      "Epoch 2 step 590: training accuarcy: 0.9985\n",
      "Epoch 2 step 590: training loss: 367.9470000391867\n",
      "Epoch 2 step 591: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 591: training loss: 353.5276041963622\n",
      "Epoch 2 step 592: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 592: training loss: 336.4697801831603\n",
      "Epoch 2 step 593: training accuarcy: 0.9986\n",
      "Epoch 2 step 593: training loss: 335.1241363827919\n",
      "Epoch 2 step 594: training accuarcy: 0.9992000000000001\n",
      "Epoch 2 step 594: training loss: 376.4041706874642\n",
      "Epoch 2 step 595: training accuarcy: 0.9968\n",
      "Epoch 2 step 595: training loss: 351.20085928554806\n",
      "Epoch 2 step 596: training accuarcy: 0.9965\n",
      "Epoch 2 step 596: training loss: 359.3888810092476\n",
      "Epoch 2 step 597: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 597: training loss: 337.15438965992905\n",
      "Epoch 2 step 598: training accuarcy: 0.9984000000000001\n",
      "Epoch 2 step 598: training loss: 340.7751973150144\n",
      "Epoch 2 step 599: training accuarcy: 0.9977\n",
      "Epoch 2 step 599: training loss: 337.9178402028988\n",
      "Epoch 2 step 600: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 600: training loss: 379.41080183861254\n",
      "Epoch 2 step 601: training accuarcy: 0.9965\n",
      "Epoch 2 step 601: training loss: 359.85471834783596\n",
      "Epoch 2 step 602: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 602: training loss: 366.71100822624965\n",
      "Epoch 2 step 603: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 603: training loss: 365.0055615732794\n",
      "Epoch 2 step 604: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 604: training loss: 352.4230398306701\n",
      "Epoch 2 step 605: training accuarcy: 0.9978\n",
      "Epoch 2 step 605: training loss: 358.33843887028877\n",
      "Epoch 2 step 606: training accuarcy: 0.9978\n",
      "Epoch 2 step 606: training loss: 357.5174482604878\n",
      "Epoch 2 step 607: training accuarcy: 0.9979\n",
      "Epoch 2 step 607: training loss: 358.6218510251732\n",
      "Epoch 2 step 608: training accuarcy: 0.9975\n",
      "Epoch 2 step 608: training loss: 361.3149624561489\n",
      "Epoch 2 step 609: training accuarcy: 0.9968\n",
      "Epoch 2 step 609: training loss: 377.2613636394208\n",
      "Epoch 2 step 610: training accuarcy: 0.9969\n",
      "Epoch 2 step 610: training loss: 354.86207737352083\n",
      "Epoch 2 step 611: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 611: training loss: 385.3298007830897\n",
      "Epoch 2 step 612: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 612: training loss: 338.9122788196206\n",
      "Epoch 2 step 613: training accuarcy: 0.9976\n",
      "Epoch 2 step 613: training loss: 371.9828201566735\n",
      "Epoch 2 step 614: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 614: training loss: 351.38334072019495\n",
      "Epoch 2 step 615: training accuarcy: 0.9979\n",
      "Epoch 2 step 615: training loss: 351.75857508924406\n",
      "Epoch 2 step 616: training accuarcy: 0.9977\n",
      "Epoch 2 step 616: training loss: 381.56188285866347\n",
      "Epoch 2 step 617: training accuarcy: 0.9968\n",
      "Epoch 2 step 617: training loss: 362.6179404059551\n",
      "Epoch 2 step 618: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 618: training loss: 338.4652591841173\n",
      "Epoch 2 step 619: training accuarcy: 0.9977\n",
      "Epoch 2 step 619: training loss: 368.4567131681408\n",
      "Epoch 2 step 620: training accuarcy: 0.9965\n",
      "Epoch 2 step 620: training loss: 361.2430724056792\n",
      "Epoch 2 step 621: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 621: training loss: 379.6275678429901\n",
      "Epoch 2 step 622: training accuarcy: 0.9956\n",
      "Epoch 2 step 622: training loss: 353.75629651016305\n",
      "Epoch 2 step 623: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 623: training loss: 374.0740467924328\n",
      "Epoch 2 step 624: training accuarcy: 0.9969\n",
      "Epoch 2 step 624: training loss: 354.88263673478593\n",
      "Epoch 2 step 625: training accuarcy: 0.9992000000000001\n",
      "Epoch 2 step 625: training loss: 384.56685138746445\n",
      "Epoch 2 step 626: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 626: training loss: 363.460376412968\n",
      "Epoch 2 step 627: training accuarcy: 0.9965\n",
      "Epoch 2 step 627: training loss: 348.2096215927206\n",
      "Epoch 2 step 628: training accuarcy: 0.9979\n",
      "Epoch 2 step 628: training loss: 364.83539064214494\n",
      "Epoch 2 step 629: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 629: training loss: 363.6592723875462\n",
      "Epoch 2 step 630: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 630: training loss: 370.383697529124\n",
      "Epoch 2 step 631: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 631: training loss: 379.4740846139017\n",
      "Epoch 2 step 632: training accuarcy: 0.9955\n",
      "Epoch 2 step 632: training loss: 360.1349731238254\n",
      "Epoch 2 step 633: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 633: training loss: 346.67115760619834\n",
      "Epoch 2 step 634: training accuarcy: 0.9978\n",
      "Epoch 2 step 634: training loss: 375.8318307421523\n",
      "Epoch 2 step 635: training accuarcy: 0.9947\n",
      "Epoch 2 step 635: training loss: 333.58470533839824\n",
      "Epoch 2 step 636: training accuarcy: 0.9975\n",
      "Epoch 2 step 636: training loss: 330.69455548473843\n",
      "Epoch 2 step 637: training accuarcy: 0.9975\n",
      "Epoch 2 step 637: training loss: 394.5084889018067\n",
      "Epoch 2 step 638: training accuarcy: 0.9941000000000001\n",
      "Epoch 2 step 638: training loss: 329.41403760503556\n",
      "Epoch 2 step 639: training accuarcy: 0.9977\n",
      "Epoch 2 step 639: training loss: 361.5276467477304\n",
      "Epoch 2 step 640: training accuarcy: 0.998\n",
      "Epoch 2 step 640: training loss: 357.53557152001804\n",
      "Epoch 2 step 641: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 641: training loss: 337.8011282586017\n",
      "Epoch 2 step 642: training accuarcy: 0.9987\n",
      "Epoch 2 step 642: training loss: 397.2371577270547\n",
      "Epoch 2 step 643: training accuarcy: 0.9959\n",
      "Epoch 2 step 643: training loss: 367.50955967588254\n",
      "Epoch 2 step 644: training accuarcy: 0.9967\n",
      "Epoch 2 step 644: training loss: 342.2903952736709\n",
      "Epoch 2 step 645: training accuarcy: 0.9978\n",
      "Epoch 2 step 645: training loss: 379.09047522794094\n",
      "Epoch 2 step 646: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 646: training loss: 358.0343522125755\n",
      "Epoch 2 step 647: training accuarcy: 0.9968\n",
      "Epoch 2 step 647: training loss: 347.01637193317407\n",
      "Epoch 2 step 648: training accuarcy: 0.9966\n",
      "Epoch 2 step 648: training loss: 318.8431520998342\n",
      "Epoch 2 step 649: training accuarcy: 0.9975\n",
      "Epoch 2 step 649: training loss: 341.5988737790746\n",
      "Epoch 2 step 650: training accuarcy: 0.998\n",
      "Epoch 2 step 650: training loss: 354.63488170962535\n",
      "Epoch 2 step 651: training accuarcy: 0.9968\n",
      "Epoch 2 step 651: training loss: 353.888431964632\n",
      "Epoch 2 step 652: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 652: training loss: 358.8014540066648\n",
      "Epoch 2 step 653: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 653: training loss: 350.4446174479548\n",
      "Epoch 2 step 654: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 654: training loss: 364.82570030384943\n",
      "Epoch 2 step 655: training accuarcy: 0.998\n",
      "Epoch 2 step 655: training loss: 331.93993865177913\n",
      "Epoch 2 step 656: training accuarcy: 0.9988\n",
      "Epoch 2 step 656: training loss: 328.0148760652769\n",
      "Epoch 2 step 657: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 657: training loss: 330.13652584867225\n",
      "Epoch 2 step 658: training accuarcy: 0.9979\n",
      "Epoch 2 step 658: training loss: 362.0568249032648\n",
      "Epoch 2 step 659: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 659: training loss: 336.54481920751766\n",
      "Epoch 2 step 660: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 660: training loss: 330.06189186857097\n",
      "Epoch 2 step 661: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 661: training loss: 356.6659034374406\n",
      "Epoch 2 step 662: training accuarcy: 0.9976\n",
      "Epoch 2 step 662: training loss: 359.35041742209205\n",
      "Epoch 2 step 663: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 663: training loss: 355.8676030211391\n",
      "Epoch 2 step 664: training accuarcy: 0.997\n",
      "Epoch 2 step 664: training loss: 314.5755082774908\n",
      "Epoch 2 step 665: training accuarcy: 0.9992000000000001\n",
      "Epoch 2 step 665: training loss: 363.7677740320702\n",
      "Epoch 2 step 666: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 666: training loss: 330.24852192892627\n",
      "Epoch 2 step 667: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 667: training loss: 333.3938140417955\n",
      "Epoch 2 step 668: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 668: training loss: 376.0100193604327\n",
      "Epoch 2 step 669: training accuarcy: 0.9956\n",
      "Epoch 2 step 669: training loss: 383.5658761358915\n",
      "Epoch 2 step 670: training accuarcy: 0.9957\n",
      "Epoch 2 step 670: training loss: 338.20838045826\n",
      "Epoch 2 step 671: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 671: training loss: 361.2538602095847\n",
      "Epoch 2 step 672: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 672: training loss: 365.11334876911474\n",
      "Epoch 2 step 673: training accuarcy: 0.9958\n",
      "Epoch 2 step 673: training loss: 344.755005567836\n",
      "Epoch 2 step 674: training accuarcy: 0.9966\n",
      "Epoch 2 step 674: training loss: 372.31105450493976\n",
      "Epoch 2 step 675: training accuarcy: 0.9957\n",
      "Epoch 2 step 675: training loss: 397.21334721083855\n",
      "Epoch 2 step 676: training accuarcy: 0.9957\n",
      "Epoch 2 step 676: training loss: 344.4193342291922\n",
      "Epoch 2 step 677: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 677: training loss: 306.6249585706901\n",
      "Epoch 2 step 678: training accuarcy: 0.9997\n",
      "Epoch 2 step 678: training loss: 357.2143380967941\n",
      "Epoch 2 step 679: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 679: training loss: 369.21508167425947\n",
      "Epoch 2 step 680: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 680: training loss: 365.66073298632966\n",
      "Epoch 2 step 681: training accuarcy: 0.997\n",
      "Epoch 2 step 681: training loss: 339.4179774802659\n",
      "Epoch 2 step 682: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 682: training loss: 328.7522289053637\n",
      "Epoch 2 step 683: training accuarcy: 0.9979\n",
      "Epoch 2 step 683: training loss: 378.9641090195603\n",
      "Epoch 2 step 684: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 684: training loss: 362.1808103676728\n",
      "Epoch 2 step 685: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 685: training loss: 359.57152711057097\n",
      "Epoch 2 step 686: training accuarcy: 0.9948\n",
      "Epoch 2 step 686: training loss: 370.0959265566074\n",
      "Epoch 2 step 687: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 687: training loss: 342.33541911580363\n",
      "Epoch 2 step 688: training accuarcy: 0.9979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 688: training loss: 375.2483641722556\n",
      "Epoch 2 step 689: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 689: training loss: 342.9448350660174\n",
      "Epoch 2 step 690: training accuarcy: 0.9979\n",
      "Epoch 2 step 690: training loss: 359.4577075171537\n",
      "Epoch 2 step 691: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 691: training loss: 330.59003584137616\n",
      "Epoch 2 step 692: training accuarcy: 0.9986\n",
      "Epoch 2 step 692: training loss: 340.5669023189479\n",
      "Epoch 2 step 693: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 693: training loss: 350.6996320764523\n",
      "Epoch 2 step 694: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 694: training loss: 374.09681592900597\n",
      "Epoch 2 step 695: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 695: training loss: 336.74055270371014\n",
      "Epoch 2 step 696: training accuarcy: 0.9966\n",
      "Epoch 2 step 696: training loss: 363.0581063136573\n",
      "Epoch 2 step 697: training accuarcy: 0.996\n",
      "Epoch 2 step 697: training loss: 353.8461701288368\n",
      "Epoch 2 step 698: training accuarcy: 0.998\n",
      "Epoch 2 step 698: training loss: 346.48174880757233\n",
      "Epoch 2 step 699: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 699: training loss: 366.3953028332457\n",
      "Epoch 2 step 700: training accuarcy: 0.9966\n",
      "Epoch 2 step 700: training loss: 353.2986712959754\n",
      "Epoch 2 step 701: training accuarcy: 0.9967\n",
      "Epoch 2 step 701: training loss: 337.9109375619919\n",
      "Epoch 2 step 702: training accuarcy: 0.9966\n",
      "Epoch 2 step 702: training loss: 321.97590800902543\n",
      "Epoch 2 step 703: training accuarcy: 0.9988\n",
      "Epoch 2 step 703: training loss: 355.4922886022997\n",
      "Epoch 2 step 704: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 704: training loss: 351.9492961681575\n",
      "Epoch 2 step 705: training accuarcy: 0.9965\n",
      "Epoch 2 step 705: training loss: 376.5694435666845\n",
      "Epoch 2 step 706: training accuarcy: 0.9958\n",
      "Epoch 2 step 706: training loss: 366.60685949482047\n",
      "Epoch 2 step 707: training accuarcy: 0.996\n",
      "Epoch 2 step 707: training loss: 343.000173114528\n",
      "Epoch 2 step 708: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 708: training loss: 328.47254214016095\n",
      "Epoch 2 step 709: training accuarcy: 0.9967\n",
      "Epoch 2 step 709: training loss: 363.06228456629435\n",
      "Epoch 2 step 710: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 710: training loss: 351.9054243140592\n",
      "Epoch 2 step 711: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 711: training loss: 350.3911392662476\n",
      "Epoch 2 step 712: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 712: training loss: 354.84423391094674\n",
      "Epoch 2 step 713: training accuarcy: 0.9975\n",
      "Epoch 2 step 713: training loss: 342.9516403980697\n",
      "Epoch 2 step 714: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 714: training loss: 354.1496678180624\n",
      "Epoch 2 step 715: training accuarcy: 0.9975\n",
      "Epoch 2 step 715: training loss: 341.16230344486524\n",
      "Epoch 2 step 716: training accuarcy: 0.9977\n",
      "Epoch 2 step 716: training loss: 330.37194068005397\n",
      "Epoch 2 step 717: training accuarcy: 0.9977\n",
      "Epoch 2 step 717: training loss: 341.4238000784554\n",
      "Epoch 2 step 718: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 718: training loss: 341.0250937381714\n",
      "Epoch 2 step 719: training accuarcy: 0.9979\n",
      "Epoch 2 step 719: training loss: 363.44580514563717\n",
      "Epoch 2 step 720: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 720: training loss: 317.0743763460424\n",
      "Epoch 2 step 721: training accuarcy: 0.9988\n",
      "Epoch 2 step 721: training loss: 342.442113927565\n",
      "Epoch 2 step 722: training accuarcy: 0.9978\n",
      "Epoch 2 step 722: training loss: 320.76303873016457\n",
      "Epoch 2 step 723: training accuarcy: 0.9976\n",
      "Epoch 2 step 723: training loss: 349.0991043096379\n",
      "Epoch 2 step 724: training accuarcy: 0.9976\n",
      "Epoch 2 step 724: training loss: 349.6538493531317\n",
      "Epoch 2 step 725: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 725: training loss: 330.9070277904176\n",
      "Epoch 2 step 726: training accuarcy: 0.9978\n",
      "Epoch 2 step 726: training loss: 341.3221729675764\n",
      "Epoch 2 step 727: training accuarcy: 0.9966\n",
      "Epoch 2 step 727: training loss: 369.49201546375286\n",
      "Epoch 2 step 728: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 728: training loss: 356.0413004996992\n",
      "Epoch 2 step 729: training accuarcy: 0.9957\n",
      "Epoch 2 step 729: training loss: 348.36775007659935\n",
      "Epoch 2 step 730: training accuarcy: 0.9975\n",
      "Epoch 2 step 730: training loss: 362.52593861837727\n",
      "Epoch 2 step 731: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 731: training loss: 320.6515702698697\n",
      "Epoch 2 step 732: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 732: training loss: 343.479713642736\n",
      "Epoch 2 step 733: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 733: training loss: 387.159942808804\n",
      "Epoch 2 step 734: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 734: training loss: 323.57117883929396\n",
      "Epoch 2 step 735: training accuarcy: 0.9985\n",
      "Epoch 2 step 735: training loss: 367.5442672599605\n",
      "Epoch 2 step 736: training accuarcy: 0.9959\n",
      "Epoch 2 step 736: training loss: 361.906422090398\n",
      "Epoch 2 step 737: training accuarcy: 0.9956\n",
      "Epoch 2 step 737: training loss: 350.28769813446024\n",
      "Epoch 2 step 738: training accuarcy: 0.9966\n",
      "Epoch 2 step 738: training loss: 316.608898539856\n",
      "Epoch 2 step 739: training accuarcy: 0.998\n",
      "Epoch 2 step 739: training loss: 319.8835822319743\n",
      "Epoch 2 step 740: training accuarcy: 0.9969\n",
      "Epoch 2 step 740: training loss: 337.7134805816683\n",
      "Epoch 2 step 741: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 741: training loss: 330.6158029176834\n",
      "Epoch 2 step 742: training accuarcy: 0.9977\n",
      "Epoch 2 step 742: training loss: 347.04980849561105\n",
      "Epoch 2 step 743: training accuarcy: 0.9966\n",
      "Epoch 2 step 743: training loss: 341.30567266294366\n",
      "Epoch 2 step 744: training accuarcy: 0.9978\n",
      "Epoch 2 step 744: training loss: 336.51740425019\n",
      "Epoch 2 step 745: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 745: training loss: 340.619660640746\n",
      "Epoch 2 step 746: training accuarcy: 0.9966\n",
      "Epoch 2 step 746: training loss: 314.6267441734062\n",
      "Epoch 2 step 747: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 747: training loss: 322.24350643773823\n",
      "Epoch 2 step 748: training accuarcy: 0.9986\n",
      "Epoch 2 step 748: training loss: 338.91178978385017\n",
      "Epoch 2 step 749: training accuarcy: 0.9967\n",
      "Epoch 2 step 749: training loss: 338.04749513926294\n",
      "Epoch 2 step 750: training accuarcy: 0.9976\n",
      "Epoch 2 step 750: training loss: 325.5071169604344\n",
      "Epoch 2 step 751: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 751: training loss: 370.8038343550178\n",
      "Epoch 2 step 752: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 752: training loss: 358.71709865387425\n",
      "Epoch 2 step 753: training accuarcy: 0.9958\n",
      "Epoch 2 step 753: training loss: 336.97550154774984\n",
      "Epoch 2 step 754: training accuarcy: 0.9967\n",
      "Epoch 2 step 754: training loss: 307.56948044235315\n",
      "Epoch 2 step 755: training accuarcy: 0.9979\n",
      "Epoch 2 step 755: training loss: 343.3394035979005\n",
      "Epoch 2 step 756: training accuarcy: 0.9978\n",
      "Epoch 2 step 756: training loss: 373.9033261952111\n",
      "Epoch 2 step 757: training accuarcy: 0.9958\n",
      "Epoch 2 step 757: training loss: 346.9314333634575\n",
      "Epoch 2 step 758: training accuarcy: 0.997\n",
      "Epoch 2 step 758: training loss: 322.245233588344\n",
      "Epoch 2 step 759: training accuarcy: 0.9983000000000001\n",
      "Epoch 2 step 759: training loss: 320.85327715662623\n",
      "Epoch 2 step 760: training accuarcy: 0.9978\n",
      "Epoch 2 step 760: training loss: 319.86422597763516\n",
      "Epoch 2 step 761: training accuarcy: 0.9978\n",
      "Epoch 2 step 761: training loss: 322.3973425019395\n",
      "Epoch 2 step 762: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 762: training loss: 338.74485204193\n",
      "Epoch 2 step 763: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 763: training loss: 296.91274880164235\n",
      "Epoch 2 step 764: training accuarcy: 0.9993000000000001\n",
      "Epoch 2 step 764: training loss: 339.5134393094062\n",
      "Epoch 2 step 765: training accuarcy: 0.9969\n",
      "Epoch 2 step 765: training loss: 340.0097827744453\n",
      "Epoch 2 step 766: training accuarcy: 0.9976\n",
      "Epoch 2 step 766: training loss: 342.34396381551403\n",
      "Epoch 2 step 767: training accuarcy: 0.9968\n",
      "Epoch 2 step 767: training loss: 315.157275263374\n",
      "Epoch 2 step 768: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 768: training loss: 338.3196822784192\n",
      "Epoch 2 step 769: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 769: training loss: 345.69685066353344\n",
      "Epoch 2 step 770: training accuarcy: 0.9955\n",
      "Epoch 2 step 770: training loss: 331.6722159311154\n",
      "Epoch 2 step 771: training accuarcy: 0.9975\n",
      "Epoch 2 step 771: training loss: 318.5717784066992\n",
      "Epoch 2 step 772: training accuarcy: 0.9989\n",
      "Epoch 2 step 772: training loss: 304.64454724768274\n",
      "Epoch 2 step 773: training accuarcy: 0.9993000000000001\n",
      "Epoch 2 step 773: training loss: 339.63084325082866\n",
      "Epoch 2 step 774: training accuarcy: 0.9955\n",
      "Epoch 2 step 774: training loss: 341.6755270532709\n",
      "Epoch 2 step 775: training accuarcy: 0.997\n",
      "Epoch 2 step 775: training loss: 301.7743740648524\n",
      "Epoch 2 step 776: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 776: training loss: 326.94688439260335\n",
      "Epoch 2 step 777: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 777: training loss: 318.78065681837705\n",
      "Epoch 2 step 778: training accuarcy: 0.997\n",
      "Epoch 2 step 778: training loss: 343.08533286276105\n",
      "Epoch 2 step 779: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 779: training loss: 319.9862566148262\n",
      "Epoch 2 step 780: training accuarcy: 0.9975\n",
      "Epoch 2 step 780: training loss: 340.50833114911086\n",
      "Epoch 2 step 781: training accuarcy: 0.9968\n",
      "Epoch 2 step 781: training loss: 356.4449644833875\n",
      "Epoch 2 step 782: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 782: training loss: 325.2431180090905\n",
      "Epoch 2 step 783: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 783: training loss: 355.9937899533502\n",
      "Epoch 2 step 784: training accuarcy: 0.997\n",
      "Epoch 2 step 784: training loss: 354.0272731925885\n",
      "Epoch 2 step 785: training accuarcy: 0.9955\n",
      "Epoch 2 step 785: training loss: 312.1987452069394\n",
      "Epoch 2 step 786: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 786: training loss: 339.13717831656425\n",
      "Epoch 2 step 787: training accuarcy: 0.9967\n",
      "Epoch 2 step 787: training loss: 353.4691181950666\n",
      "Epoch 2 step 788: training accuarcy: 0.995\n",
      "Epoch 2 step 788: training loss: 267.00680214411557\n",
      "Epoch 2 step 789: training accuarcy: 0.9956410256410256\n",
      "Epoch 2: train loss 352.318687755944, train accuarcy 0.993584930896759\n",
      "Epoch 2: valid loss 626.6299229773125, valid accuarcy 0.9942793846130371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 3/5 [14:28<09:41, 290.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Epoch 3 step 789: training loss: 296.23032878304855\n",
      "Epoch 3 step 790: training accuarcy: 0.999\n",
      "Epoch 3 step 790: training loss: 300.7896643394858\n",
      "Epoch 3 step 791: training accuarcy: 0.9983000000000001\n",
      "Epoch 3 step 791: training loss: 293.4835120560487\n",
      "Epoch 3 step 792: training accuarcy: 0.9989\n",
      "Epoch 3 step 792: training loss: 285.5181151599255\n",
      "Epoch 3 step 793: training accuarcy: 0.9991000000000001\n",
      "Epoch 3 step 793: training loss: 302.478868166378\n",
      "Epoch 3 step 794: training accuarcy: 0.9982000000000001\n",
      "Epoch 3 step 794: training loss: 316.27082484827275\n",
      "Epoch 3 step 795: training accuarcy: 0.9989\n",
      "Epoch 3 step 795: training loss: 293.5194634860173\n",
      "Epoch 3 step 796: training accuarcy: 0.9977\n",
      "Epoch 3 step 796: training loss: 298.44195453152906\n",
      "Epoch 3 step 797: training accuarcy: 0.9985\n",
      "Epoch 3 step 797: training loss: 274.5905129792551\n",
      "Epoch 3 step 798: training accuarcy: 0.9989\n",
      "Epoch 3 step 798: training loss: 313.0679676110951\n",
      "Epoch 3 step 799: training accuarcy: 0.9989\n",
      "Epoch 3 step 799: training loss: 289.94282451757886\n",
      "Epoch 3 step 800: training accuarcy: 0.9988\n",
      "Epoch 3 step 800: training loss: 304.66987975262606\n",
      "Epoch 3 step 801: training accuarcy: 0.997\n",
      "Epoch 3 step 801: training loss: 290.99150960606505\n",
      "Epoch 3 step 802: training accuarcy: 0.9979\n",
      "Epoch 3 step 802: training loss: 303.28746394201846\n",
      "Epoch 3 step 803: training accuarcy: 0.9979\n",
      "Epoch 3 step 803: training loss: 278.6161234543749\n",
      "Epoch 3 step 804: training accuarcy: 0.9991000000000001\n",
      "Epoch 3 step 804: training loss: 307.3240067918505\n",
      "Epoch 3 step 805: training accuarcy: 0.9979\n",
      "Epoch 3 step 805: training loss: 294.73003690929716\n",
      "Epoch 3 step 806: training accuarcy: 0.9971000000000001\n",
      "Epoch 3 step 806: training loss: 306.44739375162226\n",
      "Epoch 3 step 807: training accuarcy: 0.9977\n",
      "Epoch 3 step 807: training loss: 302.0016683125134\n",
      "Epoch 3 step 808: training accuarcy: 0.9975\n",
      "Epoch 3 step 808: training loss: 281.5326337209339\n",
      "Epoch 3 step 809: training accuarcy: 0.9986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-30a929e26ac3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m fm_learner.fit(epoch=5,\n\u001b[1;32m----> 2\u001b[1;33m                log_dir=get_log_dir('simple_topcoder', 'fm'))\n\u001b[0m",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, epoch, log_dir)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mschedular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\models\\fm_learner.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muser_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\datasets\\torch_topcoder.py\u001b[0m in \u001b[0;36m_seq_collate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    201\u001b[0m             chag_df[chag_df['period'] == per].sample(n=per_counts[per],\n\u001b[0;32m    202\u001b[0m                                                      replace=True)\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mper\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mper_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         ]\n\u001b[0;32m    205\u001b[0m         \u001b[0mneg_feat_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\python\\recommender\\datasets\\torch_topcoder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    201\u001b[0m             chag_df[chag_df['period'] == per].sample(n=per_counts[per],\n\u001b[0;32m    202\u001b[0m                                                      replace=True)\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mper\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mper_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         ]\n\u001b[0;32m    205\u001b[0m         \u001b[0mneg_feat_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2916\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2918\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2920\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2967\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take\u001b[1;34m(self, indices, axis, is_copy)\u001b[0m\n\u001b[0;32m   3357\u001b[0m         new_data = self._data.take(indices,\n\u001b[0;32m   3358\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3359\u001b[1;33m                                    verify=True)\n\u001b[0m\u001b[0;32m   3360\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[1;32m-> 1350\u001b[1;33m                                     axis=axis, allow_dups=True)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1237\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[0mnew_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slice_take_blocks_ax0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_or_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rebuild_blknos_and_blklocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\python\\recommender\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_rebuild_blknos_and_blklocs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0mnew_blknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[0mnew_blklocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_blknos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=5,\n",
    "               log_dir=get_log_dir('simple_topcoder', 'fm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:56:07.718333Z",
     "start_time": "2019-10-09T11:41:44.391956Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 96099.51260676965\n",
      "Epoch 0 step 1: training accuarcy: 0.29510000000000003\n",
      "Epoch 0 step 1: training loss: 88335.88527898806\n",
      "Epoch 0 step 2: training accuarcy: 0.3618\n",
      "Epoch 0 step 2: training loss: 82955.90096143921\n",
      "Epoch 0 step 3: training accuarcy: 0.41050000000000003\n",
      "Epoch 0 step 3: training loss: 75841.65049929358\n",
      "Epoch 0 step 4: training accuarcy: 0.47500000000000003\n",
      "Epoch 0 step 4: training loss: 69900.84115374796\n",
      "Epoch 0 step 5: training accuarcy: 0.5316000000000001\n",
      "Epoch 0 step 5: training loss: 66515.16308581155\n",
      "Epoch 0 step 6: training accuarcy: 0.5718\n",
      "Epoch 0 step 6: training loss: 62505.37561871747\n",
      "Epoch 0 step 7: training accuarcy: 0.6056\n",
      "Epoch 0 step 7: training loss: 57537.948671329985\n",
      "Epoch 0 step 8: training accuarcy: 0.6546000000000001\n",
      "Epoch 0 step 8: training loss: 56306.74673244863\n",
      "Epoch 0 step 9: training accuarcy: 0.6639\n",
      "Epoch 0 step 9: training loss: 54004.51565162408\n",
      "Epoch 0 step 10: training accuarcy: 0.6841\n",
      "Epoch 0 step 10: training loss: 52092.79156344346\n",
      "Epoch 0 step 11: training accuarcy: 0.6947\n",
      "Epoch 0 step 11: training loss: 50497.44627196995\n",
      "Epoch 0 step 12: training accuarcy: 0.7148\n",
      "Epoch 0 step 12: training loss: 47546.291487897244\n",
      "Epoch 0 step 13: training accuarcy: 0.7436\n",
      "Epoch 0 step 13: training loss: 45964.037996165964\n",
      "Epoch 0 step 14: training accuarcy: 0.7632\n",
      "Epoch 0 step 14: training loss: 44430.30105638853\n",
      "Epoch 0 step 15: training accuarcy: 0.7721\n",
      "Epoch 0 step 15: training loss: 40924.010835627276\n",
      "Epoch 0 step 16: training accuarcy: 0.8109000000000001\n",
      "Epoch 0 step 16: training loss: 41229.720291221936\n",
      "Epoch 0 step 17: training accuarcy: 0.8054\n",
      "Epoch 0 step 17: training loss: 39660.46386698006\n",
      "Epoch 0 step 18: training accuarcy: 0.8180000000000001\n",
      "Epoch 0 step 18: training loss: 38270.73735652237\n",
      "Epoch 0 step 19: training accuarcy: 0.8273\n",
      "Epoch 0 step 19: training loss: 38662.085452056665\n",
      "Epoch 0 step 20: training accuarcy: 0.8112\n",
      "Epoch 0 step 20: training loss: 36002.56431866517\n",
      "Epoch 0 step 21: training accuarcy: 0.8411000000000001\n",
      "Epoch 0 step 21: training loss: 36748.02245214078\n",
      "Epoch 0 step 22: training accuarcy: 0.8440000000000001\n",
      "Epoch 0 step 22: training loss: 34816.30376877714\n",
      "Epoch 0 step 23: training accuarcy: 0.8547\n",
      "Epoch 0 step 23: training loss: 34413.769964374034\n",
      "Epoch 0 step 24: training accuarcy: 0.8544\n",
      "Epoch 0 step 24: training loss: 33353.930288466916\n",
      "Epoch 0 step 25: training accuarcy: 0.8564\n",
      "Epoch 0 step 25: training loss: 32390.97325353021\n",
      "Epoch 0 step 26: training accuarcy: 0.8659\n",
      "Epoch 0 step 26: training loss: 31799.210569694238\n",
      "Epoch 0 step 27: training accuarcy: 0.8725\n",
      "Epoch 0 step 27: training loss: 32223.745278635073\n",
      "Epoch 0 step 28: training accuarcy: 0.8617\n",
      "Epoch 0 step 28: training loss: 30613.122838401832\n",
      "Epoch 0 step 29: training accuarcy: 0.8781\n",
      "Epoch 0 step 29: training loss: 30037.00062051333\n",
      "Epoch 0 step 30: training accuarcy: 0.8855000000000001\n",
      "Epoch 0 step 30: training loss: 28916.69939003615\n",
      "Epoch 0 step 31: training accuarcy: 0.89\n",
      "Epoch 0 step 31: training loss: 29597.94337084506\n",
      "Epoch 0 step 32: training accuarcy: 0.8803000000000001\n",
      "Epoch 0 step 32: training loss: 29590.242222751673\n",
      "Epoch 0 step 33: training accuarcy: 0.8709\n",
      "Epoch 0 step 33: training loss: 28097.505616760987\n",
      "Epoch 0 step 34: training accuarcy: 0.8925000000000001\n",
      "Epoch 0 step 34: training loss: 28150.805828712546\n",
      "Epoch 0 step 35: training accuarcy: 0.889\n",
      "Epoch 0 step 35: training loss: 27233.659450903477\n",
      "Epoch 0 step 36: training accuarcy: 0.8976000000000001\n",
      "Epoch 0 step 36: training loss: 26624.133039187334\n",
      "Epoch 0 step 37: training accuarcy: 0.8973000000000001\n",
      "Epoch 0 step 37: training loss: 26503.81546120587\n",
      "Epoch 0 step 38: training accuarcy: 0.9001\n",
      "Epoch 0 step 38: training loss: 26488.246382310874\n",
      "Epoch 0 step 39: training accuarcy: 0.8926000000000001\n",
      "Epoch 0 step 39: training loss: 26020.141105323157\n",
      "Epoch 0 step 40: training accuarcy: 0.9004000000000001\n",
      "Epoch 0 step 40: training loss: 25317.482695624214\n",
      "Epoch 0 step 41: training accuarcy: 0.9106000000000001\n",
      "Epoch 0 step 41: training loss: 24997.46986938546\n",
      "Epoch 0 step 42: training accuarcy: 0.9088\n",
      "Epoch 0 step 42: training loss: 25720.19103453218\n",
      "Epoch 0 step 43: training accuarcy: 0.8967\n",
      "Epoch 0 step 43: training loss: 25692.01407790656\n",
      "Epoch 0 step 44: training accuarcy: 0.8949\n",
      "Epoch 0 step 44: training loss: 24794.65014836484\n",
      "Epoch 0 step 45: training accuarcy: 0.8985000000000001\n",
      "Epoch 0 step 45: training loss: 23879.490042990743\n",
      "Epoch 0 step 46: training accuarcy: 0.9071\n",
      "Epoch 0 step 46: training loss: 23098.69881980529\n",
      "Epoch 0 step 47: training accuarcy: 0.9193\n",
      "Epoch 0 step 47: training loss: 23372.88277172666\n",
      "Epoch 0 step 48: training accuarcy: 0.9117000000000001\n",
      "Epoch 0 step 48: training loss: 23253.937540642422\n",
      "Epoch 0 step 49: training accuarcy: 0.9098\n",
      "Epoch 0 step 49: training loss: 22841.621651341033\n",
      "Epoch 0 step 50: training accuarcy: 0.9114000000000001\n",
      "Epoch 0 step 50: training loss: 23357.760056842657\n",
      "Epoch 0 step 51: training accuarcy: 0.9076000000000001\n",
      "Epoch 0 step 51: training loss: 22356.596470092525\n",
      "Epoch 0 step 52: training accuarcy: 0.9185000000000001\n",
      "Epoch 0 step 52: training loss: 23360.514251395638\n",
      "Epoch 0 step 53: training accuarcy: 0.9006000000000001\n",
      "Epoch 0 step 53: training loss: 22326.663776895988\n",
      "Epoch 0 step 54: training accuarcy: 0.912\n",
      "Epoch 0 step 54: training loss: 21689.281521883106\n",
      "Epoch 0 step 55: training accuarcy: 0.9178000000000001\n",
      "Epoch 0 step 55: training loss: 21097.615564544372\n",
      "Epoch 0 step 56: training accuarcy: 0.9246000000000001\n",
      "Epoch 0 step 56: training loss: 22708.324555639578\n",
      "Epoch 0 step 57: training accuarcy: 0.9002\n",
      "Epoch 0 step 57: training loss: 20469.958688770454\n",
      "Epoch 0 step 58: training accuarcy: 0.9324\n",
      "Epoch 0 step 58: training loss: 21619.449955774486\n",
      "Epoch 0 step 59: training accuarcy: 0.9098\n",
      "Epoch 0 step 59: training loss: 20692.84302621839\n",
      "Epoch 0 step 60: training accuarcy: 0.93\n",
      "Epoch 0 step 60: training loss: 20359.31372995396\n",
      "Epoch 0 step 61: training accuarcy: 0.932\n",
      "Epoch 0 step 61: training loss: 20765.642503152034\n",
      "Epoch 0 step 62: training accuarcy: 0.9161\n",
      "Epoch 0 step 62: training loss: 20042.27445190447\n",
      "Epoch 0 step 63: training accuarcy: 0.9319000000000001\n",
      "Epoch 0 step 63: training loss: 20133.863765588285\n",
      "Epoch 0 step 64: training accuarcy: 0.9292\n",
      "Epoch 0 step 64: training loss: 20029.099930118544\n",
      "Epoch 0 step 65: training accuarcy: 0.9254\n",
      "Epoch 0 step 65: training loss: 19944.906890262784\n",
      "Epoch 0 step 66: training accuarcy: 0.9293\n",
      "Epoch 0 step 66: training loss: 19440.507952469794\n",
      "Epoch 0 step 67: training accuarcy: 0.9282\n",
      "Epoch 0 step 67: training loss: 19584.844403267394\n",
      "Epoch 0 step 68: training accuarcy: 0.9298000000000001\n",
      "Epoch 0 step 68: training loss: 19609.24674330537\n",
      "Epoch 0 step 69: training accuarcy: 0.9264\n",
      "Epoch 0 step 69: training loss: 18712.44428658543\n",
      "Epoch 0 step 70: training accuarcy: 0.9388000000000001\n",
      "Epoch 0 step 70: training loss: 18612.018486475565\n",
      "Epoch 0 step 71: training accuarcy: 0.9361\n",
      "Epoch 0 step 71: training loss: 19170.023134561176\n",
      "Epoch 0 step 72: training accuarcy: 0.9288000000000001\n",
      "Epoch 0 step 72: training loss: 18988.16521152961\n",
      "Epoch 0 step 73: training accuarcy: 0.9284\n",
      "Epoch 0 step 73: training loss: 18498.454600846162\n",
      "Epoch 0 step 74: training accuarcy: 0.9376000000000001\n",
      "Epoch 0 step 74: training loss: 17761.195573976598\n",
      "Epoch 0 step 75: training accuarcy: 0.9457000000000001\n",
      "Epoch 0 step 75: training loss: 17561.367685931862\n",
      "Epoch 0 step 76: training accuarcy: 0.9436\n",
      "Epoch 0 step 76: training loss: 17814.685101782987\n",
      "Epoch 0 step 77: training accuarcy: 0.9378000000000001\n",
      "Epoch 0 step 77: training loss: 18242.89641170801\n",
      "Epoch 0 step 78: training accuarcy: 0.9383\n",
      "Epoch 0 step 78: training loss: 17366.3679513772\n",
      "Epoch 0 step 79: training accuarcy: 0.9452\n",
      "Epoch 0 step 79: training loss: 16763.451861627756\n",
      "Epoch 0 step 80: training accuarcy: 0.9473\n",
      "Epoch 0 step 80: training loss: 17898.783786915672\n",
      "Epoch 0 step 81: training accuarcy: 0.9351\n",
      "Epoch 0 step 81: training loss: 17485.37705481673\n",
      "Epoch 0 step 82: training accuarcy: 0.9401\n",
      "Epoch 0 step 82: training loss: 17368.558014366747\n",
      "Epoch 0 step 83: training accuarcy: 0.9436\n",
      "Epoch 0 step 83: training loss: 17305.680949592283\n",
      "Epoch 0 step 84: training accuarcy: 0.9396\n",
      "Epoch 0 step 84: training loss: 17522.18548765844\n",
      "Epoch 0 step 85: training accuarcy: 0.9384\n",
      "Epoch 0 step 85: training loss: 17190.615107079884\n",
      "Epoch 0 step 86: training accuarcy: 0.9427000000000001\n",
      "Epoch 0 step 86: training loss: 17452.658316554072\n",
      "Epoch 0 step 87: training accuarcy: 0.9365\n",
      "Epoch 0 step 87: training loss: 16615.43538274709\n",
      "Epoch 0 step 88: training accuarcy: 0.9475\n",
      "Epoch 0 step 88: training loss: 17267.05967346716\n",
      "Epoch 0 step 89: training accuarcy: 0.9379000000000001\n",
      "Epoch 0 step 89: training loss: 17218.19447258259\n",
      "Epoch 0 step 90: training accuarcy: 0.9376000000000001\n",
      "Epoch 0 step 90: training loss: 17251.018886259422\n",
      "Epoch 0 step 91: training accuarcy: 0.9377000000000001\n",
      "Epoch 0 step 91: training loss: 16880.034898250327\n",
      "Epoch 0 step 92: training accuarcy: 0.9408000000000001\n",
      "Epoch 0 step 92: training loss: 17046.815792650184\n",
      "Epoch 0 step 93: training accuarcy: 0.9420000000000001\n",
      "Epoch 0 step 93: training loss: 16981.366729458292\n",
      "Epoch 0 step 94: training accuarcy: 0.9410000000000001\n",
      "Epoch 0 step 94: training loss: 16582.176131456225\n",
      "Epoch 0 step 95: training accuarcy: 0.9408000000000001\n",
      "Epoch 0 step 95: training loss: 16524.09528657326\n",
      "Epoch 0 step 96: training accuarcy: 0.9478000000000001\n",
      "Epoch 0 step 96: training loss: 16392.848802019846\n",
      "Epoch 0 step 97: training accuarcy: 0.9455\n",
      "Epoch 0 step 97: training loss: 15938.563089904595\n",
      "Epoch 0 step 98: training accuarcy: 0.9471\n",
      "Epoch 0 step 98: training loss: 16142.910312748667\n",
      "Epoch 0 step 99: training accuarcy: 0.9462\n",
      "Epoch 0 step 99: training loss: 15464.642470222421\n",
      "Epoch 0 step 100: training accuarcy: 0.9518000000000001\n",
      "Epoch 0 step 100: training loss: 15721.953138874771\n",
      "Epoch 0 step 101: training accuarcy: 0.9537\n",
      "Epoch 0 step 101: training loss: 15250.317636316146\n",
      "Epoch 0 step 102: training accuarcy: 0.9604\n",
      "Epoch 0 step 102: training loss: 15546.220233197277\n",
      "Epoch 0 step 103: training accuarcy: 0.9526\n",
      "Epoch 0 step 103: training loss: 15057.185484768532\n",
      "Epoch 0 step 104: training accuarcy: 0.9601000000000001\n",
      "Epoch 0 step 104: training loss: 15200.358839278993\n",
      "Epoch 0 step 105: training accuarcy: 0.9554\n",
      "Epoch 0 step 105: training loss: 15433.620181630098\n",
      "Epoch 0 step 106: training accuarcy: 0.9503\n",
      "Epoch 0 step 106: training loss: 15788.511535800599\n",
      "Epoch 0 step 107: training accuarcy: 0.9467000000000001\n",
      "Epoch 0 step 107: training loss: 15105.331458631723\n",
      "Epoch 0 step 108: training accuarcy: 0.9563\n",
      "Epoch 0 step 108: training loss: 15421.752858195536\n",
      "Epoch 0 step 109: training accuarcy: 0.9479000000000001\n",
      "Epoch 0 step 109: training loss: 15026.99867882596\n",
      "Epoch 0 step 110: training accuarcy: 0.9565\n",
      "Epoch 0 step 110: training loss: 15712.21122219024\n",
      "Epoch 0 step 111: training accuarcy: 0.9456\n",
      "Epoch 0 step 111: training loss: 15294.682989028199\n",
      "Epoch 0 step 112: training accuarcy: 0.9476\n",
      "Epoch 0 step 112: training loss: 14891.876549584158\n",
      "Epoch 0 step 113: training accuarcy: 0.9552\n",
      "Epoch 0 step 113: training loss: 15719.811242344418\n",
      "Epoch 0 step 114: training accuarcy: 0.9438000000000001\n",
      "Epoch 0 step 114: training loss: 15071.44911293159\n",
      "Epoch 0 step 115: training accuarcy: 0.9514\n",
      "Epoch 0 step 115: training loss: 15042.630888428414\n",
      "Epoch 0 step 116: training accuarcy: 0.9532\n",
      "Epoch 0 step 116: training loss: 15188.284222677748\n",
      "Epoch 0 step 117: training accuarcy: 0.9491\n",
      "Epoch 0 step 117: training loss: 15180.24757263358\n",
      "Epoch 0 step 118: training accuarcy: 0.9485\n",
      "Epoch 0 step 118: training loss: 14923.722516492588\n",
      "Epoch 0 step 119: training accuarcy: 0.9528000000000001\n",
      "Epoch 0 step 119: training loss: 14722.222945991332\n",
      "Epoch 0 step 120: training accuarcy: 0.9555\n",
      "Epoch 0 step 120: training loss: 14603.571899187427\n",
      "Epoch 0 step 121: training accuarcy: 0.9522\n",
      "Epoch 0 step 121: training loss: 14645.952224493853\n",
      "Epoch 0 step 122: training accuarcy: 0.9552\n",
      "Epoch 0 step 122: training loss: 15137.655515681376\n",
      "Epoch 0 step 123: training accuarcy: 0.9490000000000001\n",
      "Epoch 0 step 123: training loss: 14161.958357887328\n",
      "Epoch 0 step 124: training accuarcy: 0.9603\n",
      "Epoch 0 step 124: training loss: 14009.2638075811\n",
      "Epoch 0 step 125: training accuarcy: 0.9597\n",
      "Epoch 0 step 125: training loss: 14522.44045160238\n",
      "Epoch 0 step 126: training accuarcy: 0.9550000000000001\n",
      "Epoch 0 step 126: training loss: 13956.606421151764\n",
      "Epoch 0 step 127: training accuarcy: 0.9611000000000001\n",
      "Epoch 0 step 127: training loss: 14093.17414597288\n",
      "Epoch 0 step 128: training accuarcy: 0.9612\n",
      "Epoch 0 step 128: training loss: 14459.995997556238\n",
      "Epoch 0 step 129: training accuarcy: 0.9556\n",
      "Epoch 0 step 129: training loss: 14143.107892010754\n",
      "Epoch 0 step 130: training accuarcy: 0.9584\n",
      "Epoch 0 step 130: training loss: 13671.56552596375\n",
      "Epoch 0 step 131: training accuarcy: 0.9626\n",
      "Epoch 0 step 131: training loss: 14453.469142539747\n",
      "Epoch 0 step 132: training accuarcy: 0.9524\n",
      "Epoch 0 step 132: training loss: 14165.397096180011\n",
      "Epoch 0 step 133: training accuarcy: 0.9574\n",
      "Epoch 0 step 133: training loss: 14192.278586091621\n",
      "Epoch 0 step 134: training accuarcy: 0.9567\n",
      "Epoch 0 step 134: training loss: 13915.658593014956\n",
      "Epoch 0 step 135: training accuarcy: 0.9600000000000001\n",
      "Epoch 0 step 135: training loss: 13349.218114891977\n",
      "Epoch 0 step 136: training accuarcy: 0.9676\n",
      "Epoch 0 step 136: training loss: 14225.006677352874\n",
      "Epoch 0 step 137: training accuarcy: 0.9557\n",
      "Epoch 0 step 137: training loss: 13990.560980495658\n",
      "Epoch 0 step 138: training accuarcy: 0.9577\n",
      "Epoch 0 step 138: training loss: 13638.016521656995\n",
      "Epoch 0 step 139: training accuarcy: 0.9616\n",
      "Epoch 0 step 139: training loss: 13536.911959251898\n",
      "Epoch 0 step 140: training accuarcy: 0.9658\n",
      "Epoch 0 step 140: training loss: 13903.48800547942\n",
      "Epoch 0 step 141: training accuarcy: 0.9569000000000001\n",
      "Epoch 0 step 141: training loss: 12972.741660654992\n",
      "Epoch 0 step 142: training accuarcy: 0.9695\n",
      "Epoch 0 step 142: training loss: 13223.448164209352\n",
      "Epoch 0 step 143: training accuarcy: 0.9628000000000001\n",
      "Epoch 0 step 143: training loss: 13551.705939639953\n",
      "Epoch 0 step 144: training accuarcy: 0.9634\n",
      "Epoch 0 step 144: training loss: 12835.0353162918\n",
      "Epoch 0 step 145: training accuarcy: 0.9668\n",
      "Epoch 0 step 145: training loss: 13143.301310964816\n",
      "Epoch 0 step 146: training accuarcy: 0.9649000000000001\n",
      "Epoch 0 step 146: training loss: 13661.443958433163\n",
      "Epoch 0 step 147: training accuarcy: 0.9620000000000001\n",
      "Epoch 0 step 147: training loss: 13109.87439975031\n",
      "Epoch 0 step 148: training accuarcy: 0.9656\n",
      "Epoch 0 step 148: training loss: 13265.77477425618\n",
      "Epoch 0 step 149: training accuarcy: 0.9637\n",
      "Epoch 0 step 149: training loss: 13666.992427817057\n",
      "Epoch 0 step 150: training accuarcy: 0.9546\n",
      "Epoch 0 step 150: training loss: 13664.489506068508\n",
      "Epoch 0 step 151: training accuarcy: 0.9576\n",
      "Epoch 0 step 151: training loss: 13181.564582939647\n",
      "Epoch 0 step 152: training accuarcy: 0.9586\n",
      "Epoch 0 step 152: training loss: 13493.625794894691\n",
      "Epoch 0 step 153: training accuarcy: 0.9598000000000001\n",
      "Epoch 0 step 153: training loss: 13241.214839970417\n",
      "Epoch 0 step 154: training accuarcy: 0.9604\n",
      "Epoch 0 step 154: training loss: 12612.68920207381\n",
      "Epoch 0 step 155: training accuarcy: 0.9710000000000001\n",
      "Epoch 0 step 155: training loss: 12495.329823242391\n",
      "Epoch 0 step 156: training accuarcy: 0.9700000000000001\n",
      "Epoch 0 step 156: training loss: 13115.609290902139\n",
      "Epoch 0 step 157: training accuarcy: 0.9592\n",
      "Epoch 0 step 157: training loss: 12729.939956163074\n",
      "Epoch 0 step 158: training accuarcy: 0.9657\n",
      "Epoch 0 step 158: training loss: 12791.807752566143\n",
      "Epoch 0 step 159: training accuarcy: 0.9656\n",
      "Epoch 0 step 159: training loss: 12706.58528443638\n",
      "Epoch 0 step 160: training accuarcy: 0.9681000000000001\n",
      "Epoch 0 step 160: training loss: 12981.718605545873\n",
      "Epoch 0 step 161: training accuarcy: 0.9651000000000001\n",
      "Epoch 0 step 161: training loss: 12518.647232851972\n",
      "Epoch 0 step 162: training accuarcy: 0.9665\n",
      "Epoch 0 step 162: training loss: 12884.350636051544\n",
      "Epoch 0 step 163: training accuarcy: 0.9632000000000001\n",
      "Epoch 0 step 163: training loss: 12605.36906451976\n",
      "Epoch 0 step 164: training accuarcy: 0.9649000000000001\n",
      "Epoch 0 step 164: training loss: 12195.54440208216\n",
      "Epoch 0 step 165: training accuarcy: 0.9711000000000001\n",
      "Epoch 0 step 165: training loss: 12576.814590101403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 166: training accuarcy: 0.9649000000000001\n",
      "Epoch 0 step 166: training loss: 12025.231994584245\n",
      "Epoch 0 step 167: training accuarcy: 0.9740000000000001\n",
      "Epoch 0 step 167: training loss: 12975.623940105927\n",
      "Epoch 0 step 168: training accuarcy: 0.9597\n",
      "Epoch 0 step 168: training loss: 12528.237092144336\n",
      "Epoch 0 step 169: training accuarcy: 0.9664\n",
      "Epoch 0 step 169: training loss: 12116.927013772427\n",
      "Epoch 0 step 170: training accuarcy: 0.9694\n",
      "Epoch 0 step 170: training loss: 12190.54159559381\n",
      "Epoch 0 step 171: training accuarcy: 0.9705\n",
      "Epoch 0 step 171: training loss: 11859.6976563459\n",
      "Epoch 0 step 172: training accuarcy: 0.9723\n",
      "Epoch 0 step 172: training loss: 12199.927979699658\n",
      "Epoch 0 step 173: training accuarcy: 0.9687\n",
      "Epoch 0 step 173: training loss: 12409.036370485526\n",
      "Epoch 0 step 174: training accuarcy: 0.9647\n",
      "Epoch 0 step 174: training loss: 11917.692596284536\n",
      "Epoch 0 step 175: training accuarcy: 0.9752000000000001\n",
      "Epoch 0 step 175: training loss: 12528.170015238262\n",
      "Epoch 0 step 176: training accuarcy: 0.9663\n",
      "Epoch 0 step 176: training loss: 12419.433897772411\n",
      "Epoch 0 step 177: training accuarcy: 0.9627\n",
      "Epoch 0 step 177: training loss: 12372.508188206912\n",
      "Epoch 0 step 178: training accuarcy: 0.9646\n",
      "Epoch 0 step 178: training loss: 12037.425587410624\n",
      "Epoch 0 step 179: training accuarcy: 0.9695\n",
      "Epoch 0 step 179: training loss: 12214.717546438593\n",
      "Epoch 0 step 180: training accuarcy: 0.9673\n",
      "Epoch 0 step 180: training loss: 12006.220994774696\n",
      "Epoch 0 step 181: training accuarcy: 0.9687\n",
      "Epoch 0 step 181: training loss: 11765.084526515055\n",
      "Epoch 0 step 182: training accuarcy: 0.9700000000000001\n",
      "Epoch 0 step 182: training loss: 11564.1940765397\n",
      "Epoch 0 step 183: training accuarcy: 0.9731000000000001\n",
      "Epoch 0 step 183: training loss: 12130.032757621046\n",
      "Epoch 0 step 184: training accuarcy: 0.9646\n",
      "Epoch 0 step 184: training loss: 11858.035322307744\n",
      "Epoch 0 step 185: training accuarcy: 0.9684\n",
      "Epoch 0 step 185: training loss: 11542.629519501435\n",
      "Epoch 0 step 186: training accuarcy: 0.9737\n",
      "Epoch 0 step 186: training loss: 11603.873981131283\n",
      "Epoch 0 step 187: training accuarcy: 0.9700000000000001\n",
      "Epoch 0 step 187: training loss: 11566.553030727833\n",
      "Epoch 0 step 188: training accuarcy: 0.9724\n",
      "Epoch 0 step 188: training loss: 11470.838870963382\n",
      "Epoch 0 step 189: training accuarcy: 0.9723\n",
      "Epoch 0 step 189: training loss: 12271.066339657078\n",
      "Epoch 0 step 190: training accuarcy: 0.9603\n",
      "Epoch 0 step 190: training loss: 11508.779428013382\n",
      "Epoch 0 step 191: training accuarcy: 0.9769000000000001\n",
      "Epoch 0 step 191: training loss: 11413.224444810152\n",
      "Epoch 0 step 192: training accuarcy: 0.9732000000000001\n",
      "Epoch 0 step 192: training loss: 11511.259473806453\n",
      "Epoch 0 step 193: training accuarcy: 0.9698\n",
      "Epoch 0 step 193: training loss: 11238.234921292575\n",
      "Epoch 0 step 194: training accuarcy: 0.9746\n",
      "Epoch 0 step 194: training loss: 11891.929029493533\n",
      "Epoch 0 step 195: training accuarcy: 0.9641000000000001\n",
      "Epoch 0 step 195: training loss: 11735.260678061117\n",
      "Epoch 0 step 196: training accuarcy: 0.9687\n",
      "Epoch 0 step 196: training loss: 11554.809953225105\n",
      "Epoch 0 step 197: training accuarcy: 0.9711000000000001\n",
      "Epoch 0 step 197: training loss: 11279.135287598985\n",
      "Epoch 0 step 198: training accuarcy: 0.9731000000000001\n",
      "Epoch 0 step 198: training loss: 11067.754269954658\n",
      "Epoch 0 step 199: training accuarcy: 0.9763000000000001\n",
      "Epoch 0 step 199: training loss: 11429.263487858741\n",
      "Epoch 0 step 200: training accuarcy: 0.9693\n",
      "Epoch 0 step 200: training loss: 11390.260001822182\n",
      "Epoch 0 step 201: training accuarcy: 0.9706\n",
      "Epoch 0 step 201: training loss: 11669.929721883165\n",
      "Epoch 0 step 202: training accuarcy: 0.9672000000000001\n",
      "Epoch 0 step 202: training loss: 11372.54907509447\n",
      "Epoch 0 step 203: training accuarcy: 0.9707\n",
      "Epoch 0 step 203: training loss: 11546.01089018879\n",
      "Epoch 0 step 204: training accuarcy: 0.9711000000000001\n",
      "Epoch 0 step 204: training loss: 11012.79408319137\n",
      "Epoch 0 step 205: training accuarcy: 0.9751000000000001\n",
      "Epoch 0 step 205: training loss: 11303.053260536684\n",
      "Epoch 0 step 206: training accuarcy: 0.9696\n",
      "Epoch 0 step 206: training loss: 10659.71616331544\n",
      "Epoch 0 step 207: training accuarcy: 0.9785\n",
      "Epoch 0 step 207: training loss: 10769.516128772602\n",
      "Epoch 0 step 208: training accuarcy: 0.9769000000000001\n",
      "Epoch 0 step 208: training loss: 11065.56634820042\n",
      "Epoch 0 step 209: training accuarcy: 0.9712000000000001\n",
      "Epoch 0 step 209: training loss: 11093.818176822124\n",
      "Epoch 0 step 210: training accuarcy: 0.9703\n",
      "Epoch 0 step 210: training loss: 11052.878490730827\n",
      "Epoch 0 step 211: training accuarcy: 0.9733\n",
      "Epoch 0 step 211: training loss: 10733.324894337024\n",
      "Epoch 0 step 212: training accuarcy: 0.9774\n",
      "Epoch 0 step 212: training loss: 10671.935809319406\n",
      "Epoch 0 step 213: training accuarcy: 0.9776\n",
      "Epoch 0 step 213: training loss: 11136.504808698895\n",
      "Epoch 0 step 214: training accuarcy: 0.9742000000000001\n",
      "Epoch 0 step 214: training loss: 10498.837992165045\n",
      "Epoch 0 step 215: training accuarcy: 0.9784\n",
      "Epoch 0 step 215: training loss: 10680.466843025632\n",
      "Epoch 0 step 216: training accuarcy: 0.9762000000000001\n",
      "Epoch 0 step 216: training loss: 10939.72038022563\n",
      "Epoch 0 step 217: training accuarcy: 0.9750000000000001\n",
      "Epoch 0 step 217: training loss: 11328.31088957916\n",
      "Epoch 0 step 218: training accuarcy: 0.9665\n",
      "Epoch 0 step 218: training loss: 10591.302725271662\n",
      "Epoch 0 step 219: training accuarcy: 0.9795\n",
      "Epoch 0 step 219: training loss: 11012.654055277975\n",
      "Epoch 0 step 220: training accuarcy: 0.9716\n",
      "Epoch 0 step 220: training loss: 10720.71278778989\n",
      "Epoch 0 step 221: training accuarcy: 0.9732000000000001\n",
      "Epoch 0 step 221: training loss: 10602.37240817212\n",
      "Epoch 0 step 222: training accuarcy: 0.9767\n",
      "Epoch 0 step 222: training loss: 10909.567224610862\n",
      "Epoch 0 step 223: training accuarcy: 0.9688\n",
      "Epoch 0 step 223: training loss: 10672.273837644836\n",
      "Epoch 0 step 224: training accuarcy: 0.9763000000000001\n",
      "Epoch 0 step 224: training loss: 10426.512844546935\n",
      "Epoch 0 step 225: training accuarcy: 0.9769000000000001\n",
      "Epoch 0 step 225: training loss: 10207.451875273588\n",
      "Epoch 0 step 226: training accuarcy: 0.9794\n",
      "Epoch 0 step 226: training loss: 10589.67952646756\n",
      "Epoch 0 step 227: training accuarcy: 0.9756\n",
      "Epoch 0 step 227: training loss: 10465.71676126993\n",
      "Epoch 0 step 228: training accuarcy: 0.9770000000000001\n",
      "Epoch 0 step 228: training loss: 10467.50865536282\n",
      "Epoch 0 step 229: training accuarcy: 0.9768\n",
      "Epoch 0 step 229: training loss: 10542.935256357523\n",
      "Epoch 0 step 230: training accuarcy: 0.9730000000000001\n",
      "Epoch 0 step 230: training loss: 10554.70579242522\n",
      "Epoch 0 step 231: training accuarcy: 0.9744\n",
      "Epoch 0 step 231: training loss: 10433.059968583104\n",
      "Epoch 0 step 232: training accuarcy: 0.9770000000000001\n",
      "Epoch 0 step 232: training loss: 10508.198805289281\n",
      "Epoch 0 step 233: training accuarcy: 0.9736\n",
      "Epoch 0 step 233: training loss: 10021.950350062953\n",
      "Epoch 0 step 234: training accuarcy: 0.9797\n",
      "Epoch 0 step 234: training loss: 10374.736424283268\n",
      "Epoch 0 step 235: training accuarcy: 0.9740000000000001\n",
      "Epoch 0 step 235: training loss: 10146.324856118088\n",
      "Epoch 0 step 236: training accuarcy: 0.9776\n",
      "Epoch 0 step 236: training loss: 10361.999331314177\n",
      "Epoch 0 step 237: training accuarcy: 0.9750000000000001\n",
      "Epoch 0 step 237: training loss: 10114.320540083108\n",
      "Epoch 0 step 238: training accuarcy: 0.9776\n",
      "Epoch 0 step 238: training loss: 10492.605423289802\n",
      "Epoch 0 step 239: training accuarcy: 0.9705\n",
      "Epoch 0 step 239: training loss: 10368.608010637743\n",
      "Epoch 0 step 240: training accuarcy: 0.9723\n",
      "Epoch 0 step 240: training loss: 9943.705751937892\n",
      "Epoch 0 step 241: training accuarcy: 0.9776\n",
      "Epoch 0 step 241: training loss: 10127.286638427426\n",
      "Epoch 0 step 242: training accuarcy: 0.9768\n",
      "Epoch 0 step 242: training loss: 9777.243165415666\n",
      "Epoch 0 step 243: training accuarcy: 0.9823000000000001\n",
      "Epoch 0 step 243: training loss: 10276.156721707011\n",
      "Epoch 0 step 244: training accuarcy: 0.9773000000000001\n",
      "Epoch 0 step 244: training loss: 10034.01735636959\n",
      "Epoch 0 step 245: training accuarcy: 0.9753000000000001\n",
      "Epoch 0 step 245: training loss: 10085.943107265768\n",
      "Epoch 0 step 246: training accuarcy: 0.9780000000000001\n",
      "Epoch 0 step 246: training loss: 9771.859453765355\n",
      "Epoch 0 step 247: training accuarcy: 0.9782000000000001\n",
      "Epoch 0 step 247: training loss: 9721.948464146415\n",
      "Epoch 0 step 248: training accuarcy: 0.9806\n",
      "Epoch 0 step 248: training loss: 9823.732362416913\n",
      "Epoch 0 step 249: training accuarcy: 0.9808\n",
      "Epoch 0 step 249: training loss: 9744.033195292506\n",
      "Epoch 0 step 250: training accuarcy: 0.9796\n",
      "Epoch 0 step 250: training loss: 9772.963041826892\n",
      "Epoch 0 step 251: training accuarcy: 0.9773000000000001\n",
      "Epoch 0 step 251: training loss: 10060.710117865334\n",
      "Epoch 0 step 252: training accuarcy: 0.9766\n",
      "Epoch 0 step 252: training loss: 9693.226185739817\n",
      "Epoch 0 step 253: training accuarcy: 0.9778\n",
      "Epoch 0 step 253: training loss: 9905.918822931282\n",
      "Epoch 0 step 254: training accuarcy: 0.9758\n",
      "Epoch 0 step 254: training loss: 9470.095188920608\n",
      "Epoch 0 step 255: training accuarcy: 0.9821000000000001\n",
      "Epoch 0 step 255: training loss: 9617.223845037699\n",
      "Epoch 0 step 256: training accuarcy: 0.9786\n",
      "Epoch 0 step 256: training loss: 9699.16240197113\n",
      "Epoch 0 step 257: training accuarcy: 0.9782000000000001\n",
      "Epoch 0 step 257: training loss: 9336.635739774216\n",
      "Epoch 0 step 258: training accuarcy: 0.9818\n",
      "Epoch 0 step 258: training loss: 9404.36891035516\n",
      "Epoch 0 step 259: training accuarcy: 0.9843000000000001\n",
      "Epoch 0 step 259: training loss: 9799.416559065854\n",
      "Epoch 0 step 260: training accuarcy: 0.9776\n",
      "Epoch 0 step 260: training loss: 9458.733748646097\n",
      "Epoch 0 step 261: training accuarcy: 0.9801000000000001\n",
      "Epoch 0 step 261: training loss: 9256.330211831288\n",
      "Epoch 0 step 262: training accuarcy: 0.9843000000000001\n",
      "Epoch 0 step 262: training loss: 8292.971848743331\n",
      "Epoch 0 step 263: training accuarcy: 0.9794871794871794\n",
      "Epoch 0: train loss 18720.79175424917, train accuarcy 0.9062932133674622\n",
      "Epoch 0: valid loss 11510.802419596064, valid accuarcy 0.9607641696929932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|█████████████████████████████████████████████████████████▎                                                                                                                  | 1/3 [04:50<09:41, 290.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 8943.806104887635\n",
      "Epoch 1 step 264: training accuarcy: 0.9852000000000001\n",
      "Epoch 1 step 264: training loss: 8739.16893308108\n",
      "Epoch 1 step 265: training accuarcy: 0.9881000000000001\n",
      "Epoch 1 step 265: training loss: 8939.825616020917\n",
      "Epoch 1 step 266: training accuarcy: 0.9856\n",
      "Epoch 1 step 266: training loss: 8735.961095957951\n",
      "Epoch 1 step 267: training accuarcy: 0.9875\n",
      "Epoch 1 step 267: training loss: 8881.61574397262\n",
      "Epoch 1 step 268: training accuarcy: 0.9870000000000001\n",
      "Epoch 1 step 268: training loss: 8927.158958352806\n",
      "Epoch 1 step 269: training accuarcy: 0.9848\n",
      "Epoch 1 step 269: training loss: 8834.69927256897\n",
      "Epoch 1 step 270: training accuarcy: 0.9830000000000001\n",
      "Epoch 1 step 270: training loss: 8841.092411649668\n",
      "Epoch 1 step 271: training accuarcy: 0.9851000000000001\n",
      "Epoch 1 step 271: training loss: 8631.794868342997\n",
      "Epoch 1 step 272: training accuarcy: 0.9862000000000001\n",
      "Epoch 1 step 272: training loss: 8458.694663804956\n",
      "Epoch 1 step 273: training accuarcy: 0.9899\n",
      "Epoch 1 step 273: training loss: 8402.805924040107\n",
      "Epoch 1 step 274: training accuarcy: 0.9888\n",
      "Epoch 1 step 274: training loss: 8561.221411214618\n",
      "Epoch 1 step 275: training accuarcy: 0.9900000000000001\n",
      "Epoch 1 step 275: training loss: 8616.648463012181\n",
      "Epoch 1 step 276: training accuarcy: 0.9886\n",
      "Epoch 1 step 276: training loss: 8549.2860729817\n",
      "Epoch 1 step 277: training accuarcy: 0.9881000000000001\n",
      "Epoch 1 step 277: training loss: 8517.731821443747\n",
      "Epoch 1 step 278: training accuarcy: 0.9865\n",
      "Epoch 1 step 278: training loss: 8620.158624188134\n",
      "Epoch 1 step 279: training accuarcy: 0.9865\n",
      "Epoch 1 step 279: training loss: 8252.224077047504\n",
      "Epoch 1 step 280: training accuarcy: 0.9899\n",
      "Epoch 1 step 280: training loss: 8321.355522748572\n",
      "Epoch 1 step 281: training accuarcy: 0.9881000000000001\n",
      "Epoch 1 step 281: training loss: 8546.927612961941\n",
      "Epoch 1 step 282: training accuarcy: 0.9841000000000001\n",
      "Epoch 1 step 282: training loss: 8082.438600832669\n",
      "Epoch 1 step 283: training accuarcy: 0.9908\n",
      "Epoch 1 step 283: training loss: 8328.178234232171\n",
      "Epoch 1 step 284: training accuarcy: 0.9884000000000001\n",
      "Epoch 1 step 284: training loss: 8255.70390190783\n",
      "Epoch 1 step 285: training accuarcy: 0.9884000000000001\n",
      "Epoch 1 step 285: training loss: 8028.33617262362\n",
      "Epoch 1 step 286: training accuarcy: 0.9915\n",
      "Epoch 1 step 286: training loss: 8124.581471358369\n",
      "Epoch 1 step 287: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 287: training loss: 8266.034317614765\n",
      "Epoch 1 step 288: training accuarcy: 0.9854\n",
      "Epoch 1 step 288: training loss: 8208.738095550096\n",
      "Epoch 1 step 289: training accuarcy: 0.9876\n",
      "Epoch 1 step 289: training loss: 8530.001724007407\n",
      "Epoch 1 step 290: training accuarcy: 0.9819\n",
      "Epoch 1 step 290: training loss: 8551.172267036209\n",
      "Epoch 1 step 291: training accuarcy: 0.9822000000000001\n",
      "Epoch 1 step 291: training loss: 8322.754656292538\n",
      "Epoch 1 step 292: training accuarcy: 0.9865\n",
      "Epoch 1 step 292: training loss: 8114.007475052497\n",
      "Epoch 1 step 293: training accuarcy: 0.9864\n",
      "Epoch 1 step 293: training loss: 7986.890944535151\n",
      "Epoch 1 step 294: training accuarcy: 0.9907\n",
      "Epoch 1 step 294: training loss: 8341.10623887905\n",
      "Epoch 1 step 295: training accuarcy: 0.9868\n",
      "Epoch 1 step 295: training loss: 8551.565963001542\n",
      "Epoch 1 step 296: training accuarcy: 0.9824\n",
      "Epoch 1 step 296: training loss: 8027.46692676193\n",
      "Epoch 1 step 297: training accuarcy: 0.9898\n",
      "Epoch 1 step 297: training loss: 8371.83554750462\n",
      "Epoch 1 step 298: training accuarcy: 0.9847\n",
      "Epoch 1 step 298: training loss: 7986.819186736468\n",
      "Epoch 1 step 299: training accuarcy: 0.9884000000000001\n",
      "Epoch 1 step 299: training loss: 8365.537747217091\n",
      "Epoch 1 step 300: training accuarcy: 0.9850000000000001\n",
      "Epoch 1 step 300: training loss: 8057.062598130377\n",
      "Epoch 1 step 301: training accuarcy: 0.9872000000000001\n",
      "Epoch 1 step 301: training loss: 8185.100411781701\n",
      "Epoch 1 step 302: training accuarcy: 0.9879\n",
      "Epoch 1 step 302: training loss: 7894.208844132833\n",
      "Epoch 1 step 303: training accuarcy: 0.991\n",
      "Epoch 1 step 303: training loss: 8060.78326461716\n",
      "Epoch 1 step 304: training accuarcy: 0.9875\n",
      "Epoch 1 step 304: training loss: 7963.359951127811\n",
      "Epoch 1 step 305: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 305: training loss: 8165.718719998613\n",
      "Epoch 1 step 306: training accuarcy: 0.9871000000000001\n",
      "Epoch 1 step 306: training loss: 7658.488026731717\n",
      "Epoch 1 step 307: training accuarcy: 0.9913000000000001\n",
      "Epoch 1 step 307: training loss: 7871.680790631116\n",
      "Epoch 1 step 308: training accuarcy: 0.9899\n",
      "Epoch 1 step 308: training loss: 7863.706967783619\n",
      "Epoch 1 step 309: training accuarcy: 0.9882000000000001\n",
      "Epoch 1 step 309: training loss: 7908.104044718812\n",
      "Epoch 1 step 310: training accuarcy: 0.9873000000000001\n",
      "Epoch 1 step 310: training loss: 7878.129297672805\n",
      "Epoch 1 step 311: training accuarcy: 0.9854\n",
      "Epoch 1 step 311: training loss: 8001.051849734138\n",
      "Epoch 1 step 312: training accuarcy: 0.9855\n",
      "Epoch 1 step 312: training loss: 7868.503315830117\n",
      "Epoch 1 step 313: training accuarcy: 0.9878\n",
      "Epoch 1 step 313: training loss: 7786.796071054669\n",
      "Epoch 1 step 314: training accuarcy: 0.9896\n",
      "Epoch 1 step 314: training loss: 7643.549038541394\n",
      "Epoch 1 step 315: training accuarcy: 0.9902000000000001\n",
      "Epoch 1 step 315: training loss: 7632.387119456839\n",
      "Epoch 1 step 316: training accuarcy: 0.9909\n",
      "Epoch 1 step 316: training loss: 7757.823738250553\n",
      "Epoch 1 step 317: training accuarcy: 0.9874\n",
      "Epoch 1 step 317: training loss: 7530.324928563196\n",
      "Epoch 1 step 318: training accuarcy: 0.9905\n",
      "Epoch 1 step 318: training loss: 7833.050914958493\n",
      "Epoch 1 step 319: training accuarcy: 0.9861000000000001\n",
      "Epoch 1 step 319: training loss: 7967.89112088583\n",
      "Epoch 1 step 320: training accuarcy: 0.9830000000000001\n",
      "Epoch 1 step 320: training loss: 7440.191969947069\n",
      "Epoch 1 step 321: training accuarcy: 0.9927\n",
      "Epoch 1 step 321: training loss: 7590.587452404397\n",
      "Epoch 1 step 322: training accuarcy: 0.9892000000000001\n",
      "Epoch 1 step 322: training loss: 7880.152276257269\n",
      "Epoch 1 step 323: training accuarcy: 0.9855\n",
      "Epoch 1 step 323: training loss: 7478.8084202793825\n",
      "Epoch 1 step 324: training accuarcy: 0.9909\n",
      "Epoch 1 step 324: training loss: 7614.649082601036\n",
      "Epoch 1 step 325: training accuarcy: 0.9887\n",
      "Epoch 1 step 325: training loss: 7520.66241818933\n",
      "Epoch 1 step 326: training accuarcy: 0.9893000000000001\n",
      "Epoch 1 step 326: training loss: 7636.248693801268\n",
      "Epoch 1 step 327: training accuarcy: 0.9876\n",
      "Epoch 1 step 327: training loss: 7464.985464839806\n",
      "Epoch 1 step 328: training accuarcy: 0.9894000000000001\n",
      "Epoch 1 step 328: training loss: 7384.339921762844\n",
      "Epoch 1 step 329: training accuarcy: 0.9887\n",
      "Epoch 1 step 329: training loss: 7628.045786429377\n",
      "Epoch 1 step 330: training accuarcy: 0.9877\n",
      "Epoch 1 step 330: training loss: 7491.583125376788\n",
      "Epoch 1 step 331: training accuarcy: 0.9885\n",
      "Epoch 1 step 331: training loss: 7454.485612507786\n",
      "Epoch 1 step 332: training accuarcy: 0.9885\n",
      "Epoch 1 step 332: training loss: 7528.108617049505\n",
      "Epoch 1 step 333: training accuarcy: 0.9887\n",
      "Epoch 1 step 333: training loss: 7875.882799837177\n",
      "Epoch 1 step 334: training accuarcy: 0.9866\n",
      "Epoch 1 step 334: training loss: 7098.958339808454\n",
      "Epoch 1 step 335: training accuarcy: 0.9921000000000001\n",
      "Epoch 1 step 335: training loss: 7419.216220309522\n",
      "Epoch 1 step 336: training accuarcy: 0.9897\n",
      "Epoch 1 step 336: training loss: 7536.887717229649\n",
      "Epoch 1 step 337: training accuarcy: 0.9874\n",
      "Epoch 1 step 337: training loss: 7385.824560541427\n",
      "Epoch 1 step 338: training accuarcy: 0.9879\n",
      "Epoch 1 step 338: training loss: 7459.6131161836765\n",
      "Epoch 1 step 339: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 339: training loss: 7270.044771342775\n",
      "Epoch 1 step 340: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 340: training loss: 7394.158599178054\n",
      "Epoch 1 step 341: training accuarcy: 0.9862000000000001\n",
      "Epoch 1 step 341: training loss: 7289.869284869177\n",
      "Epoch 1 step 342: training accuarcy: 0.9898\n",
      "Epoch 1 step 342: training loss: 7400.381993459224\n",
      "Epoch 1 step 343: training accuarcy: 0.9878\n",
      "Epoch 1 step 343: training loss: 6968.9743304231215\n",
      "Epoch 1 step 344: training accuarcy: 0.9917\n",
      "Epoch 1 step 344: training loss: 7075.69334786834\n",
      "Epoch 1 step 345: training accuarcy: 0.9926\n",
      "Epoch 1 step 345: training loss: 7097.7550662350495\n",
      "Epoch 1 step 346: training accuarcy: 0.9922000000000001\n",
      "Epoch 1 step 346: training loss: 6987.1913263054275\n",
      "Epoch 1 step 347: training accuarcy: 0.9923000000000001\n",
      "Epoch 1 step 347: training loss: 7495.912534489609\n",
      "Epoch 1 step 348: training accuarcy: 0.9863000000000001\n",
      "Epoch 1 step 348: training loss: 6922.838613224958\n",
      "Epoch 1 step 349: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 349: training loss: 7211.283547732277\n",
      "Epoch 1 step 350: training accuarcy: 0.9882000000000001\n",
      "Epoch 1 step 350: training loss: 7211.562581088768\n",
      "Epoch 1 step 351: training accuarcy: 0.9895\n",
      "Epoch 1 step 351: training loss: 7260.450980638821\n",
      "Epoch 1 step 352: training accuarcy: 0.9869\n",
      "Epoch 1 step 352: training loss: 7056.225038647559\n",
      "Epoch 1 step 353: training accuarcy: 0.9892000000000001\n",
      "Epoch 1 step 353: training loss: 7011.345909196959\n",
      "Epoch 1 step 354: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 354: training loss: 7170.815583432084\n",
      "Epoch 1 step 355: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 355: training loss: 6887.737366423443\n",
      "Epoch 1 step 356: training accuarcy: 0.992\n",
      "Epoch 1 step 356: training loss: 7014.451934659821\n",
      "Epoch 1 step 357: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 357: training loss: 6852.215916342452\n",
      "Epoch 1 step 358: training accuarcy: 0.9919\n",
      "Epoch 1 step 358: training loss: 7060.7496278493145\n",
      "Epoch 1 step 359: training accuarcy: 0.9865\n",
      "Epoch 1 step 359: training loss: 7266.947190659256\n",
      "Epoch 1 step 360: training accuarcy: 0.9861000000000001\n",
      "Epoch 1 step 360: training loss: 7224.854970274842\n",
      "Epoch 1 step 361: training accuarcy: 0.9865\n",
      "Epoch 1 step 361: training loss: 6935.314243321259\n",
      "Epoch 1 step 362: training accuarcy: 0.9884000000000001\n",
      "Epoch 1 step 362: training loss: 6983.246257017316\n",
      "Epoch 1 step 363: training accuarcy: 0.9889\n",
      "Epoch 1 step 363: training loss: 7048.5098295541\n",
      "Epoch 1 step 364: training accuarcy: 0.9876\n",
      "Epoch 1 step 364: training loss: 7254.0574878783855\n",
      "Epoch 1 step 365: training accuarcy: 0.9861000000000001\n",
      "Epoch 1 step 365: training loss: 6633.353305420405\n",
      "Epoch 1 step 366: training accuarcy: 0.9926\n",
      "Epoch 1 step 366: training loss: 7121.776968674159\n",
      "Epoch 1 step 367: training accuarcy: 0.9851000000000001\n",
      "Epoch 1 step 367: training loss: 6823.482437240352\n",
      "Epoch 1 step 368: training accuarcy: 0.9887\n",
      "Epoch 1 step 368: training loss: 6843.778996361816\n",
      "Epoch 1 step 369: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 369: training loss: 6999.43969377324\n",
      "Epoch 1 step 370: training accuarcy: 0.9871000000000001\n",
      "Epoch 1 step 370: training loss: 6813.888043831478\n",
      "Epoch 1 step 371: training accuarcy: 0.9919\n",
      "Epoch 1 step 371: training loss: 6911.693708871885\n",
      "Epoch 1 step 372: training accuarcy: 0.9875\n",
      "Epoch 1 step 372: training loss: 6745.030027848807\n",
      "Epoch 1 step 373: training accuarcy: 0.991\n",
      "Epoch 1 step 373: training loss: 6811.1819439535475\n",
      "Epoch 1 step 374: training accuarcy: 0.9889\n",
      "Epoch 1 step 374: training loss: 6836.55276648917\n",
      "Epoch 1 step 375: training accuarcy: 0.9878\n",
      "Epoch 1 step 375: training loss: 6814.959647450711\n",
      "Epoch 1 step 376: training accuarcy: 0.9873000000000001\n",
      "Epoch 1 step 376: training loss: 7064.115027876311\n",
      "Epoch 1 step 377: training accuarcy: 0.9838\n",
      "Epoch 1 step 377: training loss: 6856.284307473134\n",
      "Epoch 1 step 378: training accuarcy: 0.9870000000000001\n",
      "Epoch 1 step 378: training loss: 6815.129527788584\n",
      "Epoch 1 step 379: training accuarcy: 0.9868\n",
      "Epoch 1 step 379: training loss: 6764.460147745344\n",
      "Epoch 1 step 380: training accuarcy: 0.9895\n",
      "Epoch 1 step 380: training loss: 6454.708665916857\n",
      "Epoch 1 step 381: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 381: training loss: 6442.8861151389965\n",
      "Epoch 1 step 382: training accuarcy: 0.9917\n",
      "Epoch 1 step 382: training loss: 6646.221136068353\n",
      "Epoch 1 step 383: training accuarcy: 0.9900000000000001\n",
      "Epoch 1 step 383: training loss: 6450.281183386941\n",
      "Epoch 1 step 384: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 384: training loss: 6578.4118428508955\n",
      "Epoch 1 step 385: training accuarcy: 0.9915\n",
      "Epoch 1 step 385: training loss: 6702.572354340387\n",
      "Epoch 1 step 386: training accuarcy: 0.9889\n",
      "Epoch 1 step 386: training loss: 6700.153514695225\n",
      "Epoch 1 step 387: training accuarcy: 0.9888\n",
      "Epoch 1 step 387: training loss: 6666.74789524879\n",
      "Epoch 1 step 388: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 388: training loss: 6776.2099465626525\n",
      "Epoch 1 step 389: training accuarcy: 0.9871000000000001\n",
      "Epoch 1 step 389: training loss: 6453.651853456294\n",
      "Epoch 1 step 390: training accuarcy: 0.9909\n",
      "Epoch 1 step 390: training loss: 6600.250126948434\n",
      "Epoch 1 step 391: training accuarcy: 0.9878\n",
      "Epoch 1 step 391: training loss: 6638.055043040886\n",
      "Epoch 1 step 392: training accuarcy: 0.9876\n",
      "Epoch 1 step 392: training loss: 6419.640900457287\n",
      "Epoch 1 step 393: training accuarcy: 0.9916\n",
      "Epoch 1 step 393: training loss: 6526.807323501766\n",
      "Epoch 1 step 394: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 394: training loss: 6562.388735992496\n",
      "Epoch 1 step 395: training accuarcy: 0.9874\n",
      "Epoch 1 step 395: training loss: 6268.091653686759\n",
      "Epoch 1 step 396: training accuarcy: 0.9937\n",
      "Epoch 1 step 396: training loss: 6323.369231700078\n",
      "Epoch 1 step 397: training accuarcy: 0.9906\n",
      "Epoch 1 step 397: training loss: 6260.028536117499\n",
      "Epoch 1 step 398: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 398: training loss: 6566.485962743731\n",
      "Epoch 1 step 399: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 399: training loss: 6197.05261278635\n",
      "Epoch 1 step 400: training accuarcy: 0.9915\n",
      "Epoch 1 step 400: training loss: 6385.975661587736\n",
      "Epoch 1 step 401: training accuarcy: 0.9890000000000001\n",
      "Epoch 1 step 401: training loss: 6418.361382527368\n",
      "Epoch 1 step 402: training accuarcy: 0.9888\n",
      "Epoch 1 step 402: training loss: 6291.7502856095825\n",
      "Epoch 1 step 403: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 403: training loss: 6384.55997246207\n",
      "Epoch 1 step 404: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 404: training loss: 6321.75280316118\n",
      "Epoch 1 step 405: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 405: training loss: 6308.718225447948\n",
      "Epoch 1 step 406: training accuarcy: 0.9888\n",
      "Epoch 1 step 406: training loss: 6209.758087908938\n",
      "Epoch 1 step 407: training accuarcy: 0.9897\n",
      "Epoch 1 step 407: training loss: 6421.9587483281975\n",
      "Epoch 1 step 408: training accuarcy: 0.9880000000000001\n",
      "Epoch 1 step 408: training loss: 6190.770627794197\n",
      "Epoch 1 step 409: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 409: training loss: 6011.552830212262\n",
      "Epoch 1 step 410: training accuarcy: 0.9919\n",
      "Epoch 1 step 410: training loss: 6353.578292123686\n",
      "Epoch 1 step 411: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 411: training loss: 6068.41859954842\n",
      "Epoch 1 step 412: training accuarcy: 0.9919\n",
      "Epoch 1 step 412: training loss: 6362.193412165918\n",
      "Epoch 1 step 413: training accuarcy: 0.9894000000000001\n",
      "Epoch 1 step 413: training loss: 6335.22644147479\n",
      "Epoch 1 step 414: training accuarcy: 0.9854\n",
      "Epoch 1 step 414: training loss: 6204.615801384204\n",
      "Epoch 1 step 415: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 415: training loss: 6047.600539132025\n",
      "Epoch 1 step 416: training accuarcy: 0.992\n",
      "Epoch 1 step 416: training loss: 5956.504991906599\n",
      "Epoch 1 step 417: training accuarcy: 0.9929\n",
      "Epoch 1 step 417: training loss: 6092.912753834333\n",
      "Epoch 1 step 418: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 418: training loss: 5891.060833625375\n",
      "Epoch 1 step 419: training accuarcy: 0.9946\n",
      "Epoch 1 step 419: training loss: 6070.074859342089\n",
      "Epoch 1 step 420: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 420: training loss: 6036.406360582217\n",
      "Epoch 1 step 421: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 421: training loss: 5898.498952559178\n",
      "Epoch 1 step 422: training accuarcy: 0.9932000000000001\n",
      "Epoch 1 step 422: training loss: 6126.560106065228\n",
      "Epoch 1 step 423: training accuarcy: 0.9893000000000001\n",
      "Epoch 1 step 423: training loss: 6231.997441955531\n",
      "Epoch 1 step 424: training accuarcy: 0.9861000000000001\n",
      "Epoch 1 step 424: training loss: 5937.703923899526\n",
      "Epoch 1 step 425: training accuarcy: 0.9914000000000001\n",
      "Epoch 1 step 425: training loss: 5960.532924343954\n",
      "Epoch 1 step 426: training accuarcy: 0.9896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 426: training loss: 6221.554330038238\n",
      "Epoch 1 step 427: training accuarcy: 0.9876\n",
      "Epoch 1 step 427: training loss: 5927.4489242996915\n",
      "Epoch 1 step 428: training accuarcy: 0.9934000000000001\n",
      "Epoch 1 step 428: training loss: 6041.6842210320665\n",
      "Epoch 1 step 429: training accuarcy: 0.9885\n",
      "Epoch 1 step 429: training loss: 5944.180503888112\n",
      "Epoch 1 step 430: training accuarcy: 0.9895\n",
      "Epoch 1 step 430: training loss: 5981.051606968576\n",
      "Epoch 1 step 431: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 431: training loss: 5886.426304914507\n",
      "Epoch 1 step 432: training accuarcy: 0.9914000000000001\n",
      "Epoch 1 step 432: training loss: 5959.966622372545\n",
      "Epoch 1 step 433: training accuarcy: 0.9907\n",
      "Epoch 1 step 433: training loss: 5846.8005121209135\n",
      "Epoch 1 step 434: training accuarcy: 0.9912000000000001\n",
      "Epoch 1 step 434: training loss: 5879.018690676422\n",
      "Epoch 1 step 435: training accuarcy: 0.9896\n",
      "Epoch 1 step 435: training loss: 5900.992912393967\n",
      "Epoch 1 step 436: training accuarcy: 0.9900000000000001\n",
      "Epoch 1 step 436: training loss: 5711.353033907936\n",
      "Epoch 1 step 437: training accuarcy: 0.9921000000000001\n",
      "Epoch 1 step 437: training loss: 5756.4644723899255\n",
      "Epoch 1 step 438: training accuarcy: 0.9934000000000001\n",
      "Epoch 1 step 438: training loss: 6034.09066728977\n",
      "Epoch 1 step 439: training accuarcy: 0.9886\n",
      "Epoch 1 step 439: training loss: 6132.924847906665\n",
      "Epoch 1 step 440: training accuarcy: 0.9865\n",
      "Epoch 1 step 440: training loss: 5836.943321873535\n",
      "Epoch 1 step 441: training accuarcy: 0.9909\n",
      "Epoch 1 step 441: training loss: 5791.994040068501\n",
      "Epoch 1 step 442: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 442: training loss: 5698.032796405522\n",
      "Epoch 1 step 443: training accuarcy: 0.9906\n",
      "Epoch 1 step 443: training loss: 5738.904096738552\n",
      "Epoch 1 step 444: training accuarcy: 0.9887\n",
      "Epoch 1 step 444: training loss: 5808.117280423676\n",
      "Epoch 1 step 445: training accuarcy: 0.9877\n",
      "Epoch 1 step 445: training loss: 5720.944239178067\n",
      "Epoch 1 step 446: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 446: training loss: 5655.1905669443795\n",
      "Epoch 1 step 447: training accuarcy: 0.9926\n",
      "Epoch 1 step 447: training loss: 5858.105963965609\n",
      "Epoch 1 step 448: training accuarcy: 0.9886\n",
      "Epoch 1 step 448: training loss: 5662.32375448088\n",
      "Epoch 1 step 449: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 449: training loss: 5800.204731668591\n",
      "Epoch 1 step 450: training accuarcy: 0.9887\n",
      "Epoch 1 step 450: training loss: 5648.419396526001\n",
      "Epoch 1 step 451: training accuarcy: 0.9905\n",
      "Epoch 1 step 451: training loss: 5788.2222634778445\n",
      "Epoch 1 step 452: training accuarcy: 0.9899\n",
      "Epoch 1 step 452: training loss: 5685.70576620097\n",
      "Epoch 1 step 453: training accuarcy: 0.9893000000000001\n",
      "Epoch 1 step 453: training loss: 5588.854349743072\n",
      "Epoch 1 step 454: training accuarcy: 0.9907\n",
      "Epoch 1 step 454: training loss: 5447.183874701358\n",
      "Epoch 1 step 455: training accuarcy: 0.9925\n",
      "Epoch 1 step 455: training loss: 5683.269178725956\n",
      "Epoch 1 step 456: training accuarcy: 0.9902000000000001\n",
      "Epoch 1 step 456: training loss: 5477.978339449386\n",
      "Epoch 1 step 457: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 457: training loss: 5722.73578096058\n",
      "Epoch 1 step 458: training accuarcy: 0.9882000000000001\n",
      "Epoch 1 step 458: training loss: 5608.905541424537\n",
      "Epoch 1 step 459: training accuarcy: 0.9900000000000001\n",
      "Epoch 1 step 459: training loss: 5655.056793975122\n",
      "Epoch 1 step 460: training accuarcy: 0.9893000000000001\n",
      "Epoch 1 step 460: training loss: 5887.567994239271\n",
      "Epoch 1 step 461: training accuarcy: 0.9852000000000001\n",
      "Epoch 1 step 461: training loss: 5521.258913597797\n",
      "Epoch 1 step 462: training accuarcy: 0.9918\n",
      "Epoch 1 step 462: training loss: 5580.315748799157\n",
      "Epoch 1 step 463: training accuarcy: 0.9909\n",
      "Epoch 1 step 463: training loss: 5600.52323444183\n",
      "Epoch 1 step 464: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 464: training loss: 5663.864984108074\n",
      "Epoch 1 step 465: training accuarcy: 0.9894000000000001\n",
      "Epoch 1 step 465: training loss: 5723.606887683758\n",
      "Epoch 1 step 466: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 466: training loss: 5480.0751693848415\n",
      "Epoch 1 step 467: training accuarcy: 0.9928\n",
      "Epoch 1 step 467: training loss: 5810.051582856529\n",
      "Epoch 1 step 468: training accuarcy: 0.9877\n",
      "Epoch 1 step 468: training loss: 5487.972953431334\n",
      "Epoch 1 step 469: training accuarcy: 0.9895\n",
      "Epoch 1 step 469: training loss: 5656.834195844168\n",
      "Epoch 1 step 470: training accuarcy: 0.9875\n",
      "Epoch 1 step 470: training loss: 5629.311735662933\n",
      "Epoch 1 step 471: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 471: training loss: 5270.228391131032\n",
      "Epoch 1 step 472: training accuarcy: 0.9927\n",
      "Epoch 1 step 472: training loss: 5390.2585246205745\n",
      "Epoch 1 step 473: training accuarcy: 0.9915\n",
      "Epoch 1 step 473: training loss: 5519.719131952854\n",
      "Epoch 1 step 474: training accuarcy: 0.9885\n",
      "Epoch 1 step 474: training loss: 5484.736298527824\n",
      "Epoch 1 step 475: training accuarcy: 0.9895\n",
      "Epoch 1 step 475: training loss: 5510.416364660758\n",
      "Epoch 1 step 476: training accuarcy: 0.9889\n",
      "Epoch 1 step 476: training loss: 5391.405810846619\n",
      "Epoch 1 step 477: training accuarcy: 0.9913000000000001\n",
      "Epoch 1 step 477: training loss: 5637.897956212426\n",
      "Epoch 1 step 478: training accuarcy: 0.9867\n",
      "Epoch 1 step 478: training loss: 5325.631868816194\n",
      "Epoch 1 step 479: training accuarcy: 0.9938\n",
      "Epoch 1 step 479: training loss: 5226.129669859205\n",
      "Epoch 1 step 480: training accuarcy: 0.9928\n",
      "Epoch 1 step 480: training loss: 5306.417235241644\n",
      "Epoch 1 step 481: training accuarcy: 0.9914000000000001\n",
      "Epoch 1 step 481: training loss: 5362.74047373792\n",
      "Epoch 1 step 482: training accuarcy: 0.9899\n",
      "Epoch 1 step 482: training loss: 5331.445173597955\n",
      "Epoch 1 step 483: training accuarcy: 0.9918\n",
      "Epoch 1 step 483: training loss: 5178.624495409098\n",
      "Epoch 1 step 484: training accuarcy: 0.9909\n",
      "Epoch 1 step 484: training loss: 5486.197405881222\n",
      "Epoch 1 step 485: training accuarcy: 0.9897\n",
      "Epoch 1 step 485: training loss: 5437.9321169707655\n",
      "Epoch 1 step 486: training accuarcy: 0.9871000000000001\n",
      "Epoch 1 step 486: training loss: 5391.045121864234\n",
      "Epoch 1 step 487: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 487: training loss: 5165.059331979584\n",
      "Epoch 1 step 488: training accuarcy: 0.9918\n",
      "Epoch 1 step 488: training loss: 5109.925926399835\n",
      "Epoch 1 step 489: training accuarcy: 0.9935\n",
      "Epoch 1 step 489: training loss: 5287.315389598602\n",
      "Epoch 1 step 490: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 490: training loss: 5222.090289984806\n",
      "Epoch 1 step 491: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 491: training loss: 5149.230994236997\n",
      "Epoch 1 step 492: training accuarcy: 0.9904000000000001\n",
      "Epoch 1 step 492: training loss: 5210.075858357756\n",
      "Epoch 1 step 493: training accuarcy: 0.9895\n",
      "Epoch 1 step 493: training loss: 5083.095688674087\n",
      "Epoch 1 step 494: training accuarcy: 0.9923000000000001\n",
      "Epoch 1 step 494: training loss: 5301.7753774775265\n",
      "Epoch 1 step 495: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 495: training loss: 5053.201538496582\n",
      "Epoch 1 step 496: training accuarcy: 0.9936\n",
      "Epoch 1 step 496: training loss: 5298.378687050765\n",
      "Epoch 1 step 497: training accuarcy: 0.991\n",
      "Epoch 1 step 497: training loss: 5038.256271901444\n",
      "Epoch 1 step 498: training accuarcy: 0.9929\n",
      "Epoch 1 step 498: training loss: 5339.472228112268\n",
      "Epoch 1 step 499: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 499: training loss: 5502.234093548709\n",
      "Epoch 1 step 500: training accuarcy: 0.9888\n",
      "Epoch 1 step 500: training loss: 4852.196573524754\n",
      "Epoch 1 step 501: training accuarcy: 0.9968\n",
      "Epoch 1 step 501: training loss: 5010.28854159548\n",
      "Epoch 1 step 502: training accuarcy: 0.9936\n",
      "Epoch 1 step 502: training loss: 4953.03485751679\n",
      "Epoch 1 step 503: training accuarcy: 0.9942000000000001\n",
      "Epoch 1 step 503: training loss: 4914.034184399153\n",
      "Epoch 1 step 504: training accuarcy: 0.9928\n",
      "Epoch 1 step 504: training loss: 4979.198309321041\n",
      "Epoch 1 step 505: training accuarcy: 0.9919\n",
      "Epoch 1 step 505: training loss: 5060.0250912299025\n",
      "Epoch 1 step 506: training accuarcy: 0.9924000000000001\n",
      "Epoch 1 step 506: training loss: 5122.462022046057\n",
      "Epoch 1 step 507: training accuarcy: 0.9916\n",
      "Epoch 1 step 507: training loss: 5062.974255037603\n",
      "Epoch 1 step 508: training accuarcy: 0.9911000000000001\n",
      "Epoch 1 step 508: training loss: 5268.513102068115\n",
      "Epoch 1 step 509: training accuarcy: 0.9874\n",
      "Epoch 1 step 509: training loss: 5116.304354988928\n",
      "Epoch 1 step 510: training accuarcy: 0.991\n",
      "Epoch 1 step 510: training loss: 4981.044080838401\n",
      "Epoch 1 step 511: training accuarcy: 0.9926\n",
      "Epoch 1 step 511: training loss: 4986.915260795151\n",
      "Epoch 1 step 512: training accuarcy: 0.9909\n",
      "Epoch 1 step 512: training loss: 5060.415814811108\n",
      "Epoch 1 step 513: training accuarcy: 0.9891000000000001\n",
      "Epoch 1 step 513: training loss: 4990.116700843382\n",
      "Epoch 1 step 514: training accuarcy: 0.9916\n",
      "Epoch 1 step 514: training loss: 4760.047024217125\n",
      "Epoch 1 step 515: training accuarcy: 0.9938\n",
      "Epoch 1 step 515: training loss: 5149.57028470459\n",
      "Epoch 1 step 516: training accuarcy: 0.9903000000000001\n",
      "Epoch 1 step 516: training loss: 5023.456092177116\n",
      "Epoch 1 step 517: training accuarcy: 0.9908\n",
      "Epoch 1 step 517: training loss: 5183.479453972701\n",
      "Epoch 1 step 518: training accuarcy: 0.9881000000000001\n",
      "Epoch 1 step 518: training loss: 5038.692499720786\n",
      "Epoch 1 step 519: training accuarcy: 0.9892000000000001\n",
      "Epoch 1 step 519: training loss: 5030.880027989722\n",
      "Epoch 1 step 520: training accuarcy: 0.9901000000000001\n",
      "Epoch 1 step 520: training loss: 4986.236115476854\n",
      "Epoch 1 step 521: training accuarcy: 0.992\n",
      "Epoch 1 step 521: training loss: 4921.557711966993\n",
      "Epoch 1 step 522: training accuarcy: 0.993\n",
      "Epoch 1 step 522: training loss: 4829.256577622125\n",
      "Epoch 1 step 523: training accuarcy: 0.992\n",
      "Epoch 1 step 523: training loss: 5016.442766075729\n",
      "Epoch 1 step 524: training accuarcy: 0.9883000000000001\n",
      "Epoch 1 step 524: training loss: 4978.944549427739\n",
      "Epoch 1 step 525: training accuarcy: 0.991\n",
      "Epoch 1 step 525: training loss: 4131.729062578906\n",
      "Epoch 1 step 526: training accuarcy: 0.9941025641025641\n",
      "Epoch 1: train loss 6595.833687502627, train accuarcy 0.9858560562133789\n",
      "Epoch 1: valid loss 6667.3031028699115, valid accuarcy 0.9780675172805786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 2/3 [09:35<04:49, 289.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 4345.961915111372\n",
      "Epoch 2 step 527: training accuarcy: 0.9978\n",
      "Epoch 2 step 527: training loss: 4260.850086770885\n",
      "Epoch 2 step 528: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 528: training loss: 4487.605909708647\n",
      "Epoch 2 step 529: training accuarcy: 0.9956\n",
      "Epoch 2 step 529: training loss: 4215.843818764394\n",
      "Epoch 2 step 530: training accuarcy: 0.9966\n",
      "Epoch 2 step 530: training loss: 4288.54462929906\n",
      "Epoch 2 step 531: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 531: training loss: 4238.23354934376\n",
      "Epoch 2 step 532: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 532: training loss: 4265.6264095508295\n",
      "Epoch 2 step 533: training accuarcy: 0.997\n",
      "Epoch 2 step 533: training loss: 4356.499136132832\n",
      "Epoch 2 step 534: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 534: training loss: 4433.148706856522\n",
      "Epoch 2 step 535: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 535: training loss: 4427.379311821591\n",
      "Epoch 2 step 536: training accuarcy: 0.9939\n",
      "Epoch 2 step 536: training loss: 4169.88670738351\n",
      "Epoch 2 step 537: training accuarcy: 0.997\n",
      "Epoch 2 step 537: training loss: 4305.927888942322\n",
      "Epoch 2 step 538: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 538: training loss: 4243.584449965448\n",
      "Epoch 2 step 539: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 539: training loss: 4157.81447246271\n",
      "Epoch 2 step 540: training accuarcy: 0.9979\n",
      "Epoch 2 step 540: training loss: 4240.708558308206\n",
      "Epoch 2 step 541: training accuarcy: 0.9965\n",
      "Epoch 2 step 541: training loss: 4248.280102968247\n",
      "Epoch 2 step 542: training accuarcy: 0.9949\n",
      "Epoch 2 step 542: training loss: 4201.3912768384225\n",
      "Epoch 2 step 543: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 543: training loss: 4223.545373308863\n",
      "Epoch 2 step 544: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 544: training loss: 4353.972185098846\n",
      "Epoch 2 step 545: training accuarcy: 0.9948\n",
      "Epoch 2 step 545: training loss: 4164.550310099451\n",
      "Epoch 2 step 546: training accuarcy: 0.9975\n",
      "Epoch 2 step 546: training loss: 4240.265889723425\n",
      "Epoch 2 step 547: training accuarcy: 0.995\n",
      "Epoch 2 step 547: training loss: 4316.3984927175825\n",
      "Epoch 2 step 548: training accuarcy: 0.9959\n",
      "Epoch 2 step 548: training loss: 4133.295903149377\n",
      "Epoch 2 step 549: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 549: training loss: 4032.0519258399268\n",
      "Epoch 2 step 550: training accuarcy: 0.9979\n",
      "Epoch 2 step 550: training loss: 4180.8870767930175\n",
      "Epoch 2 step 551: training accuarcy: 0.9956\n",
      "Epoch 2 step 551: training loss: 4102.518201880825\n",
      "Epoch 2 step 552: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 552: training loss: 4226.553528364369\n",
      "Epoch 2 step 553: training accuarcy: 0.9943000000000001\n",
      "Epoch 2 step 553: training loss: 4237.412583125778\n",
      "Epoch 2 step 554: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 554: training loss: 4074.3700405321297\n",
      "Epoch 2 step 555: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 555: training loss: 4139.496359936687\n",
      "Epoch 2 step 556: training accuarcy: 0.995\n",
      "Epoch 2 step 556: training loss: 4046.125957638458\n",
      "Epoch 2 step 557: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 557: training loss: 4114.500145402349\n",
      "Epoch 2 step 558: training accuarcy: 0.9967\n",
      "Epoch 2 step 558: training loss: 4033.0356502250415\n",
      "Epoch 2 step 559: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 559: training loss: 4118.117838887245\n",
      "Epoch 2 step 560: training accuarcy: 0.9956\n",
      "Epoch 2 step 560: training loss: 3932.852525492859\n",
      "Epoch 2 step 561: training accuarcy: 0.9968\n",
      "Epoch 2 step 561: training loss: 4109.313059112459\n",
      "Epoch 2 step 562: training accuarcy: 0.9935\n",
      "Epoch 2 step 562: training loss: 4022.0651228246634\n",
      "Epoch 2 step 563: training accuarcy: 0.9979\n",
      "Epoch 2 step 563: training loss: 4055.768335108992\n",
      "Epoch 2 step 564: training accuarcy: 0.9957\n",
      "Epoch 2 step 564: training loss: 4059.752241568733\n",
      "Epoch 2 step 565: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 565: training loss: 4131.947744123897\n",
      "Epoch 2 step 566: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 566: training loss: 3983.73439121904\n",
      "Epoch 2 step 567: training accuarcy: 0.9967\n",
      "Epoch 2 step 567: training loss: 4036.516729908665\n",
      "Epoch 2 step 568: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 568: training loss: 3989.980658840961\n",
      "Epoch 2 step 569: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 569: training loss: 3866.7778529416946\n",
      "Epoch 2 step 570: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 570: training loss: 3981.205173160471\n",
      "Epoch 2 step 571: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 571: training loss: 4023.1129320080427\n",
      "Epoch 2 step 572: training accuarcy: 0.9956\n",
      "Epoch 2 step 572: training loss: 3870.5668843986523\n",
      "Epoch 2 step 573: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 573: training loss: 3869.0171739113716\n",
      "Epoch 2 step 574: training accuarcy: 0.9978\n",
      "Epoch 2 step 574: training loss: 4028.7174755186875\n",
      "Epoch 2 step 575: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 575: training loss: 3875.5957005181926\n",
      "Epoch 2 step 576: training accuarcy: 0.9967\n",
      "Epoch 2 step 576: training loss: 3923.0124991388875\n",
      "Epoch 2 step 577: training accuarcy: 0.9956\n",
      "Epoch 2 step 577: training loss: 3971.4240621052295\n",
      "Epoch 2 step 578: training accuarcy: 0.9958\n",
      "Epoch 2 step 578: training loss: 3878.930307762113\n",
      "Epoch 2 step 579: training accuarcy: 0.9957\n",
      "Epoch 2 step 579: training loss: 3943.518950384016\n",
      "Epoch 2 step 580: training accuarcy: 0.9958\n",
      "Epoch 2 step 580: training loss: 3858.808031683253\n",
      "Epoch 2 step 581: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 581: training loss: 3881.0651118358346\n",
      "Epoch 2 step 582: training accuarcy: 0.9956\n",
      "Epoch 2 step 582: training loss: 3777.6766500359436\n",
      "Epoch 2 step 583: training accuarcy: 0.997\n",
      "Epoch 2 step 583: training loss: 3922.5318743971793\n",
      "Epoch 2 step 584: training accuarcy: 0.9978\n",
      "Epoch 2 step 584: training loss: 3831.6070174670413\n",
      "Epoch 2 step 585: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 585: training loss: 3902.2188885451897\n",
      "Epoch 2 step 586: training accuarcy: 0.996\n",
      "Epoch 2 step 586: training loss: 3830.2312544843853\n",
      "Epoch 2 step 587: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 587: training loss: 3958.6899656625496\n",
      "Epoch 2 step 588: training accuarcy: 0.995\n",
      "Epoch 2 step 588: training loss: 3945.0867238765063\n",
      "Epoch 2 step 589: training accuarcy: 0.9948\n",
      "Epoch 2 step 589: training loss: 3825.192352615844\n",
      "Epoch 2 step 590: training accuarcy: 0.9966\n",
      "Epoch 2 step 590: training loss: 3877.3101839249875\n",
      "Epoch 2 step 591: training accuarcy: 0.9967\n",
      "Epoch 2 step 591: training loss: 3794.1576236589945\n",
      "Epoch 2 step 592: training accuarcy: 0.9958\n",
      "Epoch 2 step 592: training loss: 3851.3969711649906\n",
      "Epoch 2 step 593: training accuarcy: 0.995\n",
      "Epoch 2 step 593: training loss: 4071.1782271968013\n",
      "Epoch 2 step 594: training accuarcy: 0.9927\n",
      "Epoch 2 step 594: training loss: 3907.049762606766\n",
      "Epoch 2 step 595: training accuarcy: 0.9949\n",
      "Epoch 2 step 595: training loss: 3779.6760663532546\n",
      "Epoch 2 step 596: training accuarcy: 0.9959\n",
      "Epoch 2 step 596: training loss: 3777.6950815941773\n",
      "Epoch 2 step 597: training accuarcy: 0.997\n",
      "Epoch 2 step 597: training loss: 3710.3198316333996\n",
      "Epoch 2 step 598: training accuarcy: 0.9985\n",
      "Epoch 2 step 598: training loss: 3609.4950684578016\n",
      "Epoch 2 step 599: training accuarcy: 0.998\n",
      "Epoch 2 step 599: training loss: 3944.3719237686632\n",
      "Epoch 2 step 600: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 600: training loss: 3721.5577199746645\n",
      "Epoch 2 step 601: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 601: training loss: 3776.8497570017807\n",
      "Epoch 2 step 602: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 602: training loss: 3720.627148811125\n",
      "Epoch 2 step 603: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 603: training loss: 3708.707838011487\n",
      "Epoch 2 step 604: training accuarcy: 0.9967\n",
      "Epoch 2 step 604: training loss: 3710.9908876132895\n",
      "Epoch 2 step 605: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 605: training loss: 3650.483637184607\n",
      "Epoch 2 step 606: training accuarcy: 0.9966\n",
      "Epoch 2 step 606: training loss: 3734.467857911567\n",
      "Epoch 2 step 607: training accuarcy: 0.9973000000000001\n",
      "Epoch 2 step 607: training loss: 3852.975857816791\n",
      "Epoch 2 step 608: training accuarcy: 0.9965\n",
      "Epoch 2 step 608: training loss: 3714.3946581420864\n",
      "Epoch 2 step 609: training accuarcy: 0.9955\n",
      "Epoch 2 step 609: training loss: 3764.9458870785325\n",
      "Epoch 2 step 610: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 610: training loss: 3666.9206516563445\n",
      "Epoch 2 step 611: training accuarcy: 0.9966\n",
      "Epoch 2 step 611: training loss: 3649.9610777944317\n",
      "Epoch 2 step 612: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 612: training loss: 3654.3432675779686\n",
      "Epoch 2 step 613: training accuarcy: 0.9958\n",
      "Epoch 2 step 613: training loss: 3608.0745050062524\n",
      "Epoch 2 step 614: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 614: training loss: 3623.1259730719275\n",
      "Epoch 2 step 615: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 615: training loss: 3706.5483294964233\n",
      "Epoch 2 step 616: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 616: training loss: 3736.1308040295894\n",
      "Epoch 2 step 617: training accuarcy: 0.9948\n",
      "Epoch 2 step 617: training loss: 3551.5897887793517\n",
      "Epoch 2 step 618: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 618: training loss: 3660.6962631584443\n",
      "Epoch 2 step 619: training accuarcy: 0.9955\n",
      "Epoch 2 step 619: training loss: 3881.1845887031814\n",
      "Epoch 2 step 620: training accuarcy: 0.9921000000000001\n",
      "Epoch 2 step 620: training loss: 3747.924949606191\n",
      "Epoch 2 step 621: training accuarcy: 0.9948\n",
      "Epoch 2 step 621: training loss: 3560.2043207271904\n",
      "Epoch 2 step 622: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 622: training loss: 3703.6872166569697\n",
      "Epoch 2 step 623: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 623: training loss: 3722.4903845168124\n",
      "Epoch 2 step 624: training accuarcy: 0.9956\n",
      "Epoch 2 step 624: training loss: 3643.5276636101094\n",
      "Epoch 2 step 625: training accuarcy: 0.9956\n",
      "Epoch 2 step 625: training loss: 3510.0782723338725\n",
      "Epoch 2 step 626: training accuarcy: 0.9975\n",
      "Epoch 2 step 626: training loss: 3614.1337283198636\n",
      "Epoch 2 step 627: training accuarcy: 0.9966\n",
      "Epoch 2 step 627: training loss: 3482.2418181877983\n",
      "Epoch 2 step 628: training accuarcy: 0.9984000000000001\n",
      "Epoch 2 step 628: training loss: 3731.5328900969353\n",
      "Epoch 2 step 629: training accuarcy: 0.9958\n",
      "Epoch 2 step 629: training loss: 3460.9138447103182\n",
      "Epoch 2 step 630: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 630: training loss: 3700.806968871475\n",
      "Epoch 2 step 631: training accuarcy: 0.9935\n",
      "Epoch 2 step 631: training loss: 3617.792945048167\n",
      "Epoch 2 step 632: training accuarcy: 0.9947\n",
      "Epoch 2 step 632: training loss: 3371.7156067846213\n",
      "Epoch 2 step 633: training accuarcy: 0.9979\n",
      "Epoch 2 step 633: training loss: 3295.358433819355\n",
      "Epoch 2 step 634: training accuarcy: 0.9988\n",
      "Epoch 2 step 634: training loss: 3669.1243670458125\n",
      "Epoch 2 step 635: training accuarcy: 0.9943000000000001\n",
      "Epoch 2 step 635: training loss: 3512.8424528648848\n",
      "Epoch 2 step 636: training accuarcy: 0.9965\n",
      "Epoch 2 step 636: training loss: 3415.4135158155086\n",
      "Epoch 2 step 637: training accuarcy: 0.9968\n",
      "Epoch 2 step 637: training loss: 3484.4077262912588\n",
      "Epoch 2 step 638: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 638: training loss: 3873.8263282902412\n",
      "Epoch 2 step 639: training accuarcy: 0.9935\n",
      "Epoch 2 step 639: training loss: 3529.474362040521\n",
      "Epoch 2 step 640: training accuarcy: 0.9955\n",
      "Epoch 2 step 640: training loss: 3406.751883550567\n",
      "Epoch 2 step 641: training accuarcy: 0.996\n",
      "Epoch 2 step 641: training loss: 3516.2288304938847\n",
      "Epoch 2 step 642: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 642: training loss: 3445.1570302471146\n",
      "Epoch 2 step 643: training accuarcy: 0.9958\n",
      "Epoch 2 step 643: training loss: 3423.211883640824\n",
      "Epoch 2 step 644: training accuarcy: 0.9965\n",
      "Epoch 2 step 644: training loss: 3488.5223587784117\n",
      "Epoch 2 step 645: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 645: training loss: 3472.0917183869697\n",
      "Epoch 2 step 646: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 646: training loss: 3555.0830821640743\n",
      "Epoch 2 step 647: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 647: training loss: 3342.9680744358066\n",
      "Epoch 2 step 648: training accuarcy: 0.9975\n",
      "Epoch 2 step 648: training loss: 3522.515065834003\n",
      "Epoch 2 step 649: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 649: training loss: 3424.4938224011235\n",
      "Epoch 2 step 650: training accuarcy: 0.9967\n",
      "Epoch 2 step 650: training loss: 3519.645680722427\n",
      "Epoch 2 step 651: training accuarcy: 0.9945\n",
      "Epoch 2 step 651: training loss: 3499.201104581739\n",
      "Epoch 2 step 652: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 652: training loss: 3408.289518720034\n",
      "Epoch 2 step 653: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 653: training loss: 3579.072491323386\n",
      "Epoch 2 step 654: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 654: training loss: 3352.9786941563752\n",
      "Epoch 2 step 655: training accuarcy: 0.997\n",
      "Epoch 2 step 655: training loss: 3481.5606847119034\n",
      "Epoch 2 step 656: training accuarcy: 0.9958\n",
      "Epoch 2 step 656: training loss: 3380.5383284537056\n",
      "Epoch 2 step 657: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 657: training loss: 3403.579598612594\n",
      "Epoch 2 step 658: training accuarcy: 0.997\n",
      "Epoch 2 step 658: training loss: 3510.25587203297\n",
      "Epoch 2 step 659: training accuarcy: 0.9931000000000001\n",
      "Epoch 2 step 659: training loss: 3444.068833405552\n",
      "Epoch 2 step 660: training accuarcy: 0.9948\n",
      "Epoch 2 step 660: training loss: 3458.7467871288263\n",
      "Epoch 2 step 661: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 661: training loss: 3396.8472755312187\n",
      "Epoch 2 step 662: training accuarcy: 0.9957\n",
      "Epoch 2 step 662: training loss: 3503.2260850646044\n",
      "Epoch 2 step 663: training accuarcy: 0.9938\n",
      "Epoch 2 step 663: training loss: 3344.497704772469\n",
      "Epoch 2 step 664: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 664: training loss: 3340.7904072800175\n",
      "Epoch 2 step 665: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 665: training loss: 3404.231450756593\n",
      "Epoch 2 step 666: training accuarcy: 0.9957\n",
      "Epoch 2 step 666: training loss: 3504.112403652807\n",
      "Epoch 2 step 667: training accuarcy: 0.9932000000000001\n",
      "Epoch 2 step 667: training loss: 3375.0835479681627\n",
      "Epoch 2 step 668: training accuarcy: 0.9947\n",
      "Epoch 2 step 668: training loss: 3392.776511447315\n",
      "Epoch 2 step 669: training accuarcy: 0.9955\n",
      "Epoch 2 step 669: training loss: 3372.4222073203136\n",
      "Epoch 2 step 670: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 670: training loss: 3414.0028973082267\n",
      "Epoch 2 step 671: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 671: training loss: 3416.7312155073714\n",
      "Epoch 2 step 672: training accuarcy: 0.9967\n",
      "Epoch 2 step 672: training loss: 3231.7943513619025\n",
      "Epoch 2 step 673: training accuarcy: 0.9975\n",
      "Epoch 2 step 673: training loss: 3286.4109096689904\n",
      "Epoch 2 step 674: training accuarcy: 0.9967\n",
      "Epoch 2 step 674: training loss: 3244.477930530732\n",
      "Epoch 2 step 675: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 675: training loss: 3244.1651869085845\n",
      "Epoch 2 step 676: training accuarcy: 0.9969\n",
      "Epoch 2 step 676: training loss: 3324.831176426674\n",
      "Epoch 2 step 677: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 677: training loss: 3241.755697195716\n",
      "Epoch 2 step 678: training accuarcy: 0.9957\n",
      "Epoch 2 step 678: training loss: 3501.632548846551\n",
      "Epoch 2 step 679: training accuarcy: 0.9929\n",
      "Epoch 2 step 679: training loss: 3289.947324990967\n",
      "Epoch 2 step 680: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 680: training loss: 3306.833962760761\n",
      "Epoch 2 step 681: training accuarcy: 0.9976\n",
      "Epoch 2 step 681: training loss: 3309.754312875615\n",
      "Epoch 2 step 682: training accuarcy: 0.9949\n",
      "Epoch 2 step 682: training loss: 3282.992945026115\n",
      "Epoch 2 step 683: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 683: training loss: 3362.9064888764174\n",
      "Epoch 2 step 684: training accuarcy: 0.9948\n",
      "Epoch 2 step 684: training loss: 3228.989364680623\n",
      "Epoch 2 step 685: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 685: training loss: 3221.0310390393793\n",
      "Epoch 2 step 686: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 686: training loss: 3348.618608588694\n",
      "Epoch 2 step 687: training accuarcy: 0.9969\n",
      "Epoch 2 step 687: training loss: 3089.7805719110943\n",
      "Epoch 2 step 688: training accuarcy: 0.9964000000000001\n",
      "Epoch 2 step 688: training loss: 3350.435075723738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 689: training accuarcy: 0.9943000000000001\n",
      "Epoch 2 step 689: training loss: 3416.9060598718124\n",
      "Epoch 2 step 690: training accuarcy: 0.9939\n",
      "Epoch 2 step 690: training loss: 3396.018059200225\n",
      "Epoch 2 step 691: training accuarcy: 0.994\n",
      "Epoch 2 step 691: training loss: 3282.170893567474\n",
      "Epoch 2 step 692: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 692: training loss: 3317.985513265985\n",
      "Epoch 2 step 693: training accuarcy: 0.9928\n",
      "Epoch 2 step 693: training loss: 3234.8365542682873\n",
      "Epoch 2 step 694: training accuarcy: 0.9958\n",
      "Epoch 2 step 694: training loss: 3220.3541611956457\n",
      "Epoch 2 step 695: training accuarcy: 0.9939\n",
      "Epoch 2 step 695: training loss: 3209.0935824468315\n",
      "Epoch 2 step 696: training accuarcy: 0.995\n",
      "Epoch 2 step 696: training loss: 3167.241672729309\n",
      "Epoch 2 step 697: training accuarcy: 0.997\n",
      "Epoch 2 step 697: training loss: 3224.179567806962\n",
      "Epoch 2 step 698: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 698: training loss: 3485.1692240131806\n",
      "Epoch 2 step 699: training accuarcy: 0.9928\n",
      "Epoch 2 step 699: training loss: 3149.800559159925\n",
      "Epoch 2 step 700: training accuarcy: 0.9958\n",
      "Epoch 2 step 700: training loss: 3143.2942593202324\n",
      "Epoch 2 step 701: training accuarcy: 0.997\n",
      "Epoch 2 step 701: training loss: 3208.0985726330355\n",
      "Epoch 2 step 702: training accuarcy: 0.9952000000000001\n",
      "Epoch 2 step 702: training loss: 3286.2961750475074\n",
      "Epoch 2 step 703: training accuarcy: 0.9936\n",
      "Epoch 2 step 703: training loss: 2994.381216610832\n",
      "Epoch 2 step 704: training accuarcy: 0.9982000000000001\n",
      "Epoch 2 step 704: training loss: 3236.486995839174\n",
      "Epoch 2 step 705: training accuarcy: 0.9947\n",
      "Epoch 2 step 705: training loss: 3284.2242745958984\n",
      "Epoch 2 step 706: training accuarcy: 0.9949\n",
      "Epoch 2 step 706: training loss: 3286.9843572882887\n",
      "Epoch 2 step 707: training accuarcy: 0.9933000000000001\n",
      "Epoch 2 step 707: training loss: 3130.92780938732\n",
      "Epoch 2 step 708: training accuarcy: 0.9959\n",
      "Epoch 2 step 708: training loss: 3021.3947492678203\n",
      "Epoch 2 step 709: training accuarcy: 0.9972000000000001\n",
      "Epoch 2 step 709: training loss: 3096.6816916307057\n",
      "Epoch 2 step 710: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 710: training loss: 3098.8612251488485\n",
      "Epoch 2 step 711: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 711: training loss: 3267.8262553777968\n",
      "Epoch 2 step 712: training accuarcy: 0.9936\n",
      "Epoch 2 step 712: training loss: 3109.992747905695\n",
      "Epoch 2 step 713: training accuarcy: 0.9957\n",
      "Epoch 2 step 713: training loss: 3139.2470495461284\n",
      "Epoch 2 step 714: training accuarcy: 0.995\n",
      "Epoch 2 step 714: training loss: 3243.01896371966\n",
      "Epoch 2 step 715: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 715: training loss: 3396.7960417148324\n",
      "Epoch 2 step 716: training accuarcy: 0.9915\n",
      "Epoch 2 step 716: training loss: 3131.7242452746095\n",
      "Epoch 2 step 717: training accuarcy: 0.9966\n",
      "Epoch 2 step 717: training loss: 3076.732594363521\n",
      "Epoch 2 step 718: training accuarcy: 0.9957\n",
      "Epoch 2 step 718: training loss: 3322.2491576660104\n",
      "Epoch 2 step 719: training accuarcy: 0.9925\n",
      "Epoch 2 step 719: training loss: 3308.419466418408\n",
      "Epoch 2 step 720: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 720: training loss: 3072.5599835144067\n",
      "Epoch 2 step 721: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 721: training loss: 3082.14002523912\n",
      "Epoch 2 step 722: training accuarcy: 0.997\n",
      "Epoch 2 step 722: training loss: 3174.7889780111377\n",
      "Epoch 2 step 723: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 723: training loss: 2936.6709965948066\n",
      "Epoch 2 step 724: training accuarcy: 0.9966\n",
      "Epoch 2 step 724: training loss: 3089.5762986643026\n",
      "Epoch 2 step 725: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 725: training loss: 3103.2966236576235\n",
      "Epoch 2 step 726: training accuarcy: 0.9947\n",
      "Epoch 2 step 726: training loss: 3016.9078374420974\n",
      "Epoch 2 step 727: training accuarcy: 0.9965\n",
      "Epoch 2 step 727: training loss: 2959.9121376825697\n",
      "Epoch 2 step 728: training accuarcy: 0.9963000000000001\n",
      "Epoch 2 step 728: training loss: 2873.6039966444127\n",
      "Epoch 2 step 729: training accuarcy: 0.9971000000000001\n",
      "Epoch 2 step 729: training loss: 3300.7557898239284\n",
      "Epoch 2 step 730: training accuarcy: 0.9914000000000001\n",
      "Epoch 2 step 730: training loss: 3027.0748532367274\n",
      "Epoch 2 step 731: training accuarcy: 0.9956\n",
      "Epoch 2 step 731: training loss: 2991.3518057419965\n",
      "Epoch 2 step 732: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 732: training loss: 3028.635707455238\n",
      "Epoch 2 step 733: training accuarcy: 0.996\n",
      "Epoch 2 step 733: training loss: 2975.4510342990166\n",
      "Epoch 2 step 734: training accuarcy: 0.997\n",
      "Epoch 2 step 734: training loss: 2995.689429654468\n",
      "Epoch 2 step 735: training accuarcy: 0.9947\n",
      "Epoch 2 step 735: training loss: 3223.512886042687\n",
      "Epoch 2 step 736: training accuarcy: 0.9939\n",
      "Epoch 2 step 736: training loss: 3097.6381497977973\n",
      "Epoch 2 step 737: training accuarcy: 0.9945\n",
      "Epoch 2 step 737: training loss: 3055.4525002959226\n",
      "Epoch 2 step 738: training accuarcy: 0.9947\n",
      "Epoch 2 step 738: training loss: 3056.018853263244\n",
      "Epoch 2 step 739: training accuarcy: 0.9944000000000001\n",
      "Epoch 2 step 739: training loss: 3063.2797278738713\n",
      "Epoch 2 step 740: training accuarcy: 0.9948\n",
      "Epoch 2 step 740: training loss: 2968.9021892808846\n",
      "Epoch 2 step 741: training accuarcy: 0.9953000000000001\n",
      "Epoch 2 step 741: training loss: 3123.029885641755\n",
      "Epoch 2 step 742: training accuarcy: 0.9934000000000001\n",
      "Epoch 2 step 742: training loss: 3219.6261242838027\n",
      "Epoch 2 step 743: training accuarcy: 0.9931000000000001\n",
      "Epoch 2 step 743: training loss: 2961.4681745776843\n",
      "Epoch 2 step 744: training accuarcy: 0.9947\n",
      "Epoch 2 step 744: training loss: 3205.1517761558534\n",
      "Epoch 2 step 745: training accuarcy: 0.9928\n",
      "Epoch 2 step 745: training loss: 3224.17834648957\n",
      "Epoch 2 step 746: training accuarcy: 0.9939\n",
      "Epoch 2 step 746: training loss: 3258.139322190708\n",
      "Epoch 2 step 747: training accuarcy: 0.9909\n",
      "Epoch 2 step 747: training loss: 2877.0700505356676\n",
      "Epoch 2 step 748: training accuarcy: 0.9977\n",
      "Epoch 2 step 748: training loss: 3107.9320572671168\n",
      "Epoch 2 step 749: training accuarcy: 0.9938\n",
      "Epoch 2 step 749: training loss: 3085.005247547433\n",
      "Epoch 2 step 750: training accuarcy: 0.994\n",
      "Epoch 2 step 750: training loss: 2871.782426942408\n",
      "Epoch 2 step 751: training accuarcy: 0.9959\n",
      "Epoch 2 step 751: training loss: 3053.708780896621\n",
      "Epoch 2 step 752: training accuarcy: 0.9945\n",
      "Epoch 2 step 752: training loss: 2939.708846653879\n",
      "Epoch 2 step 753: training accuarcy: 0.9955\n",
      "Epoch 2 step 753: training loss: 3101.334915069524\n",
      "Epoch 2 step 754: training accuarcy: 0.9937\n",
      "Epoch 2 step 754: training loss: 2889.677767050205\n",
      "Epoch 2 step 755: training accuarcy: 0.9957\n",
      "Epoch 2 step 755: training loss: 3106.94055189997\n",
      "Epoch 2 step 756: training accuarcy: 0.9934000000000001\n",
      "Epoch 2 step 756: training loss: 2807.2557873074684\n",
      "Epoch 2 step 757: training accuarcy: 0.9977\n",
      "Epoch 2 step 757: training loss: 2922.9605749069397\n",
      "Epoch 2 step 758: training accuarcy: 0.9945\n",
      "Epoch 2 step 758: training loss: 3021.051618569402\n",
      "Epoch 2 step 759: training accuarcy: 0.9931000000000001\n",
      "Epoch 2 step 759: training loss: 2845.13526048805\n",
      "Epoch 2 step 760: training accuarcy: 0.9965\n",
      "Epoch 2 step 760: training loss: 2835.6248344463256\n",
      "Epoch 2 step 761: training accuarcy: 0.9981000000000001\n",
      "Epoch 2 step 761: training loss: 2935.7164943341013\n",
      "Epoch 2 step 762: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 762: training loss: 2803.169595133986\n",
      "Epoch 2 step 763: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 763: training loss: 2984.060494066367\n",
      "Epoch 2 step 764: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 764: training loss: 2836.3816725333654\n",
      "Epoch 2 step 765: training accuarcy: 0.997\n",
      "Epoch 2 step 765: training loss: 2881.9041257716262\n",
      "Epoch 2 step 766: training accuarcy: 0.9959\n",
      "Epoch 2 step 766: training loss: 3122.216476310705\n",
      "Epoch 2 step 767: training accuarcy: 0.9916\n",
      "Epoch 2 step 767: training loss: 3115.7819896079022\n",
      "Epoch 2 step 768: training accuarcy: 0.9928\n",
      "Epoch 2 step 768: training loss: 2987.204187997322\n",
      "Epoch 2 step 769: training accuarcy: 0.9948\n",
      "Epoch 2 step 769: training loss: 2790.5135685728346\n",
      "Epoch 2 step 770: training accuarcy: 0.9974000000000001\n",
      "Epoch 2 step 770: training loss: 2972.701783000728\n",
      "Epoch 2 step 771: training accuarcy: 0.9948\n",
      "Epoch 2 step 771: training loss: 2848.5194950118935\n",
      "Epoch 2 step 772: training accuarcy: 0.9959\n",
      "Epoch 2 step 772: training loss: 3065.9653765286075\n",
      "Epoch 2 step 773: training accuarcy: 0.9932000000000001\n",
      "Epoch 2 step 773: training loss: 2794.284189296995\n",
      "Epoch 2 step 774: training accuarcy: 0.9961000000000001\n",
      "Epoch 2 step 774: training loss: 2850.787944047416\n",
      "Epoch 2 step 775: training accuarcy: 0.9957\n",
      "Epoch 2 step 775: training loss: 2878.8481362079683\n",
      "Epoch 2 step 776: training accuarcy: 0.9948\n",
      "Epoch 2 step 776: training loss: 2875.5821691032265\n",
      "Epoch 2 step 777: training accuarcy: 0.996\n",
      "Epoch 2 step 777: training loss: 2978.1554223886587\n",
      "Epoch 2 step 778: training accuarcy: 0.9922000000000001\n",
      "Epoch 2 step 778: training loss: 2909.6197048614304\n",
      "Epoch 2 step 779: training accuarcy: 0.9947\n",
      "Epoch 2 step 779: training loss: 2917.244620465904\n",
      "Epoch 2 step 780: training accuarcy: 0.9954000000000001\n",
      "Epoch 2 step 780: training loss: 2909.17975647773\n",
      "Epoch 2 step 781: training accuarcy: 0.9942000000000001\n",
      "Epoch 2 step 781: training loss: 2828.4751487965264\n",
      "Epoch 2 step 782: training accuarcy: 0.9949\n",
      "Epoch 2 step 782: training loss: 2916.3683263783887\n",
      "Epoch 2 step 783: training accuarcy: 0.9936\n",
      "Epoch 2 step 783: training loss: 2964.695481609655\n",
      "Epoch 2 step 784: training accuarcy: 0.994\n",
      "Epoch 2 step 784: training loss: 2830.4699081228987\n",
      "Epoch 2 step 785: training accuarcy: 0.9947\n",
      "Epoch 2 step 785: training loss: 2847.413845458139\n",
      "Epoch 2 step 786: training accuarcy: 0.9962000000000001\n",
      "Epoch 2 step 786: training loss: 2887.9039055105495\n",
      "Epoch 2 step 787: training accuarcy: 0.9949\n",
      "Epoch 2 step 787: training loss: 2805.0505817426233\n",
      "Epoch 2 step 788: training accuarcy: 0.9951000000000001\n",
      "Epoch 2 step 788: training loss: 2335.5849611159742\n",
      "Epoch 2 step 789: training accuarcy: 0.9953846153846154\n",
      "Epoch 2: train loss 3487.6549919167196, train accuarcy 0.9944347739219666\n",
      "Epoch 2: valid loss 4571.206134291568, valid accuarcy 0.9847785830497742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [14:23<00:00, 288.59s/it]"
     ]
    }
   ],
   "source": [
    "fm_learner.fit(epoch=3, \n",
    "               log_dir=get_log_dir(\"weight_topcoder\", \"fm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:41:29.079257Z",
     "start_time": "2019-10-09T11:41:29.070256Z"
    }
   },
   "outputs": [],
   "source": [
    "del fm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HRM FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:24:18.466108Z",
     "start_time": "2019-10-09T12:24:18.223111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchHrmFM()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_model = TorchHrmFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "hrm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:24:18.907719Z",
     "start_time": "2019-10-09T12:24:18.904716Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(hrm_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:24:19.464160Z",
     "start_time": "2019-10-09T12:24:19.418136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x20b8ec2b710>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrm_learner = FMLearner(hrm_model, adam_opt, schedular, db)\n",
    "hrm_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:14:32.789624Z",
     "start_time": "2019-10-08T01:14:32.785628Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T11:58:47.213679Z",
     "start_time": "2019-10-09T11:58:47.209675Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='seq',\n",
    "                    valid_col='seq',\n",
    "                    test_col='seq',\n",
    "                    loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:24:23.644511Z",
     "start_time": "2019-10-09T12:24:23.641537Z"
    }
   },
   "outputs": [],
   "source": [
    "hrm_learner.compile(train_col='seq',\n",
    "                    valid_col='seq',\n",
    "                    test_col='seq',\n",
    "                    loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:14:00.396159Z",
     "start_time": "2019-10-09T11:58:48.416705Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 67269.71110027473\n",
      "Epoch 0 step 1: training accuarcy: 0.505\n",
      "Epoch 0 step 1: training loss: 64582.12846184061\n",
      "Epoch 0 step 2: training accuarcy: 0.5214\n",
      "Epoch 0 step 2: training loss: 62127.96202200205\n",
      "Epoch 0 step 3: training accuarcy: 0.5177\n",
      "Epoch 0 step 3: training loss: 60768.18910236\n",
      "Epoch 0 step 4: training accuarcy: 0.5222\n",
      "Epoch 0 step 4: training loss: 60158.479771239865\n",
      "Epoch 0 step 5: training accuarcy: 0.5176000000000001\n",
      "Epoch 0 step 5: training loss: 58003.848322113045\n",
      "Epoch 0 step 6: training accuarcy: 0.5195000000000001\n",
      "Epoch 0 step 6: training loss: 56235.06773408047\n",
      "Epoch 0 step 7: training accuarcy: 0.529\n",
      "Epoch 0 step 7: training loss: 54929.95489985055\n",
      "Epoch 0 step 8: training accuarcy: 0.5187\n",
      "Epoch 0 step 8: training loss: 53652.64739050824\n",
      "Epoch 0 step 9: training accuarcy: 0.5205000000000001\n",
      "Epoch 0 step 9: training loss: 52567.50304010458\n",
      "Epoch 0 step 10: training accuarcy: 0.52\n",
      "Epoch 0 step 10: training loss: 51398.316919357516\n",
      "Epoch 0 step 11: training accuarcy: 0.5235000000000001\n",
      "Epoch 0 step 11: training loss: 49311.725650946384\n",
      "Epoch 0 step 12: training accuarcy: 0.5181\n",
      "Epoch 0 step 12: training loss: 48810.74921517825\n",
      "Epoch 0 step 13: training accuarcy: 0.5393\n",
      "Epoch 0 step 13: training loss: 46351.346601162324\n",
      "Epoch 0 step 14: training accuarcy: 0.5345\n",
      "Epoch 0 step 14: training loss: 45983.532859107945\n",
      "Epoch 0 step 15: training accuarcy: 0.5235000000000001\n",
      "Epoch 0 step 15: training loss: 44147.146320497326\n",
      "Epoch 0 step 16: training accuarcy: 0.5362\n",
      "Epoch 0 step 16: training loss: 43014.01670625856\n",
      "Epoch 0 step 17: training accuarcy: 0.5439\n",
      "Epoch 0 step 17: training loss: 42433.62385934102\n",
      "Epoch 0 step 18: training accuarcy: 0.5308\n",
      "Epoch 0 step 18: training loss: 40625.177306324375\n",
      "Epoch 0 step 19: training accuarcy: 0.5449\n",
      "Epoch 0 step 19: training loss: 40372.87492787087\n",
      "Epoch 0 step 20: training accuarcy: 0.5243\n",
      "Epoch 0 step 20: training loss: 39368.85589771\n",
      "Epoch 0 step 21: training accuarcy: 0.5259\n",
      "Epoch 0 step 21: training loss: 37366.99584853999\n",
      "Epoch 0 step 22: training accuarcy: 0.5314\n",
      "Epoch 0 step 22: training loss: 36880.40910670567\n",
      "Epoch 0 step 23: training accuarcy: 0.5334\n",
      "Epoch 0 step 23: training loss: 35778.59138647643\n",
      "Epoch 0 step 24: training accuarcy: 0.5352\n",
      "Epoch 0 step 24: training loss: 34710.037987868025\n",
      "Epoch 0 step 25: training accuarcy: 0.5387000000000001\n",
      "Epoch 0 step 25: training loss: 33667.28829162816\n",
      "Epoch 0 step 26: training accuarcy: 0.5262\n",
      "Epoch 0 step 26: training loss: 32858.8240039216\n",
      "Epoch 0 step 27: training accuarcy: 0.545\n",
      "Epoch 0 step 27: training loss: 32408.61791804302\n",
      "Epoch 0 step 28: training accuarcy: 0.529\n",
      "Epoch 0 step 28: training loss: 31277.762917566673\n",
      "Epoch 0 step 29: training accuarcy: 0.5483\n",
      "Epoch 0 step 29: training loss: 30233.975272244388\n",
      "Epoch 0 step 30: training accuarcy: 0.5509000000000001\n",
      "Epoch 0 step 30: training loss: 29943.80826723045\n",
      "Epoch 0 step 31: training accuarcy: 0.5449\n",
      "Epoch 0 step 31: training loss: 29150.183098047317\n",
      "Epoch 0 step 32: training accuarcy: 0.534\n",
      "Epoch 0 step 32: training loss: 28440.88833076197\n",
      "Epoch 0 step 33: training accuarcy: 0.5397000000000001\n",
      "Epoch 0 step 33: training loss: 27818.57052744129\n",
      "Epoch 0 step 34: training accuarcy: 0.5279\n",
      "Epoch 0 step 34: training loss: 27062.12963075623\n",
      "Epoch 0 step 35: training accuarcy: 0.5267000000000001\n",
      "Epoch 0 step 35: training loss: 26020.680571067358\n",
      "Epoch 0 step 36: training accuarcy: 0.5425\n",
      "Epoch 0 step 36: training loss: 25675.429139703872\n",
      "Epoch 0 step 37: training accuarcy: 0.5379\n",
      "Epoch 0 step 37: training loss: 25123.159786979566\n",
      "Epoch 0 step 38: training accuarcy: 0.5351\n",
      "Epoch 0 step 38: training loss: 24092.609858108575\n",
      "Epoch 0 step 39: training accuarcy: 0.5413\n",
      "Epoch 0 step 39: training loss: 23683.861617273287\n",
      "Epoch 0 step 40: training accuarcy: 0.5504\n",
      "Epoch 0 step 40: training loss: 23305.833790576315\n",
      "Epoch 0 step 41: training accuarcy: 0.5438000000000001\n",
      "Epoch 0 step 41: training loss: 22826.841352728592\n",
      "Epoch 0 step 42: training accuarcy: 0.5243\n",
      "Epoch 0 step 42: training loss: 22068.128537347206\n",
      "Epoch 0 step 43: training accuarcy: 0.5386000000000001\n",
      "Epoch 0 step 43: training loss: 21767.768867943392\n",
      "Epoch 0 step 44: training accuarcy: 0.535\n",
      "Epoch 0 step 44: training loss: 20765.077370393956\n",
      "Epoch 0 step 45: training accuarcy: 0.549\n",
      "Epoch 0 step 45: training loss: 20920.51826998534\n",
      "Epoch 0 step 46: training accuarcy: 0.5391\n",
      "Epoch 0 step 46: training loss: 20514.94818857925\n",
      "Epoch 0 step 47: training accuarcy: 0.5396000000000001\n",
      "Epoch 0 step 47: training loss: 19628.857768489543\n",
      "Epoch 0 step 48: training accuarcy: 0.5505\n",
      "Epoch 0 step 48: training loss: 19072.773140762634\n",
      "Epoch 0 step 49: training accuarcy: 0.551\n",
      "Epoch 0 step 49: training loss: 19158.1118722798\n",
      "Epoch 0 step 50: training accuarcy: 0.5339\n",
      "Epoch 0 step 50: training loss: 18530.024431754762\n",
      "Epoch 0 step 51: training accuarcy: 0.552\n",
      "Epoch 0 step 51: training loss: 18084.565018917394\n",
      "Epoch 0 step 52: training accuarcy: 0.551\n",
      "Epoch 0 step 52: training loss: 17981.14371296351\n",
      "Epoch 0 step 53: training accuarcy: 0.5488000000000001\n",
      "Epoch 0 step 53: training loss: 17418.20592674864\n",
      "Epoch 0 step 54: training accuarcy: 0.557\n",
      "Epoch 0 step 54: training loss: 17230.391318382208\n",
      "Epoch 0 step 55: training accuarcy: 0.5415\n",
      "Epoch 0 step 55: training loss: 16955.063307558354\n",
      "Epoch 0 step 56: training accuarcy: 0.5423\n",
      "Epoch 0 step 56: training loss: 16412.38566011442\n",
      "Epoch 0 step 57: training accuarcy: 0.5524\n",
      "Epoch 0 step 57: training loss: 16199.128256809314\n",
      "Epoch 0 step 58: training accuarcy: 0.5517000000000001\n",
      "Epoch 0 step 58: training loss: 16003.237183850591\n",
      "Epoch 0 step 59: training accuarcy: 0.5537000000000001\n",
      "Epoch 0 step 59: training loss: 15554.26397733155\n",
      "Epoch 0 step 60: training accuarcy: 0.5602\n",
      "Epoch 0 step 60: training loss: 15247.085784469655\n",
      "Epoch 0 step 61: training accuarcy: 0.5675\n",
      "Epoch 0 step 61: training loss: 15201.139271271419\n",
      "Epoch 0 step 62: training accuarcy: 0.5536\n",
      "Epoch 0 step 62: training loss: 14879.947066811335\n",
      "Epoch 0 step 63: training accuarcy: 0.5589000000000001\n",
      "Epoch 0 step 63: training loss: 14690.851170142974\n",
      "Epoch 0 step 64: training accuarcy: 0.5623\n",
      "Epoch 0 step 64: training loss: 14249.974315121994\n",
      "Epoch 0 step 65: training accuarcy: 0.5742\n",
      "Epoch 0 step 65: training loss: 14092.20258559256\n",
      "Epoch 0 step 66: training accuarcy: 0.5736\n",
      "Epoch 0 step 66: training loss: 13951.975300610156\n",
      "Epoch 0 step 67: training accuarcy: 0.5653\n",
      "Epoch 0 step 67: training loss: 13875.997604601029\n",
      "Epoch 0 step 68: training accuarcy: 0.553\n",
      "Epoch 0 step 68: training loss: 13647.285656685535\n",
      "Epoch 0 step 69: training accuarcy: 0.5642\n",
      "Epoch 0 step 69: training loss: 13528.55348230774\n",
      "Epoch 0 step 70: training accuarcy: 0.5633\n",
      "Epoch 0 step 70: training loss: 13251.84275469038\n",
      "Epoch 0 step 71: training accuarcy: 0.5664\n",
      "Epoch 0 step 71: training loss: 13104.815778562745\n",
      "Epoch 0 step 72: training accuarcy: 0.5677\n",
      "Epoch 0 step 72: training loss: 12867.438948747098\n",
      "Epoch 0 step 73: training accuarcy: 0.5728\n",
      "Epoch 0 step 73: training loss: 12626.786762217527\n",
      "Epoch 0 step 74: training accuarcy: 0.5831000000000001\n",
      "Epoch 0 step 74: training loss: 12535.131034760047\n",
      "Epoch 0 step 75: training accuarcy: 0.5764\n",
      "Epoch 0 step 75: training loss: 12417.993234139321\n",
      "Epoch 0 step 76: training accuarcy: 0.5775\n",
      "Epoch 0 step 76: training loss: 12183.000206815741\n",
      "Epoch 0 step 77: training accuarcy: 0.5897\n",
      "Epoch 0 step 77: training loss: 12087.898050990178\n",
      "Epoch 0 step 78: training accuarcy: 0.5857\n",
      "Epoch 0 step 78: training loss: 12054.324170806041\n",
      "Epoch 0 step 79: training accuarcy: 0.5814\n",
      "Epoch 0 step 79: training loss: 11840.283461106843\n",
      "Epoch 0 step 80: training accuarcy: 0.5827\n",
      "Epoch 0 step 80: training loss: 11781.252045093923\n",
      "Epoch 0 step 81: training accuarcy: 0.5752\n",
      "Epoch 0 step 81: training loss: 11575.635950037135\n",
      "Epoch 0 step 82: training accuarcy: 0.5922000000000001\n",
      "Epoch 0 step 82: training loss: 11509.87292863354\n",
      "Epoch 0 step 83: training accuarcy: 0.5852\n",
      "Epoch 0 step 83: training loss: 11445.124919555557\n",
      "Epoch 0 step 84: training accuarcy: 0.5787\n",
      "Epoch 0 step 84: training loss: 11292.780834293204\n",
      "Epoch 0 step 85: training accuarcy: 0.5840000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 85: training loss: 11096.330334479437\n",
      "Epoch 0 step 86: training accuarcy: 0.5952000000000001\n",
      "Epoch 0 step 86: training loss: 10942.775706173958\n",
      "Epoch 0 step 87: training accuarcy: 0.5995\n",
      "Epoch 0 step 87: training loss: 10942.955861821483\n",
      "Epoch 0 step 88: training accuarcy: 0.5996\n",
      "Epoch 0 step 88: training loss: 10831.690345903531\n",
      "Epoch 0 step 89: training accuarcy: 0.6018\n",
      "Epoch 0 step 89: training loss: 10771.428398885564\n",
      "Epoch 0 step 90: training accuarcy: 0.5910000000000001\n",
      "Epoch 0 step 90: training loss: 10587.251798078165\n",
      "Epoch 0 step 91: training accuarcy: 0.6021000000000001\n",
      "Epoch 0 step 91: training loss: 10574.731046866284\n",
      "Epoch 0 step 92: training accuarcy: 0.6083000000000001\n",
      "Epoch 0 step 92: training loss: 10456.53724118756\n",
      "Epoch 0 step 93: training accuarcy: 0.6053000000000001\n",
      "Epoch 0 step 93: training loss: 10391.278152943694\n",
      "Epoch 0 step 94: training accuarcy: 0.5991000000000001\n",
      "Epoch 0 step 94: training loss: 10311.247992718909\n",
      "Epoch 0 step 95: training accuarcy: 0.6114\n",
      "Epoch 0 step 95: training loss: 10230.569067425688\n",
      "Epoch 0 step 96: training accuarcy: 0.6013000000000001\n",
      "Epoch 0 step 96: training loss: 9978.836108381962\n",
      "Epoch 0 step 97: training accuarcy: 0.6163000000000001\n",
      "Epoch 0 step 97: training loss: 9998.451746815734\n",
      "Epoch 0 step 98: training accuarcy: 0.6187\n",
      "Epoch 0 step 98: training loss: 9985.964902362035\n",
      "Epoch 0 step 99: training accuarcy: 0.6157\n",
      "Epoch 0 step 99: training loss: 9921.812068391211\n",
      "Epoch 0 step 100: training accuarcy: 0.6093000000000001\n",
      "Epoch 0 step 100: training loss: 9814.264032745963\n",
      "Epoch 0 step 101: training accuarcy: 0.6207\n",
      "Epoch 0 step 101: training loss: 9650.760305556918\n",
      "Epoch 0 step 102: training accuarcy: 0.6255000000000001\n",
      "Epoch 0 step 102: training loss: 9642.422930568098\n",
      "Epoch 0 step 103: training accuarcy: 0.6327\n",
      "Epoch 0 step 103: training loss: 9519.913985832176\n",
      "Epoch 0 step 104: training accuarcy: 0.6312\n",
      "Epoch 0 step 104: training loss: 9493.274681936005\n",
      "Epoch 0 step 105: training accuarcy: 0.6233000000000001\n",
      "Epoch 0 step 105: training loss: 9500.077207708844\n",
      "Epoch 0 step 106: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 106: training loss: 9399.589301498192\n",
      "Epoch 0 step 107: training accuarcy: 0.6262\n",
      "Epoch 0 step 107: training loss: 9320.266203763622\n",
      "Epoch 0 step 108: training accuarcy: 0.6272\n",
      "Epoch 0 step 108: training loss: 9195.385134111246\n",
      "Epoch 0 step 109: training accuarcy: 0.6463\n",
      "Epoch 0 step 109: training loss: 9244.635023980063\n",
      "Epoch 0 step 110: training accuarcy: 0.6309\n",
      "Epoch 0 step 110: training loss: 9106.127139001765\n",
      "Epoch 0 step 111: training accuarcy: 0.6444000000000001\n",
      "Epoch 0 step 111: training loss: 9044.433208834933\n",
      "Epoch 0 step 112: training accuarcy: 0.6456000000000001\n",
      "Epoch 0 step 112: training loss: 9108.292584051409\n",
      "Epoch 0 step 113: training accuarcy: 0.6303000000000001\n",
      "Epoch 0 step 113: training loss: 8999.368081639228\n",
      "Epoch 0 step 114: training accuarcy: 0.638\n",
      "Epoch 0 step 114: training loss: 8900.243234554177\n",
      "Epoch 0 step 115: training accuarcy: 0.6571\n",
      "Epoch 0 step 115: training loss: 8878.71784601279\n",
      "Epoch 0 step 116: training accuarcy: 0.6399\n",
      "Epoch 0 step 116: training loss: 8869.386334206269\n",
      "Epoch 0 step 117: training accuarcy: 0.6488\n",
      "Epoch 0 step 117: training loss: 8834.244447151414\n",
      "Epoch 0 step 118: training accuarcy: 0.6417\n",
      "Epoch 0 step 118: training loss: 8762.284431377311\n",
      "Epoch 0 step 119: training accuarcy: 0.6448\n",
      "Epoch 0 step 119: training loss: 8713.442538516534\n",
      "Epoch 0 step 120: training accuarcy: 0.6467\n",
      "Epoch 0 step 120: training loss: 8595.456024646945\n",
      "Epoch 0 step 121: training accuarcy: 0.6461\n",
      "Epoch 0 step 121: training loss: 8638.475952145256\n",
      "Epoch 0 step 122: training accuarcy: 0.6422\n",
      "Epoch 0 step 122: training loss: 8594.289908860625\n",
      "Epoch 0 step 123: training accuarcy: 0.645\n",
      "Epoch 0 step 123: training loss: 8614.044351920973\n",
      "Epoch 0 step 124: training accuarcy: 0.6403\n",
      "Epoch 0 step 124: training loss: 8460.737522851741\n",
      "Epoch 0 step 125: training accuarcy: 0.6529\n",
      "Epoch 0 step 125: training loss: 8465.956450132468\n",
      "Epoch 0 step 126: training accuarcy: 0.6518\n",
      "Epoch 0 step 126: training loss: 8471.099783809499\n",
      "Epoch 0 step 127: training accuarcy: 0.6412\n",
      "Epoch 0 step 127: training loss: 8354.962017797778\n",
      "Epoch 0 step 128: training accuarcy: 0.6631\n",
      "Epoch 0 step 128: training loss: 8335.249672222692\n",
      "Epoch 0 step 129: training accuarcy: 0.6585000000000001\n",
      "Epoch 0 step 129: training loss: 8222.11745229274\n",
      "Epoch 0 step 130: training accuarcy: 0.6665\n",
      "Epoch 0 step 130: training loss: 8283.08513661917\n",
      "Epoch 0 step 131: training accuarcy: 0.6616000000000001\n",
      "Epoch 0 step 131: training loss: 8231.154233288964\n",
      "Epoch 0 step 132: training accuarcy: 0.6586000000000001\n",
      "Epoch 0 step 132: training loss: 8249.676148690465\n",
      "Epoch 0 step 133: training accuarcy: 0.6567000000000001\n",
      "Epoch 0 step 133: training loss: 8243.539027371926\n",
      "Epoch 0 step 134: training accuarcy: 0.6567000000000001\n",
      "Epoch 0 step 134: training loss: 8091.978043367824\n",
      "Epoch 0 step 135: training accuarcy: 0.6631\n",
      "Epoch 0 step 135: training loss: 8085.210101543957\n",
      "Epoch 0 step 136: training accuarcy: 0.6627000000000001\n",
      "Epoch 0 step 136: training loss: 8018.847246781318\n",
      "Epoch 0 step 137: training accuarcy: 0.6704\n",
      "Epoch 0 step 137: training loss: 8087.556965769005\n",
      "Epoch 0 step 138: training accuarcy: 0.6598\n",
      "Epoch 0 step 138: training loss: 7984.967977489525\n",
      "Epoch 0 step 139: training accuarcy: 0.6723\n",
      "Epoch 0 step 139: training loss: 7948.712671790704\n",
      "Epoch 0 step 140: training accuarcy: 0.6717000000000001\n",
      "Epoch 0 step 140: training loss: 7905.884243811237\n",
      "Epoch 0 step 141: training accuarcy: 0.6745\n",
      "Epoch 0 step 141: training loss: 7904.525189299236\n",
      "Epoch 0 step 142: training accuarcy: 0.6764\n",
      "Epoch 0 step 142: training loss: 7854.763936581972\n",
      "Epoch 0 step 143: training accuarcy: 0.6757000000000001\n",
      "Epoch 0 step 143: training loss: 7912.349002393374\n",
      "Epoch 0 step 144: training accuarcy: 0.6735\n",
      "Epoch 0 step 144: training loss: 7864.322997574124\n",
      "Epoch 0 step 145: training accuarcy: 0.6718000000000001\n",
      "Epoch 0 step 145: training loss: 7819.1113722211585\n",
      "Epoch 0 step 146: training accuarcy: 0.6785\n",
      "Epoch 0 step 146: training loss: 7805.306065355646\n",
      "Epoch 0 step 147: training accuarcy: 0.671\n",
      "Epoch 0 step 147: training loss: 7823.237972084265\n",
      "Epoch 0 step 148: training accuarcy: 0.6662\n",
      "Epoch 0 step 148: training loss: 7659.595313937152\n",
      "Epoch 0 step 149: training accuarcy: 0.6937\n",
      "Epoch 0 step 149: training loss: 7706.9288565192455\n",
      "Epoch 0 step 150: training accuarcy: 0.6827000000000001\n",
      "Epoch 0 step 150: training loss: 7624.298753386738\n",
      "Epoch 0 step 151: training accuarcy: 0.6837000000000001\n",
      "Epoch 0 step 151: training loss: 7572.685772770485\n",
      "Epoch 0 step 152: training accuarcy: 0.6848000000000001\n",
      "Epoch 0 step 152: training loss: 7521.426135303975\n",
      "Epoch 0 step 153: training accuarcy: 0.6933\n",
      "Epoch 0 step 153: training loss: 7678.235991271428\n",
      "Epoch 0 step 154: training accuarcy: 0.6724\n",
      "Epoch 0 step 154: training loss: 7491.918367295117\n",
      "Epoch 0 step 155: training accuarcy: 0.7002\n",
      "Epoch 0 step 155: training loss: 7503.459574860554\n",
      "Epoch 0 step 156: training accuarcy: 0.6940000000000001\n",
      "Epoch 0 step 156: training loss: 7604.524080827993\n",
      "Epoch 0 step 157: training accuarcy: 0.6795\n",
      "Epoch 0 step 157: training loss: 7454.695060265081\n",
      "Epoch 0 step 158: training accuarcy: 0.6881\n",
      "Epoch 0 step 158: training loss: 7493.517672505615\n",
      "Epoch 0 step 159: training accuarcy: 0.6915\n",
      "Epoch 0 step 159: training loss: 7394.53856653668\n",
      "Epoch 0 step 160: training accuarcy: 0.6965\n",
      "Epoch 0 step 160: training loss: 7463.201228642771\n",
      "Epoch 0 step 161: training accuarcy: 0.6943\n",
      "Epoch 0 step 161: training loss: 7458.136827993816\n",
      "Epoch 0 step 162: training accuarcy: 0.6873\n",
      "Epoch 0 step 162: training loss: 7306.718075181015\n",
      "Epoch 0 step 163: training accuarcy: 0.6968000000000001\n",
      "Epoch 0 step 163: training loss: 7438.315743577026\n",
      "Epoch 0 step 164: training accuarcy: 0.6941\n",
      "Epoch 0 step 164: training loss: 7402.752980436914\n",
      "Epoch 0 step 165: training accuarcy: 0.6932\n",
      "Epoch 0 step 165: training loss: 7313.955503036314\n",
      "Epoch 0 step 166: training accuarcy: 0.7053\n",
      "Epoch 0 step 166: training loss: 7296.133660301853\n",
      "Epoch 0 step 167: training accuarcy: 0.7086\n",
      "Epoch 0 step 167: training loss: 7328.063930230292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 168: training accuarcy: 0.6943\n",
      "Epoch 0 step 168: training loss: 7368.357885820251\n",
      "Epoch 0 step 169: training accuarcy: 0.6969000000000001\n",
      "Epoch 0 step 169: training loss: 7305.313833381052\n",
      "Epoch 0 step 170: training accuarcy: 0.6973\n",
      "Epoch 0 step 170: training loss: 7218.462005929325\n",
      "Epoch 0 step 171: training accuarcy: 0.7048\n",
      "Epoch 0 step 171: training loss: 7362.048658092097\n",
      "Epoch 0 step 172: training accuarcy: 0.6807000000000001\n",
      "Epoch 0 step 172: training loss: 7287.953607258028\n",
      "Epoch 0 step 173: training accuarcy: 0.6861\n",
      "Epoch 0 step 173: training loss: 7179.1870674379525\n",
      "Epoch 0 step 174: training accuarcy: 0.7009000000000001\n",
      "Epoch 0 step 174: training loss: 7143.729533500957\n",
      "Epoch 0 step 175: training accuarcy: 0.7035\n",
      "Epoch 0 step 175: training loss: 7166.902411097375\n",
      "Epoch 0 step 176: training accuarcy: 0.6954\n",
      "Epoch 0 step 176: training loss: 7129.452412200186\n",
      "Epoch 0 step 177: training accuarcy: 0.7050000000000001\n",
      "Epoch 0 step 177: training loss: 7172.740418741923\n",
      "Epoch 0 step 178: training accuarcy: 0.7034\n",
      "Epoch 0 step 178: training loss: 7109.325977800729\n",
      "Epoch 0 step 179: training accuarcy: 0.7036\n",
      "Epoch 0 step 179: training loss: 7115.787973274282\n",
      "Epoch 0 step 180: training accuarcy: 0.6866\n",
      "Epoch 0 step 180: training loss: 7078.5677284393005\n",
      "Epoch 0 step 181: training accuarcy: 0.7087\n",
      "Epoch 0 step 181: training loss: 7096.473336464571\n",
      "Epoch 0 step 182: training accuarcy: 0.7046\n",
      "Epoch 0 step 182: training loss: 7046.826452443326\n",
      "Epoch 0 step 183: training accuarcy: 0.7027\n",
      "Epoch 0 step 183: training loss: 7111.885270561386\n",
      "Epoch 0 step 184: training accuarcy: 0.7038\n",
      "Epoch 0 step 184: training loss: 7073.268802341452\n",
      "Epoch 0 step 185: training accuarcy: 0.7063\n",
      "Epoch 0 step 185: training loss: 6984.538155834438\n",
      "Epoch 0 step 186: training accuarcy: 0.7104\n",
      "Epoch 0 step 186: training loss: 6976.091835230107\n",
      "Epoch 0 step 187: training accuarcy: 0.7060000000000001\n",
      "Epoch 0 step 187: training loss: 7037.200433115546\n",
      "Epoch 0 step 188: training accuarcy: 0.7031000000000001\n",
      "Epoch 0 step 188: training loss: 6962.7053340808725\n",
      "Epoch 0 step 189: training accuarcy: 0.7105\n",
      "Epoch 0 step 189: training loss: 6913.786002471932\n",
      "Epoch 0 step 190: training accuarcy: 0.7165\n",
      "Epoch 0 step 190: training loss: 6980.336004926457\n",
      "Epoch 0 step 191: training accuarcy: 0.7095\n",
      "Epoch 0 step 191: training loss: 6939.1662281955305\n",
      "Epoch 0 step 192: training accuarcy: 0.7075\n",
      "Epoch 0 step 192: training loss: 6877.339641940351\n",
      "Epoch 0 step 193: training accuarcy: 0.7137\n",
      "Epoch 0 step 193: training loss: 7012.048064726993\n",
      "Epoch 0 step 194: training accuarcy: 0.7068\n",
      "Epoch 0 step 194: training loss: 6880.196754041593\n",
      "Epoch 0 step 195: training accuarcy: 0.7170000000000001\n",
      "Epoch 0 step 195: training loss: 6924.398161975084\n",
      "Epoch 0 step 196: training accuarcy: 0.7086\n",
      "Epoch 0 step 196: training loss: 6929.21290681234\n",
      "Epoch 0 step 197: training accuarcy: 0.7090000000000001\n",
      "Epoch 0 step 197: training loss: 6877.485724517166\n",
      "Epoch 0 step 198: training accuarcy: 0.7110000000000001\n",
      "Epoch 0 step 198: training loss: 6802.9779097552055\n",
      "Epoch 0 step 199: training accuarcy: 0.7160000000000001\n",
      "Epoch 0 step 199: training loss: 6858.582725711627\n",
      "Epoch 0 step 200: training accuarcy: 0.7105\n",
      "Epoch 0 step 200: training loss: 6823.2093863308855\n",
      "Epoch 0 step 201: training accuarcy: 0.7139\n",
      "Epoch 0 step 201: training loss: 6790.264810904549\n",
      "Epoch 0 step 202: training accuarcy: 0.7142000000000001\n",
      "Epoch 0 step 202: training loss: 6836.72106835969\n",
      "Epoch 0 step 203: training accuarcy: 0.7136\n",
      "Epoch 0 step 203: training loss: 6763.536763564859\n",
      "Epoch 0 step 204: training accuarcy: 0.7219\n",
      "Epoch 0 step 204: training loss: 6853.630485906923\n",
      "Epoch 0 step 205: training accuarcy: 0.7112\n",
      "Epoch 0 step 205: training loss: 6809.043267118621\n",
      "Epoch 0 step 206: training accuarcy: 0.7110000000000001\n",
      "Epoch 0 step 206: training loss: 6751.432656763314\n",
      "Epoch 0 step 207: training accuarcy: 0.7170000000000001\n",
      "Epoch 0 step 207: training loss: 6764.015777138733\n",
      "Epoch 0 step 208: training accuarcy: 0.7128\n",
      "Epoch 0 step 208: training loss: 6712.066651984171\n",
      "Epoch 0 step 209: training accuarcy: 0.7168\n",
      "Epoch 0 step 209: training loss: 6814.765025238106\n",
      "Epoch 0 step 210: training accuarcy: 0.7086\n",
      "Epoch 0 step 210: training loss: 6806.010244908612\n",
      "Epoch 0 step 211: training accuarcy: 0.7004\n",
      "Epoch 0 step 211: training loss: 6724.4614890896855\n",
      "Epoch 0 step 212: training accuarcy: 0.7220000000000001\n",
      "Epoch 0 step 212: training loss: 6794.047766040079\n",
      "Epoch 0 step 213: training accuarcy: 0.7033\n",
      "Epoch 0 step 213: training loss: 6729.241395523447\n",
      "Epoch 0 step 214: training accuarcy: 0.7077\n",
      "Epoch 0 step 214: training loss: 6711.627414992067\n",
      "Epoch 0 step 215: training accuarcy: 0.7126\n",
      "Epoch 0 step 215: training loss: 6722.479010746651\n",
      "Epoch 0 step 216: training accuarcy: 0.7136\n",
      "Epoch 0 step 216: training loss: 6630.580967637137\n",
      "Epoch 0 step 217: training accuarcy: 0.7149\n",
      "Epoch 0 step 217: training loss: 6726.061565942041\n",
      "Epoch 0 step 218: training accuarcy: 0.7018\n",
      "Epoch 0 step 218: training loss: 6648.253794325571\n",
      "Epoch 0 step 219: training accuarcy: 0.7248\n",
      "Epoch 0 step 219: training loss: 6607.079357363955\n",
      "Epoch 0 step 220: training accuarcy: 0.7281000000000001\n",
      "Epoch 0 step 220: training loss: 6584.886247539992\n",
      "Epoch 0 step 221: training accuarcy: 0.7316\n",
      "Epoch 0 step 221: training loss: 6591.382355794852\n",
      "Epoch 0 step 222: training accuarcy: 0.7366\n",
      "Epoch 0 step 222: training loss: 6639.034024714785\n",
      "Epoch 0 step 223: training accuarcy: 0.7199\n",
      "Epoch 0 step 223: training loss: 6608.014896047879\n",
      "Epoch 0 step 224: training accuarcy: 0.7258\n",
      "Epoch 0 step 224: training loss: 6611.542235161222\n",
      "Epoch 0 step 225: training accuarcy: 0.7156\n",
      "Epoch 0 step 225: training loss: 6675.113425831235\n",
      "Epoch 0 step 226: training accuarcy: 0.7037\n",
      "Epoch 0 step 226: training loss: 6654.400841117495\n",
      "Epoch 0 step 227: training accuarcy: 0.7257\n",
      "Epoch 0 step 227: training loss: 6676.526500432138\n",
      "Epoch 0 step 228: training accuarcy: 0.7117\n",
      "Epoch 0 step 228: training loss: 6606.799255007993\n",
      "Epoch 0 step 229: training accuarcy: 0.7219\n",
      "Epoch 0 step 229: training loss: 6631.723080357534\n",
      "Epoch 0 step 230: training accuarcy: 0.7204\n",
      "Epoch 0 step 230: training loss: 6583.65230353181\n",
      "Epoch 0 step 231: training accuarcy: 0.7120000000000001\n",
      "Epoch 0 step 231: training loss: 6532.297223028556\n",
      "Epoch 0 step 232: training accuarcy: 0.7151000000000001\n",
      "Epoch 0 step 232: training loss: 6619.997002177751\n",
      "Epoch 0 step 233: training accuarcy: 0.7178\n",
      "Epoch 0 step 233: training loss: 6574.695495348595\n",
      "Epoch 0 step 234: training accuarcy: 0.7215\n",
      "Epoch 0 step 234: training loss: 6552.146874378737\n",
      "Epoch 0 step 235: training accuarcy: 0.7178\n",
      "Epoch 0 step 235: training loss: 6578.275549592841\n",
      "Epoch 0 step 236: training accuarcy: 0.7148\n",
      "Epoch 0 step 236: training loss: 6630.022317988135\n",
      "Epoch 0 step 237: training accuarcy: 0.7162000000000001\n",
      "Epoch 0 step 237: training loss: 6420.501574671681\n",
      "Epoch 0 step 238: training accuarcy: 0.7402000000000001\n",
      "Epoch 0 step 238: training loss: 6544.062851724769\n",
      "Epoch 0 step 239: training accuarcy: 0.7150000000000001\n",
      "Epoch 0 step 239: training loss: 6527.833823968925\n",
      "Epoch 0 step 240: training accuarcy: 0.7253000000000001\n",
      "Epoch 0 step 240: training loss: 6554.145903128097\n",
      "Epoch 0 step 241: training accuarcy: 0.7134\n",
      "Epoch 0 step 241: training loss: 6554.695700857084\n",
      "Epoch 0 step 242: training accuarcy: 0.7144\n",
      "Epoch 0 step 242: training loss: 6485.998298570907\n",
      "Epoch 0 step 243: training accuarcy: 0.7294\n",
      "Epoch 0 step 243: training loss: 6401.2375681889225\n",
      "Epoch 0 step 244: training accuarcy: 0.7308\n",
      "Epoch 0 step 244: training loss: 6477.000260914687\n",
      "Epoch 0 step 245: training accuarcy: 0.7183\n",
      "Epoch 0 step 245: training loss: 6470.425658915503\n",
      "Epoch 0 step 246: training accuarcy: 0.7266\n",
      "Epoch 0 step 246: training loss: 6490.781742853228\n",
      "Epoch 0 step 247: training accuarcy: 0.7207\n",
      "Epoch 0 step 247: training loss: 6508.490523984857\n",
      "Epoch 0 step 248: training accuarcy: 0.7210000000000001\n",
      "Epoch 0 step 248: training loss: 6403.691144526527\n",
      "Epoch 0 step 249: training accuarcy: 0.7193\n",
      "Epoch 0 step 249: training loss: 6418.882590371519\n",
      "Epoch 0 step 250: training accuarcy: 0.7264\n",
      "Epoch 0 step 250: training loss: 6359.235740115769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 251: training accuarcy: 0.7265\n",
      "Epoch 0 step 251: training loss: 6444.195912060558\n",
      "Epoch 0 step 252: training accuarcy: 0.7342000000000001\n",
      "Epoch 0 step 252: training loss: 6471.524739626552\n",
      "Epoch 0 step 253: training accuarcy: 0.7113\n",
      "Epoch 0 step 253: training loss: 6352.942340754675\n",
      "Epoch 0 step 254: training accuarcy: 0.728\n",
      "Epoch 0 step 254: training loss: 6440.121396515456\n",
      "Epoch 0 step 255: training accuarcy: 0.7275\n",
      "Epoch 0 step 255: training loss: 6394.942488010187\n",
      "Epoch 0 step 256: training accuarcy: 0.7234\n",
      "Epoch 0 step 256: training loss: 6440.387036586294\n",
      "Epoch 0 step 257: training accuarcy: 0.7189\n",
      "Epoch 0 step 257: training loss: 6441.134515679929\n",
      "Epoch 0 step 258: training accuarcy: 0.7188\n",
      "Epoch 0 step 258: training loss: 6436.9168380502615\n",
      "Epoch 0 step 259: training accuarcy: 0.7164\n",
      "Epoch 0 step 259: training loss: 6428.164205314782\n",
      "Epoch 0 step 260: training accuarcy: 0.7286\n",
      "Epoch 0 step 260: training loss: 6357.88764939358\n",
      "Epoch 0 step 261: training accuarcy: 0.7303000000000001\n",
      "Epoch 0 step 261: training loss: 6408.182094551605\n",
      "Epoch 0 step 262: training accuarcy: 0.7158\n",
      "Epoch 0 step 262: training loss: 2781.6804732012774\n",
      "Epoch 0 step 263: training accuarcy: 0.7271794871794872\n",
      "Epoch 0: train loss 14135.164649383027, train accuarcy 0.6299827694892883\n",
      "Epoch 0: valid loss 6121.680600967492, valid accuarcy 0.7406310439109802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [05:07<10:15, 307.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 6352.020083370025\n",
      "Epoch 1 step 264: training accuarcy: 0.7343000000000001\n",
      "Epoch 1 step 264: training loss: 6356.78036107799\n",
      "Epoch 1 step 265: training accuarcy: 0.7353000000000001\n",
      "Epoch 1 step 265: training loss: 6289.819788772316\n",
      "Epoch 1 step 266: training accuarcy: 0.7278\n",
      "Epoch 1 step 266: training loss: 6388.43891003307\n",
      "Epoch 1 step 267: training accuarcy: 0.7266\n",
      "Epoch 1 step 267: training loss: 6251.877148081942\n",
      "Epoch 1 step 268: training accuarcy: 0.744\n",
      "Epoch 1 step 268: training loss: 6278.946983295689\n",
      "Epoch 1 step 269: training accuarcy: 0.7432000000000001\n",
      "Epoch 1 step 269: training loss: 6282.12112510797\n",
      "Epoch 1 step 270: training accuarcy: 0.7392000000000001\n",
      "Epoch 1 step 270: training loss: 6226.918788990642\n",
      "Epoch 1 step 271: training accuarcy: 0.7446\n",
      "Epoch 1 step 271: training loss: 6353.194015985156\n",
      "Epoch 1 step 272: training accuarcy: 0.7276\n",
      "Epoch 1 step 272: training loss: 6217.286224836532\n",
      "Epoch 1 step 273: training accuarcy: 0.7421\n",
      "Epoch 1 step 273: training loss: 6311.605649440896\n",
      "Epoch 1 step 274: training accuarcy: 0.7354\n",
      "Epoch 1 step 274: training loss: 6268.619558388492\n",
      "Epoch 1 step 275: training accuarcy: 0.7466\n",
      "Epoch 1 step 275: training loss: 6250.623755751555\n",
      "Epoch 1 step 276: training accuarcy: 0.7362000000000001\n",
      "Epoch 1 step 276: training loss: 6290.71297671862\n",
      "Epoch 1 step 277: training accuarcy: 0.732\n",
      "Epoch 1 step 277: training loss: 6327.338931964151\n",
      "Epoch 1 step 278: training accuarcy: 0.7263000000000001\n",
      "Epoch 1 step 278: training loss: 6234.024418312341\n",
      "Epoch 1 step 279: training accuarcy: 0.7447\n",
      "Epoch 1 step 279: training loss: 6357.704498810506\n",
      "Epoch 1 step 280: training accuarcy: 0.7256\n",
      "Epoch 1 step 280: training loss: 6354.370369196633\n",
      "Epoch 1 step 281: training accuarcy: 0.7354\n",
      "Epoch 1 step 281: training loss: 6217.227776740361\n",
      "Epoch 1 step 282: training accuarcy: 0.7458\n",
      "Epoch 1 step 282: training loss: 6292.996306455519\n",
      "Epoch 1 step 283: training accuarcy: 0.7365\n",
      "Epoch 1 step 283: training loss: 6331.3319242169355\n",
      "Epoch 1 step 284: training accuarcy: 0.727\n",
      "Epoch 1 step 284: training loss: 6339.661977686991\n",
      "Epoch 1 step 285: training accuarcy: 0.7307\n",
      "Epoch 1 step 285: training loss: 6235.592355618987\n",
      "Epoch 1 step 286: training accuarcy: 0.7325\n",
      "Epoch 1 step 286: training loss: 6165.337600523859\n",
      "Epoch 1 step 287: training accuarcy: 0.7371000000000001\n",
      "Epoch 1 step 287: training loss: 6265.440977049333\n",
      "Epoch 1 step 288: training accuarcy: 0.7281000000000001\n",
      "Epoch 1 step 288: training loss: 6209.808885352493\n",
      "Epoch 1 step 289: training accuarcy: 0.7354\n",
      "Epoch 1 step 289: training loss: 6169.041937486233\n",
      "Epoch 1 step 290: training accuarcy: 0.7409\n",
      "Epoch 1 step 290: training loss: 6330.5683941662055\n",
      "Epoch 1 step 291: training accuarcy: 0.7286\n",
      "Epoch 1 step 291: training loss: 6312.163542940316\n",
      "Epoch 1 step 292: training accuarcy: 0.7297\n",
      "Epoch 1 step 292: training loss: 6249.77693617505\n",
      "Epoch 1 step 293: training accuarcy: 0.7369\n",
      "Epoch 1 step 293: training loss: 6254.7456867343835\n",
      "Epoch 1 step 294: training accuarcy: 0.7322000000000001\n",
      "Epoch 1 step 294: training loss: 6258.490683740897\n",
      "Epoch 1 step 295: training accuarcy: 0.7299\n",
      "Epoch 1 step 295: training loss: 6199.577600849036\n",
      "Epoch 1 step 296: training accuarcy: 0.745\n",
      "Epoch 1 step 296: training loss: 6208.441996694195\n",
      "Epoch 1 step 297: training accuarcy: 0.7404000000000001\n",
      "Epoch 1 step 297: training loss: 6179.568285070614\n",
      "Epoch 1 step 298: training accuarcy: 0.7465\n",
      "Epoch 1 step 298: training loss: 6303.098244986142\n",
      "Epoch 1 step 299: training accuarcy: 0.7261000000000001\n",
      "Epoch 1 step 299: training loss: 6185.179869265013\n",
      "Epoch 1 step 300: training accuarcy: 0.7424000000000001\n",
      "Epoch 1 step 300: training loss: 6328.525472507785\n",
      "Epoch 1 step 301: training accuarcy: 0.7297\n",
      "Epoch 1 step 301: training loss: 6188.102219485057\n",
      "Epoch 1 step 302: training accuarcy: 0.7379\n",
      "Epoch 1 step 302: training loss: 6181.935258114196\n",
      "Epoch 1 step 303: training accuarcy: 0.7374\n",
      "Epoch 1 step 303: training loss: 6207.797525170257\n",
      "Epoch 1 step 304: training accuarcy: 0.7322000000000001\n",
      "Epoch 1 step 304: training loss: 6293.1167373298795\n",
      "Epoch 1 step 305: training accuarcy: 0.7282000000000001\n",
      "Epoch 1 step 305: training loss: 6185.916899966865\n",
      "Epoch 1 step 306: training accuarcy: 0.7345\n",
      "Epoch 1 step 306: training loss: 6268.424039858761\n",
      "Epoch 1 step 307: training accuarcy: 0.7257\n",
      "Epoch 1 step 307: training loss: 6128.440180806112\n",
      "Epoch 1 step 308: training accuarcy: 0.744\n",
      "Epoch 1 step 308: training loss: 6207.037328039078\n",
      "Epoch 1 step 309: training accuarcy: 0.7326\n",
      "Epoch 1 step 309: training loss: 6119.152756745251\n",
      "Epoch 1 step 310: training accuarcy: 0.7517\n",
      "Epoch 1 step 310: training loss: 6159.345888263051\n",
      "Epoch 1 step 311: training accuarcy: 0.7354\n",
      "Epoch 1 step 311: training loss: 6172.387968641207\n",
      "Epoch 1 step 312: training accuarcy: 0.7458\n",
      "Epoch 1 step 312: training loss: 6171.631022016886\n",
      "Epoch 1 step 313: training accuarcy: 0.7444000000000001\n",
      "Epoch 1 step 313: training loss: 6204.146507271551\n",
      "Epoch 1 step 314: training accuarcy: 0.7408\n",
      "Epoch 1 step 314: training loss: 6200.815161760755\n",
      "Epoch 1 step 315: training accuarcy: 0.7373000000000001\n",
      "Epoch 1 step 315: training loss: 6153.888218302715\n",
      "Epoch 1 step 316: training accuarcy: 0.7416\n",
      "Epoch 1 step 316: training loss: 6139.5126246329355\n",
      "Epoch 1 step 317: training accuarcy: 0.7457\n",
      "Epoch 1 step 317: training loss: 6167.491639713523\n",
      "Epoch 1 step 318: training accuarcy: 0.7387\n",
      "Epoch 1 step 318: training loss: 6082.526796797862\n",
      "Epoch 1 step 319: training accuarcy: 0.7489\n",
      "Epoch 1 step 319: training loss: 6168.617163927898\n",
      "Epoch 1 step 320: training accuarcy: 0.7399\n",
      "Epoch 1 step 320: training loss: 6151.591399502691\n",
      "Epoch 1 step 321: training accuarcy: 0.7368\n",
      "Epoch 1 step 321: training loss: 6104.7661814878065\n",
      "Epoch 1 step 322: training accuarcy: 0.7478\n",
      "Epoch 1 step 322: training loss: 6163.546158533501\n",
      "Epoch 1 step 323: training accuarcy: 0.738\n",
      "Epoch 1 step 323: training loss: 6146.609317005453\n",
      "Epoch 1 step 324: training accuarcy: 0.7438\n",
      "Epoch 1 step 324: training loss: 6176.516487860256\n",
      "Epoch 1 step 325: training accuarcy: 0.736\n",
      "Epoch 1 step 325: training loss: 6153.614364989495\n",
      "Epoch 1 step 326: training accuarcy: 0.7377\n",
      "Epoch 1 step 326: training loss: 6137.523886585071\n",
      "Epoch 1 step 327: training accuarcy: 0.7299\n",
      "Epoch 1 step 327: training loss: 6127.076162992312\n",
      "Epoch 1 step 328: training accuarcy: 0.7382000000000001\n",
      "Epoch 1 step 328: training loss: 6185.665259448066\n",
      "Epoch 1 step 329: training accuarcy: 0.7346\n",
      "Epoch 1 step 329: training loss: 6229.51623963794\n",
      "Epoch 1 step 330: training accuarcy: 0.7341000000000001\n",
      "Epoch 1 step 330: training loss: 6097.848474196675\n",
      "Epoch 1 step 331: training accuarcy: 0.7528\n",
      "Epoch 1 step 331: training loss: 6147.669931631149\n",
      "Epoch 1 step 332: training accuarcy: 0.7371000000000001\n",
      "Epoch 1 step 332: training loss: 6140.5056831477195\n",
      "Epoch 1 step 333: training accuarcy: 0.7407\n",
      "Epoch 1 step 333: training loss: 6145.681343421487\n",
      "Epoch 1 step 334: training accuarcy: 0.7359\n",
      "Epoch 1 step 334: training loss: 6145.633517540956\n",
      "Epoch 1 step 335: training accuarcy: 0.7381000000000001\n",
      "Epoch 1 step 335: training loss: 6146.296701320289\n",
      "Epoch 1 step 336: training accuarcy: 0.7479\n",
      "Epoch 1 step 336: training loss: 6107.815151721746\n",
      "Epoch 1 step 337: training accuarcy: 0.7402000000000001\n",
      "Epoch 1 step 337: training loss: 6169.231069949302\n",
      "Epoch 1 step 338: training accuarcy: 0.7373000000000001\n",
      "Epoch 1 step 338: training loss: 6184.037775143447\n",
      "Epoch 1 step 339: training accuarcy: 0.7435\n",
      "Epoch 1 step 339: training loss: 6147.944090228101\n",
      "Epoch 1 step 340: training accuarcy: 0.7396\n",
      "Epoch 1 step 340: training loss: 6217.738195992053\n",
      "Epoch 1 step 341: training accuarcy: 0.7295\n",
      "Epoch 1 step 341: training loss: 6146.183147465048\n",
      "Epoch 1 step 342: training accuarcy: 0.7356\n",
      "Epoch 1 step 342: training loss: 6154.090509952171\n",
      "Epoch 1 step 343: training accuarcy: 0.7312000000000001\n",
      "Epoch 1 step 343: training loss: 6158.999265197292\n",
      "Epoch 1 step 344: training accuarcy: 0.7412000000000001\n",
      "Epoch 1 step 344: training loss: 6167.617846647298\n",
      "Epoch 1 step 345: training accuarcy: 0.7353000000000001\n",
      "Epoch 1 step 345: training loss: 6205.62504480779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 346: training accuarcy: 0.7384000000000001\n",
      "Epoch 1 step 346: training loss: 6204.817786827222\n",
      "Epoch 1 step 347: training accuarcy: 0.7281000000000001\n",
      "Epoch 1 step 347: training loss: 6087.952101416331\n",
      "Epoch 1 step 348: training accuarcy: 0.7488\n",
      "Epoch 1 step 348: training loss: 6141.077121152311\n",
      "Epoch 1 step 349: training accuarcy: 0.7387\n",
      "Epoch 1 step 349: training loss: 6168.551013996522\n",
      "Epoch 1 step 350: training accuarcy: 0.7299\n",
      "Epoch 1 step 350: training loss: 6179.807101094469\n",
      "Epoch 1 step 351: training accuarcy: 0.7403000000000001\n",
      "Epoch 1 step 351: training loss: 6148.250227735305\n",
      "Epoch 1 step 352: training accuarcy: 0.7342000000000001\n",
      "Epoch 1 step 352: training loss: 6119.142534441012\n",
      "Epoch 1 step 353: training accuarcy: 0.745\n",
      "Epoch 1 step 353: training loss: 6078.62486412798\n",
      "Epoch 1 step 354: training accuarcy: 0.7412000000000001\n",
      "Epoch 1 step 354: training loss: 6054.1923762210545\n",
      "Epoch 1 step 355: training accuarcy: 0.7532\n",
      "Epoch 1 step 355: training loss: 6092.895774122925\n",
      "Epoch 1 step 356: training accuarcy: 0.7361000000000001\n",
      "Epoch 1 step 356: training loss: 6173.570016975283\n",
      "Epoch 1 step 357: training accuarcy: 0.7335\n",
      "Epoch 1 step 357: training loss: 6033.092515747018\n",
      "Epoch 1 step 358: training accuarcy: 0.7519\n",
      "Epoch 1 step 358: training loss: 6065.696134904128\n",
      "Epoch 1 step 359: training accuarcy: 0.7469\n",
      "Epoch 1 step 359: training loss: 6048.076852618045\n",
      "Epoch 1 step 360: training accuarcy: 0.7446\n",
      "Epoch 1 step 360: training loss: 6052.503989205404\n",
      "Epoch 1 step 361: training accuarcy: 0.7437\n",
      "Epoch 1 step 361: training loss: 6032.211964404782\n",
      "Epoch 1 step 362: training accuarcy: 0.7413000000000001\n",
      "Epoch 1 step 362: training loss: 6115.422993241022\n",
      "Epoch 1 step 363: training accuarcy: 0.7397\n",
      "Epoch 1 step 363: training loss: 6167.738153082116\n",
      "Epoch 1 step 364: training accuarcy: 0.7316\n",
      "Epoch 1 step 364: training loss: 6048.888189863036\n",
      "Epoch 1 step 365: training accuarcy: 0.7471\n",
      "Epoch 1 step 365: training loss: 6144.836543965297\n",
      "Epoch 1 step 366: training accuarcy: 0.7339\n",
      "Epoch 1 step 366: training loss: 6089.345548285132\n",
      "Epoch 1 step 367: training accuarcy: 0.7396\n",
      "Epoch 1 step 367: training loss: 6115.067074271042\n",
      "Epoch 1 step 368: training accuarcy: 0.74\n",
      "Epoch 1 step 368: training loss: 6094.040489313733\n",
      "Epoch 1 step 369: training accuarcy: 0.7428\n",
      "Epoch 1 step 369: training loss: 6076.618903896078\n",
      "Epoch 1 step 370: training accuarcy: 0.7347\n",
      "Epoch 1 step 370: training loss: 6042.898283668147\n",
      "Epoch 1 step 371: training accuarcy: 0.7435\n",
      "Epoch 1 step 371: training loss: 6040.79902429831\n",
      "Epoch 1 step 372: training accuarcy: 0.7456\n",
      "Epoch 1 step 372: training loss: 6122.098491599938\n",
      "Epoch 1 step 373: training accuarcy: 0.7285\n",
      "Epoch 1 step 373: training loss: 6122.1634686162\n",
      "Epoch 1 step 374: training accuarcy: 0.7404000000000001\n",
      "Epoch 1 step 374: training loss: 5938.870998535127\n",
      "Epoch 1 step 375: training accuarcy: 0.757\n",
      "Epoch 1 step 375: training loss: 6130.6792243059235\n",
      "Epoch 1 step 376: training accuarcy: 0.7318\n",
      "Epoch 1 step 376: training loss: 6089.117094533823\n",
      "Epoch 1 step 377: training accuarcy: 0.7355\n",
      "Epoch 1 step 377: training loss: 6076.495623945624\n",
      "Epoch 1 step 378: training accuarcy: 0.7403000000000001\n",
      "Epoch 1 step 378: training loss: 6022.566229088542\n",
      "Epoch 1 step 379: training accuarcy: 0.7468\n",
      "Epoch 1 step 379: training loss: 6099.171256885989\n",
      "Epoch 1 step 380: training accuarcy: 0.7329\n",
      "Epoch 1 step 380: training loss: 6096.5413237431885\n",
      "Epoch 1 step 381: training accuarcy: 0.7413000000000001\n",
      "Epoch 1 step 381: training loss: 6016.879374203086\n",
      "Epoch 1 step 382: training accuarcy: 0.7439\n",
      "Epoch 1 step 382: training loss: 6114.398093166856\n",
      "Epoch 1 step 383: training accuarcy: 0.7339\n",
      "Epoch 1 step 383: training loss: 6080.573926810988\n",
      "Epoch 1 step 384: training accuarcy: 0.7387\n",
      "Epoch 1 step 384: training loss: 5970.068197680197\n",
      "Epoch 1 step 385: training accuarcy: 0.7461\n",
      "Epoch 1 step 385: training loss: 6026.949055721322\n",
      "Epoch 1 step 386: training accuarcy: 0.7435\n",
      "Epoch 1 step 386: training loss: 6117.623754981044\n",
      "Epoch 1 step 387: training accuarcy: 0.7348\n",
      "Epoch 1 step 387: training loss: 6100.249969638434\n",
      "Epoch 1 step 388: training accuarcy: 0.7386\n",
      "Epoch 1 step 388: training loss: 5998.732934042135\n",
      "Epoch 1 step 389: training accuarcy: 0.7513000000000001\n",
      "Epoch 1 step 389: training loss: 6102.151313132529\n",
      "Epoch 1 step 390: training accuarcy: 0.7399\n",
      "Epoch 1 step 390: training loss: 5989.766491783504\n",
      "Epoch 1 step 391: training accuarcy: 0.7471\n",
      "Epoch 1 step 391: training loss: 6108.09967248612\n",
      "Epoch 1 step 392: training accuarcy: 0.7283000000000001\n",
      "Epoch 1 step 392: training loss: 6027.097316535944\n",
      "Epoch 1 step 393: training accuarcy: 0.7464000000000001\n",
      "Epoch 1 step 393: training loss: 6015.19475913781\n",
      "Epoch 1 step 394: training accuarcy: 0.736\n",
      "Epoch 1 step 394: training loss: 6185.796360527258\n",
      "Epoch 1 step 395: training accuarcy: 0.7295\n",
      "Epoch 1 step 395: training loss: 6131.197416351215\n",
      "Epoch 1 step 396: training accuarcy: 0.7335\n",
      "Epoch 1 step 396: training loss: 6130.36431391767\n",
      "Epoch 1 step 397: training accuarcy: 0.7325\n",
      "Epoch 1 step 397: training loss: 5977.534275514116\n",
      "Epoch 1 step 398: training accuarcy: 0.7378\n",
      "Epoch 1 step 398: training loss: 6110.758415645755\n",
      "Epoch 1 step 399: training accuarcy: 0.7401\n",
      "Epoch 1 step 399: training loss: 6074.773934080079\n",
      "Epoch 1 step 400: training accuarcy: 0.7337\n",
      "Epoch 1 step 400: training loss: 6162.085480988335\n",
      "Epoch 1 step 401: training accuarcy: 0.7309\n",
      "Epoch 1 step 401: training loss: 5991.789258147185\n",
      "Epoch 1 step 402: training accuarcy: 0.7418\n",
      "Epoch 1 step 402: training loss: 6129.998927604668\n",
      "Epoch 1 step 403: training accuarcy: 0.7403000000000001\n",
      "Epoch 1 step 403: training loss: 6081.241373673617\n",
      "Epoch 1 step 404: training accuarcy: 0.7365\n",
      "Epoch 1 step 404: training loss: 5995.515327785491\n",
      "Epoch 1 step 405: training accuarcy: 0.7402000000000001\n",
      "Epoch 1 step 405: training loss: 6027.205367176159\n",
      "Epoch 1 step 406: training accuarcy: 0.7391000000000001\n",
      "Epoch 1 step 406: training loss: 6076.104834005231\n",
      "Epoch 1 step 407: training accuarcy: 0.7324\n",
      "Epoch 1 step 407: training loss: 6065.287877002194\n",
      "Epoch 1 step 408: training accuarcy: 0.7402000000000001\n",
      "Epoch 1 step 408: training loss: 6050.765217269897\n",
      "Epoch 1 step 409: training accuarcy: 0.7378\n",
      "Epoch 1 step 409: training loss: 5940.302426884764\n",
      "Epoch 1 step 410: training accuarcy: 0.7573000000000001\n",
      "Epoch 1 step 410: training loss: 6078.756847048733\n",
      "Epoch 1 step 411: training accuarcy: 0.7374\n",
      "Epoch 1 step 411: training loss: 6030.799595412124\n",
      "Epoch 1 step 412: training accuarcy: 0.7469\n",
      "Epoch 1 step 412: training loss: 6064.2379798900465\n",
      "Epoch 1 step 413: training accuarcy: 0.7323000000000001\n",
      "Epoch 1 step 413: training loss: 6026.697178846752\n",
      "Epoch 1 step 414: training accuarcy: 0.7454000000000001\n",
      "Epoch 1 step 414: training loss: 6002.939735506892\n",
      "Epoch 1 step 415: training accuarcy: 0.7462000000000001\n",
      "Epoch 1 step 415: training loss: 6078.467211586133\n",
      "Epoch 1 step 416: training accuarcy: 0.7428\n",
      "Epoch 1 step 416: training loss: 6003.835697617074\n",
      "Epoch 1 step 417: training accuarcy: 0.743\n",
      "Epoch 1 step 417: training loss: 5952.847024452427\n",
      "Epoch 1 step 418: training accuarcy: 0.7523000000000001\n",
      "Epoch 1 step 418: training loss: 5985.153588112577\n",
      "Epoch 1 step 419: training accuarcy: 0.746\n",
      "Epoch 1 step 419: training loss: 5977.000231997915\n",
      "Epoch 1 step 420: training accuarcy: 0.7402000000000001\n",
      "Epoch 1 step 420: training loss: 5984.937531356968\n",
      "Epoch 1 step 421: training accuarcy: 0.7484000000000001\n",
      "Epoch 1 step 421: training loss: 6002.070695100509\n",
      "Epoch 1 step 422: training accuarcy: 0.7467\n",
      "Epoch 1 step 422: training loss: 5949.745002806385\n",
      "Epoch 1 step 423: training accuarcy: 0.7465\n",
      "Epoch 1 step 423: training loss: 6018.275039060367\n",
      "Epoch 1 step 424: training accuarcy: 0.7447\n",
      "Epoch 1 step 424: training loss: 5972.093042789805\n",
      "Epoch 1 step 425: training accuarcy: 0.7502000000000001\n",
      "Epoch 1 step 425: training loss: 5988.603019025253\n",
      "Epoch 1 step 426: training accuarcy: 0.7355\n",
      "Epoch 1 step 426: training loss: 6023.704114763399\n",
      "Epoch 1 step 427: training accuarcy: 0.7419\n",
      "Epoch 1 step 427: training loss: 6093.826419498887\n",
      "Epoch 1 step 428: training accuarcy: 0.7254\n",
      "Epoch 1 step 428: training loss: 6048.469588073142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 429: training accuarcy: 0.734\n",
      "Epoch 1 step 429: training loss: 6019.216328807309\n",
      "Epoch 1 step 430: training accuarcy: 0.7407\n",
      "Epoch 1 step 430: training loss: 5903.184664906983\n",
      "Epoch 1 step 431: training accuarcy: 0.7458\n",
      "Epoch 1 step 431: training loss: 5998.685231921287\n",
      "Epoch 1 step 432: training accuarcy: 0.7437\n",
      "Epoch 1 step 432: training loss: 6079.1343592094845\n",
      "Epoch 1 step 433: training accuarcy: 0.7393000000000001\n",
      "Epoch 1 step 433: training loss: 6027.837478139143\n",
      "Epoch 1 step 434: training accuarcy: 0.746\n",
      "Epoch 1 step 434: training loss: 6060.735091229552\n",
      "Epoch 1 step 435: training accuarcy: 0.736\n",
      "Epoch 1 step 435: training loss: 6033.486352133152\n",
      "Epoch 1 step 436: training accuarcy: 0.7451\n",
      "Epoch 1 step 436: training loss: 6039.385605088667\n",
      "Epoch 1 step 437: training accuarcy: 0.7337\n",
      "Epoch 1 step 437: training loss: 6071.712148546372\n",
      "Epoch 1 step 438: training accuarcy: 0.7346\n",
      "Epoch 1 step 438: training loss: 6010.812825520987\n",
      "Epoch 1 step 439: training accuarcy: 0.7538\n",
      "Epoch 1 step 439: training loss: 5953.97574234145\n",
      "Epoch 1 step 440: training accuarcy: 0.7546\n",
      "Epoch 1 step 440: training loss: 6077.636807333232\n",
      "Epoch 1 step 441: training accuarcy: 0.7449\n",
      "Epoch 1 step 441: training loss: 6036.9539387201785\n",
      "Epoch 1 step 442: training accuarcy: 0.7529\n",
      "Epoch 1 step 442: training loss: 6050.748622826675\n",
      "Epoch 1 step 443: training accuarcy: 0.7349\n",
      "Epoch 1 step 443: training loss: 5955.699976643428\n",
      "Epoch 1 step 444: training accuarcy: 0.7544000000000001\n",
      "Epoch 1 step 444: training loss: 6009.289305884659\n",
      "Epoch 1 step 445: training accuarcy: 0.7419\n",
      "Epoch 1 step 445: training loss: 5938.025969593426\n",
      "Epoch 1 step 446: training accuarcy: 0.7409\n",
      "Epoch 1 step 446: training loss: 5998.846498012022\n",
      "Epoch 1 step 447: training accuarcy: 0.7412000000000001\n",
      "Epoch 1 step 447: training loss: 6011.280519008247\n",
      "Epoch 1 step 448: training accuarcy: 0.7502000000000001\n",
      "Epoch 1 step 448: training loss: 5939.595523783435\n",
      "Epoch 1 step 449: training accuarcy: 0.7508\n",
      "Epoch 1 step 449: training loss: 6076.922983060203\n",
      "Epoch 1 step 450: training accuarcy: 0.7386\n",
      "Epoch 1 step 450: training loss: 5897.792323228926\n",
      "Epoch 1 step 451: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 451: training loss: 6024.723195251155\n",
      "Epoch 1 step 452: training accuarcy: 0.7428\n",
      "Epoch 1 step 452: training loss: 6004.033457642556\n",
      "Epoch 1 step 453: training accuarcy: 0.7427\n",
      "Epoch 1 step 453: training loss: 5945.967355441808\n",
      "Epoch 1 step 454: training accuarcy: 0.7469\n",
      "Epoch 1 step 454: training loss: 6032.693354118985\n",
      "Epoch 1 step 455: training accuarcy: 0.7447\n",
      "Epoch 1 step 455: training loss: 5961.849044340253\n",
      "Epoch 1 step 456: training accuarcy: 0.7473000000000001\n",
      "Epoch 1 step 456: training loss: 6029.274463720704\n",
      "Epoch 1 step 457: training accuarcy: 0.7436\n",
      "Epoch 1 step 457: training loss: 5991.722604719531\n",
      "Epoch 1 step 458: training accuarcy: 0.7426\n",
      "Epoch 1 step 458: training loss: 6046.892661971487\n",
      "Epoch 1 step 459: training accuarcy: 0.7299\n",
      "Epoch 1 step 459: training loss: 5904.041176350552\n",
      "Epoch 1 step 460: training accuarcy: 0.7426\n",
      "Epoch 1 step 460: training loss: 5970.37628344018\n",
      "Epoch 1 step 461: training accuarcy: 0.7375\n",
      "Epoch 1 step 461: training loss: 5942.187372405621\n",
      "Epoch 1 step 462: training accuarcy: 0.7473000000000001\n",
      "Epoch 1 step 462: training loss: 5984.4309319085905\n",
      "Epoch 1 step 463: training accuarcy: 0.7453000000000001\n",
      "Epoch 1 step 463: training loss: 5977.52070672511\n",
      "Epoch 1 step 464: training accuarcy: 0.7431\n",
      "Epoch 1 step 464: training loss: 5936.69144936735\n",
      "Epoch 1 step 465: training accuarcy: 0.7446\n",
      "Epoch 1 step 465: training loss: 5983.397198729952\n",
      "Epoch 1 step 466: training accuarcy: 0.7418\n",
      "Epoch 1 step 466: training loss: 5943.420462144565\n",
      "Epoch 1 step 467: training accuarcy: 0.7533000000000001\n",
      "Epoch 1 step 467: training loss: 5951.854190233375\n",
      "Epoch 1 step 468: training accuarcy: 0.7431\n",
      "Epoch 1 step 468: training loss: 5899.328188636668\n",
      "Epoch 1 step 469: training accuarcy: 0.7458\n",
      "Epoch 1 step 469: training loss: 6091.008590061899\n",
      "Epoch 1 step 470: training accuarcy: 0.7314\n",
      "Epoch 1 step 470: training loss: 5890.198124351244\n",
      "Epoch 1 step 471: training accuarcy: 0.75\n",
      "Epoch 1 step 471: training loss: 5954.806910944403\n",
      "Epoch 1 step 472: training accuarcy: 0.7361000000000001\n",
      "Epoch 1 step 472: training loss: 6069.60744676818\n",
      "Epoch 1 step 473: training accuarcy: 0.7348\n",
      "Epoch 1 step 473: training loss: 5965.493473571406\n",
      "Epoch 1 step 474: training accuarcy: 0.7427\n",
      "Epoch 1 step 474: training loss: 6066.080462435232\n",
      "Epoch 1 step 475: training accuarcy: 0.7277\n",
      "Epoch 1 step 475: training loss: 6001.163239714026\n",
      "Epoch 1 step 476: training accuarcy: 0.74\n",
      "Epoch 1 step 476: training loss: 5974.75932650721\n",
      "Epoch 1 step 477: training accuarcy: 0.7426\n",
      "Epoch 1 step 477: training loss: 6043.924463346227\n",
      "Epoch 1 step 478: training accuarcy: 0.73\n",
      "Epoch 1 step 478: training loss: 6071.51108837191\n",
      "Epoch 1 step 479: training accuarcy: 0.7305\n",
      "Epoch 1 step 479: training loss: 5959.561808567415\n",
      "Epoch 1 step 480: training accuarcy: 0.7475\n",
      "Epoch 1 step 480: training loss: 5921.284447205553\n",
      "Epoch 1 step 481: training accuarcy: 0.7466\n",
      "Epoch 1 step 481: training loss: 6019.522395633909\n",
      "Epoch 1 step 482: training accuarcy: 0.7404000000000001\n",
      "Epoch 1 step 482: training loss: 5914.109303712875\n",
      "Epoch 1 step 483: training accuarcy: 0.7493000000000001\n",
      "Epoch 1 step 483: training loss: 5980.285039204025\n",
      "Epoch 1 step 484: training accuarcy: 0.7491\n",
      "Epoch 1 step 484: training loss: 5910.531429019471\n",
      "Epoch 1 step 485: training accuarcy: 0.7457\n",
      "Epoch 1 step 485: training loss: 5979.542187771638\n",
      "Epoch 1 step 486: training accuarcy: 0.743\n",
      "Epoch 1 step 486: training loss: 6005.211336839834\n",
      "Epoch 1 step 487: training accuarcy: 0.7447\n",
      "Epoch 1 step 487: training loss: 6024.182507758795\n",
      "Epoch 1 step 488: training accuarcy: 0.7321000000000001\n",
      "Epoch 1 step 488: training loss: 5944.383953525054\n",
      "Epoch 1 step 489: training accuarcy: 0.7433000000000001\n",
      "Epoch 1 step 489: training loss: 5998.447535995019\n",
      "Epoch 1 step 490: training accuarcy: 0.7428\n",
      "Epoch 1 step 490: training loss: 6006.407079248189\n",
      "Epoch 1 step 491: training accuarcy: 0.7338\n",
      "Epoch 1 step 491: training loss: 5929.733611465983\n",
      "Epoch 1 step 492: training accuarcy: 0.7381000000000001\n",
      "Epoch 1 step 492: training loss: 6007.472283639041\n",
      "Epoch 1 step 493: training accuarcy: 0.7389\n",
      "Epoch 1 step 493: training loss: 5938.566644825805\n",
      "Epoch 1 step 494: training accuarcy: 0.7478\n",
      "Epoch 1 step 494: training loss: 5997.957116410847\n",
      "Epoch 1 step 495: training accuarcy: 0.7435\n",
      "Epoch 1 step 495: training loss: 5975.797776167572\n",
      "Epoch 1 step 496: training accuarcy: 0.7361000000000001\n",
      "Epoch 1 step 496: training loss: 6018.190953976972\n",
      "Epoch 1 step 497: training accuarcy: 0.7395\n",
      "Epoch 1 step 497: training loss: 5952.405191179468\n",
      "Epoch 1 step 498: training accuarcy: 0.7481\n",
      "Epoch 1 step 498: training loss: 5821.78283611633\n",
      "Epoch 1 step 499: training accuarcy: 0.7566\n",
      "Epoch 1 step 499: training loss: 5919.971372317815\n",
      "Epoch 1 step 500: training accuarcy: 0.7532\n",
      "Epoch 1 step 500: training loss: 5965.535558154919\n",
      "Epoch 1 step 501: training accuarcy: 0.7417\n",
      "Epoch 1 step 501: training loss: 5942.9069775549015\n",
      "Epoch 1 step 502: training accuarcy: 0.7359\n",
      "Epoch 1 step 502: training loss: 5985.628183286128\n",
      "Epoch 1 step 503: training accuarcy: 0.7451\n",
      "Epoch 1 step 503: training loss: 5804.137456249185\n",
      "Epoch 1 step 504: training accuarcy: 0.7559\n",
      "Epoch 1 step 504: training loss: 5842.288462749183\n",
      "Epoch 1 step 505: training accuarcy: 0.7494000000000001\n",
      "Epoch 1 step 505: training loss: 5919.23836241518\n",
      "Epoch 1 step 506: training accuarcy: 0.7489\n",
      "Epoch 1 step 506: training loss: 5960.873423299467\n",
      "Epoch 1 step 507: training accuarcy: 0.7367\n",
      "Epoch 1 step 507: training loss: 5892.497338478861\n",
      "Epoch 1 step 508: training accuarcy: 0.7452000000000001\n",
      "Epoch 1 step 508: training loss: 5924.971089732582\n",
      "Epoch 1 step 509: training accuarcy: 0.7349\n",
      "Epoch 1 step 509: training loss: 5960.396953628177\n",
      "Epoch 1 step 510: training accuarcy: 0.7449\n",
      "Epoch 1 step 510: training loss: 5947.184503332979\n",
      "Epoch 1 step 511: training accuarcy: 0.7441\n",
      "Epoch 1 step 511: training loss: 5976.872357251718\n",
      "Epoch 1 step 512: training accuarcy: 0.7407\n",
      "Epoch 1 step 512: training loss: 6032.0041340330135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 513: training accuarcy: 0.7314\n",
      "Epoch 1 step 513: training loss: 5915.183302136818\n",
      "Epoch 1 step 514: training accuarcy: 0.7432000000000001\n",
      "Epoch 1 step 514: training loss: 5985.148822604724\n",
      "Epoch 1 step 515: training accuarcy: 0.7415\n",
      "Epoch 1 step 515: training loss: 5943.4291087977435\n",
      "Epoch 1 step 516: training accuarcy: 0.7345\n",
      "Epoch 1 step 516: training loss: 6035.250281806478\n",
      "Epoch 1 step 517: training accuarcy: 0.7333000000000001\n",
      "Epoch 1 step 517: training loss: 5962.7701256992805\n",
      "Epoch 1 step 518: training accuarcy: 0.7469\n",
      "Epoch 1 step 518: training loss: 5915.759768675031\n",
      "Epoch 1 step 519: training accuarcy: 0.7393000000000001\n",
      "Epoch 1 step 519: training loss: 5932.736323433711\n",
      "Epoch 1 step 520: training accuarcy: 0.7501\n",
      "Epoch 1 step 520: training loss: 5941.948782619669\n",
      "Epoch 1 step 521: training accuarcy: 0.7405\n",
      "Epoch 1 step 521: training loss: 5902.9626024506715\n",
      "Epoch 1 step 522: training accuarcy: 0.7382000000000001\n",
      "Epoch 1 step 522: training loss: 5945.959625731218\n",
      "Epoch 1 step 523: training accuarcy: 0.738\n",
      "Epoch 1 step 523: training loss: 6017.5632830648865\n",
      "Epoch 1 step 524: training accuarcy: 0.7318\n",
      "Epoch 1 step 524: training loss: 5858.645081920509\n",
      "Epoch 1 step 525: training accuarcy: 0.7466\n",
      "Epoch 1 step 525: training loss: 2471.594702574298\n",
      "Epoch 1 step 526: training accuarcy: 0.7389743589743589\n",
      "Epoch 1: train loss 6065.939713406965, train accuarcy 0.7150062322616577\n",
      "Epoch 1: valid loss 5698.439823820523, valid accuarcy 0.7606229782104492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [10:19<05:08, 309.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 5834.380269160445\n",
      "Epoch 2 step 527: training accuarcy: 0.7543000000000001\n",
      "Epoch 2 step 527: training loss: 5891.897092834732\n",
      "Epoch 2 step 528: training accuarcy: 0.7492000000000001\n",
      "Epoch 2 step 528: training loss: 5894.3913743412995\n",
      "Epoch 2 step 529: training accuarcy: 0.7562\n",
      "Epoch 2 step 529: training loss: 5927.385883920442\n",
      "Epoch 2 step 530: training accuarcy: 0.7477\n",
      "Epoch 2 step 530: training loss: 5895.924112599176\n",
      "Epoch 2 step 531: training accuarcy: 0.7507\n",
      "Epoch 2 step 531: training loss: 5918.430441360658\n",
      "Epoch 2 step 532: training accuarcy: 0.7543000000000001\n",
      "Epoch 2 step 532: training loss: 5936.418458656189\n",
      "Epoch 2 step 533: training accuarcy: 0.7459\n",
      "Epoch 2 step 533: training loss: 5886.256474486059\n",
      "Epoch 2 step 534: training accuarcy: 0.7557\n",
      "Epoch 2 step 534: training loss: 5894.4572986412895\n",
      "Epoch 2 step 535: training accuarcy: 0.7393000000000001\n",
      "Epoch 2 step 535: training loss: 5886.154137588903\n",
      "Epoch 2 step 536: training accuarcy: 0.7449\n",
      "Epoch 2 step 536: training loss: 5839.779398233915\n",
      "Epoch 2 step 537: training accuarcy: 0.7545000000000001\n",
      "Epoch 2 step 537: training loss: 5847.353753357706\n",
      "Epoch 2 step 538: training accuarcy: 0.757\n",
      "Epoch 2 step 538: training loss: 5863.955182436804\n",
      "Epoch 2 step 539: training accuarcy: 0.7508\n",
      "Epoch 2 step 539: training loss: 5915.01303767838\n",
      "Epoch 2 step 540: training accuarcy: 0.7539\n",
      "Epoch 2 step 540: training loss: 5854.462667959395\n",
      "Epoch 2 step 541: training accuarcy: 0.7514000000000001\n",
      "Epoch 2 step 541: training loss: 5903.163508106897\n",
      "Epoch 2 step 542: training accuarcy: 0.7545000000000001\n",
      "Epoch 2 step 542: training loss: 5871.931693950692\n",
      "Epoch 2 step 543: training accuarcy: 0.7522000000000001\n",
      "Epoch 2 step 543: training loss: 5861.294145697414\n",
      "Epoch 2 step 544: training accuarcy: 0.7476\n",
      "Epoch 2 step 544: training loss: 5872.990362434782\n",
      "Epoch 2 step 545: training accuarcy: 0.7529\n",
      "Epoch 2 step 545: training loss: 5844.024571496234\n",
      "Epoch 2 step 546: training accuarcy: 0.7587\n",
      "Epoch 2 step 546: training loss: 5892.315612145213\n",
      "Epoch 2 step 547: training accuarcy: 0.7503000000000001\n",
      "Epoch 2 step 547: training loss: 5852.912354013661\n",
      "Epoch 2 step 548: training accuarcy: 0.7497\n",
      "Epoch 2 step 548: training loss: 5900.826849688005\n",
      "Epoch 2 step 549: training accuarcy: 0.7459\n",
      "Epoch 2 step 549: training loss: 5892.970683323785\n",
      "Epoch 2 step 550: training accuarcy: 0.7544000000000001\n",
      "Epoch 2 step 550: training loss: 5847.545720401617\n",
      "Epoch 2 step 551: training accuarcy: 0.7457\n",
      "Epoch 2 step 551: training loss: 5806.614345792374\n",
      "Epoch 2 step 552: training accuarcy: 0.7621\n",
      "Epoch 2 step 552: training loss: 5855.804364828613\n",
      "Epoch 2 step 553: training accuarcy: 0.7539\n",
      "Epoch 2 step 553: training loss: 5875.821023796622\n",
      "Epoch 2 step 554: training accuarcy: 0.7534000000000001\n",
      "Epoch 2 step 554: training loss: 5883.525554138229\n",
      "Epoch 2 step 555: training accuarcy: 0.7487\n",
      "Epoch 2 step 555: training loss: 5824.966681029553\n",
      "Epoch 2 step 556: training accuarcy: 0.7509\n",
      "Epoch 2 step 556: training loss: 5875.934094510178\n",
      "Epoch 2 step 557: training accuarcy: 0.7522000000000001\n",
      "Epoch 2 step 557: training loss: 5925.407993168064\n",
      "Epoch 2 step 558: training accuarcy: 0.7374\n",
      "Epoch 2 step 558: training loss: 5867.143488479191\n",
      "Epoch 2 step 559: training accuarcy: 0.7449\n",
      "Epoch 2 step 559: training loss: 5959.748056809196\n",
      "Epoch 2 step 560: training accuarcy: 0.7448\n",
      "Epoch 2 step 560: training loss: 5903.749994421047\n",
      "Epoch 2 step 561: training accuarcy: 0.7468\n",
      "Epoch 2 step 561: training loss: 5975.160321705929\n",
      "Epoch 2 step 562: training accuarcy: 0.7425\n",
      "Epoch 2 step 562: training loss: 5869.476750039949\n",
      "Epoch 2 step 563: training accuarcy: 0.7508\n",
      "Epoch 2 step 563: training loss: 5943.408815100875\n",
      "Epoch 2 step 564: training accuarcy: 0.7475\n",
      "Epoch 2 step 564: training loss: 5960.403306561955\n",
      "Epoch 2 step 565: training accuarcy: 0.7391000000000001\n",
      "Epoch 2 step 565: training loss: 5825.134287245905\n",
      "Epoch 2 step 566: training accuarcy: 0.7571\n",
      "Epoch 2 step 566: training loss: 5877.867367026711\n",
      "Epoch 2 step 567: training accuarcy: 0.7481\n",
      "Epoch 2 step 567: training loss: 5791.098699090422\n",
      "Epoch 2 step 568: training accuarcy: 0.7616\n",
      "Epoch 2 step 568: training loss: 5935.9441387862\n",
      "Epoch 2 step 569: training accuarcy: 0.7356\n",
      "Epoch 2 step 569: training loss: 5903.677344704686\n",
      "Epoch 2 step 570: training accuarcy: 0.7591\n",
      "Epoch 2 step 570: training loss: 5928.937555995538\n",
      "Epoch 2 step 571: training accuarcy: 0.7433000000000001\n",
      "Epoch 2 step 571: training loss: 5964.371640506734\n",
      "Epoch 2 step 572: training accuarcy: 0.7489\n",
      "Epoch 2 step 572: training loss: 5933.966184207207\n",
      "Epoch 2 step 573: training accuarcy: 0.7324\n",
      "Epoch 2 step 573: training loss: 5941.573033924266\n",
      "Epoch 2 step 574: training accuarcy: 0.7387\n",
      "Epoch 2 step 574: training loss: 5902.754771546877\n",
      "Epoch 2 step 575: training accuarcy: 0.7382000000000001\n",
      "Epoch 2 step 575: training loss: 5880.585619136538\n",
      "Epoch 2 step 576: training accuarcy: 0.7495\n",
      "Epoch 2 step 576: training loss: 5814.410987856418\n",
      "Epoch 2 step 577: training accuarcy: 0.7626000000000001\n",
      "Epoch 2 step 577: training loss: 5914.22290697072\n",
      "Epoch 2 step 578: training accuarcy: 0.7438\n",
      "Epoch 2 step 578: training loss: 5935.332659304664\n",
      "Epoch 2 step 579: training accuarcy: 0.7331000000000001\n",
      "Epoch 2 step 579: training loss: 5874.454614492151\n",
      "Epoch 2 step 580: training accuarcy: 0.754\n",
      "Epoch 2 step 580: training loss: 5914.629765573208\n",
      "Epoch 2 step 581: training accuarcy: 0.7449\n",
      "Epoch 2 step 581: training loss: 5874.952906177665\n",
      "Epoch 2 step 582: training accuarcy: 0.7587\n",
      "Epoch 2 step 582: training loss: 5900.334006936648\n",
      "Epoch 2 step 583: training accuarcy: 0.7478\n",
      "Epoch 2 step 583: training loss: 5851.845723510019\n",
      "Epoch 2 step 584: training accuarcy: 0.751\n",
      "Epoch 2 step 584: training loss: 5957.051556778879\n",
      "Epoch 2 step 585: training accuarcy: 0.7329\n",
      "Epoch 2 step 585: training loss: 5928.790561073095\n",
      "Epoch 2 step 586: training accuarcy: 0.7415\n",
      "Epoch 2 step 586: training loss: 5901.255151219382\n",
      "Epoch 2 step 587: training accuarcy: 0.7482000000000001\n",
      "Epoch 2 step 587: training loss: 5853.029958839279\n",
      "Epoch 2 step 588: training accuarcy: 0.7543000000000001\n",
      "Epoch 2 step 588: training loss: 5952.730740660934\n",
      "Epoch 2 step 589: training accuarcy: 0.738\n",
      "Epoch 2 step 589: training loss: 5731.256447621895\n",
      "Epoch 2 step 590: training accuarcy: 0.7565000000000001\n",
      "Epoch 2 step 590: training loss: 5838.57821297949\n",
      "Epoch 2 step 591: training accuarcy: 0.7507\n",
      "Epoch 2 step 591: training loss: 5959.911194887709\n",
      "Epoch 2 step 592: training accuarcy: 0.7381000000000001\n",
      "Epoch 2 step 592: training loss: 5870.399537691243\n",
      "Epoch 2 step 593: training accuarcy: 0.7429\n",
      "Epoch 2 step 593: training loss: 5842.914552442435\n",
      "Epoch 2 step 594: training accuarcy: 0.7498\n",
      "Epoch 2 step 594: training loss: 5846.982198046585\n",
      "Epoch 2 step 595: training accuarcy: 0.7509\n",
      "Epoch 2 step 595: training loss: 5896.869751260576\n",
      "Epoch 2 step 596: training accuarcy: 0.7503000000000001\n",
      "Epoch 2 step 596: training loss: 5933.134999396856\n",
      "Epoch 2 step 597: training accuarcy: 0.7427\n",
      "Epoch 2 step 597: training loss: 5898.909218844099\n",
      "Epoch 2 step 598: training accuarcy: 0.7451\n",
      "Epoch 2 step 598: training loss: 5901.234837555504\n",
      "Epoch 2 step 599: training accuarcy: 0.7411\n",
      "Epoch 2 step 599: training loss: 5851.095614562591\n",
      "Epoch 2 step 600: training accuarcy: 0.7574000000000001\n",
      "Epoch 2 step 600: training loss: 5889.31098513838\n",
      "Epoch 2 step 601: training accuarcy: 0.7517\n",
      "Epoch 2 step 601: training loss: 5917.661735259413\n",
      "Epoch 2 step 602: training accuarcy: 0.74\n",
      "Epoch 2 step 602: training loss: 5959.859486792111\n",
      "Epoch 2 step 603: training accuarcy: 0.7419\n",
      "Epoch 2 step 603: training loss: 5878.490777150916\n",
      "Epoch 2 step 604: training accuarcy: 0.7496\n",
      "Epoch 2 step 604: training loss: 5903.064623073086\n",
      "Epoch 2 step 605: training accuarcy: 0.7463000000000001\n",
      "Epoch 2 step 605: training loss: 5906.459332605239\n",
      "Epoch 2 step 606: training accuarcy: 0.7506\n",
      "Epoch 2 step 606: training loss: 5860.48237986145\n",
      "Epoch 2 step 607: training accuarcy: 0.7478\n",
      "Epoch 2 step 607: training loss: 5902.5405513016985\n",
      "Epoch 2 step 608: training accuarcy: 0.7396\n",
      "Epoch 2 step 608: training loss: 5863.205839357306\n",
      "Epoch 2 step 609: training accuarcy: 0.7494000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 609: training loss: 5895.3893517752085\n",
      "Epoch 2 step 610: training accuarcy: 0.7539\n",
      "Epoch 2 step 610: training loss: 5830.257277635872\n",
      "Epoch 2 step 611: training accuarcy: 0.7564000000000001\n",
      "Epoch 2 step 611: training loss: 5923.443772707324\n",
      "Epoch 2 step 612: training accuarcy: 0.7369\n",
      "Epoch 2 step 612: training loss: 5900.989644823518\n",
      "Epoch 2 step 613: training accuarcy: 0.7384000000000001\n",
      "Epoch 2 step 613: training loss: 5900.434202490933\n",
      "Epoch 2 step 614: training accuarcy: 0.7432000000000001\n",
      "Epoch 2 step 614: training loss: 5816.797334856353\n",
      "Epoch 2 step 615: training accuarcy: 0.7524000000000001\n",
      "Epoch 2 step 615: training loss: 5866.640021173241\n",
      "Epoch 2 step 616: training accuarcy: 0.7551\n",
      "Epoch 2 step 616: training loss: 5854.442000147157\n",
      "Epoch 2 step 617: training accuarcy: 0.7437\n",
      "Epoch 2 step 617: training loss: 5822.733543877129\n",
      "Epoch 2 step 618: training accuarcy: 0.7572\n",
      "Epoch 2 step 618: training loss: 5841.33672958609\n",
      "Epoch 2 step 619: training accuarcy: 0.7505000000000001\n",
      "Epoch 2 step 619: training loss: 5817.2033085051935\n",
      "Epoch 2 step 620: training accuarcy: 0.7498\n",
      "Epoch 2 step 620: training loss: 5848.552614626583\n",
      "Epoch 2 step 621: training accuarcy: 0.7499\n",
      "Epoch 2 step 621: training loss: 6039.945233881539\n",
      "Epoch 2 step 622: training accuarcy: 0.7314\n",
      "Epoch 2 step 622: training loss: 5826.914020162869\n",
      "Epoch 2 step 623: training accuarcy: 0.7533000000000001\n",
      "Epoch 2 step 623: training loss: 5843.049653047397\n",
      "Epoch 2 step 624: training accuarcy: 0.7478\n",
      "Epoch 2 step 624: training loss: 5946.972279439288\n",
      "Epoch 2 step 625: training accuarcy: 0.743\n",
      "Epoch 2 step 625: training loss: 5881.986720304189\n",
      "Epoch 2 step 626: training accuarcy: 0.7447\n",
      "Epoch 2 step 626: training loss: 5940.815987467957\n",
      "Epoch 2 step 627: training accuarcy: 0.7364\n",
      "Epoch 2 step 627: training loss: 5887.82913712856\n",
      "Epoch 2 step 628: training accuarcy: 0.7429\n",
      "Epoch 2 step 628: training loss: 5941.146321647215\n",
      "Epoch 2 step 629: training accuarcy: 0.7411\n",
      "Epoch 2 step 629: training loss: 5814.496331764575\n",
      "Epoch 2 step 630: training accuarcy: 0.7489\n",
      "Epoch 2 step 630: training loss: 5756.437886296424\n",
      "Epoch 2 step 631: training accuarcy: 0.7632\n",
      "Epoch 2 step 631: training loss: 5847.807254994572\n",
      "Epoch 2 step 632: training accuarcy: 0.7526\n",
      "Epoch 2 step 632: training loss: 5877.214104778208\n",
      "Epoch 2 step 633: training accuarcy: 0.745\n",
      "Epoch 2 step 633: training loss: 5879.4223199173\n",
      "Epoch 2 step 634: training accuarcy: 0.753\n",
      "Epoch 2 step 634: training loss: 5952.574732714198\n",
      "Epoch 2 step 635: training accuarcy: 0.7436\n",
      "Epoch 2 step 635: training loss: 5832.587630286989\n",
      "Epoch 2 step 636: training accuarcy: 0.7528\n",
      "Epoch 2 step 636: training loss: 5796.376311621665\n",
      "Epoch 2 step 637: training accuarcy: 0.7574000000000001\n",
      "Epoch 2 step 637: training loss: 5902.334234024162\n",
      "Epoch 2 step 638: training accuarcy: 0.7468\n",
      "Epoch 2 step 638: training loss: 5876.653805430721\n",
      "Epoch 2 step 639: training accuarcy: 0.7484000000000001\n",
      "Epoch 2 step 639: training loss: 5891.018099821735\n",
      "Epoch 2 step 640: training accuarcy: 0.7448\n",
      "Epoch 2 step 640: training loss: 5791.985205211814\n",
      "Epoch 2 step 641: training accuarcy: 0.757\n",
      "Epoch 2 step 641: training loss: 5843.711322602834\n",
      "Epoch 2 step 642: training accuarcy: 0.7498\n",
      "Epoch 2 step 642: training loss: 5854.719113904983\n",
      "Epoch 2 step 643: training accuarcy: 0.7478\n",
      "Epoch 2 step 643: training loss: 5843.878660686054\n",
      "Epoch 2 step 644: training accuarcy: 0.7461\n",
      "Epoch 2 step 644: training loss: 5909.105775423974\n",
      "Epoch 2 step 645: training accuarcy: 0.7488\n",
      "Epoch 2 step 645: training loss: 5828.120483390255\n",
      "Epoch 2 step 646: training accuarcy: 0.7554000000000001\n",
      "Epoch 2 step 646: training loss: 5803.277368863399\n",
      "Epoch 2 step 647: training accuarcy: 0.7522000000000001\n",
      "Epoch 2 step 647: training loss: 5944.530463542142\n",
      "Epoch 2 step 648: training accuarcy: 0.7493000000000001\n",
      "Epoch 2 step 648: training loss: 5772.177644324044\n",
      "Epoch 2 step 649: training accuarcy: 0.7531\n",
      "Epoch 2 step 649: training loss: 5839.9659499401605\n",
      "Epoch 2 step 650: training accuarcy: 0.7548\n",
      "Epoch 2 step 650: training loss: 5861.500381169333\n",
      "Epoch 2 step 651: training accuarcy: 0.7528\n",
      "Epoch 2 step 651: training loss: 5920.322021545151\n",
      "Epoch 2 step 652: training accuarcy: 0.7448\n",
      "Epoch 2 step 652: training loss: 5759.130237591311\n",
      "Epoch 2 step 653: training accuarcy: 0.7549\n",
      "Epoch 2 step 653: training loss: 5930.548921418022\n",
      "Epoch 2 step 654: training accuarcy: 0.7416\n",
      "Epoch 2 step 654: training loss: 5864.521399459182\n",
      "Epoch 2 step 655: training accuarcy: 0.7486\n",
      "Epoch 2 step 655: training loss: 5949.472938769797\n",
      "Epoch 2 step 656: training accuarcy: 0.7412000000000001\n",
      "Epoch 2 step 656: training loss: 5805.997235351766\n",
      "Epoch 2 step 657: training accuarcy: 0.757\n",
      "Epoch 2 step 657: training loss: 5855.756719005927\n",
      "Epoch 2 step 658: training accuarcy: 0.7466\n",
      "Epoch 2 step 658: training loss: 5891.673785222133\n",
      "Epoch 2 step 659: training accuarcy: 0.7533000000000001\n",
      "Epoch 2 step 659: training loss: 5804.001845209888\n",
      "Epoch 2 step 660: training accuarcy: 0.7507\n",
      "Epoch 2 step 660: training loss: 5869.79245842831\n",
      "Epoch 2 step 661: training accuarcy: 0.7555000000000001\n",
      "Epoch 2 step 661: training loss: 5777.784900904411\n",
      "Epoch 2 step 662: training accuarcy: 0.7532\n",
      "Epoch 2 step 662: training loss: 5913.7265071883785\n",
      "Epoch 2 step 663: training accuarcy: 0.7468\n",
      "Epoch 2 step 663: training loss: 5741.749880025157\n",
      "Epoch 2 step 664: training accuarcy: 0.7591\n",
      "Epoch 2 step 664: training loss: 5872.213678706698\n",
      "Epoch 2 step 665: training accuarcy: 0.7427\n",
      "Epoch 2 step 665: training loss: 5803.288025163421\n",
      "Epoch 2 step 666: training accuarcy: 0.7483000000000001\n",
      "Epoch 2 step 666: training loss: 5789.962587184656\n",
      "Epoch 2 step 667: training accuarcy: 0.7549\n",
      "Epoch 2 step 667: training loss: 5893.2197228425075\n",
      "Epoch 2 step 668: training accuarcy: 0.7506\n",
      "Epoch 2 step 668: training loss: 5795.154579150808\n",
      "Epoch 2 step 669: training accuarcy: 0.7491\n",
      "Epoch 2 step 669: training loss: 5808.164922054162\n",
      "Epoch 2 step 670: training accuarcy: 0.7557\n",
      "Epoch 2 step 670: training loss: 5789.982001636807\n",
      "Epoch 2 step 671: training accuarcy: 0.7558\n",
      "Epoch 2 step 671: training loss: 5803.634881407709\n",
      "Epoch 2 step 672: training accuarcy: 0.7475\n",
      "Epoch 2 step 672: training loss: 5838.150057460033\n",
      "Epoch 2 step 673: training accuarcy: 0.7532\n",
      "Epoch 2 step 673: training loss: 5840.050478299264\n",
      "Epoch 2 step 674: training accuarcy: 0.7475\n",
      "Epoch 2 step 674: training loss: 5846.826131982281\n",
      "Epoch 2 step 675: training accuarcy: 0.7479\n",
      "Epoch 2 step 675: training loss: 5784.512309639619\n",
      "Epoch 2 step 676: training accuarcy: 0.7493000000000001\n",
      "Epoch 2 step 676: training loss: 5775.971453041023\n",
      "Epoch 2 step 677: training accuarcy: 0.7522000000000001\n",
      "Epoch 2 step 677: training loss: 5895.682691185331\n",
      "Epoch 2 step 678: training accuarcy: 0.7395\n",
      "Epoch 2 step 678: training loss: 5805.7638601078\n",
      "Epoch 2 step 679: training accuarcy: 0.7536\n",
      "Epoch 2 step 679: training loss: 5896.886514899733\n",
      "Epoch 2 step 680: training accuarcy: 0.7342000000000001\n",
      "Epoch 2 step 680: training loss: 5928.992364149848\n",
      "Epoch 2 step 681: training accuarcy: 0.7426\n",
      "Epoch 2 step 681: training loss: 5799.771129186064\n",
      "Epoch 2 step 682: training accuarcy: 0.7475\n",
      "Epoch 2 step 682: training loss: 5881.419061006167\n",
      "Epoch 2 step 683: training accuarcy: 0.7365\n",
      "Epoch 2 step 683: training loss: 5786.922791099683\n",
      "Epoch 2 step 684: training accuarcy: 0.7487\n",
      "Epoch 2 step 684: training loss: 5935.692509988803\n",
      "Epoch 2 step 685: training accuarcy: 0.7352000000000001\n",
      "Epoch 2 step 685: training loss: 5860.04580059247\n",
      "Epoch 2 step 686: training accuarcy: 0.7469\n",
      "Epoch 2 step 686: training loss: 5916.619349716375\n",
      "Epoch 2 step 687: training accuarcy: 0.7389\n",
      "Epoch 2 step 687: training loss: 5814.143008097059\n",
      "Epoch 2 step 688: training accuarcy: 0.7556\n",
      "Epoch 2 step 688: training loss: 5781.04240398044\n",
      "Epoch 2 step 689: training accuarcy: 0.7492000000000001\n",
      "Epoch 2 step 689: training loss: 5792.292325079094\n",
      "Epoch 2 step 690: training accuarcy: 0.7503000000000001\n",
      "Epoch 2 step 690: training loss: 5868.992056750236\n",
      "Epoch 2 step 691: training accuarcy: 0.7422000000000001\n",
      "Epoch 2 step 691: training loss: 5843.80698121204\n",
      "Epoch 2 step 692: training accuarcy: 0.7505000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 692: training loss: 5811.835020194658\n",
      "Epoch 2 step 693: training accuarcy: 0.7552\n",
      "Epoch 2 step 693: training loss: 5883.713707461016\n",
      "Epoch 2 step 694: training accuarcy: 0.738\n",
      "Epoch 2 step 694: training loss: 5907.087331879984\n",
      "Epoch 2 step 695: training accuarcy: 0.7435\n",
      "Epoch 2 step 695: training loss: 5846.979468365015\n",
      "Epoch 2 step 696: training accuarcy: 0.743\n",
      "Epoch 2 step 696: training loss: 5817.106911084528\n",
      "Epoch 2 step 697: training accuarcy: 0.7534000000000001\n",
      "Epoch 2 step 697: training loss: 5791.586188503723\n",
      "Epoch 2 step 698: training accuarcy: 0.7467\n",
      "Epoch 2 step 698: training loss: 5867.808732984938\n",
      "Epoch 2 step 699: training accuarcy: 0.751\n",
      "Epoch 2 step 699: training loss: 5850.6833622012555\n",
      "Epoch 2 step 700: training accuarcy: 0.7494000000000001\n",
      "Epoch 2 step 700: training loss: 5849.148307561865\n",
      "Epoch 2 step 701: training accuarcy: 0.7473000000000001\n",
      "Epoch 2 step 701: training loss: 5875.796910377224\n",
      "Epoch 2 step 702: training accuarcy: 0.7435\n",
      "Epoch 2 step 702: training loss: 5844.236902594297\n",
      "Epoch 2 step 703: training accuarcy: 0.7442000000000001\n",
      "Epoch 2 step 703: training loss: 5824.415848629645\n",
      "Epoch 2 step 704: training accuarcy: 0.7488\n",
      "Epoch 2 step 704: training loss: 5904.228091003899\n",
      "Epoch 2 step 705: training accuarcy: 0.7413000000000001\n",
      "Epoch 2 step 705: training loss: 5778.16305275026\n",
      "Epoch 2 step 706: training accuarcy: 0.7474000000000001\n",
      "Epoch 2 step 706: training loss: 5867.160448869483\n",
      "Epoch 2 step 707: training accuarcy: 0.7432000000000001\n",
      "Epoch 2 step 707: training loss: 6000.238829225562\n",
      "Epoch 2 step 708: training accuarcy: 0.7298\n",
      "Epoch 2 step 708: training loss: 5975.922038590049\n",
      "Epoch 2 step 709: training accuarcy: 0.7386\n",
      "Epoch 2 step 709: training loss: 5792.122545809225\n",
      "Epoch 2 step 710: training accuarcy: 0.755\n",
      "Epoch 2 step 710: training loss: 5804.257785289928\n",
      "Epoch 2 step 711: training accuarcy: 0.7558\n",
      "Epoch 2 step 711: training loss: 5917.934348198503\n",
      "Epoch 2 step 712: training accuarcy: 0.7371000000000001\n",
      "Epoch 2 step 712: training loss: 5812.541931969946\n",
      "Epoch 2 step 713: training accuarcy: 0.751\n",
      "Epoch 2 step 713: training loss: 5845.278191252344\n",
      "Epoch 2 step 714: training accuarcy: 0.7446\n",
      "Epoch 2 step 714: training loss: 5798.156835752914\n",
      "Epoch 2 step 715: training accuarcy: 0.7541\n",
      "Epoch 2 step 715: training loss: 5826.8939607803\n",
      "Epoch 2 step 716: training accuarcy: 0.7456\n",
      "Epoch 2 step 716: training loss: 5867.635039176555\n",
      "Epoch 2 step 717: training accuarcy: 0.747\n",
      "Epoch 2 step 717: training loss: 5860.290371079386\n",
      "Epoch 2 step 718: training accuarcy: 0.7458\n",
      "Epoch 2 step 718: training loss: 5740.404025101877\n",
      "Epoch 2 step 719: training accuarcy: 0.7613000000000001\n",
      "Epoch 2 step 719: training loss: 5875.57674674668\n",
      "Epoch 2 step 720: training accuarcy: 0.7396\n",
      "Epoch 2 step 720: training loss: 5768.057678441205\n",
      "Epoch 2 step 721: training accuarcy: 0.7564000000000001\n",
      "Epoch 2 step 721: training loss: 5870.258133674928\n",
      "Epoch 2 step 722: training accuarcy: 0.7432000000000001\n",
      "Epoch 2 step 722: training loss: 5865.725880619908\n",
      "Epoch 2 step 723: training accuarcy: 0.7354\n",
      "Epoch 2 step 723: training loss: 5782.706877052344\n",
      "Epoch 2 step 724: training accuarcy: 0.7528\n",
      "Epoch 2 step 724: training loss: 5989.793746414349\n",
      "Epoch 2 step 725: training accuarcy: 0.736\n",
      "Epoch 2 step 725: training loss: 5856.867479088743\n",
      "Epoch 2 step 726: training accuarcy: 0.7481\n",
      "Epoch 2 step 726: training loss: 5803.848194431929\n",
      "Epoch 2 step 727: training accuarcy: 0.7478\n",
      "Epoch 2 step 727: training loss: 5774.3648764025575\n",
      "Epoch 2 step 728: training accuarcy: 0.7566\n",
      "Epoch 2 step 728: training loss: 5778.569639078925\n",
      "Epoch 2 step 729: training accuarcy: 0.7569\n",
      "Epoch 2 step 729: training loss: 5812.127371957291\n",
      "Epoch 2 step 730: training accuarcy: 0.7468\n",
      "Epoch 2 step 730: training loss: 5835.348660965655\n",
      "Epoch 2 step 731: training accuarcy: 0.7401\n",
      "Epoch 2 step 731: training loss: 5731.966916399427\n",
      "Epoch 2 step 732: training accuarcy: 0.7516\n",
      "Epoch 2 step 732: training loss: 5801.038196085052\n",
      "Epoch 2 step 733: training accuarcy: 0.7508\n",
      "Epoch 2 step 733: training loss: 5797.682349004115\n",
      "Epoch 2 step 734: training accuarcy: 0.7489\n",
      "Epoch 2 step 734: training loss: 5879.929716568677\n",
      "Epoch 2 step 735: training accuarcy: 0.7424000000000001\n",
      "Epoch 2 step 735: training loss: 5850.41257966865\n",
      "Epoch 2 step 736: training accuarcy: 0.7481\n",
      "Epoch 2 step 736: training loss: 5852.785342175186\n",
      "Epoch 2 step 737: training accuarcy: 0.7467\n",
      "Epoch 2 step 737: training loss: 5865.964848706516\n",
      "Epoch 2 step 738: training accuarcy: 0.7394000000000001\n",
      "Epoch 2 step 738: training loss: 5810.5785393377055\n",
      "Epoch 2 step 739: training accuarcy: 0.7463000000000001\n",
      "Epoch 2 step 739: training loss: 5756.910995441116\n",
      "Epoch 2 step 740: training accuarcy: 0.7572\n",
      "Epoch 2 step 740: training loss: 5857.936064184478\n",
      "Epoch 2 step 741: training accuarcy: 0.752\n",
      "Epoch 2 step 741: training loss: 5850.174476878185\n",
      "Epoch 2 step 742: training accuarcy: 0.7487\n",
      "Epoch 2 step 742: training loss: 5889.399552408793\n",
      "Epoch 2 step 743: training accuarcy: 0.7447\n",
      "Epoch 2 step 743: training loss: 5865.201376147666\n",
      "Epoch 2 step 744: training accuarcy: 0.7459\n",
      "Epoch 2 step 744: training loss: 5796.475937164314\n",
      "Epoch 2 step 745: training accuarcy: 0.7562\n",
      "Epoch 2 step 745: training loss: 5914.928946207418\n",
      "Epoch 2 step 746: training accuarcy: 0.7429\n",
      "Epoch 2 step 746: training loss: 5840.988756648303\n",
      "Epoch 2 step 747: training accuarcy: 0.7428\n",
      "Epoch 2 step 747: training loss: 5856.807475527941\n",
      "Epoch 2 step 748: training accuarcy: 0.7475\n",
      "Epoch 2 step 748: training loss: 5773.659097010426\n",
      "Epoch 2 step 749: training accuarcy: 0.756\n",
      "Epoch 2 step 749: training loss: 5842.096395396016\n",
      "Epoch 2 step 750: training accuarcy: 0.7486\n",
      "Epoch 2 step 750: training loss: 5835.540485911211\n",
      "Epoch 2 step 751: training accuarcy: 0.7435\n",
      "Epoch 2 step 751: training loss: 5841.331466793665\n",
      "Epoch 2 step 752: training accuarcy: 0.7444000000000001\n",
      "Epoch 2 step 752: training loss: 5907.142220298205\n",
      "Epoch 2 step 753: training accuarcy: 0.7441\n",
      "Epoch 2 step 753: training loss: 5886.030555085767\n",
      "Epoch 2 step 754: training accuarcy: 0.744\n",
      "Epoch 2 step 754: training loss: 5900.858073088443\n",
      "Epoch 2 step 755: training accuarcy: 0.7442000000000001\n",
      "Epoch 2 step 755: training loss: 5860.841564478911\n",
      "Epoch 2 step 756: training accuarcy: 0.7477\n",
      "Epoch 2 step 756: training loss: 5847.7665691830025\n",
      "Epoch 2 step 757: training accuarcy: 0.7497\n",
      "Epoch 2 step 757: training loss: 5841.637708400304\n",
      "Epoch 2 step 758: training accuarcy: 0.7453000000000001\n",
      "Epoch 2 step 758: training loss: 5824.964706813172\n",
      "Epoch 2 step 759: training accuarcy: 0.7455\n",
      "Epoch 2 step 759: training loss: 5818.769773535656\n",
      "Epoch 2 step 760: training accuarcy: 0.7509\n",
      "Epoch 2 step 760: training loss: 5768.630326613786\n",
      "Epoch 2 step 761: training accuarcy: 0.7529\n",
      "Epoch 2 step 761: training loss: 5839.728429479027\n",
      "Epoch 2 step 762: training accuarcy: 0.7446\n",
      "Epoch 2 step 762: training loss: 5866.117306942925\n",
      "Epoch 2 step 763: training accuarcy: 0.7475\n",
      "Epoch 2 step 763: training loss: 5895.600001740931\n",
      "Epoch 2 step 764: training accuarcy: 0.7418\n",
      "Epoch 2 step 764: training loss: 5772.519567354865\n",
      "Epoch 2 step 765: training accuarcy: 0.7574000000000001\n",
      "Epoch 2 step 765: training loss: 5782.941421820374\n",
      "Epoch 2 step 766: training accuarcy: 0.7545000000000001\n",
      "Epoch 2 step 766: training loss: 5923.786171435049\n",
      "Epoch 2 step 767: training accuarcy: 0.7423000000000001\n",
      "Epoch 2 step 767: training loss: 5881.841020131806\n",
      "Epoch 2 step 768: training accuarcy: 0.7422000000000001\n",
      "Epoch 2 step 768: training loss: 5860.066596385081\n",
      "Epoch 2 step 769: training accuarcy: 0.7468\n",
      "Epoch 2 step 769: training loss: 5982.6361175493785\n",
      "Epoch 2 step 770: training accuarcy: 0.7294\n",
      "Epoch 2 step 770: training loss: 5897.765618641532\n",
      "Epoch 2 step 771: training accuarcy: 0.7365\n",
      "Epoch 2 step 771: training loss: 5793.847217532955\n",
      "Epoch 2 step 772: training accuarcy: 0.7481\n",
      "Epoch 2 step 772: training loss: 5885.944811365987\n",
      "Epoch 2 step 773: training accuarcy: 0.7432000000000001\n",
      "Epoch 2 step 773: training loss: 5853.779688682283\n",
      "Epoch 2 step 774: training accuarcy: 0.7412000000000001\n",
      "Epoch 2 step 774: training loss: 5796.190627918683\n",
      "Epoch 2 step 775: training accuarcy: 0.7406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 775: training loss: 5890.86072994669\n",
      "Epoch 2 step 776: training accuarcy: 0.7434000000000001\n",
      "Epoch 2 step 776: training loss: 5846.890042634417\n",
      "Epoch 2 step 777: training accuarcy: 0.7473000000000001\n",
      "Epoch 2 step 777: training loss: 5765.618175042394\n",
      "Epoch 2 step 778: training accuarcy: 0.7655000000000001\n",
      "Epoch 2 step 778: training loss: 5775.844352081733\n",
      "Epoch 2 step 779: training accuarcy: 0.7468\n",
      "Epoch 2 step 779: training loss: 5842.100830162649\n",
      "Epoch 2 step 780: training accuarcy: 0.7516\n",
      "Epoch 2 step 780: training loss: 5750.1457929048875\n",
      "Epoch 2 step 781: training accuarcy: 0.7579\n",
      "Epoch 2 step 781: training loss: 5909.569845082015\n",
      "Epoch 2 step 782: training accuarcy: 0.7381000000000001\n",
      "Epoch 2 step 782: training loss: 5763.105566623627\n",
      "Epoch 2 step 783: training accuarcy: 0.7535000000000001\n",
      "Epoch 2 step 783: training loss: 5861.269826782854\n",
      "Epoch 2 step 784: training accuarcy: 0.7473000000000001\n",
      "Epoch 2 step 784: training loss: 5780.871625873077\n",
      "Epoch 2 step 785: training accuarcy: 0.7524000000000001\n",
      "Epoch 2 step 785: training loss: 5764.5001047713595\n",
      "Epoch 2 step 786: training accuarcy: 0.7554000000000001\n",
      "Epoch 2 step 786: training loss: 5855.407944845836\n",
      "Epoch 2 step 787: training accuarcy: 0.7526\n",
      "Epoch 2 step 787: training loss: 5790.039839729377\n",
      "Epoch 2 step 788: training accuarcy: 0.7399\n",
      "Epoch 2 step 788: training loss: 2491.057229476086\n",
      "Epoch 2 step 789: training accuarcy: 0.7325641025641025\n",
      "Epoch 2: train loss 5847.56098889614, train accuarcy 0.7231202721595764\n",
      "Epoch 2: valid loss 5595.650480084768, valid accuarcy 0.7639785408973694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [15:11<00:00, 303.90s/it]\n"
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=3,\n",
    "                log_dir=get_log_dir('simple_topcoder', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:39:42.508036Z",
     "start_time": "2019-10-09T12:24:27.256918Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 193365.16163974453\n",
      "Epoch 0 step 1: training accuarcy: 0.51\n",
      "Epoch 0 step 1: training loss: 185119.86205282228\n",
      "Epoch 0 step 2: training accuarcy: 0.5207\n",
      "Epoch 0 step 2: training loss: 189420.40994518064\n",
      "Epoch 0 step 3: training accuarcy: 0.5195000000000001\n",
      "Epoch 0 step 3: training loss: 179850.18585890112\n",
      "Epoch 0 step 4: training accuarcy: 0.5253\n",
      "Epoch 0 step 4: training loss: 176592.3830378908\n",
      "Epoch 0 step 5: training accuarcy: 0.5118\n",
      "Epoch 0 step 5: training loss: 171351.80347387\n",
      "Epoch 0 step 6: training accuarcy: 0.5213\n",
      "Epoch 0 step 6: training loss: 164100.8087699164\n",
      "Epoch 0 step 7: training accuarcy: 0.5284\n",
      "Epoch 0 step 7: training loss: 161553.85251411583\n",
      "Epoch 0 step 8: training accuarcy: 0.5347000000000001\n",
      "Epoch 0 step 8: training loss: 162316.5572234422\n",
      "Epoch 0 step 9: training accuarcy: 0.5323\n",
      "Epoch 0 step 9: training loss: 153082.98121592737\n",
      "Epoch 0 step 10: training accuarcy: 0.522\n",
      "Epoch 0 step 10: training loss: 150206.3848270157\n",
      "Epoch 0 step 11: training accuarcy: 0.5258\n",
      "Epoch 0 step 11: training loss: 144639.407001255\n",
      "Epoch 0 step 12: training accuarcy: 0.5371\n",
      "Epoch 0 step 12: training loss: 145414.24965864274\n",
      "Epoch 0 step 13: training accuarcy: 0.5306000000000001\n",
      "Epoch 0 step 13: training loss: 141295.10355182062\n",
      "Epoch 0 step 14: training accuarcy: 0.527\n",
      "Epoch 0 step 14: training loss: 135046.33918794277\n",
      "Epoch 0 step 15: training accuarcy: 0.5378000000000001\n",
      "Epoch 0 step 15: training loss: 134339.02288177123\n",
      "Epoch 0 step 16: training accuarcy: 0.5233\n",
      "Epoch 0 step 16: training loss: 131281.90260255878\n",
      "Epoch 0 step 17: training accuarcy: 0.5215000000000001\n",
      "Epoch 0 step 17: training loss: 127101.5524351792\n",
      "Epoch 0 step 18: training accuarcy: 0.5173\n",
      "Epoch 0 step 18: training loss: 126965.93325599973\n",
      "Epoch 0 step 19: training accuarcy: 0.5211\n",
      "Epoch 0 step 19: training loss: 119878.51462630939\n",
      "Epoch 0 step 20: training accuarcy: 0.5352\n",
      "Epoch 0 step 20: training loss: 119982.03796053487\n",
      "Epoch 0 step 21: training accuarcy: 0.5158\n",
      "Epoch 0 step 21: training loss: 114535.9586199146\n",
      "Epoch 0 step 22: training accuarcy: 0.5259\n",
      "Epoch 0 step 22: training loss: 111771.03850666934\n",
      "Epoch 0 step 23: training accuarcy: 0.5242\n",
      "Epoch 0 step 23: training loss: 111137.40762711813\n",
      "Epoch 0 step 24: training accuarcy: 0.527\n",
      "Epoch 0 step 24: training loss: 107410.92474167107\n",
      "Epoch 0 step 25: training accuarcy: 0.5292\n",
      "Epoch 0 step 25: training loss: 103893.81330921061\n",
      "Epoch 0 step 26: training accuarcy: 0.5234\n",
      "Epoch 0 step 26: training loss: 99039.4704165088\n",
      "Epoch 0 step 27: training accuarcy: 0.547\n",
      "Epoch 0 step 27: training loss: 97368.56665713676\n",
      "Epoch 0 step 28: training accuarcy: 0.5392\n",
      "Epoch 0 step 28: training loss: 97689.92931006211\n",
      "Epoch 0 step 29: training accuarcy: 0.5216000000000001\n",
      "Epoch 0 step 29: training loss: 92562.97555925263\n",
      "Epoch 0 step 30: training accuarcy: 0.5386000000000001\n",
      "Epoch 0 step 30: training loss: 91093.86903852389\n",
      "Epoch 0 step 31: training accuarcy: 0.5352\n",
      "Epoch 0 step 31: training loss: 87765.49222048059\n",
      "Epoch 0 step 32: training accuarcy: 0.5469\n",
      "Epoch 0 step 32: training loss: 89542.59140375617\n",
      "Epoch 0 step 33: training accuarcy: 0.5313\n",
      "Epoch 0 step 33: training loss: 86627.06900394175\n",
      "Epoch 0 step 34: training accuarcy: 0.5325\n",
      "Epoch 0 step 34: training loss: 82551.08588498089\n",
      "Epoch 0 step 35: training accuarcy: 0.5324\n",
      "Epoch 0 step 35: training loss: 79838.5569073899\n",
      "Epoch 0 step 36: training accuarcy: 0.5395\n",
      "Epoch 0 step 36: training loss: 80650.7825718748\n",
      "Epoch 0 step 37: training accuarcy: 0.5424\n",
      "Epoch 0 step 37: training loss: 76755.96957680708\n",
      "Epoch 0 step 38: training accuarcy: 0.5308\n",
      "Epoch 0 step 38: training loss: 74575.61042324468\n",
      "Epoch 0 step 39: training accuarcy: 0.5419\n",
      "Epoch 0 step 39: training loss: 73003.01111989663\n",
      "Epoch 0 step 40: training accuarcy: 0.549\n",
      "Epoch 0 step 40: training loss: 72182.6130454413\n",
      "Epoch 0 step 41: training accuarcy: 0.5475\n",
      "Epoch 0 step 41: training loss: 71740.61104428832\n",
      "Epoch 0 step 42: training accuarcy: 0.5361\n",
      "Epoch 0 step 42: training loss: 69055.97206726775\n",
      "Epoch 0 step 43: training accuarcy: 0.5478000000000001\n",
      "Epoch 0 step 43: training loss: 71443.60865343726\n",
      "Epoch 0 step 44: training accuarcy: 0.5313\n",
      "Epoch 0 step 44: training loss: 67496.54850426059\n",
      "Epoch 0 step 45: training accuarcy: 0.5394\n",
      "Epoch 0 step 45: training loss: 66884.09848947471\n",
      "Epoch 0 step 46: training accuarcy: 0.5416000000000001\n",
      "Epoch 0 step 46: training loss: 65354.551555240425\n",
      "Epoch 0 step 47: training accuarcy: 0.547\n",
      "Epoch 0 step 47: training loss: 63372.73146955027\n",
      "Epoch 0 step 48: training accuarcy: 0.5575\n",
      "Epoch 0 step 48: training loss: 62309.72307750742\n",
      "Epoch 0 step 49: training accuarcy: 0.556\n",
      "Epoch 0 step 49: training loss: 62484.846466148396\n",
      "Epoch 0 step 50: training accuarcy: 0.5453\n",
      "Epoch 0 step 50: training loss: 62004.870543039775\n",
      "Epoch 0 step 51: training accuarcy: 0.5468000000000001\n",
      "Epoch 0 step 51: training loss: 60214.28097059796\n",
      "Epoch 0 step 52: training accuarcy: 0.5491\n",
      "Epoch 0 step 52: training loss: 59073.11136021465\n",
      "Epoch 0 step 53: training accuarcy: 0.5472\n",
      "Epoch 0 step 53: training loss: 57907.096257366095\n",
      "Epoch 0 step 54: training accuarcy: 0.5667\n",
      "Epoch 0 step 54: training loss: 57446.971101294985\n",
      "Epoch 0 step 55: training accuarcy: 0.5592\n",
      "Epoch 0 step 55: training loss: 57187.69814665722\n",
      "Epoch 0 step 56: training accuarcy: 0.5515\n",
      "Epoch 0 step 56: training loss: 55681.3639589378\n",
      "Epoch 0 step 57: training accuarcy: 0.5640000000000001\n",
      "Epoch 0 step 57: training loss: 55098.648536720626\n",
      "Epoch 0 step 58: training accuarcy: 0.5528000000000001\n",
      "Epoch 0 step 58: training loss: 55576.07604314837\n",
      "Epoch 0 step 59: training accuarcy: 0.5503\n",
      "Epoch 0 step 59: training loss: 53351.653025195046\n",
      "Epoch 0 step 60: training accuarcy: 0.5694\n",
      "Epoch 0 step 60: training loss: 53476.73057687022\n",
      "Epoch 0 step 61: training accuarcy: 0.5725\n",
      "Epoch 0 step 61: training loss: 53182.46572757854\n",
      "Epoch 0 step 62: training accuarcy: 0.5567\n",
      "Epoch 0 step 62: training loss: 51921.74304342278\n",
      "Epoch 0 step 63: training accuarcy: 0.5738\n",
      "Epoch 0 step 63: training loss: 52452.13308390027\n",
      "Epoch 0 step 64: training accuarcy: 0.553\n",
      "Epoch 0 step 64: training loss: 51308.65159587097\n",
      "Epoch 0 step 65: training accuarcy: 0.5650000000000001\n",
      "Epoch 0 step 65: training loss: 50963.134148719284\n",
      "Epoch 0 step 66: training accuarcy: 0.5646\n",
      "Epoch 0 step 66: training loss: 50695.183484107365\n",
      "Epoch 0 step 67: training accuarcy: 0.5653\n",
      "Epoch 0 step 67: training loss: 49738.203942428416\n",
      "Epoch 0 step 68: training accuarcy: 0.5779000000000001\n",
      "Epoch 0 step 68: training loss: 50297.04415502014\n",
      "Epoch 0 step 69: training accuarcy: 0.5671\n",
      "Epoch 0 step 69: training loss: 49365.82384885503\n",
      "Epoch 0 step 70: training accuarcy: 0.5762\n",
      "Epoch 0 step 70: training loss: 48793.720091769836\n",
      "Epoch 0 step 71: training accuarcy: 0.5826\n",
      "Epoch 0 step 71: training loss: 47683.69072467066\n",
      "Epoch 0 step 72: training accuarcy: 0.5914\n",
      "Epoch 0 step 72: training loss: 47894.8782217871\n",
      "Epoch 0 step 73: training accuarcy: 0.5839\n",
      "Epoch 0 step 73: training loss: 47544.03427265578\n",
      "Epoch 0 step 74: training accuarcy: 0.5862\n",
      "Epoch 0 step 74: training loss: 47294.23433352224\n",
      "Epoch 0 step 75: training accuarcy: 0.5873\n",
      "Epoch 0 step 75: training loss: 47614.946790266375\n",
      "Epoch 0 step 76: training accuarcy: 0.5866\n",
      "Epoch 0 step 76: training loss: 47897.67473745393\n",
      "Epoch 0 step 77: training accuarcy: 0.5831000000000001\n",
      "Epoch 0 step 77: training loss: 46492.498989672546\n",
      "Epoch 0 step 78: training accuarcy: 0.5958\n",
      "Epoch 0 step 78: training loss: 46802.10184603045\n",
      "Epoch 0 step 79: training accuarcy: 0.5884\n",
      "Epoch 0 step 79: training loss: 46621.560306314226\n",
      "Epoch 0 step 80: training accuarcy: 0.5846\n",
      "Epoch 0 step 80: training loss: 45886.56656124981\n",
      "Epoch 0 step 81: training accuarcy: 0.5963\n",
      "Epoch 0 step 81: training loss: 46624.26668270258\n",
      "Epoch 0 step 82: training accuarcy: 0.5804\n",
      "Epoch 0 step 82: training loss: 45886.41567402624\n",
      "Epoch 0 step 83: training accuarcy: 0.6001000000000001\n",
      "Epoch 0 step 83: training loss: 45748.32500895973\n",
      "Epoch 0 step 84: training accuarcy: 0.5881000000000001\n",
      "Epoch 0 step 84: training loss: 45492.795963150034\n",
      "Epoch 0 step 85: training accuarcy: 0.5875\n",
      "Epoch 0 step 85: training loss: 45236.212746289246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 86: training accuarcy: 0.5819\n",
      "Epoch 0 step 86: training loss: 44889.39903795921\n",
      "Epoch 0 step 87: training accuarcy: 0.5967\n",
      "Epoch 0 step 87: training loss: 44594.90324295683\n",
      "Epoch 0 step 88: training accuarcy: 0.6064\n",
      "Epoch 0 step 88: training loss: 44248.53584450766\n",
      "Epoch 0 step 89: training accuarcy: 0.607\n",
      "Epoch 0 step 89: training loss: 44968.39339135873\n",
      "Epoch 0 step 90: training accuarcy: 0.5903\n",
      "Epoch 0 step 90: training loss: 44462.97636799871\n",
      "Epoch 0 step 91: training accuarcy: 0.5972000000000001\n",
      "Epoch 0 step 91: training loss: 43692.35513216111\n",
      "Epoch 0 step 92: training accuarcy: 0.6045\n",
      "Epoch 0 step 92: training loss: 43768.13933397746\n",
      "Epoch 0 step 93: training accuarcy: 0.6159\n",
      "Epoch 0 step 93: training loss: 44247.50242781735\n",
      "Epoch 0 step 94: training accuarcy: 0.5985\n",
      "Epoch 0 step 94: training loss: 43860.23705441014\n",
      "Epoch 0 step 95: training accuarcy: 0.612\n",
      "Epoch 0 step 95: training loss: 44254.405241494256\n",
      "Epoch 0 step 96: training accuarcy: 0.5932000000000001\n",
      "Epoch 0 step 96: training loss: 43674.00069915353\n",
      "Epoch 0 step 97: training accuarcy: 0.6089\n",
      "Epoch 0 step 97: training loss: 42896.64588761967\n",
      "Epoch 0 step 98: training accuarcy: 0.6168\n",
      "Epoch 0 step 98: training loss: 42969.35864003167\n",
      "Epoch 0 step 99: training accuarcy: 0.6207\n",
      "Epoch 0 step 99: training loss: 42837.57999000067\n",
      "Epoch 0 step 100: training accuarcy: 0.6102000000000001\n",
      "Epoch 0 step 100: training loss: 42651.02559994623\n",
      "Epoch 0 step 101: training accuarcy: 0.6238\n",
      "Epoch 0 step 101: training loss: 42833.840299424584\n",
      "Epoch 0 step 102: training accuarcy: 0.616\n",
      "Epoch 0 step 102: training loss: 42675.25066872553\n",
      "Epoch 0 step 103: training accuarcy: 0.6203000000000001\n",
      "Epoch 0 step 103: training loss: 42656.57484858908\n",
      "Epoch 0 step 104: training accuarcy: 0.6095\n",
      "Epoch 0 step 104: training loss: 42185.881449090084\n",
      "Epoch 0 step 105: training accuarcy: 0.6265000000000001\n",
      "Epoch 0 step 105: training loss: 42648.305081872124\n",
      "Epoch 0 step 106: training accuarcy: 0.6086\n",
      "Epoch 0 step 106: training loss: 41451.634755201856\n",
      "Epoch 0 step 107: training accuarcy: 0.6297\n",
      "Epoch 0 step 107: training loss: 41891.54073461717\n",
      "Epoch 0 step 108: training accuarcy: 0.623\n",
      "Epoch 0 step 108: training loss: 42133.13205485652\n",
      "Epoch 0 step 109: training accuarcy: 0.6185\n",
      "Epoch 0 step 109: training loss: 41417.558175373204\n",
      "Epoch 0 step 110: training accuarcy: 0.6346\n",
      "Epoch 0 step 110: training loss: 41498.88263415163\n",
      "Epoch 0 step 111: training accuarcy: 0.6303000000000001\n",
      "Epoch 0 step 111: training loss: 41915.51315287541\n",
      "Epoch 0 step 112: training accuarcy: 0.6187\n",
      "Epoch 0 step 112: training loss: 41507.584704272056\n",
      "Epoch 0 step 113: training accuarcy: 0.6309\n",
      "Epoch 0 step 113: training loss: 41963.67200764065\n",
      "Epoch 0 step 114: training accuarcy: 0.6189\n",
      "Epoch 0 step 114: training loss: 41226.71086686451\n",
      "Epoch 0 step 115: training accuarcy: 0.6305000000000001\n",
      "Epoch 0 step 115: training loss: 41041.51766209875\n",
      "Epoch 0 step 116: training accuarcy: 0.6415000000000001\n",
      "Epoch 0 step 116: training loss: 41108.55217214449\n",
      "Epoch 0 step 117: training accuarcy: 0.6372\n",
      "Epoch 0 step 117: training loss: 41296.46640927435\n",
      "Epoch 0 step 118: training accuarcy: 0.6278\n",
      "Epoch 0 step 118: training loss: 41879.50548272974\n",
      "Epoch 0 step 119: training accuarcy: 0.6225\n",
      "Epoch 0 step 119: training loss: 41402.41689481221\n",
      "Epoch 0 step 120: training accuarcy: 0.6334000000000001\n",
      "Epoch 0 step 120: training loss: 41187.50437681527\n",
      "Epoch 0 step 121: training accuarcy: 0.6337\n",
      "Epoch 0 step 121: training loss: 40769.51412673939\n",
      "Epoch 0 step 122: training accuarcy: 0.6346\n",
      "Epoch 0 step 122: training loss: 40724.69916503954\n",
      "Epoch 0 step 123: training accuarcy: 0.6389\n",
      "Epoch 0 step 123: training loss: 40074.516892071246\n",
      "Epoch 0 step 124: training accuarcy: 0.6501\n",
      "Epoch 0 step 124: training loss: 40321.97791010557\n",
      "Epoch 0 step 125: training accuarcy: 0.6486000000000001\n",
      "Epoch 0 step 125: training loss: 40049.24204191955\n",
      "Epoch 0 step 126: training accuarcy: 0.6483\n",
      "Epoch 0 step 126: training loss: 40112.32568264703\n",
      "Epoch 0 step 127: training accuarcy: 0.6495000000000001\n",
      "Epoch 0 step 127: training loss: 39663.56236194968\n",
      "Epoch 0 step 128: training accuarcy: 0.6618\n",
      "Epoch 0 step 128: training loss: 39905.95570731075\n",
      "Epoch 0 step 129: training accuarcy: 0.6477\n",
      "Epoch 0 step 129: training loss: 39886.04678616509\n",
      "Epoch 0 step 130: training accuarcy: 0.6557000000000001\n",
      "Epoch 0 step 130: training loss: 40020.16563802789\n",
      "Epoch 0 step 131: training accuarcy: 0.655\n",
      "Epoch 0 step 131: training loss: 39625.68408617367\n",
      "Epoch 0 step 132: training accuarcy: 0.65\n",
      "Epoch 0 step 132: training loss: 39705.92042242375\n",
      "Epoch 0 step 133: training accuarcy: 0.6605000000000001\n",
      "Epoch 0 step 133: training loss: 39490.04742371687\n",
      "Epoch 0 step 134: training accuarcy: 0.6538\n",
      "Epoch 0 step 134: training loss: 39557.696141043176\n",
      "Epoch 0 step 135: training accuarcy: 0.6652\n",
      "Epoch 0 step 135: training loss: 39610.512137629565\n",
      "Epoch 0 step 136: training accuarcy: 0.6487\n",
      "Epoch 0 step 136: training loss: 39764.356623355976\n",
      "Epoch 0 step 137: training accuarcy: 0.65\n",
      "Epoch 0 step 137: training loss: 39399.646686360386\n",
      "Epoch 0 step 138: training accuarcy: 0.6584\n",
      "Epoch 0 step 138: training loss: 38824.642481342016\n",
      "Epoch 0 step 139: training accuarcy: 0.6729\n",
      "Epoch 0 step 139: training loss: 38837.01806262264\n",
      "Epoch 0 step 140: training accuarcy: 0.6698000000000001\n",
      "Epoch 0 step 140: training loss: 38834.93104748565\n",
      "Epoch 0 step 141: training accuarcy: 0.6696000000000001\n",
      "Epoch 0 step 141: training loss: 38912.460544360816\n",
      "Epoch 0 step 142: training accuarcy: 0.6696000000000001\n",
      "Epoch 0 step 142: training loss: 38680.60371682092\n",
      "Epoch 0 step 143: training accuarcy: 0.6661\n",
      "Epoch 0 step 143: training loss: 38702.238929985455\n",
      "Epoch 0 step 144: training accuarcy: 0.6687000000000001\n",
      "Epoch 0 step 144: training loss: 38849.27618623343\n",
      "Epoch 0 step 145: training accuarcy: 0.6633\n",
      "Epoch 0 step 145: training loss: 39188.26484068709\n",
      "Epoch 0 step 146: training accuarcy: 0.6588\n",
      "Epoch 0 step 146: training loss: 38622.890297750986\n",
      "Epoch 0 step 147: training accuarcy: 0.671\n",
      "Epoch 0 step 147: training loss: 38405.99375159457\n",
      "Epoch 0 step 148: training accuarcy: 0.6759000000000001\n",
      "Epoch 0 step 148: training loss: 38453.17007851935\n",
      "Epoch 0 step 149: training accuarcy: 0.6781\n",
      "Epoch 0 step 149: training loss: 38872.21685071643\n",
      "Epoch 0 step 150: training accuarcy: 0.6695\n",
      "Epoch 0 step 150: training loss: 38739.66316213471\n",
      "Epoch 0 step 151: training accuarcy: 0.6663\n",
      "Epoch 0 step 151: training loss: 38343.395765643\n",
      "Epoch 0 step 152: training accuarcy: 0.6712\n",
      "Epoch 0 step 152: training loss: 37979.318837618805\n",
      "Epoch 0 step 153: training accuarcy: 0.6875\n",
      "Epoch 0 step 153: training loss: 38156.486212527176\n",
      "Epoch 0 step 154: training accuarcy: 0.6744\n",
      "Epoch 0 step 154: training loss: 38491.53359396441\n",
      "Epoch 0 step 155: training accuarcy: 0.6762\n",
      "Epoch 0 step 155: training loss: 37920.01717030075\n",
      "Epoch 0 step 156: training accuarcy: 0.685\n",
      "Epoch 0 step 156: training loss: 37981.37228142651\n",
      "Epoch 0 step 157: training accuarcy: 0.683\n",
      "Epoch 0 step 157: training loss: 37904.011466645585\n",
      "Epoch 0 step 158: training accuarcy: 0.678\n",
      "Epoch 0 step 158: training loss: 37718.649433871855\n",
      "Epoch 0 step 159: training accuarcy: 0.6871\n",
      "Epoch 0 step 159: training loss: 37479.22279795881\n",
      "Epoch 0 step 160: training accuarcy: 0.6885\n",
      "Epoch 0 step 160: training loss: 37601.55259248121\n",
      "Epoch 0 step 161: training accuarcy: 0.6826\n",
      "Epoch 0 step 161: training loss: 38085.27381216524\n",
      "Epoch 0 step 162: training accuarcy: 0.6687000000000001\n",
      "Epoch 0 step 162: training loss: 38039.10241673222\n",
      "Epoch 0 step 163: training accuarcy: 0.6852\n",
      "Epoch 0 step 163: training loss: 37862.9427208481\n",
      "Epoch 0 step 164: training accuarcy: 0.6809000000000001\n",
      "Epoch 0 step 164: training loss: 37749.71662737567\n",
      "Epoch 0 step 165: training accuarcy: 0.6844\n",
      "Epoch 0 step 165: training loss: 37164.49553803697\n",
      "Epoch 0 step 166: training accuarcy: 0.6961\n",
      "Epoch 0 step 166: training loss: 37449.77239260851\n",
      "Epoch 0 step 167: training accuarcy: 0.6874\n",
      "Epoch 0 step 167: training loss: 37659.393278794065\n",
      "Epoch 0 step 168: training accuarcy: 0.6875\n",
      "Epoch 0 step 168: training loss: 37239.63905204004\n",
      "Epoch 0 step 169: training accuarcy: 0.6829000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 169: training loss: 37197.22417312359\n",
      "Epoch 0 step 170: training accuarcy: 0.6948000000000001\n",
      "Epoch 0 step 170: training loss: 37271.32799917286\n",
      "Epoch 0 step 171: training accuarcy: 0.6865\n",
      "Epoch 0 step 171: training loss: 38013.06343429402\n",
      "Epoch 0 step 172: training accuarcy: 0.6849000000000001\n",
      "Epoch 0 step 172: training loss: 37532.25739286332\n",
      "Epoch 0 step 173: training accuarcy: 0.6824\n",
      "Epoch 0 step 173: training loss: 37072.33954684183\n",
      "Epoch 0 step 174: training accuarcy: 0.6904\n",
      "Epoch 0 step 174: training loss: 36897.478919225105\n",
      "Epoch 0 step 175: training accuarcy: 0.6941\n",
      "Epoch 0 step 175: training loss: 36839.429808392735\n",
      "Epoch 0 step 176: training accuarcy: 0.6974\n",
      "Epoch 0 step 176: training loss: 36865.56303404251\n",
      "Epoch 0 step 177: training accuarcy: 0.6892\n",
      "Epoch 0 step 177: training loss: 37232.450899596624\n",
      "Epoch 0 step 178: training accuarcy: 0.6881\n",
      "Epoch 0 step 178: training loss: 36279.362217262074\n",
      "Epoch 0 step 179: training accuarcy: 0.7096\n",
      "Epoch 0 step 179: training loss: 37100.95838853873\n",
      "Epoch 0 step 180: training accuarcy: 0.6922\n",
      "Epoch 0 step 180: training loss: 36831.99898559486\n",
      "Epoch 0 step 181: training accuarcy: 0.7022\n",
      "Epoch 0 step 181: training loss: 36853.10260686503\n",
      "Epoch 0 step 182: training accuarcy: 0.6942\n",
      "Epoch 0 step 182: training loss: 36625.08324917516\n",
      "Epoch 0 step 183: training accuarcy: 0.6989000000000001\n",
      "Epoch 0 step 183: training loss: 36402.67693204575\n",
      "Epoch 0 step 184: training accuarcy: 0.7065\n",
      "Epoch 0 step 184: training loss: 36386.27030161718\n",
      "Epoch 0 step 185: training accuarcy: 0.7014\n",
      "Epoch 0 step 185: training loss: 36597.94718139978\n",
      "Epoch 0 step 186: training accuarcy: 0.6997\n",
      "Epoch 0 step 186: training loss: 37097.61758564309\n",
      "Epoch 0 step 187: training accuarcy: 0.6864\n",
      "Epoch 0 step 187: training loss: 36830.25099546311\n",
      "Epoch 0 step 188: training accuarcy: 0.6964\n",
      "Epoch 0 step 188: training loss: 35935.56804394504\n",
      "Epoch 0 step 189: training accuarcy: 0.7081000000000001\n",
      "Epoch 0 step 189: training loss: 36162.000293282006\n",
      "Epoch 0 step 190: training accuarcy: 0.7022\n",
      "Epoch 0 step 190: training loss: 36242.13291677952\n",
      "Epoch 0 step 191: training accuarcy: 0.7071000000000001\n",
      "Epoch 0 step 191: training loss: 36725.14780642603\n",
      "Epoch 0 step 192: training accuarcy: 0.7014\n",
      "Epoch 0 step 192: training loss: 36571.533839624586\n",
      "Epoch 0 step 193: training accuarcy: 0.6979000000000001\n",
      "Epoch 0 step 193: training loss: 36252.634259176426\n",
      "Epoch 0 step 194: training accuarcy: 0.7061000000000001\n",
      "Epoch 0 step 194: training loss: 36257.994766221935\n",
      "Epoch 0 step 195: training accuarcy: 0.7035\n",
      "Epoch 0 step 195: training loss: 35966.98966892451\n",
      "Epoch 0 step 196: training accuarcy: 0.7067\n",
      "Epoch 0 step 196: training loss: 36017.82349128469\n",
      "Epoch 0 step 197: training accuarcy: 0.7056\n",
      "Epoch 0 step 197: training loss: 36356.7010422043\n",
      "Epoch 0 step 198: training accuarcy: 0.7035\n",
      "Epoch 0 step 198: training loss: 35675.668082417265\n",
      "Epoch 0 step 199: training accuarcy: 0.7174\n",
      "Epoch 0 step 199: training loss: 36115.203165077604\n",
      "Epoch 0 step 200: training accuarcy: 0.6965\n",
      "Epoch 0 step 200: training loss: 36083.36830895803\n",
      "Epoch 0 step 201: training accuarcy: 0.7058\n",
      "Epoch 0 step 201: training loss: 35874.982346960096\n",
      "Epoch 0 step 202: training accuarcy: 0.7063\n",
      "Epoch 0 step 202: training loss: 35996.52356107235\n",
      "Epoch 0 step 203: training accuarcy: 0.7020000000000001\n",
      "Epoch 0 step 203: training loss: 36285.48536118589\n",
      "Epoch 0 step 204: training accuarcy: 0.7058\n",
      "Epoch 0 step 204: training loss: 35830.093796765614\n",
      "Epoch 0 step 205: training accuarcy: 0.7090000000000001\n",
      "Epoch 0 step 205: training loss: 35973.99167644843\n",
      "Epoch 0 step 206: training accuarcy: 0.7047\n",
      "Epoch 0 step 206: training loss: 35379.650118237616\n",
      "Epoch 0 step 207: training accuarcy: 0.7239\n",
      "Epoch 0 step 207: training loss: 35818.87913388075\n",
      "Epoch 0 step 208: training accuarcy: 0.7152000000000001\n",
      "Epoch 0 step 208: training loss: 34963.551392016365\n",
      "Epoch 0 step 209: training accuarcy: 0.7253000000000001\n",
      "Epoch 0 step 209: training loss: 35463.48597475151\n",
      "Epoch 0 step 210: training accuarcy: 0.7098\n",
      "Epoch 0 step 210: training loss: 35152.59618966754\n",
      "Epoch 0 step 211: training accuarcy: 0.7118\n",
      "Epoch 0 step 211: training loss: 35448.428277139814\n",
      "Epoch 0 step 212: training accuarcy: 0.7165\n",
      "Epoch 0 step 212: training loss: 35229.46708689221\n",
      "Epoch 0 step 213: training accuarcy: 0.7118\n",
      "Epoch 0 step 213: training loss: 35509.76467399786\n",
      "Epoch 0 step 214: training accuarcy: 0.7205\n",
      "Epoch 0 step 214: training loss: 35984.482258246826\n",
      "Epoch 0 step 215: training accuarcy: 0.7034\n",
      "Epoch 0 step 215: training loss: 35460.55754635639\n",
      "Epoch 0 step 216: training accuarcy: 0.7199\n",
      "Epoch 0 step 216: training loss: 35591.21015305679\n",
      "Epoch 0 step 217: training accuarcy: 0.7188\n",
      "Epoch 0 step 217: training loss: 35356.71136722554\n",
      "Epoch 0 step 218: training accuarcy: 0.7101000000000001\n",
      "Epoch 0 step 218: training loss: 35248.732488283276\n",
      "Epoch 0 step 219: training accuarcy: 0.7187\n",
      "Epoch 0 step 219: training loss: 35249.81769069251\n",
      "Epoch 0 step 220: training accuarcy: 0.7219\n",
      "Epoch 0 step 220: training loss: 34993.213837316536\n",
      "Epoch 0 step 221: training accuarcy: 0.7162000000000001\n",
      "Epoch 0 step 221: training loss: 34986.43602715645\n",
      "Epoch 0 step 222: training accuarcy: 0.7241000000000001\n",
      "Epoch 0 step 222: training loss: 34863.558679947775\n",
      "Epoch 0 step 223: training accuarcy: 0.7252000000000001\n",
      "Epoch 0 step 223: training loss: 35340.2167809559\n",
      "Epoch 0 step 224: training accuarcy: 0.7184\n",
      "Epoch 0 step 224: training loss: 35016.86586821722\n",
      "Epoch 0 step 225: training accuarcy: 0.7206\n",
      "Epoch 0 step 225: training loss: 34995.72357523601\n",
      "Epoch 0 step 226: training accuarcy: 0.7311000000000001\n",
      "Epoch 0 step 226: training loss: 34821.15321005895\n",
      "Epoch 0 step 227: training accuarcy: 0.7250000000000001\n",
      "Epoch 0 step 227: training loss: 35183.5636525012\n",
      "Epoch 0 step 228: training accuarcy: 0.7198\n",
      "Epoch 0 step 228: training loss: 35137.72738377773\n",
      "Epoch 0 step 229: training accuarcy: 0.7297\n",
      "Epoch 0 step 229: training loss: 35069.935788089824\n",
      "Epoch 0 step 230: training accuarcy: 0.7229\n",
      "Epoch 0 step 230: training loss: 34309.64009126546\n",
      "Epoch 0 step 231: training accuarcy: 0.7258\n",
      "Epoch 0 step 231: training loss: 34957.74452757635\n",
      "Epoch 0 step 232: training accuarcy: 0.7159\n",
      "Epoch 0 step 232: training loss: 34447.535412812285\n",
      "Epoch 0 step 233: training accuarcy: 0.7247\n",
      "Epoch 0 step 233: training loss: 34936.56444875852\n",
      "Epoch 0 step 234: training accuarcy: 0.7203\n",
      "Epoch 0 step 234: training loss: 34241.0661770185\n",
      "Epoch 0 step 235: training accuarcy: 0.727\n",
      "Epoch 0 step 235: training loss: 34668.49831956962\n",
      "Epoch 0 step 236: training accuarcy: 0.7225\n",
      "Epoch 0 step 236: training loss: 34572.54377675141\n",
      "Epoch 0 step 237: training accuarcy: 0.7247\n",
      "Epoch 0 step 237: training loss: 34746.18165536891\n",
      "Epoch 0 step 238: training accuarcy: 0.7216\n",
      "Epoch 0 step 238: training loss: 34957.41972071829\n",
      "Epoch 0 step 239: training accuarcy: 0.7182000000000001\n",
      "Epoch 0 step 239: training loss: 35169.91435184982\n",
      "Epoch 0 step 240: training accuarcy: 0.7206\n",
      "Epoch 0 step 240: training loss: 34708.416601433855\n",
      "Epoch 0 step 241: training accuarcy: 0.7219\n",
      "Epoch 0 step 241: training loss: 34897.30972102288\n",
      "Epoch 0 step 242: training accuarcy: 0.7145\n",
      "Epoch 0 step 242: training loss: 34467.124104514805\n",
      "Epoch 0 step 243: training accuarcy: 0.7242000000000001\n",
      "Epoch 0 step 243: training loss: 34294.283932403814\n",
      "Epoch 0 step 244: training accuarcy: 0.7263000000000001\n",
      "Epoch 0 step 244: training loss: 33967.64283128842\n",
      "Epoch 0 step 245: training accuarcy: 0.7353000000000001\n",
      "Epoch 0 step 245: training loss: 34128.75063798379\n",
      "Epoch 0 step 246: training accuarcy: 0.7279\n",
      "Epoch 0 step 246: training loss: 34471.19088489437\n",
      "Epoch 0 step 247: training accuarcy: 0.7248\n",
      "Epoch 0 step 247: training loss: 33934.62552101892\n",
      "Epoch 0 step 248: training accuarcy: 0.7378\n",
      "Epoch 0 step 248: training loss: 33843.055729428794\n",
      "Epoch 0 step 249: training accuarcy: 0.7381000000000001\n",
      "Epoch 0 step 249: training loss: 34483.14589502088\n",
      "Epoch 0 step 250: training accuarcy: 0.7250000000000001\n",
      "Epoch 0 step 250: training loss: 33921.281117615516\n",
      "Epoch 0 step 251: training accuarcy: 0.7295\n",
      "Epoch 0 step 251: training loss: 34146.1995418\n",
      "Epoch 0 step 252: training accuarcy: 0.7222000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 252: training loss: 34278.952959497314\n",
      "Epoch 0 step 253: training accuarcy: 0.7257\n",
      "Epoch 0 step 253: training loss: 33501.52451137232\n",
      "Epoch 0 step 254: training accuarcy: 0.7494000000000001\n",
      "Epoch 0 step 254: training loss: 33864.68539032806\n",
      "Epoch 0 step 255: training accuarcy: 0.7278\n",
      "Epoch 0 step 255: training loss: 34020.38493017855\n",
      "Epoch 0 step 256: training accuarcy: 0.7354\n",
      "Epoch 0 step 256: training loss: 34341.72044269684\n",
      "Epoch 0 step 257: training accuarcy: 0.7245\n",
      "Epoch 0 step 257: training loss: 33554.52557347421\n",
      "Epoch 0 step 258: training accuarcy: 0.7383000000000001\n",
      "Epoch 0 step 258: training loss: 33889.85953733517\n",
      "Epoch 0 step 259: training accuarcy: 0.736\n",
      "Epoch 0 step 259: training loss: 34254.14105188148\n",
      "Epoch 0 step 260: training accuarcy: 0.7288\n",
      "Epoch 0 step 260: training loss: 34180.93643714745\n",
      "Epoch 0 step 261: training accuarcy: 0.7249\n",
      "Epoch 0 step 261: training loss: 33865.30062586484\n",
      "Epoch 0 step 262: training accuarcy: 0.7386\n",
      "Epoch 0 step 262: training loss: 17068.280397425377\n",
      "Epoch 0 step 263: training accuarcy: 0.7035897435897436\n",
      "Epoch 0: train loss 53804.09029535396, train accuarcy 0.6310576796531677\n",
      "Epoch 0: valid loss 32219.746296435827, valid accuarcy 0.7518095374107361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [05:04<10:08, 304.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 33232.64051768092\n",
      "Epoch 1 step 264: training accuarcy: 0.7549\n",
      "Epoch 1 step 264: training loss: 32550.221703703206\n",
      "Epoch 1 step 265: training accuarcy: 0.7577\n",
      "Epoch 1 step 265: training loss: 33045.24654042096\n",
      "Epoch 1 step 266: training accuarcy: 0.756\n",
      "Epoch 1 step 266: training loss: 32446.43821047878\n",
      "Epoch 1 step 267: training accuarcy: 0.763\n",
      "Epoch 1 step 267: training loss: 32815.873558117186\n",
      "Epoch 1 step 268: training accuarcy: 0.7507\n",
      "Epoch 1 step 268: training loss: 33085.9551245544\n",
      "Epoch 1 step 269: training accuarcy: 0.7499\n",
      "Epoch 1 step 269: training loss: 32525.589556083534\n",
      "Epoch 1 step 270: training accuarcy: 0.7584000000000001\n",
      "Epoch 1 step 270: training loss: 33522.12690631198\n",
      "Epoch 1 step 271: training accuarcy: 0.7422000000000001\n",
      "Epoch 1 step 271: training loss: 32712.234827701042\n",
      "Epoch 1 step 272: training accuarcy: 0.7588\n",
      "Epoch 1 step 272: training loss: 32848.03136463337\n",
      "Epoch 1 step 273: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 273: training loss: 32438.109261423837\n",
      "Epoch 1 step 274: training accuarcy: 0.764\n",
      "Epoch 1 step 274: training loss: 32326.68082628503\n",
      "Epoch 1 step 275: training accuarcy: 0.7589\n",
      "Epoch 1 step 275: training loss: 32714.730519611898\n",
      "Epoch 1 step 276: training accuarcy: 0.753\n",
      "Epoch 1 step 276: training loss: 32781.46014026087\n",
      "Epoch 1 step 277: training accuarcy: 0.7547\n",
      "Epoch 1 step 277: training loss: 32710.062666459504\n",
      "Epoch 1 step 278: training accuarcy: 0.7477\n",
      "Epoch 1 step 278: training loss: 32777.24265549321\n",
      "Epoch 1 step 279: training accuarcy: 0.7542\n",
      "Epoch 1 step 279: training loss: 33034.572193548636\n",
      "Epoch 1 step 280: training accuarcy: 0.754\n",
      "Epoch 1 step 280: training loss: 32184.979440527415\n",
      "Epoch 1 step 281: training accuarcy: 0.7667\n",
      "Epoch 1 step 281: training loss: 32500.732316935966\n",
      "Epoch 1 step 282: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 282: training loss: 32801.344761694105\n",
      "Epoch 1 step 283: training accuarcy: 0.7509\n",
      "Epoch 1 step 283: training loss: 32421.99898550783\n",
      "Epoch 1 step 284: training accuarcy: 0.7637\n",
      "Epoch 1 step 284: training loss: 32623.88849869165\n",
      "Epoch 1 step 285: training accuarcy: 0.7511\n",
      "Epoch 1 step 285: training loss: 32625.768870462463\n",
      "Epoch 1 step 286: training accuarcy: 0.7524000000000001\n",
      "Epoch 1 step 286: training loss: 32472.385506414146\n",
      "Epoch 1 step 287: training accuarcy: 0.7586\n",
      "Epoch 1 step 287: training loss: 32410.306529012545\n",
      "Epoch 1 step 288: training accuarcy: 0.7618\n",
      "Epoch 1 step 288: training loss: 32281.626598752933\n",
      "Epoch 1 step 289: training accuarcy: 0.7597\n",
      "Epoch 1 step 289: training loss: 32431.91071622417\n",
      "Epoch 1 step 290: training accuarcy: 0.7461\n",
      "Epoch 1 step 290: training loss: 32415.44152479219\n",
      "Epoch 1 step 291: training accuarcy: 0.755\n",
      "Epoch 1 step 291: training loss: 32133.183084253695\n",
      "Epoch 1 step 292: training accuarcy: 0.7637\n",
      "Epoch 1 step 292: training loss: 32202.37825502399\n",
      "Epoch 1 step 293: training accuarcy: 0.7592\n",
      "Epoch 1 step 293: training loss: 32149.828259541286\n",
      "Epoch 1 step 294: training accuarcy: 0.7603000000000001\n",
      "Epoch 1 step 294: training loss: 32142.236458521013\n",
      "Epoch 1 step 295: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 295: training loss: 32810.51110371607\n",
      "Epoch 1 step 296: training accuarcy: 0.7548\n",
      "Epoch 1 step 296: training loss: 32609.915803462707\n",
      "Epoch 1 step 297: training accuarcy: 0.7519\n",
      "Epoch 1 step 297: training loss: 32398.983750882286\n",
      "Epoch 1 step 298: training accuarcy: 0.7583000000000001\n",
      "Epoch 1 step 298: training loss: 32634.584070323603\n",
      "Epoch 1 step 299: training accuarcy: 0.7559\n",
      "Epoch 1 step 299: training loss: 32095.92971117602\n",
      "Epoch 1 step 300: training accuarcy: 0.7611\n",
      "Epoch 1 step 300: training loss: 32036.828925822756\n",
      "Epoch 1 step 301: training accuarcy: 0.7674000000000001\n",
      "Epoch 1 step 301: training loss: 32357.262575967245\n",
      "Epoch 1 step 302: training accuarcy: 0.753\n",
      "Epoch 1 step 302: training loss: 32653.39880757966\n",
      "Epoch 1 step 303: training accuarcy: 0.7598\n",
      "Epoch 1 step 303: training loss: 31746.143656092292\n",
      "Epoch 1 step 304: training accuarcy: 0.7626000000000001\n",
      "Epoch 1 step 304: training loss: 32887.23617950478\n",
      "Epoch 1 step 305: training accuarcy: 0.7481\n",
      "Epoch 1 step 305: training loss: 32957.94534597535\n",
      "Epoch 1 step 306: training accuarcy: 0.7434000000000001\n",
      "Epoch 1 step 306: training loss: 32125.76097023602\n",
      "Epoch 1 step 307: training accuarcy: 0.7637\n",
      "Epoch 1 step 307: training loss: 31790.33154205657\n",
      "Epoch 1 step 308: training accuarcy: 0.7708\n",
      "Epoch 1 step 308: training loss: 32223.472628073803\n",
      "Epoch 1 step 309: training accuarcy: 0.759\n",
      "Epoch 1 step 309: training loss: 32752.66387678003\n",
      "Epoch 1 step 310: training accuarcy: 0.7487\n",
      "Epoch 1 step 310: training loss: 32492.584041923572\n",
      "Epoch 1 step 311: training accuarcy: 0.7517\n",
      "Epoch 1 step 311: training loss: 32298.572653205832\n",
      "Epoch 1 step 312: training accuarcy: 0.7569\n",
      "Epoch 1 step 312: training loss: 31887.53380642345\n",
      "Epoch 1 step 313: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 313: training loss: 32276.071421960896\n",
      "Epoch 1 step 314: training accuarcy: 0.7558\n",
      "Epoch 1 step 314: training loss: 32239.843426922438\n",
      "Epoch 1 step 315: training accuarcy: 0.755\n",
      "Epoch 1 step 315: training loss: 32307.765107586187\n",
      "Epoch 1 step 316: training accuarcy: 0.754\n",
      "Epoch 1 step 316: training loss: 32348.144215344444\n",
      "Epoch 1 step 317: training accuarcy: 0.7532\n",
      "Epoch 1 step 317: training loss: 32271.32956229514\n",
      "Epoch 1 step 318: training accuarcy: 0.7535000000000001\n",
      "Epoch 1 step 318: training loss: 32246.90469019874\n",
      "Epoch 1 step 319: training accuarcy: 0.7571\n",
      "Epoch 1 step 319: training loss: 31532.897504623757\n",
      "Epoch 1 step 320: training accuarcy: 0.7652\n",
      "Epoch 1 step 320: training loss: 31827.16260635207\n",
      "Epoch 1 step 321: training accuarcy: 0.7619\n",
      "Epoch 1 step 321: training loss: 31956.12593010076\n",
      "Epoch 1 step 322: training accuarcy: 0.7643000000000001\n",
      "Epoch 1 step 322: training loss: 32017.55271222202\n",
      "Epoch 1 step 323: training accuarcy: 0.766\n",
      "Epoch 1 step 323: training loss: 31736.473004625055\n",
      "Epoch 1 step 324: training accuarcy: 0.7647\n",
      "Epoch 1 step 324: training loss: 31709.06598605278\n",
      "Epoch 1 step 325: training accuarcy: 0.7621\n",
      "Epoch 1 step 325: training loss: 31924.006125437798\n",
      "Epoch 1 step 326: training accuarcy: 0.7616\n",
      "Epoch 1 step 326: training loss: 31618.512693530876\n",
      "Epoch 1 step 327: training accuarcy: 0.7711\n",
      "Epoch 1 step 327: training loss: 31713.122684935006\n",
      "Epoch 1 step 328: training accuarcy: 0.7616\n",
      "Epoch 1 step 328: training loss: 32333.604517920678\n",
      "Epoch 1 step 329: training accuarcy: 0.7551\n",
      "Epoch 1 step 329: training loss: 31384.69985166389\n",
      "Epoch 1 step 330: training accuarcy: 0.7686000000000001\n",
      "Epoch 1 step 330: training loss: 31299.53629864839\n",
      "Epoch 1 step 331: training accuarcy: 0.7707\n",
      "Epoch 1 step 331: training loss: 31678.95570333552\n",
      "Epoch 1 step 332: training accuarcy: 0.7681\n",
      "Epoch 1 step 332: training loss: 31771.000761021674\n",
      "Epoch 1 step 333: training accuarcy: 0.7628\n",
      "Epoch 1 step 333: training loss: 31744.03933102204\n",
      "Epoch 1 step 334: training accuarcy: 0.7616\n",
      "Epoch 1 step 334: training loss: 32130.11635831442\n",
      "Epoch 1 step 335: training accuarcy: 0.7551\n",
      "Epoch 1 step 335: training loss: 31866.733389249763\n",
      "Epoch 1 step 336: training accuarcy: 0.7612\n",
      "Epoch 1 step 336: training loss: 32278.305861882473\n",
      "Epoch 1 step 337: training accuarcy: 0.7529\n",
      "Epoch 1 step 337: training loss: 32022.283973499394\n",
      "Epoch 1 step 338: training accuarcy: 0.7542\n",
      "Epoch 1 step 338: training loss: 31666.63837453708\n",
      "Epoch 1 step 339: training accuarcy: 0.7574000000000001\n",
      "Epoch 1 step 339: training loss: 31643.697942267954\n",
      "Epoch 1 step 340: training accuarcy: 0.7603000000000001\n",
      "Epoch 1 step 340: training loss: 31551.943370669123\n",
      "Epoch 1 step 341: training accuarcy: 0.7616\n",
      "Epoch 1 step 341: training loss: 31622.181495846664\n",
      "Epoch 1 step 342: training accuarcy: 0.7619\n",
      "Epoch 1 step 342: training loss: 31760.474914751445\n",
      "Epoch 1 step 343: training accuarcy: 0.7581\n",
      "Epoch 1 step 343: training loss: 31193.410981377125\n",
      "Epoch 1 step 344: training accuarcy: 0.7715000000000001\n",
      "Epoch 1 step 344: training loss: 32015.038855736468\n",
      "Epoch 1 step 345: training accuarcy: 0.7626000000000001\n",
      "Epoch 1 step 345: training loss: 31664.908712051132\n",
      "Epoch 1 step 346: training accuarcy: 0.7588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 346: training loss: 32194.906405058213\n",
      "Epoch 1 step 347: training accuarcy: 0.7536\n",
      "Epoch 1 step 347: training loss: 31560.79408243436\n",
      "Epoch 1 step 348: training accuarcy: 0.7566\n",
      "Epoch 1 step 348: training loss: 31812.411681668676\n",
      "Epoch 1 step 349: training accuarcy: 0.7591\n",
      "Epoch 1 step 349: training loss: 31729.45014434647\n",
      "Epoch 1 step 350: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 350: training loss: 32174.51372787211\n",
      "Epoch 1 step 351: training accuarcy: 0.7611\n",
      "Epoch 1 step 351: training loss: 31700.859085089607\n",
      "Epoch 1 step 352: training accuarcy: 0.761\n",
      "Epoch 1 step 352: training loss: 31729.56458239881\n",
      "Epoch 1 step 353: training accuarcy: 0.7597\n",
      "Epoch 1 step 353: training loss: 31176.129505494973\n",
      "Epoch 1 step 354: training accuarcy: 0.7673\n",
      "Epoch 1 step 354: training loss: 31678.289222722444\n",
      "Epoch 1 step 355: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 355: training loss: 31425.195905576504\n",
      "Epoch 1 step 356: training accuarcy: 0.7606\n",
      "Epoch 1 step 356: training loss: 31710.545952590895\n",
      "Epoch 1 step 357: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 357: training loss: 32208.94566137876\n",
      "Epoch 1 step 358: training accuarcy: 0.7568\n",
      "Epoch 1 step 358: training loss: 31558.03434089813\n",
      "Epoch 1 step 359: training accuarcy: 0.7641\n",
      "Epoch 1 step 359: training loss: 31494.04929353452\n",
      "Epoch 1 step 360: training accuarcy: 0.7598\n",
      "Epoch 1 step 360: training loss: 31711.352987032096\n",
      "Epoch 1 step 361: training accuarcy: 0.7616\n",
      "Epoch 1 step 361: training loss: 31581.124297565824\n",
      "Epoch 1 step 362: training accuarcy: 0.759\n",
      "Epoch 1 step 362: training loss: 31421.383612980997\n",
      "Epoch 1 step 363: training accuarcy: 0.7668\n",
      "Epoch 1 step 363: training loss: 31744.584091739405\n",
      "Epoch 1 step 364: training accuarcy: 0.7523000000000001\n",
      "Epoch 1 step 364: training loss: 31259.674818120893\n",
      "Epoch 1 step 365: training accuarcy: 0.7568\n",
      "Epoch 1 step 365: training loss: 31614.04772607352\n",
      "Epoch 1 step 366: training accuarcy: 0.7598\n",
      "Epoch 1 step 366: training loss: 31524.46534086342\n",
      "Epoch 1 step 367: training accuarcy: 0.7636000000000001\n",
      "Epoch 1 step 367: training loss: 31058.27363574502\n",
      "Epoch 1 step 368: training accuarcy: 0.7726000000000001\n",
      "Epoch 1 step 368: training loss: 31499.721411458868\n",
      "Epoch 1 step 369: training accuarcy: 0.7555000000000001\n",
      "Epoch 1 step 369: training loss: 31010.89614317582\n",
      "Epoch 1 step 370: training accuarcy: 0.7673\n",
      "Epoch 1 step 370: training loss: 31361.096852028666\n",
      "Epoch 1 step 371: training accuarcy: 0.7698\n",
      "Epoch 1 step 371: training loss: 31239.171433093397\n",
      "Epoch 1 step 372: training accuarcy: 0.7632\n",
      "Epoch 1 step 372: training loss: 31953.91437300783\n",
      "Epoch 1 step 373: training accuarcy: 0.7584000000000001\n",
      "Epoch 1 step 373: training loss: 31379.350071597048\n",
      "Epoch 1 step 374: training accuarcy: 0.7698\n",
      "Epoch 1 step 374: training loss: 31106.89145195963\n",
      "Epoch 1 step 375: training accuarcy: 0.766\n",
      "Epoch 1 step 375: training loss: 31394.73361193596\n",
      "Epoch 1 step 376: training accuarcy: 0.7602\n",
      "Epoch 1 step 376: training loss: 31151.93889376422\n",
      "Epoch 1 step 377: training accuarcy: 0.7696000000000001\n",
      "Epoch 1 step 377: training loss: 31725.718980787737\n",
      "Epoch 1 step 378: training accuarcy: 0.7589\n",
      "Epoch 1 step 378: training loss: 31566.270594937203\n",
      "Epoch 1 step 379: training accuarcy: 0.7576\n",
      "Epoch 1 step 379: training loss: 30687.59255314443\n",
      "Epoch 1 step 380: training accuarcy: 0.7735000000000001\n",
      "Epoch 1 step 380: training loss: 31723.811903180675\n",
      "Epoch 1 step 381: training accuarcy: 0.7583000000000001\n",
      "Epoch 1 step 381: training loss: 31377.963611127576\n",
      "Epoch 1 step 382: training accuarcy: 0.7692\n",
      "Epoch 1 step 382: training loss: 31580.93331720585\n",
      "Epoch 1 step 383: training accuarcy: 0.7579\n",
      "Epoch 1 step 383: training loss: 31534.66692024107\n",
      "Epoch 1 step 384: training accuarcy: 0.7602\n",
      "Epoch 1 step 384: training loss: 30678.69974812981\n",
      "Epoch 1 step 385: training accuarcy: 0.7745000000000001\n",
      "Epoch 1 step 385: training loss: 31531.23150585139\n",
      "Epoch 1 step 386: training accuarcy: 0.7557\n",
      "Epoch 1 step 386: training loss: 31377.615544637414\n",
      "Epoch 1 step 387: training accuarcy: 0.76\n",
      "Epoch 1 step 387: training loss: 31314.950385229422\n",
      "Epoch 1 step 388: training accuarcy: 0.7695000000000001\n",
      "Epoch 1 step 388: training loss: 31281.926670015666\n",
      "Epoch 1 step 389: training accuarcy: 0.7599\n",
      "Epoch 1 step 389: training loss: 31356.682704669234\n",
      "Epoch 1 step 390: training accuarcy: 0.7564000000000001\n",
      "Epoch 1 step 390: training loss: 31234.72517855964\n",
      "Epoch 1 step 391: training accuarcy: 0.7671\n",
      "Epoch 1 step 391: training loss: 30852.17194270255\n",
      "Epoch 1 step 392: training accuarcy: 0.7717\n",
      "Epoch 1 step 392: training loss: 31407.587375048268\n",
      "Epoch 1 step 393: training accuarcy: 0.7631\n",
      "Epoch 1 step 393: training loss: 31418.511798527823\n",
      "Epoch 1 step 394: training accuarcy: 0.7586\n",
      "Epoch 1 step 394: training loss: 31514.425379820597\n",
      "Epoch 1 step 395: training accuarcy: 0.7639\n",
      "Epoch 1 step 395: training loss: 31569.229775034466\n",
      "Epoch 1 step 396: training accuarcy: 0.755\n",
      "Epoch 1 step 396: training loss: 31207.870646881536\n",
      "Epoch 1 step 397: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 397: training loss: 31221.885827494727\n",
      "Epoch 1 step 398: training accuarcy: 0.756\n",
      "Epoch 1 step 398: training loss: 31107.00899907674\n",
      "Epoch 1 step 399: training accuarcy: 0.7685000000000001\n",
      "Epoch 1 step 399: training loss: 31052.854619237463\n",
      "Epoch 1 step 400: training accuarcy: 0.764\n",
      "Epoch 1 step 400: training loss: 31125.98208065675\n",
      "Epoch 1 step 401: training accuarcy: 0.7687\n",
      "Epoch 1 step 401: training loss: 31382.03960597923\n",
      "Epoch 1 step 402: training accuarcy: 0.7589\n",
      "Epoch 1 step 402: training loss: 30880.918554051314\n",
      "Epoch 1 step 403: training accuarcy: 0.7715000000000001\n",
      "Epoch 1 step 403: training loss: 31613.873508981036\n",
      "Epoch 1 step 404: training accuarcy: 0.7691\n",
      "Epoch 1 step 404: training loss: 31489.398562282844\n",
      "Epoch 1 step 405: training accuarcy: 0.7617\n",
      "Epoch 1 step 405: training loss: 31195.61334809867\n",
      "Epoch 1 step 406: training accuarcy: 0.7689\n",
      "Epoch 1 step 406: training loss: 31468.537604666984\n",
      "Epoch 1 step 407: training accuarcy: 0.7587\n",
      "Epoch 1 step 407: training loss: 30977.100641356526\n",
      "Epoch 1 step 408: training accuarcy: 0.7625000000000001\n",
      "Epoch 1 step 408: training loss: 30942.77682963838\n",
      "Epoch 1 step 409: training accuarcy: 0.7623000000000001\n",
      "Epoch 1 step 409: training loss: 31305.955361758373\n",
      "Epoch 1 step 410: training accuarcy: 0.7663000000000001\n",
      "Epoch 1 step 410: training loss: 30485.349009571164\n",
      "Epoch 1 step 411: training accuarcy: 0.7746000000000001\n",
      "Epoch 1 step 411: training loss: 30974.52671877022\n",
      "Epoch 1 step 412: training accuarcy: 0.7699\n",
      "Epoch 1 step 412: training loss: 31379.357903761495\n",
      "Epoch 1 step 413: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 413: training loss: 31537.872759823164\n",
      "Epoch 1 step 414: training accuarcy: 0.7648\n",
      "Epoch 1 step 414: training loss: 30565.808989369092\n",
      "Epoch 1 step 415: training accuarcy: 0.7727\n",
      "Epoch 1 step 415: training loss: 31063.562765507428\n",
      "Epoch 1 step 416: training accuarcy: 0.761\n",
      "Epoch 1 step 416: training loss: 30995.418105416487\n",
      "Epoch 1 step 417: training accuarcy: 0.7652\n",
      "Epoch 1 step 417: training loss: 30685.60967283623\n",
      "Epoch 1 step 418: training accuarcy: 0.7811\n",
      "Epoch 1 step 418: training loss: 30746.85361680783\n",
      "Epoch 1 step 419: training accuarcy: 0.773\n",
      "Epoch 1 step 419: training loss: 30786.69762689219\n",
      "Epoch 1 step 420: training accuarcy: 0.779\n",
      "Epoch 1 step 420: training loss: 31221.5897492686\n",
      "Epoch 1 step 421: training accuarcy: 0.7666000000000001\n",
      "Epoch 1 step 421: training loss: 30652.186061481174\n",
      "Epoch 1 step 422: training accuarcy: 0.7698\n",
      "Epoch 1 step 422: training loss: 30726.35386600817\n",
      "Epoch 1 step 423: training accuarcy: 0.7692\n",
      "Epoch 1 step 423: training loss: 31182.487390992454\n",
      "Epoch 1 step 424: training accuarcy: 0.761\n",
      "Epoch 1 step 424: training loss: 31205.40822959424\n",
      "Epoch 1 step 425: training accuarcy: 0.7578\n",
      "Epoch 1 step 425: training loss: 30845.759116453595\n",
      "Epoch 1 step 426: training accuarcy: 0.7799\n",
      "Epoch 1 step 426: training loss: 31712.512698770137\n",
      "Epoch 1 step 427: training accuarcy: 0.7479\n",
      "Epoch 1 step 427: training loss: 30511.92565827003\n",
      "Epoch 1 step 428: training accuarcy: 0.7715000000000001\n",
      "Epoch 1 step 428: training loss: 30776.138689504784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 429: training accuarcy: 0.7694000000000001\n",
      "Epoch 1 step 429: training loss: 31576.346707594264\n",
      "Epoch 1 step 430: training accuarcy: 0.7592\n",
      "Epoch 1 step 430: training loss: 31232.155290046936\n",
      "Epoch 1 step 431: training accuarcy: 0.7638\n",
      "Epoch 1 step 431: training loss: 31054.72281708138\n",
      "Epoch 1 step 432: training accuarcy: 0.7674000000000001\n",
      "Epoch 1 step 432: training loss: 30774.843264047104\n",
      "Epoch 1 step 433: training accuarcy: 0.7667\n",
      "Epoch 1 step 433: training loss: 30815.03723026877\n",
      "Epoch 1 step 434: training accuarcy: 0.7647\n",
      "Epoch 1 step 434: training loss: 30262.529443554227\n",
      "Epoch 1 step 435: training accuarcy: 0.7756000000000001\n",
      "Epoch 1 step 435: training loss: 30784.972607255135\n",
      "Epoch 1 step 436: training accuarcy: 0.769\n",
      "Epoch 1 step 436: training loss: 31040.70105093181\n",
      "Epoch 1 step 437: training accuarcy: 0.7639\n",
      "Epoch 1 step 437: training loss: 30499.637345536426\n",
      "Epoch 1 step 438: training accuarcy: 0.765\n",
      "Epoch 1 step 438: training loss: 30841.3660873036\n",
      "Epoch 1 step 439: training accuarcy: 0.7707\n",
      "Epoch 1 step 439: training loss: 30267.391535003673\n",
      "Epoch 1 step 440: training accuarcy: 0.7847000000000001\n",
      "Epoch 1 step 440: training loss: 30382.236977774584\n",
      "Epoch 1 step 441: training accuarcy: 0.7723\n",
      "Epoch 1 step 441: training loss: 31159.844297741256\n",
      "Epoch 1 step 442: training accuarcy: 0.7679\n",
      "Epoch 1 step 442: training loss: 30329.59088668149\n",
      "Epoch 1 step 443: training accuarcy: 0.769\n",
      "Epoch 1 step 443: training loss: 30723.102829506213\n",
      "Epoch 1 step 444: training accuarcy: 0.7641\n",
      "Epoch 1 step 444: training loss: 30532.69233711214\n",
      "Epoch 1 step 445: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 445: training loss: 30699.779613326173\n",
      "Epoch 1 step 446: training accuarcy: 0.7697\n",
      "Epoch 1 step 446: training loss: 30280.358940608865\n",
      "Epoch 1 step 447: training accuarcy: 0.78\n",
      "Epoch 1 step 447: training loss: 30392.028768327535\n",
      "Epoch 1 step 448: training accuarcy: 0.7755000000000001\n",
      "Epoch 1 step 448: training loss: 30695.219019504846\n",
      "Epoch 1 step 449: training accuarcy: 0.7731\n",
      "Epoch 1 step 449: training loss: 30196.882568575984\n",
      "Epoch 1 step 450: training accuarcy: 0.7729\n",
      "Epoch 1 step 450: training loss: 30788.94533554171\n",
      "Epoch 1 step 451: training accuarcy: 0.7651\n",
      "Epoch 1 step 451: training loss: 30714.31262146528\n",
      "Epoch 1 step 452: training accuarcy: 0.7708\n",
      "Epoch 1 step 452: training loss: 30532.429557966338\n",
      "Epoch 1 step 453: training accuarcy: 0.7697\n",
      "Epoch 1 step 453: training loss: 30652.193955050803\n",
      "Epoch 1 step 454: training accuarcy: 0.7677\n",
      "Epoch 1 step 454: training loss: 30917.751680833706\n",
      "Epoch 1 step 455: training accuarcy: 0.7669\n",
      "Epoch 1 step 455: training loss: 30818.7373468453\n",
      "Epoch 1 step 456: training accuarcy: 0.7648\n",
      "Epoch 1 step 456: training loss: 30577.233767858474\n",
      "Epoch 1 step 457: training accuarcy: 0.769\n",
      "Epoch 1 step 457: training loss: 30444.12135944697\n",
      "Epoch 1 step 458: training accuarcy: 0.7731\n",
      "Epoch 1 step 458: training loss: 30409.2201706168\n",
      "Epoch 1 step 459: training accuarcy: 0.7759\n",
      "Epoch 1 step 459: training loss: 30556.886715626984\n",
      "Epoch 1 step 460: training accuarcy: 0.7683\n",
      "Epoch 1 step 460: training loss: 30386.63804449131\n",
      "Epoch 1 step 461: training accuarcy: 0.7747\n",
      "Epoch 1 step 461: training loss: 30305.148046035567\n",
      "Epoch 1 step 462: training accuarcy: 0.7686000000000001\n",
      "Epoch 1 step 462: training loss: 30348.922135911384\n",
      "Epoch 1 step 463: training accuarcy: 0.7763\n",
      "Epoch 1 step 463: training loss: 30509.21013050514\n",
      "Epoch 1 step 464: training accuarcy: 0.7663000000000001\n",
      "Epoch 1 step 464: training loss: 30587.5847518721\n",
      "Epoch 1 step 465: training accuarcy: 0.762\n",
      "Epoch 1 step 465: training loss: 30230.99828171007\n",
      "Epoch 1 step 466: training accuarcy: 0.7726000000000001\n",
      "Epoch 1 step 466: training loss: 30536.249750471296\n",
      "Epoch 1 step 467: training accuarcy: 0.7787000000000001\n",
      "Epoch 1 step 467: training loss: 30686.588086274845\n",
      "Epoch 1 step 468: training accuarcy: 0.7679\n",
      "Epoch 1 step 468: training loss: 30002.03097074968\n",
      "Epoch 1 step 469: training accuarcy: 0.7753\n",
      "Epoch 1 step 469: training loss: 30392.535026608573\n",
      "Epoch 1 step 470: training accuarcy: 0.7729\n",
      "Epoch 1 step 470: training loss: 30836.90114340756\n",
      "Epoch 1 step 471: training accuarcy: 0.7587\n",
      "Epoch 1 step 471: training loss: 30082.809883708873\n",
      "Epoch 1 step 472: training accuarcy: 0.7735000000000001\n",
      "Epoch 1 step 472: training loss: 30396.83523516943\n",
      "Epoch 1 step 473: training accuarcy: 0.7719\n",
      "Epoch 1 step 473: training loss: 30771.28427944635\n",
      "Epoch 1 step 474: training accuarcy: 0.7669\n",
      "Epoch 1 step 474: training loss: 30882.27545266177\n",
      "Epoch 1 step 475: training accuarcy: 0.7578\n",
      "Epoch 1 step 475: training loss: 30729.180178959883\n",
      "Epoch 1 step 476: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 476: training loss: 31417.09069917605\n",
      "Epoch 1 step 477: training accuarcy: 0.7589\n",
      "Epoch 1 step 477: training loss: 29985.772560471418\n",
      "Epoch 1 step 478: training accuarcy: 0.7781\n",
      "Epoch 1 step 478: training loss: 30428.630019516626\n",
      "Epoch 1 step 479: training accuarcy: 0.7644000000000001\n",
      "Epoch 1 step 479: training loss: 30021.358985297276\n",
      "Epoch 1 step 480: training accuarcy: 0.7839\n",
      "Epoch 1 step 480: training loss: 30518.129797998437\n",
      "Epoch 1 step 481: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 481: training loss: 30102.619016039396\n",
      "Epoch 1 step 482: training accuarcy: 0.7749\n",
      "Epoch 1 step 482: training loss: 30640.631277020842\n",
      "Epoch 1 step 483: training accuarcy: 0.7672\n",
      "Epoch 1 step 483: training loss: 30582.081468472497\n",
      "Epoch 1 step 484: training accuarcy: 0.7689\n",
      "Epoch 1 step 484: training loss: 30712.367867200443\n",
      "Epoch 1 step 485: training accuarcy: 0.7631\n",
      "Epoch 1 step 485: training loss: 30522.13727158122\n",
      "Epoch 1 step 486: training accuarcy: 0.7702\n",
      "Epoch 1 step 486: training loss: 30415.34179197301\n",
      "Epoch 1 step 487: training accuarcy: 0.7729\n",
      "Epoch 1 step 487: training loss: 30627.437591016766\n",
      "Epoch 1 step 488: training accuarcy: 0.769\n",
      "Epoch 1 step 488: training loss: 30259.193470072343\n",
      "Epoch 1 step 489: training accuarcy: 0.773\n",
      "Epoch 1 step 489: training loss: 29826.206431885632\n",
      "Epoch 1 step 490: training accuarcy: 0.7752\n",
      "Epoch 1 step 490: training loss: 30373.234578079144\n",
      "Epoch 1 step 491: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 491: training loss: 30076.757735048155\n",
      "Epoch 1 step 492: training accuarcy: 0.78\n",
      "Epoch 1 step 492: training loss: 30278.694868233295\n",
      "Epoch 1 step 493: training accuarcy: 0.7769\n",
      "Epoch 1 step 493: training loss: 30169.81773775445\n",
      "Epoch 1 step 494: training accuarcy: 0.7686000000000001\n",
      "Epoch 1 step 494: training loss: 29864.934555699492\n",
      "Epoch 1 step 495: training accuarcy: 0.7817000000000001\n",
      "Epoch 1 step 495: training loss: 30225.517097026466\n",
      "Epoch 1 step 496: training accuarcy: 0.7738\n",
      "Epoch 1 step 496: training loss: 30258.10792005444\n",
      "Epoch 1 step 497: training accuarcy: 0.7754000000000001\n",
      "Epoch 1 step 497: training loss: 29890.021164098733\n",
      "Epoch 1 step 498: training accuarcy: 0.786\n",
      "Epoch 1 step 498: training loss: 30290.506509567425\n",
      "Epoch 1 step 499: training accuarcy: 0.77\n",
      "Epoch 1 step 499: training loss: 29937.46273652114\n",
      "Epoch 1 step 500: training accuarcy: 0.7759\n",
      "Epoch 1 step 500: training loss: 31125.13714004818\n",
      "Epoch 1 step 501: training accuarcy: 0.7634000000000001\n",
      "Epoch 1 step 501: training loss: 30054.354080233024\n",
      "Epoch 1 step 502: training accuarcy: 0.7732\n",
      "Epoch 1 step 502: training loss: 30558.47607828644\n",
      "Epoch 1 step 503: training accuarcy: 0.7659\n",
      "Epoch 1 step 503: training loss: 29755.516404817714\n",
      "Epoch 1 step 504: training accuarcy: 0.7791\n",
      "Epoch 1 step 504: training loss: 30007.321599230847\n",
      "Epoch 1 step 505: training accuarcy: 0.7799\n",
      "Epoch 1 step 505: training loss: 30031.21894262087\n",
      "Epoch 1 step 506: training accuarcy: 0.7761\n",
      "Epoch 1 step 506: training loss: 29874.64737304374\n",
      "Epoch 1 step 507: training accuarcy: 0.777\n",
      "Epoch 1 step 507: training loss: 30332.660131711455\n",
      "Epoch 1 step 508: training accuarcy: 0.7737\n",
      "Epoch 1 step 508: training loss: 30321.42983256355\n",
      "Epoch 1 step 509: training accuarcy: 0.7677\n",
      "Epoch 1 step 509: training loss: 30389.08744728059\n",
      "Epoch 1 step 510: training accuarcy: 0.7726000000000001\n",
      "Epoch 1 step 510: training loss: 30486.7307010176\n",
      "Epoch 1 step 511: training accuarcy: 0.7622\n",
      "Epoch 1 step 511: training loss: 30167.813032167265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 512: training accuarcy: 0.7677\n",
      "Epoch 1 step 512: training loss: 30231.842841948674\n",
      "Epoch 1 step 513: training accuarcy: 0.7727\n",
      "Epoch 1 step 513: training loss: 30298.75123263509\n",
      "Epoch 1 step 514: training accuarcy: 0.7773\n",
      "Epoch 1 step 514: training loss: 30291.555082274008\n",
      "Epoch 1 step 515: training accuarcy: 0.7682\n",
      "Epoch 1 step 515: training loss: 29753.810008661727\n",
      "Epoch 1 step 516: training accuarcy: 0.7805000000000001\n",
      "Epoch 1 step 516: training loss: 30225.881385176515\n",
      "Epoch 1 step 517: training accuarcy: 0.7747\n",
      "Epoch 1 step 517: training loss: 29906.184142027236\n",
      "Epoch 1 step 518: training accuarcy: 0.774\n",
      "Epoch 1 step 518: training loss: 29605.025573338407\n",
      "Epoch 1 step 519: training accuarcy: 0.7803\n",
      "Epoch 1 step 519: training loss: 30821.358642994215\n",
      "Epoch 1 step 520: training accuarcy: 0.7602\n",
      "Epoch 1 step 520: training loss: 29736.963569083877\n",
      "Epoch 1 step 521: training accuarcy: 0.7771\n",
      "Epoch 1 step 521: training loss: 29822.423405976493\n",
      "Epoch 1 step 522: training accuarcy: 0.7794000000000001\n",
      "Epoch 1 step 522: training loss: 30586.114514511897\n",
      "Epoch 1 step 523: training accuarcy: 0.7671\n",
      "Epoch 1 step 523: training loss: 30009.887945643848\n",
      "Epoch 1 step 524: training accuarcy: 0.7738\n",
      "Epoch 1 step 524: training loss: 30476.615864347776\n",
      "Epoch 1 step 525: training accuarcy: 0.7753\n",
      "Epoch 1 step 525: training loss: 13834.57346807209\n",
      "Epoch 1 step 526: training accuarcy: 0.7774358974358975\n",
      "Epoch 1: train loss 31221.192366396055, train accuarcy 0.7492716312408447\n",
      "Epoch 1: valid loss 28757.06106598878, valid accuarcy 0.7869622111320496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [10:13<05:05, 305.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 29444.827389827682\n",
      "Epoch 2 step 527: training accuarcy: 0.7862\n",
      "Epoch 2 step 527: training loss: 28523.125714488488\n",
      "Epoch 2 step 528: training accuarcy: 0.8022\n",
      "Epoch 2 step 528: training loss: 29318.442319866088\n",
      "Epoch 2 step 529: training accuarcy: 0.7877000000000001\n",
      "Epoch 2 step 529: training loss: 29288.732426444403\n",
      "Epoch 2 step 530: training accuarcy: 0.7866000000000001\n",
      "Epoch 2 step 530: training loss: 28845.008535668574\n",
      "Epoch 2 step 531: training accuarcy: 0.7902\n",
      "Epoch 2 step 531: training loss: 29109.83316338543\n",
      "Epoch 2 step 532: training accuarcy: 0.7919\n",
      "Epoch 2 step 532: training loss: 29101.957448656427\n",
      "Epoch 2 step 533: training accuarcy: 0.79\n",
      "Epoch 2 step 533: training loss: 29421.874596014342\n",
      "Epoch 2 step 534: training accuarcy: 0.7853\n",
      "Epoch 2 step 534: training loss: 29064.878122278165\n",
      "Epoch 2 step 535: training accuarcy: 0.7874\n",
      "Epoch 2 step 535: training loss: 29181.4835376725\n",
      "Epoch 2 step 536: training accuarcy: 0.7942\n",
      "Epoch 2 step 536: training loss: 29453.53374325736\n",
      "Epoch 2 step 537: training accuarcy: 0.7805000000000001\n",
      "Epoch 2 step 537: training loss: 29878.02809153224\n",
      "Epoch 2 step 538: training accuarcy: 0.7834\n",
      "Epoch 2 step 538: training loss: 29364.450210509647\n",
      "Epoch 2 step 539: training accuarcy: 0.7886000000000001\n",
      "Epoch 2 step 539: training loss: 29063.07266618019\n",
      "Epoch 2 step 540: training accuarcy: 0.7896000000000001\n",
      "Epoch 2 step 540: training loss: 29185.05185825834\n",
      "Epoch 2 step 541: training accuarcy: 0.7881\n",
      "Epoch 2 step 541: training loss: 29090.051557095296\n",
      "Epoch 2 step 542: training accuarcy: 0.7922\n",
      "Epoch 2 step 542: training loss: 29402.42274992395\n",
      "Epoch 2 step 543: training accuarcy: 0.7823\n",
      "Epoch 2 step 543: training loss: 29239.86144547651\n",
      "Epoch 2 step 544: training accuarcy: 0.7948000000000001\n",
      "Epoch 2 step 544: training loss: 29822.61274772259\n",
      "Epoch 2 step 545: training accuarcy: 0.7758\n",
      "Epoch 2 step 545: training loss: 29580.487478265546\n",
      "Epoch 2 step 546: training accuarcy: 0.7872\n",
      "Epoch 2 step 546: training loss: 29696.443725420406\n",
      "Epoch 2 step 547: training accuarcy: 0.7811\n",
      "Epoch 2 step 547: training loss: 29324.56057661057\n",
      "Epoch 2 step 548: training accuarcy: 0.7854\n",
      "Epoch 2 step 548: training loss: 29409.46637440659\n",
      "Epoch 2 step 549: training accuarcy: 0.7813\n",
      "Epoch 2 step 549: training loss: 29072.908467389243\n",
      "Epoch 2 step 550: training accuarcy: 0.7909\n",
      "Epoch 2 step 550: training loss: 28985.58704631179\n",
      "Epoch 2 step 551: training accuarcy: 0.788\n",
      "Epoch 2 step 551: training loss: 29386.975939971904\n",
      "Epoch 2 step 552: training accuarcy: 0.7821\n",
      "Epoch 2 step 552: training loss: 29699.026946826583\n",
      "Epoch 2 step 553: training accuarcy: 0.7777000000000001\n",
      "Epoch 2 step 553: training loss: 29766.73525826648\n",
      "Epoch 2 step 554: training accuarcy: 0.783\n",
      "Epoch 2 step 554: training loss: 29514.416579993056\n",
      "Epoch 2 step 555: training accuarcy: 0.7902\n",
      "Epoch 2 step 555: training loss: 29359.54013128171\n",
      "Epoch 2 step 556: training accuarcy: 0.7856000000000001\n",
      "Epoch 2 step 556: training loss: 29332.708258187016\n",
      "Epoch 2 step 557: training accuarcy: 0.786\n",
      "Epoch 2 step 557: training loss: 28989.724934936043\n",
      "Epoch 2 step 558: training accuarcy: 0.7895000000000001\n",
      "Epoch 2 step 558: training loss: 28970.488273861185\n",
      "Epoch 2 step 559: training accuarcy: 0.7955\n",
      "Epoch 2 step 559: training loss: 29317.68700580615\n",
      "Epoch 2 step 560: training accuarcy: 0.7942\n",
      "Epoch 2 step 560: training loss: 29525.59420805888\n",
      "Epoch 2 step 561: training accuarcy: 0.7822\n",
      "Epoch 2 step 561: training loss: 29077.119154715725\n",
      "Epoch 2 step 562: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 562: training loss: 29037.698445381186\n",
      "Epoch 2 step 563: training accuarcy: 0.7945\n",
      "Epoch 2 step 563: training loss: 29194.61885738949\n",
      "Epoch 2 step 564: training accuarcy: 0.7905000000000001\n",
      "Epoch 2 step 564: training loss: 29203.167281447048\n",
      "Epoch 2 step 565: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 565: training loss: 28902.256859953817\n",
      "Epoch 2 step 566: training accuarcy: 0.7896000000000001\n",
      "Epoch 2 step 566: training loss: 29818.504248136556\n",
      "Epoch 2 step 567: training accuarcy: 0.7768\n",
      "Epoch 2 step 567: training loss: 29163.08278269532\n",
      "Epoch 2 step 568: training accuarcy: 0.7863\n",
      "Epoch 2 step 568: training loss: 28862.51744139387\n",
      "Epoch 2 step 569: training accuarcy: 0.7994\n",
      "Epoch 2 step 569: training loss: 29266.87527688684\n",
      "Epoch 2 step 570: training accuarcy: 0.7854\n",
      "Epoch 2 step 570: training loss: 29590.033780018297\n",
      "Epoch 2 step 571: training accuarcy: 0.7912\n",
      "Epoch 2 step 571: training loss: 29263.30625840515\n",
      "Epoch 2 step 572: training accuarcy: 0.7814\n",
      "Epoch 2 step 572: training loss: 29318.87829814509\n",
      "Epoch 2 step 573: training accuarcy: 0.7803\n",
      "Epoch 2 step 573: training loss: 29823.69713776901\n",
      "Epoch 2 step 574: training accuarcy: 0.7783\n",
      "Epoch 2 step 574: training loss: 29174.78097912875\n",
      "Epoch 2 step 575: training accuarcy: 0.7898000000000001\n",
      "Epoch 2 step 575: training loss: 29080.14253402895\n",
      "Epoch 2 step 576: training accuarcy: 0.7889\n",
      "Epoch 2 step 576: training loss: 29166.45118001599\n",
      "Epoch 2 step 577: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 577: training loss: 29869.5542285227\n",
      "Epoch 2 step 578: training accuarcy: 0.7785000000000001\n",
      "Epoch 2 step 578: training loss: 29309.871692875466\n",
      "Epoch 2 step 579: training accuarcy: 0.7818\n",
      "Epoch 2 step 579: training loss: 29017.131526877478\n",
      "Epoch 2 step 580: training accuarcy: 0.7906000000000001\n",
      "Epoch 2 step 580: training loss: 29242.479782615395\n",
      "Epoch 2 step 581: training accuarcy: 0.7818\n",
      "Epoch 2 step 581: training loss: 29468.993074662445\n",
      "Epoch 2 step 582: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 582: training loss: 29358.616156332006\n",
      "Epoch 2 step 583: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 583: training loss: 29157.314818471612\n",
      "Epoch 2 step 584: training accuarcy: 0.7853\n",
      "Epoch 2 step 584: training loss: 29425.503962265822\n",
      "Epoch 2 step 585: training accuarcy: 0.7779\n",
      "Epoch 2 step 585: training loss: 28944.832611514812\n",
      "Epoch 2 step 586: training accuarcy: 0.7811\n",
      "Epoch 2 step 586: training loss: 29672.840451559314\n",
      "Epoch 2 step 587: training accuarcy: 0.7829\n",
      "Epoch 2 step 587: training loss: 29568.8348934746\n",
      "Epoch 2 step 588: training accuarcy: 0.7797000000000001\n",
      "Epoch 2 step 588: training loss: 29555.95698196651\n",
      "Epoch 2 step 589: training accuarcy: 0.7779\n",
      "Epoch 2 step 589: training loss: 28955.891374639265\n",
      "Epoch 2 step 590: training accuarcy: 0.7913\n",
      "Epoch 2 step 590: training loss: 29417.595100384737\n",
      "Epoch 2 step 591: training accuarcy: 0.7838\n",
      "Epoch 2 step 591: training loss: 29026.675532994454\n",
      "Epoch 2 step 592: training accuarcy: 0.7852\n",
      "Epoch 2 step 592: training loss: 28817.48163878658\n",
      "Epoch 2 step 593: training accuarcy: 0.7861\n",
      "Epoch 2 step 593: training loss: 28825.52660954916\n",
      "Epoch 2 step 594: training accuarcy: 0.7944\n",
      "Epoch 2 step 594: training loss: 29199.067921380036\n",
      "Epoch 2 step 595: training accuarcy: 0.7803\n",
      "Epoch 2 step 595: training loss: 29311.894953481373\n",
      "Epoch 2 step 596: training accuarcy: 0.7812\n",
      "Epoch 2 step 596: training loss: 29041.751685752275\n",
      "Epoch 2 step 597: training accuarcy: 0.7841\n",
      "Epoch 2 step 597: training loss: 29458.40383998506\n",
      "Epoch 2 step 598: training accuarcy: 0.79\n",
      "Epoch 2 step 598: training loss: 29262.51077433983\n",
      "Epoch 2 step 599: training accuarcy: 0.7744000000000001\n",
      "Epoch 2 step 599: training loss: 28820.153754717518\n",
      "Epoch 2 step 600: training accuarcy: 0.7882\n",
      "Epoch 2 step 600: training loss: 29630.482056725687\n",
      "Epoch 2 step 601: training accuarcy: 0.7763\n",
      "Epoch 2 step 601: training loss: 29381.401671161304\n",
      "Epoch 2 step 602: training accuarcy: 0.7834\n",
      "Epoch 2 step 602: training loss: 29078.169890520305\n",
      "Epoch 2 step 603: training accuarcy: 0.7849\n",
      "Epoch 2 step 603: training loss: 29152.520053204484\n",
      "Epoch 2 step 604: training accuarcy: 0.7845000000000001\n",
      "Epoch 2 step 604: training loss: 29613.227843468852\n",
      "Epoch 2 step 605: training accuarcy: 0.7785000000000001\n",
      "Epoch 2 step 605: training loss: 29207.367798351494\n",
      "Epoch 2 step 606: training accuarcy: 0.7871\n",
      "Epoch 2 step 606: training loss: 29047.818569122064\n",
      "Epoch 2 step 607: training accuarcy: 0.7833\n",
      "Epoch 2 step 607: training loss: 29749.007707717854\n",
      "Epoch 2 step 608: training accuarcy: 0.7689\n",
      "Epoch 2 step 608: training loss: 29166.480280921758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 609: training accuarcy: 0.7785000000000001\n",
      "Epoch 2 step 609: training loss: 29393.46329736679\n",
      "Epoch 2 step 610: training accuarcy: 0.7835000000000001\n",
      "Epoch 2 step 610: training loss: 29467.41881039033\n",
      "Epoch 2 step 611: training accuarcy: 0.778\n",
      "Epoch 2 step 611: training loss: 29608.2089999082\n",
      "Epoch 2 step 612: training accuarcy: 0.7785000000000001\n",
      "Epoch 2 step 612: training loss: 29824.281984113375\n",
      "Epoch 2 step 613: training accuarcy: 0.7704000000000001\n",
      "Epoch 2 step 613: training loss: 28741.93647236604\n",
      "Epoch 2 step 614: training accuarcy: 0.786\n",
      "Epoch 2 step 614: training loss: 29166.909862043896\n",
      "Epoch 2 step 615: training accuarcy: 0.7808\n",
      "Epoch 2 step 615: training loss: 29332.23251420685\n",
      "Epoch 2 step 616: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 616: training loss: 29218.20570179459\n",
      "Epoch 2 step 617: training accuarcy: 0.7796000000000001\n",
      "Epoch 2 step 617: training loss: 29429.351695108195\n",
      "Epoch 2 step 618: training accuarcy: 0.7812\n",
      "Epoch 2 step 618: training loss: 28766.813896309814\n",
      "Epoch 2 step 619: training accuarcy: 0.7888000000000001\n",
      "Epoch 2 step 619: training loss: 29345.246333567924\n",
      "Epoch 2 step 620: training accuarcy: 0.7878000000000001\n",
      "Epoch 2 step 620: training loss: 29123.18777994977\n",
      "Epoch 2 step 621: training accuarcy: 0.7844\n",
      "Epoch 2 step 621: training loss: 29523.13598342963\n",
      "Epoch 2 step 622: training accuarcy: 0.776\n",
      "Epoch 2 step 622: training loss: 28523.815924780592\n",
      "Epoch 2 step 623: training accuarcy: 0.7991\n",
      "Epoch 2 step 623: training loss: 29010.160017725913\n",
      "Epoch 2 step 624: training accuarcy: 0.7834\n",
      "Epoch 2 step 624: training loss: 29601.254947516758\n",
      "Epoch 2 step 625: training accuarcy: 0.7771\n",
      "Epoch 2 step 625: training loss: 29261.51096835529\n",
      "Epoch 2 step 626: training accuarcy: 0.7859\n",
      "Epoch 2 step 626: training loss: 29092.091829184043\n",
      "Epoch 2 step 627: training accuarcy: 0.7807000000000001\n",
      "Epoch 2 step 627: training loss: 28748.515282539407\n",
      "Epoch 2 step 628: training accuarcy: 0.7834\n",
      "Epoch 2 step 628: training loss: 29284.23322661876\n",
      "Epoch 2 step 629: training accuarcy: 0.7778\n",
      "Epoch 2 step 629: training loss: 29229.571548893702\n",
      "Epoch 2 step 630: training accuarcy: 0.7862\n",
      "Epoch 2 step 630: training loss: 28663.48667079176\n",
      "Epoch 2 step 631: training accuarcy: 0.7908000000000001\n",
      "Epoch 2 step 631: training loss: 29009.16800230555\n",
      "Epoch 2 step 632: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 632: training loss: 29691.22086094047\n",
      "Epoch 2 step 633: training accuarcy: 0.7852\n",
      "Epoch 2 step 633: training loss: 28848.308027448755\n",
      "Epoch 2 step 634: training accuarcy: 0.7825000000000001\n",
      "Epoch 2 step 634: training loss: 28463.432356462563\n",
      "Epoch 2 step 635: training accuarcy: 0.7854\n",
      "Epoch 2 step 635: training loss: 29442.638063916904\n",
      "Epoch 2 step 636: training accuarcy: 0.78\n",
      "Epoch 2 step 636: training loss: 28938.662051492196\n",
      "Epoch 2 step 637: training accuarcy: 0.7854\n",
      "Epoch 2 step 637: training loss: 29134.0690355818\n",
      "Epoch 2 step 638: training accuarcy: 0.7763\n",
      "Epoch 2 step 638: training loss: 29751.67044448101\n",
      "Epoch 2 step 639: training accuarcy: 0.7686000000000001\n",
      "Epoch 2 step 639: training loss: 29325.951542394425\n",
      "Epoch 2 step 640: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 640: training loss: 28682.444494799878\n",
      "Epoch 2 step 641: training accuarcy: 0.7869\n",
      "Epoch 2 step 641: training loss: 29340.043439750203\n",
      "Epoch 2 step 642: training accuarcy: 0.7828\n",
      "Epoch 2 step 642: training loss: 28850.207028881585\n",
      "Epoch 2 step 643: training accuarcy: 0.7891\n",
      "Epoch 2 step 643: training loss: 29330.425320176044\n",
      "Epoch 2 step 644: training accuarcy: 0.7803\n",
      "Epoch 2 step 644: training loss: 28403.428877573322\n",
      "Epoch 2 step 645: training accuarcy: 0.7976000000000001\n",
      "Epoch 2 step 645: training loss: 30027.22205800142\n",
      "Epoch 2 step 646: training accuarcy: 0.7689\n",
      "Epoch 2 step 646: training loss: 28844.761903004503\n",
      "Epoch 2 step 647: training accuarcy: 0.7883\n",
      "Epoch 2 step 647: training loss: 29344.495084766102\n",
      "Epoch 2 step 648: training accuarcy: 0.7725000000000001\n",
      "Epoch 2 step 648: training loss: 29232.597810307798\n",
      "Epoch 2 step 649: training accuarcy: 0.7821\n",
      "Epoch 2 step 649: training loss: 29208.170726144996\n",
      "Epoch 2 step 650: training accuarcy: 0.7776000000000001\n",
      "Epoch 2 step 650: training loss: 29788.36120915086\n",
      "Epoch 2 step 651: training accuarcy: 0.7675000000000001\n",
      "Epoch 2 step 651: training loss: 29079.15210815913\n",
      "Epoch 2 step 652: training accuarcy: 0.7794000000000001\n",
      "Epoch 2 step 652: training loss: 29069.55006978442\n",
      "Epoch 2 step 653: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 653: training loss: 29004.526924429847\n",
      "Epoch 2 step 654: training accuarcy: 0.7892\n",
      "Epoch 2 step 654: training loss: 28712.149169071923\n",
      "Epoch 2 step 655: training accuarcy: 0.7892\n",
      "Epoch 2 step 655: training loss: 29203.28988894337\n",
      "Epoch 2 step 656: training accuarcy: 0.7833\n",
      "Epoch 2 step 656: training loss: 29209.852577134363\n",
      "Epoch 2 step 657: training accuarcy: 0.7817000000000001\n",
      "Epoch 2 step 657: training loss: 28842.762337529726\n",
      "Epoch 2 step 658: training accuarcy: 0.7908000000000001\n",
      "Epoch 2 step 658: training loss: 29129.45809284199\n",
      "Epoch 2 step 659: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 659: training loss: 29032.441870628896\n",
      "Epoch 2 step 660: training accuarcy: 0.7866000000000001\n",
      "Epoch 2 step 660: training loss: 29496.666599277392\n",
      "Epoch 2 step 661: training accuarcy: 0.7792\n",
      "Epoch 2 step 661: training loss: 28494.627662823983\n",
      "Epoch 2 step 662: training accuarcy: 0.7959\n",
      "Epoch 2 step 662: training loss: 28626.610918150454\n",
      "Epoch 2 step 663: training accuarcy: 0.7911\n",
      "Epoch 2 step 663: training loss: 28937.099985190333\n",
      "Epoch 2 step 664: training accuarcy: 0.7895000000000001\n",
      "Epoch 2 step 664: training loss: 29310.639193375955\n",
      "Epoch 2 step 665: training accuarcy: 0.7807000000000001\n",
      "Epoch 2 step 665: training loss: 29022.926880787934\n",
      "Epoch 2 step 666: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 666: training loss: 29329.770623871314\n",
      "Epoch 2 step 667: training accuarcy: 0.776\n",
      "Epoch 2 step 667: training loss: 28974.332612925107\n",
      "Epoch 2 step 668: training accuarcy: 0.7859\n",
      "Epoch 2 step 668: training loss: 28558.45295577674\n",
      "Epoch 2 step 669: training accuarcy: 0.7758\n",
      "Epoch 2 step 669: training loss: 29154.645394302726\n",
      "Epoch 2 step 670: training accuarcy: 0.7892\n",
      "Epoch 2 step 670: training loss: 28769.11758166853\n",
      "Epoch 2 step 671: training accuarcy: 0.7877000000000001\n",
      "Epoch 2 step 671: training loss: 28715.042710195063\n",
      "Epoch 2 step 672: training accuarcy: 0.791\n",
      "Epoch 2 step 672: training loss: 29260.44041861438\n",
      "Epoch 2 step 673: training accuarcy: 0.7728\n",
      "Epoch 2 step 673: training loss: 28900.987905804897\n",
      "Epoch 2 step 674: training accuarcy: 0.7795000000000001\n",
      "Epoch 2 step 674: training loss: 29005.514637805547\n",
      "Epoch 2 step 675: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 675: training loss: 28928.442803494974\n",
      "Epoch 2 step 676: training accuarcy: 0.7821\n",
      "Epoch 2 step 676: training loss: 28692.709350711124\n",
      "Epoch 2 step 677: training accuarcy: 0.794\n",
      "Epoch 2 step 677: training loss: 29036.13198568846\n",
      "Epoch 2 step 678: training accuarcy: 0.7827000000000001\n",
      "Epoch 2 step 678: training loss: 28493.956733851424\n",
      "Epoch 2 step 679: training accuarcy: 0.7933\n",
      "Epoch 2 step 679: training loss: 28229.806701503447\n",
      "Epoch 2 step 680: training accuarcy: 0.7929\n",
      "Epoch 2 step 680: training loss: 28983.894536384047\n",
      "Epoch 2 step 681: training accuarcy: 0.7814\n",
      "Epoch 2 step 681: training loss: 29189.20759762916\n",
      "Epoch 2 step 682: training accuarcy: 0.7821\n",
      "Epoch 2 step 682: training loss: 28474.08652218888\n",
      "Epoch 2 step 683: training accuarcy: 0.7894\n",
      "Epoch 2 step 683: training loss: 28969.017622066218\n",
      "Epoch 2 step 684: training accuarcy: 0.7804\n",
      "Epoch 2 step 684: training loss: 29374.18733473781\n",
      "Epoch 2 step 685: training accuarcy: 0.7772\n",
      "Epoch 2 step 685: training loss: 28816.229974363963\n",
      "Epoch 2 step 686: training accuarcy: 0.7825000000000001\n",
      "Epoch 2 step 686: training loss: 28439.806784752764\n",
      "Epoch 2 step 687: training accuarcy: 0.7928000000000001\n",
      "Epoch 2 step 687: training loss: 29098.181310649656\n",
      "Epoch 2 step 688: training accuarcy: 0.7842\n",
      "Epoch 2 step 688: training loss: 28766.481536053314\n",
      "Epoch 2 step 689: training accuarcy: 0.7781\n",
      "Epoch 2 step 689: training loss: 28384.118761261147\n",
      "Epoch 2 step 690: training accuarcy: 0.7918000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 690: training loss: 28597.40305173381\n",
      "Epoch 2 step 691: training accuarcy: 0.7861\n",
      "Epoch 2 step 691: training loss: 28287.801654713963\n",
      "Epoch 2 step 692: training accuarcy: 0.7956000000000001\n",
      "Epoch 2 step 692: training loss: 29244.290852748567\n",
      "Epoch 2 step 693: training accuarcy: 0.7778\n",
      "Epoch 2 step 693: training loss: 28655.785406814524\n",
      "Epoch 2 step 694: training accuarcy: 0.7754000000000001\n",
      "Epoch 2 step 694: training loss: 28746.35303665627\n",
      "Epoch 2 step 695: training accuarcy: 0.7852\n",
      "Epoch 2 step 695: training loss: 28817.037135293744\n",
      "Epoch 2 step 696: training accuarcy: 0.7873\n",
      "Epoch 2 step 696: training loss: 28885.19211856198\n",
      "Epoch 2 step 697: training accuarcy: 0.7854\n",
      "Epoch 2 step 697: training loss: 28467.659738975814\n",
      "Epoch 2 step 698: training accuarcy: 0.7874\n",
      "Epoch 2 step 698: training loss: 29266.5728954465\n",
      "Epoch 2 step 699: training accuarcy: 0.7695000000000001\n",
      "Epoch 2 step 699: training loss: 28353.769339563405\n",
      "Epoch 2 step 700: training accuarcy: 0.7932\n",
      "Epoch 2 step 700: training loss: 28290.250054959255\n",
      "Epoch 2 step 701: training accuarcy: 0.7952\n",
      "Epoch 2 step 701: training loss: 28773.895843726415\n",
      "Epoch 2 step 702: training accuarcy: 0.7957000000000001\n",
      "Epoch 2 step 702: training loss: 28743.637393879966\n",
      "Epoch 2 step 703: training accuarcy: 0.7853\n",
      "Epoch 2 step 703: training loss: 28248.28531792525\n",
      "Epoch 2 step 704: training accuarcy: 0.7869\n",
      "Epoch 2 step 704: training loss: 28282.18390829066\n",
      "Epoch 2 step 705: training accuarcy: 0.7894\n",
      "Epoch 2 step 705: training loss: 29198.213430329844\n",
      "Epoch 2 step 706: training accuarcy: 0.7824\n",
      "Epoch 2 step 706: training loss: 29271.697129179614\n",
      "Epoch 2 step 707: training accuarcy: 0.7797000000000001\n",
      "Epoch 2 step 707: training loss: 28644.396721033227\n",
      "Epoch 2 step 708: training accuarcy: 0.7844\n",
      "Epoch 2 step 708: training loss: 28776.19965283303\n",
      "Epoch 2 step 709: training accuarcy: 0.7821\n",
      "Epoch 2 step 709: training loss: 29300.43565913195\n",
      "Epoch 2 step 710: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 710: training loss: 28348.475130434603\n",
      "Epoch 2 step 711: training accuarcy: 0.7905000000000001\n",
      "Epoch 2 step 711: training loss: 29251.06650847407\n",
      "Epoch 2 step 712: training accuarcy: 0.7735000000000001\n",
      "Epoch 2 step 712: training loss: 28785.50547472757\n",
      "Epoch 2 step 713: training accuarcy: 0.7827000000000001\n",
      "Epoch 2 step 713: training loss: 28636.51055709015\n",
      "Epoch 2 step 714: training accuarcy: 0.7884\n",
      "Epoch 2 step 714: training loss: 28595.775964620665\n",
      "Epoch 2 step 715: training accuarcy: 0.7898000000000001\n",
      "Epoch 2 step 715: training loss: 29063.323060266885\n",
      "Epoch 2 step 716: training accuarcy: 0.7862\n",
      "Epoch 2 step 716: training loss: 28585.504333468634\n",
      "Epoch 2 step 717: training accuarcy: 0.7873\n",
      "Epoch 2 step 717: training loss: 28981.736375586104\n",
      "Epoch 2 step 718: training accuarcy: 0.7792\n",
      "Epoch 2 step 718: training loss: 29302.309080758674\n",
      "Epoch 2 step 719: training accuarcy: 0.7823\n",
      "Epoch 2 step 719: training loss: 28245.892666200365\n",
      "Epoch 2 step 720: training accuarcy: 0.7874\n",
      "Epoch 2 step 720: training loss: 28688.046366039278\n",
      "Epoch 2 step 721: training accuarcy: 0.7906000000000001\n",
      "Epoch 2 step 721: training loss: 28663.8036260292\n",
      "Epoch 2 step 722: training accuarcy: 0.7934\n",
      "Epoch 2 step 722: training loss: 28575.148407328594\n",
      "Epoch 2 step 723: training accuarcy: 0.7837000000000001\n",
      "Epoch 2 step 723: training loss: 29106.143967722925\n",
      "Epoch 2 step 724: training accuarcy: 0.7876000000000001\n",
      "Epoch 2 step 724: training loss: 28256.03608964547\n",
      "Epoch 2 step 725: training accuarcy: 0.7919\n",
      "Epoch 2 step 725: training loss: 29193.592025958176\n",
      "Epoch 2 step 726: training accuarcy: 0.7807000000000001\n",
      "Epoch 2 step 726: training loss: 29336.185142134174\n",
      "Epoch 2 step 727: training accuarcy: 0.7722\n",
      "Epoch 2 step 727: training loss: 28962.898446169096\n",
      "Epoch 2 step 728: training accuarcy: 0.7863\n",
      "Epoch 2 step 728: training loss: 28117.231096757332\n",
      "Epoch 2 step 729: training accuarcy: 0.7918000000000001\n",
      "Epoch 2 step 729: training loss: 28839.910518495446\n",
      "Epoch 2 step 730: training accuarcy: 0.7927000000000001\n",
      "Epoch 2 step 730: training loss: 29192.371305927754\n",
      "Epoch 2 step 731: training accuarcy: 0.7725000000000001\n",
      "Epoch 2 step 731: training loss: 28286.986854705072\n",
      "Epoch 2 step 732: training accuarcy: 0.7969\n",
      "Epoch 2 step 732: training loss: 28738.418873785613\n",
      "Epoch 2 step 733: training accuarcy: 0.7874\n",
      "Epoch 2 step 733: training loss: 28636.943144297893\n",
      "Epoch 2 step 734: training accuarcy: 0.79\n",
      "Epoch 2 step 734: training loss: 28984.31754437962\n",
      "Epoch 2 step 735: training accuarcy: 0.7773\n",
      "Epoch 2 step 735: training loss: 28432.015120395125\n",
      "Epoch 2 step 736: training accuarcy: 0.7911\n",
      "Epoch 2 step 736: training loss: 29122.971294540046\n",
      "Epoch 2 step 737: training accuarcy: 0.7775000000000001\n",
      "Epoch 2 step 737: training loss: 28919.885674738336\n",
      "Epoch 2 step 738: training accuarcy: 0.7883\n",
      "Epoch 2 step 738: training loss: 28567.877430484154\n",
      "Epoch 2 step 739: training accuarcy: 0.7906000000000001\n",
      "Epoch 2 step 739: training loss: 28567.63124940709\n",
      "Epoch 2 step 740: training accuarcy: 0.7873\n",
      "Epoch 2 step 740: training loss: 28907.772476631122\n",
      "Epoch 2 step 741: training accuarcy: 0.7792\n",
      "Epoch 2 step 741: training loss: 27887.41274420786\n",
      "Epoch 2 step 742: training accuarcy: 0.8027000000000001\n",
      "Epoch 2 step 742: training loss: 29028.324327086815\n",
      "Epoch 2 step 743: training accuarcy: 0.78\n",
      "Epoch 2 step 743: training loss: 28830.06989388102\n",
      "Epoch 2 step 744: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 744: training loss: 28425.582902862763\n",
      "Epoch 2 step 745: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 745: training loss: 28775.990500963253\n",
      "Epoch 2 step 746: training accuarcy: 0.7888000000000001\n",
      "Epoch 2 step 746: training loss: 29061.4637093625\n",
      "Epoch 2 step 747: training accuarcy: 0.7751\n",
      "Epoch 2 step 747: training loss: 28828.50106405306\n",
      "Epoch 2 step 748: training accuarcy: 0.7835000000000001\n",
      "Epoch 2 step 748: training loss: 28972.024653152843\n",
      "Epoch 2 step 749: training accuarcy: 0.7808\n",
      "Epoch 2 step 749: training loss: 28843.62691110761\n",
      "Epoch 2 step 750: training accuarcy: 0.7822\n",
      "Epoch 2 step 750: training loss: 28762.181621039705\n",
      "Epoch 2 step 751: training accuarcy: 0.7791\n",
      "Epoch 2 step 751: training loss: 29646.20372179889\n",
      "Epoch 2 step 752: training accuarcy: 0.7702\n",
      "Epoch 2 step 752: training loss: 29002.307199279934\n",
      "Epoch 2 step 753: training accuarcy: 0.7852\n",
      "Epoch 2 step 753: training loss: 28957.899330592652\n",
      "Epoch 2 step 754: training accuarcy: 0.7803\n",
      "Epoch 2 step 754: training loss: 28459.815206606152\n",
      "Epoch 2 step 755: training accuarcy: 0.7828\n",
      "Epoch 2 step 755: training loss: 29003.09437638173\n",
      "Epoch 2 step 756: training accuarcy: 0.7819\n",
      "Epoch 2 step 756: training loss: 28700.181682970277\n",
      "Epoch 2 step 757: training accuarcy: 0.791\n",
      "Epoch 2 step 757: training loss: 28418.64620531527\n",
      "Epoch 2 step 758: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 758: training loss: 28832.746849772426\n",
      "Epoch 2 step 759: training accuarcy: 0.7872\n",
      "Epoch 2 step 759: training loss: 29189.99429249127\n",
      "Epoch 2 step 760: training accuarcy: 0.7791\n",
      "Epoch 2 step 760: training loss: 28673.37523548882\n",
      "Epoch 2 step 761: training accuarcy: 0.7896000000000001\n",
      "Epoch 2 step 761: training loss: 28362.29151992638\n",
      "Epoch 2 step 762: training accuarcy: 0.7853\n",
      "Epoch 2 step 762: training loss: 28196.19600945464\n",
      "Epoch 2 step 763: training accuarcy: 0.7864\n",
      "Epoch 2 step 763: training loss: 29165.83731743296\n",
      "Epoch 2 step 764: training accuarcy: 0.7801\n",
      "Epoch 2 step 764: training loss: 28541.616338218148\n",
      "Epoch 2 step 765: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 765: training loss: 28811.15698666816\n",
      "Epoch 2 step 766: training accuarcy: 0.7766000000000001\n",
      "Epoch 2 step 766: training loss: 28740.1923601297\n",
      "Epoch 2 step 767: training accuarcy: 0.7782\n",
      "Epoch 2 step 767: training loss: 29320.73238198382\n",
      "Epoch 2 step 768: training accuarcy: 0.7742\n",
      "Epoch 2 step 768: training loss: 28260.84672725874\n",
      "Epoch 2 step 769: training accuarcy: 0.7919\n",
      "Epoch 2 step 769: training loss: 28957.526672058677\n",
      "Epoch 2 step 770: training accuarcy: 0.7762\n",
      "Epoch 2 step 770: training loss: 28562.103378275435\n",
      "Epoch 2 step 771: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 771: training loss: 28475.05956443003\n",
      "Epoch 2 step 772: training accuarcy: 0.7803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 772: training loss: 29019.41296146404\n",
      "Epoch 2 step 773: training accuarcy: 0.7732\n",
      "Epoch 2 step 773: training loss: 28652.311985166358\n",
      "Epoch 2 step 774: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 774: training loss: 28928.243440792026\n",
      "Epoch 2 step 775: training accuarcy: 0.7745000000000001\n",
      "Epoch 2 step 775: training loss: 28051.80545566762\n",
      "Epoch 2 step 776: training accuarcy: 0.7965\n",
      "Epoch 2 step 776: training loss: 28278.83419560922\n",
      "Epoch 2 step 777: training accuarcy: 0.7818\n",
      "Epoch 2 step 777: training loss: 28967.560284302526\n",
      "Epoch 2 step 778: training accuarcy: 0.7825000000000001\n",
      "Epoch 2 step 778: training loss: 28562.803906150028\n",
      "Epoch 2 step 779: training accuarcy: 0.7824\n",
      "Epoch 2 step 779: training loss: 28608.27201409693\n",
      "Epoch 2 step 780: training accuarcy: 0.7795000000000001\n",
      "Epoch 2 step 780: training loss: 28173.920706854355\n",
      "Epoch 2 step 781: training accuarcy: 0.791\n",
      "Epoch 2 step 781: training loss: 29469.059511362255\n",
      "Epoch 2 step 782: training accuarcy: 0.769\n",
      "Epoch 2 step 782: training loss: 28080.376779116737\n",
      "Epoch 2 step 783: training accuarcy: 0.7934\n",
      "Epoch 2 step 783: training loss: 28181.44120468637\n",
      "Epoch 2 step 784: training accuarcy: 0.7884\n",
      "Epoch 2 step 784: training loss: 27811.88896753308\n",
      "Epoch 2 step 785: training accuarcy: 0.7913\n",
      "Epoch 2 step 785: training loss: 28492.916805675373\n",
      "Epoch 2 step 786: training accuarcy: 0.782\n",
      "Epoch 2 step 786: training loss: 29131.799150168416\n",
      "Epoch 2 step 787: training accuarcy: 0.779\n",
      "Epoch 2 step 787: training loss: 27981.950554054547\n",
      "Epoch 2 step 788: training accuarcy: 0.7897000000000001\n",
      "Epoch 2 step 788: training loss: 12910.41790157804\n",
      "Epoch 2 step 789: training accuarcy: 0.796923076923077\n",
      "Epoch 2: train loss 28942.57826501978, train accuarcy 0.7592906951904297\n",
      "Epoch 2: valid loss 27549.076667420246, valid accuarcy 0.793188214302063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [15:15<00:00, 304.55s/it]\n"
     ]
    }
   ],
   "source": [
    "hrm_learner.fit(epoch=3,\n",
    "                log_dir=get_log_dir('weight_topcoder', 'hrm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:41:50.652406Z",
     "start_time": "2019-10-09T12:41:50.640486Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hrm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3769a409bbf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mhrm_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hrm_model' is not defined"
     ]
    }
   ],
   "source": [
    "del hrm_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PRME FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:59:35.373835Z",
     "start_time": "2019-10-09T12:59:35.105867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchPrmeFM()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_model = TorchPrmeFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "prme_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:59:35.470837Z",
     "start_time": "2019-10-09T12:59:35.467837Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(prme_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:59:36.255740Z",
     "start_time": "2019-10-09T12:59:36.210759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x20b9d503c18>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prme_learner = FMLearner(prme_model, adam_opt, schedular, db)\n",
    "prme_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T01:39:56.824240Z",
     "start_time": "2019-10-08T01:39:56.821271Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='base',\n",
    "                   valid_col='seq',\n",
    "                   test_col='seq',\n",
    "                   loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:42:28.132692Z",
     "start_time": "2019-10-09T12:42:28.129692Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='seq',\n",
    "                     valid_col='seq',\n",
    "                     test_col='seq',\n",
    "                     loss_callback=simple_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:59:52.359203Z",
     "start_time": "2019-10-09T12:59:52.355203Z"
    }
   },
   "outputs": [],
   "source": [
    "prme_learner.compile(train_col='seq',\n",
    "                     valid_col='seq',\n",
    "                     test_col='seq',\n",
    "                     loss_callback=simple_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T12:57:34.765057Z",
     "start_time": "2019-10-09T12:42:35.915428Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 87517.35232690867\n",
      "Epoch 0 step 1: training accuarcy: 0.5117\n",
      "Epoch 0 step 1: training loss: 83730.58049910842\n",
      "Epoch 0 step 2: training accuarcy: 0.5198\n",
      "Epoch 0 step 2: training loss: 82299.59224676236\n",
      "Epoch 0 step 3: training accuarcy: 0.5243\n",
      "Epoch 0 step 3: training loss: 80546.39131474958\n",
      "Epoch 0 step 4: training accuarcy: 0.5163\n",
      "Epoch 0 step 4: training loss: 78172.21529430218\n",
      "Epoch 0 step 5: training accuarcy: 0.5307000000000001\n",
      "Epoch 0 step 5: training loss: 76259.36374719077\n",
      "Epoch 0 step 6: training accuarcy: 0.524\n",
      "Epoch 0 step 6: training loss: 75570.69756017407\n",
      "Epoch 0 step 7: training accuarcy: 0.5138\n",
      "Epoch 0 step 7: training loss: 73277.05738821287\n",
      "Epoch 0 step 8: training accuarcy: 0.5251\n",
      "Epoch 0 step 8: training loss: 72495.04433095024\n",
      "Epoch 0 step 9: training accuarcy: 0.5235000000000001\n",
      "Epoch 0 step 9: training loss: 71819.02370920154\n",
      "Epoch 0 step 10: training accuarcy: 0.5094000000000001\n",
      "Epoch 0 step 10: training loss: 69082.18034795846\n",
      "Epoch 0 step 11: training accuarcy: 0.5133\n",
      "Epoch 0 step 11: training loss: 66877.16875609514\n",
      "Epoch 0 step 12: training accuarcy: 0.5186000000000001\n",
      "Epoch 0 step 12: training loss: 64749.632491121825\n",
      "Epoch 0 step 13: training accuarcy: 0.5302\n",
      "Epoch 0 step 13: training loss: 63325.85759855259\n",
      "Epoch 0 step 14: training accuarcy: 0.5267000000000001\n",
      "Epoch 0 step 14: training loss: 61563.42848652738\n",
      "Epoch 0 step 15: training accuarcy: 0.537\n",
      "Epoch 0 step 15: training loss: 61298.04851327125\n",
      "Epoch 0 step 16: training accuarcy: 0.5267000000000001\n",
      "Epoch 0 step 16: training loss: 60537.966416472016\n",
      "Epoch 0 step 17: training accuarcy: 0.5273\n",
      "Epoch 0 step 17: training loss: 57762.35584947796\n",
      "Epoch 0 step 18: training accuarcy: 0.5384\n",
      "Epoch 0 step 18: training loss: 56901.84140921853\n",
      "Epoch 0 step 19: training accuarcy: 0.527\n",
      "Epoch 0 step 19: training loss: 54033.99377045151\n",
      "Epoch 0 step 20: training accuarcy: 0.5368\n",
      "Epoch 0 step 20: training loss: 54145.47463699391\n",
      "Epoch 0 step 21: training accuarcy: 0.5171\n",
      "Epoch 0 step 21: training loss: 51634.72565275485\n",
      "Epoch 0 step 22: training accuarcy: 0.5367000000000001\n",
      "Epoch 0 step 22: training loss: 51586.851363624766\n",
      "Epoch 0 step 23: training accuarcy: 0.5306000000000001\n",
      "Epoch 0 step 23: training loss: 49235.66224195833\n",
      "Epoch 0 step 24: training accuarcy: 0.5398000000000001\n",
      "Epoch 0 step 24: training loss: 48320.45321690332\n",
      "Epoch 0 step 25: training accuarcy: 0.5298\n",
      "Epoch 0 step 25: training loss: 47168.084628533026\n",
      "Epoch 0 step 26: training accuarcy: 0.5378000000000001\n",
      "Epoch 0 step 26: training loss: 46612.493672878525\n",
      "Epoch 0 step 27: training accuarcy: 0.5269\n",
      "Epoch 0 step 27: training loss: 45518.18464476978\n",
      "Epoch 0 step 28: training accuarcy: 0.5312\n",
      "Epoch 0 step 28: training loss: 42395.108825056275\n",
      "Epoch 0 step 29: training accuarcy: 0.5538000000000001\n",
      "Epoch 0 step 29: training loss: 41738.88846169003\n",
      "Epoch 0 step 30: training accuarcy: 0.5331\n",
      "Epoch 0 step 30: training loss: 41672.41580402286\n",
      "Epoch 0 step 31: training accuarcy: 0.5298\n",
      "Epoch 0 step 31: training loss: 39760.867231949785\n",
      "Epoch 0 step 32: training accuarcy: 0.5369\n",
      "Epoch 0 step 32: training loss: 38147.08291056146\n",
      "Epoch 0 step 33: training accuarcy: 0.5398000000000001\n",
      "Epoch 0 step 33: training loss: 37868.746550403564\n",
      "Epoch 0 step 34: training accuarcy: 0.5373\n",
      "Epoch 0 step 34: training loss: 36350.77389293899\n",
      "Epoch 0 step 35: training accuarcy: 0.55\n",
      "Epoch 0 step 35: training loss: 35591.62620827771\n",
      "Epoch 0 step 36: training accuarcy: 0.5397000000000001\n",
      "Epoch 0 step 36: training loss: 34961.79221983322\n",
      "Epoch 0 step 37: training accuarcy: 0.5379\n",
      "Epoch 0 step 37: training loss: 33519.4979408937\n",
      "Epoch 0 step 38: training accuarcy: 0.5469\n",
      "Epoch 0 step 38: training loss: 32599.83618840651\n",
      "Epoch 0 step 39: training accuarcy: 0.5422\n",
      "Epoch 0 step 39: training loss: 31938.81035051866\n",
      "Epoch 0 step 40: training accuarcy: 0.5274\n",
      "Epoch 0 step 40: training loss: 31127.959942827558\n",
      "Epoch 0 step 41: training accuarcy: 0.5488000000000001\n",
      "Epoch 0 step 41: training loss: 29921.18902528448\n",
      "Epoch 0 step 42: training accuarcy: 0.5383\n",
      "Epoch 0 step 42: training loss: 28978.70031176827\n",
      "Epoch 0 step 43: training accuarcy: 0.5445\n",
      "Epoch 0 step 43: training loss: 27959.031609618734\n",
      "Epoch 0 step 44: training accuarcy: 0.551\n",
      "Epoch 0 step 44: training loss: 26793.15253782716\n",
      "Epoch 0 step 45: training accuarcy: 0.5645\n",
      "Epoch 0 step 45: training loss: 26927.628686210606\n",
      "Epoch 0 step 46: training accuarcy: 0.5495\n",
      "Epoch 0 step 46: training loss: 25764.45517160491\n",
      "Epoch 0 step 47: training accuarcy: 0.5475\n",
      "Epoch 0 step 47: training loss: 25426.759361923796\n",
      "Epoch 0 step 48: training accuarcy: 0.5357000000000001\n",
      "Epoch 0 step 48: training loss: 24661.913095304535\n",
      "Epoch 0 step 49: training accuarcy: 0.5533\n",
      "Epoch 0 step 49: training loss: 24258.282248480587\n",
      "Epoch 0 step 50: training accuarcy: 0.5504\n",
      "Epoch 0 step 50: training loss: 23427.283456426187\n",
      "Epoch 0 step 51: training accuarcy: 0.5595\n",
      "Epoch 0 step 51: training loss: 22998.74176121893\n",
      "Epoch 0 step 52: training accuarcy: 0.5433\n",
      "Epoch 0 step 52: training loss: 22440.22926367689\n",
      "Epoch 0 step 53: training accuarcy: 0.5515\n",
      "Epoch 0 step 53: training loss: 21845.82893921974\n",
      "Epoch 0 step 54: training accuarcy: 0.5665\n",
      "Epoch 0 step 54: training loss: 21517.609902033608\n",
      "Epoch 0 step 55: training accuarcy: 0.5499\n",
      "Epoch 0 step 55: training loss: 21104.891684363938\n",
      "Epoch 0 step 56: training accuarcy: 0.5482\n",
      "Epoch 0 step 56: training loss: 20422.117714888085\n",
      "Epoch 0 step 57: training accuarcy: 0.5546\n",
      "Epoch 0 step 57: training loss: 20084.162580600627\n",
      "Epoch 0 step 58: training accuarcy: 0.5573\n",
      "Epoch 0 step 58: training loss: 19565.78440456569\n",
      "Epoch 0 step 59: training accuarcy: 0.5622\n",
      "Epoch 0 step 59: training loss: 19039.014775901087\n",
      "Epoch 0 step 60: training accuarcy: 0.5718\n",
      "Epoch 0 step 60: training loss: 18979.545192660735\n",
      "Epoch 0 step 61: training accuarcy: 0.5606\n",
      "Epoch 0 step 61: training loss: 18587.391384838324\n",
      "Epoch 0 step 62: training accuarcy: 0.5582\n",
      "Epoch 0 step 62: training loss: 18179.884217529125\n",
      "Epoch 0 step 63: training accuarcy: 0.5778\n",
      "Epoch 0 step 63: training loss: 18058.86613537757\n",
      "Epoch 0 step 64: training accuarcy: 0.5565\n",
      "Epoch 0 step 64: training loss: 17568.388381361907\n",
      "Epoch 0 step 65: training accuarcy: 0.5725\n",
      "Epoch 0 step 65: training loss: 17277.601416604404\n",
      "Epoch 0 step 66: training accuarcy: 0.5681\n",
      "Epoch 0 step 66: training loss: 17056.80954232844\n",
      "Epoch 0 step 67: training accuarcy: 0.5653\n",
      "Epoch 0 step 67: training loss: 16810.46923228536\n",
      "Epoch 0 step 68: training accuarcy: 0.5682\n",
      "Epoch 0 step 68: training loss: 16516.50075911459\n",
      "Epoch 0 step 69: training accuarcy: 0.5804\n",
      "Epoch 0 step 69: training loss: 16336.649406576322\n",
      "Epoch 0 step 70: training accuarcy: 0.5689000000000001\n",
      "Epoch 0 step 70: training loss: 16045.561528286984\n",
      "Epoch 0 step 71: training accuarcy: 0.5756\n",
      "Epoch 0 step 71: training loss: 15833.511002214003\n",
      "Epoch 0 step 72: training accuarcy: 0.5716\n",
      "Epoch 0 step 72: training loss: 15555.181642990505\n",
      "Epoch 0 step 73: training accuarcy: 0.5831000000000001\n",
      "Epoch 0 step 73: training loss: 15269.335794397855\n",
      "Epoch 0 step 74: training accuarcy: 0.5874\n",
      "Epoch 0 step 74: training loss: 15486.230565216883\n",
      "Epoch 0 step 75: training accuarcy: 0.5684\n",
      "Epoch 0 step 75: training loss: 15236.051845421858\n",
      "Epoch 0 step 76: training accuarcy: 0.5757\n",
      "Epoch 0 step 76: training loss: 14868.17889869288\n",
      "Epoch 0 step 77: training accuarcy: 0.5837\n",
      "Epoch 0 step 77: training loss: 14773.467987708556\n",
      "Epoch 0 step 78: training accuarcy: 0.5757\n",
      "Epoch 0 step 78: training loss: 14698.770942936419\n",
      "Epoch 0 step 79: training accuarcy: 0.5769000000000001\n",
      "Epoch 0 step 79: training loss: 14403.852807479563\n",
      "Epoch 0 step 80: training accuarcy: 0.5831000000000001\n",
      "Epoch 0 step 80: training loss: 14270.130633951536\n",
      "Epoch 0 step 81: training accuarcy: 0.5834\n",
      "Epoch 0 step 81: training loss: 14034.411253992843\n",
      "Epoch 0 step 82: training accuarcy: 0.5999\n",
      "Epoch 0 step 82: training loss: 13884.149643709363\n",
      "Epoch 0 step 83: training accuarcy: 0.5928\n",
      "Epoch 0 step 83: training loss: 13787.28215609198\n",
      "Epoch 0 step 84: training accuarcy: 0.5999\n",
      "Epoch 0 step 84: training loss: 13741.462652807299\n",
      "Epoch 0 step 85: training accuarcy: 0.5924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 85: training loss: 13646.205122513511\n",
      "Epoch 0 step 86: training accuarcy: 0.5923\n",
      "Epoch 0 step 86: training loss: 13544.976658736261\n",
      "Epoch 0 step 87: training accuarcy: 0.5948\n",
      "Epoch 0 step 87: training loss: 13313.003614547353\n",
      "Epoch 0 step 88: training accuarcy: 0.6074\n",
      "Epoch 0 step 88: training loss: 13213.417451061403\n",
      "Epoch 0 step 89: training accuarcy: 0.5946\n",
      "Epoch 0 step 89: training loss: 13196.37139699827\n",
      "Epoch 0 step 90: training accuarcy: 0.6019\n",
      "Epoch 0 step 90: training loss: 13278.49043070666\n",
      "Epoch 0 step 91: training accuarcy: 0.5785\n",
      "Epoch 0 step 91: training loss: 12814.977529725827\n",
      "Epoch 0 step 92: training accuarcy: 0.6094\n",
      "Epoch 0 step 92: training loss: 12860.3565593419\n",
      "Epoch 0 step 93: training accuarcy: 0.5952000000000001\n",
      "Epoch 0 step 93: training loss: 12706.35801854423\n",
      "Epoch 0 step 94: training accuarcy: 0.6013000000000001\n",
      "Epoch 0 step 94: training loss: 12666.417241812676\n",
      "Epoch 0 step 95: training accuarcy: 0.6029\n",
      "Epoch 0 step 95: training loss: 12544.48202252977\n",
      "Epoch 0 step 96: training accuarcy: 0.6168\n",
      "Epoch 0 step 96: training loss: 12421.202904603952\n",
      "Epoch 0 step 97: training accuarcy: 0.6027\n",
      "Epoch 0 step 97: training loss: 12285.641167756996\n",
      "Epoch 0 step 98: training accuarcy: 0.6115\n",
      "Epoch 0 step 98: training loss: 12191.020601359054\n",
      "Epoch 0 step 99: training accuarcy: 0.6158\n",
      "Epoch 0 step 99: training loss: 12163.031145747866\n",
      "Epoch 0 step 100: training accuarcy: 0.609\n",
      "Epoch 0 step 100: training loss: 12012.934866387921\n",
      "Epoch 0 step 101: training accuarcy: 0.6143000000000001\n",
      "Epoch 0 step 101: training loss: 11965.976001405936\n",
      "Epoch 0 step 102: training accuarcy: 0.6104\n",
      "Epoch 0 step 102: training loss: 11795.138381573532\n",
      "Epoch 0 step 103: training accuarcy: 0.6191\n",
      "Epoch 0 step 103: training loss: 11703.084798688318\n",
      "Epoch 0 step 104: training accuarcy: 0.629\n",
      "Epoch 0 step 104: training loss: 11686.881380905836\n",
      "Epoch 0 step 105: training accuarcy: 0.6188\n",
      "Epoch 0 step 105: training loss: 11727.900958603088\n",
      "Epoch 0 step 106: training accuarcy: 0.6105\n",
      "Epoch 0 step 106: training loss: 11693.498022122074\n",
      "Epoch 0 step 107: training accuarcy: 0.6136\n",
      "Epoch 0 step 107: training loss: 11538.936603084116\n",
      "Epoch 0 step 108: training accuarcy: 0.6068\n",
      "Epoch 0 step 108: training loss: 11280.175648239874\n",
      "Epoch 0 step 109: training accuarcy: 0.6333000000000001\n",
      "Epoch 0 step 109: training loss: 11386.535368751896\n",
      "Epoch 0 step 110: training accuarcy: 0.616\n",
      "Epoch 0 step 110: training loss: 11265.955831524663\n",
      "Epoch 0 step 111: training accuarcy: 0.6254000000000001\n",
      "Epoch 0 step 111: training loss: 11229.558550229853\n",
      "Epoch 0 step 112: training accuarcy: 0.6123000000000001\n",
      "Epoch 0 step 112: training loss: 11069.749935582418\n",
      "Epoch 0 step 113: training accuarcy: 0.6398\n",
      "Epoch 0 step 113: training loss: 11028.642477238096\n",
      "Epoch 0 step 114: training accuarcy: 0.6418\n",
      "Epoch 0 step 114: training loss: 10914.538023493169\n",
      "Epoch 0 step 115: training accuarcy: 0.6306\n",
      "Epoch 0 step 115: training loss: 10843.754846146276\n",
      "Epoch 0 step 116: training accuarcy: 0.6334000000000001\n",
      "Epoch 0 step 116: training loss: 10730.91024430939\n",
      "Epoch 0 step 117: training accuarcy: 0.6351\n",
      "Epoch 0 step 117: training loss: 10799.788578445921\n",
      "Epoch 0 step 118: training accuarcy: 0.6319\n",
      "Epoch 0 step 118: training loss: 10652.048672590237\n",
      "Epoch 0 step 119: training accuarcy: 0.6368\n",
      "Epoch 0 step 119: training loss: 10573.835300025206\n",
      "Epoch 0 step 120: training accuarcy: 0.6356\n",
      "Epoch 0 step 120: training loss: 10531.01097399351\n",
      "Epoch 0 step 121: training accuarcy: 0.649\n",
      "Epoch 0 step 121: training loss: 10566.608060413506\n",
      "Epoch 0 step 122: training accuarcy: 0.6371\n",
      "Epoch 0 step 122: training loss: 10482.754955629323\n",
      "Epoch 0 step 123: training accuarcy: 0.6312\n",
      "Epoch 0 step 123: training loss: 10402.546619437471\n",
      "Epoch 0 step 124: training accuarcy: 0.6345000000000001\n",
      "Epoch 0 step 124: training loss: 10389.017203966947\n",
      "Epoch 0 step 125: training accuarcy: 0.6435000000000001\n",
      "Epoch 0 step 125: training loss: 10335.36063578512\n",
      "Epoch 0 step 126: training accuarcy: 0.6379\n",
      "Epoch 0 step 126: training loss: 10308.845681172887\n",
      "Epoch 0 step 127: training accuarcy: 0.6368\n",
      "Epoch 0 step 127: training loss: 10317.410698125273\n",
      "Epoch 0 step 128: training accuarcy: 0.6321\n",
      "Epoch 0 step 128: training loss: 10027.22335888797\n",
      "Epoch 0 step 129: training accuarcy: 0.6578\n",
      "Epoch 0 step 129: training loss: 10091.48347720973\n",
      "Epoch 0 step 130: training accuarcy: 0.6511\n",
      "Epoch 0 step 130: training loss: 10005.899112635374\n",
      "Epoch 0 step 131: training accuarcy: 0.6473\n",
      "Epoch 0 step 131: training loss: 9996.042021732246\n",
      "Epoch 0 step 132: training accuarcy: 0.6523\n",
      "Epoch 0 step 132: training loss: 10031.739794754152\n",
      "Epoch 0 step 133: training accuarcy: 0.6442\n",
      "Epoch 0 step 133: training loss: 9919.025444086777\n",
      "Epoch 0 step 134: training accuarcy: 0.6563\n",
      "Epoch 0 step 134: training loss: 9778.29594019202\n",
      "Epoch 0 step 135: training accuarcy: 0.6542\n",
      "Epoch 0 step 135: training loss: 9868.538671715929\n",
      "Epoch 0 step 136: training accuarcy: 0.6433\n",
      "Epoch 0 step 136: training loss: 9859.814323700208\n",
      "Epoch 0 step 137: training accuarcy: 0.6413\n",
      "Epoch 0 step 137: training loss: 9734.106341505229\n",
      "Epoch 0 step 138: training accuarcy: 0.6523\n",
      "Epoch 0 step 138: training loss: 9551.030698638851\n",
      "Epoch 0 step 139: training accuarcy: 0.6657000000000001\n",
      "Epoch 0 step 139: training loss: 9660.62086491931\n",
      "Epoch 0 step 140: training accuarcy: 0.6533\n",
      "Epoch 0 step 140: training loss: 9667.528278019287\n",
      "Epoch 0 step 141: training accuarcy: 0.6555000000000001\n",
      "Epoch 0 step 141: training loss: 9542.752290178927\n",
      "Epoch 0 step 142: training accuarcy: 0.6614\n",
      "Epoch 0 step 142: training loss: 9410.857449501775\n",
      "Epoch 0 step 143: training accuarcy: 0.6719\n",
      "Epoch 0 step 143: training loss: 9435.103600330121\n",
      "Epoch 0 step 144: training accuarcy: 0.6615\n",
      "Epoch 0 step 144: training loss: 9407.931753276003\n",
      "Epoch 0 step 145: training accuarcy: 0.6759000000000001\n",
      "Epoch 0 step 145: training loss: 9418.579871900813\n",
      "Epoch 0 step 146: training accuarcy: 0.6609\n",
      "Epoch 0 step 146: training loss: 9345.344365005705\n",
      "Epoch 0 step 147: training accuarcy: 0.6582\n",
      "Epoch 0 step 147: training loss: 9267.598160439076\n",
      "Epoch 0 step 148: training accuarcy: 0.6695\n",
      "Epoch 0 step 148: training loss: 9303.264564429812\n",
      "Epoch 0 step 149: training accuarcy: 0.6617000000000001\n",
      "Epoch 0 step 149: training loss: 9179.491725184214\n",
      "Epoch 0 step 150: training accuarcy: 0.6728000000000001\n",
      "Epoch 0 step 150: training loss: 9234.144360869583\n",
      "Epoch 0 step 151: training accuarcy: 0.6677000000000001\n",
      "Epoch 0 step 151: training loss: 9163.695572574046\n",
      "Epoch 0 step 152: training accuarcy: 0.6722\n",
      "Epoch 0 step 152: training loss: 9099.869569760323\n",
      "Epoch 0 step 153: training accuarcy: 0.6744\n",
      "Epoch 0 step 153: training loss: 9088.209177600682\n",
      "Epoch 0 step 154: training accuarcy: 0.6742\n",
      "Epoch 0 step 154: training loss: 9049.224422813593\n",
      "Epoch 0 step 155: training accuarcy: 0.6761\n",
      "Epoch 0 step 155: training loss: 8976.081867728415\n",
      "Epoch 0 step 156: training accuarcy: 0.6721\n",
      "Epoch 0 step 156: training loss: 8965.519649567668\n",
      "Epoch 0 step 157: training accuarcy: 0.6775\n",
      "Epoch 0 step 157: training loss: 8948.143820248424\n",
      "Epoch 0 step 158: training accuarcy: 0.676\n",
      "Epoch 0 step 158: training loss: 8928.82881500224\n",
      "Epoch 0 step 159: training accuarcy: 0.6841\n",
      "Epoch 0 step 159: training loss: 8965.590865335485\n",
      "Epoch 0 step 160: training accuarcy: 0.6755\n",
      "Epoch 0 step 160: training loss: 8873.154583070189\n",
      "Epoch 0 step 161: training accuarcy: 0.6766\n",
      "Epoch 0 step 161: training loss: 8901.125350587796\n",
      "Epoch 0 step 162: training accuarcy: 0.6779000000000001\n",
      "Epoch 0 step 162: training loss: 8688.321164432908\n",
      "Epoch 0 step 163: training accuarcy: 0.6939000000000001\n",
      "Epoch 0 step 163: training loss: 8815.157482692899\n",
      "Epoch 0 step 164: training accuarcy: 0.6708000000000001\n",
      "Epoch 0 step 164: training loss: 8664.208824409696\n",
      "Epoch 0 step 165: training accuarcy: 0.6895\n",
      "Epoch 0 step 165: training loss: 8723.240166057702\n",
      "Epoch 0 step 166: training accuarcy: 0.6782\n",
      "Epoch 0 step 166: training loss: 8680.245274658002\n",
      "Epoch 0 step 167: training accuarcy: 0.6858000000000001\n",
      "Epoch 0 step 167: training loss: 8628.532251786526\n",
      "Epoch 0 step 168: training accuarcy: 0.6880000000000001\n",
      "Epoch 0 step 168: training loss: 8583.843390091397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 169: training accuarcy: 0.6842\n",
      "Epoch 0 step 169: training loss: 8733.840265688857\n",
      "Epoch 0 step 170: training accuarcy: 0.6779000000000001\n",
      "Epoch 0 step 170: training loss: 8581.667510698779\n",
      "Epoch 0 step 171: training accuarcy: 0.682\n",
      "Epoch 0 step 171: training loss: 8596.243670823784\n",
      "Epoch 0 step 172: training accuarcy: 0.6866\n",
      "Epoch 0 step 172: training loss: 8394.18079441406\n",
      "Epoch 0 step 173: training accuarcy: 0.6979000000000001\n",
      "Epoch 0 step 173: training loss: 8453.858895602505\n",
      "Epoch 0 step 174: training accuarcy: 0.6916\n",
      "Epoch 0 step 174: training loss: 8413.480537686512\n",
      "Epoch 0 step 175: training accuarcy: 0.6936\n",
      "Epoch 0 step 175: training loss: 8403.545066030718\n",
      "Epoch 0 step 176: training accuarcy: 0.6989000000000001\n",
      "Epoch 0 step 176: training loss: 8468.05392176198\n",
      "Epoch 0 step 177: training accuarcy: 0.6851\n",
      "Epoch 0 step 177: training loss: 8398.80120811788\n",
      "Epoch 0 step 178: training accuarcy: 0.6916\n",
      "Epoch 0 step 178: training loss: 8370.494028797255\n",
      "Epoch 0 step 179: training accuarcy: 0.6987\n",
      "Epoch 0 step 179: training loss: 8231.736620465428\n",
      "Epoch 0 step 180: training accuarcy: 0.7066\n",
      "Epoch 0 step 180: training loss: 8283.806721370878\n",
      "Epoch 0 step 181: training accuarcy: 0.6960000000000001\n",
      "Epoch 0 step 181: training loss: 8256.541230642997\n",
      "Epoch 0 step 182: training accuarcy: 0.7071000000000001\n",
      "Epoch 0 step 182: training loss: 8224.802742674701\n",
      "Epoch 0 step 183: training accuarcy: 0.7012\n",
      "Epoch 0 step 183: training loss: 8183.753620601905\n",
      "Epoch 0 step 184: training accuarcy: 0.7000000000000001\n",
      "Epoch 0 step 184: training loss: 8146.657678793526\n",
      "Epoch 0 step 185: training accuarcy: 0.7153\n",
      "Epoch 0 step 185: training loss: 8125.700067873303\n",
      "Epoch 0 step 186: training accuarcy: 0.7074\n",
      "Epoch 0 step 186: training loss: 8130.050710730884\n",
      "Epoch 0 step 187: training accuarcy: 0.7061000000000001\n",
      "Epoch 0 step 187: training loss: 8133.497498010682\n",
      "Epoch 0 step 188: training accuarcy: 0.7041000000000001\n",
      "Epoch 0 step 188: training loss: 8118.212842903897\n",
      "Epoch 0 step 189: training accuarcy: 0.7044\n",
      "Epoch 0 step 189: training loss: 8147.584281870703\n",
      "Epoch 0 step 190: training accuarcy: 0.6982\n",
      "Epoch 0 step 190: training loss: 8095.091880782127\n",
      "Epoch 0 step 191: training accuarcy: 0.7077\n",
      "Epoch 0 step 191: training loss: 8044.300059873434\n",
      "Epoch 0 step 192: training accuarcy: 0.7002\n",
      "Epoch 0 step 192: training loss: 7977.950247443136\n",
      "Epoch 0 step 193: training accuarcy: 0.7107\n",
      "Epoch 0 step 193: training loss: 8045.542581987556\n",
      "Epoch 0 step 194: training accuarcy: 0.7024\n",
      "Epoch 0 step 194: training loss: 7979.178201706656\n",
      "Epoch 0 step 195: training accuarcy: 0.7138\n",
      "Epoch 0 step 195: training loss: 7845.831590043981\n",
      "Epoch 0 step 196: training accuarcy: 0.7175\n",
      "Epoch 0 step 196: training loss: 7985.844632356308\n",
      "Epoch 0 step 197: training accuarcy: 0.7019000000000001\n",
      "Epoch 0 step 197: training loss: 7889.869729149919\n",
      "Epoch 0 step 198: training accuarcy: 0.7093\n",
      "Epoch 0 step 198: training loss: 7980.031639006559\n",
      "Epoch 0 step 199: training accuarcy: 0.7067\n",
      "Epoch 0 step 199: training loss: 7963.829911123963\n",
      "Epoch 0 step 200: training accuarcy: 0.7055\n",
      "Epoch 0 step 200: training loss: 7831.143354396894\n",
      "Epoch 0 step 201: training accuarcy: 0.7121000000000001\n",
      "Epoch 0 step 201: training loss: 7923.355203039933\n",
      "Epoch 0 step 202: training accuarcy: 0.7016\n",
      "Epoch 0 step 202: training loss: 7835.350133816257\n",
      "Epoch 0 step 203: training accuarcy: 0.7108\n",
      "Epoch 0 step 203: training loss: 7709.474244492674\n",
      "Epoch 0 step 204: training accuarcy: 0.7251000000000001\n",
      "Epoch 0 step 204: training loss: 7638.465531513475\n",
      "Epoch 0 step 205: training accuarcy: 0.7282000000000001\n",
      "Epoch 0 step 205: training loss: 7813.816868450854\n",
      "Epoch 0 step 206: training accuarcy: 0.7075\n",
      "Epoch 0 step 206: training loss: 7735.071871681667\n",
      "Epoch 0 step 207: training accuarcy: 0.7123\n",
      "Epoch 0 step 207: training loss: 7741.163514551342\n",
      "Epoch 0 step 208: training accuarcy: 0.7038\n",
      "Epoch 0 step 208: training loss: 7744.751540881975\n",
      "Epoch 0 step 209: training accuarcy: 0.7076\n",
      "Epoch 0 step 209: training loss: 7717.64829818515\n",
      "Epoch 0 step 210: training accuarcy: 0.7127\n",
      "Epoch 0 step 210: training loss: 7726.104117580318\n",
      "Epoch 0 step 211: training accuarcy: 0.7053\n",
      "Epoch 0 step 211: training loss: 7575.659182930962\n",
      "Epoch 0 step 212: training accuarcy: 0.7198\n",
      "Epoch 0 step 212: training loss: 7617.896652511301\n",
      "Epoch 0 step 213: training accuarcy: 0.7205\n",
      "Epoch 0 step 213: training loss: 7659.562933751067\n",
      "Epoch 0 step 214: training accuarcy: 0.7175\n",
      "Epoch 0 step 214: training loss: 7683.102231705101\n",
      "Epoch 0 step 215: training accuarcy: 0.7037\n",
      "Epoch 0 step 215: training loss: 7546.241812075761\n",
      "Epoch 0 step 216: training accuarcy: 0.7162000000000001\n",
      "Epoch 0 step 216: training loss: 7546.395830386218\n",
      "Epoch 0 step 217: training accuarcy: 0.7187\n",
      "Epoch 0 step 217: training loss: 7593.887117445251\n",
      "Epoch 0 step 218: training accuarcy: 0.7202000000000001\n",
      "Epoch 0 step 218: training loss: 7622.88332506367\n",
      "Epoch 0 step 219: training accuarcy: 0.7182000000000001\n",
      "Epoch 0 step 219: training loss: 7467.474596981287\n",
      "Epoch 0 step 220: training accuarcy: 0.7254\n",
      "Epoch 0 step 220: training loss: 7525.059989689101\n",
      "Epoch 0 step 221: training accuarcy: 0.7199\n",
      "Epoch 0 step 221: training loss: 7531.778797157633\n",
      "Epoch 0 step 222: training accuarcy: 0.7152000000000001\n",
      "Epoch 0 step 222: training loss: 7561.889914377147\n",
      "Epoch 0 step 223: training accuarcy: 0.7240000000000001\n",
      "Epoch 0 step 223: training loss: 7594.042682527412\n",
      "Epoch 0 step 224: training accuarcy: 0.7083\n",
      "Epoch 0 step 224: training loss: 7357.664096797884\n",
      "Epoch 0 step 225: training accuarcy: 0.7327\n",
      "Epoch 0 step 225: training loss: 7458.807825391694\n",
      "Epoch 0 step 226: training accuarcy: 0.7251000000000001\n",
      "Epoch 0 step 226: training loss: 7435.9456022090635\n",
      "Epoch 0 step 227: training accuarcy: 0.7201000000000001\n",
      "Epoch 0 step 227: training loss: 7398.7287284768845\n",
      "Epoch 0 step 228: training accuarcy: 0.7275\n",
      "Epoch 0 step 228: training loss: 7366.335137083093\n",
      "Epoch 0 step 229: training accuarcy: 0.7224\n",
      "Epoch 0 step 229: training loss: 7328.295796667256\n",
      "Epoch 0 step 230: training accuarcy: 0.7321000000000001\n",
      "Epoch 0 step 230: training loss: 7328.118684539975\n",
      "Epoch 0 step 231: training accuarcy: 0.7332000000000001\n",
      "Epoch 0 step 231: training loss: 7324.177195511819\n",
      "Epoch 0 step 232: training accuarcy: 0.7301000000000001\n",
      "Epoch 0 step 232: training loss: 7300.317816843738\n",
      "Epoch 0 step 233: training accuarcy: 0.7325\n",
      "Epoch 0 step 233: training loss: 7339.249850666358\n",
      "Epoch 0 step 234: training accuarcy: 0.727\n",
      "Epoch 0 step 234: training loss: 7310.923616100725\n",
      "Epoch 0 step 235: training accuarcy: 0.7296\n",
      "Epoch 0 step 235: training loss: 7333.834752100256\n",
      "Epoch 0 step 236: training accuarcy: 0.7261000000000001\n",
      "Epoch 0 step 236: training loss: 7279.732379409275\n",
      "Epoch 0 step 237: training accuarcy: 0.7322000000000001\n",
      "Epoch 0 step 237: training loss: 7315.391001746636\n",
      "Epoch 0 step 238: training accuarcy: 0.7274\n",
      "Epoch 0 step 238: training loss: 7196.640107886059\n",
      "Epoch 0 step 239: training accuarcy: 0.7371000000000001\n",
      "Epoch 0 step 239: training loss: 7117.966300569163\n",
      "Epoch 0 step 240: training accuarcy: 0.7402000000000001\n",
      "Epoch 0 step 240: training loss: 7250.355516813997\n",
      "Epoch 0 step 241: training accuarcy: 0.7295\n",
      "Epoch 0 step 241: training loss: 7227.251324390982\n",
      "Epoch 0 step 242: training accuarcy: 0.7275\n",
      "Epoch 0 step 242: training loss: 7315.182633785553\n",
      "Epoch 0 step 243: training accuarcy: 0.7207\n",
      "Epoch 0 step 243: training loss: 7234.261367957515\n",
      "Epoch 0 step 244: training accuarcy: 0.7259\n",
      "Epoch 0 step 244: training loss: 7153.762975554752\n",
      "Epoch 0 step 245: training accuarcy: 0.7321000000000001\n",
      "Epoch 0 step 245: training loss: 7193.955945844249\n",
      "Epoch 0 step 246: training accuarcy: 0.7336\n",
      "Epoch 0 step 246: training loss: 7134.343359239868\n",
      "Epoch 0 step 247: training accuarcy: 0.7307\n",
      "Epoch 0 step 247: training loss: 7159.700292994614\n",
      "Epoch 0 step 248: training accuarcy: 0.7334\n",
      "Epoch 0 step 248: training loss: 7164.547783798711\n",
      "Epoch 0 step 249: training accuarcy: 0.7364\n",
      "Epoch 0 step 249: training loss: 7205.138712471993\n",
      "Epoch 0 step 250: training accuarcy: 0.7248\n",
      "Epoch 0 step 250: training loss: 7123.868641201826\n",
      "Epoch 0 step 251: training accuarcy: 0.7291000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 251: training loss: 7070.205044751962\n",
      "Epoch 0 step 252: training accuarcy: 0.735\n",
      "Epoch 0 step 252: training loss: 7123.175271988205\n",
      "Epoch 0 step 253: training accuarcy: 0.7287\n",
      "Epoch 0 step 253: training loss: 7049.062276029311\n",
      "Epoch 0 step 254: training accuarcy: 0.7397\n",
      "Epoch 0 step 254: training loss: 7135.725203376929\n",
      "Epoch 0 step 255: training accuarcy: 0.7260000000000001\n",
      "Epoch 0 step 255: training loss: 7094.504852643588\n",
      "Epoch 0 step 256: training accuarcy: 0.7266\n",
      "Epoch 0 step 256: training loss: 7054.46104374724\n",
      "Epoch 0 step 257: training accuarcy: 0.736\n",
      "Epoch 0 step 257: training loss: 7040.11812783607\n",
      "Epoch 0 step 258: training accuarcy: 0.736\n",
      "Epoch 0 step 258: training loss: 7043.223295296297\n",
      "Epoch 0 step 259: training accuarcy: 0.7363000000000001\n",
      "Epoch 0 step 259: training loss: 7029.740564556023\n",
      "Epoch 0 step 260: training accuarcy: 0.7281000000000001\n",
      "Epoch 0 step 260: training loss: 6942.707432642393\n",
      "Epoch 0 step 261: training accuarcy: 0.7453000000000001\n",
      "Epoch 0 step 261: training loss: 7090.770141558777\n",
      "Epoch 0 step 262: training accuarcy: 0.7292000000000001\n",
      "Epoch 0 step 262: training loss: 3595.08122371733\n",
      "Epoch 0 step 263: training accuarcy: 0.7346153846153847\n",
      "Epoch 0: train loss 17946.159019611463, train accuarcy 0.6341165900230408\n",
      "Epoch 0: valid loss 6665.11699839934, valid accuarcy 0.7533862590789795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [04:58<09:57, 298.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 6885.240900244807\n",
      "Epoch 1 step 264: training accuarcy: 0.7539\n",
      "Epoch 1 step 264: training loss: 6910.169089475512\n",
      "Epoch 1 step 265: training accuarcy: 0.7402000000000001\n",
      "Epoch 1 step 265: training loss: 6827.067938014206\n",
      "Epoch 1 step 266: training accuarcy: 0.7537\n",
      "Epoch 1 step 266: training loss: 6866.531850690769\n",
      "Epoch 1 step 267: training accuarcy: 0.7532\n",
      "Epoch 1 step 267: training loss: 6874.006186583549\n",
      "Epoch 1 step 268: training accuarcy: 0.7482000000000001\n",
      "Epoch 1 step 268: training loss: 6873.974570958343\n",
      "Epoch 1 step 269: training accuarcy: 0.746\n",
      "Epoch 1 step 269: training loss: 6848.02075492916\n",
      "Epoch 1 step 270: training accuarcy: 0.751\n",
      "Epoch 1 step 270: training loss: 6797.149440197526\n",
      "Epoch 1 step 271: training accuarcy: 0.755\n",
      "Epoch 1 step 271: training loss: 6773.330958175871\n",
      "Epoch 1 step 272: training accuarcy: 0.752\n",
      "Epoch 1 step 272: training loss: 6838.876523278067\n",
      "Epoch 1 step 273: training accuarcy: 0.7529\n",
      "Epoch 1 step 273: training loss: 6784.719613821083\n",
      "Epoch 1 step 274: training accuarcy: 0.755\n",
      "Epoch 1 step 274: training loss: 6665.818359202679\n",
      "Epoch 1 step 275: training accuarcy: 0.764\n",
      "Epoch 1 step 275: training loss: 6820.44201359183\n",
      "Epoch 1 step 276: training accuarcy: 0.7492000000000001\n",
      "Epoch 1 step 276: training loss: 6794.319933842282\n",
      "Epoch 1 step 277: training accuarcy: 0.7528\n",
      "Epoch 1 step 277: training loss: 6673.159317029036\n",
      "Epoch 1 step 278: training accuarcy: 0.7592\n",
      "Epoch 1 step 278: training loss: 6730.521327949379\n",
      "Epoch 1 step 279: training accuarcy: 0.755\n",
      "Epoch 1 step 279: training loss: 6782.959764123532\n",
      "Epoch 1 step 280: training accuarcy: 0.7598\n",
      "Epoch 1 step 280: training loss: 6693.266813758164\n",
      "Epoch 1 step 281: training accuarcy: 0.7566\n",
      "Epoch 1 step 281: training loss: 6802.9329948008235\n",
      "Epoch 1 step 282: training accuarcy: 0.7502000000000001\n",
      "Epoch 1 step 282: training loss: 6738.083966855207\n",
      "Epoch 1 step 283: training accuarcy: 0.7538\n",
      "Epoch 1 step 283: training loss: 6646.076181030285\n",
      "Epoch 1 step 284: training accuarcy: 0.7642\n",
      "Epoch 1 step 284: training loss: 6745.569773123889\n",
      "Epoch 1 step 285: training accuarcy: 0.7515000000000001\n",
      "Epoch 1 step 285: training loss: 6632.409426059962\n",
      "Epoch 1 step 286: training accuarcy: 0.757\n",
      "Epoch 1 step 286: training loss: 6710.295246844119\n",
      "Epoch 1 step 287: training accuarcy: 0.7523000000000001\n",
      "Epoch 1 step 287: training loss: 6622.8580689674145\n",
      "Epoch 1 step 288: training accuarcy: 0.7678\n",
      "Epoch 1 step 288: training loss: 6721.554201269643\n",
      "Epoch 1 step 289: training accuarcy: 0.7513000000000001\n",
      "Epoch 1 step 289: training loss: 6644.941493876624\n",
      "Epoch 1 step 290: training accuarcy: 0.7588\n",
      "Epoch 1 step 290: training loss: 6635.675450867058\n",
      "Epoch 1 step 291: training accuarcy: 0.7659\n",
      "Epoch 1 step 291: training loss: 6652.871388323509\n",
      "Epoch 1 step 292: training accuarcy: 0.7575000000000001\n",
      "Epoch 1 step 292: training loss: 6595.4332973538085\n",
      "Epoch 1 step 293: training accuarcy: 0.7589\n",
      "Epoch 1 step 293: training loss: 6651.898398419771\n",
      "Epoch 1 step 294: training accuarcy: 0.7564000000000001\n",
      "Epoch 1 step 294: training loss: 6705.737216779398\n",
      "Epoch 1 step 295: training accuarcy: 0.7521\n",
      "Epoch 1 step 295: training loss: 6624.9085763242265\n",
      "Epoch 1 step 296: training accuarcy: 0.7534000000000001\n",
      "Epoch 1 step 296: training loss: 6652.453057647075\n",
      "Epoch 1 step 297: training accuarcy: 0.7522000000000001\n",
      "Epoch 1 step 297: training loss: 6594.083658156859\n",
      "Epoch 1 step 298: training accuarcy: 0.7582\n",
      "Epoch 1 step 298: training loss: 6642.307950108534\n",
      "Epoch 1 step 299: training accuarcy: 0.7591\n",
      "Epoch 1 step 299: training loss: 6650.381724882159\n",
      "Epoch 1 step 300: training accuarcy: 0.7477\n",
      "Epoch 1 step 300: training loss: 6596.743158419293\n",
      "Epoch 1 step 301: training accuarcy: 0.7555000000000001\n",
      "Epoch 1 step 301: training loss: 6627.575775561562\n",
      "Epoch 1 step 302: training accuarcy: 0.754\n",
      "Epoch 1 step 302: training loss: 6628.863512738992\n",
      "Epoch 1 step 303: training accuarcy: 0.7555000000000001\n",
      "Epoch 1 step 303: training loss: 6653.407839567099\n",
      "Epoch 1 step 304: training accuarcy: 0.7488\n",
      "Epoch 1 step 304: training loss: 6567.4170840327915\n",
      "Epoch 1 step 305: training accuarcy: 0.7576\n",
      "Epoch 1 step 305: training loss: 6477.988310822406\n",
      "Epoch 1 step 306: training accuarcy: 0.7668\n",
      "Epoch 1 step 306: training loss: 6554.3825813444\n",
      "Epoch 1 step 307: training accuarcy: 0.7538\n",
      "Epoch 1 step 307: training loss: 6557.385140765256\n",
      "Epoch 1 step 308: training accuarcy: 0.7592\n",
      "Epoch 1 step 308: training loss: 6595.447059091936\n",
      "Epoch 1 step 309: training accuarcy: 0.7554000000000001\n",
      "Epoch 1 step 309: training loss: 6607.446902981328\n",
      "Epoch 1 step 310: training accuarcy: 0.7468\n",
      "Epoch 1 step 310: training loss: 6511.5516561173945\n",
      "Epoch 1 step 311: training accuarcy: 0.7576\n",
      "Epoch 1 step 311: training loss: 6433.4929392253625\n",
      "Epoch 1 step 312: training accuarcy: 0.7646000000000001\n",
      "Epoch 1 step 312: training loss: 6569.165489213807\n",
      "Epoch 1 step 313: training accuarcy: 0.7538\n",
      "Epoch 1 step 313: training loss: 6595.2573580154985\n",
      "Epoch 1 step 314: training accuarcy: 0.7625000000000001\n",
      "Epoch 1 step 314: training loss: 6519.637037022101\n",
      "Epoch 1 step 315: training accuarcy: 0.7584000000000001\n",
      "Epoch 1 step 315: training loss: 6593.743073455791\n",
      "Epoch 1 step 316: training accuarcy: 0.7474000000000001\n",
      "Epoch 1 step 316: training loss: 6458.163120651381\n",
      "Epoch 1 step 317: training accuarcy: 0.7641\n",
      "Epoch 1 step 317: training loss: 6501.350124724488\n",
      "Epoch 1 step 318: training accuarcy: 0.7542\n",
      "Epoch 1 step 318: training loss: 6468.491613057602\n",
      "Epoch 1 step 319: training accuarcy: 0.7528\n",
      "Epoch 1 step 319: training loss: 6552.666227511117\n",
      "Epoch 1 step 320: training accuarcy: 0.7543000000000001\n",
      "Epoch 1 step 320: training loss: 6504.80110943728\n",
      "Epoch 1 step 321: training accuarcy: 0.7587\n",
      "Epoch 1 step 321: training loss: 6569.600125736917\n",
      "Epoch 1 step 322: training accuarcy: 0.7434000000000001\n",
      "Epoch 1 step 322: training loss: 6512.3664461564495\n",
      "Epoch 1 step 323: training accuarcy: 0.752\n",
      "Epoch 1 step 323: training loss: 6556.812171123369\n",
      "Epoch 1 step 324: training accuarcy: 0.7415\n",
      "Epoch 1 step 324: training loss: 6527.249687669813\n",
      "Epoch 1 step 325: training accuarcy: 0.7501\n",
      "Epoch 1 step 325: training loss: 6426.738206214518\n",
      "Epoch 1 step 326: training accuarcy: 0.7599\n",
      "Epoch 1 step 326: training loss: 6415.169382648252\n",
      "Epoch 1 step 327: training accuarcy: 0.7616\n",
      "Epoch 1 step 327: training loss: 6420.463948742004\n",
      "Epoch 1 step 328: training accuarcy: 0.7536\n",
      "Epoch 1 step 328: training loss: 6477.210214637685\n",
      "Epoch 1 step 329: training accuarcy: 0.7494000000000001\n",
      "Epoch 1 step 329: training loss: 6523.575230113651\n",
      "Epoch 1 step 330: training accuarcy: 0.752\n",
      "Epoch 1 step 330: training loss: 6484.151810517308\n",
      "Epoch 1 step 331: training accuarcy: 0.7579\n",
      "Epoch 1 step 331: training loss: 6376.84713277749\n",
      "Epoch 1 step 332: training accuarcy: 0.7723\n",
      "Epoch 1 step 332: training loss: 6426.538553004892\n",
      "Epoch 1 step 333: training accuarcy: 0.7606\n",
      "Epoch 1 step 333: training loss: 6443.816136498546\n",
      "Epoch 1 step 334: training accuarcy: 0.7515000000000001\n",
      "Epoch 1 step 334: training loss: 6438.300097521383\n",
      "Epoch 1 step 335: training accuarcy: 0.7535000000000001\n",
      "Epoch 1 step 335: training loss: 6434.317805024331\n",
      "Epoch 1 step 336: training accuarcy: 0.7585000000000001\n",
      "Epoch 1 step 336: training loss: 6411.882071197163\n",
      "Epoch 1 step 337: training accuarcy: 0.7549\n",
      "Epoch 1 step 337: training loss: 6490.990944939699\n",
      "Epoch 1 step 338: training accuarcy: 0.7502000000000001\n",
      "Epoch 1 step 338: training loss: 6518.358372919614\n",
      "Epoch 1 step 339: training accuarcy: 0.7451\n",
      "Epoch 1 step 339: training loss: 6370.7079575725475\n",
      "Epoch 1 step 340: training accuarcy: 0.7611\n",
      "Epoch 1 step 340: training loss: 6455.594583651163\n",
      "Epoch 1 step 341: training accuarcy: 0.7513000000000001\n",
      "Epoch 1 step 341: training loss: 6485.350135219094\n",
      "Epoch 1 step 342: training accuarcy: 0.7582\n",
      "Epoch 1 step 342: training loss: 6327.216295635918\n",
      "Epoch 1 step 343: training accuarcy: 0.766\n",
      "Epoch 1 step 343: training loss: 6558.128087995605\n",
      "Epoch 1 step 344: training accuarcy: 0.7331000000000001\n",
      "Epoch 1 step 344: training loss: 6428.367644934495\n",
      "Epoch 1 step 345: training accuarcy: 0.7501\n",
      "Epoch 1 step 345: training loss: 6384.90506159742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 346: training accuarcy: 0.7553000000000001\n",
      "Epoch 1 step 346: training loss: 6379.780847168366\n",
      "Epoch 1 step 347: training accuarcy: 0.7596\n",
      "Epoch 1 step 347: training loss: 6270.408468812926\n",
      "Epoch 1 step 348: training accuarcy: 0.7599\n",
      "Epoch 1 step 348: training loss: 6390.783819778589\n",
      "Epoch 1 step 349: training accuarcy: 0.7516\n",
      "Epoch 1 step 349: training loss: 6346.040841853497\n",
      "Epoch 1 step 350: training accuarcy: 0.757\n",
      "Epoch 1 step 350: training loss: 6295.501941149474\n",
      "Epoch 1 step 351: training accuarcy: 0.7647\n",
      "Epoch 1 step 351: training loss: 6334.770321606702\n",
      "Epoch 1 step 352: training accuarcy: 0.7617\n",
      "Epoch 1 step 352: training loss: 6298.894602969466\n",
      "Epoch 1 step 353: training accuarcy: 0.7624000000000001\n",
      "Epoch 1 step 353: training loss: 6235.2999397220865\n",
      "Epoch 1 step 354: training accuarcy: 0.7725000000000001\n",
      "Epoch 1 step 354: training loss: 6282.857176051414\n",
      "Epoch 1 step 355: training accuarcy: 0.7579\n",
      "Epoch 1 step 355: training loss: 6380.244907507589\n",
      "Epoch 1 step 356: training accuarcy: 0.7512000000000001\n",
      "Epoch 1 step 356: training loss: 6314.115000665006\n",
      "Epoch 1 step 357: training accuarcy: 0.7559\n",
      "Epoch 1 step 357: training loss: 6337.071581008109\n",
      "Epoch 1 step 358: training accuarcy: 0.7554000000000001\n",
      "Epoch 1 step 358: training loss: 6286.534335830082\n",
      "Epoch 1 step 359: training accuarcy: 0.7619\n",
      "Epoch 1 step 359: training loss: 6328.061616975248\n",
      "Epoch 1 step 360: training accuarcy: 0.7634000000000001\n",
      "Epoch 1 step 360: training loss: 6308.022903546738\n",
      "Epoch 1 step 361: training accuarcy: 0.7558\n",
      "Epoch 1 step 361: training loss: 6382.802940930191\n",
      "Epoch 1 step 362: training accuarcy: 0.753\n",
      "Epoch 1 step 362: training loss: 6383.098359411076\n",
      "Epoch 1 step 363: training accuarcy: 0.7486\n",
      "Epoch 1 step 363: training loss: 6387.7149725316085\n",
      "Epoch 1 step 364: training accuarcy: 0.7538\n",
      "Epoch 1 step 364: training loss: 6332.350637898428\n",
      "Epoch 1 step 365: training accuarcy: 0.7564000000000001\n",
      "Epoch 1 step 365: training loss: 6273.151221279669\n",
      "Epoch 1 step 366: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 366: training loss: 6321.566969351265\n",
      "Epoch 1 step 367: training accuarcy: 0.7546\n",
      "Epoch 1 step 367: training loss: 6302.183710223294\n",
      "Epoch 1 step 368: training accuarcy: 0.7504000000000001\n",
      "Epoch 1 step 368: training loss: 6178.8613034519085\n",
      "Epoch 1 step 369: training accuarcy: 0.7686000000000001\n",
      "Epoch 1 step 369: training loss: 6286.965879398284\n",
      "Epoch 1 step 370: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 370: training loss: 6217.738775722481\n",
      "Epoch 1 step 371: training accuarcy: 0.7701\n",
      "Epoch 1 step 371: training loss: 6282.132628676468\n",
      "Epoch 1 step 372: training accuarcy: 0.7506\n",
      "Epoch 1 step 372: training loss: 6353.584985054027\n",
      "Epoch 1 step 373: training accuarcy: 0.7492000000000001\n",
      "Epoch 1 step 373: training loss: 6195.705231838405\n",
      "Epoch 1 step 374: training accuarcy: 0.7651\n",
      "Epoch 1 step 374: training loss: 6242.333044544801\n",
      "Epoch 1 step 375: training accuarcy: 0.7577\n",
      "Epoch 1 step 375: training loss: 6311.530015052317\n",
      "Epoch 1 step 376: training accuarcy: 0.7626000000000001\n",
      "Epoch 1 step 376: training loss: 6247.271692071251\n",
      "Epoch 1 step 377: training accuarcy: 0.76\n",
      "Epoch 1 step 377: training loss: 6257.583693381056\n",
      "Epoch 1 step 378: training accuarcy: 0.7588\n",
      "Epoch 1 step 378: training loss: 6291.842114146872\n",
      "Epoch 1 step 379: training accuarcy: 0.7587\n",
      "Epoch 1 step 379: training loss: 6312.528942275522\n",
      "Epoch 1 step 380: training accuarcy: 0.7513000000000001\n",
      "Epoch 1 step 380: training loss: 6217.9326763507015\n",
      "Epoch 1 step 381: training accuarcy: 0.7663000000000001\n",
      "Epoch 1 step 381: training loss: 6344.686620432117\n",
      "Epoch 1 step 382: training accuarcy: 0.7569\n",
      "Epoch 1 step 382: training loss: 6205.227695378686\n",
      "Epoch 1 step 383: training accuarcy: 0.7587\n",
      "Epoch 1 step 383: training loss: 6271.4142400582805\n",
      "Epoch 1 step 384: training accuarcy: 0.7579\n",
      "Epoch 1 step 384: training loss: 6214.181258426969\n",
      "Epoch 1 step 385: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 385: training loss: 6146.519757093018\n",
      "Epoch 1 step 386: training accuarcy: 0.7618\n",
      "Epoch 1 step 386: training loss: 6214.4789101863435\n",
      "Epoch 1 step 387: training accuarcy: 0.7636000000000001\n",
      "Epoch 1 step 387: training loss: 6296.54264579803\n",
      "Epoch 1 step 388: training accuarcy: 0.7526\n",
      "Epoch 1 step 388: training loss: 6208.8644054596925\n",
      "Epoch 1 step 389: training accuarcy: 0.7588\n",
      "Epoch 1 step 389: training loss: 6212.009847116752\n",
      "Epoch 1 step 390: training accuarcy: 0.757\n",
      "Epoch 1 step 390: training loss: 6287.420977150794\n",
      "Epoch 1 step 391: training accuarcy: 0.7512000000000001\n",
      "Epoch 1 step 391: training loss: 6226.470773699039\n",
      "Epoch 1 step 392: training accuarcy: 0.753\n",
      "Epoch 1 step 392: training loss: 6219.687477141835\n",
      "Epoch 1 step 393: training accuarcy: 0.7534000000000001\n",
      "Epoch 1 step 393: training loss: 6166.098982846391\n",
      "Epoch 1 step 394: training accuarcy: 0.7598\n",
      "Epoch 1 step 394: training loss: 6156.640488956675\n",
      "Epoch 1 step 395: training accuarcy: 0.7645000000000001\n",
      "Epoch 1 step 395: training loss: 6245.305274852968\n",
      "Epoch 1 step 396: training accuarcy: 0.7515000000000001\n",
      "Epoch 1 step 396: training loss: 6274.105376561825\n",
      "Epoch 1 step 397: training accuarcy: 0.754\n",
      "Epoch 1 step 397: training loss: 6238.2386028180945\n",
      "Epoch 1 step 398: training accuarcy: 0.7544000000000001\n",
      "Epoch 1 step 398: training loss: 6217.708651379846\n",
      "Epoch 1 step 399: training accuarcy: 0.7555000000000001\n",
      "Epoch 1 step 399: training loss: 6276.0630457507295\n",
      "Epoch 1 step 400: training accuarcy: 0.7522000000000001\n",
      "Epoch 1 step 400: training loss: 6231.501140991819\n",
      "Epoch 1 step 401: training accuarcy: 0.7552\n",
      "Epoch 1 step 401: training loss: 6226.452408116874\n",
      "Epoch 1 step 402: training accuarcy: 0.751\n",
      "Epoch 1 step 402: training loss: 6118.0137019863\n",
      "Epoch 1 step 403: training accuarcy: 0.7578\n",
      "Epoch 1 step 403: training loss: 6152.396964870749\n",
      "Epoch 1 step 404: training accuarcy: 0.7703\n",
      "Epoch 1 step 404: training loss: 6287.2435678076845\n",
      "Epoch 1 step 405: training accuarcy: 0.7469\n",
      "Epoch 1 step 405: training loss: 6238.245365345569\n",
      "Epoch 1 step 406: training accuarcy: 0.7588\n",
      "Epoch 1 step 406: training loss: 6189.083906187621\n",
      "Epoch 1 step 407: training accuarcy: 0.7556\n",
      "Epoch 1 step 407: training loss: 6158.953843600071\n",
      "Epoch 1 step 408: training accuarcy: 0.7623000000000001\n",
      "Epoch 1 step 408: training loss: 6086.814724689488\n",
      "Epoch 1 step 409: training accuarcy: 0.7647\n",
      "Epoch 1 step 409: training loss: 6232.324900566261\n",
      "Epoch 1 step 410: training accuarcy: 0.7611\n",
      "Epoch 1 step 410: training loss: 6130.158167076594\n",
      "Epoch 1 step 411: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 411: training loss: 6182.201478473038\n",
      "Epoch 1 step 412: training accuarcy: 0.7605000000000001\n",
      "Epoch 1 step 412: training loss: 6141.027629296505\n",
      "Epoch 1 step 413: training accuarcy: 0.7584000000000001\n",
      "Epoch 1 step 413: training loss: 6129.336220068671\n",
      "Epoch 1 step 414: training accuarcy: 0.7629\n",
      "Epoch 1 step 414: training loss: 6188.72619210641\n",
      "Epoch 1 step 415: training accuarcy: 0.7544000000000001\n",
      "Epoch 1 step 415: training loss: 6169.425239766167\n",
      "Epoch 1 step 416: training accuarcy: 0.7604000000000001\n",
      "Epoch 1 step 416: training loss: 6087.714512409379\n",
      "Epoch 1 step 417: training accuarcy: 0.7698\n",
      "Epoch 1 step 417: training loss: 6139.147445262295\n",
      "Epoch 1 step 418: training accuarcy: 0.7612\n",
      "Epoch 1 step 418: training loss: 6119.7628091704855\n",
      "Epoch 1 step 419: training accuarcy: 0.7604000000000001\n",
      "Epoch 1 step 419: training loss: 6161.087088777498\n",
      "Epoch 1 step 420: training accuarcy: 0.7608\n",
      "Epoch 1 step 420: training loss: 6135.922484931046\n",
      "Epoch 1 step 421: training accuarcy: 0.754\n",
      "Epoch 1 step 421: training loss: 6204.661250048467\n",
      "Epoch 1 step 422: training accuarcy: 0.7619\n",
      "Epoch 1 step 422: training loss: 6082.180813206692\n",
      "Epoch 1 step 423: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 423: training loss: 6072.055071003184\n",
      "Epoch 1 step 424: training accuarcy: 0.7612\n",
      "Epoch 1 step 424: training loss: 6046.810030649888\n",
      "Epoch 1 step 425: training accuarcy: 0.767\n",
      "Epoch 1 step 425: training loss: 6161.48769299169\n",
      "Epoch 1 step 426: training accuarcy: 0.7577\n",
      "Epoch 1 step 426: training loss: 6252.735651018296\n",
      "Epoch 1 step 427: training accuarcy: 0.7476\n",
      "Epoch 1 step 427: training loss: 6276.873009856103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 428: training accuarcy: 0.7531\n",
      "Epoch 1 step 428: training loss: 6128.00307103102\n",
      "Epoch 1 step 429: training accuarcy: 0.7668\n",
      "Epoch 1 step 429: training loss: 6077.9095652357355\n",
      "Epoch 1 step 430: training accuarcy: 0.7649\n",
      "Epoch 1 step 430: training loss: 6149.935452297881\n",
      "Epoch 1 step 431: training accuarcy: 0.7514000000000001\n",
      "Epoch 1 step 431: training loss: 6125.839192099744\n",
      "Epoch 1 step 432: training accuarcy: 0.7543000000000001\n",
      "Epoch 1 step 432: training loss: 6011.6386151408315\n",
      "Epoch 1 step 433: training accuarcy: 0.7726000000000001\n",
      "Epoch 1 step 433: training loss: 6176.693484908564\n",
      "Epoch 1 step 434: training accuarcy: 0.7558\n",
      "Epoch 1 step 434: training loss: 6093.5912295455855\n",
      "Epoch 1 step 435: training accuarcy: 0.7616\n",
      "Epoch 1 step 435: training loss: 6135.654376939337\n",
      "Epoch 1 step 436: training accuarcy: 0.7514000000000001\n",
      "Epoch 1 step 436: training loss: 6089.894422652479\n",
      "Epoch 1 step 437: training accuarcy: 0.7608\n",
      "Epoch 1 step 437: training loss: 6134.979924521758\n",
      "Epoch 1 step 438: training accuarcy: 0.7536\n",
      "Epoch 1 step 438: training loss: 6033.490396577712\n",
      "Epoch 1 step 439: training accuarcy: 0.7663000000000001\n",
      "Epoch 1 step 439: training loss: 6037.188439447737\n",
      "Epoch 1 step 440: training accuarcy: 0.7598\n",
      "Epoch 1 step 440: training loss: 6172.289311824597\n",
      "Epoch 1 step 441: training accuarcy: 0.7468\n",
      "Epoch 1 step 441: training loss: 6175.220169563724\n",
      "Epoch 1 step 442: training accuarcy: 0.7526\n",
      "Epoch 1 step 442: training loss: 6243.633505214623\n",
      "Epoch 1 step 443: training accuarcy: 0.7452000000000001\n",
      "Epoch 1 step 443: training loss: 6062.246089914899\n",
      "Epoch 1 step 444: training accuarcy: 0.7696000000000001\n",
      "Epoch 1 step 444: training loss: 6022.787313644454\n",
      "Epoch 1 step 445: training accuarcy: 0.7659\n",
      "Epoch 1 step 445: training loss: 6111.859163580151\n",
      "Epoch 1 step 446: training accuarcy: 0.7495\n",
      "Epoch 1 step 446: training loss: 6018.35287586497\n",
      "Epoch 1 step 447: training accuarcy: 0.7755000000000001\n",
      "Epoch 1 step 447: training loss: 6117.874992505921\n",
      "Epoch 1 step 448: training accuarcy: 0.7608\n",
      "Epoch 1 step 448: training loss: 6107.422867416269\n",
      "Epoch 1 step 449: training accuarcy: 0.7614000000000001\n",
      "Epoch 1 step 449: training loss: 6102.221039918822\n",
      "Epoch 1 step 450: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 450: training loss: 5991.3672673565125\n",
      "Epoch 1 step 451: training accuarcy: 0.7726000000000001\n",
      "Epoch 1 step 451: training loss: 6084.576031986974\n",
      "Epoch 1 step 452: training accuarcy: 0.7605000000000001\n",
      "Epoch 1 step 452: training loss: 6067.125139513765\n",
      "Epoch 1 step 453: training accuarcy: 0.7532\n",
      "Epoch 1 step 453: training loss: 6159.465743253519\n",
      "Epoch 1 step 454: training accuarcy: 0.7547\n",
      "Epoch 1 step 454: training loss: 6072.993103484774\n",
      "Epoch 1 step 455: training accuarcy: 0.7668\n",
      "Epoch 1 step 455: training loss: 6107.329512273942\n",
      "Epoch 1 step 456: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 456: training loss: 6043.392276133396\n",
      "Epoch 1 step 457: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 457: training loss: 5956.329397664495\n",
      "Epoch 1 step 458: training accuarcy: 0.774\n",
      "Epoch 1 step 458: training loss: 6028.6958064217215\n",
      "Epoch 1 step 459: training accuarcy: 0.7636000000000001\n",
      "Epoch 1 step 459: training loss: 6094.410690505279\n",
      "Epoch 1 step 460: training accuarcy: 0.763\n",
      "Epoch 1 step 460: training loss: 6071.909099305267\n",
      "Epoch 1 step 461: training accuarcy: 0.7472000000000001\n",
      "Epoch 1 step 461: training loss: 6105.182276757263\n",
      "Epoch 1 step 462: training accuarcy: 0.7536\n",
      "Epoch 1 step 462: training loss: 6079.071331965279\n",
      "Epoch 1 step 463: training accuarcy: 0.7594000000000001\n",
      "Epoch 1 step 463: training loss: 6083.223328952956\n",
      "Epoch 1 step 464: training accuarcy: 0.7532\n",
      "Epoch 1 step 464: training loss: 6043.008086005813\n",
      "Epoch 1 step 465: training accuarcy: 0.7574000000000001\n",
      "Epoch 1 step 465: training loss: 6126.3304135420485\n",
      "Epoch 1 step 466: training accuarcy: 0.7533000000000001\n",
      "Epoch 1 step 466: training loss: 5984.590144204087\n",
      "Epoch 1 step 467: training accuarcy: 0.7708\n",
      "Epoch 1 step 467: training loss: 6118.749287674886\n",
      "Epoch 1 step 468: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 468: training loss: 5983.125429363603\n",
      "Epoch 1 step 469: training accuarcy: 0.7642\n",
      "Epoch 1 step 469: training loss: 6052.012059966467\n",
      "Epoch 1 step 470: training accuarcy: 0.7523000000000001\n",
      "Epoch 1 step 470: training loss: 6104.1070253545\n",
      "Epoch 1 step 471: training accuarcy: 0.761\n",
      "Epoch 1 step 471: training loss: 6004.393432786013\n",
      "Epoch 1 step 472: training accuarcy: 0.7622\n",
      "Epoch 1 step 472: training loss: 6061.712419464632\n",
      "Epoch 1 step 473: training accuarcy: 0.7601\n",
      "Epoch 1 step 473: training loss: 6063.858344088501\n",
      "Epoch 1 step 474: training accuarcy: 0.7578\n",
      "Epoch 1 step 474: training loss: 5958.03616715758\n",
      "Epoch 1 step 475: training accuarcy: 0.7634000000000001\n",
      "Epoch 1 step 475: training loss: 5995.494213805039\n",
      "Epoch 1 step 476: training accuarcy: 0.7686000000000001\n",
      "Epoch 1 step 476: training loss: 6110.752549880179\n",
      "Epoch 1 step 477: training accuarcy: 0.7481\n",
      "Epoch 1 step 477: training loss: 6041.60275885004\n",
      "Epoch 1 step 478: training accuarcy: 0.7532\n",
      "Epoch 1 step 478: training loss: 6082.267982326284\n",
      "Epoch 1 step 479: training accuarcy: 0.7571\n",
      "Epoch 1 step 479: training loss: 6014.332923339658\n",
      "Epoch 1 step 480: training accuarcy: 0.7576\n",
      "Epoch 1 step 480: training loss: 5971.309032445537\n",
      "Epoch 1 step 481: training accuarcy: 0.7605000000000001\n",
      "Epoch 1 step 481: training loss: 6049.402010047219\n",
      "Epoch 1 step 482: training accuarcy: 0.7541\n",
      "Epoch 1 step 482: training loss: 5983.271308099788\n",
      "Epoch 1 step 483: training accuarcy: 0.7619\n",
      "Epoch 1 step 483: training loss: 6021.443128763514\n",
      "Epoch 1 step 484: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 484: training loss: 6055.491609509269\n",
      "Epoch 1 step 485: training accuarcy: 0.763\n",
      "Epoch 1 step 485: training loss: 6059.030646367455\n",
      "Epoch 1 step 486: training accuarcy: 0.7496\n",
      "Epoch 1 step 486: training loss: 5952.483369732544\n",
      "Epoch 1 step 487: training accuarcy: 0.7619\n",
      "Epoch 1 step 487: training loss: 5987.196145581793\n",
      "Epoch 1 step 488: training accuarcy: 0.7572\n",
      "Epoch 1 step 488: training loss: 6037.3771080858705\n",
      "Epoch 1 step 489: training accuarcy: 0.756\n",
      "Epoch 1 step 489: training loss: 5937.434454503695\n",
      "Epoch 1 step 490: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 490: training loss: 6114.349004783903\n",
      "Epoch 1 step 491: training accuarcy: 0.7492000000000001\n",
      "Epoch 1 step 491: training loss: 5933.673167918798\n",
      "Epoch 1 step 492: training accuarcy: 0.7679\n",
      "Epoch 1 step 492: training loss: 6080.828496631746\n",
      "Epoch 1 step 493: training accuarcy: 0.7606\n",
      "Epoch 1 step 493: training loss: 5880.87437258961\n",
      "Epoch 1 step 494: training accuarcy: 0.7675000000000001\n",
      "Epoch 1 step 494: training loss: 5992.846724897012\n",
      "Epoch 1 step 495: training accuarcy: 0.7642\n",
      "Epoch 1 step 495: training loss: 5948.795931924228\n",
      "Epoch 1 step 496: training accuarcy: 0.7605000000000001\n",
      "Epoch 1 step 496: training loss: 5928.727912697982\n",
      "Epoch 1 step 497: training accuarcy: 0.7652\n",
      "Epoch 1 step 497: training loss: 5888.629070988702\n",
      "Epoch 1 step 498: training accuarcy: 0.7685000000000001\n",
      "Epoch 1 step 498: training loss: 6059.498948562832\n",
      "Epoch 1 step 499: training accuarcy: 0.7543000000000001\n",
      "Epoch 1 step 499: training loss: 5944.374928305806\n",
      "Epoch 1 step 500: training accuarcy: 0.7648\n",
      "Epoch 1 step 500: training loss: 6025.467050612167\n",
      "Epoch 1 step 501: training accuarcy: 0.7626000000000001\n",
      "Epoch 1 step 501: training loss: 6115.969204010462\n",
      "Epoch 1 step 502: training accuarcy: 0.7567\n",
      "Epoch 1 step 502: training loss: 6037.701683047164\n",
      "Epoch 1 step 503: training accuarcy: 0.7577\n",
      "Epoch 1 step 503: training loss: 6017.695677603451\n",
      "Epoch 1 step 504: training accuarcy: 0.7514000000000001\n",
      "Epoch 1 step 504: training loss: 5976.060424400059\n",
      "Epoch 1 step 505: training accuarcy: 0.7551\n",
      "Epoch 1 step 505: training loss: 6053.797437207459\n",
      "Epoch 1 step 506: training accuarcy: 0.7504000000000001\n",
      "Epoch 1 step 506: training loss: 5973.8168386438\n",
      "Epoch 1 step 507: training accuarcy: 0.7594000000000001\n",
      "Epoch 1 step 507: training loss: 5968.312823899055\n",
      "Epoch 1 step 508: training accuarcy: 0.7582\n",
      "Epoch 1 step 508: training loss: 5999.998581138025\n",
      "Epoch 1 step 509: training accuarcy: 0.7515000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 509: training loss: 5986.656883127622\n",
      "Epoch 1 step 510: training accuarcy: 0.766\n",
      "Epoch 1 step 510: training loss: 5901.297342055963\n",
      "Epoch 1 step 511: training accuarcy: 0.7558\n",
      "Epoch 1 step 511: training loss: 5986.763701278865\n",
      "Epoch 1 step 512: training accuarcy: 0.7616\n",
      "Epoch 1 step 512: training loss: 5976.2362199517465\n",
      "Epoch 1 step 513: training accuarcy: 0.7583000000000001\n",
      "Epoch 1 step 513: training loss: 6022.994422663855\n",
      "Epoch 1 step 514: training accuarcy: 0.7537\n",
      "Epoch 1 step 514: training loss: 5957.289298435262\n",
      "Epoch 1 step 515: training accuarcy: 0.7581\n",
      "Epoch 1 step 515: training loss: 5956.950900864045\n",
      "Epoch 1 step 516: training accuarcy: 0.7646000000000001\n",
      "Epoch 1 step 516: training loss: 5960.786510733158\n",
      "Epoch 1 step 517: training accuarcy: 0.7622\n",
      "Epoch 1 step 517: training loss: 5952.906611048345\n",
      "Epoch 1 step 518: training accuarcy: 0.7564000000000001\n",
      "Epoch 1 step 518: training loss: 6053.882163732641\n",
      "Epoch 1 step 519: training accuarcy: 0.7539\n",
      "Epoch 1 step 519: training loss: 5995.0014970622215\n",
      "Epoch 1 step 520: training accuarcy: 0.7589\n",
      "Epoch 1 step 520: training loss: 5968.782260305745\n",
      "Epoch 1 step 521: training accuarcy: 0.7524000000000001\n",
      "Epoch 1 step 521: training loss: 6007.705903209573\n",
      "Epoch 1 step 522: training accuarcy: 0.7577\n",
      "Epoch 1 step 522: training loss: 6006.841859524245\n",
      "Epoch 1 step 523: training accuarcy: 0.754\n",
      "Epoch 1 step 523: training loss: 5914.422531956225\n",
      "Epoch 1 step 524: training accuarcy: 0.7675000000000001\n",
      "Epoch 1 step 524: training loss: 6009.719673966284\n",
      "Epoch 1 step 525: training accuarcy: 0.7515000000000001\n",
      "Epoch 1 step 525: training loss: 2688.1074380111786\n",
      "Epoch 1 step 526: training accuarcy: 0.7433333333333333\n",
      "Epoch 1: train loss 6266.544985319913, train accuarcy 0.7382861971855164\n",
      "Epoch 1: valid loss 5714.975720779306, valid accuarcy 0.7776635885238647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [09:54<04:57, 297.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 5959.227442539103\n",
      "Epoch 2 step 527: training accuarcy: 0.766\n",
      "Epoch 2 step 527: training loss: 5811.93729502977\n",
      "Epoch 2 step 528: training accuarcy: 0.7775000000000001\n",
      "Epoch 2 step 528: training loss: 5794.827736664424\n",
      "Epoch 2 step 529: training accuarcy: 0.7781\n",
      "Epoch 2 step 529: training loss: 5927.356809450854\n",
      "Epoch 2 step 530: training accuarcy: 0.7644000000000001\n",
      "Epoch 2 step 530: training loss: 5839.781489082489\n",
      "Epoch 2 step 531: training accuarcy: 0.774\n",
      "Epoch 2 step 531: training loss: 5938.867081760262\n",
      "Epoch 2 step 532: training accuarcy: 0.7657\n",
      "Epoch 2 step 532: training loss: 5874.202767021185\n",
      "Epoch 2 step 533: training accuarcy: 0.7662\n",
      "Epoch 2 step 533: training loss: 5907.050374286284\n",
      "Epoch 2 step 534: training accuarcy: 0.7687\n",
      "Epoch 2 step 534: training loss: 5866.3597528698665\n",
      "Epoch 2 step 535: training accuarcy: 0.7724000000000001\n",
      "Epoch 2 step 535: training loss: 5973.441046847933\n",
      "Epoch 2 step 536: training accuarcy: 0.7514000000000001\n",
      "Epoch 2 step 536: training loss: 5835.804987198678\n",
      "Epoch 2 step 537: training accuarcy: 0.7781\n",
      "Epoch 2 step 537: training loss: 5842.167612288875\n",
      "Epoch 2 step 538: training accuarcy: 0.7667\n",
      "Epoch 2 step 538: training loss: 5857.300634895049\n",
      "Epoch 2 step 539: training accuarcy: 0.7636000000000001\n",
      "Epoch 2 step 539: training loss: 5963.525005803991\n",
      "Epoch 2 step 540: training accuarcy: 0.7614000000000001\n",
      "Epoch 2 step 540: training loss: 5874.685715957015\n",
      "Epoch 2 step 541: training accuarcy: 0.7717\n",
      "Epoch 2 step 541: training loss: 5805.047320010137\n",
      "Epoch 2 step 542: training accuarcy: 0.7771\n",
      "Epoch 2 step 542: training loss: 5840.627926516457\n",
      "Epoch 2 step 543: training accuarcy: 0.7688\n",
      "Epoch 2 step 543: training loss: 5863.394255168947\n",
      "Epoch 2 step 544: training accuarcy: 0.7665000000000001\n",
      "Epoch 2 step 544: training loss: 5912.437538025293\n",
      "Epoch 2 step 545: training accuarcy: 0.7642\n",
      "Epoch 2 step 545: training loss: 5720.224472568938\n",
      "Epoch 2 step 546: training accuarcy: 0.7745000000000001\n",
      "Epoch 2 step 546: training loss: 5915.260388741585\n",
      "Epoch 2 step 547: training accuarcy: 0.7611\n",
      "Epoch 2 step 547: training loss: 5839.99490565896\n",
      "Epoch 2 step 548: training accuarcy: 0.7677\n",
      "Epoch 2 step 548: training loss: 5863.385139141824\n",
      "Epoch 2 step 549: training accuarcy: 0.7692\n",
      "Epoch 2 step 549: training loss: 5868.677339731859\n",
      "Epoch 2 step 550: training accuarcy: 0.7695000000000001\n",
      "Epoch 2 step 550: training loss: 5827.268663170941\n",
      "Epoch 2 step 551: training accuarcy: 0.7747\n",
      "Epoch 2 step 551: training loss: 5842.479754770067\n",
      "Epoch 2 step 552: training accuarcy: 0.7734000000000001\n",
      "Epoch 2 step 552: training loss: 5974.190365145607\n",
      "Epoch 2 step 553: training accuarcy: 0.759\n",
      "Epoch 2 step 553: training loss: 5862.857978779395\n",
      "Epoch 2 step 554: training accuarcy: 0.7739\n",
      "Epoch 2 step 554: training loss: 5970.98569537105\n",
      "Epoch 2 step 555: training accuarcy: 0.7705000000000001\n",
      "Epoch 2 step 555: training loss: 5756.612058403713\n",
      "Epoch 2 step 556: training accuarcy: 0.7789\n",
      "Epoch 2 step 556: training loss: 5884.679326751275\n",
      "Epoch 2 step 557: training accuarcy: 0.765\n",
      "Epoch 2 step 557: training loss: 5847.731847736059\n",
      "Epoch 2 step 558: training accuarcy: 0.7699\n",
      "Epoch 2 step 558: training loss: 5917.314721889248\n",
      "Epoch 2 step 559: training accuarcy: 0.7598\n",
      "Epoch 2 step 559: training loss: 5860.072704777159\n",
      "Epoch 2 step 560: training accuarcy: 0.7633000000000001\n",
      "Epoch 2 step 560: training loss: 5840.169594364149\n",
      "Epoch 2 step 561: training accuarcy: 0.7672\n",
      "Epoch 2 step 561: training loss: 5938.776339578312\n",
      "Epoch 2 step 562: training accuarcy: 0.7554000000000001\n",
      "Epoch 2 step 562: training loss: 5847.011234397936\n",
      "Epoch 2 step 563: training accuarcy: 0.7647\n",
      "Epoch 2 step 563: training loss: 5792.2507689428185\n",
      "Epoch 2 step 564: training accuarcy: 0.7731\n",
      "Epoch 2 step 564: training loss: 5869.602159288369\n",
      "Epoch 2 step 565: training accuarcy: 0.771\n",
      "Epoch 2 step 565: training loss: 5793.487502005469\n",
      "Epoch 2 step 566: training accuarcy: 0.7695000000000001\n",
      "Epoch 2 step 566: training loss: 5828.050903494404\n",
      "Epoch 2 step 567: training accuarcy: 0.765\n",
      "Epoch 2 step 567: training loss: 5851.211792519518\n",
      "Epoch 2 step 568: training accuarcy: 0.7609\n",
      "Epoch 2 step 568: training loss: 5955.545700798276\n",
      "Epoch 2 step 569: training accuarcy: 0.7604000000000001\n",
      "Epoch 2 step 569: training loss: 5928.193537599889\n",
      "Epoch 2 step 570: training accuarcy: 0.7631\n",
      "Epoch 2 step 570: training loss: 5919.879015266545\n",
      "Epoch 2 step 571: training accuarcy: 0.7633000000000001\n",
      "Epoch 2 step 571: training loss: 5858.036918178573\n",
      "Epoch 2 step 572: training accuarcy: 0.7679\n",
      "Epoch 2 step 572: training loss: 5910.530942724293\n",
      "Epoch 2 step 573: training accuarcy: 0.7679\n",
      "Epoch 2 step 573: training loss: 5894.021085027293\n",
      "Epoch 2 step 574: training accuarcy: 0.7658\n",
      "Epoch 2 step 574: training loss: 5859.632697003737\n",
      "Epoch 2 step 575: training accuarcy: 0.7634000000000001\n",
      "Epoch 2 step 575: training loss: 5845.901400575083\n",
      "Epoch 2 step 576: training accuarcy: 0.7629\n",
      "Epoch 2 step 576: training loss: 5913.747720890184\n",
      "Epoch 2 step 577: training accuarcy: 0.7559\n",
      "Epoch 2 step 577: training loss: 5762.659370356464\n",
      "Epoch 2 step 578: training accuarcy: 0.7704000000000001\n",
      "Epoch 2 step 578: training loss: 5837.132986734583\n",
      "Epoch 2 step 579: training accuarcy: 0.7792\n",
      "Epoch 2 step 579: training loss: 5895.761340293151\n",
      "Epoch 2 step 580: training accuarcy: 0.7613000000000001\n",
      "Epoch 2 step 580: training loss: 5721.490565666973\n",
      "Epoch 2 step 581: training accuarcy: 0.7821\n",
      "Epoch 2 step 581: training loss: 5807.4156741420065\n",
      "Epoch 2 step 582: training accuarcy: 0.7687\n",
      "Epoch 2 step 582: training loss: 5936.764909548585\n",
      "Epoch 2 step 583: training accuarcy: 0.763\n",
      "Epoch 2 step 583: training loss: 5988.479808858453\n",
      "Epoch 2 step 584: training accuarcy: 0.7472000000000001\n",
      "Epoch 2 step 584: training loss: 5928.621251965804\n",
      "Epoch 2 step 585: training accuarcy: 0.7533000000000001\n",
      "Epoch 2 step 585: training loss: 5946.1192267371625\n",
      "Epoch 2 step 586: training accuarcy: 0.7711\n",
      "Epoch 2 step 586: training loss: 5835.804025437777\n",
      "Epoch 2 step 587: training accuarcy: 0.7648\n",
      "Epoch 2 step 587: training loss: 5894.328518351845\n",
      "Epoch 2 step 588: training accuarcy: 0.758\n",
      "Epoch 2 step 588: training loss: 5909.846156823894\n",
      "Epoch 2 step 589: training accuarcy: 0.7559\n",
      "Epoch 2 step 589: training loss: 5842.602381478644\n",
      "Epoch 2 step 590: training accuarcy: 0.7661\n",
      "Epoch 2 step 590: training loss: 5775.560702969198\n",
      "Epoch 2 step 591: training accuarcy: 0.7758\n",
      "Epoch 2 step 591: training loss: 5827.931212202064\n",
      "Epoch 2 step 592: training accuarcy: 0.7692\n",
      "Epoch 2 step 592: training loss: 5897.953498485228\n",
      "Epoch 2 step 593: training accuarcy: 0.7577\n",
      "Epoch 2 step 593: training loss: 5847.783385627956\n",
      "Epoch 2 step 594: training accuarcy: 0.7679\n",
      "Epoch 2 step 594: training loss: 5843.4650164733575\n",
      "Epoch 2 step 595: training accuarcy: 0.7719\n",
      "Epoch 2 step 595: training loss: 5895.240184403866\n",
      "Epoch 2 step 596: training accuarcy: 0.7633000000000001\n",
      "Epoch 2 step 596: training loss: 5738.933420300903\n",
      "Epoch 2 step 597: training accuarcy: 0.7662\n",
      "Epoch 2 step 597: training loss: 5952.178204743972\n",
      "Epoch 2 step 598: training accuarcy: 0.7592\n",
      "Epoch 2 step 598: training loss: 5796.013798322221\n",
      "Epoch 2 step 599: training accuarcy: 0.7721\n",
      "Epoch 2 step 599: training loss: 5873.526872020286\n",
      "Epoch 2 step 600: training accuarcy: 0.7581\n",
      "Epoch 2 step 600: training loss: 5867.136233958154\n",
      "Epoch 2 step 601: training accuarcy: 0.7641\n",
      "Epoch 2 step 601: training loss: 5878.091511757535\n",
      "Epoch 2 step 602: training accuarcy: 0.7596\n",
      "Epoch 2 step 602: training loss: 5839.833103766225\n",
      "Epoch 2 step 603: training accuarcy: 0.7648\n",
      "Epoch 2 step 603: training loss: 5865.740713332086\n",
      "Epoch 2 step 604: training accuarcy: 0.7644000000000001\n",
      "Epoch 2 step 604: training loss: 5869.496303829244\n",
      "Epoch 2 step 605: training accuarcy: 0.7658\n",
      "Epoch 2 step 605: training loss: 5884.125294987281\n",
      "Epoch 2 step 606: training accuarcy: 0.7534000000000001\n",
      "Epoch 2 step 606: training loss: 5786.791380864911\n",
      "Epoch 2 step 607: training accuarcy: 0.7792\n",
      "Epoch 2 step 607: training loss: 5760.4681671848775\n",
      "Epoch 2 step 608: training accuarcy: 0.7805000000000001\n",
      "Epoch 2 step 608: training loss: 5865.2250440054\n",
      "Epoch 2 step 609: training accuarcy: 0.764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 609: training loss: 5868.803261274316\n",
      "Epoch 2 step 610: training accuarcy: 0.7607\n",
      "Epoch 2 step 610: training loss: 5821.171576013072\n",
      "Epoch 2 step 611: training accuarcy: 0.7678\n",
      "Epoch 2 step 611: training loss: 5812.326536367214\n",
      "Epoch 2 step 612: training accuarcy: 0.7585000000000001\n",
      "Epoch 2 step 612: training loss: 5713.2006609790205\n",
      "Epoch 2 step 613: training accuarcy: 0.7754000000000001\n",
      "Epoch 2 step 613: training loss: 5947.645456339093\n",
      "Epoch 2 step 614: training accuarcy: 0.7582\n",
      "Epoch 2 step 614: training loss: 5871.072265700461\n",
      "Epoch 2 step 615: training accuarcy: 0.7597\n",
      "Epoch 2 step 615: training loss: 5869.44051157142\n",
      "Epoch 2 step 616: training accuarcy: 0.756\n",
      "Epoch 2 step 616: training loss: 5921.751201514423\n",
      "Epoch 2 step 617: training accuarcy: 0.7582\n",
      "Epoch 2 step 617: training loss: 5823.039565433438\n",
      "Epoch 2 step 618: training accuarcy: 0.7719\n",
      "Epoch 2 step 618: training loss: 5787.280896895166\n",
      "Epoch 2 step 619: training accuarcy: 0.7732\n",
      "Epoch 2 step 619: training loss: 5770.402988088685\n",
      "Epoch 2 step 620: training accuarcy: 0.7648\n",
      "Epoch 2 step 620: training loss: 5843.114584344029\n",
      "Epoch 2 step 621: training accuarcy: 0.7613000000000001\n",
      "Epoch 2 step 621: training loss: 5837.069309189101\n",
      "Epoch 2 step 622: training accuarcy: 0.7629\n",
      "Epoch 2 step 622: training loss: 5859.371721309894\n",
      "Epoch 2 step 623: training accuarcy: 0.7607\n",
      "Epoch 2 step 623: training loss: 5865.518336004211\n",
      "Epoch 2 step 624: training accuarcy: 0.7596\n",
      "Epoch 2 step 624: training loss: 5820.351612861637\n",
      "Epoch 2 step 625: training accuarcy: 0.7627\n",
      "Epoch 2 step 625: training loss: 5792.638495609228\n",
      "Epoch 2 step 626: training accuarcy: 0.7608\n",
      "Epoch 2 step 626: training loss: 5827.496055526965\n",
      "Epoch 2 step 627: training accuarcy: 0.7618\n",
      "Epoch 2 step 627: training loss: 5843.490499525937\n",
      "Epoch 2 step 628: training accuarcy: 0.764\n",
      "Epoch 2 step 628: training loss: 5793.668624165791\n",
      "Epoch 2 step 629: training accuarcy: 0.7698\n",
      "Epoch 2 step 629: training loss: 5845.063664819411\n",
      "Epoch 2 step 630: training accuarcy: 0.7578\n",
      "Epoch 2 step 630: training loss: 5818.033761224517\n",
      "Epoch 2 step 631: training accuarcy: 0.7647\n",
      "Epoch 2 step 631: training loss: 5849.527568269623\n",
      "Epoch 2 step 632: training accuarcy: 0.7567\n",
      "Epoch 2 step 632: training loss: 5757.5868092229575\n",
      "Epoch 2 step 633: training accuarcy: 0.7679\n",
      "Epoch 2 step 633: training loss: 5871.962989502545\n",
      "Epoch 2 step 634: training accuarcy: 0.7593000000000001\n",
      "Epoch 2 step 634: training loss: 5798.548030440644\n",
      "Epoch 2 step 635: training accuarcy: 0.7669\n",
      "Epoch 2 step 635: training loss: 5855.062053664198\n",
      "Epoch 2 step 636: training accuarcy: 0.7676000000000001\n",
      "Epoch 2 step 636: training loss: 5864.632430796638\n",
      "Epoch 2 step 637: training accuarcy: 0.7552\n",
      "Epoch 2 step 637: training loss: 5830.763036357724\n",
      "Epoch 2 step 638: training accuarcy: 0.7647\n",
      "Epoch 2 step 638: training loss: 5837.410382662521\n",
      "Epoch 2 step 639: training accuarcy: 0.7577\n",
      "Epoch 2 step 639: training loss: 5715.756344133292\n",
      "Epoch 2 step 640: training accuarcy: 0.7718\n",
      "Epoch 2 step 640: training loss: 5833.307800901713\n",
      "Epoch 2 step 641: training accuarcy: 0.7641\n",
      "Epoch 2 step 641: training loss: 5788.136092371231\n",
      "Epoch 2 step 642: training accuarcy: 0.7713\n",
      "Epoch 2 step 642: training loss: 5698.600951720576\n",
      "Epoch 2 step 643: training accuarcy: 0.7708\n",
      "Epoch 2 step 643: training loss: 5797.479889451009\n",
      "Epoch 2 step 644: training accuarcy: 0.7642\n",
      "Epoch 2 step 644: training loss: 5786.641970696785\n",
      "Epoch 2 step 645: training accuarcy: 0.7613000000000001\n",
      "Epoch 2 step 645: training loss: 5776.7075169563195\n",
      "Epoch 2 step 646: training accuarcy: 0.7652\n",
      "Epoch 2 step 646: training loss: 5841.447041950999\n",
      "Epoch 2 step 647: training accuarcy: 0.7663000000000001\n",
      "Epoch 2 step 647: training loss: 5858.817765338606\n",
      "Epoch 2 step 648: training accuarcy: 0.7586\n",
      "Epoch 2 step 648: training loss: 5825.518244342186\n",
      "Epoch 2 step 649: training accuarcy: 0.7635000000000001\n",
      "Epoch 2 step 649: training loss: 5784.756058126319\n",
      "Epoch 2 step 650: training accuarcy: 0.7665000000000001\n",
      "Epoch 2 step 650: training loss: 5851.12514008699\n",
      "Epoch 2 step 651: training accuarcy: 0.753\n",
      "Epoch 2 step 651: training loss: 5809.3958851652715\n",
      "Epoch 2 step 652: training accuarcy: 0.7703\n",
      "Epoch 2 step 652: training loss: 5875.276539865575\n",
      "Epoch 2 step 653: training accuarcy: 0.7579\n",
      "Epoch 2 step 653: training loss: 5807.416415207346\n",
      "Epoch 2 step 654: training accuarcy: 0.7652\n",
      "Epoch 2 step 654: training loss: 5866.734474841068\n",
      "Epoch 2 step 655: training accuarcy: 0.7603000000000001\n",
      "Epoch 2 step 655: training loss: 5760.401977710219\n",
      "Epoch 2 step 656: training accuarcy: 0.7637\n",
      "Epoch 2 step 656: training loss: 5684.767176328634\n",
      "Epoch 2 step 657: training accuarcy: 0.7659\n",
      "Epoch 2 step 657: training loss: 5832.047745325857\n",
      "Epoch 2 step 658: training accuarcy: 0.7615000000000001\n",
      "Epoch 2 step 658: training loss: 5781.588417084332\n",
      "Epoch 2 step 659: training accuarcy: 0.7554000000000001\n",
      "Epoch 2 step 659: training loss: 5845.903276597626\n",
      "Epoch 2 step 660: training accuarcy: 0.7668\n",
      "Epoch 2 step 660: training loss: 5735.55877599816\n",
      "Epoch 2 step 661: training accuarcy: 0.765\n",
      "Epoch 2 step 661: training loss: 5883.161669756003\n",
      "Epoch 2 step 662: training accuarcy: 0.7617\n",
      "Epoch 2 step 662: training loss: 5867.784785105586\n",
      "Epoch 2 step 663: training accuarcy: 0.7513000000000001\n",
      "Epoch 2 step 663: training loss: 5889.361719310231\n",
      "Epoch 2 step 664: training accuarcy: 0.7613000000000001\n",
      "Epoch 2 step 664: training loss: 5860.295057291667\n",
      "Epoch 2 step 665: training accuarcy: 0.7537\n",
      "Epoch 2 step 665: training loss: 5746.02418169753\n",
      "Epoch 2 step 666: training accuarcy: 0.7653000000000001\n",
      "Epoch 2 step 666: training loss: 5812.56094354107\n",
      "Epoch 2 step 667: training accuarcy: 0.7659\n",
      "Epoch 2 step 667: training loss: 5789.721437713932\n",
      "Epoch 2 step 668: training accuarcy: 0.7656000000000001\n",
      "Epoch 2 step 668: training loss: 5826.954780977187\n",
      "Epoch 2 step 669: training accuarcy: 0.7565000000000001\n",
      "Epoch 2 step 669: training loss: 5786.624261887974\n",
      "Epoch 2 step 670: training accuarcy: 0.7699\n",
      "Epoch 2 step 670: training loss: 5802.08565599532\n",
      "Epoch 2 step 671: training accuarcy: 0.7675000000000001\n",
      "Epoch 2 step 671: training loss: 5701.773148342576\n",
      "Epoch 2 step 672: training accuarcy: 0.7741\n",
      "Epoch 2 step 672: training loss: 5798.258922336352\n",
      "Epoch 2 step 673: training accuarcy: 0.7668\n",
      "Epoch 2 step 673: training loss: 5755.3701052858605\n",
      "Epoch 2 step 674: training accuarcy: 0.7702\n",
      "Epoch 2 step 674: training loss: 5751.167263864784\n",
      "Epoch 2 step 675: training accuarcy: 0.7705000000000001\n",
      "Epoch 2 step 675: training loss: 5842.645445858535\n",
      "Epoch 2 step 676: training accuarcy: 0.7555000000000001\n",
      "Epoch 2 step 676: training loss: 5803.468783657128\n",
      "Epoch 2 step 677: training accuarcy: 0.7586\n",
      "Epoch 2 step 677: training loss: 5754.357886029981\n",
      "Epoch 2 step 678: training accuarcy: 0.7678\n",
      "Epoch 2 step 678: training loss: 5870.172444668484\n",
      "Epoch 2 step 679: training accuarcy: 0.7569\n",
      "Epoch 2 step 679: training loss: 5823.620887003102\n",
      "Epoch 2 step 680: training accuarcy: 0.7636000000000001\n",
      "Epoch 2 step 680: training loss: 5780.313573599042\n",
      "Epoch 2 step 681: training accuarcy: 0.7694000000000001\n",
      "Epoch 2 step 681: training loss: 5740.939692844715\n",
      "Epoch 2 step 682: training accuarcy: 0.7643000000000001\n",
      "Epoch 2 step 682: training loss: 5750.681860038146\n",
      "Epoch 2 step 683: training accuarcy: 0.7709\n",
      "Epoch 2 step 683: training loss: 5822.08059428262\n",
      "Epoch 2 step 684: training accuarcy: 0.7582\n",
      "Epoch 2 step 684: training loss: 5738.040777526299\n",
      "Epoch 2 step 685: training accuarcy: 0.7664000000000001\n",
      "Epoch 2 step 685: training loss: 5772.9999082724\n",
      "Epoch 2 step 686: training accuarcy: 0.76\n",
      "Epoch 2 step 686: training loss: 5777.68093556884\n",
      "Epoch 2 step 687: training accuarcy: 0.7617\n",
      "Epoch 2 step 687: training loss: 5858.841340930222\n",
      "Epoch 2 step 688: training accuarcy: 0.7576\n",
      "Epoch 2 step 688: training loss: 5739.117604864343\n",
      "Epoch 2 step 689: training accuarcy: 0.7644000000000001\n",
      "Epoch 2 step 689: training loss: 5835.971426613851\n",
      "Epoch 2 step 690: training accuarcy: 0.7538\n",
      "Epoch 2 step 690: training loss: 5756.7593069740815\n",
      "Epoch 2 step 691: training accuarcy: 0.7698\n",
      "Epoch 2 step 691: training loss: 5761.836686433021\n",
      "Epoch 2 step 692: training accuarcy: 0.7681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 692: training loss: 5808.003758137652\n",
      "Epoch 2 step 693: training accuarcy: 0.7599\n",
      "Epoch 2 step 693: training loss: 5840.707691860372\n",
      "Epoch 2 step 694: training accuarcy: 0.7573000000000001\n",
      "Epoch 2 step 694: training loss: 5785.5937861688335\n",
      "Epoch 2 step 695: training accuarcy: 0.7553000000000001\n",
      "Epoch 2 step 695: training loss: 5733.99923466894\n",
      "Epoch 2 step 696: training accuarcy: 0.771\n",
      "Epoch 2 step 696: training loss: 5852.235513384204\n",
      "Epoch 2 step 697: training accuarcy: 0.7562\n",
      "Epoch 2 step 697: training loss: 5778.643419491904\n",
      "Epoch 2 step 698: training accuarcy: 0.7635000000000001\n",
      "Epoch 2 step 698: training loss: 5815.865021710468\n",
      "Epoch 2 step 699: training accuarcy: 0.7537\n",
      "Epoch 2 step 699: training loss: 5777.87393824913\n",
      "Epoch 2 step 700: training accuarcy: 0.7724000000000001\n",
      "Epoch 2 step 700: training loss: 5796.416080089128\n",
      "Epoch 2 step 701: training accuarcy: 0.7594000000000001\n",
      "Epoch 2 step 701: training loss: 5751.790553334917\n",
      "Epoch 2 step 702: training accuarcy: 0.7668\n",
      "Epoch 2 step 702: training loss: 5843.243072640909\n",
      "Epoch 2 step 703: training accuarcy: 0.7592\n",
      "Epoch 2 step 703: training loss: 5855.995508438858\n",
      "Epoch 2 step 704: training accuarcy: 0.7518\n",
      "Epoch 2 step 704: training loss: 5762.621585526579\n",
      "Epoch 2 step 705: training accuarcy: 0.7584000000000001\n",
      "Epoch 2 step 705: training loss: 5746.502695253041\n",
      "Epoch 2 step 706: training accuarcy: 0.7745000000000001\n",
      "Epoch 2 step 706: training loss: 5831.36117674396\n",
      "Epoch 2 step 707: training accuarcy: 0.7596\n",
      "Epoch 2 step 707: training loss: 5671.718474441267\n",
      "Epoch 2 step 708: training accuarcy: 0.771\n",
      "Epoch 2 step 708: training loss: 5754.866807608811\n",
      "Epoch 2 step 709: training accuarcy: 0.7562\n",
      "Epoch 2 step 709: training loss: 5796.713314913013\n",
      "Epoch 2 step 710: training accuarcy: 0.7525000000000001\n",
      "Epoch 2 step 710: training loss: 5832.898603486489\n",
      "Epoch 2 step 711: training accuarcy: 0.7611\n",
      "Epoch 2 step 711: training loss: 5852.980562852705\n",
      "Epoch 2 step 712: training accuarcy: 0.7535000000000001\n",
      "Epoch 2 step 712: training loss: 5768.653427079309\n",
      "Epoch 2 step 713: training accuarcy: 0.7619\n",
      "Epoch 2 step 713: training loss: 5776.444118207237\n",
      "Epoch 2 step 714: training accuarcy: 0.763\n",
      "Epoch 2 step 714: training loss: 5800.455035835939\n",
      "Epoch 2 step 715: training accuarcy: 0.7671\n",
      "Epoch 2 step 715: training loss: 5748.670157905011\n",
      "Epoch 2 step 716: training accuarcy: 0.7589\n",
      "Epoch 2 step 716: training loss: 5779.064404259712\n",
      "Epoch 2 step 717: training accuarcy: 0.7636000000000001\n",
      "Epoch 2 step 717: training loss: 5782.894676206297\n",
      "Epoch 2 step 718: training accuarcy: 0.762\n",
      "Epoch 2 step 718: training loss: 5730.522674155288\n",
      "Epoch 2 step 719: training accuarcy: 0.7694000000000001\n",
      "Epoch 2 step 719: training loss: 5737.740536291061\n",
      "Epoch 2 step 720: training accuarcy: 0.7618\n",
      "Epoch 2 step 720: training loss: 5846.400581076008\n",
      "Epoch 2 step 721: training accuarcy: 0.7553000000000001\n",
      "Epoch 2 step 721: training loss: 5904.403280082422\n",
      "Epoch 2 step 722: training accuarcy: 0.7523000000000001\n",
      "Epoch 2 step 722: training loss: 5704.233690700965\n",
      "Epoch 2 step 723: training accuarcy: 0.7657\n",
      "Epoch 2 step 723: training loss: 5754.43751479253\n",
      "Epoch 2 step 724: training accuarcy: 0.7614000000000001\n",
      "Epoch 2 step 724: training loss: 5834.508031211586\n",
      "Epoch 2 step 725: training accuarcy: 0.7483000000000001\n",
      "Epoch 2 step 725: training loss: 5770.295658488485\n",
      "Epoch 2 step 726: training accuarcy: 0.7683\n",
      "Epoch 2 step 726: training loss: 5699.105896063813\n",
      "Epoch 2 step 727: training accuarcy: 0.7649\n",
      "Epoch 2 step 727: training loss: 5799.8223901741385\n",
      "Epoch 2 step 728: training accuarcy: 0.7544000000000001\n",
      "Epoch 2 step 728: training loss: 5782.259084511168\n",
      "Epoch 2 step 729: training accuarcy: 0.7533000000000001\n",
      "Epoch 2 step 729: training loss: 5765.282098567806\n",
      "Epoch 2 step 730: training accuarcy: 0.7592\n",
      "Epoch 2 step 730: training loss: 5777.360972401456\n",
      "Epoch 2 step 731: training accuarcy: 0.7607\n",
      "Epoch 2 step 731: training loss: 5730.940164954854\n",
      "Epoch 2 step 732: training accuarcy: 0.7645000000000001\n",
      "Epoch 2 step 732: training loss: 5718.432739268942\n",
      "Epoch 2 step 733: training accuarcy: 0.7676000000000001\n",
      "Epoch 2 step 733: training loss: 5728.860505778854\n",
      "Epoch 2 step 734: training accuarcy: 0.7615000000000001\n",
      "Epoch 2 step 734: training loss: 5713.917518401611\n",
      "Epoch 2 step 735: training accuarcy: 0.769\n",
      "Epoch 2 step 735: training loss: 5842.762091147111\n",
      "Epoch 2 step 736: training accuarcy: 0.7562\n",
      "Epoch 2 step 736: training loss: 5888.51218912366\n",
      "Epoch 2 step 737: training accuarcy: 0.7551\n",
      "Epoch 2 step 737: training loss: 5760.160782326127\n",
      "Epoch 2 step 738: training accuarcy: 0.7649\n",
      "Epoch 2 step 738: training loss: 5755.059004635958\n",
      "Epoch 2 step 739: training accuarcy: 0.7607\n",
      "Epoch 2 step 739: training loss: 5615.710054693015\n",
      "Epoch 2 step 740: training accuarcy: 0.7742\n",
      "Epoch 2 step 740: training loss: 5791.827989288633\n",
      "Epoch 2 step 741: training accuarcy: 0.7584000000000001\n",
      "Epoch 2 step 741: training loss: 5805.868760908241\n",
      "Epoch 2 step 742: training accuarcy: 0.7586\n",
      "Epoch 2 step 742: training loss: 5697.769702965218\n",
      "Epoch 2 step 743: training accuarcy: 0.7721\n",
      "Epoch 2 step 743: training loss: 5737.927944549525\n",
      "Epoch 2 step 744: training accuarcy: 0.7672\n",
      "Epoch 2 step 744: training loss: 5783.779977370564\n",
      "Epoch 2 step 745: training accuarcy: 0.7588\n",
      "Epoch 2 step 745: training loss: 5731.307745760421\n",
      "Epoch 2 step 746: training accuarcy: 0.7603000000000001\n",
      "Epoch 2 step 746: training loss: 5764.195974802755\n",
      "Epoch 2 step 747: training accuarcy: 0.7649\n",
      "Epoch 2 step 747: training loss: 5741.213644531315\n",
      "Epoch 2 step 748: training accuarcy: 0.7632\n",
      "Epoch 2 step 748: training loss: 5816.225306324832\n",
      "Epoch 2 step 749: training accuarcy: 0.7468\n",
      "Epoch 2 step 749: training loss: 5859.9278157649605\n",
      "Epoch 2 step 750: training accuarcy: 0.7499\n",
      "Epoch 2 step 750: training loss: 5688.854413696588\n",
      "Epoch 2 step 751: training accuarcy: 0.7692\n",
      "Epoch 2 step 751: training loss: 5803.2806402109145\n",
      "Epoch 2 step 752: training accuarcy: 0.7567\n",
      "Epoch 2 step 752: training loss: 5720.638408951267\n",
      "Epoch 2 step 753: training accuarcy: 0.7589\n",
      "Epoch 2 step 753: training loss: 5724.684924835259\n",
      "Epoch 2 step 754: training accuarcy: 0.7685000000000001\n",
      "Epoch 2 step 754: training loss: 5777.547532520113\n",
      "Epoch 2 step 755: training accuarcy: 0.7613000000000001\n",
      "Epoch 2 step 755: training loss: 5727.678669548576\n",
      "Epoch 2 step 756: training accuarcy: 0.7735000000000001\n",
      "Epoch 2 step 756: training loss: 5854.689623781337\n",
      "Epoch 2 step 757: training accuarcy: 0.7561\n",
      "Epoch 2 step 757: training loss: 5738.661083243898\n",
      "Epoch 2 step 758: training accuarcy: 0.766\n",
      "Epoch 2 step 758: training loss: 5652.614709039406\n",
      "Epoch 2 step 759: training accuarcy: 0.7746000000000001\n",
      "Epoch 2 step 759: training loss: 5713.946585451884\n",
      "Epoch 2 step 760: training accuarcy: 0.7741\n",
      "Epoch 2 step 760: training loss: 5747.785687011001\n",
      "Epoch 2 step 761: training accuarcy: 0.7679\n",
      "Epoch 2 step 761: training loss: 5826.9768018147315\n",
      "Epoch 2 step 762: training accuarcy: 0.7625000000000001\n",
      "Epoch 2 step 762: training loss: 5796.703764459904\n",
      "Epoch 2 step 763: training accuarcy: 0.7619\n",
      "Epoch 2 step 763: training loss: 5717.563956839426\n",
      "Epoch 2 step 764: training accuarcy: 0.7636000000000001\n",
      "Epoch 2 step 764: training loss: 5872.523752769058\n",
      "Epoch 2 step 765: training accuarcy: 0.7512000000000001\n",
      "Epoch 2 step 765: training loss: 5723.757822469152\n",
      "Epoch 2 step 766: training accuarcy: 0.7723\n",
      "Epoch 2 step 766: training loss: 5714.906052784727\n",
      "Epoch 2 step 767: training accuarcy: 0.7675000000000001\n",
      "Epoch 2 step 767: training loss: 5756.8561794922025\n",
      "Epoch 2 step 768: training accuarcy: 0.7657\n",
      "Epoch 2 step 768: training loss: 5748.0382052348095\n",
      "Epoch 2 step 769: training accuarcy: 0.7609\n",
      "Epoch 2 step 769: training loss: 5687.990299579942\n",
      "Epoch 2 step 770: training accuarcy: 0.7712\n",
      "Epoch 2 step 770: training loss: 5800.946473839159\n",
      "Epoch 2 step 771: training accuarcy: 0.7523000000000001\n",
      "Epoch 2 step 771: training loss: 5705.472982488529\n",
      "Epoch 2 step 772: training accuarcy: 0.7608\n",
      "Epoch 2 step 772: training loss: 5705.733287399986\n",
      "Epoch 2 step 773: training accuarcy: 0.7672\n",
      "Epoch 2 step 773: training loss: 5772.962908532386\n",
      "Epoch 2 step 774: training accuarcy: 0.761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 774: training loss: 5791.259595682465\n",
      "Epoch 2 step 775: training accuarcy: 0.7603000000000001\n",
      "Epoch 2 step 775: training loss: 5745.034903773548\n",
      "Epoch 2 step 776: training accuarcy: 0.7605000000000001\n",
      "Epoch 2 step 776: training loss: 5731.012454404853\n",
      "Epoch 2 step 777: training accuarcy: 0.7651\n",
      "Epoch 2 step 777: training loss: 5750.86977893035\n",
      "Epoch 2 step 778: training accuarcy: 0.7672\n",
      "Epoch 2 step 778: training loss: 5734.934685328116\n",
      "Epoch 2 step 779: training accuarcy: 0.7657\n",
      "Epoch 2 step 779: training loss: 5780.4799475845475\n",
      "Epoch 2 step 780: training accuarcy: 0.7595000000000001\n",
      "Epoch 2 step 780: training loss: 5742.457617607512\n",
      "Epoch 2 step 781: training accuarcy: 0.7624000000000001\n",
      "Epoch 2 step 781: training loss: 5692.319438713783\n",
      "Epoch 2 step 782: training accuarcy: 0.7679\n",
      "Epoch 2 step 782: training loss: 5704.255661695401\n",
      "Epoch 2 step 783: training accuarcy: 0.7632\n",
      "Epoch 2 step 783: training loss: 5657.374682291392\n",
      "Epoch 2 step 784: training accuarcy: 0.7682\n",
      "Epoch 2 step 784: training loss: 5619.1307297396725\n",
      "Epoch 2 step 785: training accuarcy: 0.7699\n",
      "Epoch 2 step 785: training loss: 5846.885400640186\n",
      "Epoch 2 step 786: training accuarcy: 0.761\n",
      "Epoch 2 step 786: training loss: 5710.977400559913\n",
      "Epoch 2 step 787: training accuarcy: 0.7767000000000001\n",
      "Epoch 2 step 787: training loss: 5793.444959953572\n",
      "Epoch 2 step 788: training accuarcy: 0.7605000000000001\n",
      "Epoch 2 step 788: training loss: 2497.804246468991\n",
      "Epoch 2 step 789: training accuarcy: 0.7558974358974359\n",
      "Epoch 2: train loss 5798.668792454225, train accuarcy 0.7382795214653015\n",
      "Epoch 2: valid loss 5496.316506377508, valid accuarcy 0.7811000943183899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [14:58<00:00, 299.88s/it]\n"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=3,\n",
    "                 log_dir=get_log_dir('simple_topcoder', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:14:55.886401Z",
     "start_time": "2019-10-09T13:00:22.609968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 294606.13853030634\n",
      "Epoch 0 step 1: training accuarcy: 0.5142\n",
      "Epoch 0 step 1: training loss: 286108.444642681\n",
      "Epoch 0 step 2: training accuarcy: 0.5282\n",
      "Epoch 0 step 2: training loss: 293987.4134806083\n",
      "Epoch 0 step 3: training accuarcy: 0.5083\n",
      "Epoch 0 step 3: training loss: 284043.4721217331\n",
      "Epoch 0 step 4: training accuarcy: 0.5192\n",
      "Epoch 0 step 4: training loss: 278149.06834666454\n",
      "Epoch 0 step 5: training accuarcy: 0.5146000000000001\n",
      "Epoch 0 step 5: training loss: 279162.83377816185\n",
      "Epoch 0 step 6: training accuarcy: 0.5137\n",
      "Epoch 0 step 6: training loss: 263925.41081593785\n",
      "Epoch 0 step 7: training accuarcy: 0.533\n",
      "Epoch 0 step 7: training loss: 272980.8215130734\n",
      "Epoch 0 step 8: training accuarcy: 0.5086\n",
      "Epoch 0 step 8: training loss: 261422.55033331588\n",
      "Epoch 0 step 9: training accuarcy: 0.5252\n",
      "Epoch 0 step 9: training loss: 246522.51367903984\n",
      "Epoch 0 step 10: training accuarcy: 0.5244\n",
      "Epoch 0 step 10: training loss: 247796.70586116522\n",
      "Epoch 0 step 11: training accuarcy: 0.5341\n",
      "Epoch 0 step 11: training loss: 238984.72670435722\n",
      "Epoch 0 step 12: training accuarcy: 0.5286000000000001\n",
      "Epoch 0 step 12: training loss: 232933.34247990797\n",
      "Epoch 0 step 13: training accuarcy: 0.5319\n",
      "Epoch 0 step 13: training loss: 230210.2971837398\n",
      "Epoch 0 step 14: training accuarcy: 0.5256000000000001\n",
      "Epoch 0 step 14: training loss: 233485.07635888772\n",
      "Epoch 0 step 15: training accuarcy: 0.5235000000000001\n",
      "Epoch 0 step 15: training loss: 223323.81134403386\n",
      "Epoch 0 step 16: training accuarcy: 0.5248\n",
      "Epoch 0 step 16: training loss: 213327.34659066066\n",
      "Epoch 0 step 17: training accuarcy: 0.533\n",
      "Epoch 0 step 17: training loss: 209879.5444714632\n",
      "Epoch 0 step 18: training accuarcy: 0.5348\n",
      "Epoch 0 step 18: training loss: 205113.50736993892\n",
      "Epoch 0 step 19: training accuarcy: 0.5286000000000001\n",
      "Epoch 0 step 19: training loss: 199281.58614203692\n",
      "Epoch 0 step 20: training accuarcy: 0.5296000000000001\n",
      "Epoch 0 step 20: training loss: 192111.9664741734\n",
      "Epoch 0 step 21: training accuarcy: 0.5415\n",
      "Epoch 0 step 21: training loss: 189311.72476804312\n",
      "Epoch 0 step 22: training accuarcy: 0.5344\n",
      "Epoch 0 step 22: training loss: 180091.76698244683\n",
      "Epoch 0 step 23: training accuarcy: 0.5414\n",
      "Epoch 0 step 23: training loss: 177140.9345186835\n",
      "Epoch 0 step 24: training accuarcy: 0.5324\n",
      "Epoch 0 step 24: training loss: 174560.93247408385\n",
      "Epoch 0 step 25: training accuarcy: 0.544\n",
      "Epoch 0 step 25: training loss: 174409.33184522943\n",
      "Epoch 0 step 26: training accuarcy: 0.5296000000000001\n",
      "Epoch 0 step 26: training loss: 163819.90785822915\n",
      "Epoch 0 step 27: training accuarcy: 0.5372\n",
      "Epoch 0 step 27: training loss: 160254.54592900007\n",
      "Epoch 0 step 28: training accuarcy: 0.5302\n",
      "Epoch 0 step 28: training loss: 149365.67896409388\n",
      "Epoch 0 step 29: training accuarcy: 0.5451\n",
      "Epoch 0 step 29: training loss: 157405.21053059236\n",
      "Epoch 0 step 30: training accuarcy: 0.5251\n",
      "Epoch 0 step 30: training loss: 149808.0587127385\n",
      "Epoch 0 step 31: training accuarcy: 0.5335\n",
      "Epoch 0 step 31: training loss: 140425.23113409049\n",
      "Epoch 0 step 32: training accuarcy: 0.5416000000000001\n",
      "Epoch 0 step 32: training loss: 142977.7634677522\n",
      "Epoch 0 step 33: training accuarcy: 0.5191\n",
      "Epoch 0 step 33: training loss: 133403.7320041012\n",
      "Epoch 0 step 34: training accuarcy: 0.5362\n",
      "Epoch 0 step 34: training loss: 133048.10475581329\n",
      "Epoch 0 step 35: training accuarcy: 0.5268\n",
      "Epoch 0 step 35: training loss: 130997.7794195195\n",
      "Epoch 0 step 36: training accuarcy: 0.5298\n",
      "Epoch 0 step 36: training loss: 117542.52060988337\n",
      "Epoch 0 step 37: training accuarcy: 0.5575\n",
      "Epoch 0 step 37: training loss: 120609.30332886823\n",
      "Epoch 0 step 38: training accuarcy: 0.5401\n",
      "Epoch 0 step 38: training loss: 115569.06764903638\n",
      "Epoch 0 step 39: training accuarcy: 0.5464\n",
      "Epoch 0 step 39: training loss: 112037.18054295078\n",
      "Epoch 0 step 40: training accuarcy: 0.549\n",
      "Epoch 0 step 40: training loss: 110365.01318704188\n",
      "Epoch 0 step 41: training accuarcy: 0.533\n",
      "Epoch 0 step 41: training loss: 106127.02435295642\n",
      "Epoch 0 step 42: training accuarcy: 0.544\n",
      "Epoch 0 step 42: training loss: 104320.11406568302\n",
      "Epoch 0 step 43: training accuarcy: 0.5456\n",
      "Epoch 0 step 43: training loss: 98910.70261878318\n",
      "Epoch 0 step 44: training accuarcy: 0.5483\n",
      "Epoch 0 step 44: training loss: 93891.3381596104\n",
      "Epoch 0 step 45: training accuarcy: 0.5588000000000001\n",
      "Epoch 0 step 45: training loss: 92323.9410254358\n",
      "Epoch 0 step 46: training accuarcy: 0.5606\n",
      "Epoch 0 step 46: training loss: 90345.71008458006\n",
      "Epoch 0 step 47: training accuarcy: 0.5539000000000001\n",
      "Epoch 0 step 47: training loss: 89075.05734434206\n",
      "Epoch 0 step 48: training accuarcy: 0.5515\n",
      "Epoch 0 step 48: training loss: 85570.67977686659\n",
      "Epoch 0 step 49: training accuarcy: 0.5483\n",
      "Epoch 0 step 49: training loss: 83580.82839934735\n",
      "Epoch 0 step 50: training accuarcy: 0.5473\n",
      "Epoch 0 step 50: training loss: 83958.36806107688\n",
      "Epoch 0 step 51: training accuarcy: 0.541\n",
      "Epoch 0 step 51: training loss: 78010.6080669183\n",
      "Epoch 0 step 52: training accuarcy: 0.5535\n",
      "Epoch 0 step 52: training loss: 78606.9700591866\n",
      "Epoch 0 step 53: training accuarcy: 0.5413\n",
      "Epoch 0 step 53: training loss: 75289.90786955843\n",
      "Epoch 0 step 54: training accuarcy: 0.5594\n",
      "Epoch 0 step 54: training loss: 74269.66343819878\n",
      "Epoch 0 step 55: training accuarcy: 0.5406000000000001\n",
      "Epoch 0 step 55: training loss: 71777.96568676515\n",
      "Epoch 0 step 56: training accuarcy: 0.5585\n",
      "Epoch 0 step 56: training loss: 68616.96793213318\n",
      "Epoch 0 step 57: training accuarcy: 0.5773\n",
      "Epoch 0 step 57: training loss: 68390.37479003408\n",
      "Epoch 0 step 58: training accuarcy: 0.5592\n",
      "Epoch 0 step 58: training loss: 68699.03107022232\n",
      "Epoch 0 step 59: training accuarcy: 0.5482\n",
      "Epoch 0 step 59: training loss: 66820.50152926438\n",
      "Epoch 0 step 60: training accuarcy: 0.5677\n",
      "Epoch 0 step 60: training loss: 65912.27819156047\n",
      "Epoch 0 step 61: training accuarcy: 0.5522\n",
      "Epoch 0 step 61: training loss: 65288.7963928904\n",
      "Epoch 0 step 62: training accuarcy: 0.5630000000000001\n",
      "Epoch 0 step 62: training loss: 64387.5546108618\n",
      "Epoch 0 step 63: training accuarcy: 0.5553\n",
      "Epoch 0 step 63: training loss: 61795.30018373211\n",
      "Epoch 0 step 64: training accuarcy: 0.5653\n",
      "Epoch 0 step 64: training loss: 60548.35658973036\n",
      "Epoch 0 step 65: training accuarcy: 0.5652\n",
      "Epoch 0 step 65: training loss: 59251.686173066795\n",
      "Epoch 0 step 66: training accuarcy: 0.5776\n",
      "Epoch 0 step 66: training loss: 58983.84799881805\n",
      "Epoch 0 step 67: training accuarcy: 0.5613\n",
      "Epoch 0 step 67: training loss: 58176.41571022961\n",
      "Epoch 0 step 68: training accuarcy: 0.5632\n",
      "Epoch 0 step 68: training loss: 59662.420715975764\n",
      "Epoch 0 step 69: training accuarcy: 0.5519000000000001\n",
      "Epoch 0 step 69: training loss: 55926.721778058185\n",
      "Epoch 0 step 70: training accuarcy: 0.5847\n",
      "Epoch 0 step 70: training loss: 55870.49273634621\n",
      "Epoch 0 step 71: training accuarcy: 0.5772\n",
      "Epoch 0 step 71: training loss: 54822.54724703757\n",
      "Epoch 0 step 72: training accuarcy: 0.5748\n",
      "Epoch 0 step 72: training loss: 55376.4544312036\n",
      "Epoch 0 step 73: training accuarcy: 0.5683\n",
      "Epoch 0 step 73: training loss: 55932.68138140744\n",
      "Epoch 0 step 74: training accuarcy: 0.5717\n",
      "Epoch 0 step 74: training loss: 53070.58369867261\n",
      "Epoch 0 step 75: training accuarcy: 0.5835\n",
      "Epoch 0 step 75: training loss: 52623.302606139085\n",
      "Epoch 0 step 76: training accuarcy: 0.5847\n",
      "Epoch 0 step 76: training loss: 53877.06177768153\n",
      "Epoch 0 step 77: training accuarcy: 0.5744\n",
      "Epoch 0 step 77: training loss: 52386.36637776054\n",
      "Epoch 0 step 78: training accuarcy: 0.5942000000000001\n",
      "Epoch 0 step 78: training loss: 53577.345621185996\n",
      "Epoch 0 step 79: training accuarcy: 0.5705\n",
      "Epoch 0 step 79: training loss: 52631.920330626584\n",
      "Epoch 0 step 80: training accuarcy: 0.5742\n",
      "Epoch 0 step 80: training loss: 51559.69921226196\n",
      "Epoch 0 step 81: training accuarcy: 0.5803\n",
      "Epoch 0 step 81: training loss: 51009.090783616586\n",
      "Epoch 0 step 82: training accuarcy: 0.5842\n",
      "Epoch 0 step 82: training loss: 50999.70593507299\n",
      "Epoch 0 step 83: training accuarcy: 0.5726\n",
      "Epoch 0 step 83: training loss: 50631.768650179554\n",
      "Epoch 0 step 84: training accuarcy: 0.5851000000000001\n",
      "Epoch 0 step 84: training loss: 50266.478389920805\n",
      "Epoch 0 step 85: training accuarcy: 0.5847\n",
      "Epoch 0 step 85: training loss: 50051.87387286336\n",
      "Epoch 0 step 86: training accuarcy: 0.5888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 86: training loss: 49351.753423952265\n",
      "Epoch 0 step 87: training accuarcy: 0.5918\n",
      "Epoch 0 step 87: training loss: 50024.61976459105\n",
      "Epoch 0 step 88: training accuarcy: 0.5839\n",
      "Epoch 0 step 88: training loss: 49202.58714893614\n",
      "Epoch 0 step 89: training accuarcy: 0.5897\n",
      "Epoch 0 step 89: training loss: 48909.41989555757\n",
      "Epoch 0 step 90: training accuarcy: 0.5923\n",
      "Epoch 0 step 90: training loss: 48638.808378756796\n",
      "Epoch 0 step 91: training accuarcy: 0.5889\n",
      "Epoch 0 step 91: training loss: 49006.547890239744\n",
      "Epoch 0 step 92: training accuarcy: 0.5831000000000001\n",
      "Epoch 0 step 92: training loss: 47676.21878923138\n",
      "Epoch 0 step 93: training accuarcy: 0.6065\n",
      "Epoch 0 step 93: training loss: 48449.83790047556\n",
      "Epoch 0 step 94: training accuarcy: 0.5983\n",
      "Epoch 0 step 94: training loss: 47852.71035297997\n",
      "Epoch 0 step 95: training accuarcy: 0.6043000000000001\n",
      "Epoch 0 step 95: training loss: 47793.16903701683\n",
      "Epoch 0 step 96: training accuarcy: 0.6057\n",
      "Epoch 0 step 96: training loss: 47773.42434647083\n",
      "Epoch 0 step 97: training accuarcy: 0.606\n",
      "Epoch 0 step 97: training loss: 47182.65632621608\n",
      "Epoch 0 step 98: training accuarcy: 0.6027\n",
      "Epoch 0 step 98: training loss: 46329.32514615025\n",
      "Epoch 0 step 99: training accuarcy: 0.6107\n",
      "Epoch 0 step 99: training loss: 46981.77931949707\n",
      "Epoch 0 step 100: training accuarcy: 0.6065\n",
      "Epoch 0 step 100: training loss: 46543.16237072859\n",
      "Epoch 0 step 101: training accuarcy: 0.6156\n",
      "Epoch 0 step 101: training loss: 46435.400443131526\n",
      "Epoch 0 step 102: training accuarcy: 0.6032000000000001\n",
      "Epoch 0 step 102: training loss: 47512.59828230449\n",
      "Epoch 0 step 103: training accuarcy: 0.5887\n",
      "Epoch 0 step 103: training loss: 46067.822292044475\n",
      "Epoch 0 step 104: training accuarcy: 0.6125\n",
      "Epoch 0 step 104: training loss: 46796.80096719917\n",
      "Epoch 0 step 105: training accuarcy: 0.5991000000000001\n",
      "Epoch 0 step 105: training loss: 46316.188668303344\n",
      "Epoch 0 step 106: training accuarcy: 0.6089\n",
      "Epoch 0 step 106: training loss: 45687.46383163568\n",
      "Epoch 0 step 107: training accuarcy: 0.6191\n",
      "Epoch 0 step 107: training loss: 46449.984935144515\n",
      "Epoch 0 step 108: training accuarcy: 0.607\n",
      "Epoch 0 step 108: training loss: 45663.26056754888\n",
      "Epoch 0 step 109: training accuarcy: 0.6047\n",
      "Epoch 0 step 109: training loss: 45111.89735275197\n",
      "Epoch 0 step 110: training accuarcy: 0.6171\n",
      "Epoch 0 step 110: training loss: 46276.78862554478\n",
      "Epoch 0 step 111: training accuarcy: 0.6038\n",
      "Epoch 0 step 111: training loss: 45835.18766594704\n",
      "Epoch 0 step 112: training accuarcy: 0.6144000000000001\n",
      "Epoch 0 step 112: training loss: 45142.622092836355\n",
      "Epoch 0 step 113: training accuarcy: 0.608\n",
      "Epoch 0 step 113: training loss: 43777.51586382462\n",
      "Epoch 0 step 114: training accuarcy: 0.6401\n",
      "Epoch 0 step 114: training loss: 44507.51385826258\n",
      "Epoch 0 step 115: training accuarcy: 0.6134000000000001\n",
      "Epoch 0 step 115: training loss: 44226.673914934225\n",
      "Epoch 0 step 116: training accuarcy: 0.6205\n",
      "Epoch 0 step 116: training loss: 44520.87527122697\n",
      "Epoch 0 step 117: training accuarcy: 0.615\n",
      "Epoch 0 step 117: training loss: 45095.91779371997\n",
      "Epoch 0 step 118: training accuarcy: 0.6107\n",
      "Epoch 0 step 118: training loss: 44245.30065843064\n",
      "Epoch 0 step 119: training accuarcy: 0.626\n",
      "Epoch 0 step 119: training loss: 43605.0472264889\n",
      "Epoch 0 step 120: training accuarcy: 0.6369\n",
      "Epoch 0 step 120: training loss: 43771.77642261997\n",
      "Epoch 0 step 121: training accuarcy: 0.6251\n",
      "Epoch 0 step 121: training loss: 44068.88023852835\n",
      "Epoch 0 step 122: training accuarcy: 0.6237\n",
      "Epoch 0 step 122: training loss: 43056.729197456945\n",
      "Epoch 0 step 123: training accuarcy: 0.6439\n",
      "Epoch 0 step 123: training loss: 44600.51004993345\n",
      "Epoch 0 step 124: training accuarcy: 0.6124\n",
      "Epoch 0 step 124: training loss: 42684.00262842473\n",
      "Epoch 0 step 125: training accuarcy: 0.6394000000000001\n",
      "Epoch 0 step 125: training loss: 43390.331974001805\n",
      "Epoch 0 step 126: training accuarcy: 0.6358\n",
      "Epoch 0 step 126: training loss: 43397.91575089304\n",
      "Epoch 0 step 127: training accuarcy: 0.6369\n",
      "Epoch 0 step 127: training loss: 43529.58814174399\n",
      "Epoch 0 step 128: training accuarcy: 0.6361\n",
      "Epoch 0 step 128: training loss: 43313.329297820244\n",
      "Epoch 0 step 129: training accuarcy: 0.6282\n",
      "Epoch 0 step 129: training loss: 42699.85129977658\n",
      "Epoch 0 step 130: training accuarcy: 0.6351\n",
      "Epoch 0 step 130: training loss: 42684.47279645517\n",
      "Epoch 0 step 131: training accuarcy: 0.6497\n",
      "Epoch 0 step 131: training loss: 42916.56752804259\n",
      "Epoch 0 step 132: training accuarcy: 0.6407\n",
      "Epoch 0 step 132: training loss: 42368.3230918084\n",
      "Epoch 0 step 133: training accuarcy: 0.6431\n",
      "Epoch 0 step 133: training loss: 42887.16401329611\n",
      "Epoch 0 step 134: training accuarcy: 0.641\n",
      "Epoch 0 step 134: training loss: 42231.54951044463\n",
      "Epoch 0 step 135: training accuarcy: 0.6446000000000001\n",
      "Epoch 0 step 135: training loss: 42595.259927037776\n",
      "Epoch 0 step 136: training accuarcy: 0.6472\n",
      "Epoch 0 step 136: training loss: 42013.6985443755\n",
      "Epoch 0 step 137: training accuarcy: 0.6356\n",
      "Epoch 0 step 137: training loss: 42868.15281768461\n",
      "Epoch 0 step 138: training accuarcy: 0.643\n",
      "Epoch 0 step 138: training loss: 42220.30084343733\n",
      "Epoch 0 step 139: training accuarcy: 0.6471\n",
      "Epoch 0 step 139: training loss: 41952.01573456642\n",
      "Epoch 0 step 140: training accuarcy: 0.6458\n",
      "Epoch 0 step 140: training loss: 42007.574040781066\n",
      "Epoch 0 step 141: training accuarcy: 0.6458\n",
      "Epoch 0 step 141: training loss: 41297.929040276875\n",
      "Epoch 0 step 142: training accuarcy: 0.6527000000000001\n",
      "Epoch 0 step 142: training loss: 42353.42488434606\n",
      "Epoch 0 step 143: training accuarcy: 0.6402\n",
      "Epoch 0 step 143: training loss: 42402.404460157835\n",
      "Epoch 0 step 144: training accuarcy: 0.6468\n",
      "Epoch 0 step 144: training loss: 41679.55103347928\n",
      "Epoch 0 step 145: training accuarcy: 0.6496000000000001\n",
      "Epoch 0 step 145: training loss: 41379.19932344828\n",
      "Epoch 0 step 146: training accuarcy: 0.6585000000000001\n",
      "Epoch 0 step 146: training loss: 41709.56146845432\n",
      "Epoch 0 step 147: training accuarcy: 0.6478\n",
      "Epoch 0 step 147: training loss: 41974.13301484132\n",
      "Epoch 0 step 148: training accuarcy: 0.6538\n",
      "Epoch 0 step 148: training loss: 41876.355383770075\n",
      "Epoch 0 step 149: training accuarcy: 0.645\n",
      "Epoch 0 step 149: training loss: 41492.87403060056\n",
      "Epoch 0 step 150: training accuarcy: 0.6583\n",
      "Epoch 0 step 150: training loss: 40792.73472122817\n",
      "Epoch 0 step 151: training accuarcy: 0.6574\n",
      "Epoch 0 step 151: training loss: 41834.10270010105\n",
      "Epoch 0 step 152: training accuarcy: 0.6499\n",
      "Epoch 0 step 152: training loss: 39638.43272081352\n",
      "Epoch 0 step 153: training accuarcy: 0.6698000000000001\n",
      "Epoch 0 step 153: training loss: 41242.16898559815\n",
      "Epoch 0 step 154: training accuarcy: 0.6498\n",
      "Epoch 0 step 154: training loss: 40919.7628744457\n",
      "Epoch 0 step 155: training accuarcy: 0.6574\n",
      "Epoch 0 step 155: training loss: 41202.12115341227\n",
      "Epoch 0 step 156: training accuarcy: 0.6545000000000001\n",
      "Epoch 0 step 156: training loss: 40532.22515137412\n",
      "Epoch 0 step 157: training accuarcy: 0.6715\n",
      "Epoch 0 step 157: training loss: 40654.27276752574\n",
      "Epoch 0 step 158: training accuarcy: 0.6631\n",
      "Epoch 0 step 158: training loss: 40428.70883380744\n",
      "Epoch 0 step 159: training accuarcy: 0.6648000000000001\n",
      "Epoch 0 step 159: training loss: 40430.16551252845\n",
      "Epoch 0 step 160: training accuarcy: 0.6625\n",
      "Epoch 0 step 160: training loss: 40652.237257111454\n",
      "Epoch 0 step 161: training accuarcy: 0.6658000000000001\n",
      "Epoch 0 step 161: training loss: 40442.03522371963\n",
      "Epoch 0 step 162: training accuarcy: 0.6639\n",
      "Epoch 0 step 162: training loss: 40645.42320885142\n",
      "Epoch 0 step 163: training accuarcy: 0.66\n",
      "Epoch 0 step 163: training loss: 39996.68266652413\n",
      "Epoch 0 step 164: training accuarcy: 0.6768000000000001\n",
      "Epoch 0 step 164: training loss: 39451.0297184849\n",
      "Epoch 0 step 165: training accuarcy: 0.6804\n",
      "Epoch 0 step 165: training loss: 39950.06631709235\n",
      "Epoch 0 step 166: training accuarcy: 0.6674\n",
      "Epoch 0 step 166: training loss: 39731.829896121955\n",
      "Epoch 0 step 167: training accuarcy: 0.6697000000000001\n",
      "Epoch 0 step 167: training loss: 39967.67965756997\n",
      "Epoch 0 step 168: training accuarcy: 0.6711\n",
      "Epoch 0 step 168: training loss: 40143.3291097706\n",
      "Epoch 0 step 169: training accuarcy: 0.6776\n",
      "Epoch 0 step 169: training loss: 38946.2327888802\n",
      "Epoch 0 step 170: training accuarcy: 0.6893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 170: training loss: 39202.0034520271\n",
      "Epoch 0 step 171: training accuarcy: 0.6786\n",
      "Epoch 0 step 171: training loss: 39170.801937912816\n",
      "Epoch 0 step 172: training accuarcy: 0.6756\n",
      "Epoch 0 step 172: training loss: 39557.2955702388\n",
      "Epoch 0 step 173: training accuarcy: 0.669\n",
      "Epoch 0 step 173: training loss: 39067.8986854975\n",
      "Epoch 0 step 174: training accuarcy: 0.6762\n",
      "Epoch 0 step 174: training loss: 38976.90380625868\n",
      "Epoch 0 step 175: training accuarcy: 0.6856\n",
      "Epoch 0 step 175: training loss: 39097.7987199493\n",
      "Epoch 0 step 176: training accuarcy: 0.6751\n",
      "Epoch 0 step 176: training loss: 39096.42293060083\n",
      "Epoch 0 step 177: training accuarcy: 0.6756\n",
      "Epoch 0 step 177: training loss: 38485.49366859559\n",
      "Epoch 0 step 178: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 178: training loss: 39472.11341639771\n",
      "Epoch 0 step 179: training accuarcy: 0.6763\n",
      "Epoch 0 step 179: training loss: 39325.646085751294\n",
      "Epoch 0 step 180: training accuarcy: 0.6801\n",
      "Epoch 0 step 180: training loss: 39364.10734793135\n",
      "Epoch 0 step 181: training accuarcy: 0.6736000000000001\n",
      "Epoch 0 step 181: training loss: 38849.94146060744\n",
      "Epoch 0 step 182: training accuarcy: 0.6786\n",
      "Epoch 0 step 182: training loss: 39173.9603958497\n",
      "Epoch 0 step 183: training accuarcy: 0.677\n",
      "Epoch 0 step 183: training loss: 39635.106958330536\n",
      "Epoch 0 step 184: training accuarcy: 0.6771\n",
      "Epoch 0 step 184: training loss: 39208.81132969295\n",
      "Epoch 0 step 185: training accuarcy: 0.6734\n",
      "Epoch 0 step 185: training loss: 38789.465841460915\n",
      "Epoch 0 step 186: training accuarcy: 0.6871\n",
      "Epoch 0 step 186: training loss: 39736.747229759225\n",
      "Epoch 0 step 187: training accuarcy: 0.6766\n",
      "Epoch 0 step 187: training loss: 39323.29212486597\n",
      "Epoch 0 step 188: training accuarcy: 0.6768000000000001\n",
      "Epoch 0 step 188: training loss: 39048.59629402049\n",
      "Epoch 0 step 189: training accuarcy: 0.6846\n",
      "Epoch 0 step 189: training loss: 37374.00909550568\n",
      "Epoch 0 step 190: training accuarcy: 0.7057\n",
      "Epoch 0 step 190: training loss: 39056.9780573401\n",
      "Epoch 0 step 191: training accuarcy: 0.6754\n",
      "Epoch 0 step 191: training loss: 38486.23274049359\n",
      "Epoch 0 step 192: training accuarcy: 0.6865\n",
      "Epoch 0 step 192: training loss: 38398.69184772979\n",
      "Epoch 0 step 193: training accuarcy: 0.6885\n",
      "Epoch 0 step 193: training loss: 38413.40951290011\n",
      "Epoch 0 step 194: training accuarcy: 0.6892\n",
      "Epoch 0 step 194: training loss: 37501.47006082535\n",
      "Epoch 0 step 195: training accuarcy: 0.6972\n",
      "Epoch 0 step 195: training loss: 37677.232122782094\n",
      "Epoch 0 step 196: training accuarcy: 0.6943\n",
      "Epoch 0 step 196: training loss: 38406.32554695066\n",
      "Epoch 0 step 197: training accuarcy: 0.6913\n",
      "Epoch 0 step 197: training loss: 38433.87161495251\n",
      "Epoch 0 step 198: training accuarcy: 0.6915\n",
      "Epoch 0 step 198: training loss: 37971.87203167089\n",
      "Epoch 0 step 199: training accuarcy: 0.6994\n",
      "Epoch 0 step 199: training loss: 38504.17769634327\n",
      "Epoch 0 step 200: training accuarcy: 0.6929000000000001\n",
      "Epoch 0 step 200: training loss: 37708.26631785342\n",
      "Epoch 0 step 201: training accuarcy: 0.6937\n",
      "Epoch 0 step 201: training loss: 38587.582549317885\n",
      "Epoch 0 step 202: training accuarcy: 0.6879000000000001\n",
      "Epoch 0 step 202: training loss: 38174.53621833149\n",
      "Epoch 0 step 203: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 203: training loss: 38244.40277356554\n",
      "Epoch 0 step 204: training accuarcy: 0.6902\n",
      "Epoch 0 step 204: training loss: 38081.89409782494\n",
      "Epoch 0 step 205: training accuarcy: 0.6994\n",
      "Epoch 0 step 205: training loss: 36718.55301575451\n",
      "Epoch 0 step 206: training accuarcy: 0.7115\n",
      "Epoch 0 step 206: training loss: 37567.999639955895\n",
      "Epoch 0 step 207: training accuarcy: 0.6929000000000001\n",
      "Epoch 0 step 207: training loss: 37371.9546372238\n",
      "Epoch 0 step 208: training accuarcy: 0.6997\n",
      "Epoch 0 step 208: training loss: 37974.332624995164\n",
      "Epoch 0 step 209: training accuarcy: 0.6910000000000001\n",
      "Epoch 0 step 209: training loss: 37521.38619182989\n",
      "Epoch 0 step 210: training accuarcy: 0.6998000000000001\n",
      "Epoch 0 step 210: training loss: 37179.59751053986\n",
      "Epoch 0 step 211: training accuarcy: 0.7019000000000001\n",
      "Epoch 0 step 211: training loss: 36988.69344382011\n",
      "Epoch 0 step 212: training accuarcy: 0.7055\n",
      "Epoch 0 step 212: training loss: 36589.97509181199\n",
      "Epoch 0 step 213: training accuarcy: 0.7147\n",
      "Epoch 0 step 213: training loss: 36941.319995972925\n",
      "Epoch 0 step 214: training accuarcy: 0.7092\n",
      "Epoch 0 step 214: training loss: 37679.369496022955\n",
      "Epoch 0 step 215: training accuarcy: 0.6985\n",
      "Epoch 0 step 215: training loss: 37111.10253890523\n",
      "Epoch 0 step 216: training accuarcy: 0.7133\n",
      "Epoch 0 step 216: training loss: 36797.690407800495\n",
      "Epoch 0 step 217: training accuarcy: 0.7106\n",
      "Epoch 0 step 217: training loss: 37396.748765061464\n",
      "Epoch 0 step 218: training accuarcy: 0.7021000000000001\n",
      "Epoch 0 step 218: training loss: 37298.756098203376\n",
      "Epoch 0 step 219: training accuarcy: 0.6949000000000001\n",
      "Epoch 0 step 219: training loss: 37059.86564541167\n",
      "Epoch 0 step 220: training accuarcy: 0.7037\n",
      "Epoch 0 step 220: training loss: 37526.324279176064\n",
      "Epoch 0 step 221: training accuarcy: 0.7043\n",
      "Epoch 0 step 221: training loss: 37386.98382985782\n",
      "Epoch 0 step 222: training accuarcy: 0.6981\n",
      "Epoch 0 step 222: training loss: 37279.72338488457\n",
      "Epoch 0 step 223: training accuarcy: 0.7073\n",
      "Epoch 0 step 223: training loss: 37251.92267229447\n",
      "Epoch 0 step 224: training accuarcy: 0.7037\n",
      "Epoch 0 step 224: training loss: 36297.81249349617\n",
      "Epoch 0 step 225: training accuarcy: 0.7177\n",
      "Epoch 0 step 225: training loss: 36157.84497395159\n",
      "Epoch 0 step 226: training accuarcy: 0.7209\n",
      "Epoch 0 step 226: training loss: 37621.21377942727\n",
      "Epoch 0 step 227: training accuarcy: 0.6937\n",
      "Epoch 0 step 227: training loss: 37135.0136806387\n",
      "Epoch 0 step 228: training accuarcy: 0.7022\n",
      "Epoch 0 step 228: training loss: 37183.45011177058\n",
      "Epoch 0 step 229: training accuarcy: 0.7019000000000001\n",
      "Epoch 0 step 229: training loss: 36426.575025588594\n",
      "Epoch 0 step 230: training accuarcy: 0.7085\n",
      "Epoch 0 step 230: training loss: 36664.59114357017\n",
      "Epoch 0 step 231: training accuarcy: 0.7139\n",
      "Epoch 0 step 231: training loss: 36839.11500575305\n",
      "Epoch 0 step 232: training accuarcy: 0.7130000000000001\n",
      "Epoch 0 step 232: training loss: 36309.244827085444\n",
      "Epoch 0 step 233: training accuarcy: 0.7162000000000001\n",
      "Epoch 0 step 233: training loss: 35975.57297486562\n",
      "Epoch 0 step 234: training accuarcy: 0.7215\n",
      "Epoch 0 step 234: training loss: 36087.88928335821\n",
      "Epoch 0 step 235: training accuarcy: 0.7166\n",
      "Epoch 0 step 235: training loss: 36701.06899910561\n",
      "Epoch 0 step 236: training accuarcy: 0.7017\n",
      "Epoch 0 step 236: training loss: 36718.86285378187\n",
      "Epoch 0 step 237: training accuarcy: 0.7119000000000001\n",
      "Epoch 0 step 237: training loss: 36629.56285714996\n",
      "Epoch 0 step 238: training accuarcy: 0.7059000000000001\n",
      "Epoch 0 step 238: training loss: 35987.84967568511\n",
      "Epoch 0 step 239: training accuarcy: 0.7112\n",
      "Epoch 0 step 239: training loss: 35997.69075592249\n",
      "Epoch 0 step 240: training accuarcy: 0.7141000000000001\n",
      "Epoch 0 step 240: training loss: 36590.57572169325\n",
      "Epoch 0 step 241: training accuarcy: 0.7110000000000001\n",
      "Epoch 0 step 241: training loss: 36250.50318568181\n",
      "Epoch 0 step 242: training accuarcy: 0.7111000000000001\n",
      "Epoch 0 step 242: training loss: 36174.23376206614\n",
      "Epoch 0 step 243: training accuarcy: 0.7136\n",
      "Epoch 0 step 243: training loss: 36219.77574512607\n",
      "Epoch 0 step 244: training accuarcy: 0.7059000000000001\n",
      "Epoch 0 step 244: training loss: 35556.55232705056\n",
      "Epoch 0 step 245: training accuarcy: 0.7276\n",
      "Epoch 0 step 245: training loss: 35552.941122404154\n",
      "Epoch 0 step 246: training accuarcy: 0.7244\n",
      "Epoch 0 step 246: training loss: 36245.50773222487\n",
      "Epoch 0 step 247: training accuarcy: 0.7073\n",
      "Epoch 0 step 247: training loss: 36335.14419351912\n",
      "Epoch 0 step 248: training accuarcy: 0.7117\n",
      "Epoch 0 step 248: training loss: 35749.66628758664\n",
      "Epoch 0 step 249: training accuarcy: 0.7223\n",
      "Epoch 0 step 249: training loss: 36109.76084318312\n",
      "Epoch 0 step 250: training accuarcy: 0.7144\n",
      "Epoch 0 step 250: training loss: 36130.09847991765\n",
      "Epoch 0 step 251: training accuarcy: 0.7194\n",
      "Epoch 0 step 251: training loss: 35708.386421698015\n",
      "Epoch 0 step 252: training accuarcy: 0.7160000000000001\n",
      "Epoch 0 step 252: training loss: 35171.33923651849\n",
      "Epoch 0 step 253: training accuarcy: 0.7263000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 253: training loss: 35283.5836472917\n",
      "Epoch 0 step 254: training accuarcy: 0.7287\n",
      "Epoch 0 step 254: training loss: 36261.26602595637\n",
      "Epoch 0 step 255: training accuarcy: 0.7130000000000001\n",
      "Epoch 0 step 255: training loss: 35349.418825336514\n",
      "Epoch 0 step 256: training accuarcy: 0.7243\n",
      "Epoch 0 step 256: training loss: 35795.27394716085\n",
      "Epoch 0 step 257: training accuarcy: 0.7185\n",
      "Epoch 0 step 257: training loss: 35761.73537550089\n",
      "Epoch 0 step 258: training accuarcy: 0.7175\n",
      "Epoch 0 step 258: training loss: 35705.99420170844\n",
      "Epoch 0 step 259: training accuarcy: 0.7271000000000001\n",
      "Epoch 0 step 259: training loss: 35823.21189438501\n",
      "Epoch 0 step 260: training accuarcy: 0.7209\n",
      "Epoch 0 step 260: training loss: 36391.402454865245\n",
      "Epoch 0 step 261: training accuarcy: 0.7184\n",
      "Epoch 0 step 261: training loss: 35469.310334724505\n",
      "Epoch 0 step 262: training accuarcy: 0.7233\n",
      "Epoch 0 step 262: training loss: 19016.696263744274\n",
      "Epoch 0 step 263: training accuarcy: 0.7282051282051282\n",
      "Epoch 0: train loss 69507.04873029521, train accuarcy 0.6197700500488281\n",
      "Epoch 0: valid loss 34176.60520719804, valid accuarcy 0.7367500066757202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [04:44<09:28, 284.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 33897.73526608891\n",
      "Epoch 1 step 264: training accuarcy: 0.7512000000000001\n",
      "Epoch 1 step 264: training loss: 33627.92073109104\n",
      "Epoch 1 step 265: training accuarcy: 0.7629\n",
      "Epoch 1 step 265: training loss: 33859.43875398199\n",
      "Epoch 1 step 266: training accuarcy: 0.755\n",
      "Epoch 1 step 266: training loss: 34419.494406719954\n",
      "Epoch 1 step 267: training accuarcy: 0.7502000000000001\n",
      "Epoch 1 step 267: training loss: 33958.08873282592\n",
      "Epoch 1 step 268: training accuarcy: 0.7549\n",
      "Epoch 1 step 268: training loss: 34579.60479484293\n",
      "Epoch 1 step 269: training accuarcy: 0.744\n",
      "Epoch 1 step 269: training loss: 33975.558784306486\n",
      "Epoch 1 step 270: training accuarcy: 0.75\n",
      "Epoch 1 step 270: training loss: 33680.218308659765\n",
      "Epoch 1 step 271: training accuarcy: 0.7542\n",
      "Epoch 1 step 271: training loss: 34217.83717496517\n",
      "Epoch 1 step 272: training accuarcy: 0.7432000000000001\n",
      "Epoch 1 step 272: training loss: 33931.95661477577\n",
      "Epoch 1 step 273: training accuarcy: 0.7496\n",
      "Epoch 1 step 273: training loss: 33898.45510332137\n",
      "Epoch 1 step 274: training accuarcy: 0.7546\n",
      "Epoch 1 step 274: training loss: 34387.85024004977\n",
      "Epoch 1 step 275: training accuarcy: 0.7467\n",
      "Epoch 1 step 275: training loss: 33618.189885150656\n",
      "Epoch 1 step 276: training accuarcy: 0.7493000000000001\n",
      "Epoch 1 step 276: training loss: 33447.34793927985\n",
      "Epoch 1 step 277: training accuarcy: 0.755\n",
      "Epoch 1 step 277: training loss: 34338.75263358942\n",
      "Epoch 1 step 278: training accuarcy: 0.7507\n",
      "Epoch 1 step 278: training loss: 33778.05629960699\n",
      "Epoch 1 step 279: training accuarcy: 0.751\n",
      "Epoch 1 step 279: training loss: 33160.649189603835\n",
      "Epoch 1 step 280: training accuarcy: 0.7623000000000001\n",
      "Epoch 1 step 280: training loss: 33962.248747366466\n",
      "Epoch 1 step 281: training accuarcy: 0.7488\n",
      "Epoch 1 step 281: training loss: 33441.95899129592\n",
      "Epoch 1 step 282: training accuarcy: 0.7608\n",
      "Epoch 1 step 282: training loss: 33327.85852112321\n",
      "Epoch 1 step 283: training accuarcy: 0.7549\n",
      "Epoch 1 step 283: training loss: 33923.82041164738\n",
      "Epoch 1 step 284: training accuarcy: 0.7516\n",
      "Epoch 1 step 284: training loss: 33179.29775718394\n",
      "Epoch 1 step 285: training accuarcy: 0.7589\n",
      "Epoch 1 step 285: training loss: 33426.693047328634\n",
      "Epoch 1 step 286: training accuarcy: 0.7611\n",
      "Epoch 1 step 286: training loss: 33864.99457203272\n",
      "Epoch 1 step 287: training accuarcy: 0.7496\n",
      "Epoch 1 step 287: training loss: 33575.075097061606\n",
      "Epoch 1 step 288: training accuarcy: 0.7569\n",
      "Epoch 1 step 288: training loss: 33550.44780277942\n",
      "Epoch 1 step 289: training accuarcy: 0.7527\n",
      "Epoch 1 step 289: training loss: 33598.1135096469\n",
      "Epoch 1 step 290: training accuarcy: 0.7524000000000001\n",
      "Epoch 1 step 290: training loss: 33484.45293324196\n",
      "Epoch 1 step 291: training accuarcy: 0.7505000000000001\n",
      "Epoch 1 step 291: training loss: 33893.31591089372\n",
      "Epoch 1 step 292: training accuarcy: 0.7468\n",
      "Epoch 1 step 292: training loss: 32821.87965828134\n",
      "Epoch 1 step 293: training accuarcy: 0.7568\n",
      "Epoch 1 step 293: training loss: 33950.121212744685\n",
      "Epoch 1 step 294: training accuarcy: 0.7464000000000001\n",
      "Epoch 1 step 294: training loss: 33342.88358841283\n",
      "Epoch 1 step 295: training accuarcy: 0.7498\n",
      "Epoch 1 step 295: training loss: 32637.083185044903\n",
      "Epoch 1 step 296: training accuarcy: 0.7684000000000001\n",
      "Epoch 1 step 296: training loss: 33690.67167560696\n",
      "Epoch 1 step 297: training accuarcy: 0.7521\n",
      "Epoch 1 step 297: training loss: 33643.373256766135\n",
      "Epoch 1 step 298: training accuarcy: 0.7513000000000001\n",
      "Epoch 1 step 298: training loss: 33330.11631209559\n",
      "Epoch 1 step 299: training accuarcy: 0.7551\n",
      "Epoch 1 step 299: training loss: 33810.25767421289\n",
      "Epoch 1 step 300: training accuarcy: 0.7427\n",
      "Epoch 1 step 300: training loss: 32916.467535879645\n",
      "Epoch 1 step 301: training accuarcy: 0.7632\n",
      "Epoch 1 step 301: training loss: 33502.34684640385\n",
      "Epoch 1 step 302: training accuarcy: 0.7495\n",
      "Epoch 1 step 302: training loss: 33457.573543494545\n",
      "Epoch 1 step 303: training accuarcy: 0.7556\n",
      "Epoch 1 step 303: training loss: 33195.32564912731\n",
      "Epoch 1 step 304: training accuarcy: 0.7522000000000001\n",
      "Epoch 1 step 304: training loss: 33068.92145992437\n",
      "Epoch 1 step 305: training accuarcy: 0.7526\n",
      "Epoch 1 step 305: training loss: 33946.44745189696\n",
      "Epoch 1 step 306: training accuarcy: 0.7539\n",
      "Epoch 1 step 306: training loss: 33162.79082708075\n",
      "Epoch 1 step 307: training accuarcy: 0.7568\n",
      "Epoch 1 step 307: training loss: 33483.48400808708\n",
      "Epoch 1 step 308: training accuarcy: 0.7475\n",
      "Epoch 1 step 308: training loss: 33038.44627216138\n",
      "Epoch 1 step 309: training accuarcy: 0.7573000000000001\n",
      "Epoch 1 step 309: training loss: 33363.73430353974\n",
      "Epoch 1 step 310: training accuarcy: 0.7611\n",
      "Epoch 1 step 310: training loss: 32713.876858404597\n",
      "Epoch 1 step 311: training accuarcy: 0.7601\n",
      "Epoch 1 step 311: training loss: 32752.870026137392\n",
      "Epoch 1 step 312: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 312: training loss: 33337.90962280267\n",
      "Epoch 1 step 313: training accuarcy: 0.755\n",
      "Epoch 1 step 313: training loss: 33440.00165638448\n",
      "Epoch 1 step 314: training accuarcy: 0.7553000000000001\n",
      "Epoch 1 step 314: training loss: 33080.83486002656\n",
      "Epoch 1 step 315: training accuarcy: 0.7565000000000001\n",
      "Epoch 1 step 315: training loss: 33557.914359070375\n",
      "Epoch 1 step 316: training accuarcy: 0.7553000000000001\n",
      "Epoch 1 step 316: training loss: 33290.84712263517\n",
      "Epoch 1 step 317: training accuarcy: 0.7517\n",
      "Epoch 1 step 317: training loss: 32794.41550657168\n",
      "Epoch 1 step 318: training accuarcy: 0.7596\n",
      "Epoch 1 step 318: training loss: 33309.86865743852\n",
      "Epoch 1 step 319: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 319: training loss: 33463.351941748355\n",
      "Epoch 1 step 320: training accuarcy: 0.7502000000000001\n",
      "Epoch 1 step 320: training loss: 32963.2043193485\n",
      "Epoch 1 step 321: training accuarcy: 0.7574000000000001\n",
      "Epoch 1 step 321: training loss: 33265.96348891458\n",
      "Epoch 1 step 322: training accuarcy: 0.744\n",
      "Epoch 1 step 322: training loss: 32878.95551609124\n",
      "Epoch 1 step 323: training accuarcy: 0.7539\n",
      "Epoch 1 step 323: training loss: 32430.965472522377\n",
      "Epoch 1 step 324: training accuarcy: 0.762\n",
      "Epoch 1 step 324: training loss: 33063.33835467138\n",
      "Epoch 1 step 325: training accuarcy: 0.7575000000000001\n",
      "Epoch 1 step 325: training loss: 33046.692496496\n",
      "Epoch 1 step 326: training accuarcy: 0.7546\n",
      "Epoch 1 step 326: training loss: 33003.77914897269\n",
      "Epoch 1 step 327: training accuarcy: 0.7554000000000001\n",
      "Epoch 1 step 327: training loss: 33494.02876265966\n",
      "Epoch 1 step 328: training accuarcy: 0.7496\n",
      "Epoch 1 step 328: training loss: 33344.277414221106\n",
      "Epoch 1 step 329: training accuarcy: 0.7515000000000001\n",
      "Epoch 1 step 329: training loss: 33040.93526571947\n",
      "Epoch 1 step 330: training accuarcy: 0.7552\n",
      "Epoch 1 step 330: training loss: 32671.145095879747\n",
      "Epoch 1 step 331: training accuarcy: 0.7584000000000001\n",
      "Epoch 1 step 331: training loss: 32936.312434262334\n",
      "Epoch 1 step 332: training accuarcy: 0.758\n",
      "Epoch 1 step 332: training loss: 32669.99215151014\n",
      "Epoch 1 step 333: training accuarcy: 0.7587\n",
      "Epoch 1 step 333: training loss: 32941.216285688046\n",
      "Epoch 1 step 334: training accuarcy: 0.7567\n",
      "Epoch 1 step 334: training loss: 33074.02828427464\n",
      "Epoch 1 step 335: training accuarcy: 0.7512000000000001\n",
      "Epoch 1 step 335: training loss: 32871.65197144772\n",
      "Epoch 1 step 336: training accuarcy: 0.7544000000000001\n",
      "Epoch 1 step 336: training loss: 33798.76394094384\n",
      "Epoch 1 step 337: training accuarcy: 0.7418\n",
      "Epoch 1 step 337: training loss: 32565.333036479475\n",
      "Epoch 1 step 338: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 338: training loss: 32668.522789018873\n",
      "Epoch 1 step 339: training accuarcy: 0.7609\n",
      "Epoch 1 step 339: training loss: 33041.611571360125\n",
      "Epoch 1 step 340: training accuarcy: 0.7544000000000001\n",
      "Epoch 1 step 340: training loss: 32847.492937207164\n",
      "Epoch 1 step 341: training accuarcy: 0.7533000000000001\n",
      "Epoch 1 step 341: training loss: 32841.13285299356\n",
      "Epoch 1 step 342: training accuarcy: 0.7604000000000001\n",
      "Epoch 1 step 342: training loss: 33178.733410511915\n",
      "Epoch 1 step 343: training accuarcy: 0.7447\n",
      "Epoch 1 step 343: training loss: 33413.976098994826\n",
      "Epoch 1 step 344: training accuarcy: 0.7519\n",
      "Epoch 1 step 344: training loss: 32709.625318869756\n",
      "Epoch 1 step 345: training accuarcy: 0.7549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 345: training loss: 32833.12797665099\n",
      "Epoch 1 step 346: training accuarcy: 0.7472000000000001\n",
      "Epoch 1 step 346: training loss: 32301.024615882332\n",
      "Epoch 1 step 347: training accuarcy: 0.7606\n",
      "Epoch 1 step 347: training loss: 32612.646994211987\n",
      "Epoch 1 step 348: training accuarcy: 0.7573000000000001\n",
      "Epoch 1 step 348: training loss: 32789.26414775667\n",
      "Epoch 1 step 349: training accuarcy: 0.7627\n",
      "Epoch 1 step 349: training loss: 32875.51866975961\n",
      "Epoch 1 step 350: training accuarcy: 0.755\n",
      "Epoch 1 step 350: training loss: 32283.088627578123\n",
      "Epoch 1 step 351: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 351: training loss: 32758.290694526935\n",
      "Epoch 1 step 352: training accuarcy: 0.7544000000000001\n",
      "Epoch 1 step 352: training loss: 32729.832013708365\n",
      "Epoch 1 step 353: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 353: training loss: 32552.40321312695\n",
      "Epoch 1 step 354: training accuarcy: 0.7593000000000001\n",
      "Epoch 1 step 354: training loss: 33065.70785128449\n",
      "Epoch 1 step 355: training accuarcy: 0.7602\n",
      "Epoch 1 step 355: training loss: 32746.836344509866\n",
      "Epoch 1 step 356: training accuarcy: 0.7551\n",
      "Epoch 1 step 356: training loss: 32792.174047386645\n",
      "Epoch 1 step 357: training accuarcy: 0.7545000000000001\n",
      "Epoch 1 step 357: training loss: 31907.405215076466\n",
      "Epoch 1 step 358: training accuarcy: 0.7672\n",
      "Epoch 1 step 358: training loss: 31889.518848441057\n",
      "Epoch 1 step 359: training accuarcy: 0.7585000000000001\n",
      "Epoch 1 step 359: training loss: 32531.070171478135\n",
      "Epoch 1 step 360: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 360: training loss: 32817.18331652684\n",
      "Epoch 1 step 361: training accuarcy: 0.7556\n",
      "Epoch 1 step 361: training loss: 32352.905730441646\n",
      "Epoch 1 step 362: training accuarcy: 0.7566\n",
      "Epoch 1 step 362: training loss: 32599.608607521204\n",
      "Epoch 1 step 363: training accuarcy: 0.7594000000000001\n",
      "Epoch 1 step 363: training loss: 32196.501833628714\n",
      "Epoch 1 step 364: training accuarcy: 0.762\n",
      "Epoch 1 step 364: training loss: 32321.346374023153\n",
      "Epoch 1 step 365: training accuarcy: 0.7592\n",
      "Epoch 1 step 365: training loss: 32669.111020846936\n",
      "Epoch 1 step 366: training accuarcy: 0.7662\n",
      "Epoch 1 step 366: training loss: 32840.23340395061\n",
      "Epoch 1 step 367: training accuarcy: 0.7496\n",
      "Epoch 1 step 367: training loss: 32704.535348515645\n",
      "Epoch 1 step 368: training accuarcy: 0.7505000000000001\n",
      "Epoch 1 step 368: training loss: 31966.6246183866\n",
      "Epoch 1 step 369: training accuarcy: 0.7598\n",
      "Epoch 1 step 369: training loss: 32029.66763187691\n",
      "Epoch 1 step 370: training accuarcy: 0.764\n",
      "Epoch 1 step 370: training loss: 32168.448043466473\n",
      "Epoch 1 step 371: training accuarcy: 0.7587\n",
      "Epoch 1 step 371: training loss: 33164.511349409724\n",
      "Epoch 1 step 372: training accuarcy: 0.7483000000000001\n",
      "Epoch 1 step 372: training loss: 32207.86436295799\n",
      "Epoch 1 step 373: training accuarcy: 0.7633000000000001\n",
      "Epoch 1 step 373: training loss: 32318.409828974625\n",
      "Epoch 1 step 374: training accuarcy: 0.7607\n",
      "Epoch 1 step 374: training loss: 32534.864342578825\n",
      "Epoch 1 step 375: training accuarcy: 0.7601\n",
      "Epoch 1 step 375: training loss: 31711.09836718583\n",
      "Epoch 1 step 376: training accuarcy: 0.7687\n",
      "Epoch 1 step 376: training loss: 31745.63953451581\n",
      "Epoch 1 step 377: training accuarcy: 0.7677\n",
      "Epoch 1 step 377: training loss: 32807.243137457124\n",
      "Epoch 1 step 378: training accuarcy: 0.7497\n",
      "Epoch 1 step 378: training loss: 32417.872862001605\n",
      "Epoch 1 step 379: training accuarcy: 0.7496\n",
      "Epoch 1 step 379: training loss: 32335.290759972147\n",
      "Epoch 1 step 380: training accuarcy: 0.7639\n",
      "Epoch 1 step 380: training loss: 32199.06204037211\n",
      "Epoch 1 step 381: training accuarcy: 0.7586\n",
      "Epoch 1 step 381: training loss: 32340.294214253678\n",
      "Epoch 1 step 382: training accuarcy: 0.7572\n",
      "Epoch 1 step 382: training loss: 32006.661291125332\n",
      "Epoch 1 step 383: training accuarcy: 0.7614000000000001\n",
      "Epoch 1 step 383: training loss: 32535.134146780878\n",
      "Epoch 1 step 384: training accuarcy: 0.7566\n",
      "Epoch 1 step 384: training loss: 31724.313978381484\n",
      "Epoch 1 step 385: training accuarcy: 0.7628\n",
      "Epoch 1 step 385: training loss: 32749.06921809152\n",
      "Epoch 1 step 386: training accuarcy: 0.7523000000000001\n",
      "Epoch 1 step 386: training loss: 31601.202044580958\n",
      "Epoch 1 step 387: training accuarcy: 0.7692\n",
      "Epoch 1 step 387: training loss: 31960.277607696822\n",
      "Epoch 1 step 388: training accuarcy: 0.7511\n",
      "Epoch 1 step 388: training loss: 32803.5282594162\n",
      "Epoch 1 step 389: training accuarcy: 0.7528\n",
      "Epoch 1 step 389: training loss: 31893.80161555691\n",
      "Epoch 1 step 390: training accuarcy: 0.7572\n",
      "Epoch 1 step 390: training loss: 32377.686256778063\n",
      "Epoch 1 step 391: training accuarcy: 0.7553000000000001\n",
      "Epoch 1 step 391: training loss: 32239.707307534947\n",
      "Epoch 1 step 392: training accuarcy: 0.7572\n",
      "Epoch 1 step 392: training loss: 32682.71947744873\n",
      "Epoch 1 step 393: training accuarcy: 0.7557\n",
      "Epoch 1 step 393: training loss: 32210.92010356725\n",
      "Epoch 1 step 394: training accuarcy: 0.7604000000000001\n",
      "Epoch 1 step 394: training loss: 31968.581868517063\n",
      "Epoch 1 step 395: training accuarcy: 0.7575000000000001\n",
      "Epoch 1 step 395: training loss: 32165.300826280483\n",
      "Epoch 1 step 396: training accuarcy: 0.7609\n",
      "Epoch 1 step 396: training loss: 31790.990701162493\n",
      "Epoch 1 step 397: training accuarcy: 0.7675000000000001\n",
      "Epoch 1 step 397: training loss: 32255.09904862519\n",
      "Epoch 1 step 398: training accuarcy: 0.7546\n",
      "Epoch 1 step 398: training loss: 32186.83098786801\n",
      "Epoch 1 step 399: training accuarcy: 0.7534000000000001\n",
      "Epoch 1 step 399: training loss: 32414.69396507468\n",
      "Epoch 1 step 400: training accuarcy: 0.7541\n",
      "Epoch 1 step 400: training loss: 31813.506357457867\n",
      "Epoch 1 step 401: training accuarcy: 0.7708\n",
      "Epoch 1 step 401: training loss: 31927.954225857153\n",
      "Epoch 1 step 402: training accuarcy: 0.769\n",
      "Epoch 1 step 402: training loss: 32057.412532388593\n",
      "Epoch 1 step 403: training accuarcy: 0.765\n",
      "Epoch 1 step 403: training loss: 31809.398318097803\n",
      "Epoch 1 step 404: training accuarcy: 0.7659\n",
      "Epoch 1 step 404: training loss: 32243.087835203733\n",
      "Epoch 1 step 405: training accuarcy: 0.7564000000000001\n",
      "Epoch 1 step 405: training loss: 31521.77031785419\n",
      "Epoch 1 step 406: training accuarcy: 0.7721\n",
      "Epoch 1 step 406: training loss: 31640.17240192563\n",
      "Epoch 1 step 407: training accuarcy: 0.7636000000000001\n",
      "Epoch 1 step 407: training loss: 31297.734369875365\n",
      "Epoch 1 step 408: training accuarcy: 0.7694000000000001\n",
      "Epoch 1 step 408: training loss: 31967.934905719936\n",
      "Epoch 1 step 409: training accuarcy: 0.759\n",
      "Epoch 1 step 409: training loss: 31191.621507700176\n",
      "Epoch 1 step 410: training accuarcy: 0.769\n",
      "Epoch 1 step 410: training loss: 32410.047695145528\n",
      "Epoch 1 step 411: training accuarcy: 0.758\n",
      "Epoch 1 step 411: training loss: 32072.95696705023\n",
      "Epoch 1 step 412: training accuarcy: 0.7617\n",
      "Epoch 1 step 412: training loss: 31815.97274926536\n",
      "Epoch 1 step 413: training accuarcy: 0.7542\n",
      "Epoch 1 step 413: training loss: 31100.34296249274\n",
      "Epoch 1 step 414: training accuarcy: 0.7677\n",
      "Epoch 1 step 414: training loss: 31807.693189530313\n",
      "Epoch 1 step 415: training accuarcy: 0.763\n",
      "Epoch 1 step 415: training loss: 31683.69475984109\n",
      "Epoch 1 step 416: training accuarcy: 0.7664000000000001\n",
      "Epoch 1 step 416: training loss: 31492.21572833546\n",
      "Epoch 1 step 417: training accuarcy: 0.7615000000000001\n",
      "Epoch 1 step 417: training loss: 32030.29921588873\n",
      "Epoch 1 step 418: training accuarcy: 0.7539\n",
      "Epoch 1 step 418: training loss: 31742.47585022003\n",
      "Epoch 1 step 419: training accuarcy: 0.768\n",
      "Epoch 1 step 419: training loss: 31850.55601001879\n",
      "Epoch 1 step 420: training accuarcy: 0.757\n",
      "Epoch 1 step 420: training loss: 31596.01608605757\n",
      "Epoch 1 step 421: training accuarcy: 0.7702\n",
      "Epoch 1 step 421: training loss: 31446.833316659286\n",
      "Epoch 1 step 422: training accuarcy: 0.765\n",
      "Epoch 1 step 422: training loss: 31418.75077276092\n",
      "Epoch 1 step 423: training accuarcy: 0.7662\n",
      "Epoch 1 step 423: training loss: 32438.971586927968\n",
      "Epoch 1 step 424: training accuarcy: 0.7549\n",
      "Epoch 1 step 424: training loss: 31720.199972442824\n",
      "Epoch 1 step 425: training accuarcy: 0.7598\n",
      "Epoch 1 step 425: training loss: 31931.866868319114\n",
      "Epoch 1 step 426: training accuarcy: 0.7585000000000001\n",
      "Epoch 1 step 426: training loss: 31794.391303338823\n",
      "Epoch 1 step 427: training accuarcy: 0.7599\n",
      "Epoch 1 step 427: training loss: 31916.702145435374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 428: training accuarcy: 0.7621\n",
      "Epoch 1 step 428: training loss: 31340.68653146692\n",
      "Epoch 1 step 429: training accuarcy: 0.7647\n",
      "Epoch 1 step 429: training loss: 31231.03471677229\n",
      "Epoch 1 step 430: training accuarcy: 0.7698\n",
      "Epoch 1 step 430: training loss: 31528.196175496978\n",
      "Epoch 1 step 431: training accuarcy: 0.7651\n",
      "Epoch 1 step 431: training loss: 31313.9731503945\n",
      "Epoch 1 step 432: training accuarcy: 0.7635000000000001\n",
      "Epoch 1 step 432: training loss: 32011.114723363236\n",
      "Epoch 1 step 433: training accuarcy: 0.7627\n",
      "Epoch 1 step 433: training loss: 31685.05147831228\n",
      "Epoch 1 step 434: training accuarcy: 0.7606\n",
      "Epoch 1 step 434: training loss: 31442.05208022455\n",
      "Epoch 1 step 435: training accuarcy: 0.7678\n",
      "Epoch 1 step 435: training loss: 30933.32182664942\n",
      "Epoch 1 step 436: training accuarcy: 0.7799\n",
      "Epoch 1 step 436: training loss: 31461.20992526184\n",
      "Epoch 1 step 437: training accuarcy: 0.769\n",
      "Epoch 1 step 437: training loss: 31561.013876726684\n",
      "Epoch 1 step 438: training accuarcy: 0.7659\n",
      "Epoch 1 step 438: training loss: 31364.77981067232\n",
      "Epoch 1 step 439: training accuarcy: 0.7624000000000001\n",
      "Epoch 1 step 439: training loss: 31239.816823571964\n",
      "Epoch 1 step 440: training accuarcy: 0.7616\n",
      "Epoch 1 step 440: training loss: 31933.709632518756\n",
      "Epoch 1 step 441: training accuarcy: 0.7577\n",
      "Epoch 1 step 441: training loss: 31711.891328222006\n",
      "Epoch 1 step 442: training accuarcy: 0.7576\n",
      "Epoch 1 step 442: training loss: 31189.2733942036\n",
      "Epoch 1 step 443: training accuarcy: 0.7704000000000001\n",
      "Epoch 1 step 443: training loss: 31198.186381080544\n",
      "Epoch 1 step 444: training accuarcy: 0.7643000000000001\n",
      "Epoch 1 step 444: training loss: 31530.42542307868\n",
      "Epoch 1 step 445: training accuarcy: 0.7563000000000001\n",
      "Epoch 1 step 445: training loss: 31254.359650476166\n",
      "Epoch 1 step 446: training accuarcy: 0.7661\n",
      "Epoch 1 step 446: training loss: 31344.21264066226\n",
      "Epoch 1 step 447: training accuarcy: 0.7633000000000001\n",
      "Epoch 1 step 447: training loss: 31398.549995984442\n",
      "Epoch 1 step 448: training accuarcy: 0.7621\n",
      "Epoch 1 step 448: training loss: 31208.688716033197\n",
      "Epoch 1 step 449: training accuarcy: 0.7673\n",
      "Epoch 1 step 449: training loss: 30811.59059958956\n",
      "Epoch 1 step 450: training accuarcy: 0.7705000000000001\n",
      "Epoch 1 step 450: training loss: 31516.754948560676\n",
      "Epoch 1 step 451: training accuarcy: 0.7743\n",
      "Epoch 1 step 451: training loss: 31761.84011342301\n",
      "Epoch 1 step 452: training accuarcy: 0.7618\n",
      "Epoch 1 step 452: training loss: 31041.296678358605\n",
      "Epoch 1 step 453: training accuarcy: 0.7699\n",
      "Epoch 1 step 453: training loss: 31639.513659157183\n",
      "Epoch 1 step 454: training accuarcy: 0.7613000000000001\n",
      "Epoch 1 step 454: training loss: 31127.263651678582\n",
      "Epoch 1 step 455: training accuarcy: 0.7656000000000001\n",
      "Epoch 1 step 455: training loss: 30789.276657678794\n",
      "Epoch 1 step 456: training accuarcy: 0.7689\n",
      "Epoch 1 step 456: training loss: 31322.65418290154\n",
      "Epoch 1 step 457: training accuarcy: 0.7649\n",
      "Epoch 1 step 457: training loss: 30397.547531135362\n",
      "Epoch 1 step 458: training accuarcy: 0.777\n",
      "Epoch 1 step 458: training loss: 31011.154911169462\n",
      "Epoch 1 step 459: training accuarcy: 0.7665000000000001\n",
      "Epoch 1 step 459: training loss: 31243.590652410912\n",
      "Epoch 1 step 460: training accuarcy: 0.7623000000000001\n",
      "Epoch 1 step 460: training loss: 30780.38506041343\n",
      "Epoch 1 step 461: training accuarcy: 0.7698\n",
      "Epoch 1 step 461: training loss: 31329.320897118687\n",
      "Epoch 1 step 462: training accuarcy: 0.7625000000000001\n",
      "Epoch 1 step 462: training loss: 31231.386822193595\n",
      "Epoch 1 step 463: training accuarcy: 0.7679\n",
      "Epoch 1 step 463: training loss: 30532.79111943255\n",
      "Epoch 1 step 464: training accuarcy: 0.7722\n",
      "Epoch 1 step 464: training loss: 30177.550774406896\n",
      "Epoch 1 step 465: training accuarcy: 0.7777000000000001\n",
      "Epoch 1 step 465: training loss: 31177.168457109827\n",
      "Epoch 1 step 466: training accuarcy: 0.7703\n",
      "Epoch 1 step 466: training loss: 30743.949649857106\n",
      "Epoch 1 step 467: training accuarcy: 0.7731\n",
      "Epoch 1 step 467: training loss: 31971.961389356933\n",
      "Epoch 1 step 468: training accuarcy: 0.7559\n",
      "Epoch 1 step 468: training loss: 31534.106986922947\n",
      "Epoch 1 step 469: training accuarcy: 0.7634000000000001\n",
      "Epoch 1 step 469: training loss: 30927.3646369544\n",
      "Epoch 1 step 470: training accuarcy: 0.7637\n",
      "Epoch 1 step 470: training loss: 31196.57612729846\n",
      "Epoch 1 step 471: training accuarcy: 0.7523000000000001\n",
      "Epoch 1 step 471: training loss: 30690.6552643336\n",
      "Epoch 1 step 472: training accuarcy: 0.7682\n",
      "Epoch 1 step 472: training loss: 31515.4903580242\n",
      "Epoch 1 step 473: training accuarcy: 0.7693\n",
      "Epoch 1 step 473: training loss: 30558.818359656518\n",
      "Epoch 1 step 474: training accuarcy: 0.7727\n",
      "Epoch 1 step 474: training loss: 31655.666046955404\n",
      "Epoch 1 step 475: training accuarcy: 0.7585000000000001\n",
      "Epoch 1 step 475: training loss: 31191.623881723255\n",
      "Epoch 1 step 476: training accuarcy: 0.7675000000000001\n",
      "Epoch 1 step 476: training loss: 31234.00270141621\n",
      "Epoch 1 step 477: training accuarcy: 0.7746000000000001\n",
      "Epoch 1 step 477: training loss: 30509.689755217674\n",
      "Epoch 1 step 478: training accuarcy: 0.7754000000000001\n",
      "Epoch 1 step 478: training loss: 30428.226165934226\n",
      "Epoch 1 step 479: training accuarcy: 0.7788\n",
      "Epoch 1 step 479: training loss: 30213.59417472701\n",
      "Epoch 1 step 480: training accuarcy: 0.7753\n",
      "Epoch 1 step 480: training loss: 31317.067532237128\n",
      "Epoch 1 step 481: training accuarcy: 0.7721\n",
      "Epoch 1 step 481: training loss: 31047.274294729763\n",
      "Epoch 1 step 482: training accuarcy: 0.7676000000000001\n",
      "Epoch 1 step 482: training loss: 30449.31369445541\n",
      "Epoch 1 step 483: training accuarcy: 0.7767000000000001\n",
      "Epoch 1 step 483: training loss: 30874.05929711971\n",
      "Epoch 1 step 484: training accuarcy: 0.7788\n",
      "Epoch 1 step 484: training loss: 30874.263249790347\n",
      "Epoch 1 step 485: training accuarcy: 0.7621\n",
      "Epoch 1 step 485: training loss: 30786.90260452978\n",
      "Epoch 1 step 486: training accuarcy: 0.7736000000000001\n",
      "Epoch 1 step 486: training loss: 30786.908273640103\n",
      "Epoch 1 step 487: training accuarcy: 0.7685000000000001\n",
      "Epoch 1 step 487: training loss: 30817.730149620955\n",
      "Epoch 1 step 488: training accuarcy: 0.7648\n",
      "Epoch 1 step 488: training loss: 31200.305699358658\n",
      "Epoch 1 step 489: training accuarcy: 0.7613000000000001\n",
      "Epoch 1 step 489: training loss: 30336.690212818536\n",
      "Epoch 1 step 490: training accuarcy: 0.7804\n",
      "Epoch 1 step 490: training loss: 31006.561547345373\n",
      "Epoch 1 step 491: training accuarcy: 0.7673\n",
      "Epoch 1 step 491: training loss: 30709.58730930012\n",
      "Epoch 1 step 492: training accuarcy: 0.7688\n",
      "Epoch 1 step 492: training loss: 30986.071755612833\n",
      "Epoch 1 step 493: training accuarcy: 0.7639\n",
      "Epoch 1 step 493: training loss: 30672.419556552413\n",
      "Epoch 1 step 494: training accuarcy: 0.7715000000000001\n",
      "Epoch 1 step 494: training loss: 30845.331882341314\n",
      "Epoch 1 step 495: training accuarcy: 0.7632\n",
      "Epoch 1 step 495: training loss: 30785.577561886672\n",
      "Epoch 1 step 496: training accuarcy: 0.766\n",
      "Epoch 1 step 496: training loss: 30879.468770475927\n",
      "Epoch 1 step 497: training accuarcy: 0.7658\n",
      "Epoch 1 step 497: training loss: 30860.1428067725\n",
      "Epoch 1 step 498: training accuarcy: 0.7682\n",
      "Epoch 1 step 498: training loss: 30316.78826274063\n",
      "Epoch 1 step 499: training accuarcy: 0.7712\n",
      "Epoch 1 step 499: training loss: 30878.565826657614\n",
      "Epoch 1 step 500: training accuarcy: 0.7683\n",
      "Epoch 1 step 500: training loss: 30742.70694046261\n",
      "Epoch 1 step 501: training accuarcy: 0.7727\n",
      "Epoch 1 step 501: training loss: 29924.750974033872\n",
      "Epoch 1 step 502: training accuarcy: 0.7851\n",
      "Epoch 1 step 502: training loss: 30890.079474844802\n",
      "Epoch 1 step 503: training accuarcy: 0.7692\n",
      "Epoch 1 step 503: training loss: 30413.945626521447\n",
      "Epoch 1 step 504: training accuarcy: 0.7676000000000001\n",
      "Epoch 1 step 504: training loss: 30681.637316146698\n",
      "Epoch 1 step 505: training accuarcy: 0.7751\n",
      "Epoch 1 step 505: training loss: 31194.729674088536\n",
      "Epoch 1 step 506: training accuarcy: 0.7659\n",
      "Epoch 1 step 506: training loss: 30984.620404682406\n",
      "Epoch 1 step 507: training accuarcy: 0.7677\n",
      "Epoch 1 step 507: training loss: 30847.67498040521\n",
      "Epoch 1 step 508: training accuarcy: 0.7723\n",
      "Epoch 1 step 508: training loss: 30661.78931097259\n",
      "Epoch 1 step 509: training accuarcy: 0.7729\n",
      "Epoch 1 step 509: training loss: 30210.360307688556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 510: training accuarcy: 0.7787000000000001\n",
      "Epoch 1 step 510: training loss: 31088.248385703675\n",
      "Epoch 1 step 511: training accuarcy: 0.7649\n",
      "Epoch 1 step 511: training loss: 30091.698038304497\n",
      "Epoch 1 step 512: training accuarcy: 0.7865000000000001\n",
      "Epoch 1 step 512: training loss: 31080.41246341725\n",
      "Epoch 1 step 513: training accuarcy: 0.765\n",
      "Epoch 1 step 513: training loss: 30956.608226797467\n",
      "Epoch 1 step 514: training accuarcy: 0.766\n",
      "Epoch 1 step 514: training loss: 30494.49962101173\n",
      "Epoch 1 step 515: training accuarcy: 0.7668\n",
      "Epoch 1 step 515: training loss: 30308.58264942144\n",
      "Epoch 1 step 516: training accuarcy: 0.7669\n",
      "Epoch 1 step 516: training loss: 30016.736601742497\n",
      "Epoch 1 step 517: training accuarcy: 0.7771\n",
      "Epoch 1 step 517: training loss: 30402.490754482762\n",
      "Epoch 1 step 518: training accuarcy: 0.7728\n",
      "Epoch 1 step 518: training loss: 30600.781787366817\n",
      "Epoch 1 step 519: training accuarcy: 0.7682\n",
      "Epoch 1 step 519: training loss: 30155.471997878347\n",
      "Epoch 1 step 520: training accuarcy: 0.773\n",
      "Epoch 1 step 520: training loss: 29612.74201782978\n",
      "Epoch 1 step 521: training accuarcy: 0.7826000000000001\n",
      "Epoch 1 step 521: training loss: 30837.533832018366\n",
      "Epoch 1 step 522: training accuarcy: 0.7701\n",
      "Epoch 1 step 522: training loss: 30179.623673998038\n",
      "Epoch 1 step 523: training accuarcy: 0.7759\n",
      "Epoch 1 step 523: training loss: 30232.397151933594\n",
      "Epoch 1 step 524: training accuarcy: 0.7821\n",
      "Epoch 1 step 524: training loss: 30275.16690638527\n",
      "Epoch 1 step 525: training accuarcy: 0.7775000000000001\n",
      "Epoch 1 step 525: training loss: 15045.977374179745\n",
      "Epoch 1 step 526: training accuarcy: 0.7612820512820513\n",
      "Epoch 1: train loss 32029.661246177333, train accuarcy 0.7435723543167114\n",
      "Epoch 1: valid loss 29596.49971615582, valid accuarcy 0.7761274576187134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [09:40<04:47, 287.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 29592.70226770191\n",
      "Epoch 2 step 527: training accuarcy: 0.7885000000000001\n",
      "Epoch 2 step 527: training loss: 29542.998646134674\n",
      "Epoch 2 step 528: training accuarcy: 0.7934\n",
      "Epoch 2 step 528: training loss: 29294.765387400574\n",
      "Epoch 2 step 529: training accuarcy: 0.7911\n",
      "Epoch 2 step 529: training loss: 29082.70863222242\n",
      "Epoch 2 step 530: training accuarcy: 0.7921\n",
      "Epoch 2 step 530: training loss: 29819.621505648596\n",
      "Epoch 2 step 531: training accuarcy: 0.7924\n",
      "Epoch 2 step 531: training loss: 30067.853013730157\n",
      "Epoch 2 step 532: training accuarcy: 0.781\n",
      "Epoch 2 step 532: training loss: 29661.87719487754\n",
      "Epoch 2 step 533: training accuarcy: 0.7889\n",
      "Epoch 2 step 533: training loss: 29798.27668340026\n",
      "Epoch 2 step 534: training accuarcy: 0.79\n",
      "Epoch 2 step 534: training loss: 29361.29082672926\n",
      "Epoch 2 step 535: training accuarcy: 0.7918000000000001\n",
      "Epoch 2 step 535: training loss: 29210.004528644535\n",
      "Epoch 2 step 536: training accuarcy: 0.8016000000000001\n",
      "Epoch 2 step 536: training loss: 29313.07267684312\n",
      "Epoch 2 step 537: training accuarcy: 0.7994\n",
      "Epoch 2 step 537: training loss: 29782.11569499864\n",
      "Epoch 2 step 538: training accuarcy: 0.7873\n",
      "Epoch 2 step 538: training loss: 30129.044534906418\n",
      "Epoch 2 step 539: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 539: training loss: 29454.069567793107\n",
      "Epoch 2 step 540: training accuarcy: 0.7921\n",
      "Epoch 2 step 540: training loss: 28574.59616904313\n",
      "Epoch 2 step 541: training accuarcy: 0.8063\n",
      "Epoch 2 step 541: training loss: 29838.54690035057\n",
      "Epoch 2 step 542: training accuarcy: 0.7823\n",
      "Epoch 2 step 542: training loss: 28906.652694586268\n",
      "Epoch 2 step 543: training accuarcy: 0.797\n",
      "Epoch 2 step 543: training loss: 29031.074843555907\n",
      "Epoch 2 step 544: training accuarcy: 0.7935\n",
      "Epoch 2 step 544: training loss: 28578.668650391926\n",
      "Epoch 2 step 545: training accuarcy: 0.8006000000000001\n",
      "Epoch 2 step 545: training loss: 29307.085513161117\n",
      "Epoch 2 step 546: training accuarcy: 0.7903\n",
      "Epoch 2 step 546: training loss: 29699.77302650849\n",
      "Epoch 2 step 547: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 547: training loss: 29383.518824447412\n",
      "Epoch 2 step 548: training accuarcy: 0.7923\n",
      "Epoch 2 step 548: training loss: 29654.199444825746\n",
      "Epoch 2 step 549: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 549: training loss: 29351.94995168376\n",
      "Epoch 2 step 550: training accuarcy: 0.7916000000000001\n",
      "Epoch 2 step 550: training loss: 29044.994336089054\n",
      "Epoch 2 step 551: training accuarcy: 0.7979\n",
      "Epoch 2 step 551: training loss: 29108.202428564284\n",
      "Epoch 2 step 552: training accuarcy: 0.7932\n",
      "Epoch 2 step 552: training loss: 29735.644941333987\n",
      "Epoch 2 step 553: training accuarcy: 0.7799\n",
      "Epoch 2 step 553: training loss: 29967.53177383885\n",
      "Epoch 2 step 554: training accuarcy: 0.7772\n",
      "Epoch 2 step 554: training loss: 29242.24277931359\n",
      "Epoch 2 step 555: training accuarcy: 0.7862\n",
      "Epoch 2 step 555: training loss: 29596.190817581566\n",
      "Epoch 2 step 556: training accuarcy: 0.7881\n",
      "Epoch 2 step 556: training loss: 28957.44585856754\n",
      "Epoch 2 step 557: training accuarcy: 0.7971\n",
      "Epoch 2 step 557: training loss: 29505.843722498947\n",
      "Epoch 2 step 558: training accuarcy: 0.79\n",
      "Epoch 2 step 558: training loss: 29782.493275117697\n",
      "Epoch 2 step 559: training accuarcy: 0.788\n",
      "Epoch 2 step 559: training loss: 29803.36887174484\n",
      "Epoch 2 step 560: training accuarcy: 0.7826000000000001\n",
      "Epoch 2 step 560: training loss: 29520.29926603414\n",
      "Epoch 2 step 561: training accuarcy: 0.7942\n",
      "Epoch 2 step 561: training loss: 28919.897783858905\n",
      "Epoch 2 step 562: training accuarcy: 0.8006000000000001\n",
      "Epoch 2 step 562: training loss: 29246.71949639271\n",
      "Epoch 2 step 563: training accuarcy: 0.7828\n",
      "Epoch 2 step 563: training loss: 29319.908516175776\n",
      "Epoch 2 step 564: training accuarcy: 0.7918000000000001\n",
      "Epoch 2 step 564: training loss: 29241.69408771834\n",
      "Epoch 2 step 565: training accuarcy: 0.793\n",
      "Epoch 2 step 565: training loss: 29362.758050844946\n",
      "Epoch 2 step 566: training accuarcy: 0.789\n",
      "Epoch 2 step 566: training loss: 28588.216366276414\n",
      "Epoch 2 step 567: training accuarcy: 0.8017000000000001\n",
      "Epoch 2 step 567: training loss: 29930.059238298054\n",
      "Epoch 2 step 568: training accuarcy: 0.777\n",
      "Epoch 2 step 568: training loss: 29353.942767636516\n",
      "Epoch 2 step 569: training accuarcy: 0.7917000000000001\n",
      "Epoch 2 step 569: training loss: 29630.713938903453\n",
      "Epoch 2 step 570: training accuarcy: 0.7907000000000001\n",
      "Epoch 2 step 570: training loss: 30025.39618620897\n",
      "Epoch 2 step 571: training accuarcy: 0.7799\n",
      "Epoch 2 step 571: training loss: 29581.479106219376\n",
      "Epoch 2 step 572: training accuarcy: 0.7871\n",
      "Epoch 2 step 572: training loss: 29331.058177272724\n",
      "Epoch 2 step 573: training accuarcy: 0.7836000000000001\n",
      "Epoch 2 step 573: training loss: 28952.682584222654\n",
      "Epoch 2 step 574: training accuarcy: 0.794\n",
      "Epoch 2 step 574: training loss: 29869.23987908857\n",
      "Epoch 2 step 575: training accuarcy: 0.7812\n",
      "Epoch 2 step 575: training loss: 28782.740972623185\n",
      "Epoch 2 step 576: training accuarcy: 0.7984\n",
      "Epoch 2 step 576: training loss: 29372.46665865988\n",
      "Epoch 2 step 577: training accuarcy: 0.7844\n",
      "Epoch 2 step 577: training loss: 29655.38608143528\n",
      "Epoch 2 step 578: training accuarcy: 0.7782\n",
      "Epoch 2 step 578: training loss: 29136.346873116403\n",
      "Epoch 2 step 579: training accuarcy: 0.7929\n",
      "Epoch 2 step 579: training loss: 29373.066083078123\n",
      "Epoch 2 step 580: training accuarcy: 0.79\n",
      "Epoch 2 step 580: training loss: 29450.0366908692\n",
      "Epoch 2 step 581: training accuarcy: 0.7872\n",
      "Epoch 2 step 581: training loss: 28796.731242783415\n",
      "Epoch 2 step 582: training accuarcy: 0.799\n",
      "Epoch 2 step 582: training loss: 28867.897364721153\n",
      "Epoch 2 step 583: training accuarcy: 0.7946000000000001\n",
      "Epoch 2 step 583: training loss: 29164.56162547939\n",
      "Epoch 2 step 584: training accuarcy: 0.796\n",
      "Epoch 2 step 584: training loss: 29076.93183656131\n",
      "Epoch 2 step 585: training accuarcy: 0.7924\n",
      "Epoch 2 step 585: training loss: 29686.923821919157\n",
      "Epoch 2 step 586: training accuarcy: 0.784\n",
      "Epoch 2 step 586: training loss: 29082.754937043042\n",
      "Epoch 2 step 587: training accuarcy: 0.7888000000000001\n",
      "Epoch 2 step 587: training loss: 29340.841613078155\n",
      "Epoch 2 step 588: training accuarcy: 0.7935\n",
      "Epoch 2 step 588: training loss: 29229.146435051036\n",
      "Epoch 2 step 589: training accuarcy: 0.7862\n",
      "Epoch 2 step 589: training loss: 29498.51622159986\n",
      "Epoch 2 step 590: training accuarcy: 0.785\n",
      "Epoch 2 step 590: training loss: 29319.28209467603\n",
      "Epoch 2 step 591: training accuarcy: 0.7858\n",
      "Epoch 2 step 591: training loss: 29446.521588428015\n",
      "Epoch 2 step 592: training accuarcy: 0.786\n",
      "Epoch 2 step 592: training loss: 29129.682313523223\n",
      "Epoch 2 step 593: training accuarcy: 0.7922\n",
      "Epoch 2 step 593: training loss: 28997.6840747182\n",
      "Epoch 2 step 594: training accuarcy: 0.7939\n",
      "Epoch 2 step 594: training loss: 29279.64938810943\n",
      "Epoch 2 step 595: training accuarcy: 0.7902\n",
      "Epoch 2 step 595: training loss: 29173.638207021082\n",
      "Epoch 2 step 596: training accuarcy: 0.793\n",
      "Epoch 2 step 596: training loss: 29118.857006716564\n",
      "Epoch 2 step 597: training accuarcy: 0.7852\n",
      "Epoch 2 step 597: training loss: 29015.774729309313\n",
      "Epoch 2 step 598: training accuarcy: 0.7934\n",
      "Epoch 2 step 598: training loss: 29567.645075780198\n",
      "Epoch 2 step 599: training accuarcy: 0.7773\n",
      "Epoch 2 step 599: training loss: 29232.299272910543\n",
      "Epoch 2 step 600: training accuarcy: 0.7975\n",
      "Epoch 2 step 600: training loss: 29003.424400830925\n",
      "Epoch 2 step 601: training accuarcy: 0.7904\n",
      "Epoch 2 step 601: training loss: 29416.127496305693\n",
      "Epoch 2 step 602: training accuarcy: 0.7815000000000001\n",
      "Epoch 2 step 602: training loss: 30198.573117435993\n",
      "Epoch 2 step 603: training accuarcy: 0.7757000000000001\n",
      "Epoch 2 step 603: training loss: 28910.245387487594\n",
      "Epoch 2 step 604: training accuarcy: 0.7933\n",
      "Epoch 2 step 604: training loss: 29365.40855316794\n",
      "Epoch 2 step 605: training accuarcy: 0.794\n",
      "Epoch 2 step 605: training loss: 29227.741946519465\n",
      "Epoch 2 step 606: training accuarcy: 0.7886000000000001\n",
      "Epoch 2 step 606: training loss: 29379.316952877296\n",
      "Epoch 2 step 607: training accuarcy: 0.7857000000000001\n",
      "Epoch 2 step 607: training loss: 29160.96117390586\n",
      "Epoch 2 step 608: training accuarcy: 0.7928000000000001\n",
      "Epoch 2 step 608: training loss: 28947.337594862744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 609: training accuarcy: 0.7956000000000001\n",
      "Epoch 2 step 609: training loss: 29664.6323343107\n",
      "Epoch 2 step 610: training accuarcy: 0.7792\n",
      "Epoch 2 step 610: training loss: 28915.086062798895\n",
      "Epoch 2 step 611: training accuarcy: 0.8003\n",
      "Epoch 2 step 611: training loss: 29135.636845866713\n",
      "Epoch 2 step 612: training accuarcy: 0.7852\n",
      "Epoch 2 step 612: training loss: 29208.651239100323\n",
      "Epoch 2 step 613: training accuarcy: 0.7909\n",
      "Epoch 2 step 613: training loss: 29833.65878955085\n",
      "Epoch 2 step 614: training accuarcy: 0.7789\n",
      "Epoch 2 step 614: training loss: 29102.917230686304\n",
      "Epoch 2 step 615: training accuarcy: 0.7908000000000001\n",
      "Epoch 2 step 615: training loss: 28425.888775653133\n",
      "Epoch 2 step 616: training accuarcy: 0.7987000000000001\n",
      "Epoch 2 step 616: training loss: 28327.93327843025\n",
      "Epoch 2 step 617: training accuarcy: 0.7948000000000001\n",
      "Epoch 2 step 617: training loss: 28958.10608098931\n",
      "Epoch 2 step 618: training accuarcy: 0.7822\n",
      "Epoch 2 step 618: training loss: 29738.384967976854\n",
      "Epoch 2 step 619: training accuarcy: 0.7855000000000001\n",
      "Epoch 2 step 619: training loss: 28722.463021611256\n",
      "Epoch 2 step 620: training accuarcy: 0.7967000000000001\n",
      "Epoch 2 step 620: training loss: 29221.95531176193\n",
      "Epoch 2 step 621: training accuarcy: 0.7821\n",
      "Epoch 2 step 621: training loss: 29476.394192977397\n",
      "Epoch 2 step 622: training accuarcy: 0.7877000000000001\n",
      "Epoch 2 step 622: training loss: 29016.490063008503\n",
      "Epoch 2 step 623: training accuarcy: 0.7885000000000001\n",
      "Epoch 2 step 623: training loss: 28781.34436586748\n",
      "Epoch 2 step 624: training accuarcy: 0.7884\n",
      "Epoch 2 step 624: training loss: 28916.0131754624\n",
      "Epoch 2 step 625: training accuarcy: 0.794\n",
      "Epoch 2 step 625: training loss: 29160.314973256744\n",
      "Epoch 2 step 626: training accuarcy: 0.7975\n",
      "Epoch 2 step 626: training loss: 28818.147962691575\n",
      "Epoch 2 step 627: training accuarcy: 0.7884\n",
      "Epoch 2 step 627: training loss: 28716.376924122076\n",
      "Epoch 2 step 628: training accuarcy: 0.7961\n",
      "Epoch 2 step 628: training loss: 28766.22747917661\n",
      "Epoch 2 step 629: training accuarcy: 0.7945\n",
      "Epoch 2 step 629: training loss: 29213.087853511377\n",
      "Epoch 2 step 630: training accuarcy: 0.7873\n",
      "Epoch 2 step 630: training loss: 28907.081675391648\n",
      "Epoch 2 step 631: training accuarcy: 0.791\n",
      "Epoch 2 step 631: training loss: 29127.501227305245\n",
      "Epoch 2 step 632: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 632: training loss: 28842.257654887195\n",
      "Epoch 2 step 633: training accuarcy: 0.7936000000000001\n",
      "Epoch 2 step 633: training loss: 29342.00721080652\n",
      "Epoch 2 step 634: training accuarcy: 0.787\n",
      "Epoch 2 step 634: training loss: 29400.292255408476\n",
      "Epoch 2 step 635: training accuarcy: 0.7791\n",
      "Epoch 2 step 635: training loss: 29324.192094900398\n",
      "Epoch 2 step 636: training accuarcy: 0.7821\n",
      "Epoch 2 step 636: training loss: 28425.317610855\n",
      "Epoch 2 step 637: training accuarcy: 0.7937000000000001\n",
      "Epoch 2 step 637: training loss: 28485.661808412973\n",
      "Epoch 2 step 638: training accuarcy: 0.7989\n",
      "Epoch 2 step 638: training loss: 29709.993184483472\n",
      "Epoch 2 step 639: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 639: training loss: 29292.926699562213\n",
      "Epoch 2 step 640: training accuarcy: 0.7804\n",
      "Epoch 2 step 640: training loss: 28592.702662598254\n",
      "Epoch 2 step 641: training accuarcy: 0.7874\n",
      "Epoch 2 step 641: training loss: 28770.376533405375\n",
      "Epoch 2 step 642: training accuarcy: 0.7971\n",
      "Epoch 2 step 642: training loss: 29233.19353683782\n",
      "Epoch 2 step 643: training accuarcy: 0.7878000000000001\n",
      "Epoch 2 step 643: training loss: 29015.81259796406\n",
      "Epoch 2 step 644: training accuarcy: 0.7822\n",
      "Epoch 2 step 644: training loss: 28869.606103327056\n",
      "Epoch 2 step 645: training accuarcy: 0.7912\n",
      "Epoch 2 step 645: training loss: 29010.76686238147\n",
      "Epoch 2 step 646: training accuarcy: 0.7911\n",
      "Epoch 2 step 646: training loss: 28696.670492112557\n",
      "Epoch 2 step 647: training accuarcy: 0.7886000000000001\n",
      "Epoch 2 step 647: training loss: 29258.24042856775\n",
      "Epoch 2 step 648: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 648: training loss: 28825.299702337685\n",
      "Epoch 2 step 649: training accuarcy: 0.79\n",
      "Epoch 2 step 649: training loss: 28822.346318854674\n",
      "Epoch 2 step 650: training accuarcy: 0.7904\n",
      "Epoch 2 step 650: training loss: 28726.977876480447\n",
      "Epoch 2 step 651: training accuarcy: 0.786\n",
      "Epoch 2 step 651: training loss: 28857.97982775903\n",
      "Epoch 2 step 652: training accuarcy: 0.7881\n",
      "Epoch 2 step 652: training loss: 29180.588657922748\n",
      "Epoch 2 step 653: training accuarcy: 0.787\n",
      "Epoch 2 step 653: training loss: 29026.098005211366\n",
      "Epoch 2 step 654: training accuarcy: 0.7874\n",
      "Epoch 2 step 654: training loss: 29290.165117189834\n",
      "Epoch 2 step 655: training accuarcy: 0.7814\n",
      "Epoch 2 step 655: training loss: 29026.87538371847\n",
      "Epoch 2 step 656: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 656: training loss: 28910.585357826618\n",
      "Epoch 2 step 657: training accuarcy: 0.7915000000000001\n",
      "Epoch 2 step 657: training loss: 29172.86361311088\n",
      "Epoch 2 step 658: training accuarcy: 0.783\n",
      "Epoch 2 step 658: training loss: 28764.715714855403\n",
      "Epoch 2 step 659: training accuarcy: 0.7878000000000001\n",
      "Epoch 2 step 659: training loss: 29330.23882392396\n",
      "Epoch 2 step 660: training accuarcy: 0.7821\n",
      "Epoch 2 step 660: training loss: 29749.607941799062\n",
      "Epoch 2 step 661: training accuarcy: 0.7815000000000001\n",
      "Epoch 2 step 661: training loss: 29519.217741520624\n",
      "Epoch 2 step 662: training accuarcy: 0.7825000000000001\n",
      "Epoch 2 step 662: training loss: 28904.82170351783\n",
      "Epoch 2 step 663: training accuarcy: 0.7905000000000001\n",
      "Epoch 2 step 663: training loss: 28561.942706874055\n",
      "Epoch 2 step 664: training accuarcy: 0.7938000000000001\n",
      "Epoch 2 step 664: training loss: 28765.656791978003\n",
      "Epoch 2 step 665: training accuarcy: 0.788\n",
      "Epoch 2 step 665: training loss: 28010.82272213498\n",
      "Epoch 2 step 666: training accuarcy: 0.8053\n",
      "Epoch 2 step 666: training loss: 28910.7291353462\n",
      "Epoch 2 step 667: training accuarcy: 0.7811\n",
      "Epoch 2 step 667: training loss: 28944.978118520907\n",
      "Epoch 2 step 668: training accuarcy: 0.7862\n",
      "Epoch 2 step 668: training loss: 29118.043348197978\n",
      "Epoch 2 step 669: training accuarcy: 0.7857000000000001\n",
      "Epoch 2 step 669: training loss: 29298.551530994388\n",
      "Epoch 2 step 670: training accuarcy: 0.7869\n",
      "Epoch 2 step 670: training loss: 28907.40814821224\n",
      "Epoch 2 step 671: training accuarcy: 0.789\n",
      "Epoch 2 step 671: training loss: 28941.760313708\n",
      "Epoch 2 step 672: training accuarcy: 0.7862\n",
      "Epoch 2 step 672: training loss: 28716.923894131316\n",
      "Epoch 2 step 673: training accuarcy: 0.7851\n",
      "Epoch 2 step 673: training loss: 28695.927300960866\n",
      "Epoch 2 step 674: training accuarcy: 0.7884\n",
      "Epoch 2 step 674: training loss: 28985.700317505532\n",
      "Epoch 2 step 675: training accuarcy: 0.7873\n",
      "Epoch 2 step 675: training loss: 29074.262318602574\n",
      "Epoch 2 step 676: training accuarcy: 0.7816000000000001\n",
      "Epoch 2 step 676: training loss: 29408.116276504803\n",
      "Epoch 2 step 677: training accuarcy: 0.7883\n",
      "Epoch 2 step 677: training loss: 28443.531798151642\n",
      "Epoch 2 step 678: training accuarcy: 0.7957000000000001\n",
      "Epoch 2 step 678: training loss: 29208.307597065756\n",
      "Epoch 2 step 679: training accuarcy: 0.783\n",
      "Epoch 2 step 679: training loss: 28756.604712270433\n",
      "Epoch 2 step 680: training accuarcy: 0.7914\n",
      "Epoch 2 step 680: training loss: 28173.538045535068\n",
      "Epoch 2 step 681: training accuarcy: 0.7958000000000001\n",
      "Epoch 2 step 681: training loss: 28901.94001889949\n",
      "Epoch 2 step 682: training accuarcy: 0.7857000000000001\n",
      "Epoch 2 step 682: training loss: 28609.604604502914\n",
      "Epoch 2 step 683: training accuarcy: 0.7865000000000001\n",
      "Epoch 2 step 683: training loss: 28740.492513194906\n",
      "Epoch 2 step 684: training accuarcy: 0.7924\n",
      "Epoch 2 step 684: training loss: 28776.13775182021\n",
      "Epoch 2 step 685: training accuarcy: 0.7897000000000001\n",
      "Epoch 2 step 685: training loss: 28806.024417505436\n",
      "Epoch 2 step 686: training accuarcy: 0.782\n",
      "Epoch 2 step 686: training loss: 28778.70471628703\n",
      "Epoch 2 step 687: training accuarcy: 0.786\n",
      "Epoch 2 step 687: training loss: 28706.768993616097\n",
      "Epoch 2 step 688: training accuarcy: 0.7885000000000001\n",
      "Epoch 2 step 688: training loss: 28978.35182889537\n",
      "Epoch 2 step 689: training accuarcy: 0.785\n",
      "Epoch 2 step 689: training loss: 29174.042497994455\n",
      "Epoch 2 step 690: training accuarcy: 0.7857000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 690: training loss: 28686.99754215468\n",
      "Epoch 2 step 691: training accuarcy: 0.7906000000000001\n",
      "Epoch 2 step 691: training loss: 28763.448810794554\n",
      "Epoch 2 step 692: training accuarcy: 0.7953\n",
      "Epoch 2 step 692: training loss: 28734.754986791093\n",
      "Epoch 2 step 693: training accuarcy: 0.7903\n",
      "Epoch 2 step 693: training loss: 28832.191333561503\n",
      "Epoch 2 step 694: training accuarcy: 0.7874\n",
      "Epoch 2 step 694: training loss: 28889.438832097236\n",
      "Epoch 2 step 695: training accuarcy: 0.7891\n",
      "Epoch 2 step 695: training loss: 29524.77223648897\n",
      "Epoch 2 step 696: training accuarcy: 0.7811\n",
      "Epoch 2 step 696: training loss: 28111.733619486877\n",
      "Epoch 2 step 697: training accuarcy: 0.793\n",
      "Epoch 2 step 697: training loss: 28398.059954069533\n",
      "Epoch 2 step 698: training accuarcy: 0.7891\n",
      "Epoch 2 step 698: training loss: 28536.42865893506\n",
      "Epoch 2 step 699: training accuarcy: 0.7935\n",
      "Epoch 2 step 699: training loss: 28458.421778173768\n",
      "Epoch 2 step 700: training accuarcy: 0.793\n",
      "Epoch 2 step 700: training loss: 28537.60089373502\n",
      "Epoch 2 step 701: training accuarcy: 0.7937000000000001\n",
      "Epoch 2 step 701: training loss: 29406.39799079691\n",
      "Epoch 2 step 702: training accuarcy: 0.7796000000000001\n",
      "Epoch 2 step 702: training loss: 28244.85140005411\n",
      "Epoch 2 step 703: training accuarcy: 0.7872\n",
      "Epoch 2 step 703: training loss: 28613.224860770002\n",
      "Epoch 2 step 704: training accuarcy: 0.7896000000000001\n",
      "Epoch 2 step 704: training loss: 28494.7043076548\n",
      "Epoch 2 step 705: training accuarcy: 0.7904\n",
      "Epoch 2 step 705: training loss: 28753.29395191621\n",
      "Epoch 2 step 706: training accuarcy: 0.7877000000000001\n",
      "Epoch 2 step 706: training loss: 28747.68011843908\n",
      "Epoch 2 step 707: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 707: training loss: 28416.392606205714\n",
      "Epoch 2 step 708: training accuarcy: 0.7985\n",
      "Epoch 2 step 708: training loss: 28689.872856565573\n",
      "Epoch 2 step 709: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 709: training loss: 29065.019023617264\n",
      "Epoch 2 step 710: training accuarcy: 0.7837000000000001\n",
      "Epoch 2 step 710: training loss: 28538.237912526663\n",
      "Epoch 2 step 711: training accuarcy: 0.7881\n",
      "Epoch 2 step 711: training loss: 28703.710739735958\n",
      "Epoch 2 step 712: training accuarcy: 0.793\n",
      "Epoch 2 step 712: training loss: 28305.865764865404\n",
      "Epoch 2 step 713: training accuarcy: 0.7959\n",
      "Epoch 2 step 713: training loss: 28239.877661572136\n",
      "Epoch 2 step 714: training accuarcy: 0.7971\n",
      "Epoch 2 step 714: training loss: 28722.112797924332\n",
      "Epoch 2 step 715: training accuarcy: 0.7888000000000001\n",
      "Epoch 2 step 715: training loss: 28229.66508466546\n",
      "Epoch 2 step 716: training accuarcy: 0.7946000000000001\n",
      "Epoch 2 step 716: training loss: 29084.309944818655\n",
      "Epoch 2 step 717: training accuarcy: 0.7919\n",
      "Epoch 2 step 717: training loss: 28653.318551916393\n",
      "Epoch 2 step 718: training accuarcy: 0.7889\n",
      "Epoch 2 step 718: training loss: 28670.12452869297\n",
      "Epoch 2 step 719: training accuarcy: 0.7876000000000001\n",
      "Epoch 2 step 719: training loss: 28704.85693992291\n",
      "Epoch 2 step 720: training accuarcy: 0.793\n",
      "Epoch 2 step 720: training loss: 28082.4735964578\n",
      "Epoch 2 step 721: training accuarcy: 0.7967000000000001\n",
      "Epoch 2 step 721: training loss: 29080.224752089696\n",
      "Epoch 2 step 722: training accuarcy: 0.7835000000000001\n",
      "Epoch 2 step 722: training loss: 29236.913340412353\n",
      "Epoch 2 step 723: training accuarcy: 0.7767000000000001\n",
      "Epoch 2 step 723: training loss: 28016.83503324126\n",
      "Epoch 2 step 724: training accuarcy: 0.8009000000000001\n",
      "Epoch 2 step 724: training loss: 27833.58825382112\n",
      "Epoch 2 step 725: training accuarcy: 0.8081\n",
      "Epoch 2 step 725: training loss: 29522.566540036954\n",
      "Epoch 2 step 726: training accuarcy: 0.781\n",
      "Epoch 2 step 726: training loss: 28020.036665895546\n",
      "Epoch 2 step 727: training accuarcy: 0.8006000000000001\n",
      "Epoch 2 step 727: training loss: 28112.14702061427\n",
      "Epoch 2 step 728: training accuarcy: 0.796\n",
      "Epoch 2 step 728: training loss: 28400.35462296768\n",
      "Epoch 2 step 729: training accuarcy: 0.7912\n",
      "Epoch 2 step 729: training loss: 28289.49292145045\n",
      "Epoch 2 step 730: training accuarcy: 0.7943\n",
      "Epoch 2 step 730: training loss: 28903.264657696676\n",
      "Epoch 2 step 731: training accuarcy: 0.7885000000000001\n",
      "Epoch 2 step 731: training loss: 28570.00273802383\n",
      "Epoch 2 step 732: training accuarcy: 0.7933\n",
      "Epoch 2 step 732: training loss: 28237.546384192756\n",
      "Epoch 2 step 733: training accuarcy: 0.7986000000000001\n",
      "Epoch 2 step 733: training loss: 28653.764064992592\n",
      "Epoch 2 step 734: training accuarcy: 0.7878000000000001\n",
      "Epoch 2 step 734: training loss: 29208.142173871365\n",
      "Epoch 2 step 735: training accuarcy: 0.7802\n",
      "Epoch 2 step 735: training loss: 28462.205706926055\n",
      "Epoch 2 step 736: training accuarcy: 0.7903\n",
      "Epoch 2 step 736: training loss: 28717.26787009107\n",
      "Epoch 2 step 737: training accuarcy: 0.7905000000000001\n",
      "Epoch 2 step 737: training loss: 28250.855104038892\n",
      "Epoch 2 step 738: training accuarcy: 0.7921\n",
      "Epoch 2 step 738: training loss: 28625.17293644898\n",
      "Epoch 2 step 739: training accuarcy: 0.789\n",
      "Epoch 2 step 739: training loss: 27822.09297728845\n",
      "Epoch 2 step 740: training accuarcy: 0.8\n",
      "Epoch 2 step 740: training loss: 28448.44924334123\n",
      "Epoch 2 step 741: training accuarcy: 0.7946000000000001\n",
      "Epoch 2 step 741: training loss: 28633.025583994306\n",
      "Epoch 2 step 742: training accuarcy: 0.7842\n",
      "Epoch 2 step 742: training loss: 28820.487354207704\n",
      "Epoch 2 step 743: training accuarcy: 0.7799\n",
      "Epoch 2 step 743: training loss: 28331.030772055554\n",
      "Epoch 2 step 744: training accuarcy: 0.7921\n",
      "Epoch 2 step 744: training loss: 28267.22536791706\n",
      "Epoch 2 step 745: training accuarcy: 0.7938000000000001\n",
      "Epoch 2 step 745: training loss: 28916.863185374714\n",
      "Epoch 2 step 746: training accuarcy: 0.7847000000000001\n",
      "Epoch 2 step 746: training loss: 28752.091790295246\n",
      "Epoch 2 step 747: training accuarcy: 0.7941\n",
      "Epoch 2 step 747: training loss: 28380.568629773235\n",
      "Epoch 2 step 748: training accuarcy: 0.7889\n",
      "Epoch 2 step 748: training loss: 28576.895844268587\n",
      "Epoch 2 step 749: training accuarcy: 0.7958000000000001\n",
      "Epoch 2 step 749: training loss: 28533.14334117301\n",
      "Epoch 2 step 750: training accuarcy: 0.7914\n",
      "Epoch 2 step 750: training loss: 28079.275060813074\n",
      "Epoch 2 step 751: training accuarcy: 0.7918000000000001\n",
      "Epoch 2 step 751: training loss: 28404.99686032047\n",
      "Epoch 2 step 752: training accuarcy: 0.7947000000000001\n",
      "Epoch 2 step 752: training loss: 29465.537269286975\n",
      "Epoch 2 step 753: training accuarcy: 0.7768\n",
      "Epoch 2 step 753: training loss: 29248.57575842818\n",
      "Epoch 2 step 754: training accuarcy: 0.7791\n",
      "Epoch 2 step 754: training loss: 28629.646279651075\n",
      "Epoch 2 step 755: training accuarcy: 0.7784000000000001\n",
      "Epoch 2 step 755: training loss: 27933.55489072378\n",
      "Epoch 2 step 756: training accuarcy: 0.7972\n",
      "Epoch 2 step 756: training loss: 28145.209405793546\n",
      "Epoch 2 step 757: training accuarcy: 0.7956000000000001\n",
      "Epoch 2 step 757: training loss: 28315.441583048127\n",
      "Epoch 2 step 758: training accuarcy: 0.795\n",
      "Epoch 2 step 758: training loss: 29106.698766068363\n",
      "Epoch 2 step 759: training accuarcy: 0.7811\n",
      "Epoch 2 step 759: training loss: 28111.746772246057\n",
      "Epoch 2 step 760: training accuarcy: 0.8002\n",
      "Epoch 2 step 760: training loss: 28154.785715353675\n",
      "Epoch 2 step 761: training accuarcy: 0.7968000000000001\n",
      "Epoch 2 step 761: training loss: 28754.37701903974\n",
      "Epoch 2 step 762: training accuarcy: 0.7908000000000001\n",
      "Epoch 2 step 762: training loss: 28802.932938167338\n",
      "Epoch 2 step 763: training accuarcy: 0.7864\n",
      "Epoch 2 step 763: training loss: 27910.566708281884\n",
      "Epoch 2 step 764: training accuarcy: 0.8035\n",
      "Epoch 2 step 764: training loss: 28999.19594803131\n",
      "Epoch 2 step 765: training accuarcy: 0.7853\n",
      "Epoch 2 step 765: training loss: 28559.22608996363\n",
      "Epoch 2 step 766: training accuarcy: 0.7857000000000001\n",
      "Epoch 2 step 766: training loss: 28920.636255479953\n",
      "Epoch 2 step 767: training accuarcy: 0.7787000000000001\n",
      "Epoch 2 step 767: training loss: 28306.182557822194\n",
      "Epoch 2 step 768: training accuarcy: 0.7889\n",
      "Epoch 2 step 768: training loss: 28360.463291460946\n",
      "Epoch 2 step 769: training accuarcy: 0.7963\n",
      "Epoch 2 step 769: training loss: 28206.315582130024\n",
      "Epoch 2 step 770: training accuarcy: 0.7903\n",
      "Epoch 2 step 770: training loss: 28485.172164374606\n",
      "Epoch 2 step 771: training accuarcy: 0.7917000000000001\n",
      "Epoch 2 step 771: training loss: 28941.42721716859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 772: training accuarcy: 0.7816000000000001\n",
      "Epoch 2 step 772: training loss: 28596.644890944728\n",
      "Epoch 2 step 773: training accuarcy: 0.7939\n",
      "Epoch 2 step 773: training loss: 28627.829615390678\n",
      "Epoch 2 step 774: training accuarcy: 0.783\n",
      "Epoch 2 step 774: training loss: 28547.175131956585\n",
      "Epoch 2 step 775: training accuarcy: 0.7868\n",
      "Epoch 2 step 775: training loss: 28239.733361198018\n",
      "Epoch 2 step 776: training accuarcy: 0.7979\n",
      "Epoch 2 step 776: training loss: 28080.279394928348\n",
      "Epoch 2 step 777: training accuarcy: 0.798\n",
      "Epoch 2 step 777: training loss: 28487.64659201348\n",
      "Epoch 2 step 778: training accuarcy: 0.7872\n",
      "Epoch 2 step 778: training loss: 27957.995743259737\n",
      "Epoch 2 step 779: training accuarcy: 0.7943\n",
      "Epoch 2 step 779: training loss: 28439.689447924073\n",
      "Epoch 2 step 780: training accuarcy: 0.7892\n",
      "Epoch 2 step 780: training loss: 28469.032599272432\n",
      "Epoch 2 step 781: training accuarcy: 0.7901\n",
      "Epoch 2 step 781: training loss: 28173.868287630146\n",
      "Epoch 2 step 782: training accuarcy: 0.7991\n",
      "Epoch 2 step 782: training loss: 28868.702169842538\n",
      "Epoch 2 step 783: training accuarcy: 0.7875000000000001\n",
      "Epoch 2 step 783: training loss: 28382.502841091624\n",
      "Epoch 2 step 784: training accuarcy: 0.7867000000000001\n",
      "Epoch 2 step 784: training loss: 28879.712151214775\n",
      "Epoch 2 step 785: training accuarcy: 0.7859\n",
      "Epoch 2 step 785: training loss: 28192.507274841017\n",
      "Epoch 2 step 786: training accuarcy: 0.7903\n",
      "Epoch 2 step 786: training loss: 28242.706637813033\n",
      "Epoch 2 step 787: training accuarcy: 0.7947000000000001\n",
      "Epoch 2 step 787: training loss: 27689.67048530811\n",
      "Epoch 2 step 788: training accuarcy: 0.7976000000000001\n",
      "Epoch 2 step 788: training loss: 13957.441669108577\n",
      "Epoch 2 step 789: training accuarcy: 0.781025641025641\n",
      "Epoch 2: train loss 28876.51624572148, train accuarcy 0.7660428285598755\n",
      "Epoch 2: valid loss 27744.73928404458, valid accuarcy 0.7934712171554565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [14:33<00:00, 289.31s/it]\n"
     ]
    }
   ],
   "source": [
    "prme_learner.fit(epoch=3,\n",
    "                 log_dir=get_log_dir('weight_topcoder', 'prme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:15:19.895263Z",
     "start_time": "2019-10-09T13:15:19.885259Z"
    }
   },
   "outputs": [],
   "source": [
    "del prme_model\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Trans FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:33:43.168863Z",
     "start_time": "2019-10-09T13:33:42.721893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTransFM()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model = TorchTransFM(feature_dim=feat_dim, num_dim=NUM_DIM, init_mean=INIT_MEAN)\n",
    "trans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:33:43.847001Z",
     "start_time": "2019-10-09T13:33:43.843997Z"
    }
   },
   "outputs": [],
   "source": [
    "adam_opt = optim.Adam(trans_model.parameters(), lr=LEARNING_RATE)\n",
    "schedular = optim.lr_scheduler.StepLR(adam_opt,\n",
    "                                      step_size=DECAY_FREQ,\n",
    "                                      gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:33:47.498465Z",
     "start_time": "2019-10-09T13:33:47.427462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.fm_learner.FMLearner at 0x20b8eabbcf8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_learner = FMLearner(trans_model, adam_opt, schedular, db)\n",
    "trans_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T02:04:52.121159Z",
     "start_time": "2019-10-08T02:04:52.118159Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='base',\n",
    "                      valid_col='seq',\n",
    "                      test_col='seq',\n",
    "                      loss_callback=trans_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:15:44.463502Z",
     "start_time": "2019-10-09T13:15:44.458502Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='seq',\n",
    "                      valid_col='seq',\n",
    "                      test_col='seq',\n",
    "                      loss_callback=trans_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:33:54.030261Z",
     "start_time": "2019-10-09T13:33:54.027294Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_learner.compile(train_col='seq',\n",
    "                      valid_col='seq',\n",
    "                      test_col='seq',\n",
    "                      loss_callback=trans_weight_loss_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:30:48.310477Z",
     "start_time": "2019-10-09T13:15:53.044709Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 90523.66535448213\n",
      "Epoch 0 step 1: training accuarcy: 0.5174\n",
      "Epoch 0 step 1: training loss: 87424.23827851049\n",
      "Epoch 0 step 2: training accuarcy: 0.5308\n",
      "Epoch 0 step 2: training loss: 86097.55888972402\n",
      "Epoch 0 step 3: training accuarcy: 0.5161\n",
      "Epoch 0 step 3: training loss: 87003.45660238003\n",
      "Epoch 0 step 4: training accuarcy: 0.5036\n",
      "Epoch 0 step 4: training loss: 82552.1749683306\n",
      "Epoch 0 step 5: training accuarcy: 0.5275000000000001\n",
      "Epoch 0 step 5: training loss: 80373.29348692777\n",
      "Epoch 0 step 6: training accuarcy: 0.5214\n",
      "Epoch 0 step 6: training loss: 78654.98681077069\n",
      "Epoch 0 step 7: training accuarcy: 0.5228\n",
      "Epoch 0 step 7: training loss: 76674.39022174261\n",
      "Epoch 0 step 8: training accuarcy: 0.5243\n",
      "Epoch 0 step 8: training loss: 74296.39480536147\n",
      "Epoch 0 step 9: training accuarcy: 0.5236000000000001\n",
      "Epoch 0 step 9: training loss: 74795.55691764891\n",
      "Epoch 0 step 10: training accuarcy: 0.5092\n",
      "Epoch 0 step 10: training loss: 71093.94043584414\n",
      "Epoch 0 step 11: training accuarcy: 0.532\n",
      "Epoch 0 step 11: training loss: 68558.51464224714\n",
      "Epoch 0 step 12: training accuarcy: 0.5394\n",
      "Epoch 0 step 12: training loss: 67868.0815665042\n",
      "Epoch 0 step 13: training accuarcy: 0.5236000000000001\n",
      "Epoch 0 step 13: training loss: 64599.293824029395\n",
      "Epoch 0 step 14: training accuarcy: 0.5357000000000001\n",
      "Epoch 0 step 14: training loss: 64949.046185415296\n",
      "Epoch 0 step 15: training accuarcy: 0.5228\n",
      "Epoch 0 step 15: training loss: 61431.45706919662\n",
      "Epoch 0 step 16: training accuarcy: 0.5343\n",
      "Epoch 0 step 16: training loss: 60284.18000963403\n",
      "Epoch 0 step 17: training accuarcy: 0.5362\n",
      "Epoch 0 step 17: training loss: 58797.67717690684\n",
      "Epoch 0 step 18: training accuarcy: 0.5308\n",
      "Epoch 0 step 18: training loss: 57909.52753361766\n",
      "Epoch 0 step 19: training accuarcy: 0.5228\n",
      "Epoch 0 step 19: training loss: 54106.83604179673\n",
      "Epoch 0 step 20: training accuarcy: 0.5437000000000001\n",
      "Epoch 0 step 20: training loss: 53825.2491510788\n",
      "Epoch 0 step 21: training accuarcy: 0.533\n",
      "Epoch 0 step 21: training loss: 51138.248506295415\n",
      "Epoch 0 step 22: training accuarcy: 0.5544\n",
      "Epoch 0 step 22: training loss: 49685.76613643226\n",
      "Epoch 0 step 23: training accuarcy: 0.5293\n",
      "Epoch 0 step 23: training loss: 48314.16849889354\n",
      "Epoch 0 step 24: training accuarcy: 0.5265000000000001\n",
      "Epoch 0 step 24: training loss: 47102.40339369849\n",
      "Epoch 0 step 25: training accuarcy: 0.5206000000000001\n",
      "Epoch 0 step 25: training loss: 43791.85294404145\n",
      "Epoch 0 step 26: training accuarcy: 0.5526\n",
      "Epoch 0 step 26: training loss: 43941.66323205841\n",
      "Epoch 0 step 27: training accuarcy: 0.5232\n",
      "Epoch 0 step 27: training loss: 41279.318093840266\n",
      "Epoch 0 step 28: training accuarcy: 0.538\n",
      "Epoch 0 step 28: training loss: 39262.39181279947\n",
      "Epoch 0 step 29: training accuarcy: 0.5564\n",
      "Epoch 0 step 29: training loss: 38133.02280326141\n",
      "Epoch 0 step 30: training accuarcy: 0.5493\n",
      "Epoch 0 step 30: training loss: 37776.23709982935\n",
      "Epoch 0 step 31: training accuarcy: 0.5316000000000001\n",
      "Epoch 0 step 31: training loss: 35476.982246296044\n",
      "Epoch 0 step 32: training accuarcy: 0.5478000000000001\n",
      "Epoch 0 step 32: training loss: 34720.58527852213\n",
      "Epoch 0 step 33: training accuarcy: 0.5325\n",
      "Epoch 0 step 33: training loss: 33738.20906729903\n",
      "Epoch 0 step 34: training accuarcy: 0.5363\n",
      "Epoch 0 step 34: training loss: 31767.26830458193\n",
      "Epoch 0 step 35: training accuarcy: 0.555\n",
      "Epoch 0 step 35: training loss: 30965.473803039768\n",
      "Epoch 0 step 36: training accuarcy: 0.5427000000000001\n",
      "Epoch 0 step 36: training loss: 29125.766128404168\n",
      "Epoch 0 step 37: training accuarcy: 0.5622\n",
      "Epoch 0 step 37: training loss: 28505.146345752873\n",
      "Epoch 0 step 38: training accuarcy: 0.548\n",
      "Epoch 0 step 38: training loss: 28029.624267830324\n",
      "Epoch 0 step 39: training accuarcy: 0.5452\n",
      "Epoch 0 step 39: training loss: 26357.10289098476\n",
      "Epoch 0 step 40: training accuarcy: 0.5525\n",
      "Epoch 0 step 40: training loss: 25327.655438472364\n",
      "Epoch 0 step 41: training accuarcy: 0.5555\n",
      "Epoch 0 step 41: training loss: 24468.448335377798\n",
      "Epoch 0 step 42: training accuarcy: 0.5578000000000001\n",
      "Epoch 0 step 42: training loss: 23601.88694105221\n",
      "Epoch 0 step 43: training accuarcy: 0.5632\n",
      "Epoch 0 step 43: training loss: 22816.60943174365\n",
      "Epoch 0 step 44: training accuarcy: 0.5662\n",
      "Epoch 0 step 44: training loss: 21680.196295992977\n",
      "Epoch 0 step 45: training accuarcy: 0.5900000000000001\n",
      "Epoch 0 step 45: training loss: 21177.73974112779\n",
      "Epoch 0 step 46: training accuarcy: 0.5757\n",
      "Epoch 0 step 46: training loss: 20773.289265700434\n",
      "Epoch 0 step 47: training accuarcy: 0.5756\n",
      "Epoch 0 step 47: training loss: 19933.30190081032\n",
      "Epoch 0 step 48: training accuarcy: 0.5944\n",
      "Epoch 0 step 48: training loss: 19330.745713484623\n",
      "Epoch 0 step 49: training accuarcy: 0.6084\n",
      "Epoch 0 step 49: training loss: 18968.81215337735\n",
      "Epoch 0 step 50: training accuarcy: 0.5929\n",
      "Epoch 0 step 50: training loss: 18333.49808544135\n",
      "Epoch 0 step 51: training accuarcy: 0.6132000000000001\n",
      "Epoch 0 step 51: training loss: 18013.461232504196\n",
      "Epoch 0 step 52: training accuarcy: 0.6154000000000001\n",
      "Epoch 0 step 52: training loss: 17544.033279768413\n",
      "Epoch 0 step 53: training accuarcy: 0.6217\n",
      "Epoch 0 step 53: training loss: 16966.33652484799\n",
      "Epoch 0 step 54: training accuarcy: 0.6312\n",
      "Epoch 0 step 54: training loss: 16405.963872939188\n",
      "Epoch 0 step 55: training accuarcy: 0.654\n",
      "Epoch 0 step 55: training loss: 16181.962310322695\n",
      "Epoch 0 step 56: training accuarcy: 0.6559\n",
      "Epoch 0 step 56: training loss: 15910.76434758701\n",
      "Epoch 0 step 57: training accuarcy: 0.6562\n",
      "Epoch 0 step 57: training loss: 15511.129719019402\n",
      "Epoch 0 step 58: training accuarcy: 0.6599\n",
      "Epoch 0 step 58: training loss: 15386.455257623016\n",
      "Epoch 0 step 59: training accuarcy: 0.6598\n",
      "Epoch 0 step 59: training loss: 15129.252235510838\n",
      "Epoch 0 step 60: training accuarcy: 0.6608\n",
      "Epoch 0 step 60: training loss: 14685.669581277567\n",
      "Epoch 0 step 61: training accuarcy: 0.6743\n",
      "Epoch 0 step 61: training loss: 14344.571887056623\n",
      "Epoch 0 step 62: training accuarcy: 0.6876\n",
      "Epoch 0 step 62: training loss: 14101.713842280806\n",
      "Epoch 0 step 63: training accuarcy: 0.6883\n",
      "Epoch 0 step 63: training loss: 13905.585074224484\n",
      "Epoch 0 step 64: training accuarcy: 0.6927\n",
      "Epoch 0 step 64: training loss: 13528.783669804669\n",
      "Epoch 0 step 65: training accuarcy: 0.7100000000000001\n",
      "Epoch 0 step 65: training loss: 13615.79808321355\n",
      "Epoch 0 step 66: training accuarcy: 0.6855\n",
      "Epoch 0 step 66: training loss: 13223.631742543208\n",
      "Epoch 0 step 67: training accuarcy: 0.7063\n",
      "Epoch 0 step 67: training loss: 13030.793873234858\n",
      "Epoch 0 step 68: training accuarcy: 0.7035\n",
      "Epoch 0 step 68: training loss: 12690.336593158077\n",
      "Epoch 0 step 69: training accuarcy: 0.7150000000000001\n",
      "Epoch 0 step 69: training loss: 12590.624410054637\n",
      "Epoch 0 step 70: training accuarcy: 0.7202000000000001\n",
      "Epoch 0 step 70: training loss: 12271.6309218889\n",
      "Epoch 0 step 71: training accuarcy: 0.7286\n",
      "Epoch 0 step 71: training loss: 12180.787994869392\n",
      "Epoch 0 step 72: training accuarcy: 0.727\n",
      "Epoch 0 step 72: training loss: 12007.631285844538\n",
      "Epoch 0 step 73: training accuarcy: 0.7225\n",
      "Epoch 0 step 73: training loss: 11817.619251707565\n",
      "Epoch 0 step 74: training accuarcy: 0.7315\n",
      "Epoch 0 step 74: training loss: 11561.22912226101\n",
      "Epoch 0 step 75: training accuarcy: 0.7362000000000001\n",
      "Epoch 0 step 75: training loss: 11523.742159320234\n",
      "Epoch 0 step 76: training accuarcy: 0.7264\n",
      "Epoch 0 step 76: training loss: 11387.764400756809\n",
      "Epoch 0 step 77: training accuarcy: 0.7269\n",
      "Epoch 0 step 77: training loss: 11147.280193998668\n",
      "Epoch 0 step 78: training accuarcy: 0.7356\n",
      "Epoch 0 step 78: training loss: 11121.206609053424\n",
      "Epoch 0 step 79: training accuarcy: 0.735\n",
      "Epoch 0 step 79: training loss: 10831.212848502062\n",
      "Epoch 0 step 80: training accuarcy: 0.7492000000000001\n",
      "Epoch 0 step 80: training loss: 10782.774171486351\n",
      "Epoch 0 step 81: training accuarcy: 0.7443000000000001\n",
      "Epoch 0 step 81: training loss: 10611.502946218316\n",
      "Epoch 0 step 82: training accuarcy: 0.7452000000000001\n",
      "Epoch 0 step 82: training loss: 10407.574715885054\n",
      "Epoch 0 step 83: training accuarcy: 0.7547\n",
      "Epoch 0 step 83: training loss: 10307.81338100852\n",
      "Epoch 0 step 84: training accuarcy: 0.7465\n",
      "Epoch 0 step 84: training loss: 10111.340330870156\n",
      "Epoch 0 step 85: training accuarcy: 0.7534000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 85: training loss: 10060.416775912365\n",
      "Epoch 0 step 86: training accuarcy: 0.7482000000000001\n",
      "Epoch 0 step 86: training loss: 9865.594381436453\n",
      "Epoch 0 step 87: training accuarcy: 0.7548\n",
      "Epoch 0 step 87: training loss: 9613.980609779977\n",
      "Epoch 0 step 88: training accuarcy: 0.7628\n",
      "Epoch 0 step 88: training loss: 9493.501054391545\n",
      "Epoch 0 step 89: training accuarcy: 0.7695000000000001\n",
      "Epoch 0 step 89: training loss: 9559.720245734807\n",
      "Epoch 0 step 90: training accuarcy: 0.7531\n",
      "Epoch 0 step 90: training loss: 9471.13026649403\n",
      "Epoch 0 step 91: training accuarcy: 0.7574000000000001\n",
      "Epoch 0 step 91: training loss: 9249.999336803165\n",
      "Epoch 0 step 92: training accuarcy: 0.7632\n",
      "Epoch 0 step 92: training loss: 9105.271713042825\n",
      "Epoch 0 step 93: training accuarcy: 0.7708\n",
      "Epoch 0 step 93: training loss: 8990.013228916067\n",
      "Epoch 0 step 94: training accuarcy: 0.7721\n",
      "Epoch 0 step 94: training loss: 8950.004093611804\n",
      "Epoch 0 step 95: training accuarcy: 0.7605000000000001\n",
      "Epoch 0 step 95: training loss: 8777.638299874383\n",
      "Epoch 0 step 96: training accuarcy: 0.7755000000000001\n",
      "Epoch 0 step 96: training loss: 8756.903408850518\n",
      "Epoch 0 step 97: training accuarcy: 0.7743\n",
      "Epoch 0 step 97: training loss: 8564.808210256359\n",
      "Epoch 0 step 98: training accuarcy: 0.7805000000000001\n",
      "Epoch 0 step 98: training loss: 8581.518793105322\n",
      "Epoch 0 step 99: training accuarcy: 0.7712\n",
      "Epoch 0 step 99: training loss: 8460.952422984927\n",
      "Epoch 0 step 100: training accuarcy: 0.7778\n",
      "Epoch 0 step 100: training loss: 8444.01899556314\n",
      "Epoch 0 step 101: training accuarcy: 0.7701\n",
      "Epoch 0 step 101: training loss: 8196.841442400528\n",
      "Epoch 0 step 102: training accuarcy: 0.7914\n",
      "Epoch 0 step 102: training loss: 8104.295984161256\n",
      "Epoch 0 step 103: training accuarcy: 0.7809\n",
      "Epoch 0 step 103: training loss: 8214.72106124895\n",
      "Epoch 0 step 104: training accuarcy: 0.7771\n",
      "Epoch 0 step 104: training loss: 8002.133780056805\n",
      "Epoch 0 step 105: training accuarcy: 0.7827000000000001\n",
      "Epoch 0 step 105: training loss: 7998.570379987448\n",
      "Epoch 0 step 106: training accuarcy: 0.7858\n",
      "Epoch 0 step 106: training loss: 7793.073689761406\n",
      "Epoch 0 step 107: training accuarcy: 0.7932\n",
      "Epoch 0 step 107: training loss: 7874.617197669155\n",
      "Epoch 0 step 108: training accuarcy: 0.7819\n",
      "Epoch 0 step 108: training loss: 7727.730973541778\n",
      "Epoch 0 step 109: training accuarcy: 0.7846000000000001\n",
      "Epoch 0 step 109: training loss: 7740.739365092179\n",
      "Epoch 0 step 110: training accuarcy: 0.7815000000000001\n",
      "Epoch 0 step 110: training loss: 7482.71418695767\n",
      "Epoch 0 step 111: training accuarcy: 0.7936000000000001\n",
      "Epoch 0 step 111: training loss: 7502.07770395282\n",
      "Epoch 0 step 112: training accuarcy: 0.7927000000000001\n",
      "Epoch 0 step 112: training loss: 7329.908633778711\n",
      "Epoch 0 step 113: training accuarcy: 0.7946000000000001\n",
      "Epoch 0 step 113: training loss: 7452.65661584032\n",
      "Epoch 0 step 114: training accuarcy: 0.7909\n",
      "Epoch 0 step 114: training loss: 7295.257793411205\n",
      "Epoch 0 step 115: training accuarcy: 0.7947000000000001\n",
      "Epoch 0 step 115: training loss: 7282.8398955004595\n",
      "Epoch 0 step 116: training accuarcy: 0.7976000000000001\n",
      "Epoch 0 step 116: training loss: 7233.891909516117\n",
      "Epoch 0 step 117: training accuarcy: 0.791\n",
      "Epoch 0 step 117: training loss: 7095.297294016598\n",
      "Epoch 0 step 118: training accuarcy: 0.7978000000000001\n",
      "Epoch 0 step 118: training loss: 6984.70909265646\n",
      "Epoch 0 step 119: training accuarcy: 0.8015\n",
      "Epoch 0 step 119: training loss: 7165.055972650367\n",
      "Epoch 0 step 120: training accuarcy: 0.7876000000000001\n",
      "Epoch 0 step 120: training loss: 6942.348501935937\n",
      "Epoch 0 step 121: training accuarcy: 0.8008000000000001\n",
      "Epoch 0 step 121: training loss: 6935.764717807186\n",
      "Epoch 0 step 122: training accuarcy: 0.7995\n",
      "Epoch 0 step 122: training loss: 6767.161854182417\n",
      "Epoch 0 step 123: training accuarcy: 0.8114\n",
      "Epoch 0 step 123: training loss: 6903.073754415837\n",
      "Epoch 0 step 124: training accuarcy: 0.7984\n",
      "Epoch 0 step 124: training loss: 6598.061591750615\n",
      "Epoch 0 step 125: training accuarcy: 0.8118000000000001\n",
      "Epoch 0 step 125: training loss: 6649.862260740043\n",
      "Epoch 0 step 126: training accuarcy: 0.8031\n",
      "Epoch 0 step 126: training loss: 6569.461832684265\n",
      "Epoch 0 step 127: training accuarcy: 0.8087000000000001\n",
      "Epoch 0 step 127: training loss: 6583.223477922253\n",
      "Epoch 0 step 128: training accuarcy: 0.8039000000000001\n",
      "Epoch 0 step 128: training loss: 6488.6692698405595\n",
      "Epoch 0 step 129: training accuarcy: 0.8119000000000001\n",
      "Epoch 0 step 129: training loss: 6561.878607390072\n",
      "Epoch 0 step 130: training accuarcy: 0.8011\n",
      "Epoch 0 step 130: training loss: 6319.276546621076\n",
      "Epoch 0 step 131: training accuarcy: 0.8181\n",
      "Epoch 0 step 131: training loss: 6221.133347908302\n",
      "Epoch 0 step 132: training accuarcy: 0.8217\n",
      "Epoch 0 step 132: training loss: 6259.055677855475\n",
      "Epoch 0 step 133: training accuarcy: 0.8177000000000001\n",
      "Epoch 0 step 133: training loss: 6347.7258808808965\n",
      "Epoch 0 step 134: training accuarcy: 0.8045\n",
      "Epoch 0 step 134: training loss: 6350.065176940356\n",
      "Epoch 0 step 135: training accuarcy: 0.8085\n",
      "Epoch 0 step 135: training loss: 6044.851871853372\n",
      "Epoch 0 step 136: training accuarcy: 0.8184\n",
      "Epoch 0 step 136: training loss: 6065.372685172315\n",
      "Epoch 0 step 137: training accuarcy: 0.8231\n",
      "Epoch 0 step 137: training loss: 6008.693654542209\n",
      "Epoch 0 step 138: training accuarcy: 0.8215\n",
      "Epoch 0 step 138: training loss: 6087.195860063581\n",
      "Epoch 0 step 139: training accuarcy: 0.8178000000000001\n",
      "Epoch 0 step 139: training loss: 6152.485014053399\n",
      "Epoch 0 step 140: training accuarcy: 0.8053\n",
      "Epoch 0 step 140: training loss: 5999.80933008965\n",
      "Epoch 0 step 141: training accuarcy: 0.8160000000000001\n",
      "Epoch 0 step 141: training loss: 6033.931248352977\n",
      "Epoch 0 step 142: training accuarcy: 0.8162\n",
      "Epoch 0 step 142: training loss: 5819.080073885414\n",
      "Epoch 0 step 143: training accuarcy: 0.8264\n",
      "Epoch 0 step 143: training loss: 5863.786522016606\n",
      "Epoch 0 step 144: training accuarcy: 0.8193\n",
      "Epoch 0 step 144: training loss: 5812.106694633292\n",
      "Epoch 0 step 145: training accuarcy: 0.8242\n",
      "Epoch 0 step 145: training loss: 5945.11332404783\n",
      "Epoch 0 step 146: training accuarcy: 0.8061\n",
      "Epoch 0 step 146: training loss: 5687.920114691328\n",
      "Epoch 0 step 147: training accuarcy: 0.8267\n",
      "Epoch 0 step 147: training loss: 5736.146541032948\n",
      "Epoch 0 step 148: training accuarcy: 0.8228000000000001\n",
      "Epoch 0 step 148: training loss: 5708.695087055596\n",
      "Epoch 0 step 149: training accuarcy: 0.8198000000000001\n",
      "Epoch 0 step 149: training loss: 5587.731764954042\n",
      "Epoch 0 step 150: training accuarcy: 0.8306\n",
      "Epoch 0 step 150: training loss: 5777.459173314808\n",
      "Epoch 0 step 151: training accuarcy: 0.8138000000000001\n",
      "Epoch 0 step 151: training loss: 5597.357172267636\n",
      "Epoch 0 step 152: training accuarcy: 0.8204\n",
      "Epoch 0 step 152: training loss: 5637.443960263584\n",
      "Epoch 0 step 153: training accuarcy: 0.8195\n",
      "Epoch 0 step 153: training loss: 5571.2773737433745\n",
      "Epoch 0 step 154: training accuarcy: 0.8188000000000001\n",
      "Epoch 0 step 154: training loss: 5474.658021913599\n",
      "Epoch 0 step 155: training accuarcy: 0.8235\n",
      "Epoch 0 step 155: training loss: 5457.871669882886\n",
      "Epoch 0 step 156: training accuarcy: 0.8275\n",
      "Epoch 0 step 156: training loss: 5373.471412765744\n",
      "Epoch 0 step 157: training accuarcy: 0.8331000000000001\n",
      "Epoch 0 step 157: training loss: 5378.784110852824\n",
      "Epoch 0 step 158: training accuarcy: 0.8309000000000001\n",
      "Epoch 0 step 158: training loss: 5293.8884622609385\n",
      "Epoch 0 step 159: training accuarcy: 0.8302\n",
      "Epoch 0 step 159: training loss: 5366.090675117516\n",
      "Epoch 0 step 160: training accuarcy: 0.8272\n",
      "Epoch 0 step 160: training loss: 5440.932804326823\n",
      "Epoch 0 step 161: training accuarcy: 0.8250000000000001\n",
      "Epoch 0 step 161: training loss: 5315.74792998599\n",
      "Epoch 0 step 162: training accuarcy: 0.8290000000000001\n",
      "Epoch 0 step 162: training loss: 5387.15779019805\n",
      "Epoch 0 step 163: training accuarcy: 0.8236\n",
      "Epoch 0 step 163: training loss: 5355.731539272142\n",
      "Epoch 0 step 164: training accuarcy: 0.8267\n",
      "Epoch 0 step 164: training loss: 5287.793909120051\n",
      "Epoch 0 step 165: training accuarcy: 0.8257\n",
      "Epoch 0 step 165: training loss: 5326.988359593315\n",
      "Epoch 0 step 166: training accuarcy: 0.8238000000000001\n",
      "Epoch 0 step 166: training loss: 5257.4614577101365\n",
      "Epoch 0 step 167: training accuarcy: 0.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 167: training loss: 5210.732623835984\n",
      "Epoch 0 step 168: training accuarcy: 0.8262\n",
      "Epoch 0 step 168: training loss: 5251.075820998959\n",
      "Epoch 0 step 169: training accuarcy: 0.8245\n",
      "Epoch 0 step 169: training loss: 5096.3778992297775\n",
      "Epoch 0 step 170: training accuarcy: 0.8371000000000001\n",
      "Epoch 0 step 170: training loss: 5021.562767639163\n",
      "Epoch 0 step 171: training accuarcy: 0.8380000000000001\n",
      "Epoch 0 step 171: training loss: 5114.859250081069\n",
      "Epoch 0 step 172: training accuarcy: 0.8324\n",
      "Epoch 0 step 172: training loss: 4982.526724791125\n",
      "Epoch 0 step 173: training accuarcy: 0.8390000000000001\n",
      "Epoch 0 step 173: training loss: 5186.525137789706\n",
      "Epoch 0 step 174: training accuarcy: 0.8232\n",
      "Epoch 0 step 174: training loss: 4994.231141879069\n",
      "Epoch 0 step 175: training accuarcy: 0.8330000000000001\n",
      "Epoch 0 step 175: training loss: 4846.676476891661\n",
      "Epoch 0 step 176: training accuarcy: 0.8450000000000001\n",
      "Epoch 0 step 176: training loss: 4917.665380151637\n",
      "Epoch 0 step 177: training accuarcy: 0.8424\n",
      "Epoch 0 step 177: training loss: 4833.709692998204\n",
      "Epoch 0 step 178: training accuarcy: 0.8435\n",
      "Epoch 0 step 178: training loss: 4894.778558961357\n",
      "Epoch 0 step 179: training accuarcy: 0.8376\n",
      "Epoch 0 step 179: training loss: 4819.63363874276\n",
      "Epoch 0 step 180: training accuarcy: 0.8443\n",
      "Epoch 0 step 180: training loss: 4785.940915751298\n",
      "Epoch 0 step 181: training accuarcy: 0.8385\n",
      "Epoch 0 step 181: training loss: 4829.7602726887\n",
      "Epoch 0 step 182: training accuarcy: 0.8448\n",
      "Epoch 0 step 182: training loss: 4856.792991652961\n",
      "Epoch 0 step 183: training accuarcy: 0.8407\n",
      "Epoch 0 step 183: training loss: 4758.717722521007\n",
      "Epoch 0 step 184: training accuarcy: 0.8455\n",
      "Epoch 0 step 184: training loss: 4656.623999505761\n",
      "Epoch 0 step 185: training accuarcy: 0.8515\n",
      "Epoch 0 step 185: training loss: 4776.047367986695\n",
      "Epoch 0 step 186: training accuarcy: 0.8408\n",
      "Epoch 0 step 186: training loss: 4782.839437812002\n",
      "Epoch 0 step 187: training accuarcy: 0.8308000000000001\n",
      "Epoch 0 step 187: training loss: 4843.047721810601\n",
      "Epoch 0 step 188: training accuarcy: 0.8351000000000001\n",
      "Epoch 0 step 188: training loss: 4703.215591415696\n",
      "Epoch 0 step 189: training accuarcy: 0.8448\n",
      "Epoch 0 step 189: training loss: 4671.140846458571\n",
      "Epoch 0 step 190: training accuarcy: 0.8406\n",
      "Epoch 0 step 190: training loss: 4688.162022294965\n",
      "Epoch 0 step 191: training accuarcy: 0.8425\n",
      "Epoch 0 step 191: training loss: 4614.93312810474\n",
      "Epoch 0 step 192: training accuarcy: 0.8445\n",
      "Epoch 0 step 192: training loss: 4590.328215155353\n",
      "Epoch 0 step 193: training accuarcy: 0.8434\n",
      "Epoch 0 step 193: training loss: 4522.969547353722\n",
      "Epoch 0 step 194: training accuarcy: 0.8470000000000001\n",
      "Epoch 0 step 194: training loss: 4536.046119540469\n",
      "Epoch 0 step 195: training accuarcy: 0.8467\n",
      "Epoch 0 step 195: training loss: 4629.747733600223\n",
      "Epoch 0 step 196: training accuarcy: 0.8422000000000001\n",
      "Epoch 0 step 196: training loss: 4486.617672077466\n",
      "Epoch 0 step 197: training accuarcy: 0.8503000000000001\n",
      "Epoch 0 step 197: training loss: 4640.156422712345\n",
      "Epoch 0 step 198: training accuarcy: 0.8370000000000001\n",
      "Epoch 0 step 198: training loss: 4421.212601441592\n",
      "Epoch 0 step 199: training accuarcy: 0.8560000000000001\n",
      "Epoch 0 step 199: training loss: 4554.560085535526\n",
      "Epoch 0 step 200: training accuarcy: 0.8445\n",
      "Epoch 0 step 200: training loss: 4459.143288099394\n",
      "Epoch 0 step 201: training accuarcy: 0.8494\n",
      "Epoch 0 step 201: training loss: 4412.467317102402\n",
      "Epoch 0 step 202: training accuarcy: 0.8519\n",
      "Epoch 0 step 202: training loss: 4479.642523025627\n",
      "Epoch 0 step 203: training accuarcy: 0.8496\n",
      "Epoch 0 step 203: training loss: 4467.8312824870945\n",
      "Epoch 0 step 204: training accuarcy: 0.8455\n",
      "Epoch 0 step 204: training loss: 4334.415399940378\n",
      "Epoch 0 step 205: training accuarcy: 0.8572000000000001\n",
      "Epoch 0 step 205: training loss: 4485.082125197432\n",
      "Epoch 0 step 206: training accuarcy: 0.8406\n",
      "Epoch 0 step 206: training loss: 4364.942094486401\n",
      "Epoch 0 step 207: training accuarcy: 0.8499\n",
      "Epoch 0 step 207: training loss: 4422.088713971884\n",
      "Epoch 0 step 208: training accuarcy: 0.8463\n",
      "Epoch 0 step 208: training loss: 4306.599275698722\n",
      "Epoch 0 step 209: training accuarcy: 0.8554\n",
      "Epoch 0 step 209: training loss: 4317.6875051768275\n",
      "Epoch 0 step 210: training accuarcy: 0.8515\n",
      "Epoch 0 step 210: training loss: 4297.931616856463\n",
      "Epoch 0 step 211: training accuarcy: 0.8505\n",
      "Epoch 0 step 211: training loss: 4269.619030381793\n",
      "Epoch 0 step 212: training accuarcy: 0.8508\n",
      "Epoch 0 step 212: training loss: 4166.485447705466\n",
      "Epoch 0 step 213: training accuarcy: 0.8633000000000001\n",
      "Epoch 0 step 213: training loss: 4237.114793278275\n",
      "Epoch 0 step 214: training accuarcy: 0.8550000000000001\n",
      "Epoch 0 step 214: training loss: 4245.361656995032\n",
      "Epoch 0 step 215: training accuarcy: 0.8572000000000001\n",
      "Epoch 0 step 215: training loss: 4285.800791901056\n",
      "Epoch 0 step 216: training accuarcy: 0.8594\n",
      "Epoch 0 step 216: training loss: 4239.365019590567\n",
      "Epoch 0 step 217: training accuarcy: 0.8560000000000001\n",
      "Epoch 0 step 217: training loss: 4237.095125262531\n",
      "Epoch 0 step 218: training accuarcy: 0.8502000000000001\n",
      "Epoch 0 step 218: training loss: 4185.03601681733\n",
      "Epoch 0 step 219: training accuarcy: 0.8589\n",
      "Epoch 0 step 219: training loss: 4157.505663550566\n",
      "Epoch 0 step 220: training accuarcy: 0.8533000000000001\n",
      "Epoch 0 step 220: training loss: 4363.344234406777\n",
      "Epoch 0 step 221: training accuarcy: 0.8466\n",
      "Epoch 0 step 221: training loss: 4221.723811542792\n",
      "Epoch 0 step 222: training accuarcy: 0.8575\n",
      "Epoch 0 step 222: training loss: 4168.548590968852\n",
      "Epoch 0 step 223: training accuarcy: 0.8541000000000001\n",
      "Epoch 0 step 223: training loss: 4120.6544839662765\n",
      "Epoch 0 step 224: training accuarcy: 0.8601000000000001\n",
      "Epoch 0 step 224: training loss: 4160.047587967812\n",
      "Epoch 0 step 225: training accuarcy: 0.8549\n",
      "Epoch 0 step 225: training loss: 4154.515584284127\n",
      "Epoch 0 step 226: training accuarcy: 0.8532000000000001\n",
      "Epoch 0 step 226: training loss: 4125.148422927754\n",
      "Epoch 0 step 227: training accuarcy: 0.8582000000000001\n",
      "Epoch 0 step 227: training loss: 4174.740266679048\n",
      "Epoch 0 step 228: training accuarcy: 0.8535\n",
      "Epoch 0 step 228: training loss: 4111.1538826407\n",
      "Epoch 0 step 229: training accuarcy: 0.8526\n",
      "Epoch 0 step 229: training loss: 4171.107961127298\n",
      "Epoch 0 step 230: training accuarcy: 0.8509\n",
      "Epoch 0 step 230: training loss: 4067.206655998977\n",
      "Epoch 0 step 231: training accuarcy: 0.8552000000000001\n",
      "Epoch 0 step 231: training loss: 4034.7787340823643\n",
      "Epoch 0 step 232: training accuarcy: 0.8625\n",
      "Epoch 0 step 232: training loss: 4135.919995766636\n",
      "Epoch 0 step 233: training accuarcy: 0.8498\n",
      "Epoch 0 step 233: training loss: 3958.773441394545\n",
      "Epoch 0 step 234: training accuarcy: 0.8632000000000001\n",
      "Epoch 0 step 234: training loss: 4037.9668306972526\n",
      "Epoch 0 step 235: training accuarcy: 0.8580000000000001\n",
      "Epoch 0 step 235: training loss: 3964.269547153519\n",
      "Epoch 0 step 236: training accuarcy: 0.8613000000000001\n",
      "Epoch 0 step 236: training loss: 3957.225387869883\n",
      "Epoch 0 step 237: training accuarcy: 0.8653000000000001\n",
      "Epoch 0 step 237: training loss: 3962.506421412754\n",
      "Epoch 0 step 238: training accuarcy: 0.8626\n",
      "Epoch 0 step 238: training loss: 4009.8706085253625\n",
      "Epoch 0 step 239: training accuarcy: 0.8582000000000001\n",
      "Epoch 0 step 239: training loss: 3993.2791365897865\n",
      "Epoch 0 step 240: training accuarcy: 0.8573000000000001\n",
      "Epoch 0 step 240: training loss: 3986.907150201213\n",
      "Epoch 0 step 241: training accuarcy: 0.8596\n",
      "Epoch 0 step 241: training loss: 4016.8244996850513\n",
      "Epoch 0 step 242: training accuarcy: 0.8561000000000001\n",
      "Epoch 0 step 242: training loss: 4000.496235686941\n",
      "Epoch 0 step 243: training accuarcy: 0.8553000000000001\n",
      "Epoch 0 step 243: training loss: 3933.20329205707\n",
      "Epoch 0 step 244: training accuarcy: 0.8611000000000001\n",
      "Epoch 0 step 244: training loss: 4008.4741785149345\n",
      "Epoch 0 step 245: training accuarcy: 0.8578\n",
      "Epoch 0 step 245: training loss: 4014.9843170392237\n",
      "Epoch 0 step 246: training accuarcy: 0.8544\n",
      "Epoch 0 step 246: training loss: 3930.513562403284\n",
      "Epoch 0 step 247: training accuarcy: 0.861\n",
      "Epoch 0 step 247: training loss: 3883.180424619603\n",
      "Epoch 0 step 248: training accuarcy: 0.8639\n",
      "Epoch 0 step 248: training loss: 3762.1093225862132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 249: training accuarcy: 0.8744000000000001\n",
      "Epoch 0 step 249: training loss: 3999.6635735679865\n",
      "Epoch 0 step 250: training accuarcy: 0.8548\n",
      "Epoch 0 step 250: training loss: 3790.502622343316\n",
      "Epoch 0 step 251: training accuarcy: 0.8615\n",
      "Epoch 0 step 251: training loss: 3842.599576680568\n",
      "Epoch 0 step 252: training accuarcy: 0.8667\n",
      "Epoch 0 step 252: training loss: 3826.4443657936545\n",
      "Epoch 0 step 253: training accuarcy: 0.8674000000000001\n",
      "Epoch 0 step 253: training loss: 3763.066579905333\n",
      "Epoch 0 step 254: training accuarcy: 0.8676\n",
      "Epoch 0 step 254: training loss: 3776.3742291766503\n",
      "Epoch 0 step 255: training accuarcy: 0.8646\n",
      "Epoch 0 step 255: training loss: 3753.2273333305825\n",
      "Epoch 0 step 256: training accuarcy: 0.8706\n",
      "Epoch 0 step 256: training loss: 3712.96122234752\n",
      "Epoch 0 step 257: training accuarcy: 0.8706\n",
      "Epoch 0 step 257: training loss: 3782.381695555322\n",
      "Epoch 0 step 258: training accuarcy: 0.8635\n",
      "Epoch 0 step 258: training loss: 3722.9243131356257\n",
      "Epoch 0 step 259: training accuarcy: 0.8722000000000001\n",
      "Epoch 0 step 259: training loss: 3758.354630359782\n",
      "Epoch 0 step 260: training accuarcy: 0.8681000000000001\n",
      "Epoch 0 step 260: training loss: 3714.8177277357513\n",
      "Epoch 0 step 261: training accuarcy: 0.8677\n",
      "Epoch 0 step 261: training loss: 3745.9074603916724\n",
      "Epoch 0 step 262: training accuarcy: 0.8703000000000001\n",
      "Epoch 0 step 262: training loss: 1944.6297765131576\n",
      "Epoch 0 step 263: training accuarcy: 0.8687179487179487\n",
      "Epoch 0: train loss 14796.22461904436, train accuarcy 0.7189987897872925\n",
      "Epoch 0: valid loss 4029.2813417010875, valid accuarcy 0.8416621685028076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [05:00<10:00, 300.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 3554.131691709318\n",
      "Epoch 1 step 264: training accuarcy: 0.8836\n",
      "Epoch 1 step 264: training loss: 3433.822092961164\n",
      "Epoch 1 step 265: training accuarcy: 0.8897\n",
      "Epoch 1 step 265: training loss: 3538.209516011465\n",
      "Epoch 1 step 266: training accuarcy: 0.8795000000000001\n",
      "Epoch 1 step 266: training loss: 3496.840120318441\n",
      "Epoch 1 step 267: training accuarcy: 0.8844000000000001\n",
      "Epoch 1 step 267: training loss: 3379.8578583689964\n",
      "Epoch 1 step 268: training accuarcy: 0.8922\n",
      "Epoch 1 step 268: training loss: 3421.306163680013\n",
      "Epoch 1 step 269: training accuarcy: 0.8874000000000001\n",
      "Epoch 1 step 269: training loss: 3415.8607577488324\n",
      "Epoch 1 step 270: training accuarcy: 0.8849\n",
      "Epoch 1 step 270: training loss: 3457.5801452044866\n",
      "Epoch 1 step 271: training accuarcy: 0.884\n",
      "Epoch 1 step 271: training loss: 3416.0697444470975\n",
      "Epoch 1 step 272: training accuarcy: 0.8857\n",
      "Epoch 1 step 272: training loss: 3362.605794277284\n",
      "Epoch 1 step 273: training accuarcy: 0.8871\n",
      "Epoch 1 step 273: training loss: 3404.9788195922592\n",
      "Epoch 1 step 274: training accuarcy: 0.8879\n",
      "Epoch 1 step 274: training loss: 3406.508971816851\n",
      "Epoch 1 step 275: training accuarcy: 0.8899\n",
      "Epoch 1 step 275: training loss: 3396.8062805584727\n",
      "Epoch 1 step 276: training accuarcy: 0.8905000000000001\n",
      "Epoch 1 step 276: training loss: 3352.9258723732128\n",
      "Epoch 1 step 277: training accuarcy: 0.8918\n",
      "Epoch 1 step 277: training loss: 3378.4460157318154\n",
      "Epoch 1 step 278: training accuarcy: 0.8922\n",
      "Epoch 1 step 278: training loss: 3419.373634324723\n",
      "Epoch 1 step 279: training accuarcy: 0.8889\n",
      "Epoch 1 step 279: training loss: 3470.351477086423\n",
      "Epoch 1 step 280: training accuarcy: 0.8856\n",
      "Epoch 1 step 280: training loss: 3394.87638531098\n",
      "Epoch 1 step 281: training accuarcy: 0.8916000000000001\n",
      "Epoch 1 step 281: training loss: 3456.9571223201683\n",
      "Epoch 1 step 282: training accuarcy: 0.8874000000000001\n",
      "Epoch 1 step 282: training loss: 3401.9840820416725\n",
      "Epoch 1 step 283: training accuarcy: 0.8871\n",
      "Epoch 1 step 283: training loss: 3270.5146952147334\n",
      "Epoch 1 step 284: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 284: training loss: 3396.0688383417905\n",
      "Epoch 1 step 285: training accuarcy: 0.8898\n",
      "Epoch 1 step 285: training loss: 3323.491006312212\n",
      "Epoch 1 step 286: training accuarcy: 0.8921\n",
      "Epoch 1 step 286: training loss: 3350.3074068218425\n",
      "Epoch 1 step 287: training accuarcy: 0.8855000000000001\n",
      "Epoch 1 step 287: training loss: 3380.13504797521\n",
      "Epoch 1 step 288: training accuarcy: 0.8828\n",
      "Epoch 1 step 288: training loss: 3405.7982843390655\n",
      "Epoch 1 step 289: training accuarcy: 0.8844000000000001\n",
      "Epoch 1 step 289: training loss: 3377.648234430631\n",
      "Epoch 1 step 290: training accuarcy: 0.8859\n",
      "Epoch 1 step 290: training loss: 3331.0109482872376\n",
      "Epoch 1 step 291: training accuarcy: 0.8902\n",
      "Epoch 1 step 291: training loss: 3290.1230822754687\n",
      "Epoch 1 step 292: training accuarcy: 0.8941\n",
      "Epoch 1 step 292: training loss: 3420.380395632007\n",
      "Epoch 1 step 293: training accuarcy: 0.884\n",
      "Epoch 1 step 293: training loss: 3369.468226961478\n",
      "Epoch 1 step 294: training accuarcy: 0.8868\n",
      "Epoch 1 step 294: training loss: 3352.4199205135083\n",
      "Epoch 1 step 295: training accuarcy: 0.8851\n",
      "Epoch 1 step 295: training loss: 3347.6644806265326\n",
      "Epoch 1 step 296: training accuarcy: 0.8843000000000001\n",
      "Epoch 1 step 296: training loss: 3276.315958895663\n",
      "Epoch 1 step 297: training accuarcy: 0.8989\n",
      "Epoch 1 step 297: training loss: 3374.0943839977863\n",
      "Epoch 1 step 298: training accuarcy: 0.8797\n",
      "Epoch 1 step 298: training loss: 3333.342622704317\n",
      "Epoch 1 step 299: training accuarcy: 0.8866\n",
      "Epoch 1 step 299: training loss: 3254.026104066953\n",
      "Epoch 1 step 300: training accuarcy: 0.8939\n",
      "Epoch 1 step 300: training loss: 3400.550050310886\n",
      "Epoch 1 step 301: training accuarcy: 0.8783000000000001\n",
      "Epoch 1 step 301: training loss: 3285.6941091322874\n",
      "Epoch 1 step 302: training accuarcy: 0.8888\n",
      "Epoch 1 step 302: training loss: 3311.8900952798194\n",
      "Epoch 1 step 303: training accuarcy: 0.8875000000000001\n",
      "Epoch 1 step 303: training loss: 3208.7893112347015\n",
      "Epoch 1 step 304: training accuarcy: 0.8931\n",
      "Epoch 1 step 304: training loss: 3312.8539648351953\n",
      "Epoch 1 step 305: training accuarcy: 0.8847\n",
      "Epoch 1 step 305: training loss: 3268.991062002894\n",
      "Epoch 1 step 306: training accuarcy: 0.8897\n",
      "Epoch 1 step 306: training loss: 3322.833592691156\n",
      "Epoch 1 step 307: training accuarcy: 0.8869\n",
      "Epoch 1 step 307: training loss: 3318.8267575553728\n",
      "Epoch 1 step 308: training accuarcy: 0.882\n",
      "Epoch 1 step 308: training loss: 3259.2956738042285\n",
      "Epoch 1 step 309: training accuarcy: 0.8843000000000001\n",
      "Epoch 1 step 309: training loss: 3270.952534782323\n",
      "Epoch 1 step 310: training accuarcy: 0.8855000000000001\n",
      "Epoch 1 step 310: training loss: 3229.021412519326\n",
      "Epoch 1 step 311: training accuarcy: 0.8881\n",
      "Epoch 1 step 311: training loss: 3255.0022760316006\n",
      "Epoch 1 step 312: training accuarcy: 0.8899\n",
      "Epoch 1 step 312: training loss: 3306.626723948447\n",
      "Epoch 1 step 313: training accuarcy: 0.886\n",
      "Epoch 1 step 313: training loss: 3224.4481692739805\n",
      "Epoch 1 step 314: training accuarcy: 0.8917\n",
      "Epoch 1 step 314: training loss: 3270.180237644099\n",
      "Epoch 1 step 315: training accuarcy: 0.8835000000000001\n",
      "Epoch 1 step 315: training loss: 3306.408342340037\n",
      "Epoch 1 step 316: training accuarcy: 0.8821\n",
      "Epoch 1 step 316: training loss: 3326.1062852815026\n",
      "Epoch 1 step 317: training accuarcy: 0.8852000000000001\n",
      "Epoch 1 step 317: training loss: 3205.676242667626\n",
      "Epoch 1 step 318: training accuarcy: 0.888\n",
      "Epoch 1 step 318: training loss: 3229.664882708307\n",
      "Epoch 1 step 319: training accuarcy: 0.8903000000000001\n",
      "Epoch 1 step 319: training loss: 3177.2740339402835\n",
      "Epoch 1 step 320: training accuarcy: 0.8926000000000001\n",
      "Epoch 1 step 320: training loss: 3234.1052575964186\n",
      "Epoch 1 step 321: training accuarcy: 0.8841\n",
      "Epoch 1 step 321: training loss: 3184.5390010843353\n",
      "Epoch 1 step 322: training accuarcy: 0.8929\n",
      "Epoch 1 step 322: training loss: 3213.489267993847\n",
      "Epoch 1 step 323: training accuarcy: 0.8927\n",
      "Epoch 1 step 323: training loss: 3131.423194871546\n",
      "Epoch 1 step 324: training accuarcy: 0.8982\n",
      "Epoch 1 step 324: training loss: 3171.306114785595\n",
      "Epoch 1 step 325: training accuarcy: 0.8892\n",
      "Epoch 1 step 325: training loss: 3155.336116478882\n",
      "Epoch 1 step 326: training accuarcy: 0.8906000000000001\n",
      "Epoch 1 step 326: training loss: 3194.9225681637877\n",
      "Epoch 1 step 327: training accuarcy: 0.8885000000000001\n",
      "Epoch 1 step 327: training loss: 3217.0408579219215\n",
      "Epoch 1 step 328: training accuarcy: 0.8903000000000001\n",
      "Epoch 1 step 328: training loss: 3090.9232043960997\n",
      "Epoch 1 step 329: training accuarcy: 0.8936000000000001\n",
      "Epoch 1 step 329: training loss: 3249.3781953510975\n",
      "Epoch 1 step 330: training accuarcy: 0.8909\n",
      "Epoch 1 step 330: training loss: 3108.5190571480266\n",
      "Epoch 1 step 331: training accuarcy: 0.8989\n",
      "Epoch 1 step 331: training loss: 3204.6910108139036\n",
      "Epoch 1 step 332: training accuarcy: 0.8881\n",
      "Epoch 1 step 332: training loss: 3172.2564498190522\n",
      "Epoch 1 step 333: training accuarcy: 0.8932\n",
      "Epoch 1 step 333: training loss: 3233.28279583958\n",
      "Epoch 1 step 334: training accuarcy: 0.8852000000000001\n",
      "Epoch 1 step 334: training loss: 3206.5166703082737\n",
      "Epoch 1 step 335: training accuarcy: 0.8864000000000001\n",
      "Epoch 1 step 335: training loss: 3178.8921198973912\n",
      "Epoch 1 step 336: training accuarcy: 0.8868\n",
      "Epoch 1 step 336: training loss: 3090.3024294321986\n",
      "Epoch 1 step 337: training accuarcy: 0.8959\n",
      "Epoch 1 step 337: training loss: 3087.8463873348446\n",
      "Epoch 1 step 338: training accuarcy: 0.8947\n",
      "Epoch 1 step 338: training loss: 3145.8531646080314\n",
      "Epoch 1 step 339: training accuarcy: 0.8902\n",
      "Epoch 1 step 339: training loss: 3237.2803150740015\n",
      "Epoch 1 step 340: training accuarcy: 0.8875000000000001\n",
      "Epoch 1 step 340: training loss: 3218.5853715898284\n",
      "Epoch 1 step 341: training accuarcy: 0.8888\n",
      "Epoch 1 step 341: training loss: 3188.474467780038\n",
      "Epoch 1 step 342: training accuarcy: 0.8867\n",
      "Epoch 1 step 342: training loss: 3150.440922335164\n",
      "Epoch 1 step 343: training accuarcy: 0.8899\n",
      "Epoch 1 step 343: training loss: 3049.8366788531034\n",
      "Epoch 1 step 344: training accuarcy: 0.8961\n",
      "Epoch 1 step 344: training loss: 3137.488853549719\n",
      "Epoch 1 step 345: training accuarcy: 0.8895000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 345: training loss: 3191.6092913690823\n",
      "Epoch 1 step 346: training accuarcy: 0.8924000000000001\n",
      "Epoch 1 step 346: training loss: 3129.1233842502543\n",
      "Epoch 1 step 347: training accuarcy: 0.8937\n",
      "Epoch 1 step 347: training loss: 3081.346671328978\n",
      "Epoch 1 step 348: training accuarcy: 0.8932\n",
      "Epoch 1 step 348: training loss: 3130.2584294553035\n",
      "Epoch 1 step 349: training accuarcy: 0.8972\n",
      "Epoch 1 step 349: training loss: 3118.745095773243\n",
      "Epoch 1 step 350: training accuarcy: 0.8894000000000001\n",
      "Epoch 1 step 350: training loss: 3162.4032541424945\n",
      "Epoch 1 step 351: training accuarcy: 0.8864000000000001\n",
      "Epoch 1 step 351: training loss: 3143.5612707413757\n",
      "Epoch 1 step 352: training accuarcy: 0.8925000000000001\n",
      "Epoch 1 step 352: training loss: 3051.6515282125088\n",
      "Epoch 1 step 353: training accuarcy: 0.8934000000000001\n",
      "Epoch 1 step 353: training loss: 3059.5492425222583\n",
      "Epoch 1 step 354: training accuarcy: 0.894\n",
      "Epoch 1 step 354: training loss: 3139.9682329346656\n",
      "Epoch 1 step 355: training accuarcy: 0.8886000000000001\n",
      "Epoch 1 step 355: training loss: 3040.8983668354945\n",
      "Epoch 1 step 356: training accuarcy: 0.8966000000000001\n",
      "Epoch 1 step 356: training loss: 3040.724879887856\n",
      "Epoch 1 step 357: training accuarcy: 0.8947\n",
      "Epoch 1 step 357: training loss: 3083.1114497148437\n",
      "Epoch 1 step 358: training accuarcy: 0.8929\n",
      "Epoch 1 step 358: training loss: 2965.9225617208535\n",
      "Epoch 1 step 359: training accuarcy: 0.8969\n",
      "Epoch 1 step 359: training loss: 3151.2659745713268\n",
      "Epoch 1 step 360: training accuarcy: 0.8883000000000001\n",
      "Epoch 1 step 360: training loss: 3064.746518967723\n",
      "Epoch 1 step 361: training accuarcy: 0.8975000000000001\n",
      "Epoch 1 step 361: training loss: 3032.6741713238275\n",
      "Epoch 1 step 362: training accuarcy: 0.8934000000000001\n",
      "Epoch 1 step 362: training loss: 3047.899151595314\n",
      "Epoch 1 step 363: training accuarcy: 0.8954000000000001\n",
      "Epoch 1 step 363: training loss: 3044.4020013958025\n",
      "Epoch 1 step 364: training accuarcy: 0.8951\n",
      "Epoch 1 step 364: training loss: 3091.1755469476434\n",
      "Epoch 1 step 365: training accuarcy: 0.8901\n",
      "Epoch 1 step 365: training loss: 3057.0413287140555\n",
      "Epoch 1 step 366: training accuarcy: 0.8942\n",
      "Epoch 1 step 366: training loss: 2999.392543057314\n",
      "Epoch 1 step 367: training accuarcy: 0.8975000000000001\n",
      "Epoch 1 step 367: training loss: 3058.045809835558\n",
      "Epoch 1 step 368: training accuarcy: 0.8957\n",
      "Epoch 1 step 368: training loss: 3023.52372604011\n",
      "Epoch 1 step 369: training accuarcy: 0.8986000000000001\n",
      "Epoch 1 step 369: training loss: 3041.325657416317\n",
      "Epoch 1 step 370: training accuarcy: 0.8927\n",
      "Epoch 1 step 370: training loss: 3062.6786055633365\n",
      "Epoch 1 step 371: training accuarcy: 0.8935000000000001\n",
      "Epoch 1 step 371: training loss: 3027.7144553484804\n",
      "Epoch 1 step 372: training accuarcy: 0.8956000000000001\n",
      "Epoch 1 step 372: training loss: 2909.341917082866\n",
      "Epoch 1 step 373: training accuarcy: 0.9043\n",
      "Epoch 1 step 373: training loss: 3008.950975577871\n",
      "Epoch 1 step 374: training accuarcy: 0.8912\n",
      "Epoch 1 step 374: training loss: 2984.827992647011\n",
      "Epoch 1 step 375: training accuarcy: 0.8982\n",
      "Epoch 1 step 375: training loss: 3031.770628171176\n",
      "Epoch 1 step 376: training accuarcy: 0.8958\n",
      "Epoch 1 step 376: training loss: 2964.165239152598\n",
      "Epoch 1 step 377: training accuarcy: 0.8986000000000001\n",
      "Epoch 1 step 377: training loss: 2896.5446015270404\n",
      "Epoch 1 step 378: training accuarcy: 0.8968\n",
      "Epoch 1 step 378: training loss: 2970.2477553970843\n",
      "Epoch 1 step 379: training accuarcy: 0.9\n",
      "Epoch 1 step 379: training loss: 2995.92604390006\n",
      "Epoch 1 step 380: training accuarcy: 0.8959\n",
      "Epoch 1 step 380: training loss: 2960.256843202061\n",
      "Epoch 1 step 381: training accuarcy: 0.9043\n",
      "Epoch 1 step 381: training loss: 2981.380202737326\n",
      "Epoch 1 step 382: training accuarcy: 0.8978\n",
      "Epoch 1 step 382: training loss: 3059.566825905364\n",
      "Epoch 1 step 383: training accuarcy: 0.8906000000000001\n",
      "Epoch 1 step 383: training loss: 2997.3814035012574\n",
      "Epoch 1 step 384: training accuarcy: 0.8944000000000001\n",
      "Epoch 1 step 384: training loss: 2981.2272891118014\n",
      "Epoch 1 step 385: training accuarcy: 0.8966000000000001\n",
      "Epoch 1 step 385: training loss: 2974.0099103613675\n",
      "Epoch 1 step 386: training accuarcy: 0.8954000000000001\n",
      "Epoch 1 step 386: training loss: 2967.787931281421\n",
      "Epoch 1 step 387: training accuarcy: 0.9002\n",
      "Epoch 1 step 387: training loss: 2910.096739038825\n",
      "Epoch 1 step 388: training accuarcy: 0.8985000000000001\n",
      "Epoch 1 step 388: training loss: 3022.0523473403773\n",
      "Epoch 1 step 389: training accuarcy: 0.8948\n",
      "Epoch 1 step 389: training loss: 2956.4818107524816\n",
      "Epoch 1 step 390: training accuarcy: 0.9006000000000001\n",
      "Epoch 1 step 390: training loss: 3042.8871654717177\n",
      "Epoch 1 step 391: training accuarcy: 0.8946000000000001\n",
      "Epoch 1 step 391: training loss: 2928.7714346093794\n",
      "Epoch 1 step 392: training accuarcy: 0.9002\n",
      "Epoch 1 step 392: training loss: 2965.337451369824\n",
      "Epoch 1 step 393: training accuarcy: 0.8953000000000001\n",
      "Epoch 1 step 393: training loss: 3039.8394388590486\n",
      "Epoch 1 step 394: training accuarcy: 0.9\n",
      "Epoch 1 step 394: training loss: 2938.6529098737406\n",
      "Epoch 1 step 395: training accuarcy: 0.8983000000000001\n",
      "Epoch 1 step 395: training loss: 3008.8517562249604\n",
      "Epoch 1 step 396: training accuarcy: 0.895\n",
      "Epoch 1 step 396: training loss: 2987.0512228489197\n",
      "Epoch 1 step 397: training accuarcy: 0.8921\n",
      "Epoch 1 step 397: training loss: 2955.9496650679303\n",
      "Epoch 1 step 398: training accuarcy: 0.8959\n",
      "Epoch 1 step 398: training loss: 2950.781890563725\n",
      "Epoch 1 step 399: training accuarcy: 0.8983000000000001\n",
      "Epoch 1 step 399: training loss: 2965.7279320647644\n",
      "Epoch 1 step 400: training accuarcy: 0.8973000000000001\n",
      "Epoch 1 step 400: training loss: 3057.511777322145\n",
      "Epoch 1 step 401: training accuarcy: 0.8921\n",
      "Epoch 1 step 401: training loss: 2962.988010975562\n",
      "Epoch 1 step 402: training accuarcy: 0.8955000000000001\n",
      "Epoch 1 step 402: training loss: 2917.9608916196626\n",
      "Epoch 1 step 403: training accuarcy: 0.8994000000000001\n",
      "Epoch 1 step 403: training loss: 2892.400752520703\n",
      "Epoch 1 step 404: training accuarcy: 0.9006000000000001\n",
      "Epoch 1 step 404: training loss: 2934.0219919960755\n",
      "Epoch 1 step 405: training accuarcy: 0.8993\n",
      "Epoch 1 step 405: training loss: 2893.471634731638\n",
      "Epoch 1 step 406: training accuarcy: 0.901\n",
      "Epoch 1 step 406: training loss: 2970.2360550531957\n",
      "Epoch 1 step 407: training accuarcy: 0.889\n",
      "Epoch 1 step 407: training loss: 2840.5859904449294\n",
      "Epoch 1 step 408: training accuarcy: 0.9031\n",
      "Epoch 1 step 408: training loss: 2956.3397740627497\n",
      "Epoch 1 step 409: training accuarcy: 0.8987\n",
      "Epoch 1 step 409: training loss: 3022.0136078580827\n",
      "Epoch 1 step 410: training accuarcy: 0.8942\n",
      "Epoch 1 step 410: training loss: 2855.3500889846555\n",
      "Epoch 1 step 411: training accuarcy: 0.8995000000000001\n",
      "Epoch 1 step 411: training loss: 2915.1311228264967\n",
      "Epoch 1 step 412: training accuarcy: 0.8946000000000001\n",
      "Epoch 1 step 412: training loss: 2929.9857728412494\n",
      "Epoch 1 step 413: training accuarcy: 0.9006000000000001\n",
      "Epoch 1 step 413: training loss: 2881.98589110545\n",
      "Epoch 1 step 414: training accuarcy: 0.903\n",
      "Epoch 1 step 414: training loss: 2892.5480572430943\n",
      "Epoch 1 step 415: training accuarcy: 0.9013\n",
      "Epoch 1 step 415: training loss: 2963.139887778594\n",
      "Epoch 1 step 416: training accuarcy: 0.8933000000000001\n",
      "Epoch 1 step 416: training loss: 2895.1711704852196\n",
      "Epoch 1 step 417: training accuarcy: 0.8994000000000001\n",
      "Epoch 1 step 417: training loss: 2940.0517011522616\n",
      "Epoch 1 step 418: training accuarcy: 0.8993\n",
      "Epoch 1 step 418: training loss: 3031.122700377241\n",
      "Epoch 1 step 419: training accuarcy: 0.8941\n",
      "Epoch 1 step 419: training loss: 2930.2856185336013\n",
      "Epoch 1 step 420: training accuarcy: 0.898\n",
      "Epoch 1 step 420: training loss: 2993.563715359325\n",
      "Epoch 1 step 421: training accuarcy: 0.892\n",
      "Epoch 1 step 421: training loss: 3054.4554324966366\n",
      "Epoch 1 step 422: training accuarcy: 0.8878\n",
      "Epoch 1 step 422: training loss: 2952.6208832123457\n",
      "Epoch 1 step 423: training accuarcy: 0.8967\n",
      "Epoch 1 step 423: training loss: 2793.5859603179997\n",
      "Epoch 1 step 424: training accuarcy: 0.9048\n",
      "Epoch 1 step 424: training loss: 2889.303908647035\n",
      "Epoch 1 step 425: training accuarcy: 0.9013\n",
      "Epoch 1 step 425: training loss: 2953.46965718803\n",
      "Epoch 1 step 426: training accuarcy: 0.897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 426: training loss: 2875.578254016077\n",
      "Epoch 1 step 427: training accuarcy: 0.9002\n",
      "Epoch 1 step 427: training loss: 2875.051900678086\n",
      "Epoch 1 step 428: training accuarcy: 0.9012\n",
      "Epoch 1 step 428: training loss: 2929.5848426487973\n",
      "Epoch 1 step 429: training accuarcy: 0.8932\n",
      "Epoch 1 step 429: training loss: 2900.579677505548\n",
      "Epoch 1 step 430: training accuarcy: 0.9027000000000001\n",
      "Epoch 1 step 430: training loss: 2960.0766086612757\n",
      "Epoch 1 step 431: training accuarcy: 0.8941\n",
      "Epoch 1 step 431: training loss: 2855.7842274440595\n",
      "Epoch 1 step 432: training accuarcy: 0.8979\n",
      "Epoch 1 step 432: training loss: 2954.200036296202\n",
      "Epoch 1 step 433: training accuarcy: 0.8915000000000001\n",
      "Epoch 1 step 433: training loss: 2854.642658898823\n",
      "Epoch 1 step 434: training accuarcy: 0.9015000000000001\n",
      "Epoch 1 step 434: training loss: 2808.1338336426793\n",
      "Epoch 1 step 435: training accuarcy: 0.9066000000000001\n",
      "Epoch 1 step 435: training loss: 2939.877292847589\n",
      "Epoch 1 step 436: training accuarcy: 0.8952\n",
      "Epoch 1 step 436: training loss: 2808.1098459849873\n",
      "Epoch 1 step 437: training accuarcy: 0.9029\n",
      "Epoch 1 step 437: training loss: 2888.854654007351\n",
      "Epoch 1 step 438: training accuarcy: 0.8985000000000001\n",
      "Epoch 1 step 438: training loss: 2884.731056878852\n",
      "Epoch 1 step 439: training accuarcy: 0.8942\n",
      "Epoch 1 step 439: training loss: 2916.3185471368433\n",
      "Epoch 1 step 440: training accuarcy: 0.8954000000000001\n",
      "Epoch 1 step 440: training loss: 2863.4272564945436\n",
      "Epoch 1 step 441: training accuarcy: 0.9022\n",
      "Epoch 1 step 441: training loss: 2849.0016297869106\n",
      "Epoch 1 step 442: training accuarcy: 0.9018\n",
      "Epoch 1 step 442: training loss: 2880.053240372769\n",
      "Epoch 1 step 443: training accuarcy: 0.8993\n",
      "Epoch 1 step 443: training loss: 2890.605780955239\n",
      "Epoch 1 step 444: training accuarcy: 0.9016000000000001\n",
      "Epoch 1 step 444: training loss: 2720.1184141277513\n",
      "Epoch 1 step 445: training accuarcy: 0.9107000000000001\n",
      "Epoch 1 step 445: training loss: 2934.9687695373977\n",
      "Epoch 1 step 446: training accuarcy: 0.8976000000000001\n",
      "Epoch 1 step 446: training loss: 2905.932681160555\n",
      "Epoch 1 step 447: training accuarcy: 0.8941\n",
      "Epoch 1 step 447: training loss: 2926.304425240435\n",
      "Epoch 1 step 448: training accuarcy: 0.8974000000000001\n",
      "Epoch 1 step 448: training loss: 2839.2220169215657\n",
      "Epoch 1 step 449: training accuarcy: 0.9022\n",
      "Epoch 1 step 449: training loss: 2872.715874934245\n",
      "Epoch 1 step 450: training accuarcy: 0.9047000000000001\n",
      "Epoch 1 step 450: training loss: 2908.6933072701518\n",
      "Epoch 1 step 451: training accuarcy: 0.8945000000000001\n",
      "Epoch 1 step 451: training loss: 2787.5383922547585\n",
      "Epoch 1 step 452: training accuarcy: 0.9053\n",
      "Epoch 1 step 452: training loss: 2881.208898829946\n",
      "Epoch 1 step 453: training accuarcy: 0.9\n",
      "Epoch 1 step 453: training loss: 2832.8249988305874\n",
      "Epoch 1 step 454: training accuarcy: 0.8983000000000001\n",
      "Epoch 1 step 454: training loss: 2798.9996439816437\n",
      "Epoch 1 step 455: training accuarcy: 0.9019\n",
      "Epoch 1 step 455: training loss: 2814.821696342445\n",
      "Epoch 1 step 456: training accuarcy: 0.9022\n",
      "Epoch 1 step 456: training loss: 2813.5085244520933\n",
      "Epoch 1 step 457: training accuarcy: 0.9022\n",
      "Epoch 1 step 457: training loss: 2808.583940540711\n",
      "Epoch 1 step 458: training accuarcy: 0.9056000000000001\n",
      "Epoch 1 step 458: training loss: 2895.627323005662\n",
      "Epoch 1 step 459: training accuarcy: 0.9011\n",
      "Epoch 1 step 459: training loss: 2780.93152876926\n",
      "Epoch 1 step 460: training accuarcy: 0.9005000000000001\n",
      "Epoch 1 step 460: training loss: 2893.6148153081663\n",
      "Epoch 1 step 461: training accuarcy: 0.8984000000000001\n",
      "Epoch 1 step 461: training loss: 2837.568702678448\n",
      "Epoch 1 step 462: training accuarcy: 0.9023\n",
      "Epoch 1 step 462: training loss: 2868.0387989214987\n",
      "Epoch 1 step 463: training accuarcy: 0.9\n",
      "Epoch 1 step 463: training loss: 2762.837231552924\n",
      "Epoch 1 step 464: training accuarcy: 0.9025000000000001\n",
      "Epoch 1 step 464: training loss: 2815.3019587459507\n",
      "Epoch 1 step 465: training accuarcy: 0.9033\n",
      "Epoch 1 step 465: training loss: 2747.546554008598\n",
      "Epoch 1 step 466: training accuarcy: 0.907\n",
      "Epoch 1 step 466: training loss: 2899.0505801341533\n",
      "Epoch 1 step 467: training accuarcy: 0.8954000000000001\n",
      "Epoch 1 step 467: training loss: 2832.0010231594506\n",
      "Epoch 1 step 468: training accuarcy: 0.9002\n",
      "Epoch 1 step 468: training loss: 2813.7891302295507\n",
      "Epoch 1 step 469: training accuarcy: 0.9039\n",
      "Epoch 1 step 469: training loss: 2774.285029148012\n",
      "Epoch 1 step 470: training accuarcy: 0.905\n",
      "Epoch 1 step 470: training loss: 2943.6174302129284\n",
      "Epoch 1 step 471: training accuarcy: 0.9006000000000001\n",
      "Epoch 1 step 471: training loss: 2810.5504322567413\n",
      "Epoch 1 step 472: training accuarcy: 0.9026000000000001\n",
      "Epoch 1 step 472: training loss: 2908.2565644212877\n",
      "Epoch 1 step 473: training accuarcy: 0.8962\n",
      "Epoch 1 step 473: training loss: 2881.954678061259\n",
      "Epoch 1 step 474: training accuarcy: 0.898\n",
      "Epoch 1 step 474: training loss: 2828.526862735051\n",
      "Epoch 1 step 475: training accuarcy: 0.9025000000000001\n",
      "Epoch 1 step 475: training loss: 2832.3500382665925\n",
      "Epoch 1 step 476: training accuarcy: 0.9019\n",
      "Epoch 1 step 476: training loss: 2749.468211362972\n",
      "Epoch 1 step 477: training accuarcy: 0.9074000000000001\n",
      "Epoch 1 step 477: training loss: 2767.3560385977653\n",
      "Epoch 1 step 478: training accuarcy: 0.9035000000000001\n",
      "Epoch 1 step 478: training loss: 2700.3314033285715\n",
      "Epoch 1 step 479: training accuarcy: 0.9069\n",
      "Epoch 1 step 479: training loss: 2787.9300730046302\n",
      "Epoch 1 step 480: training accuarcy: 0.9062\n",
      "Epoch 1 step 480: training loss: 2730.347523898475\n",
      "Epoch 1 step 481: training accuarcy: 0.9053\n",
      "Epoch 1 step 481: training loss: 2688.6730021197204\n",
      "Epoch 1 step 482: training accuarcy: 0.9081\n",
      "Epoch 1 step 482: training loss: 2726.0404620909494\n",
      "Epoch 1 step 483: training accuarcy: 0.9051\n",
      "Epoch 1 step 483: training loss: 2779.0566413552915\n",
      "Epoch 1 step 484: training accuarcy: 0.9032\n",
      "Epoch 1 step 484: training loss: 2750.1693628227995\n",
      "Epoch 1 step 485: training accuarcy: 0.9069\n",
      "Epoch 1 step 485: training loss: 2793.247119270319\n",
      "Epoch 1 step 486: training accuarcy: 0.9013\n",
      "Epoch 1 step 486: training loss: 2773.614092295199\n",
      "Epoch 1 step 487: training accuarcy: 0.9072\n",
      "Epoch 1 step 487: training loss: 2816.523971645703\n",
      "Epoch 1 step 488: training accuarcy: 0.9026000000000001\n",
      "Epoch 1 step 488: training loss: 2773.115112001716\n",
      "Epoch 1 step 489: training accuarcy: 0.9044000000000001\n",
      "Epoch 1 step 489: training loss: 2772.588737242107\n",
      "Epoch 1 step 490: training accuarcy: 0.9014000000000001\n",
      "Epoch 1 step 490: training loss: 2724.476049093121\n",
      "Epoch 1 step 491: training accuarcy: 0.908\n",
      "Epoch 1 step 491: training loss: 2701.621622930407\n",
      "Epoch 1 step 492: training accuarcy: 0.9096000000000001\n",
      "Epoch 1 step 492: training loss: 2727.512709887292\n",
      "Epoch 1 step 493: training accuarcy: 0.9076000000000001\n",
      "Epoch 1 step 493: training loss: 2845.182354389429\n",
      "Epoch 1 step 494: training accuarcy: 0.9003\n",
      "Epoch 1 step 494: training loss: 2785.4291743228873\n",
      "Epoch 1 step 495: training accuarcy: 0.9036000000000001\n",
      "Epoch 1 step 495: training loss: 2780.079259092584\n",
      "Epoch 1 step 496: training accuarcy: 0.9045000000000001\n",
      "Epoch 1 step 496: training loss: 2693.8080031508935\n",
      "Epoch 1 step 497: training accuarcy: 0.9069\n",
      "Epoch 1 step 497: training loss: 2779.8161371946617\n",
      "Epoch 1 step 498: training accuarcy: 0.9034000000000001\n",
      "Epoch 1 step 498: training loss: 2797.362469127635\n",
      "Epoch 1 step 499: training accuarcy: 0.8995000000000001\n",
      "Epoch 1 step 499: training loss: 2739.7234925276857\n",
      "Epoch 1 step 500: training accuarcy: 0.906\n",
      "Epoch 1 step 500: training loss: 2777.480775938479\n",
      "Epoch 1 step 501: training accuarcy: 0.9051\n",
      "Epoch 1 step 501: training loss: 2735.573848696351\n",
      "Epoch 1 step 502: training accuarcy: 0.9051\n",
      "Epoch 1 step 502: training loss: 2752.9593681531564\n",
      "Epoch 1 step 503: training accuarcy: 0.9072\n",
      "Epoch 1 step 503: training loss: 2720.3963900466183\n",
      "Epoch 1 step 504: training accuarcy: 0.9061\n",
      "Epoch 1 step 504: training loss: 2810.6653090524455\n",
      "Epoch 1 step 505: training accuarcy: 0.9006000000000001\n",
      "Epoch 1 step 505: training loss: 2740.260607919717\n",
      "Epoch 1 step 506: training accuarcy: 0.9072\n",
      "Epoch 1 step 506: training loss: 2751.2695046067875\n",
      "Epoch 1 step 507: training accuarcy: 0.901\n",
      "Epoch 1 step 507: training loss: 2668.7403626555833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 508: training accuarcy: 0.909\n",
      "Epoch 1 step 508: training loss: 2747.1397894186803\n",
      "Epoch 1 step 509: training accuarcy: 0.9043\n",
      "Epoch 1 step 509: training loss: 2753.469493205476\n",
      "Epoch 1 step 510: training accuarcy: 0.9027000000000001\n",
      "Epoch 1 step 510: training loss: 2748.9784185364115\n",
      "Epoch 1 step 511: training accuarcy: 0.9055000000000001\n",
      "Epoch 1 step 511: training loss: 2759.6375053286715\n",
      "Epoch 1 step 512: training accuarcy: 0.9031\n",
      "Epoch 1 step 512: training loss: 2815.6931116791147\n",
      "Epoch 1 step 513: training accuarcy: 0.9013\n",
      "Epoch 1 step 513: training loss: 2694.0017447197224\n",
      "Epoch 1 step 514: training accuarcy: 0.9083\n",
      "Epoch 1 step 514: training loss: 2759.394626819846\n",
      "Epoch 1 step 515: training accuarcy: 0.9015000000000001\n",
      "Epoch 1 step 515: training loss: 2758.6026848942633\n",
      "Epoch 1 step 516: training accuarcy: 0.9044000000000001\n",
      "Epoch 1 step 516: training loss: 2744.0564978915363\n",
      "Epoch 1 step 517: training accuarcy: 0.9042\n",
      "Epoch 1 step 517: training loss: 2665.7533077195767\n",
      "Epoch 1 step 518: training accuarcy: 0.9094000000000001\n",
      "Epoch 1 step 518: training loss: 2760.3802417865336\n",
      "Epoch 1 step 519: training accuarcy: 0.9008\n",
      "Epoch 1 step 519: training loss: 2729.9838498999143\n",
      "Epoch 1 step 520: training accuarcy: 0.9022\n",
      "Epoch 1 step 520: training loss: 2618.461972447669\n",
      "Epoch 1 step 521: training accuarcy: 0.9131\n",
      "Epoch 1 step 521: training loss: 2682.5187428865993\n",
      "Epoch 1 step 522: training accuarcy: 0.9038\n",
      "Epoch 1 step 522: training loss: 2704.6969548336297\n",
      "Epoch 1 step 523: training accuarcy: 0.903\n",
      "Epoch 1 step 523: training loss: 2777.1489448978377\n",
      "Epoch 1 step 524: training accuarcy: 0.9005000000000001\n",
      "Epoch 1 step 524: training loss: 2778.5679998624005\n",
      "Epoch 1 step 525: training accuarcy: 0.905\n",
      "Epoch 1 step 525: training loss: 1344.9518862981422\n",
      "Epoch 1 step 526: training accuarcy: 0.9048717948717949\n",
      "Epoch 1: train loss 3010.5026825702566, train accuarcy 0.8818857669830322\n",
      "Epoch 1: valid loss 3020.9696631509473, valid accuarcy 0.882151186466217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [09:53<04:58, 298.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 2463.249562397879\n",
      "Epoch 2 step 527: training accuarcy: 0.9244\n",
      "Epoch 2 step 527: training loss: 2644.7975321448034\n",
      "Epoch 2 step 528: training accuarcy: 0.9162\n",
      "Epoch 2 step 528: training loss: 2526.4155175960937\n",
      "Epoch 2 step 529: training accuarcy: 0.9206000000000001\n",
      "Epoch 2 step 529: training loss: 2498.493884063634\n",
      "Epoch 2 step 530: training accuarcy: 0.9211\n",
      "Epoch 2 step 530: training loss: 2561.152050748146\n",
      "Epoch 2 step 531: training accuarcy: 0.9204\n",
      "Epoch 2 step 531: training loss: 2576.504245167436\n",
      "Epoch 2 step 532: training accuarcy: 0.916\n",
      "Epoch 2 step 532: training loss: 2489.975740599931\n",
      "Epoch 2 step 533: training accuarcy: 0.9268000000000001\n",
      "Epoch 2 step 533: training loss: 2462.0078116794516\n",
      "Epoch 2 step 534: training accuarcy: 0.9252\n",
      "Epoch 2 step 534: training loss: 2488.068505668272\n",
      "Epoch 2 step 535: training accuarcy: 0.9231\n",
      "Epoch 2 step 535: training loss: 2501.043814013624\n",
      "Epoch 2 step 536: training accuarcy: 0.9212\n",
      "Epoch 2 step 536: training loss: 2459.007585008899\n",
      "Epoch 2 step 537: training accuarcy: 0.9271\n",
      "Epoch 2 step 537: training loss: 2486.769845555009\n",
      "Epoch 2 step 538: training accuarcy: 0.9231\n",
      "Epoch 2 step 538: training loss: 2508.988753298331\n",
      "Epoch 2 step 539: training accuarcy: 0.9195000000000001\n",
      "Epoch 2 step 539: training loss: 2534.9459231303213\n",
      "Epoch 2 step 540: training accuarcy: 0.9209\n",
      "Epoch 2 step 540: training loss: 2541.8291655652556\n",
      "Epoch 2 step 541: training accuarcy: 0.9194\n",
      "Epoch 2 step 541: training loss: 2490.574615199746\n",
      "Epoch 2 step 542: training accuarcy: 0.9247000000000001\n",
      "Epoch 2 step 542: training loss: 2466.333568914655\n",
      "Epoch 2 step 543: training accuarcy: 0.9223\n",
      "Epoch 2 step 543: training loss: 2449.504813687311\n",
      "Epoch 2 step 544: training accuarcy: 0.9246000000000001\n",
      "Epoch 2 step 544: training loss: 2495.5081794660696\n",
      "Epoch 2 step 545: training accuarcy: 0.92\n",
      "Epoch 2 step 545: training loss: 2490.301733532536\n",
      "Epoch 2 step 546: training accuarcy: 0.9257000000000001\n",
      "Epoch 2 step 546: training loss: 2515.4266162247077\n",
      "Epoch 2 step 547: training accuarcy: 0.9198000000000001\n",
      "Epoch 2 step 547: training loss: 2481.202879615607\n",
      "Epoch 2 step 548: training accuarcy: 0.9235000000000001\n",
      "Epoch 2 step 548: training loss: 2535.435438807245\n",
      "Epoch 2 step 549: training accuarcy: 0.9151\n",
      "Epoch 2 step 549: training loss: 2523.4592791426753\n",
      "Epoch 2 step 550: training accuarcy: 0.9187000000000001\n",
      "Epoch 2 step 550: training loss: 2487.9170990762495\n",
      "Epoch 2 step 551: training accuarcy: 0.921\n",
      "Epoch 2 step 551: training loss: 2558.84038630334\n",
      "Epoch 2 step 552: training accuarcy: 0.9141\n",
      "Epoch 2 step 552: training loss: 2518.7585443785783\n",
      "Epoch 2 step 553: training accuarcy: 0.9194\n",
      "Epoch 2 step 553: training loss: 2459.3199284328143\n",
      "Epoch 2 step 554: training accuarcy: 0.9196000000000001\n",
      "Epoch 2 step 554: training loss: 2414.2151861160078\n",
      "Epoch 2 step 555: training accuarcy: 0.9244\n",
      "Epoch 2 step 555: training loss: 2535.083339803181\n",
      "Epoch 2 step 556: training accuarcy: 0.9219\n",
      "Epoch 2 step 556: training loss: 2544.001069616506\n",
      "Epoch 2 step 557: training accuarcy: 0.9177000000000001\n",
      "Epoch 2 step 557: training loss: 2493.3153631303776\n",
      "Epoch 2 step 558: training accuarcy: 0.9236000000000001\n",
      "Epoch 2 step 558: training loss: 2531.7517937099524\n",
      "Epoch 2 step 559: training accuarcy: 0.9178000000000001\n",
      "Epoch 2 step 559: training loss: 2539.0541280540315\n",
      "Epoch 2 step 560: training accuarcy: 0.9204\n",
      "Epoch 2 step 560: training loss: 2611.6623763948883\n",
      "Epoch 2 step 561: training accuarcy: 0.912\n",
      "Epoch 2 step 561: training loss: 2503.2141133552464\n",
      "Epoch 2 step 562: training accuarcy: 0.9183\n",
      "Epoch 2 step 562: training loss: 2485.739079537791\n",
      "Epoch 2 step 563: training accuarcy: 0.9198000000000001\n",
      "Epoch 2 step 563: training loss: 2505.6446541861615\n",
      "Epoch 2 step 564: training accuarcy: 0.9219\n",
      "Epoch 2 step 564: training loss: 2443.64200127642\n",
      "Epoch 2 step 565: training accuarcy: 0.9239\n",
      "Epoch 2 step 565: training loss: 2585.2257896335623\n",
      "Epoch 2 step 566: training accuarcy: 0.9173\n",
      "Epoch 2 step 566: training loss: 2493.1666151314453\n",
      "Epoch 2 step 567: training accuarcy: 0.9165000000000001\n",
      "Epoch 2 step 567: training loss: 2605.8944313359416\n",
      "Epoch 2 step 568: training accuarcy: 0.9137000000000001\n",
      "Epoch 2 step 568: training loss: 2508.307245520854\n",
      "Epoch 2 step 569: training accuarcy: 0.9213\n",
      "Epoch 2 step 569: training loss: 2464.173614125565\n",
      "Epoch 2 step 570: training accuarcy: 0.9234\n",
      "Epoch 2 step 570: training loss: 2606.67606453564\n",
      "Epoch 2 step 571: training accuarcy: 0.9163\n",
      "Epoch 2 step 571: training loss: 2464.05987690599\n",
      "Epoch 2 step 572: training accuarcy: 0.9249\n",
      "Epoch 2 step 572: training loss: 2513.461868351527\n",
      "Epoch 2 step 573: training accuarcy: 0.9117000000000001\n",
      "Epoch 2 step 573: training loss: 2538.5459037480664\n",
      "Epoch 2 step 574: training accuarcy: 0.9189\n",
      "Epoch 2 step 574: training loss: 2530.451484883072\n",
      "Epoch 2 step 575: training accuarcy: 0.9216000000000001\n",
      "Epoch 2 step 575: training loss: 2445.2825960602568\n",
      "Epoch 2 step 576: training accuarcy: 0.9215000000000001\n",
      "Epoch 2 step 576: training loss: 2490.2514641344274\n",
      "Epoch 2 step 577: training accuarcy: 0.9204\n",
      "Epoch 2 step 577: training loss: 2545.4634278123413\n",
      "Epoch 2 step 578: training accuarcy: 0.9177000000000001\n",
      "Epoch 2 step 578: training loss: 2555.232614790422\n",
      "Epoch 2 step 579: training accuarcy: 0.918\n",
      "Epoch 2 step 579: training loss: 2582.6556461061537\n",
      "Epoch 2 step 580: training accuarcy: 0.9147000000000001\n",
      "Epoch 2 step 580: training loss: 2530.987381323296\n",
      "Epoch 2 step 581: training accuarcy: 0.919\n",
      "Epoch 2 step 581: training loss: 2517.529661767363\n",
      "Epoch 2 step 582: training accuarcy: 0.9189\n",
      "Epoch 2 step 582: training loss: 2528.7691658621866\n",
      "Epoch 2 step 583: training accuarcy: 0.9176000000000001\n",
      "Epoch 2 step 583: training loss: 2552.0557499902307\n",
      "Epoch 2 step 584: training accuarcy: 0.9158000000000001\n",
      "Epoch 2 step 584: training loss: 2445.505539528146\n",
      "Epoch 2 step 585: training accuarcy: 0.9195000000000001\n",
      "Epoch 2 step 585: training loss: 2507.3521042600673\n",
      "Epoch 2 step 586: training accuarcy: 0.9195000000000001\n",
      "Epoch 2 step 586: training loss: 2458.500612849488\n",
      "Epoch 2 step 587: training accuarcy: 0.9213\n",
      "Epoch 2 step 587: training loss: 2554.2309876982104\n",
      "Epoch 2 step 588: training accuarcy: 0.9149\n",
      "Epoch 2 step 588: training loss: 2534.9403498574393\n",
      "Epoch 2 step 589: training accuarcy: 0.9164\n",
      "Epoch 2 step 589: training loss: 2441.305798300308\n",
      "Epoch 2 step 590: training accuarcy: 0.9256000000000001\n",
      "Epoch 2 step 590: training loss: 2518.4827867350214\n",
      "Epoch 2 step 591: training accuarcy: 0.9174\n",
      "Epoch 2 step 591: training loss: 2514.4432205863486\n",
      "Epoch 2 step 592: training accuarcy: 0.9147000000000001\n",
      "Epoch 2 step 592: training loss: 2471.396588262379\n",
      "Epoch 2 step 593: training accuarcy: 0.9207000000000001\n",
      "Epoch 2 step 593: training loss: 2470.1051292565708\n",
      "Epoch 2 step 594: training accuarcy: 0.9203\n",
      "Epoch 2 step 594: training loss: 2578.832985704626\n",
      "Epoch 2 step 595: training accuarcy: 0.918\n",
      "Epoch 2 step 595: training loss: 2497.1200361756514\n",
      "Epoch 2 step 596: training accuarcy: 0.9187000000000001\n",
      "Epoch 2 step 596: training loss: 2632.6918537647493\n",
      "Epoch 2 step 597: training accuarcy: 0.91\n",
      "Epoch 2 step 597: training loss: 2467.872255637685\n",
      "Epoch 2 step 598: training accuarcy: 0.916\n",
      "Epoch 2 step 598: training loss: 2533.5740310983474\n",
      "Epoch 2 step 599: training accuarcy: 0.9139\n",
      "Epoch 2 step 599: training loss: 2517.0530409009502\n",
      "Epoch 2 step 600: training accuarcy: 0.9165000000000001\n",
      "Epoch 2 step 600: training loss: 2439.896893615492\n",
      "Epoch 2 step 601: training accuarcy: 0.9212\n",
      "Epoch 2 step 601: training loss: 2470.814305318471\n",
      "Epoch 2 step 602: training accuarcy: 0.9229\n",
      "Epoch 2 step 602: training loss: 2503.3790942897544\n",
      "Epoch 2 step 603: training accuarcy: 0.9157000000000001\n",
      "Epoch 2 step 603: training loss: 2558.9282038839883\n",
      "Epoch 2 step 604: training accuarcy: 0.922\n",
      "Epoch 2 step 604: training loss: 2467.882103273001\n",
      "Epoch 2 step 605: training accuarcy: 0.9176000000000001\n",
      "Epoch 2 step 605: training loss: 2554.057914435541\n",
      "Epoch 2 step 606: training accuarcy: 0.9154\n",
      "Epoch 2 step 606: training loss: 2403.5662793628308\n",
      "Epoch 2 step 607: training accuarcy: 0.9269000000000001\n",
      "Epoch 2 step 607: training loss: 2438.337017397251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 608: training accuarcy: 0.9246000000000001\n",
      "Epoch 2 step 608: training loss: 2520.2222590527595\n",
      "Epoch 2 step 609: training accuarcy: 0.9197000000000001\n",
      "Epoch 2 step 609: training loss: 2367.705036605018\n",
      "Epoch 2 step 610: training accuarcy: 0.9271\n",
      "Epoch 2 step 610: training loss: 2434.760432207689\n",
      "Epoch 2 step 611: training accuarcy: 0.92\n",
      "Epoch 2 step 611: training loss: 2414.168806405387\n",
      "Epoch 2 step 612: training accuarcy: 0.9228000000000001\n",
      "Epoch 2 step 612: training loss: 2496.5671979814115\n",
      "Epoch 2 step 613: training accuarcy: 0.9201\n",
      "Epoch 2 step 613: training loss: 2409.4713203134415\n",
      "Epoch 2 step 614: training accuarcy: 0.9246000000000001\n",
      "Epoch 2 step 614: training loss: 2498.5908265848598\n",
      "Epoch 2 step 615: training accuarcy: 0.9163\n",
      "Epoch 2 step 615: training loss: 2547.7652123532403\n",
      "Epoch 2 step 616: training accuarcy: 0.9169\n",
      "Epoch 2 step 616: training loss: 2497.6139012080275\n",
      "Epoch 2 step 617: training accuarcy: 0.9164\n",
      "Epoch 2 step 617: training loss: 2562.8692561907\n",
      "Epoch 2 step 618: training accuarcy: 0.9124\n",
      "Epoch 2 step 618: training loss: 2466.7488661763405\n",
      "Epoch 2 step 619: training accuarcy: 0.9179\n",
      "Epoch 2 step 619: training loss: 2530.7408904373547\n",
      "Epoch 2 step 620: training accuarcy: 0.9154\n",
      "Epoch 2 step 620: training loss: 2470.0366371941964\n",
      "Epoch 2 step 621: training accuarcy: 0.9172\n",
      "Epoch 2 step 621: training loss: 2469.8695692098318\n",
      "Epoch 2 step 622: training accuarcy: 0.9188000000000001\n",
      "Epoch 2 step 622: training loss: 2538.5187554987374\n",
      "Epoch 2 step 623: training accuarcy: 0.9151\n",
      "Epoch 2 step 623: training loss: 2479.7764282464777\n",
      "Epoch 2 step 624: training accuarcy: 0.9204\n",
      "Epoch 2 step 624: training loss: 2506.613154694501\n",
      "Epoch 2 step 625: training accuarcy: 0.9122\n",
      "Epoch 2 step 625: training loss: 2424.1382135465205\n",
      "Epoch 2 step 626: training accuarcy: 0.9232\n",
      "Epoch 2 step 626: training loss: 2468.3212084427464\n",
      "Epoch 2 step 627: training accuarcy: 0.9185000000000001\n",
      "Epoch 2 step 627: training loss: 2470.0571759399727\n",
      "Epoch 2 step 628: training accuarcy: 0.918\n",
      "Epoch 2 step 628: training loss: 2517.992302963943\n",
      "Epoch 2 step 629: training accuarcy: 0.9134\n",
      "Epoch 2 step 629: training loss: 2462.664902453198\n",
      "Epoch 2 step 630: training accuarcy: 0.9168000000000001\n",
      "Epoch 2 step 630: training loss: 2413.4453710554594\n",
      "Epoch 2 step 631: training accuarcy: 0.9204\n",
      "Epoch 2 step 631: training loss: 2536.786209848544\n",
      "Epoch 2 step 632: training accuarcy: 0.914\n",
      "Epoch 2 step 632: training loss: 2456.4422910737126\n",
      "Epoch 2 step 633: training accuarcy: 0.9161\n",
      "Epoch 2 step 633: training loss: 2544.768574120541\n",
      "Epoch 2 step 634: training accuarcy: 0.9132\n",
      "Epoch 2 step 634: training loss: 2448.35468838012\n",
      "Epoch 2 step 635: training accuarcy: 0.9209\n",
      "Epoch 2 step 635: training loss: 2480.26378865184\n",
      "Epoch 2 step 636: training accuarcy: 0.9192\n",
      "Epoch 2 step 636: training loss: 2426.780605710441\n",
      "Epoch 2 step 637: training accuarcy: 0.9208000000000001\n",
      "Epoch 2 step 637: training loss: 2544.743540149059\n",
      "Epoch 2 step 638: training accuarcy: 0.9112\n",
      "Epoch 2 step 638: training loss: 2429.519037713222\n",
      "Epoch 2 step 639: training accuarcy: 0.921\n",
      "Epoch 2 step 639: training loss: 2525.824467699641\n",
      "Epoch 2 step 640: training accuarcy: 0.9124\n",
      "Epoch 2 step 640: training loss: 2461.050861365831\n",
      "Epoch 2 step 641: training accuarcy: 0.9194\n",
      "Epoch 2 step 641: training loss: 2349.179839719201\n",
      "Epoch 2 step 642: training accuarcy: 0.9285\n",
      "Epoch 2 step 642: training loss: 2422.4336660706117\n",
      "Epoch 2 step 643: training accuarcy: 0.9251\n",
      "Epoch 2 step 643: training loss: 2487.260470931048\n",
      "Epoch 2 step 644: training accuarcy: 0.9163\n",
      "Epoch 2 step 644: training loss: 2411.4905917451183\n",
      "Epoch 2 step 645: training accuarcy: 0.9259000000000001\n",
      "Epoch 2 step 645: training loss: 2442.7728868882596\n",
      "Epoch 2 step 646: training accuarcy: 0.9212\n",
      "Epoch 2 step 646: training loss: 2493.4876426169017\n",
      "Epoch 2 step 647: training accuarcy: 0.9139\n",
      "Epoch 2 step 647: training loss: 2483.4176378220177\n",
      "Epoch 2 step 648: training accuarcy: 0.9182\n",
      "Epoch 2 step 648: training loss: 2506.481422021305\n",
      "Epoch 2 step 649: training accuarcy: 0.9168000000000001\n",
      "Epoch 2 step 649: training loss: 2416.575144746812\n",
      "Epoch 2 step 650: training accuarcy: 0.9232\n",
      "Epoch 2 step 650: training loss: 2515.8925412123954\n",
      "Epoch 2 step 651: training accuarcy: 0.9145000000000001\n",
      "Epoch 2 step 651: training loss: 2467.0173946602617\n",
      "Epoch 2 step 652: training accuarcy: 0.9181\n",
      "Epoch 2 step 652: training loss: 2453.573176820565\n",
      "Epoch 2 step 653: training accuarcy: 0.9183\n",
      "Epoch 2 step 653: training loss: 2434.3725900797017\n",
      "Epoch 2 step 654: training accuarcy: 0.9229\n",
      "Epoch 2 step 654: training loss: 2434.6020090657375\n",
      "Epoch 2 step 655: training accuarcy: 0.9218000000000001\n",
      "Epoch 2 step 655: training loss: 2433.4754953654356\n",
      "Epoch 2 step 656: training accuarcy: 0.9212\n",
      "Epoch 2 step 656: training loss: 2405.143343105187\n",
      "Epoch 2 step 657: training accuarcy: 0.9251\n",
      "Epoch 2 step 657: training loss: 2467.4357351995895\n",
      "Epoch 2 step 658: training accuarcy: 0.9174\n",
      "Epoch 2 step 658: training loss: 2533.7072128184136\n",
      "Epoch 2 step 659: training accuarcy: 0.9181\n",
      "Epoch 2 step 659: training loss: 2521.144982028996\n",
      "Epoch 2 step 660: training accuarcy: 0.9177000000000001\n",
      "Epoch 2 step 660: training loss: 2412.7910719161687\n",
      "Epoch 2 step 661: training accuarcy: 0.9244\n",
      "Epoch 2 step 661: training loss: 2495.9653148797793\n",
      "Epoch 2 step 662: training accuarcy: 0.9215000000000001\n",
      "Epoch 2 step 662: training loss: 2488.8454954984277\n",
      "Epoch 2 step 663: training accuarcy: 0.9205000000000001\n",
      "Epoch 2 step 663: training loss: 2437.589265719337\n",
      "Epoch 2 step 664: training accuarcy: 0.9236000000000001\n",
      "Epoch 2 step 664: training loss: 2455.6104807502447\n",
      "Epoch 2 step 665: training accuarcy: 0.9239\n",
      "Epoch 2 step 665: training loss: 2525.7399898542903\n",
      "Epoch 2 step 666: training accuarcy: 0.9163\n",
      "Epoch 2 step 666: training loss: 2465.116997008275\n",
      "Epoch 2 step 667: training accuarcy: 0.9184\n",
      "Epoch 2 step 667: training loss: 2470.065735870591\n",
      "Epoch 2 step 668: training accuarcy: 0.9181\n",
      "Epoch 2 step 668: training loss: 2456.817384716055\n",
      "Epoch 2 step 669: training accuarcy: 0.9204\n",
      "Epoch 2 step 669: training loss: 2444.2336779881484\n",
      "Epoch 2 step 670: training accuarcy: 0.9215000000000001\n",
      "Epoch 2 step 670: training loss: 2447.464602078187\n",
      "Epoch 2 step 671: training accuarcy: 0.919\n",
      "Epoch 2 step 671: training loss: 2452.799994589422\n",
      "Epoch 2 step 672: training accuarcy: 0.9208000000000001\n",
      "Epoch 2 step 672: training loss: 2504.243982423247\n",
      "Epoch 2 step 673: training accuarcy: 0.9159\n",
      "Epoch 2 step 673: training loss: 2456.2342371842606\n",
      "Epoch 2 step 674: training accuarcy: 0.9193\n",
      "Epoch 2 step 674: training loss: 2420.212466284862\n",
      "Epoch 2 step 675: training accuarcy: 0.9187000000000001\n",
      "Epoch 2 step 675: training loss: 2344.775587538802\n",
      "Epoch 2 step 676: training accuarcy: 0.927\n",
      "Epoch 2 step 676: training loss: 2421.4679362763563\n",
      "Epoch 2 step 677: training accuarcy: 0.9226000000000001\n",
      "Epoch 2 step 677: training loss: 2491.0282631757254\n",
      "Epoch 2 step 678: training accuarcy: 0.9194\n",
      "Epoch 2 step 678: training loss: 2382.2325137742127\n",
      "Epoch 2 step 679: training accuarcy: 0.9233\n",
      "Epoch 2 step 679: training loss: 2466.2066381407153\n",
      "Epoch 2 step 680: training accuarcy: 0.916\n",
      "Epoch 2 step 680: training loss: 2390.6259172142045\n",
      "Epoch 2 step 681: training accuarcy: 0.92\n",
      "Epoch 2 step 681: training loss: 2474.4787715478146\n",
      "Epoch 2 step 682: training accuarcy: 0.9194\n",
      "Epoch 2 step 682: training loss: 2401.965852377374\n",
      "Epoch 2 step 683: training accuarcy: 0.921\n",
      "Epoch 2 step 683: training loss: 2388.5896181063717\n",
      "Epoch 2 step 684: training accuarcy: 0.9205000000000001\n",
      "Epoch 2 step 684: training loss: 2499.940977151823\n",
      "Epoch 2 step 685: training accuarcy: 0.9159\n",
      "Epoch 2 step 685: training loss: 2431.5159122994155\n",
      "Epoch 2 step 686: training accuarcy: 0.9209\n",
      "Epoch 2 step 686: training loss: 2380.859618486664\n",
      "Epoch 2 step 687: training accuarcy: 0.9214\n",
      "Epoch 2 step 687: training loss: 2378.7669984902923\n",
      "Epoch 2 step 688: training accuarcy: 0.9254\n",
      "Epoch 2 step 688: training loss: 2370.006724804736\n",
      "Epoch 2 step 689: training accuarcy: 0.9236000000000001\n",
      "Epoch 2 step 689: training loss: 2412.954280420658\n",
      "Epoch 2 step 690: training accuarcy: 0.9231\n",
      "Epoch 2 step 690: training loss: 2418.564562603544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 691: training accuarcy: 0.9211\n",
      "Epoch 2 step 691: training loss: 2422.6347955982683\n",
      "Epoch 2 step 692: training accuarcy: 0.9218000000000001\n",
      "Epoch 2 step 692: training loss: 2428.002158131525\n",
      "Epoch 2 step 693: training accuarcy: 0.9197000000000001\n",
      "Epoch 2 step 693: training loss: 2430.978688118582\n",
      "Epoch 2 step 694: training accuarcy: 0.92\n",
      "Epoch 2 step 694: training loss: 2399.291982053827\n",
      "Epoch 2 step 695: training accuarcy: 0.9246000000000001\n",
      "Epoch 2 step 695: training loss: 2494.235013117953\n",
      "Epoch 2 step 696: training accuarcy: 0.9153\n",
      "Epoch 2 step 696: training loss: 2473.1082577662205\n",
      "Epoch 2 step 697: training accuarcy: 0.9198000000000001\n",
      "Epoch 2 step 697: training loss: 2384.8626362919535\n",
      "Epoch 2 step 698: training accuarcy: 0.9259000000000001\n",
      "Epoch 2 step 698: training loss: 2390.7218758842905\n",
      "Epoch 2 step 699: training accuarcy: 0.9244\n",
      "Epoch 2 step 699: training loss: 2475.6005071662676\n",
      "Epoch 2 step 700: training accuarcy: 0.9165000000000001\n",
      "Epoch 2 step 700: training loss: 2457.823237166201\n",
      "Epoch 2 step 701: training accuarcy: 0.9173\n",
      "Epoch 2 step 701: training loss: 2361.1825587189014\n",
      "Epoch 2 step 702: training accuarcy: 0.9237000000000001\n",
      "Epoch 2 step 702: training loss: 2424.9774179703063\n",
      "Epoch 2 step 703: training accuarcy: 0.9199\n",
      "Epoch 2 step 703: training loss: 2454.195590784108\n",
      "Epoch 2 step 704: training accuarcy: 0.919\n",
      "Epoch 2 step 704: training loss: 2363.496328420784\n",
      "Epoch 2 step 705: training accuarcy: 0.926\n",
      "Epoch 2 step 705: training loss: 2393.638678367864\n",
      "Epoch 2 step 706: training accuarcy: 0.923\n",
      "Epoch 2 step 706: training loss: 2462.971702015875\n",
      "Epoch 2 step 707: training accuarcy: 0.9173\n",
      "Epoch 2 step 707: training loss: 2445.688832279603\n",
      "Epoch 2 step 708: training accuarcy: 0.9176000000000001\n",
      "Epoch 2 step 708: training loss: 2407.784087544744\n",
      "Epoch 2 step 709: training accuarcy: 0.9254\n",
      "Epoch 2 step 709: training loss: 2350.616848126384\n",
      "Epoch 2 step 710: training accuarcy: 0.9272\n",
      "Epoch 2 step 710: training loss: 2423.9761751280375\n",
      "Epoch 2 step 711: training accuarcy: 0.9192\n",
      "Epoch 2 step 711: training loss: 2325.372226173751\n",
      "Epoch 2 step 712: training accuarcy: 0.9291\n",
      "Epoch 2 step 712: training loss: 2391.5159969476936\n",
      "Epoch 2 step 713: training accuarcy: 0.9237000000000001\n",
      "Epoch 2 step 713: training loss: 2434.882843677068\n",
      "Epoch 2 step 714: training accuarcy: 0.9205000000000001\n",
      "Epoch 2 step 714: training loss: 2397.2435901882304\n",
      "Epoch 2 step 715: training accuarcy: 0.9251\n",
      "Epoch 2 step 715: training loss: 2379.0279618179866\n",
      "Epoch 2 step 716: training accuarcy: 0.9213\n",
      "Epoch 2 step 716: training loss: 2442.4501144206893\n",
      "Epoch 2 step 717: training accuarcy: 0.9214\n",
      "Epoch 2 step 717: training loss: 2399.8362485989824\n",
      "Epoch 2 step 718: training accuarcy: 0.9234\n",
      "Epoch 2 step 718: training loss: 2372.9770167705633\n",
      "Epoch 2 step 719: training accuarcy: 0.9228000000000001\n",
      "Epoch 2 step 719: training loss: 2384.638110325445\n",
      "Epoch 2 step 720: training accuarcy: 0.9249\n",
      "Epoch 2 step 720: training loss: 2428.868519118302\n",
      "Epoch 2 step 721: training accuarcy: 0.9183\n",
      "Epoch 2 step 721: training loss: 2383.4472314328236\n",
      "Epoch 2 step 722: training accuarcy: 0.9213\n",
      "Epoch 2 step 722: training loss: 2360.9394936782724\n",
      "Epoch 2 step 723: training accuarcy: 0.9261\n",
      "Epoch 2 step 723: training loss: 2410.757267333674\n",
      "Epoch 2 step 724: training accuarcy: 0.9219\n",
      "Epoch 2 step 724: training loss: 2363.69188089888\n",
      "Epoch 2 step 725: training accuarcy: 0.9229\n",
      "Epoch 2 step 725: training loss: 2407.3090657133753\n",
      "Epoch 2 step 726: training accuarcy: 0.9201\n",
      "Epoch 2 step 726: training loss: 2413.532950393446\n",
      "Epoch 2 step 727: training accuarcy: 0.9252\n",
      "Epoch 2 step 727: training loss: 2276.4258518407755\n",
      "Epoch 2 step 728: training accuarcy: 0.9286000000000001\n",
      "Epoch 2 step 728: training loss: 2416.1474882870043\n",
      "Epoch 2 step 729: training accuarcy: 0.9253\n",
      "Epoch 2 step 729: training loss: 2431.0281528092737\n",
      "Epoch 2 step 730: training accuarcy: 0.9203\n",
      "Epoch 2 step 730: training loss: 2343.5814282141764\n",
      "Epoch 2 step 731: training accuarcy: 0.9303\n",
      "Epoch 2 step 731: training loss: 2328.7690056527267\n",
      "Epoch 2 step 732: training accuarcy: 0.9246000000000001\n",
      "Epoch 2 step 732: training loss: 2426.0309119056124\n",
      "Epoch 2 step 733: training accuarcy: 0.9235000000000001\n",
      "Epoch 2 step 733: training loss: 2417.4313841332396\n",
      "Epoch 2 step 734: training accuarcy: 0.9201\n",
      "Epoch 2 step 734: training loss: 2373.722098230559\n",
      "Epoch 2 step 735: training accuarcy: 0.927\n",
      "Epoch 2 step 735: training loss: 2362.7081657100425\n",
      "Epoch 2 step 736: training accuarcy: 0.9216000000000001\n",
      "Epoch 2 step 736: training loss: 2428.5319364899333\n",
      "Epoch 2 step 737: training accuarcy: 0.92\n",
      "Epoch 2 step 737: training loss: 2342.584273618527\n",
      "Epoch 2 step 738: training accuarcy: 0.9256000000000001\n",
      "Epoch 2 step 738: training loss: 2449.0529990347877\n",
      "Epoch 2 step 739: training accuarcy: 0.9222\n",
      "Epoch 2 step 739: training loss: 2386.4686956083415\n",
      "Epoch 2 step 740: training accuarcy: 0.9222\n",
      "Epoch 2 step 740: training loss: 2383.7433316466822\n",
      "Epoch 2 step 741: training accuarcy: 0.9253\n",
      "Epoch 2 step 741: training loss: 2454.5695974527916\n",
      "Epoch 2 step 742: training accuarcy: 0.9168000000000001\n",
      "Epoch 2 step 742: training loss: 2483.274338278711\n",
      "Epoch 2 step 743: training accuarcy: 0.9161\n",
      "Epoch 2 step 743: training loss: 2405.7122814188656\n",
      "Epoch 2 step 744: training accuarcy: 0.9229\n",
      "Epoch 2 step 744: training loss: 2351.1945750722443\n",
      "Epoch 2 step 745: training accuarcy: 0.9244\n",
      "Epoch 2 step 745: training loss: 2395.6994235655484\n",
      "Epoch 2 step 746: training accuarcy: 0.9237000000000001\n",
      "Epoch 2 step 746: training loss: 2281.985729634388\n",
      "Epoch 2 step 747: training accuarcy: 0.93\n",
      "Epoch 2 step 747: training loss: 2379.791516664312\n",
      "Epoch 2 step 748: training accuarcy: 0.9229\n",
      "Epoch 2 step 748: training loss: 2394.5479288328497\n",
      "Epoch 2 step 749: training accuarcy: 0.9205000000000001\n",
      "Epoch 2 step 749: training loss: 2443.2438916257533\n",
      "Epoch 2 step 750: training accuarcy: 0.9199\n",
      "Epoch 2 step 750: training loss: 2343.7612397249295\n",
      "Epoch 2 step 751: training accuarcy: 0.929\n",
      "Epoch 2 step 751: training loss: 2427.305015231822\n",
      "Epoch 2 step 752: training accuarcy: 0.9192\n",
      "Epoch 2 step 752: training loss: 2430.7409858304213\n",
      "Epoch 2 step 753: training accuarcy: 0.9222\n",
      "Epoch 2 step 753: training loss: 2383.2396880740303\n",
      "Epoch 2 step 754: training accuarcy: 0.9222\n",
      "Epoch 2 step 754: training loss: 2377.9075062395623\n",
      "Epoch 2 step 755: training accuarcy: 0.9207000000000001\n",
      "Epoch 2 step 755: training loss: 2405.6757896706777\n",
      "Epoch 2 step 756: training accuarcy: 0.9207000000000001\n",
      "Epoch 2 step 756: training loss: 2440.827608020785\n",
      "Epoch 2 step 757: training accuarcy: 0.9165000000000001\n",
      "Epoch 2 step 757: training loss: 2390.975801997839\n",
      "Epoch 2 step 758: training accuarcy: 0.9235000000000001\n",
      "Epoch 2 step 758: training loss: 2414.0826212962925\n",
      "Epoch 2 step 759: training accuarcy: 0.9218000000000001\n",
      "Epoch 2 step 759: training loss: 2378.0823496519733\n",
      "Epoch 2 step 760: training accuarcy: 0.9302\n",
      "Epoch 2 step 760: training loss: 2354.4299666850457\n",
      "Epoch 2 step 761: training accuarcy: 0.9212\n",
      "Epoch 2 step 761: training loss: 2374.956215466492\n",
      "Epoch 2 step 762: training accuarcy: 0.923\n",
      "Epoch 2 step 762: training loss: 2434.698237118233\n",
      "Epoch 2 step 763: training accuarcy: 0.9177000000000001\n",
      "Epoch 2 step 763: training loss: 2419.1480347792503\n",
      "Epoch 2 step 764: training accuarcy: 0.9188000000000001\n",
      "Epoch 2 step 764: training loss: 2341.9409999602076\n",
      "Epoch 2 step 765: training accuarcy: 0.9223\n",
      "Epoch 2 step 765: training loss: 2372.973821907366\n",
      "Epoch 2 step 766: training accuarcy: 0.9248000000000001\n",
      "Epoch 2 step 766: training loss: 2406.9186768008535\n",
      "Epoch 2 step 767: training accuarcy: 0.9202\n",
      "Epoch 2 step 767: training loss: 2463.8769252152933\n",
      "Epoch 2 step 768: training accuarcy: 0.9219\n",
      "Epoch 2 step 768: training loss: 2471.4945566367996\n",
      "Epoch 2 step 769: training accuarcy: 0.9178000000000001\n",
      "Epoch 2 step 769: training loss: 2317.913506282902\n",
      "Epoch 2 step 770: training accuarcy: 0.9269000000000001\n",
      "Epoch 2 step 770: training loss: 2391.236996646994\n",
      "Epoch 2 step 771: training accuarcy: 0.922\n",
      "Epoch 2 step 771: training loss: 2400.6157497954764\n",
      "Epoch 2 step 772: training accuarcy: 0.9232\n",
      "Epoch 2 step 772: training loss: 2275.168337773798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 773: training accuarcy: 0.9269000000000001\n",
      "Epoch 2 step 773: training loss: 2413.9861168234124\n",
      "Epoch 2 step 774: training accuarcy: 0.9197000000000001\n",
      "Epoch 2 step 774: training loss: 2418.5103837021084\n",
      "Epoch 2 step 775: training accuarcy: 0.9178000000000001\n",
      "Epoch 2 step 775: training loss: 2332.8443987128853\n",
      "Epoch 2 step 776: training accuarcy: 0.9243\n",
      "Epoch 2 step 776: training loss: 2422.3543618005665\n",
      "Epoch 2 step 777: training accuarcy: 0.9184\n",
      "Epoch 2 step 777: training loss: 2449.8599825882125\n",
      "Epoch 2 step 778: training accuarcy: 0.9159\n",
      "Epoch 2 step 778: training loss: 2331.0916550110114\n",
      "Epoch 2 step 779: training accuarcy: 0.9268000000000001\n",
      "Epoch 2 step 779: training loss: 2346.5691548063824\n",
      "Epoch 2 step 780: training accuarcy: 0.9271\n",
      "Epoch 2 step 780: training loss: 2295.2918773760234\n",
      "Epoch 2 step 781: training accuarcy: 0.9276000000000001\n",
      "Epoch 2 step 781: training loss: 2292.7681471379606\n",
      "Epoch 2 step 782: training accuarcy: 0.9297000000000001\n",
      "Epoch 2 step 782: training loss: 2349.399313094464\n",
      "Epoch 2 step 783: training accuarcy: 0.9251\n",
      "Epoch 2 step 783: training loss: 2322.3995184897276\n",
      "Epoch 2 step 784: training accuarcy: 0.9261\n",
      "Epoch 2 step 784: training loss: 2350.13211772899\n",
      "Epoch 2 step 785: training accuarcy: 0.9224\n",
      "Epoch 2 step 785: training loss: 2371.1296996468923\n",
      "Epoch 2 step 786: training accuarcy: 0.9231\n",
      "Epoch 2 step 786: training loss: 2361.5470153339365\n",
      "Epoch 2 step 787: training accuarcy: 0.9211\n",
      "Epoch 2 step 787: training loss: 2338.409188324431\n",
      "Epoch 2 step 788: training accuarcy: 0.9273\n",
      "Epoch 2 step 788: training loss: 1222.9994972420998\n",
      "Epoch 2 step 789: training accuarcy: 0.9215384615384615\n",
      "Epoch 2: train loss 2445.961096018503, train accuarcy 0.9134019613265991\n",
      "Epoch 2: valid loss 2771.6342289291133, valid accuarcy 0.8936529755592346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [14:55<00:00, 299.24s/it]\n"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=3,\n",
    "                  log_dir=get_log_dir('simple_topcoder', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:49:20.906892Z",
     "start_time": "2019-10-09T13:33:56.043932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch 0 step 0: training loss: 328693.0835166554\n",
      "Epoch 0 step 1: training accuarcy: 0.4978\n",
      "Epoch 0 step 1: training loss: 314058.7249141159\n",
      "Epoch 0 step 2: training accuarcy: 0.5168\n",
      "Epoch 0 step 2: training loss: 300176.1108415055\n",
      "Epoch 0 step 3: training accuarcy: 0.5159\n",
      "Epoch 0 step 3: training loss: 307014.09675405966\n",
      "Epoch 0 step 4: training accuarcy: 0.5114000000000001\n",
      "Epoch 0 step 4: training loss: 297557.54419490095\n",
      "Epoch 0 step 5: training accuarcy: 0.5051\n",
      "Epoch 0 step 5: training loss: 291071.6806946936\n",
      "Epoch 0 step 6: training accuarcy: 0.5202\n",
      "Epoch 0 step 6: training loss: 281828.43845424784\n",
      "Epoch 0 step 7: training accuarcy: 0.5189\n",
      "Epoch 0 step 7: training loss: 268520.46315127116\n",
      "Epoch 0 step 8: training accuarcy: 0.5279\n",
      "Epoch 0 step 8: training loss: 272973.084331861\n",
      "Epoch 0 step 9: training accuarcy: 0.5239\n",
      "Epoch 0 step 9: training loss: 262233.4747713839\n",
      "Epoch 0 step 10: training accuarcy: 0.5155000000000001\n",
      "Epoch 0 step 10: training loss: 253659.75581515126\n",
      "Epoch 0 step 11: training accuarcy: 0.5236000000000001\n",
      "Epoch 0 step 11: training loss: 245521.83561244112\n",
      "Epoch 0 step 12: training accuarcy: 0.5334\n",
      "Epoch 0 step 12: training loss: 238246.9712663459\n",
      "Epoch 0 step 13: training accuarcy: 0.5327000000000001\n",
      "Epoch 0 step 13: training loss: 227894.65987047972\n",
      "Epoch 0 step 14: training accuarcy: 0.5297000000000001\n",
      "Epoch 0 step 14: training loss: 224434.26415784127\n",
      "Epoch 0 step 15: training accuarcy: 0.5265000000000001\n",
      "Epoch 0 step 15: training loss: 227920.68982185336\n",
      "Epoch 0 step 16: training accuarcy: 0.5185000000000001\n",
      "Epoch 0 step 16: training loss: 213573.65249683443\n",
      "Epoch 0 step 17: training accuarcy: 0.5199\n",
      "Epoch 0 step 17: training loss: 205760.82216515916\n",
      "Epoch 0 step 18: training accuarcy: 0.5297000000000001\n",
      "Epoch 0 step 18: training loss: 195993.21968429457\n",
      "Epoch 0 step 19: training accuarcy: 0.5378000000000001\n",
      "Epoch 0 step 19: training loss: 190672.09664731997\n",
      "Epoch 0 step 20: training accuarcy: 0.5333\n",
      "Epoch 0 step 20: training loss: 187184.00941107835\n",
      "Epoch 0 step 21: training accuarcy: 0.5385\n",
      "Epoch 0 step 21: training loss: 172272.62197615643\n",
      "Epoch 0 step 22: training accuarcy: 0.5393\n",
      "Epoch 0 step 22: training loss: 171427.09876749298\n",
      "Epoch 0 step 23: training accuarcy: 0.5358\n",
      "Epoch 0 step 23: training loss: 158777.58968002483\n",
      "Epoch 0 step 24: training accuarcy: 0.5383\n",
      "Epoch 0 step 24: training loss: 159199.56134566866\n",
      "Epoch 0 step 25: training accuarcy: 0.5339\n",
      "Epoch 0 step 25: training loss: 149505.90415560076\n",
      "Epoch 0 step 26: training accuarcy: 0.5483\n",
      "Epoch 0 step 26: training loss: 145433.9539670169\n",
      "Epoch 0 step 27: training accuarcy: 0.5367000000000001\n",
      "Epoch 0 step 27: training loss: 145692.31919362565\n",
      "Epoch 0 step 28: training accuarcy: 0.5258\n",
      "Epoch 0 step 28: training loss: 134964.44540080492\n",
      "Epoch 0 step 29: training accuarcy: 0.5492\n",
      "Epoch 0 step 29: training loss: 129311.61446401285\n",
      "Epoch 0 step 30: training accuarcy: 0.5385\n",
      "Epoch 0 step 30: training loss: 124854.12180417485\n",
      "Epoch 0 step 31: training accuarcy: 0.5378000000000001\n",
      "Epoch 0 step 31: training loss: 120776.99816914114\n",
      "Epoch 0 step 32: training accuarcy: 0.5351\n",
      "Epoch 0 step 32: training loss: 113383.27987170972\n",
      "Epoch 0 step 33: training accuarcy: 0.5349\n",
      "Epoch 0 step 33: training loss: 104671.77793726545\n",
      "Epoch 0 step 34: training accuarcy: 0.5535\n",
      "Epoch 0 step 34: training loss: 102026.50746937367\n",
      "Epoch 0 step 35: training accuarcy: 0.5508000000000001\n",
      "Epoch 0 step 35: training loss: 96502.41438937956\n",
      "Epoch 0 step 36: training accuarcy: 0.5513\n",
      "Epoch 0 step 36: training loss: 93163.29043214142\n",
      "Epoch 0 step 37: training accuarcy: 0.5543\n",
      "Epoch 0 step 37: training loss: 91378.73037651011\n",
      "Epoch 0 step 38: training accuarcy: 0.5443\n",
      "Epoch 0 step 38: training loss: 84508.43171610298\n",
      "Epoch 0 step 39: training accuarcy: 0.5549000000000001\n",
      "Epoch 0 step 39: training loss: 80861.76837788319\n",
      "Epoch 0 step 40: training accuarcy: 0.5604\n",
      "Epoch 0 step 40: training loss: 76270.91579029309\n",
      "Epoch 0 step 41: training accuarcy: 0.5587\n",
      "Epoch 0 step 41: training loss: 74229.96361268911\n",
      "Epoch 0 step 42: training accuarcy: 0.5503\n",
      "Epoch 0 step 42: training loss: 71542.3568061638\n",
      "Epoch 0 step 43: training accuarcy: 0.5643\n",
      "Epoch 0 step 43: training loss: 67580.12355452834\n",
      "Epoch 0 step 44: training accuarcy: 0.5711\n",
      "Epoch 0 step 44: training loss: 64672.12101786064\n",
      "Epoch 0 step 45: training accuarcy: 0.5921000000000001\n",
      "Epoch 0 step 45: training loss: 62371.28063935337\n",
      "Epoch 0 step 46: training accuarcy: 0.5935\n",
      "Epoch 0 step 46: training loss: 60545.06577266203\n",
      "Epoch 0 step 47: training accuarcy: 0.6004\n",
      "Epoch 0 step 47: training loss: 59075.751157979736\n",
      "Epoch 0 step 48: training accuarcy: 0.6023000000000001\n",
      "Epoch 0 step 48: training loss: 58136.17421056572\n",
      "Epoch 0 step 49: training accuarcy: 0.6121\n",
      "Epoch 0 step 49: training loss: 57138.17813150489\n",
      "Epoch 0 step 50: training accuarcy: 0.6117\n",
      "Epoch 0 step 50: training loss: 54837.24530362412\n",
      "Epoch 0 step 51: training accuarcy: 0.6217\n",
      "Epoch 0 step 51: training loss: 54091.30360110415\n",
      "Epoch 0 step 52: training accuarcy: 0.6301\n",
      "Epoch 0 step 52: training loss: 51809.87140204931\n",
      "Epoch 0 step 53: training accuarcy: 0.6376000000000001\n",
      "Epoch 0 step 53: training loss: 50624.77967079912\n",
      "Epoch 0 step 54: training accuarcy: 0.6577000000000001\n",
      "Epoch 0 step 54: training loss: 50048.496274538964\n",
      "Epoch 0 step 55: training accuarcy: 0.661\n",
      "Epoch 0 step 55: training loss: 49833.90638095401\n",
      "Epoch 0 step 56: training accuarcy: 0.6625\n",
      "Epoch 0 step 56: training loss: 49266.03298078814\n",
      "Epoch 0 step 57: training accuarcy: 0.6669\n",
      "Epoch 0 step 57: training loss: 47676.435443680435\n",
      "Epoch 0 step 58: training accuarcy: 0.6751\n",
      "Epoch 0 step 58: training loss: 46773.99221770922\n",
      "Epoch 0 step 59: training accuarcy: 0.6849000000000001\n",
      "Epoch 0 step 59: training loss: 46109.936139697755\n",
      "Epoch 0 step 60: training accuarcy: 0.6942\n",
      "Epoch 0 step 60: training loss: 45703.45067339204\n",
      "Epoch 0 step 61: training accuarcy: 0.6922\n",
      "Epoch 0 step 61: training loss: 45085.69718524005\n",
      "Epoch 0 step 62: training accuarcy: 0.6985\n",
      "Epoch 0 step 62: training loss: 44516.40051346527\n",
      "Epoch 0 step 63: training accuarcy: 0.7000000000000001\n",
      "Epoch 0 step 63: training loss: 44075.387228416774\n",
      "Epoch 0 step 64: training accuarcy: 0.7015\n",
      "Epoch 0 step 64: training loss: 43474.38330984606\n",
      "Epoch 0 step 65: training accuarcy: 0.7144\n",
      "Epoch 0 step 65: training loss: 42640.62684665913\n",
      "Epoch 0 step 66: training accuarcy: 0.7192000000000001\n",
      "Epoch 0 step 66: training loss: 42894.610858579734\n",
      "Epoch 0 step 67: training accuarcy: 0.7146\n",
      "Epoch 0 step 67: training loss: 42640.2568469846\n",
      "Epoch 0 step 68: training accuarcy: 0.7170000000000001\n",
      "Epoch 0 step 68: training loss: 42327.94354538612\n",
      "Epoch 0 step 69: training accuarcy: 0.7318\n",
      "Epoch 0 step 69: training loss: 41480.36396119179\n",
      "Epoch 0 step 70: training accuarcy: 0.7292000000000001\n",
      "Epoch 0 step 70: training loss: 41934.09061717037\n",
      "Epoch 0 step 71: training accuarcy: 0.7256\n",
      "Epoch 0 step 71: training loss: 41292.09739420544\n",
      "Epoch 0 step 72: training accuarcy: 0.7315\n",
      "Epoch 0 step 72: training loss: 40870.481203908814\n",
      "Epoch 0 step 73: training accuarcy: 0.7305\n",
      "Epoch 0 step 73: training loss: 41161.012993446246\n",
      "Epoch 0 step 74: training accuarcy: 0.7293000000000001\n",
      "Epoch 0 step 74: training loss: 39507.157358965494\n",
      "Epoch 0 step 75: training accuarcy: 0.7423000000000001\n",
      "Epoch 0 step 75: training loss: 40427.393459666106\n",
      "Epoch 0 step 76: training accuarcy: 0.7366\n",
      "Epoch 0 step 76: training loss: 39375.65041998311\n",
      "Epoch 0 step 77: training accuarcy: 0.7422000000000001\n",
      "Epoch 0 step 77: training loss: 39678.26832467686\n",
      "Epoch 0 step 78: training accuarcy: 0.7461\n",
      "Epoch 0 step 78: training loss: 39589.59979600123\n",
      "Epoch 0 step 79: training accuarcy: 0.7381000000000001\n",
      "Epoch 0 step 79: training loss: 37650.88392499117\n",
      "Epoch 0 step 80: training accuarcy: 0.7604000000000001\n",
      "Epoch 0 step 80: training loss: 37870.03841772117\n",
      "Epoch 0 step 81: training accuarcy: 0.7557\n",
      "Epoch 0 step 81: training loss: 37779.345607288196\n",
      "Epoch 0 step 82: training accuarcy: 0.7622\n",
      "Epoch 0 step 82: training loss: 38863.920313287315\n",
      "Epoch 0 step 83: training accuarcy: 0.75\n",
      "Epoch 0 step 83: training loss: 36965.35208052884\n",
      "Epoch 0 step 84: training accuarcy: 0.7696000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 84: training loss: 37021.59833738739\n",
      "Epoch 0 step 85: training accuarcy: 0.7614000000000001\n",
      "Epoch 0 step 85: training loss: 37115.563093654804\n",
      "Epoch 0 step 86: training accuarcy: 0.7664000000000001\n",
      "Epoch 0 step 86: training loss: 36621.16092949527\n",
      "Epoch 0 step 87: training accuarcy: 0.7632\n",
      "Epoch 0 step 87: training loss: 36754.84782784991\n",
      "Epoch 0 step 88: training accuarcy: 0.7665000000000001\n",
      "Epoch 0 step 88: training loss: 37244.81074448023\n",
      "Epoch 0 step 89: training accuarcy: 0.7556\n",
      "Epoch 0 step 89: training loss: 36105.546669099494\n",
      "Epoch 0 step 90: training accuarcy: 0.7735000000000001\n",
      "Epoch 0 step 90: training loss: 36084.91944435393\n",
      "Epoch 0 step 91: training accuarcy: 0.7739\n",
      "Epoch 0 step 91: training loss: 35836.71114717996\n",
      "Epoch 0 step 92: training accuarcy: 0.7769\n",
      "Epoch 0 step 92: training loss: 35005.81801922584\n",
      "Epoch 0 step 93: training accuarcy: 0.7817000000000001\n",
      "Epoch 0 step 93: training loss: 35921.09514532122\n",
      "Epoch 0 step 94: training accuarcy: 0.7675000000000001\n",
      "Epoch 0 step 94: training loss: 34892.377483794815\n",
      "Epoch 0 step 95: training accuarcy: 0.7837000000000001\n",
      "Epoch 0 step 95: training loss: 35190.348962633565\n",
      "Epoch 0 step 96: training accuarcy: 0.7763\n",
      "Epoch 0 step 96: training loss: 34664.61664207566\n",
      "Epoch 0 step 97: training accuarcy: 0.7818\n",
      "Epoch 0 step 97: training loss: 34298.18086458065\n",
      "Epoch 0 step 98: training accuarcy: 0.7838\n",
      "Epoch 0 step 98: training loss: 34591.7176541692\n",
      "Epoch 0 step 99: training accuarcy: 0.7801\n",
      "Epoch 0 step 99: training loss: 34534.19563403107\n",
      "Epoch 0 step 100: training accuarcy: 0.7839\n",
      "Epoch 0 step 100: training loss: 33151.52792662458\n",
      "Epoch 0 step 101: training accuarcy: 0.8033\n",
      "Epoch 0 step 101: training loss: 33388.38377388371\n",
      "Epoch 0 step 102: training accuarcy: 0.7957000000000001\n",
      "Epoch 0 step 102: training loss: 33797.416059046525\n",
      "Epoch 0 step 103: training accuarcy: 0.7909\n",
      "Epoch 0 step 103: training loss: 33249.53598475779\n",
      "Epoch 0 step 104: training accuarcy: 0.7961\n",
      "Epoch 0 step 104: training loss: 33337.51388175108\n",
      "Epoch 0 step 105: training accuarcy: 0.7925000000000001\n",
      "Epoch 0 step 105: training loss: 32922.2997865511\n",
      "Epoch 0 step 106: training accuarcy: 0.7943\n",
      "Epoch 0 step 106: training loss: 32127.37501537402\n",
      "Epoch 0 step 107: training accuarcy: 0.8067000000000001\n",
      "Epoch 0 step 107: training loss: 32324.64933183186\n",
      "Epoch 0 step 108: training accuarcy: 0.801\n",
      "Epoch 0 step 108: training loss: 32514.08550298521\n",
      "Epoch 0 step 109: training accuarcy: 0.7992\n",
      "Epoch 0 step 109: training loss: 31827.207208727275\n",
      "Epoch 0 step 110: training accuarcy: 0.8102\n",
      "Epoch 0 step 110: training loss: 31710.45452926231\n",
      "Epoch 0 step 111: training accuarcy: 0.8119000000000001\n",
      "Epoch 0 step 111: training loss: 31265.795152087623\n",
      "Epoch 0 step 112: training accuarcy: 0.8099000000000001\n",
      "Epoch 0 step 112: training loss: 32409.245731691808\n",
      "Epoch 0 step 113: training accuarcy: 0.795\n",
      "Epoch 0 step 113: training loss: 31399.44816112424\n",
      "Epoch 0 step 114: training accuarcy: 0.8162\n",
      "Epoch 0 step 114: training loss: 31354.407797106018\n",
      "Epoch 0 step 115: training accuarcy: 0.8161\n",
      "Epoch 0 step 115: training loss: 31112.195974522532\n",
      "Epoch 0 step 116: training accuarcy: 0.8115\n",
      "Epoch 0 step 116: training loss: 31272.424486263175\n",
      "Epoch 0 step 117: training accuarcy: 0.8088000000000001\n",
      "Epoch 0 step 117: training loss: 31055.09677289822\n",
      "Epoch 0 step 118: training accuarcy: 0.8072\n",
      "Epoch 0 step 118: training loss: 31428.048987459348\n",
      "Epoch 0 step 119: training accuarcy: 0.8088000000000001\n",
      "Epoch 0 step 119: training loss: 29901.96742354043\n",
      "Epoch 0 step 120: training accuarcy: 0.8194\n",
      "Epoch 0 step 120: training loss: 30637.25364083889\n",
      "Epoch 0 step 121: training accuarcy: 0.8243\n",
      "Epoch 0 step 121: training loss: 31337.58632480281\n",
      "Epoch 0 step 122: training accuarcy: 0.809\n",
      "Epoch 0 step 122: training loss: 30511.181612044464\n",
      "Epoch 0 step 123: training accuarcy: 0.8118000000000001\n",
      "Epoch 0 step 123: training loss: 30073.62021305813\n",
      "Epoch 0 step 124: training accuarcy: 0.8191\n",
      "Epoch 0 step 124: training loss: 30474.023257955487\n",
      "Epoch 0 step 125: training accuarcy: 0.8174\n",
      "Epoch 0 step 125: training loss: 30253.34197182329\n",
      "Epoch 0 step 126: training accuarcy: 0.8139000000000001\n",
      "Epoch 0 step 126: training loss: 30453.10745342641\n",
      "Epoch 0 step 127: training accuarcy: 0.8126\n",
      "Epoch 0 step 127: training loss: 29670.682457874944\n",
      "Epoch 0 step 128: training accuarcy: 0.8262\n",
      "Epoch 0 step 128: training loss: 30058.955638652882\n",
      "Epoch 0 step 129: training accuarcy: 0.8199000000000001\n",
      "Epoch 0 step 129: training loss: 29530.763274516754\n",
      "Epoch 0 step 130: training accuarcy: 0.8242\n",
      "Epoch 0 step 130: training loss: 29846.56711774723\n",
      "Epoch 0 step 131: training accuarcy: 0.8224\n",
      "Epoch 0 step 131: training loss: 29862.0137429422\n",
      "Epoch 0 step 132: training accuarcy: 0.8201\n",
      "Epoch 0 step 132: training loss: 28700.00492254748\n",
      "Epoch 0 step 133: training accuarcy: 0.8299000000000001\n",
      "Epoch 0 step 133: training loss: 29459.01618064181\n",
      "Epoch 0 step 134: training accuarcy: 0.8244\n",
      "Epoch 0 step 134: training loss: 29658.303617347534\n",
      "Epoch 0 step 135: training accuarcy: 0.8188000000000001\n",
      "Epoch 0 step 135: training loss: 28455.247530921413\n",
      "Epoch 0 step 136: training accuarcy: 0.8335\n",
      "Epoch 0 step 136: training loss: 28522.491304026018\n",
      "Epoch 0 step 137: training accuarcy: 0.8313\n",
      "Epoch 0 step 137: training loss: 28708.497263467645\n",
      "Epoch 0 step 138: training accuarcy: 0.8286\n",
      "Epoch 0 step 138: training loss: 28097.860681181926\n",
      "Epoch 0 step 139: training accuarcy: 0.8389000000000001\n",
      "Epoch 0 step 139: training loss: 28396.847991233568\n",
      "Epoch 0 step 140: training accuarcy: 0.8336\n",
      "Epoch 0 step 140: training loss: 28751.91912437087\n",
      "Epoch 0 step 141: training accuarcy: 0.8268000000000001\n",
      "Epoch 0 step 141: training loss: 27912.031003635508\n",
      "Epoch 0 step 142: training accuarcy: 0.8307\n",
      "Epoch 0 step 142: training loss: 28178.212753350235\n",
      "Epoch 0 step 143: training accuarcy: 0.8322\n",
      "Epoch 0 step 143: training loss: 28750.955910971898\n",
      "Epoch 0 step 144: training accuarcy: 0.8257\n",
      "Epoch 0 step 144: training loss: 28163.9598742466\n",
      "Epoch 0 step 145: training accuarcy: 0.8299000000000001\n",
      "Epoch 0 step 145: training loss: 27588.8401365808\n",
      "Epoch 0 step 146: training accuarcy: 0.8378\n",
      "Epoch 0 step 146: training loss: 28147.975560998355\n",
      "Epoch 0 step 147: training accuarcy: 0.8343\n",
      "Epoch 0 step 147: training loss: 27353.197831402475\n",
      "Epoch 0 step 148: training accuarcy: 0.8379000000000001\n",
      "Epoch 0 step 148: training loss: 28005.565622493872\n",
      "Epoch 0 step 149: training accuarcy: 0.8276\n",
      "Epoch 0 step 149: training loss: 27938.729398723735\n",
      "Epoch 0 step 150: training accuarcy: 0.8321000000000001\n",
      "Epoch 0 step 150: training loss: 27896.478546006547\n",
      "Epoch 0 step 151: training accuarcy: 0.8348\n",
      "Epoch 0 step 151: training loss: 27073.890614802913\n",
      "Epoch 0 step 152: training accuarcy: 0.8397\n",
      "Epoch 0 step 152: training loss: 27520.91852783539\n",
      "Epoch 0 step 153: training accuarcy: 0.8382000000000001\n",
      "Epoch 0 step 153: training loss: 26215.292882835467\n",
      "Epoch 0 step 154: training accuarcy: 0.8484\n",
      "Epoch 0 step 154: training loss: 26959.230122253437\n",
      "Epoch 0 step 155: training accuarcy: 0.8417\n",
      "Epoch 0 step 155: training loss: 26907.342279097604\n",
      "Epoch 0 step 156: training accuarcy: 0.8424\n",
      "Epoch 0 step 156: training loss: 27381.96165011591\n",
      "Epoch 0 step 157: training accuarcy: 0.8365\n",
      "Epoch 0 step 157: training loss: 26837.13106412734\n",
      "Epoch 0 step 158: training accuarcy: 0.8413\n",
      "Epoch 0 step 158: training loss: 26963.201004288458\n",
      "Epoch 0 step 159: training accuarcy: 0.8398\n",
      "Epoch 0 step 159: training loss: 26539.806750381413\n",
      "Epoch 0 step 160: training accuarcy: 0.8504\n",
      "Epoch 0 step 160: training loss: 27185.16556744732\n",
      "Epoch 0 step 161: training accuarcy: 0.8370000000000001\n",
      "Epoch 0 step 161: training loss: 26409.383840583996\n",
      "Epoch 0 step 162: training accuarcy: 0.8481000000000001\n",
      "Epoch 0 step 162: training loss: 26109.963129431428\n",
      "Epoch 0 step 163: training accuarcy: 0.8503000000000001\n",
      "Epoch 0 step 163: training loss: 26397.481592604876\n",
      "Epoch 0 step 164: training accuarcy: 0.8462000000000001\n",
      "Epoch 0 step 164: training loss: 26132.0445493035\n",
      "Epoch 0 step 165: training accuarcy: 0.8439000000000001\n",
      "Epoch 0 step 165: training loss: 26329.936642214634\n",
      "Epoch 0 step 166: training accuarcy: 0.8415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 166: training loss: 26601.093323936373\n",
      "Epoch 0 step 167: training accuarcy: 0.8469\n",
      "Epoch 0 step 167: training loss: 26099.55135480284\n",
      "Epoch 0 step 168: training accuarcy: 0.8402000000000001\n",
      "Epoch 0 step 168: training loss: 25822.06086071969\n",
      "Epoch 0 step 169: training accuarcy: 0.8493\n",
      "Epoch 0 step 169: training loss: 26762.53642552209\n",
      "Epoch 0 step 170: training accuarcy: 0.8391000000000001\n",
      "Epoch 0 step 170: training loss: 25803.678595341225\n",
      "Epoch 0 step 171: training accuarcy: 0.8464\n",
      "Epoch 0 step 171: training loss: 25582.729381492667\n",
      "Epoch 0 step 172: training accuarcy: 0.8496\n",
      "Epoch 0 step 172: training loss: 26239.21400536701\n",
      "Epoch 0 step 173: training accuarcy: 0.8427\n",
      "Epoch 0 step 173: training loss: 25087.77036797405\n",
      "Epoch 0 step 174: training accuarcy: 0.8572000000000001\n",
      "Epoch 0 step 174: training loss: 25512.108946995184\n",
      "Epoch 0 step 175: training accuarcy: 0.8507\n",
      "Epoch 0 step 175: training loss: 25481.058037329185\n",
      "Epoch 0 step 176: training accuarcy: 0.8503000000000001\n",
      "Epoch 0 step 176: training loss: 25035.6313160098\n",
      "Epoch 0 step 177: training accuarcy: 0.8539\n",
      "Epoch 0 step 177: training loss: 24389.98885709177\n",
      "Epoch 0 step 178: training accuarcy: 0.8582000000000001\n",
      "Epoch 0 step 178: training loss: 25113.025152775175\n",
      "Epoch 0 step 179: training accuarcy: 0.8503000000000001\n",
      "Epoch 0 step 179: training loss: 24727.849229329193\n",
      "Epoch 0 step 180: training accuarcy: 0.8572000000000001\n",
      "Epoch 0 step 180: training loss: 25077.575902157798\n",
      "Epoch 0 step 181: training accuarcy: 0.8509\n",
      "Epoch 0 step 181: training loss: 24725.919028914686\n",
      "Epoch 0 step 182: training accuarcy: 0.8529\n",
      "Epoch 0 step 182: training loss: 24931.83979289495\n",
      "Epoch 0 step 183: training accuarcy: 0.8504\n",
      "Epoch 0 step 183: training loss: 24934.429690475044\n",
      "Epoch 0 step 184: training accuarcy: 0.8574\n",
      "Epoch 0 step 184: training loss: 25167.139641304144\n",
      "Epoch 0 step 185: training accuarcy: 0.8513000000000001\n",
      "Epoch 0 step 185: training loss: 25037.454286393782\n",
      "Epoch 0 step 186: training accuarcy: 0.8473\n",
      "Epoch 0 step 186: training loss: 24369.23054396917\n",
      "Epoch 0 step 187: training accuarcy: 0.8594\n",
      "Epoch 0 step 187: training loss: 23970.81297475435\n",
      "Epoch 0 step 188: training accuarcy: 0.8654000000000001\n",
      "Epoch 0 step 188: training loss: 24691.177080681682\n",
      "Epoch 0 step 189: training accuarcy: 0.8505\n",
      "Epoch 0 step 189: training loss: 24527.129552071405\n",
      "Epoch 0 step 190: training accuarcy: 0.8528\n",
      "Epoch 0 step 190: training loss: 23814.340829846213\n",
      "Epoch 0 step 191: training accuarcy: 0.8632000000000001\n",
      "Epoch 0 step 191: training loss: 24340.968901255917\n",
      "Epoch 0 step 192: training accuarcy: 0.8543000000000001\n",
      "Epoch 0 step 192: training loss: 23986.72111691603\n",
      "Epoch 0 step 193: training accuarcy: 0.8598\n",
      "Epoch 0 step 193: training loss: 24440.873944381852\n",
      "Epoch 0 step 194: training accuarcy: 0.8557\n",
      "Epoch 0 step 194: training loss: 24089.20448901858\n",
      "Epoch 0 step 195: training accuarcy: 0.8566\n",
      "Epoch 0 step 195: training loss: 24001.374953513652\n",
      "Epoch 0 step 196: training accuarcy: 0.8584\n",
      "Epoch 0 step 196: training loss: 24226.372424903995\n",
      "Epoch 0 step 197: training accuarcy: 0.8588\n",
      "Epoch 0 step 197: training loss: 23382.13720202564\n",
      "Epoch 0 step 198: training accuarcy: 0.8662000000000001\n",
      "Epoch 0 step 198: training loss: 24021.78489632673\n",
      "Epoch 0 step 199: training accuarcy: 0.8501000000000001\n",
      "Epoch 0 step 199: training loss: 23942.690122445365\n",
      "Epoch 0 step 200: training accuarcy: 0.8582000000000001\n",
      "Epoch 0 step 200: training loss: 23781.370632143677\n",
      "Epoch 0 step 201: training accuarcy: 0.8592000000000001\n",
      "Epoch 0 step 201: training loss: 24336.552916998953\n",
      "Epoch 0 step 202: training accuarcy: 0.8518\n",
      "Epoch 0 step 202: training loss: 23749.726943766236\n",
      "Epoch 0 step 203: training accuarcy: 0.8560000000000001\n",
      "Epoch 0 step 203: training loss: 22917.162674961408\n",
      "Epoch 0 step 204: training accuarcy: 0.8688\n",
      "Epoch 0 step 204: training loss: 23534.263480628277\n",
      "Epoch 0 step 205: training accuarcy: 0.8648\n",
      "Epoch 0 step 205: training loss: 23540.825699690056\n",
      "Epoch 0 step 206: training accuarcy: 0.8652000000000001\n",
      "Epoch 0 step 206: training loss: 23506.41150224444\n",
      "Epoch 0 step 207: training accuarcy: 0.8608\n",
      "Epoch 0 step 207: training loss: 23640.60893281858\n",
      "Epoch 0 step 208: training accuarcy: 0.863\n",
      "Epoch 0 step 208: training loss: 23603.960537486266\n",
      "Epoch 0 step 209: training accuarcy: 0.8663000000000001\n",
      "Epoch 0 step 209: training loss: 22809.899445858588\n",
      "Epoch 0 step 210: training accuarcy: 0.87\n",
      "Epoch 0 step 210: training loss: 23027.86702219095\n",
      "Epoch 0 step 211: training accuarcy: 0.8665\n",
      "Epoch 0 step 211: training loss: 22971.681975517364\n",
      "Epoch 0 step 212: training accuarcy: 0.8662000000000001\n",
      "Epoch 0 step 212: training loss: 23078.624569365402\n",
      "Epoch 0 step 213: training accuarcy: 0.8639\n",
      "Epoch 0 step 213: training loss: 23023.127176983264\n",
      "Epoch 0 step 214: training accuarcy: 0.8605\n",
      "Epoch 0 step 214: training loss: 22763.474052748537\n",
      "Epoch 0 step 215: training accuarcy: 0.8607\n",
      "Epoch 0 step 215: training loss: 23371.348405250923\n",
      "Epoch 0 step 216: training accuarcy: 0.8583000000000001\n",
      "Epoch 0 step 216: training loss: 23011.5653594249\n",
      "Epoch 0 step 217: training accuarcy: 0.8651000000000001\n",
      "Epoch 0 step 217: training loss: 22875.152046288997\n",
      "Epoch 0 step 218: training accuarcy: 0.8605\n",
      "Epoch 0 step 218: training loss: 22224.847322567955\n",
      "Epoch 0 step 219: training accuarcy: 0.8723000000000001\n",
      "Epoch 0 step 219: training loss: 22809.32773700078\n",
      "Epoch 0 step 220: training accuarcy: 0.8689\n",
      "Epoch 0 step 220: training loss: 22487.3502713198\n",
      "Epoch 0 step 221: training accuarcy: 0.865\n",
      "Epoch 0 step 221: training loss: 22062.03598915726\n",
      "Epoch 0 step 222: training accuarcy: 0.8724000000000001\n",
      "Epoch 0 step 222: training loss: 22020.420569299386\n",
      "Epoch 0 step 223: training accuarcy: 0.8706\n",
      "Epoch 0 step 223: training loss: 22186.00841821736\n",
      "Epoch 0 step 224: training accuarcy: 0.8714000000000001\n",
      "Epoch 0 step 224: training loss: 22459.86075033503\n",
      "Epoch 0 step 225: training accuarcy: 0.8716\n",
      "Epoch 0 step 225: training loss: 21734.630857384545\n",
      "Epoch 0 step 226: training accuarcy: 0.8712000000000001\n",
      "Epoch 0 step 226: training loss: 21928.93961678992\n",
      "Epoch 0 step 227: training accuarcy: 0.8733000000000001\n",
      "Epoch 0 step 227: training loss: 21937.894764508575\n",
      "Epoch 0 step 228: training accuarcy: 0.8758\n",
      "Epoch 0 step 228: training loss: 21305.5507471787\n",
      "Epoch 0 step 229: training accuarcy: 0.8775000000000001\n",
      "Epoch 0 step 229: training loss: 22432.49028313433\n",
      "Epoch 0 step 230: training accuarcy: 0.8674000000000001\n",
      "Epoch 0 step 230: training loss: 21781.22997723048\n",
      "Epoch 0 step 231: training accuarcy: 0.8709\n",
      "Epoch 0 step 231: training loss: 22689.606523235692\n",
      "Epoch 0 step 232: training accuarcy: 0.8641000000000001\n",
      "Epoch 0 step 232: training loss: 21412.4736308015\n",
      "Epoch 0 step 233: training accuarcy: 0.8753000000000001\n",
      "Epoch 0 step 233: training loss: 21855.86009807379\n",
      "Epoch 0 step 234: training accuarcy: 0.8727\n",
      "Epoch 0 step 234: training loss: 21341.81851962594\n",
      "Epoch 0 step 235: training accuarcy: 0.8769\n",
      "Epoch 0 step 235: training loss: 21328.439680003994\n",
      "Epoch 0 step 236: training accuarcy: 0.8742000000000001\n",
      "Epoch 0 step 236: training loss: 21212.01191353881\n",
      "Epoch 0 step 237: training accuarcy: 0.8779\n",
      "Epoch 0 step 237: training loss: 21681.953347736846\n",
      "Epoch 0 step 238: training accuarcy: 0.8664000000000001\n",
      "Epoch 0 step 238: training loss: 21195.0275524499\n",
      "Epoch 0 step 239: training accuarcy: 0.8778\n",
      "Epoch 0 step 239: training loss: 21505.498013554097\n",
      "Epoch 0 step 240: training accuarcy: 0.8718\n",
      "Epoch 0 step 240: training loss: 21454.793613797327\n",
      "Epoch 0 step 241: training accuarcy: 0.8744000000000001\n",
      "Epoch 0 step 241: training loss: 20871.59162369756\n",
      "Epoch 0 step 242: training accuarcy: 0.8751\n",
      "Epoch 0 step 242: training loss: 20770.768836649393\n",
      "Epoch 0 step 243: training accuarcy: 0.8806\n",
      "Epoch 0 step 243: training loss: 21891.29704530297\n",
      "Epoch 0 step 244: training accuarcy: 0.8692000000000001\n",
      "Epoch 0 step 244: training loss: 20835.111244900236\n",
      "Epoch 0 step 245: training accuarcy: 0.8845000000000001\n",
      "Epoch 0 step 245: training loss: 21238.50851981523\n",
      "Epoch 0 step 246: training accuarcy: 0.8712000000000001\n",
      "Epoch 0 step 246: training loss: 21264.08652601624\n",
      "Epoch 0 step 247: training accuarcy: 0.8746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 247: training loss: 21181.59605143062\n",
      "Epoch 0 step 248: training accuarcy: 0.8744000000000001\n",
      "Epoch 0 step 248: training loss: 20717.414362144315\n",
      "Epoch 0 step 249: training accuarcy: 0.8818\n",
      "Epoch 0 step 249: training loss: 21020.817223240316\n",
      "Epoch 0 step 250: training accuarcy: 0.8737\n",
      "Epoch 0 step 250: training loss: 20807.829010201334\n",
      "Epoch 0 step 251: training accuarcy: 0.8784000000000001\n",
      "Epoch 0 step 251: training loss: 20442.383027732518\n",
      "Epoch 0 step 252: training accuarcy: 0.8804000000000001\n",
      "Epoch 0 step 252: training loss: 20480.054775489694\n",
      "Epoch 0 step 253: training accuarcy: 0.8854000000000001\n",
      "Epoch 0 step 253: training loss: 20596.99193253302\n",
      "Epoch 0 step 254: training accuarcy: 0.8804000000000001\n",
      "Epoch 0 step 254: training loss: 20183.7893707749\n",
      "Epoch 0 step 255: training accuarcy: 0.8834000000000001\n",
      "Epoch 0 step 255: training loss: 20341.919353814716\n",
      "Epoch 0 step 256: training accuarcy: 0.8811\n",
      "Epoch 0 step 256: training loss: 20605.72333395158\n",
      "Epoch 0 step 257: training accuarcy: 0.8785000000000001\n",
      "Epoch 0 step 257: training loss: 20213.784426142403\n",
      "Epoch 0 step 258: training accuarcy: 0.8784000000000001\n",
      "Epoch 0 step 258: training loss: 21077.46703227536\n",
      "Epoch 0 step 259: training accuarcy: 0.8711000000000001\n",
      "Epoch 0 step 259: training loss: 20593.85218557363\n",
      "Epoch 0 step 260: training accuarcy: 0.8785000000000001\n",
      "Epoch 0 step 260: training loss: 20224.313027959328\n",
      "Epoch 0 step 261: training accuarcy: 0.8791\n",
      "Epoch 0 step 261: training loss: 20271.92135110731\n",
      "Epoch 0 step 262: training accuarcy: 0.8807\n",
      "Epoch 0 step 262: training loss: 11475.218761201035\n",
      "Epoch 0 step 263: training accuarcy: 0.8812820512820513\n",
      "Epoch 0: train loss 55458.42443780722, train accuarcy 0.727400004863739\n",
      "Epoch 0: valid loss 22225.606414519825, valid accuarcy 0.8519107103347778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [05:16<10:32, 316.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1 step 263: training loss: 18654.763376803552\n",
      "Epoch 1 step 264: training accuarcy: 0.904\n",
      "Epoch 1 step 264: training loss: 19169.8509397557\n",
      "Epoch 1 step 265: training accuarcy: 0.8995000000000001\n",
      "Epoch 1 step 265: training loss: 18744.404241145487\n",
      "Epoch 1 step 266: training accuarcy: 0.9029\n",
      "Epoch 1 step 266: training loss: 19100.47757906397\n",
      "Epoch 1 step 267: training accuarcy: 0.8979\n",
      "Epoch 1 step 267: training loss: 19022.88913226355\n",
      "Epoch 1 step 268: training accuarcy: 0.8978\n",
      "Epoch 1 step 268: training loss: 18556.884732716415\n",
      "Epoch 1 step 269: training accuarcy: 0.9047000000000001\n",
      "Epoch 1 step 269: training loss: 18892.98198253762\n",
      "Epoch 1 step 270: training accuarcy: 0.9004000000000001\n",
      "Epoch 1 step 270: training loss: 18943.752710191136\n",
      "Epoch 1 step 271: training accuarcy: 0.8983000000000001\n",
      "Epoch 1 step 271: training loss: 18532.35758176867\n",
      "Epoch 1 step 272: training accuarcy: 0.8993\n",
      "Epoch 1 step 272: training loss: 18644.70328773265\n",
      "Epoch 1 step 273: training accuarcy: 0.9009\n",
      "Epoch 1 step 273: training loss: 18677.632361417796\n",
      "Epoch 1 step 274: training accuarcy: 0.8979\n",
      "Epoch 1 step 274: training loss: 18574.59249575129\n",
      "Epoch 1 step 275: training accuarcy: 0.8997\n",
      "Epoch 1 step 275: training loss: 18423.709282840788\n",
      "Epoch 1 step 276: training accuarcy: 0.9009\n",
      "Epoch 1 step 276: training loss: 18242.937535876736\n",
      "Epoch 1 step 277: training accuarcy: 0.9023\n",
      "Epoch 1 step 277: training loss: 18307.70177116529\n",
      "Epoch 1 step 278: training accuarcy: 0.9018\n",
      "Epoch 1 step 278: training loss: 18638.60871057914\n",
      "Epoch 1 step 279: training accuarcy: 0.8988\n",
      "Epoch 1 step 279: training loss: 18646.505172373872\n",
      "Epoch 1 step 280: training accuarcy: 0.8987\n",
      "Epoch 1 step 280: training loss: 17913.37958253085\n",
      "Epoch 1 step 281: training accuarcy: 0.9073\n",
      "Epoch 1 step 281: training loss: 18900.287417506366\n",
      "Epoch 1 step 282: training accuarcy: 0.8911\n",
      "Epoch 1 step 282: training loss: 17865.202987801415\n",
      "Epoch 1 step 283: training accuarcy: 0.9058\n",
      "Epoch 1 step 283: training loss: 18485.584848478586\n",
      "Epoch 1 step 284: training accuarcy: 0.9004000000000001\n",
      "Epoch 1 step 284: training loss: 18144.60235281142\n",
      "Epoch 1 step 285: training accuarcy: 0.8997\n",
      "Epoch 1 step 285: training loss: 18090.609739649786\n",
      "Epoch 1 step 286: training accuarcy: 0.9029\n",
      "Epoch 1 step 286: training loss: 18291.29389960708\n",
      "Epoch 1 step 287: training accuarcy: 0.9004000000000001\n",
      "Epoch 1 step 287: training loss: 18398.17613822346\n",
      "Epoch 1 step 288: training accuarcy: 0.8966000000000001\n",
      "Epoch 1 step 288: training loss: 17956.66713683578\n",
      "Epoch 1 step 289: training accuarcy: 0.9013\n",
      "Epoch 1 step 289: training loss: 18238.686505694277\n",
      "Epoch 1 step 290: training accuarcy: 0.9027000000000001\n",
      "Epoch 1 step 290: training loss: 18635.253282116188\n",
      "Epoch 1 step 291: training accuarcy: 0.898\n",
      "Epoch 1 step 291: training loss: 18776.24876540261\n",
      "Epoch 1 step 292: training accuarcy: 0.8926000000000001\n",
      "Epoch 1 step 292: training loss: 18265.80934297526\n",
      "Epoch 1 step 293: training accuarcy: 0.8956000000000001\n",
      "Epoch 1 step 293: training loss: 18301.508553336942\n",
      "Epoch 1 step 294: training accuarcy: 0.8991\n",
      "Epoch 1 step 294: training loss: 17856.824603350175\n",
      "Epoch 1 step 295: training accuarcy: 0.9023\n",
      "Epoch 1 step 295: training loss: 17836.442881884635\n",
      "Epoch 1 step 296: training accuarcy: 0.8997\n",
      "Epoch 1 step 296: training loss: 17914.887705530964\n",
      "Epoch 1 step 297: training accuarcy: 0.9026000000000001\n",
      "Epoch 1 step 297: training loss: 18117.32244103771\n",
      "Epoch 1 step 298: training accuarcy: 0.8987\n",
      "Epoch 1 step 298: training loss: 17909.585636634332\n",
      "Epoch 1 step 299: training accuarcy: 0.9029\n",
      "Epoch 1 step 299: training loss: 17655.63218251622\n",
      "Epoch 1 step 300: training accuarcy: 0.9026000000000001\n",
      "Epoch 1 step 300: training loss: 17507.144420761902\n",
      "Epoch 1 step 301: training accuarcy: 0.9066000000000001\n",
      "Epoch 1 step 301: training loss: 17836.40267473429\n",
      "Epoch 1 step 302: training accuarcy: 0.902\n",
      "Epoch 1 step 302: training loss: 17598.208927028205\n",
      "Epoch 1 step 303: training accuarcy: 0.9033\n",
      "Epoch 1 step 303: training loss: 18532.887774053153\n",
      "Epoch 1 step 304: training accuarcy: 0.8959\n",
      "Epoch 1 step 304: training loss: 17842.93223962416\n",
      "Epoch 1 step 305: training accuarcy: 0.9026000000000001\n",
      "Epoch 1 step 305: training loss: 17649.426156860285\n",
      "Epoch 1 step 306: training accuarcy: 0.9043\n",
      "Epoch 1 step 306: training loss: 17630.297496664876\n",
      "Epoch 1 step 307: training accuarcy: 0.903\n",
      "Epoch 1 step 307: training loss: 17844.997178894115\n",
      "Epoch 1 step 308: training accuarcy: 0.9005000000000001\n",
      "Epoch 1 step 308: training loss: 17498.114594921342\n",
      "Epoch 1 step 309: training accuarcy: 0.9002\n",
      "Epoch 1 step 309: training loss: 18092.614477225492\n",
      "Epoch 1 step 310: training accuarcy: 0.8988\n",
      "Epoch 1 step 310: training loss: 17712.074182985598\n",
      "Epoch 1 step 311: training accuarcy: 0.9019\n",
      "Epoch 1 step 311: training loss: 17642.56195066231\n",
      "Epoch 1 step 312: training accuarcy: 0.9024000000000001\n",
      "Epoch 1 step 312: training loss: 17588.767272965168\n",
      "Epoch 1 step 313: training accuarcy: 0.8958\n",
      "Epoch 1 step 313: training loss: 17817.414644271666\n",
      "Epoch 1 step 314: training accuarcy: 0.8949\n",
      "Epoch 1 step 314: training loss: 17222.014457001074\n",
      "Epoch 1 step 315: training accuarcy: 0.9063\n",
      "Epoch 1 step 315: training loss: 17519.625446477174\n",
      "Epoch 1 step 316: training accuarcy: 0.899\n",
      "Epoch 1 step 316: training loss: 18261.337125606726\n",
      "Epoch 1 step 317: training accuarcy: 0.8954000000000001\n",
      "Epoch 1 step 317: training loss: 17133.61156032388\n",
      "Epoch 1 step 318: training accuarcy: 0.9054000000000001\n",
      "Epoch 1 step 318: training loss: 17620.134193159724\n",
      "Epoch 1 step 319: training accuarcy: 0.8986000000000001\n",
      "Epoch 1 step 319: training loss: 17737.395861257733\n",
      "Epoch 1 step 320: training accuarcy: 0.896\n",
      "Epoch 1 step 320: training loss: 17544.931653181622\n",
      "Epoch 1 step 321: training accuarcy: 0.8979\n",
      "Epoch 1 step 321: training loss: 17106.36859356808\n",
      "Epoch 1 step 322: training accuarcy: 0.9075000000000001\n",
      "Epoch 1 step 322: training loss: 17557.774586511983\n",
      "Epoch 1 step 323: training accuarcy: 0.8987\n",
      "Epoch 1 step 323: training loss: 17684.767447660877\n",
      "Epoch 1 step 324: training accuarcy: 0.8949\n",
      "Epoch 1 step 324: training loss: 17237.337199773407\n",
      "Epoch 1 step 325: training accuarcy: 0.9005000000000001\n",
      "Epoch 1 step 325: training loss: 17476.794641825858\n",
      "Epoch 1 step 326: training accuarcy: 0.8973000000000001\n",
      "Epoch 1 step 326: training loss: 18027.03987244766\n",
      "Epoch 1 step 327: training accuarcy: 0.894\n",
      "Epoch 1 step 327: training loss: 17125.64318146466\n",
      "Epoch 1 step 328: training accuarcy: 0.8966000000000001\n",
      "Epoch 1 step 328: training loss: 17284.561200421194\n",
      "Epoch 1 step 329: training accuarcy: 0.9014000000000001\n",
      "Epoch 1 step 329: training loss: 17849.8235879133\n",
      "Epoch 1 step 330: training accuarcy: 0.8981\n",
      "Epoch 1 step 330: training loss: 17181.364746523745\n",
      "Epoch 1 step 331: training accuarcy: 0.8978\n",
      "Epoch 1 step 331: training loss: 17317.331215900253\n",
      "Epoch 1 step 332: training accuarcy: 0.9009\n",
      "Epoch 1 step 332: training loss: 16843.812938978557\n",
      "Epoch 1 step 333: training accuarcy: 0.9048\n",
      "Epoch 1 step 333: training loss: 16546.886112102686\n",
      "Epoch 1 step 334: training accuarcy: 0.9049\n",
      "Epoch 1 step 334: training loss: 16994.947053573684\n",
      "Epoch 1 step 335: training accuarcy: 0.9025000000000001\n",
      "Epoch 1 step 335: training loss: 17291.913884888956\n",
      "Epoch 1 step 336: training accuarcy: 0.8982\n",
      "Epoch 1 step 336: training loss: 16897.25667152463\n",
      "Epoch 1 step 337: training accuarcy: 0.9055000000000001\n",
      "Epoch 1 step 337: training loss: 17180.19497621621\n",
      "Epoch 1 step 338: training accuarcy: 0.8987\n",
      "Epoch 1 step 338: training loss: 16914.788713489706\n",
      "Epoch 1 step 339: training accuarcy: 0.9056000000000001\n",
      "Epoch 1 step 339: training loss: 17278.779427104382\n",
      "Epoch 1 step 340: training accuarcy: 0.8993\n",
      "Epoch 1 step 340: training loss: 17109.246325724223\n",
      "Epoch 1 step 341: training accuarcy: 0.8999\n",
      "Epoch 1 step 341: training loss: 17046.148803613793\n",
      "Epoch 1 step 342: training accuarcy: 0.9003\n",
      "Epoch 1 step 342: training loss: 16613.243705781773\n",
      "Epoch 1 step 343: training accuarcy: 0.9046000000000001\n",
      "Epoch 1 step 343: training loss: 16470.71060697616\n",
      "Epoch 1 step 344: training accuarcy: 0.9095000000000001\n",
      "Epoch 1 step 344: training loss: 16465.470732414935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 345: training accuarcy: 0.9084000000000001\n",
      "Epoch 1 step 345: training loss: 16800.59675577887\n",
      "Epoch 1 step 346: training accuarcy: 0.9046000000000001\n",
      "Epoch 1 step 346: training loss: 16712.19062631345\n",
      "Epoch 1 step 347: training accuarcy: 0.9003\n",
      "Epoch 1 step 347: training loss: 16906.92779257843\n",
      "Epoch 1 step 348: training accuarcy: 0.9009\n",
      "Epoch 1 step 348: training loss: 16783.713519891895\n",
      "Epoch 1 step 349: training accuarcy: 0.9033\n",
      "Epoch 1 step 349: training loss: 16668.13921562823\n",
      "Epoch 1 step 350: training accuarcy: 0.9041\n",
      "Epoch 1 step 350: training loss: 16668.845680572966\n",
      "Epoch 1 step 351: training accuarcy: 0.9056000000000001\n",
      "Epoch 1 step 351: training loss: 16586.101331534668\n",
      "Epoch 1 step 352: training accuarcy: 0.9042\n",
      "Epoch 1 step 352: training loss: 16773.133050739045\n",
      "Epoch 1 step 353: training accuarcy: 0.9041\n",
      "Epoch 1 step 353: training loss: 16410.085223775175\n",
      "Epoch 1 step 354: training accuarcy: 0.9058\n",
      "Epoch 1 step 354: training loss: 16750.537457018298\n",
      "Epoch 1 step 355: training accuarcy: 0.9044000000000001\n",
      "Epoch 1 step 355: training loss: 16699.982681191043\n",
      "Epoch 1 step 356: training accuarcy: 0.9048\n",
      "Epoch 1 step 356: training loss: 16637.430941136226\n",
      "Epoch 1 step 357: training accuarcy: 0.9008\n",
      "Epoch 1 step 357: training loss: 16145.33835157563\n",
      "Epoch 1 step 358: training accuarcy: 0.9113\n",
      "Epoch 1 step 358: training loss: 16785.283942611353\n",
      "Epoch 1 step 359: training accuarcy: 0.9039\n",
      "Epoch 1 step 359: training loss: 16506.40892266547\n",
      "Epoch 1 step 360: training accuarcy: 0.9021\n",
      "Epoch 1 step 360: training loss: 16621.45373822891\n",
      "Epoch 1 step 361: training accuarcy: 0.9058\n",
      "Epoch 1 step 361: training loss: 16455.017505565535\n",
      "Epoch 1 step 362: training accuarcy: 0.9034000000000001\n",
      "Epoch 1 step 362: training loss: 16738.89367400583\n",
      "Epoch 1 step 363: training accuarcy: 0.8987\n",
      "Epoch 1 step 363: training loss: 16675.38564481992\n",
      "Epoch 1 step 364: training accuarcy: 0.9056000000000001\n",
      "Epoch 1 step 364: training loss: 16547.45779436327\n",
      "Epoch 1 step 365: training accuarcy: 0.9044000000000001\n",
      "Epoch 1 step 365: training loss: 16815.851374368714\n",
      "Epoch 1 step 366: training accuarcy: 0.9007000000000001\n",
      "Epoch 1 step 366: training loss: 16133.85277873015\n",
      "Epoch 1 step 367: training accuarcy: 0.9091\n",
      "Epoch 1 step 367: training loss: 16779.82426089553\n",
      "Epoch 1 step 368: training accuarcy: 0.8979\n",
      "Epoch 1 step 368: training loss: 15996.885559467697\n",
      "Epoch 1 step 369: training accuarcy: 0.9084000000000001\n",
      "Epoch 1 step 369: training loss: 15831.19122596573\n",
      "Epoch 1 step 370: training accuarcy: 0.9129\n",
      "Epoch 1 step 370: training loss: 16325.48645118215\n",
      "Epoch 1 step 371: training accuarcy: 0.9041\n",
      "Epoch 1 step 371: training loss: 16092.80752148877\n",
      "Epoch 1 step 372: training accuarcy: 0.9072\n",
      "Epoch 1 step 372: training loss: 16426.303546476705\n",
      "Epoch 1 step 373: training accuarcy: 0.9028\n",
      "Epoch 1 step 373: training loss: 15853.424372840764\n",
      "Epoch 1 step 374: training accuarcy: 0.9081\n",
      "Epoch 1 step 374: training loss: 16812.41666858533\n",
      "Epoch 1 step 375: training accuarcy: 0.9063\n",
      "Epoch 1 step 375: training loss: 15960.496206972097\n",
      "Epoch 1 step 376: training accuarcy: 0.907\n",
      "Epoch 1 step 376: training loss: 16213.462648075136\n",
      "Epoch 1 step 377: training accuarcy: 0.9044000000000001\n",
      "Epoch 1 step 377: training loss: 16228.209514329781\n",
      "Epoch 1 step 378: training accuarcy: 0.9075000000000001\n",
      "Epoch 1 step 378: training loss: 15907.46898994027\n",
      "Epoch 1 step 379: training accuarcy: 0.9074000000000001\n",
      "Epoch 1 step 379: training loss: 16381.5476734046\n",
      "Epoch 1 step 380: training accuarcy: 0.902\n",
      "Epoch 1 step 380: training loss: 16021.801350124988\n",
      "Epoch 1 step 381: training accuarcy: 0.906\n",
      "Epoch 1 step 381: training loss: 15862.241852203912\n",
      "Epoch 1 step 382: training accuarcy: 0.9079\n",
      "Epoch 1 step 382: training loss: 16136.99647150402\n",
      "Epoch 1 step 383: training accuarcy: 0.9063\n",
      "Epoch 1 step 383: training loss: 16296.838616782756\n",
      "Epoch 1 step 384: training accuarcy: 0.9022\n",
      "Epoch 1 step 384: training loss: 16039.820999621548\n",
      "Epoch 1 step 385: training accuarcy: 0.9047000000000001\n",
      "Epoch 1 step 385: training loss: 16061.367758574344\n",
      "Epoch 1 step 386: training accuarcy: 0.9045000000000001\n",
      "Epoch 1 step 386: training loss: 15806.434387168294\n",
      "Epoch 1 step 387: training accuarcy: 0.9085000000000001\n",
      "Epoch 1 step 387: training loss: 15797.66176702938\n",
      "Epoch 1 step 388: training accuarcy: 0.9071\n",
      "Epoch 1 step 388: training loss: 16290.045615951247\n",
      "Epoch 1 step 389: training accuarcy: 0.9017000000000001\n",
      "Epoch 1 step 389: training loss: 15646.91942696261\n",
      "Epoch 1 step 390: training accuarcy: 0.9092\n",
      "Epoch 1 step 390: training loss: 15887.659747618061\n",
      "Epoch 1 step 391: training accuarcy: 0.9063\n",
      "Epoch 1 step 391: training loss: 15907.66041694404\n",
      "Epoch 1 step 392: training accuarcy: 0.9088\n",
      "Epoch 1 step 392: training loss: 15919.383859215315\n",
      "Epoch 1 step 393: training accuarcy: 0.9046000000000001\n",
      "Epoch 1 step 393: training loss: 15868.249477165173\n",
      "Epoch 1 step 394: training accuarcy: 0.9051\n",
      "Epoch 1 step 394: training loss: 15583.261589249767\n",
      "Epoch 1 step 395: training accuarcy: 0.9119\n",
      "Epoch 1 step 395: training loss: 15437.735586540377\n",
      "Epoch 1 step 396: training accuarcy: 0.9099\n",
      "Epoch 1 step 396: training loss: 15726.088237256201\n",
      "Epoch 1 step 397: training accuarcy: 0.9096000000000001\n",
      "Epoch 1 step 397: training loss: 16215.48926017727\n",
      "Epoch 1 step 398: training accuarcy: 0.9018\n",
      "Epoch 1 step 398: training loss: 16650.842731848494\n",
      "Epoch 1 step 399: training accuarcy: 0.8942\n",
      "Epoch 1 step 399: training loss: 15596.15595897229\n",
      "Epoch 1 step 400: training accuarcy: 0.9059\n",
      "Epoch 1 step 400: training loss: 16513.06269891131\n",
      "Epoch 1 step 401: training accuarcy: 0.897\n",
      "Epoch 1 step 401: training loss: 15887.460564081512\n",
      "Epoch 1 step 402: training accuarcy: 0.9025000000000001\n",
      "Epoch 1 step 402: training loss: 15506.716448139156\n",
      "Epoch 1 step 403: training accuarcy: 0.9113\n",
      "Epoch 1 step 403: training loss: 15851.488676433088\n",
      "Epoch 1 step 404: training accuarcy: 0.9068\n",
      "Epoch 1 step 404: training loss: 15464.243239585283\n",
      "Epoch 1 step 405: training accuarcy: 0.907\n",
      "Epoch 1 step 405: training loss: 16105.794621637348\n",
      "Epoch 1 step 406: training accuarcy: 0.8976000000000001\n",
      "Epoch 1 step 406: training loss: 15511.452259603926\n",
      "Epoch 1 step 407: training accuarcy: 0.9076000000000001\n",
      "Epoch 1 step 407: training loss: 15691.01479837894\n",
      "Epoch 1 step 408: training accuarcy: 0.9068\n",
      "Epoch 1 step 408: training loss: 15629.259527781098\n",
      "Epoch 1 step 409: training accuarcy: 0.9052\n",
      "Epoch 1 step 409: training loss: 15505.074774415009\n",
      "Epoch 1 step 410: training accuarcy: 0.9072\n",
      "Epoch 1 step 410: training loss: 15566.933416191263\n",
      "Epoch 1 step 411: training accuarcy: 0.905\n",
      "Epoch 1 step 411: training loss: 15474.152423549353\n",
      "Epoch 1 step 412: training accuarcy: 0.9085000000000001\n",
      "Epoch 1 step 412: training loss: 15306.438920049824\n",
      "Epoch 1 step 413: training accuarcy: 0.9078\n",
      "Epoch 1 step 413: training loss: 15894.6940698964\n",
      "Epoch 1 step 414: training accuarcy: 0.904\n",
      "Epoch 1 step 414: training loss: 15328.016042264695\n",
      "Epoch 1 step 415: training accuarcy: 0.9099\n",
      "Epoch 1 step 415: training loss: 15186.631004897541\n",
      "Epoch 1 step 416: training accuarcy: 0.9098\n",
      "Epoch 1 step 416: training loss: 15370.280742625722\n",
      "Epoch 1 step 417: training accuarcy: 0.906\n",
      "Epoch 1 step 417: training loss: 15325.56922677136\n",
      "Epoch 1 step 418: training accuarcy: 0.9078\n",
      "Epoch 1 step 418: training loss: 15043.46218620695\n",
      "Epoch 1 step 419: training accuarcy: 0.909\n",
      "Epoch 1 step 419: training loss: 15036.338790384998\n",
      "Epoch 1 step 420: training accuarcy: 0.9091\n",
      "Epoch 1 step 420: training loss: 15801.148862494454\n",
      "Epoch 1 step 421: training accuarcy: 0.9036000000000001\n",
      "Epoch 1 step 421: training loss: 14960.187686612771\n",
      "Epoch 1 step 422: training accuarcy: 0.9134\n",
      "Epoch 1 step 422: training loss: 15194.999939582058\n",
      "Epoch 1 step 423: training accuarcy: 0.9124\n",
      "Epoch 1 step 423: training loss: 14761.99734435216\n",
      "Epoch 1 step 424: training accuarcy: 0.915\n",
      "Epoch 1 step 424: training loss: 15043.732982189023\n",
      "Epoch 1 step 425: training accuarcy: 0.9112\n",
      "Epoch 1 step 425: training loss: 15701.803154234709\n",
      "Epoch 1 step 426: training accuarcy: 0.9005000000000001\n",
      "Epoch 1 step 426: training loss: 15930.605339774967\n",
      "Epoch 1 step 427: training accuarcy: 0.8999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 427: training loss: 15603.772535963177\n",
      "Epoch 1 step 428: training accuarcy: 0.9072\n",
      "Epoch 1 step 428: training loss: 15564.097606553598\n",
      "Epoch 1 step 429: training accuarcy: 0.902\n",
      "Epoch 1 step 429: training loss: 15377.576241857663\n",
      "Epoch 1 step 430: training accuarcy: 0.9052\n",
      "Epoch 1 step 430: training loss: 15162.334503598253\n",
      "Epoch 1 step 431: training accuarcy: 0.9084000000000001\n",
      "Epoch 1 step 431: training loss: 15015.394849332051\n",
      "Epoch 1 step 432: training accuarcy: 0.9085000000000001\n",
      "Epoch 1 step 432: training loss: 15004.000818654247\n",
      "Epoch 1 step 433: training accuarcy: 0.9114000000000001\n",
      "Epoch 1 step 433: training loss: 15325.914791305458\n",
      "Epoch 1 step 434: training accuarcy: 0.9122\n",
      "Epoch 1 step 434: training loss: 14814.798455274773\n",
      "Epoch 1 step 435: training accuarcy: 0.9128000000000001\n",
      "Epoch 1 step 435: training loss: 15250.127153931393\n",
      "Epoch 1 step 436: training accuarcy: 0.9052\n",
      "Epoch 1 step 436: training loss: 14903.909304156427\n",
      "Epoch 1 step 437: training accuarcy: 0.9128000000000001\n",
      "Epoch 1 step 437: training loss: 14933.766370645433\n",
      "Epoch 1 step 438: training accuarcy: 0.907\n",
      "Epoch 1 step 438: training loss: 15298.662995896577\n",
      "Epoch 1 step 439: training accuarcy: 0.9063\n",
      "Epoch 1 step 439: training loss: 15321.813671878273\n",
      "Epoch 1 step 440: training accuarcy: 0.9065000000000001\n",
      "Epoch 1 step 440: training loss: 15191.807997212098\n",
      "Epoch 1 step 441: training accuarcy: 0.9022\n",
      "Epoch 1 step 441: training loss: 14969.261098491894\n",
      "Epoch 1 step 442: training accuarcy: 0.9095000000000001\n",
      "Epoch 1 step 442: training loss: 15571.92399511274\n",
      "Epoch 1 step 443: training accuarcy: 0.9052\n",
      "Epoch 1 step 443: training loss: 14763.906962947036\n",
      "Epoch 1 step 444: training accuarcy: 0.9143\n",
      "Epoch 1 step 444: training loss: 14573.012632122282\n",
      "Epoch 1 step 445: training accuarcy: 0.9142\n",
      "Epoch 1 step 445: training loss: 14672.695941595332\n",
      "Epoch 1 step 446: training accuarcy: 0.912\n",
      "Epoch 1 step 446: training loss: 14696.348030936157\n",
      "Epoch 1 step 447: training accuarcy: 0.9128000000000001\n",
      "Epoch 1 step 447: training loss: 14870.797881579794\n",
      "Epoch 1 step 448: training accuarcy: 0.9113\n",
      "Epoch 1 step 448: training loss: 14653.14855149926\n",
      "Epoch 1 step 449: training accuarcy: 0.9135000000000001\n",
      "Epoch 1 step 449: training loss: 14763.811187784013\n",
      "Epoch 1 step 450: training accuarcy: 0.9119\n",
      "Epoch 1 step 450: training loss: 15123.978590452622\n",
      "Epoch 1 step 451: training accuarcy: 0.9079\n",
      "Epoch 1 step 451: training loss: 14624.991473814332\n",
      "Epoch 1 step 452: training accuarcy: 0.911\n",
      "Epoch 1 step 452: training loss: 14901.976762089133\n",
      "Epoch 1 step 453: training accuarcy: 0.9086000000000001\n",
      "Epoch 1 step 453: training loss: 14738.91328380922\n",
      "Epoch 1 step 454: training accuarcy: 0.9115000000000001\n",
      "Epoch 1 step 454: training loss: 14739.76532654268\n",
      "Epoch 1 step 455: training accuarcy: 0.9106000000000001\n",
      "Epoch 1 step 455: training loss: 14606.887189051151\n",
      "Epoch 1 step 456: training accuarcy: 0.9119\n",
      "Epoch 1 step 456: training loss: 15001.275331173983\n",
      "Epoch 1 step 457: training accuarcy: 0.9046000000000001\n",
      "Epoch 1 step 457: training loss: 14833.296003054505\n",
      "Epoch 1 step 458: training accuarcy: 0.909\n",
      "Epoch 1 step 458: training loss: 14603.044949317677\n",
      "Epoch 1 step 459: training accuarcy: 0.9142\n",
      "Epoch 1 step 459: training loss: 15120.902167616687\n",
      "Epoch 1 step 460: training accuarcy: 0.9078\n",
      "Epoch 1 step 460: training loss: 14862.775451200694\n",
      "Epoch 1 step 461: training accuarcy: 0.9126000000000001\n",
      "Epoch 1 step 461: training loss: 14549.020901749773\n",
      "Epoch 1 step 462: training accuarcy: 0.9094000000000001\n",
      "Epoch 1 step 462: training loss: 14563.11485461105\n",
      "Epoch 1 step 463: training accuarcy: 0.9131\n",
      "Epoch 1 step 463: training loss: 14897.660582075661\n",
      "Epoch 1 step 464: training accuarcy: 0.9064000000000001\n",
      "Epoch 1 step 464: training loss: 14761.41014106105\n",
      "Epoch 1 step 465: training accuarcy: 0.9082\n",
      "Epoch 1 step 465: training loss: 14424.692742259058\n",
      "Epoch 1 step 466: training accuarcy: 0.9165000000000001\n",
      "Epoch 1 step 466: training loss: 14392.811180874643\n",
      "Epoch 1 step 467: training accuarcy: 0.9137000000000001\n",
      "Epoch 1 step 467: training loss: 14864.779471971488\n",
      "Epoch 1 step 468: training accuarcy: 0.9131\n",
      "Epoch 1 step 468: training loss: 14537.269231774213\n",
      "Epoch 1 step 469: training accuarcy: 0.9109\n",
      "Epoch 1 step 469: training loss: 14291.04119216332\n",
      "Epoch 1 step 470: training accuarcy: 0.9152\n",
      "Epoch 1 step 470: training loss: 14448.579808129747\n",
      "Epoch 1 step 471: training accuarcy: 0.9107000000000001\n",
      "Epoch 1 step 471: training loss: 14198.611326596942\n",
      "Epoch 1 step 472: training accuarcy: 0.9153\n",
      "Epoch 1 step 472: training loss: 14665.44269734179\n",
      "Epoch 1 step 473: training accuarcy: 0.9154\n",
      "Epoch 1 step 473: training loss: 14499.251891244146\n",
      "Epoch 1 step 474: training accuarcy: 0.9112\n",
      "Epoch 1 step 474: training loss: 14385.718746315026\n",
      "Epoch 1 step 475: training accuarcy: 0.9151\n",
      "Epoch 1 step 475: training loss: 14127.57828953709\n",
      "Epoch 1 step 476: training accuarcy: 0.9141\n",
      "Epoch 1 step 476: training loss: 13885.20859098227\n",
      "Epoch 1 step 477: training accuarcy: 0.9198000000000001\n",
      "Epoch 1 step 477: training loss: 14232.93722044163\n",
      "Epoch 1 step 478: training accuarcy: 0.9138000000000001\n",
      "Epoch 1 step 478: training loss: 14432.21421566842\n",
      "Epoch 1 step 479: training accuarcy: 0.9156000000000001\n",
      "Epoch 1 step 479: training loss: 13999.458189249859\n",
      "Epoch 1 step 480: training accuarcy: 0.9155000000000001\n",
      "Epoch 1 step 480: training loss: 14200.689659952686\n",
      "Epoch 1 step 481: training accuarcy: 0.9138000000000001\n",
      "Epoch 1 step 481: training loss: 14427.907430891137\n",
      "Epoch 1 step 482: training accuarcy: 0.9109\n",
      "Epoch 1 step 482: training loss: 14339.92702084485\n",
      "Epoch 1 step 483: training accuarcy: 0.9134\n",
      "Epoch 1 step 483: training loss: 14102.379297700678\n",
      "Epoch 1 step 484: training accuarcy: 0.9112\n",
      "Epoch 1 step 484: training loss: 14391.675487095748\n",
      "Epoch 1 step 485: training accuarcy: 0.9152\n",
      "Epoch 1 step 485: training loss: 14372.10150034111\n",
      "Epoch 1 step 486: training accuarcy: 0.9115000000000001\n",
      "Epoch 1 step 486: training loss: 14584.27540789716\n",
      "Epoch 1 step 487: training accuarcy: 0.9075000000000001\n",
      "Epoch 1 step 487: training loss: 14580.8960062066\n",
      "Epoch 1 step 488: training accuarcy: 0.9076000000000001\n",
      "Epoch 1 step 488: training loss: 14016.516429291933\n",
      "Epoch 1 step 489: training accuarcy: 0.9171\n",
      "Epoch 1 step 489: training loss: 13957.20931939118\n",
      "Epoch 1 step 490: training accuarcy: 0.9151\n",
      "Epoch 1 step 490: training loss: 14633.562803851903\n",
      "Epoch 1 step 491: training accuarcy: 0.9049\n",
      "Epoch 1 step 491: training loss: 13827.47171661998\n",
      "Epoch 1 step 492: training accuarcy: 0.9184\n",
      "Epoch 1 step 492: training loss: 14195.342183254872\n",
      "Epoch 1 step 493: training accuarcy: 0.9103\n",
      "Epoch 1 step 493: training loss: 14119.39137463236\n",
      "Epoch 1 step 494: training accuarcy: 0.914\n",
      "Epoch 1 step 494: training loss: 14133.38042817952\n",
      "Epoch 1 step 495: training accuarcy: 0.9197000000000001\n",
      "Epoch 1 step 495: training loss: 14380.259391878557\n",
      "Epoch 1 step 496: training accuarcy: 0.9117000000000001\n",
      "Epoch 1 step 496: training loss: 14169.378285577062\n",
      "Epoch 1 step 497: training accuarcy: 0.9115000000000001\n",
      "Epoch 1 step 497: training loss: 14157.316382864528\n",
      "Epoch 1 step 498: training accuarcy: 0.9154\n",
      "Epoch 1 step 498: training loss: 14341.51829462576\n",
      "Epoch 1 step 499: training accuarcy: 0.9075000000000001\n",
      "Epoch 1 step 499: training loss: 14270.414075849203\n",
      "Epoch 1 step 500: training accuarcy: 0.9138000000000001\n",
      "Epoch 1 step 500: training loss: 14668.741122586747\n",
      "Epoch 1 step 501: training accuarcy: 0.9022\n",
      "Epoch 1 step 501: training loss: 14065.655356665098\n",
      "Epoch 1 step 502: training accuarcy: 0.9114000000000001\n",
      "Epoch 1 step 502: training loss: 14047.244849692603\n",
      "Epoch 1 step 503: training accuarcy: 0.915\n",
      "Epoch 1 step 503: training loss: 14288.97750547294\n",
      "Epoch 1 step 504: training accuarcy: 0.9109\n",
      "Epoch 1 step 504: training loss: 13792.686824784027\n",
      "Epoch 1 step 505: training accuarcy: 0.9146000000000001\n",
      "Epoch 1 step 505: training loss: 14039.317728547883\n",
      "Epoch 1 step 506: training accuarcy: 0.9157000000000001\n",
      "Epoch 1 step 506: training loss: 14052.364304776725\n",
      "Epoch 1 step 507: training accuarcy: 0.9146000000000001\n",
      "Epoch 1 step 507: training loss: 13645.023833004367\n",
      "Epoch 1 step 508: training accuarcy: 0.9167000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 step 508: training loss: 13844.445084853465\n",
      "Epoch 1 step 509: training accuarcy: 0.917\n",
      "Epoch 1 step 509: training loss: 13775.352405666505\n",
      "Epoch 1 step 510: training accuarcy: 0.9159\n",
      "Epoch 1 step 510: training loss: 13937.938024954907\n",
      "Epoch 1 step 511: training accuarcy: 0.9141\n",
      "Epoch 1 step 511: training loss: 13485.92898365947\n",
      "Epoch 1 step 512: training accuarcy: 0.9215000000000001\n",
      "Epoch 1 step 512: training loss: 13773.379973430361\n",
      "Epoch 1 step 513: training accuarcy: 0.9141\n",
      "Epoch 1 step 513: training loss: 14302.648783475783\n",
      "Epoch 1 step 514: training accuarcy: 0.9108\n",
      "Epoch 1 step 514: training loss: 13754.366754954295\n",
      "Epoch 1 step 515: training accuarcy: 0.9154\n",
      "Epoch 1 step 515: training loss: 14055.161592770386\n",
      "Epoch 1 step 516: training accuarcy: 0.9127000000000001\n",
      "Epoch 1 step 516: training loss: 13590.540730265346\n",
      "Epoch 1 step 517: training accuarcy: 0.9182\n",
      "Epoch 1 step 517: training loss: 14293.18508265386\n",
      "Epoch 1 step 518: training accuarcy: 0.9125000000000001\n",
      "Epoch 1 step 518: training loss: 13659.83231767518\n",
      "Epoch 1 step 519: training accuarcy: 0.9165000000000001\n",
      "Epoch 1 step 519: training loss: 14021.44986186821\n",
      "Epoch 1 step 520: training accuarcy: 0.9106000000000001\n",
      "Epoch 1 step 520: training loss: 13677.29264200555\n",
      "Epoch 1 step 521: training accuarcy: 0.9184\n",
      "Epoch 1 step 521: training loss: 13872.296698874661\n",
      "Epoch 1 step 522: training accuarcy: 0.9146000000000001\n",
      "Epoch 1 step 522: training loss: 13903.782969430522\n",
      "Epoch 1 step 523: training accuarcy: 0.9129\n",
      "Epoch 1 step 523: training loss: 13306.622126120332\n",
      "Epoch 1 step 524: training accuarcy: 0.9186000000000001\n",
      "Epoch 1 step 524: training loss: 13301.91599346366\n",
      "Epoch 1 step 525: training accuarcy: 0.9186000000000001\n",
      "Epoch 1 step 525: training loss: 7123.241825909099\n",
      "Epoch 1 step 526: training accuarcy: 0.9158974358974359\n",
      "Epoch 1: train loss 15976.196669819967, train accuarcy 0.8896301984786987\n",
      "Epoch 1: valid loss 15801.941754075971, valid accuarcy 0.8880537152290344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [10:19<05:12, 312.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch 2 step 526: training loss: 12579.079971786003\n",
      "Epoch 2 step 527: training accuarcy: 0.932\n",
      "Epoch 2 step 527: training loss: 12467.619066556137\n",
      "Epoch 2 step 528: training accuarcy: 0.9273\n",
      "Epoch 2 step 528: training loss: 12956.344311152803\n",
      "Epoch 2 step 529: training accuarcy: 0.9261\n",
      "Epoch 2 step 529: training loss: 12257.875309358122\n",
      "Epoch 2 step 530: training accuarcy: 0.9339000000000001\n",
      "Epoch 2 step 530: training loss: 12327.81546683406\n",
      "Epoch 2 step 531: training accuarcy: 0.934\n",
      "Epoch 2 step 531: training loss: 12897.246040983051\n",
      "Epoch 2 step 532: training accuarcy: 0.9271\n",
      "Epoch 2 step 532: training loss: 12555.067693549696\n",
      "Epoch 2 step 533: training accuarcy: 0.9292\n",
      "Epoch 2 step 533: training loss: 12355.227634627197\n",
      "Epoch 2 step 534: training accuarcy: 0.9334\n",
      "Epoch 2 step 534: training loss: 12954.758991505158\n",
      "Epoch 2 step 535: training accuarcy: 0.9284\n",
      "Epoch 2 step 535: training loss: 12456.148345488487\n",
      "Epoch 2 step 536: training accuarcy: 0.9321\n",
      "Epoch 2 step 536: training loss: 12267.289753859326\n",
      "Epoch 2 step 537: training accuarcy: 0.9317000000000001\n",
      "Epoch 2 step 537: training loss: 12321.236526448349\n",
      "Epoch 2 step 538: training accuarcy: 0.9363\n",
      "Epoch 2 step 538: training loss: 12480.912403988636\n",
      "Epoch 2 step 539: training accuarcy: 0.9295\n",
      "Epoch 2 step 539: training loss: 12275.839111569077\n",
      "Epoch 2 step 540: training accuarcy: 0.9344\n",
      "Epoch 2 step 540: training loss: 12325.60669809585\n",
      "Epoch 2 step 541: training accuarcy: 0.9313\n",
      "Epoch 2 step 541: training loss: 12433.910904260623\n",
      "Epoch 2 step 542: training accuarcy: 0.9352\n",
      "Epoch 2 step 542: training loss: 12793.060817825726\n",
      "Epoch 2 step 543: training accuarcy: 0.9264\n",
      "Epoch 2 step 543: training loss: 12062.090456456732\n",
      "Epoch 2 step 544: training accuarcy: 0.9361\n",
      "Epoch 2 step 544: training loss: 12936.53844600427\n",
      "Epoch 2 step 545: training accuarcy: 0.927\n",
      "Epoch 2 step 545: training loss: 12772.251551927748\n",
      "Epoch 2 step 546: training accuarcy: 0.9281\n",
      "Epoch 2 step 546: training loss: 12028.122080641679\n",
      "Epoch 2 step 547: training accuarcy: 0.9315\n",
      "Epoch 2 step 547: training loss: 12441.671685080539\n",
      "Epoch 2 step 548: training accuarcy: 0.9343\n",
      "Epoch 2 step 548: training loss: 12230.798831367876\n",
      "Epoch 2 step 549: training accuarcy: 0.9348000000000001\n",
      "Epoch 2 step 549: training loss: 12397.000668349003\n",
      "Epoch 2 step 550: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 550: training loss: 12863.37087194978\n",
      "Epoch 2 step 551: training accuarcy: 0.924\n",
      "Epoch 2 step 551: training loss: 12602.293749940793\n",
      "Epoch 2 step 552: training accuarcy: 0.9292\n",
      "Epoch 2 step 552: training loss: 12748.903720412643\n",
      "Epoch 2 step 553: training accuarcy: 0.9228000000000001\n",
      "Epoch 2 step 553: training loss: 12636.907738385235\n",
      "Epoch 2 step 554: training accuarcy: 0.9296000000000001\n",
      "Epoch 2 step 554: training loss: 12399.67824987497\n",
      "Epoch 2 step 555: training accuarcy: 0.9278000000000001\n",
      "Epoch 2 step 555: training loss: 12514.089073828194\n",
      "Epoch 2 step 556: training accuarcy: 0.9312\n",
      "Epoch 2 step 556: training loss: 12660.01129281354\n",
      "Epoch 2 step 557: training accuarcy: 0.9266000000000001\n",
      "Epoch 2 step 557: training loss: 12373.075298495438\n",
      "Epoch 2 step 558: training accuarcy: 0.9316000000000001\n",
      "Epoch 2 step 558: training loss: 12376.913663473304\n",
      "Epoch 2 step 559: training accuarcy: 0.9312\n",
      "Epoch 2 step 559: training loss: 12861.679180628575\n",
      "Epoch 2 step 560: training accuarcy: 0.9242\n",
      "Epoch 2 step 560: training loss: 12183.146210513505\n",
      "Epoch 2 step 561: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 561: training loss: 12583.414860436009\n",
      "Epoch 2 step 562: training accuarcy: 0.9275\n",
      "Epoch 2 step 562: training loss: 12427.1461319801\n",
      "Epoch 2 step 563: training accuarcy: 0.9283\n",
      "Epoch 2 step 563: training loss: 12268.11259945213\n",
      "Epoch 2 step 564: training accuarcy: 0.93\n",
      "Epoch 2 step 564: training loss: 12409.58536401944\n",
      "Epoch 2 step 565: training accuarcy: 0.9284\n",
      "Epoch 2 step 565: training loss: 12982.030893855026\n",
      "Epoch 2 step 566: training accuarcy: 0.9207000000000001\n",
      "Epoch 2 step 566: training loss: 12285.509123749434\n",
      "Epoch 2 step 567: training accuarcy: 0.932\n",
      "Epoch 2 step 567: training loss: 12242.737663675933\n",
      "Epoch 2 step 568: training accuarcy: 0.93\n",
      "Epoch 2 step 568: training loss: 12285.381543985246\n",
      "Epoch 2 step 569: training accuarcy: 0.9274\n",
      "Epoch 2 step 569: training loss: 12708.454150655578\n",
      "Epoch 2 step 570: training accuarcy: 0.9266000000000001\n",
      "Epoch 2 step 570: training loss: 12218.739147589946\n",
      "Epoch 2 step 571: training accuarcy: 0.934\n",
      "Epoch 2 step 571: training loss: 12767.192302218384\n",
      "Epoch 2 step 572: training accuarcy: 0.922\n",
      "Epoch 2 step 572: training loss: 12791.831562561076\n",
      "Epoch 2 step 573: training accuarcy: 0.9273\n",
      "Epoch 2 step 573: training loss: 12448.080772564048\n",
      "Epoch 2 step 574: training accuarcy: 0.9314\n",
      "Epoch 2 step 574: training loss: 12337.672080294686\n",
      "Epoch 2 step 575: training accuarcy: 0.9274\n",
      "Epoch 2 step 575: training loss: 12390.376221320745\n",
      "Epoch 2 step 576: training accuarcy: 0.9301\n",
      "Epoch 2 step 576: training loss: 12253.579117310843\n",
      "Epoch 2 step 577: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 577: training loss: 12382.911057018155\n",
      "Epoch 2 step 578: training accuarcy: 0.9292\n",
      "Epoch 2 step 578: training loss: 12388.351390932487\n",
      "Epoch 2 step 579: training accuarcy: 0.9273\n",
      "Epoch 2 step 579: training loss: 12263.943314813376\n",
      "Epoch 2 step 580: training accuarcy: 0.9287000000000001\n",
      "Epoch 2 step 580: training loss: 11901.056028873652\n",
      "Epoch 2 step 581: training accuarcy: 0.9352\n",
      "Epoch 2 step 581: training loss: 12227.610360428862\n",
      "Epoch 2 step 582: training accuarcy: 0.9302\n",
      "Epoch 2 step 582: training loss: 12605.064552590959\n",
      "Epoch 2 step 583: training accuarcy: 0.9263\n",
      "Epoch 2 step 583: training loss: 12520.343108298095\n",
      "Epoch 2 step 584: training accuarcy: 0.9239\n",
      "Epoch 2 step 584: training loss: 12306.355394846658\n",
      "Epoch 2 step 585: training accuarcy: 0.9294\n",
      "Epoch 2 step 585: training loss: 12250.56602672824\n",
      "Epoch 2 step 586: training accuarcy: 0.9313\n",
      "Epoch 2 step 586: training loss: 12237.749990679438\n",
      "Epoch 2 step 587: training accuarcy: 0.9297000000000001\n",
      "Epoch 2 step 587: training loss: 12365.592013140775\n",
      "Epoch 2 step 588: training accuarcy: 0.9264\n",
      "Epoch 2 step 588: training loss: 12215.976714437902\n",
      "Epoch 2 step 589: training accuarcy: 0.9325\n",
      "Epoch 2 step 589: training loss: 12260.566548271208\n",
      "Epoch 2 step 590: training accuarcy: 0.931\n",
      "Epoch 2 step 590: training loss: 12373.277105863843\n",
      "Epoch 2 step 591: training accuarcy: 0.9254\n",
      "Epoch 2 step 591: training loss: 12622.811790882075\n",
      "Epoch 2 step 592: training accuarcy: 0.9251\n",
      "Epoch 2 step 592: training loss: 12651.808625681488\n",
      "Epoch 2 step 593: training accuarcy: 0.9249\n",
      "Epoch 2 step 593: training loss: 12458.26979199331\n",
      "Epoch 2 step 594: training accuarcy: 0.928\n",
      "Epoch 2 step 594: training loss: 12260.584604343945\n",
      "Epoch 2 step 595: training accuarcy: 0.9294\n",
      "Epoch 2 step 595: training loss: 12773.673025151815\n",
      "Epoch 2 step 596: training accuarcy: 0.9207000000000001\n",
      "Epoch 2 step 596: training loss: 12152.688711555998\n",
      "Epoch 2 step 597: training accuarcy: 0.9297000000000001\n",
      "Epoch 2 step 597: training loss: 12271.024068360988\n",
      "Epoch 2 step 598: training accuarcy: 0.9272\n",
      "Epoch 2 step 598: training loss: 12629.987784551502\n",
      "Epoch 2 step 599: training accuarcy: 0.9235000000000001\n",
      "Epoch 2 step 599: training loss: 12425.398545875063\n",
      "Epoch 2 step 600: training accuarcy: 0.9273\n",
      "Epoch 2 step 600: training loss: 12406.188696614525\n",
      "Epoch 2 step 601: training accuarcy: 0.9267000000000001\n",
      "Epoch 2 step 601: training loss: 12104.373441173559\n",
      "Epoch 2 step 602: training accuarcy: 0.9278000000000001\n",
      "Epoch 2 step 602: training loss: 12137.495938523476\n",
      "Epoch 2 step 603: training accuarcy: 0.93\n",
      "Epoch 2 step 603: training loss: 12532.696013054541\n",
      "Epoch 2 step 604: training accuarcy: 0.9239\n",
      "Epoch 2 step 604: training loss: 12767.699686667727\n",
      "Epoch 2 step 605: training accuarcy: 0.9244\n",
      "Epoch 2 step 605: training loss: 11961.526731370053\n",
      "Epoch 2 step 606: training accuarcy: 0.9362\n",
      "Epoch 2 step 606: training loss: 12476.536602976854\n",
      "Epoch 2 step 607: training accuarcy: 0.9264\n",
      "Epoch 2 step 607: training loss: 12328.7423127744\n",
      "Epoch 2 step 608: training accuarcy: 0.9304\n",
      "Epoch 2 step 608: training loss: 12713.297096140386\n",
      "Epoch 2 step 609: training accuarcy: 0.923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 609: training loss: 12326.172528546733\n",
      "Epoch 2 step 610: training accuarcy: 0.9286000000000001\n",
      "Epoch 2 step 610: training loss: 11747.185809195607\n",
      "Epoch 2 step 611: training accuarcy: 0.9336000000000001\n",
      "Epoch 2 step 611: training loss: 12297.782410734995\n",
      "Epoch 2 step 612: training accuarcy: 0.9331\n",
      "Epoch 2 step 612: training loss: 11963.905872445495\n",
      "Epoch 2 step 613: training accuarcy: 0.9299000000000001\n",
      "Epoch 2 step 613: training loss: 12053.08268501908\n",
      "Epoch 2 step 614: training accuarcy: 0.9276000000000001\n",
      "Epoch 2 step 614: training loss: 12080.945894977689\n",
      "Epoch 2 step 615: training accuarcy: 0.9289000000000001\n",
      "Epoch 2 step 615: training loss: 12388.82632281529\n",
      "Epoch 2 step 616: training accuarcy: 0.9233\n",
      "Epoch 2 step 616: training loss: 12094.645916003059\n",
      "Epoch 2 step 617: training accuarcy: 0.9303\n",
      "Epoch 2 step 617: training loss: 12037.060737366077\n",
      "Epoch 2 step 618: training accuarcy: 0.9322\n",
      "Epoch 2 step 618: training loss: 12039.309792153434\n",
      "Epoch 2 step 619: training accuarcy: 0.9323\n",
      "Epoch 2 step 619: training loss: 12242.255778695195\n",
      "Epoch 2 step 620: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 620: training loss: 12176.994180936581\n",
      "Epoch 2 step 621: training accuarcy: 0.9245000000000001\n",
      "Epoch 2 step 621: training loss: 11871.20518650512\n",
      "Epoch 2 step 622: training accuarcy: 0.9318000000000001\n",
      "Epoch 2 step 622: training loss: 12006.077342521538\n",
      "Epoch 2 step 623: training accuarcy: 0.9308000000000001\n",
      "Epoch 2 step 623: training loss: 11830.511827467495\n",
      "Epoch 2 step 624: training accuarcy: 0.9281\n",
      "Epoch 2 step 624: training loss: 12344.527444796244\n",
      "Epoch 2 step 625: training accuarcy: 0.9221\n",
      "Epoch 2 step 625: training loss: 11739.970098564092\n",
      "Epoch 2 step 626: training accuarcy: 0.9339000000000001\n",
      "Epoch 2 step 626: training loss: 11775.155182370916\n",
      "Epoch 2 step 627: training accuarcy: 0.9316000000000001\n",
      "Epoch 2 step 627: training loss: 11563.315685055966\n",
      "Epoch 2 step 628: training accuarcy: 0.9331\n",
      "Epoch 2 step 628: training loss: 12234.425675778048\n",
      "Epoch 2 step 629: training accuarcy: 0.9217000000000001\n",
      "Epoch 2 step 629: training loss: 11974.389033437608\n",
      "Epoch 2 step 630: training accuarcy: 0.9277000000000001\n",
      "Epoch 2 step 630: training loss: 11824.418717300552\n",
      "Epoch 2 step 631: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 631: training loss: 12326.469804946453\n",
      "Epoch 2 step 632: training accuarcy: 0.9264\n",
      "Epoch 2 step 632: training loss: 11949.508066088767\n",
      "Epoch 2 step 633: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 633: training loss: 12232.007424914038\n",
      "Epoch 2 step 634: training accuarcy: 0.9247000000000001\n",
      "Epoch 2 step 634: training loss: 11360.20607202164\n",
      "Epoch 2 step 635: training accuarcy: 0.9348000000000001\n",
      "Epoch 2 step 635: training loss: 11790.926020676769\n",
      "Epoch 2 step 636: training accuarcy: 0.933\n",
      "Epoch 2 step 636: training loss: 11897.147193098084\n",
      "Epoch 2 step 637: training accuarcy: 0.9303\n",
      "Epoch 2 step 637: training loss: 12120.041600671819\n",
      "Epoch 2 step 638: training accuarcy: 0.9282\n",
      "Epoch 2 step 638: training loss: 11850.81633377847\n",
      "Epoch 2 step 639: training accuarcy: 0.9293\n",
      "Epoch 2 step 639: training loss: 12029.935052960196\n",
      "Epoch 2 step 640: training accuarcy: 0.9281\n",
      "Epoch 2 step 640: training loss: 12148.477223926177\n",
      "Epoch 2 step 641: training accuarcy: 0.9264\n",
      "Epoch 2 step 641: training loss: 11716.750310218073\n",
      "Epoch 2 step 642: training accuarcy: 0.9316000000000001\n",
      "Epoch 2 step 642: training loss: 12292.283687132169\n",
      "Epoch 2 step 643: training accuarcy: 0.9242\n",
      "Epoch 2 step 643: training loss: 11537.960109953965\n",
      "Epoch 2 step 644: training accuarcy: 0.9329000000000001\n",
      "Epoch 2 step 644: training loss: 11547.947763069149\n",
      "Epoch 2 step 645: training accuarcy: 0.9342\n",
      "Epoch 2 step 645: training loss: 12015.672488907336\n",
      "Epoch 2 step 646: training accuarcy: 0.9291\n",
      "Epoch 2 step 646: training loss: 11863.355888829388\n",
      "Epoch 2 step 647: training accuarcy: 0.931\n",
      "Epoch 2 step 647: training loss: 11938.244527859963\n",
      "Epoch 2 step 648: training accuarcy: 0.9303\n",
      "Epoch 2 step 648: training loss: 12061.833477667893\n",
      "Epoch 2 step 649: training accuarcy: 0.9272\n",
      "Epoch 2 step 649: training loss: 11398.914598959633\n",
      "Epoch 2 step 650: training accuarcy: 0.9329000000000001\n",
      "Epoch 2 step 650: training loss: 11959.159986304305\n",
      "Epoch 2 step 651: training accuarcy: 0.9238000000000001\n",
      "Epoch 2 step 651: training loss: 11944.912445953752\n",
      "Epoch 2 step 652: training accuarcy: 0.9258000000000001\n",
      "Epoch 2 step 652: training loss: 12054.596292005905\n",
      "Epoch 2 step 653: training accuarcy: 0.9251\n",
      "Epoch 2 step 653: training loss: 11991.484527828687\n",
      "Epoch 2 step 654: training accuarcy: 0.9319000000000001\n",
      "Epoch 2 step 654: training loss: 12075.905525327562\n",
      "Epoch 2 step 655: training accuarcy: 0.9235000000000001\n",
      "Epoch 2 step 655: training loss: 11695.023704545143\n",
      "Epoch 2 step 656: training accuarcy: 0.9291\n",
      "Epoch 2 step 656: training loss: 11598.578139420813\n",
      "Epoch 2 step 657: training accuarcy: 0.9313\n",
      "Epoch 2 step 657: training loss: 11623.194624225274\n",
      "Epoch 2 step 658: training accuarcy: 0.9331\n",
      "Epoch 2 step 658: training loss: 11880.263086875213\n",
      "Epoch 2 step 659: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 659: training loss: 11880.525127408937\n",
      "Epoch 2 step 660: training accuarcy: 0.929\n",
      "Epoch 2 step 660: training loss: 11927.432812966625\n",
      "Epoch 2 step 661: training accuarcy: 0.9292\n",
      "Epoch 2 step 661: training loss: 11770.488546483704\n",
      "Epoch 2 step 662: training accuarcy: 0.9304\n",
      "Epoch 2 step 662: training loss: 12075.94976677821\n",
      "Epoch 2 step 663: training accuarcy: 0.926\n",
      "Epoch 2 step 663: training loss: 11837.442798963642\n",
      "Epoch 2 step 664: training accuarcy: 0.9283\n",
      "Epoch 2 step 664: training loss: 11925.576644359993\n",
      "Epoch 2 step 665: training accuarcy: 0.9266000000000001\n",
      "Epoch 2 step 665: training loss: 11982.571157794284\n",
      "Epoch 2 step 666: training accuarcy: 0.9279000000000001\n",
      "Epoch 2 step 666: training loss: 12014.668253196023\n",
      "Epoch 2 step 667: training accuarcy: 0.9269000000000001\n",
      "Epoch 2 step 667: training loss: 11739.496964330801\n",
      "Epoch 2 step 668: training accuarcy: 0.9291\n",
      "Epoch 2 step 668: training loss: 11553.305384302399\n",
      "Epoch 2 step 669: training accuarcy: 0.9311\n",
      "Epoch 2 step 669: training loss: 11816.155255761949\n",
      "Epoch 2 step 670: training accuarcy: 0.9313\n",
      "Epoch 2 step 670: training loss: 11671.141259563972\n",
      "Epoch 2 step 671: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 671: training loss: 11945.618120281824\n",
      "Epoch 2 step 672: training accuarcy: 0.9277000000000001\n",
      "Epoch 2 step 672: training loss: 11741.716352200101\n",
      "Epoch 2 step 673: training accuarcy: 0.9265\n",
      "Epoch 2 step 673: training loss: 11633.884193832697\n",
      "Epoch 2 step 674: training accuarcy: 0.9275\n",
      "Epoch 2 step 674: training loss: 11938.509849367205\n",
      "Epoch 2 step 675: training accuarcy: 0.9239\n",
      "Epoch 2 step 675: training loss: 11361.287915536996\n",
      "Epoch 2 step 676: training accuarcy: 0.9318000000000001\n",
      "Epoch 2 step 676: training loss: 11609.832039564606\n",
      "Epoch 2 step 677: training accuarcy: 0.9299000000000001\n",
      "Epoch 2 step 677: training loss: 11299.545016285796\n",
      "Epoch 2 step 678: training accuarcy: 0.9323\n",
      "Epoch 2 step 678: training loss: 11774.108265410916\n",
      "Epoch 2 step 679: training accuarcy: 0.93\n",
      "Epoch 2 step 679: training loss: 11863.307208188164\n",
      "Epoch 2 step 680: training accuarcy: 0.9284\n",
      "Epoch 2 step 680: training loss: 11461.880473751084\n",
      "Epoch 2 step 681: training accuarcy: 0.9356000000000001\n",
      "Epoch 2 step 681: training loss: 11321.040068691495\n",
      "Epoch 2 step 682: training accuarcy: 0.9328000000000001\n",
      "Epoch 2 step 682: training loss: 11810.496575139205\n",
      "Epoch 2 step 683: training accuarcy: 0.9284\n",
      "Epoch 2 step 683: training loss: 11900.054049584809\n",
      "Epoch 2 step 684: training accuarcy: 0.925\n",
      "Epoch 2 step 684: training loss: 11730.712781024413\n",
      "Epoch 2 step 685: training accuarcy: 0.9276000000000001\n",
      "Epoch 2 step 685: training loss: 11517.577470591576\n",
      "Epoch 2 step 686: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 686: training loss: 11516.29370667542\n",
      "Epoch 2 step 687: training accuarcy: 0.9314\n",
      "Epoch 2 step 687: training loss: 12089.153903208591\n",
      "Epoch 2 step 688: training accuarcy: 0.9233\n",
      "Epoch 2 step 688: training loss: 11785.648075404135\n",
      "Epoch 2 step 689: training accuarcy: 0.9288000000000001\n",
      "Epoch 2 step 689: training loss: 11445.513514406359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 690: training accuarcy: 0.9318000000000001\n",
      "Epoch 2 step 690: training loss: 11941.249305893562\n",
      "Epoch 2 step 691: training accuarcy: 0.9281\n",
      "Epoch 2 step 691: training loss: 11631.430190287456\n",
      "Epoch 2 step 692: training accuarcy: 0.9298000000000001\n",
      "Epoch 2 step 692: training loss: 11589.049984246412\n",
      "Epoch 2 step 693: training accuarcy: 0.9275\n",
      "Epoch 2 step 693: training loss: 11455.207522423885\n",
      "Epoch 2 step 694: training accuarcy: 0.9323\n",
      "Epoch 2 step 694: training loss: 11613.57102550763\n",
      "Epoch 2 step 695: training accuarcy: 0.9311\n",
      "Epoch 2 step 695: training loss: 11602.339899947161\n",
      "Epoch 2 step 696: training accuarcy: 0.9281\n",
      "Epoch 2 step 696: training loss: 11640.08287377311\n",
      "Epoch 2 step 697: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 697: training loss: 11676.68072996902\n",
      "Epoch 2 step 698: training accuarcy: 0.9277000000000001\n",
      "Epoch 2 step 698: training loss: 11423.882087064068\n",
      "Epoch 2 step 699: training accuarcy: 0.9314\n",
      "Epoch 2 step 699: training loss: 11673.403059396373\n",
      "Epoch 2 step 700: training accuarcy: 0.929\n",
      "Epoch 2 step 700: training loss: 11654.927845489674\n",
      "Epoch 2 step 701: training accuarcy: 0.9302\n",
      "Epoch 2 step 701: training loss: 11335.314574051945\n",
      "Epoch 2 step 702: training accuarcy: 0.9294\n",
      "Epoch 2 step 702: training loss: 11749.012445113787\n",
      "Epoch 2 step 703: training accuarcy: 0.9293\n",
      "Epoch 2 step 703: training loss: 11746.239944835159\n",
      "Epoch 2 step 704: training accuarcy: 0.9286000000000001\n",
      "Epoch 2 step 704: training loss: 11669.32979143395\n",
      "Epoch 2 step 705: training accuarcy: 0.9264\n",
      "Epoch 2 step 705: training loss: 11235.825676135762\n",
      "Epoch 2 step 706: training accuarcy: 0.9311\n",
      "Epoch 2 step 706: training loss: 11244.10752013971\n",
      "Epoch 2 step 707: training accuarcy: 0.9353\n",
      "Epoch 2 step 707: training loss: 11258.405568813514\n",
      "Epoch 2 step 708: training accuarcy: 0.9312\n",
      "Epoch 2 step 708: training loss: 11554.727337526487\n",
      "Epoch 2 step 709: training accuarcy: 0.9261\n",
      "Epoch 2 step 709: training loss: 11822.193484453937\n",
      "Epoch 2 step 710: training accuarcy: 0.9244\n",
      "Epoch 2 step 710: training loss: 11533.552823292752\n",
      "Epoch 2 step 711: training accuarcy: 0.928\n",
      "Epoch 2 step 711: training loss: 11558.920695873365\n",
      "Epoch 2 step 712: training accuarcy: 0.9294\n",
      "Epoch 2 step 712: training loss: 11301.704194675614\n",
      "Epoch 2 step 713: training accuarcy: 0.9324\n",
      "Epoch 2 step 713: training loss: 11362.876793843847\n",
      "Epoch 2 step 714: training accuarcy: 0.9299000000000001\n",
      "Epoch 2 step 714: training loss: 11566.324347335967\n",
      "Epoch 2 step 715: training accuarcy: 0.9317000000000001\n",
      "Epoch 2 step 715: training loss: 11322.11976715055\n",
      "Epoch 2 step 716: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 716: training loss: 11079.916791719595\n",
      "Epoch 2 step 717: training accuarcy: 0.9344\n",
      "Epoch 2 step 717: training loss: 11236.155903927098\n",
      "Epoch 2 step 718: training accuarcy: 0.9335\n",
      "Epoch 2 step 718: training loss: 11430.769524363392\n",
      "Epoch 2 step 719: training accuarcy: 0.9292\n",
      "Epoch 2 step 719: training loss: 11259.69812858109\n",
      "Epoch 2 step 720: training accuarcy: 0.9328000000000001\n",
      "Epoch 2 step 720: training loss: 11181.643792351033\n",
      "Epoch 2 step 721: training accuarcy: 0.9304\n",
      "Epoch 2 step 721: training loss: 12080.763737177786\n",
      "Epoch 2 step 722: training accuarcy: 0.9221\n",
      "Epoch 2 step 722: training loss: 11261.942456386781\n",
      "Epoch 2 step 723: training accuarcy: 0.9343\n",
      "Epoch 2 step 723: training loss: 11743.348700366716\n",
      "Epoch 2 step 724: training accuarcy: 0.9312\n",
      "Epoch 2 step 724: training loss: 11230.675970086095\n",
      "Epoch 2 step 725: training accuarcy: 0.9341\n",
      "Epoch 2 step 725: training loss: 11761.100599499374\n",
      "Epoch 2 step 726: training accuarcy: 0.9274\n",
      "Epoch 2 step 726: training loss: 11512.069917121942\n",
      "Epoch 2 step 727: training accuarcy: 0.928\n",
      "Epoch 2 step 727: training loss: 11216.426510554038\n",
      "Epoch 2 step 728: training accuarcy: 0.9325\n",
      "Epoch 2 step 728: training loss: 11364.969198328894\n",
      "Epoch 2 step 729: training accuarcy: 0.933\n",
      "Epoch 2 step 729: training loss: 11345.601454560221\n",
      "Epoch 2 step 730: training accuarcy: 0.9305\n",
      "Epoch 2 step 730: training loss: 10917.5477785084\n",
      "Epoch 2 step 731: training accuarcy: 0.9376000000000001\n",
      "Epoch 2 step 731: training loss: 11473.402800799719\n",
      "Epoch 2 step 732: training accuarcy: 0.93\n",
      "Epoch 2 step 732: training loss: 10911.405607991439\n",
      "Epoch 2 step 733: training accuarcy: 0.9346000000000001\n",
      "Epoch 2 step 733: training loss: 11386.213645049698\n",
      "Epoch 2 step 734: training accuarcy: 0.9288000000000001\n",
      "Epoch 2 step 734: training loss: 11365.228674850801\n",
      "Epoch 2 step 735: training accuarcy: 0.9323\n",
      "Epoch 2 step 735: training loss: 11232.517662412198\n",
      "Epoch 2 step 736: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 736: training loss: 11406.309748691048\n",
      "Epoch 2 step 737: training accuarcy: 0.9289000000000001\n",
      "Epoch 2 step 737: training loss: 11264.116501986398\n",
      "Epoch 2 step 738: training accuarcy: 0.9307000000000001\n",
      "Epoch 2 step 738: training loss: 10963.25297623809\n",
      "Epoch 2 step 739: training accuarcy: 0.9369000000000001\n",
      "Epoch 2 step 739: training loss: 10857.75786876929\n",
      "Epoch 2 step 740: training accuarcy: 0.9404\n",
      "Epoch 2 step 740: training loss: 11364.663105219512\n",
      "Epoch 2 step 741: training accuarcy: 0.9295\n",
      "Epoch 2 step 741: training loss: 11083.605329936283\n",
      "Epoch 2 step 742: training accuarcy: 0.9335\n",
      "Epoch 2 step 742: training loss: 11571.319400463159\n",
      "Epoch 2 step 743: training accuarcy: 0.925\n",
      "Epoch 2 step 743: training loss: 11367.030407168368\n",
      "Epoch 2 step 744: training accuarcy: 0.9314\n",
      "Epoch 2 step 744: training loss: 11051.482619387469\n",
      "Epoch 2 step 745: training accuarcy: 0.9341\n",
      "Epoch 2 step 745: training loss: 11233.40428102017\n",
      "Epoch 2 step 746: training accuarcy: 0.9331\n",
      "Epoch 2 step 746: training loss: 11073.762324073115\n",
      "Epoch 2 step 747: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 747: training loss: 11015.483837999802\n",
      "Epoch 2 step 748: training accuarcy: 0.9318000000000001\n",
      "Epoch 2 step 748: training loss: 11214.232217287301\n",
      "Epoch 2 step 749: training accuarcy: 0.9326000000000001\n",
      "Epoch 2 step 749: training loss: 10838.66032523348\n",
      "Epoch 2 step 750: training accuarcy: 0.937\n",
      "Epoch 2 step 750: training loss: 11296.564457328277\n",
      "Epoch 2 step 751: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 751: training loss: 11010.318927517254\n",
      "Epoch 2 step 752: training accuarcy: 0.9353\n",
      "Epoch 2 step 752: training loss: 11249.850744270589\n",
      "Epoch 2 step 753: training accuarcy: 0.9322\n",
      "Epoch 2 step 753: training loss: 11345.000462756348\n",
      "Epoch 2 step 754: training accuarcy: 0.9301\n",
      "Epoch 2 step 754: training loss: 11224.687621864385\n",
      "Epoch 2 step 755: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 755: training loss: 10918.875526438864\n",
      "Epoch 2 step 756: training accuarcy: 0.936\n",
      "Epoch 2 step 756: training loss: 11135.161675976165\n",
      "Epoch 2 step 757: training accuarcy: 0.9332\n",
      "Epoch 2 step 757: training loss: 11436.535951979586\n",
      "Epoch 2 step 758: training accuarcy: 0.929\n",
      "Epoch 2 step 758: training loss: 11498.694095742605\n",
      "Epoch 2 step 759: training accuarcy: 0.9295\n",
      "Epoch 2 step 759: training loss: 11269.999209233678\n",
      "Epoch 2 step 760: training accuarcy: 0.9279000000000001\n",
      "Epoch 2 step 760: training loss: 11221.088370034553\n",
      "Epoch 2 step 761: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 761: training loss: 11032.972368587192\n",
      "Epoch 2 step 762: training accuarcy: 0.9321\n",
      "Epoch 2 step 762: training loss: 11247.469509888288\n",
      "Epoch 2 step 763: training accuarcy: 0.9281\n",
      "Epoch 2 step 763: training loss: 11164.102859707538\n",
      "Epoch 2 step 764: training accuarcy: 0.9306000000000001\n",
      "Epoch 2 step 764: training loss: 11775.2354771164\n",
      "Epoch 2 step 765: training accuarcy: 0.9247000000000001\n",
      "Epoch 2 step 765: training loss: 11094.676157766307\n",
      "Epoch 2 step 766: training accuarcy: 0.9331\n",
      "Epoch 2 step 766: training loss: 10699.564524911411\n",
      "Epoch 2 step 767: training accuarcy: 0.9396\n",
      "Epoch 2 step 767: training loss: 11265.424328296795\n",
      "Epoch 2 step 768: training accuarcy: 0.9309000000000001\n",
      "Epoch 2 step 768: training loss: 11421.199630228046\n",
      "Epoch 2 step 769: training accuarcy: 0.9266000000000001\n",
      "Epoch 2 step 769: training loss: 11175.040994174198\n",
      "Epoch 2 step 770: training accuarcy: 0.9284\n",
      "Epoch 2 step 770: training loss: 11059.527509646898\n",
      "Epoch 2 step 771: training accuarcy: 0.9321\n",
      "Epoch 2 step 771: training loss: 10982.552014572946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 step 772: training accuarcy: 0.9314\n",
      "Epoch 2 step 772: training loss: 11156.548918593204\n",
      "Epoch 2 step 773: training accuarcy: 0.9317000000000001\n",
      "Epoch 2 step 773: training loss: 10960.9980655112\n",
      "Epoch 2 step 774: training accuarcy: 0.9326000000000001\n",
      "Epoch 2 step 774: training loss: 10597.352543006278\n",
      "Epoch 2 step 775: training accuarcy: 0.936\n",
      "Epoch 2 step 775: training loss: 10752.717864063485\n",
      "Epoch 2 step 776: training accuarcy: 0.9375\n",
      "Epoch 2 step 776: training loss: 10911.885200742496\n",
      "Epoch 2 step 777: training accuarcy: 0.9334\n",
      "Epoch 2 step 777: training loss: 11166.66684664055\n",
      "Epoch 2 step 778: training accuarcy: 0.9295\n",
      "Epoch 2 step 778: training loss: 10970.034636902245\n",
      "Epoch 2 step 779: training accuarcy: 0.9328000000000001\n",
      "Epoch 2 step 779: training loss: 10721.367067623518\n",
      "Epoch 2 step 780: training accuarcy: 0.9366000000000001\n",
      "Epoch 2 step 780: training loss: 11074.548780542173\n",
      "Epoch 2 step 781: training accuarcy: 0.9321\n",
      "Epoch 2 step 781: training loss: 11194.387461613402\n",
      "Epoch 2 step 782: training accuarcy: 0.933\n",
      "Epoch 2 step 782: training loss: 10774.664290708652\n",
      "Epoch 2 step 783: training accuarcy: 0.9352\n",
      "Epoch 2 step 783: training loss: 11283.825982433795\n",
      "Epoch 2 step 784: training accuarcy: 0.9315\n",
      "Epoch 2 step 784: training loss: 10618.858911559093\n",
      "Epoch 2 step 785: training accuarcy: 0.9379000000000001\n",
      "Epoch 2 step 785: training loss: 10968.066095281467\n",
      "Epoch 2 step 786: training accuarcy: 0.9317000000000001\n",
      "Epoch 2 step 786: training loss: 11000.806166123677\n",
      "Epoch 2 step 787: training accuarcy: 0.9328000000000001\n",
      "Epoch 2 step 787: training loss: 11054.02644501294\n",
      "Epoch 2 step 788: training accuarcy: 0.9327000000000001\n",
      "Epoch 2 step 788: training loss: 5691.466533856963\n",
      "Epoch 2 step 789: training accuarcy: 0.9251282051282051\n",
      "Epoch 2: train loss 11808.000112529522, train accuarcy 0.9213521480560303\n",
      "Epoch 2: valid loss 13300.429108328372, valid accuarcy 0.9048920273780823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [15:24<00:00, 310.22s/it]\n"
     ]
    }
   ],
   "source": [
    "trans_learner.fit(epoch=3,\n",
    "                  log_dir=get_log_dir('weight_topcoder', 'trans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T13:33:06.295196Z",
     "start_time": "2019-10-09T13:33:06.284191Z"
    }
   },
   "outputs": [],
   "source": [
    "del trans_model\n",
    "T.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
