{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T14:37:06.816647Z",
     "start_time": "2019-07-12T14:37:06.108144Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c54387f8e028>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_logger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import torch as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "from utils import build_logger\n",
    "\n",
    "logger = build_logger()\n",
    "\n",
    "\n",
    "def cat_collate(batch) -> Tensor:\n",
    "    return T.tensor(batch, dtype=T.long)\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, dataset_type: str):\n",
    "        self.dataset_type = dataset_type\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx].values\n",
    "\n",
    "\n",
    "class TorchMovielen10k:\n",
    "    def __init__(self, file_path: Path, user_min: int = 4, item_min=4):\n",
    "        df = pd.read_csv(file_path,\n",
    "                         header=None,\n",
    "                         sep='\\t',\n",
    "                         names=['user_id', 'item_id', 'rating', 'time'],\n",
    "                         dtype={\n",
    "                             'user_id': 'int32',\n",
    "                             'item_id': 'int32',\n",
    "                             'rating': 'int32',\n",
    "                             'time': 'int32'\n",
    "                         })\n",
    "\n",
    "        logger.info(\"Read dataset in {}\".format(file_path))\n",
    "        user_counts = df.user_id.value_counts()\n",
    "        item_counts = df.item_id.value_counts()\n",
    "\n",
    "        logger.info(\"Original user size: {}\".format(user_counts.size))\n",
    "        logger.info(\"Original item size: {}\".format(item_counts.size))\n",
    "\n",
    "        # get user and tiem category info\n",
    "        user_counts = user_counts[user_counts >= user_min]\n",
    "        item_counts = item_counts[item_counts >= item_min]\n",
    "\n",
    "        logger.info(\"Filter user size: {}\".format(user_counts.size))\n",
    "        logger.info(\"Filter item size: {}\".format(item_counts.size))\n",
    "\n",
    "        # remove sparse item\n",
    "        df = df[df.user_id.isin(user_counts.index)]\n",
    "        df = df[df.item_id.isin(item_counts.index)]\n",
    "\n",
    "        # Add previous item\n",
    "        df['prev_item_id'] = df.item_id\n",
    "        data = df['prev_item_id'].values\n",
    "        data = np.roll(data, 1)\n",
    "        df['prev_item_id'] = data\n",
    "\n",
    "        # split train and test ddataframe\n",
    "        df = df.sort_values(by=['time'])\n",
    "        duplicate_mask = df.duplicated(subset=['user_id'], keep='last')\n",
    "        remain_df = df[duplicate_mask]\n",
    "        test_df = df[~duplicate_mask]\n",
    "        duplicate_mask = remain_df.duplicated(subset=['user_id'], keep='last')\n",
    "        train_df = remain_df[duplicate_mask]\n",
    "        valid_df = remain_df[~duplicate_mask]\n",
    "\n",
    "        # encode feature\n",
    "        cat_names = ['user_id', 'item_id', 'prev_item_id']\n",
    "        ordinal_encoder = OrdinalEncoder(categories='auto', dtype='int32')\n",
    "        ordinal_encoder.fit(train_df[cat_names])\n",
    "\n",
    "        data = ordinal_encoder.transform(train_df[cat_names])\n",
    "        train_df[cat_names] = data\n",
    "\n",
    "        data = ordinal_encoder.transform(valid_df[cat_names])\n",
    "        valid_df[cat_names] = data\n",
    "\n",
    "        data = ordinal_encoder.transform(test_df[cat_names])\n",
    "        test_df[cat_names] = data\n",
    "\n",
    "        # Set first item non for each user\n",
    "        train_df.sort_values(by=['user_id'])\n",
    "        first_mask = ~train_df.duplicated(subset=['user_id'], keep='first')\n",
    "        train_df['prev_item_id'][first_mask] = -1\n",
    "\n",
    "        # set train, valid, test\n",
    "        self.cat_names = cat_names\n",
    "        self.cat_dict = {\n",
    "            name: cat_array\n",
    "            for name, cat_array in (cat_names, ordinal_encoder.categories_)\n",
    "        }\n",
    "        self.df_dict = {\n",
    "            'train': train_df[cat_names],\n",
    "            'valid': valid_df[cat_names],\n",
    "            'test': test_df[cat_names]\n",
    "        }\n",
    "\n",
    "    def get_dataloader(self,\n",
    "                       dataset_type: str,\n",
    "                       batch_size: int = 32,\n",
    "                       shuffle: bool = True,\n",
    "                       num_workers: int = 4) -> DataLoader:\n",
    "        assert dataset_type in ['train', 'valid',\n",
    "                                'test'], \"Don't contain dataset type\"\n",
    "\n",
    "        ds = TabularDataset(self.df_dict[dataset_type], dataset_type)\n",
    "        return DataLoader(ds,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          collate_fn=cat_collate,\n",
    "                          num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
